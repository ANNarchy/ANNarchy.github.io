[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ANNarchy (Artificial Neural Networks architect)",
    "section": "",
    "text": "ANNarchy (Artificial Neural Networks architect) is a Python neurosimulator designed for rate-coded and spiking neural networks, released under the GNU GPL v2 or later.\n\n\n\nResources\n\nSource code: github.com/ANNarchy/ANNarchy\nDocumentation: annarchy.github.io\nForum: Google Forum\nBug reports and feature requests: Github Issues\n\n\nDocumentation\n\nInstallation instructions.\nTutorial for a quick presentation of the simulator.\nManual with a full description of the functionalities.\nNotebooks showcasing many different networks.\nReference implementation with all classes and functions.\n\n\n\n\n\n\n\n\n\n\nCitation\nIf you use ANNarchy for your research, we would appreciate if you cite the following paper. See the full list of publications related to ANNarchy here.\n\nVitay J, Dinkelbach HÜ and Hamker FH (2015). ANNarchy: a code generation approach to neural simulations on parallel hardware. Frontiers in Neuroinformatics 9:19. doi:10.3389/fninf.2015.00019\n\n@article{Vitay2015,\n  title = {{{ANNarchy}}: A Code Generation Approach to Neural Simulations on Parallel Hardware},\n  author = {Vitay, Julien and Dinkelbach, Helge {\\\"U}. and Hamker, Fred H.},\n  year = {2015},\n  journal = {Frontiers in Neuroinformatics},\n  volume = {9},\n  number = {19},\n  doi = {10.3389/fninf.2015.00019},\n  url = {https://www.frontiersin.org/articles/10.3389/fninf.2015.00019},\n  abstract = {Many modern neural simulators focus on the simulation of networks of spiking neurons on parallel hardware. Another important framework in computational neuroscience, rate-coded neural networks, is mostly difficult or impossible to implement using these simulators. We present here the ANNarchy (Artificial Neural Networks architect) neural simulator, which allows to easily define and simulate rate-coded and spiking networks, as well as combinations of both. The interface in Python has been designed to be close to the PyNN interface, while the definition of neuron and synapse models can be specified using an equation-oriented mathematical description similar to the Brian neural simulator. This information is used to generate C++ code that will efficiently perform the simulation on the chosen parallel hardware (multi-core system or graphical processing unit). Several numerical methods are available to transform ordinary differential equations into an efficient C++ code. We compare the parallel performance of the simulator to existing solutions.}\n}",
    "crumbs": [
      "ANNarchy"
    ]
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Oliver Maith, Helge Ülo Dinkelbach, Javier Baladron, Julien Vitay, and Fred H. Hamker (2022).\nBOLD monitoring in the neural simulator ANNarchy.\nFrontiers in Neuroinformatics 16:790966\ndoi:10.3389/fninf.2022.790966\nHelge Ülo Dinkelbach, Badr-Eddine Bouhlal, Julien Vitay, and Fred H. Hamker (2022).\nAuto-selection of an optimal sparse matrix format in the neuro-simulator ANNarchy.\nFrontiers in Neuroinformatics 16:877945\ndoi:10.3389/fninf.2022.877945\nHelge Ülo Dinkelbach, Julien Vitay, and Fred H. Hamker (2019).\nScalable simulation of rate-coded and spiking neural networks on shared memory systems.\n2019 Conference on Cognitive Computational Neuroscience, 13-16 September 2019, Berlin, Germany.\ndoi:10.32470/CCN.2019.1109-0\nJulien Vitay, Helge Ülo Dinkelbach, and Fred H. Hamker (2015).\nANNarchy: a code generation approach to neural simulations on parallel hardware.\nFrontiers in Neuroinformatics 9:19\ndoi:10.3389/fninf.2015.00019\nHelge Ü. Dinkelbach, Julien Vitay, and Fred H. Hamker (2012).\nComparison of GPU- and CPU-implementations of mean-firing rate neural networks on parallel hardware.\nNetwork: Computation in Neural Systems 23(4)\ndoi:10.3109/0954898X.2012.739292",
    "crumbs": [
      "ANNarchy",
      "Publications"
    ]
  },
  {
    "objectID": "manual/Recording.html",
    "href": "manual/Recording.html",
    "title": "Monitors",
    "section": "",
    "text": "Between two calls to simulate(), all neural and synaptic variables can be accessed through the generated attributes. The evolution of neural or synaptic variables during a simulation phase can be selectively recorded using Monitor objects.\nThe Monitor object can be created at any time (before or after compile()) to record any variable of a Population, PopulationView, Dendrite or Projection.\nm_pop = net.monitor(pop, 'r')\nm_proj = net.monitor(proj.dendrite(0), 'w')\n\n\n\n\n\n\nImportant\n\n\n\nThe value of each variable is stored for every simulation step in the RAM. For huge networks and long simulations, this can very rapidly fill up the available memory and lead to cache defaults, thereby degrading strongly the performance. It is the user’s responsability to record only the needed variables and to regularly save the values in a file.\n\n\n\n\nThe Monitor object takes four arguments:\n\nobj: the object to monitor. It can be a population, a population view (a slice of a population or an individual neuron), a dendrite (the synapses of a projection which reach a single post-synaptic neuron) or a projection.\nvariables: a (list of) variable name(s) which should be recorded. They should be variables of the neuron/synapse model of the corresponding object. Although it generally makes no sense, you can also record parameters of an object. By definition a parameter is constant throughout a simulation, but it maybe useful when tracking externally-set inputs, for example. You can know which attributes are recordable by checking the attributes attribute of the object (pop.attributes or proj.attributes).\nperiod: the period in ms at which recordings should be made. By default, recording is done after each simulation step (dt), but this may be overkill in long simulations.\nstart: boolean value stating if the recordings should start immediately after the creation of the monitor (default), or if it should be started later.\n\nSome examples:\nm = net.monitor(pop, 'r') # record r in all neurons of pop\nm = net.monitor(pop, ['r', 'v']) # record r and v of all neurons\nm = net.monitor(pop[:100], 'r', period=10.0) # record r in the first 100 neurons of pop, every 10 ms\nm = net.monitor(pop, 'r', start=False) # record r in all neurons, but do not start recording\nSpiking networks additionally allow to record the spike events in a population (see later). You also can record conductances (e.g. g_exc) and weighted sums of inputs in rate-coded networks (sum(exc)) the same way:\nm = net.nonitor(pop, ['spike', 'g_exc', 'g_inh'])\nm = net.monitor(pop, ['r', 'sum(exc)', 'sum(inh)'])\n\n\nIf start is set to False, recordings can be started later by calling the start() method:\nm = net.monitor(pop, 'r', start=False)\n\nnet.simulate(100.)\nm.start()\nnet.simulate(100.)\nIn this case, only the last 100 ms of the simulation will be recorded. Otherwise, recording would start immediately after the creation of the object.\n\n\n\nIf you are interested in recording only specific periods of the simulation, you can ause and resume recordings:\nm = net.monitor(pop, 'r')\n\nnet.simulate(100.)\nm.pause()\nnet.simulate(1000.)\nm.resume()\nnet.simulate(100.)\nIn this example, only the first and last 100 ms of the simulation are recorded.\n\n\n\nThe recorded values are obtained through the get() method. If no argument is passed, a dictionary is returned with one element per recorded variable. If the name of a variable is passed (for example get('r')), the recorded values for this variable are directly returned:\nm = net.monitor(pop, ['r', 'v'])\n\nann.simulate(100.)\n\ndata = m.get() # both 'r' and 'v'\n\nann.simulate(100.)\n\nr = m.get('r')\nv = m.get('v')\nIn the example above, data is a dictionary with two keys 'r' and 'v', while r and v are directly the recorded arrays.\nThe recorded values are Numpy arrays with two dimensions, the first one representing time, the second one representing the ranks of the recorded neurons.\nFor example, the time course of the firing rate of the neuron of rank 15 is accessed through:\ndata['r'][:, 15]\nThe firing rates of the whole population after 50 ms of simulation are accessed with:\ndata['r'][50, :]\n\n\n\n\n\n\nNote\n\n\n\nOnce you call get(), the internal data is erased, so calling it immediately afterwards will return an empty recording data. You need to simulate again in order to retrieve new values.\n\n\nRepresentation of time\nThe time indices are in simulation steps (integers), not in real time (ms). If dt is different from 1.0, this indices must be multiplied by dt() in order to plot real times:\nnet = ann.Network(dt=0.1)\n\n# ...\n\nm = net.monitor(pop, 'r')\n\nnet.simulate(100.)\nr = m.get('r')\n\nplt.plot(dt()*np.arange(100), r[:, 15])\nIf recordings used the pause() and resume() methods, get() returns only one array with all recordings concatenated. You can access the steps at which the recording started or paused with the times() method:\nm = net.monitor(pop, 'r')\n\nnet.simulate(100.)\nm.pause()\nnet.simulate(1000.)\nm.resume()\nnet.simulate(100.)\n\nr = m.get('r') # A (200, N) Numpy array\nprint(m.times()) # {'start': [0, 1100], 'stop': [100, 1200]}\n\n\n\nAny variable defined in the neuron type can be recorded. An exception for spiking neurons is the spike variable itself, which is never explicitely defined in the neuron type but can be recorded:\nm = net.monitor(pop, ['v', 'spike'])\nUnlike other variables, the binary value of spike is not recorded at each time step, which would lead to very sparse matrices, but only the times (in steps, not milliseconds) at which spikes actually occur.\nAs each neuron fires differently (so each neuron will have recorded spikes of different lengths), get() in this case does not return a Numpy array, but a dictionary associating to each recorded neuron a list of spike times:\nm = net.monitor(pop, ['v', 'spike'])\n\nnet.simulate(100.0)\ndata = m.get('spike')\nprint(data[0]) # [23, 76, 98]\nIn the example above, the neuron of rank 0 has spiked 3 times (at t = 23, 76 and 98 ms if dt = 1.0) during the first 100 ms of the simulation.\nRaster plots\nIn order to easily display raster plots, the method raster_plot() is provided to transform this data into an easily plottable format:\nspike_times, ranks = m.raster_plot(data)\nplt.plot(spike_times, ranks, '.')\nraster_plot() returns two Numpy arrays, whose length is the total number of spikes emitted during the simulation. The first array contains the spike times (ín ms) while the second contains the ranks of the neurons who fired. They can be directly used t produce the raster plot with Matplotlib.\nAn example of the use of raster_plot() can be seen in the Izhikevich pulse network section.\nInter-spike interval (ISI) and coefficient of variation (ISI CV)\nIn addition to a raster plot, the distribution of inter-spike intervals could be considered for evaluation. The inter-spike interval (short ISI) is defined as the time in milliseconds between two consecutive spike events. The method inter_spike_interval() transforms the recorded spike events into a list of ISI across all neurons (default) or for indivdual neurons (add per_neuron = True to argument list) which could be fed into a histogram method provided by matplotlib:\npop_isi = m.inter_spike_interval(data)\nplt.hist(pop_isi)\nThe coefficient of variation is a measure often reported together with the inter-spike interval. These values can be easily obtained using another function of the Monitor object:\npop_isi_cv = m.coefficient_of_variation(data)\nplt.hist(pop_isi_cv)\nAn example of the use of inter_spike_interval() and coefficient_of_variation() can be seen in the COBA network section.\nMean firing rate\nThe mean firing rate in the population can be easily calculated using the length of the arrays returned by raster_plot:\nN = 1000 # number of neurons\nduration = 500. # duration of the simulation\n\ndata = m.get('spike')\nspike_times, ranks = m.raster_plot(data)\nprint('Mean firing rate:', len(spike_times)/float(N)/duration*1000., 'Hz.')\nFor convenience, this value is returned by the mean_fr() method, which has access to the number of recorded neurons and the duration of the recordings:\nprint('Mean firing rate:', m.mean_fr(data), 'Hz.')\nFiring rates\nAnother useful method is smoothed_rate(). It allows to display the instantaneous firing rate of each neuron based on the spike recordings:\nrates = m.smoothed_rate(data)\nplt.imshow(rates, aspect='auto')\nFor each neuron, it returns an array with the instantaneous firing rate during the whole simulation. The instantaneous firing rate is computed by inverting the inter-spike interval (ISI) between two consecutive spikes, and assigning it to all simulation steps between the two spikes.\nAs this value can be quite fluctuating, a smooth argument in milliseconds can be passed to smoothed_rate() to apply a low-pass filter on the firing rates:\nrates = m.smoothed_rate(data, smooth=200.0)\nplt.imshow(rates, aspect='auto')\nA smoothed firing rate for the whole population is also accessible through population_rate():\nfr = m.population_rate(data, smooth=200.0)\nHistogram\nhistogram() allows to count the spikes emitted in the whole population during successive bins of the recording duration:\nhisto = m.histogram(data, bins=1.0)\nplt.plot(histo)\nbins represents the size of each bin, here 1 ms. By default, the bin size is dt.\n\n\n\n\nRecording of synaptic variables such as weights w during learning is also possible using the monitor object. However, it can very easily lead to important memory consumption. Let’s suppose we have a network composed of two populations of 1000 neurons each, fully connected: each neuron of the second population receives 1000 synapses. This makes a total of 1 million synapses for the projection and, supposing the weights w use the double floating precision, requires 4 MB of memory. If you record w during a simulation of 1 second (1000 steps, with dt=1.0), the total added memory consumption would already be around 4GB.\nTo avoid fast memory fills, you should either 1) record the projection variables infrequently (by setting the period argument of the Monitor), or 2) selectively record particular dendrites. The corresponding dendrite should be simply passed to the monitor:\ndendrite = proj.dendrite(12) # or simply proj[12]\n\nm = net.monitor(dendrite, 'w')\n\nnet.simulate(1000.0)\n\ndata = m.get('w')\nThe Monitor object has the same start(), pause(), resume() and get() methods as for populations. get() returns also 2D Numpy arrays, the first index being time, the second being the index of the synapse. To know to which pre-synaptic neuron it corresponds, use the pre_ranks attribute of the dendrite:\ndendrite.pre_ranks # [0, 3, 12, ..]\nTo record a complete projection, simply pass it to the Monitor:\nm = net.monitor(proj, 'w', period=1000.)\n\nann.simulate(10000.0)\n\ndata = m.get('w')\nOne last time, do not record all weights of a projection at each time step!\n\n\n\n\n\n\nWarning\n\n\n\nRecording synaptic variables with CUDA is not available.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Monitors"
    ]
  },
  {
    "objectID": "manual/Recording.html#neural-variables",
    "href": "manual/Recording.html#neural-variables",
    "title": "Monitors",
    "section": "",
    "text": "The Monitor object takes four arguments:\n\nobj: the object to monitor. It can be a population, a population view (a slice of a population or an individual neuron), a dendrite (the synapses of a projection which reach a single post-synaptic neuron) or a projection.\nvariables: a (list of) variable name(s) which should be recorded. They should be variables of the neuron/synapse model of the corresponding object. Although it generally makes no sense, you can also record parameters of an object. By definition a parameter is constant throughout a simulation, but it maybe useful when tracking externally-set inputs, for example. You can know which attributes are recordable by checking the attributes attribute of the object (pop.attributes or proj.attributes).\nperiod: the period in ms at which recordings should be made. By default, recording is done after each simulation step (dt), but this may be overkill in long simulations.\nstart: boolean value stating if the recordings should start immediately after the creation of the monitor (default), or if it should be started later.\n\nSome examples:\nm = net.monitor(pop, 'r') # record r in all neurons of pop\nm = net.monitor(pop, ['r', 'v']) # record r and v of all neurons\nm = net.monitor(pop[:100], 'r', period=10.0) # record r in the first 100 neurons of pop, every 10 ms\nm = net.monitor(pop, 'r', start=False) # record r in all neurons, but do not start recording\nSpiking networks additionally allow to record the spike events in a population (see later). You also can record conductances (e.g. g_exc) and weighted sums of inputs in rate-coded networks (sum(exc)) the same way:\nm = net.nonitor(pop, ['spike', 'g_exc', 'g_inh'])\nm = net.monitor(pop, ['r', 'sum(exc)', 'sum(inh)'])\n\n\nIf start is set to False, recordings can be started later by calling the start() method:\nm = net.monitor(pop, 'r', start=False)\n\nnet.simulate(100.)\nm.start()\nnet.simulate(100.)\nIn this case, only the last 100 ms of the simulation will be recorded. Otherwise, recording would start immediately after the creation of the object.\n\n\n\nIf you are interested in recording only specific periods of the simulation, you can ause and resume recordings:\nm = net.monitor(pop, 'r')\n\nnet.simulate(100.)\nm.pause()\nnet.simulate(1000.)\nm.resume()\nnet.simulate(100.)\nIn this example, only the first and last 100 ms of the simulation are recorded.\n\n\n\nThe recorded values are obtained through the get() method. If no argument is passed, a dictionary is returned with one element per recorded variable. If the name of a variable is passed (for example get('r')), the recorded values for this variable are directly returned:\nm = net.monitor(pop, ['r', 'v'])\n\nann.simulate(100.)\n\ndata = m.get() # both 'r' and 'v'\n\nann.simulate(100.)\n\nr = m.get('r')\nv = m.get('v')\nIn the example above, data is a dictionary with two keys 'r' and 'v', while r and v are directly the recorded arrays.\nThe recorded values are Numpy arrays with two dimensions, the first one representing time, the second one representing the ranks of the recorded neurons.\nFor example, the time course of the firing rate of the neuron of rank 15 is accessed through:\ndata['r'][:, 15]\nThe firing rates of the whole population after 50 ms of simulation are accessed with:\ndata['r'][50, :]\n\n\n\n\n\n\nNote\n\n\n\nOnce you call get(), the internal data is erased, so calling it immediately afterwards will return an empty recording data. You need to simulate again in order to retrieve new values.\n\n\nRepresentation of time\nThe time indices are in simulation steps (integers), not in real time (ms). If dt is different from 1.0, this indices must be multiplied by dt() in order to plot real times:\nnet = ann.Network(dt=0.1)\n\n# ...\n\nm = net.monitor(pop, 'r')\n\nnet.simulate(100.)\nr = m.get('r')\n\nplt.plot(dt()*np.arange(100), r[:, 15])\nIf recordings used the pause() and resume() methods, get() returns only one array with all recordings concatenated. You can access the steps at which the recording started or paused with the times() method:\nm = net.monitor(pop, 'r')\n\nnet.simulate(100.)\nm.pause()\nnet.simulate(1000.)\nm.resume()\nnet.simulate(100.)\n\nr = m.get('r') # A (200, N) Numpy array\nprint(m.times()) # {'start': [0, 1100], 'stop': [100, 1200]}\n\n\n\nAny variable defined in the neuron type can be recorded. An exception for spiking neurons is the spike variable itself, which is never explicitely defined in the neuron type but can be recorded:\nm = net.monitor(pop, ['v', 'spike'])\nUnlike other variables, the binary value of spike is not recorded at each time step, which would lead to very sparse matrices, but only the times (in steps, not milliseconds) at which spikes actually occur.\nAs each neuron fires differently (so each neuron will have recorded spikes of different lengths), get() in this case does not return a Numpy array, but a dictionary associating to each recorded neuron a list of spike times:\nm = net.monitor(pop, ['v', 'spike'])\n\nnet.simulate(100.0)\ndata = m.get('spike')\nprint(data[0]) # [23, 76, 98]\nIn the example above, the neuron of rank 0 has spiked 3 times (at t = 23, 76 and 98 ms if dt = 1.0) during the first 100 ms of the simulation.\nRaster plots\nIn order to easily display raster plots, the method raster_plot() is provided to transform this data into an easily plottable format:\nspike_times, ranks = m.raster_plot(data)\nplt.plot(spike_times, ranks, '.')\nraster_plot() returns two Numpy arrays, whose length is the total number of spikes emitted during the simulation. The first array contains the spike times (ín ms) while the second contains the ranks of the neurons who fired. They can be directly used t produce the raster plot with Matplotlib.\nAn example of the use of raster_plot() can be seen in the Izhikevich pulse network section.\nInter-spike interval (ISI) and coefficient of variation (ISI CV)\nIn addition to a raster plot, the distribution of inter-spike intervals could be considered for evaluation. The inter-spike interval (short ISI) is defined as the time in milliseconds between two consecutive spike events. The method inter_spike_interval() transforms the recorded spike events into a list of ISI across all neurons (default) or for indivdual neurons (add per_neuron = True to argument list) which could be fed into a histogram method provided by matplotlib:\npop_isi = m.inter_spike_interval(data)\nplt.hist(pop_isi)\nThe coefficient of variation is a measure often reported together with the inter-spike interval. These values can be easily obtained using another function of the Monitor object:\npop_isi_cv = m.coefficient_of_variation(data)\nplt.hist(pop_isi_cv)\nAn example of the use of inter_spike_interval() and coefficient_of_variation() can be seen in the COBA network section.\nMean firing rate\nThe mean firing rate in the population can be easily calculated using the length of the arrays returned by raster_plot:\nN = 1000 # number of neurons\nduration = 500. # duration of the simulation\n\ndata = m.get('spike')\nspike_times, ranks = m.raster_plot(data)\nprint('Mean firing rate:', len(spike_times)/float(N)/duration*1000., 'Hz.')\nFor convenience, this value is returned by the mean_fr() method, which has access to the number of recorded neurons and the duration of the recordings:\nprint('Mean firing rate:', m.mean_fr(data), 'Hz.')\nFiring rates\nAnother useful method is smoothed_rate(). It allows to display the instantaneous firing rate of each neuron based on the spike recordings:\nrates = m.smoothed_rate(data)\nplt.imshow(rates, aspect='auto')\nFor each neuron, it returns an array with the instantaneous firing rate during the whole simulation. The instantaneous firing rate is computed by inverting the inter-spike interval (ISI) between two consecutive spikes, and assigning it to all simulation steps between the two spikes.\nAs this value can be quite fluctuating, a smooth argument in milliseconds can be passed to smoothed_rate() to apply a low-pass filter on the firing rates:\nrates = m.smoothed_rate(data, smooth=200.0)\nplt.imshow(rates, aspect='auto')\nA smoothed firing rate for the whole population is also accessible through population_rate():\nfr = m.population_rate(data, smooth=200.0)\nHistogram\nhistogram() allows to count the spikes emitted in the whole population during successive bins of the recording duration:\nhisto = m.histogram(data, bins=1.0)\nplt.plot(histo)\nbins represents the size of each bin, here 1 ms. By default, the bin size is dt.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Monitors"
    ]
  },
  {
    "objectID": "manual/Recording.html#synaptic-variables",
    "href": "manual/Recording.html#synaptic-variables",
    "title": "Monitors",
    "section": "",
    "text": "Recording of synaptic variables such as weights w during learning is also possible using the monitor object. However, it can very easily lead to important memory consumption. Let’s suppose we have a network composed of two populations of 1000 neurons each, fully connected: each neuron of the second population receives 1000 synapses. This makes a total of 1 million synapses for the projection and, supposing the weights w use the double floating precision, requires 4 MB of memory. If you record w during a simulation of 1 second (1000 steps, with dt=1.0), the total added memory consumption would already be around 4GB.\nTo avoid fast memory fills, you should either 1) record the projection variables infrequently (by setting the period argument of the Monitor), or 2) selectively record particular dendrites. The corresponding dendrite should be simply passed to the monitor:\ndendrite = proj.dendrite(12) # or simply proj[12]\n\nm = net.monitor(dendrite, 'w')\n\nnet.simulate(1000.0)\n\ndata = m.get('w')\nThe Monitor object has the same start(), pause(), resume() and get() methods as for populations. get() returns also 2D Numpy arrays, the first index being time, the second being the index of the synapse. To know to which pre-synaptic neuron it corresponds, use the pre_ranks attribute of the dendrite:\ndendrite.pre_ranks # [0, 3, 12, ..]\nTo record a complete projection, simply pass it to the Monitor:\nm = net.monitor(proj, 'w', period=1000.)\n\nann.simulate(10000.0)\n\ndata = m.get('w')\nOne last time, do not record all weights of a projection at each time step!\n\n\n\n\n\n\nWarning\n\n\n\nRecording synaptic variables with CUDA is not available.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Monitors"
    ]
  },
  {
    "objectID": "manual/SpikeSynapse.html",
    "href": "manual/SpikeSynapse.html",
    "title": "Spiking synapses",
    "section": "",
    "text": "Synapses in spiking networks differ from rate-coded synapses in that they are event-driven, i.e. the most important changes occur whenever a pre- or post-synaptic spike is emitted. For this reason, additional arguments have to be passed to the Synapse object.\n\n\nIn the simplest case, a pre-synaptic spike increases a target conductance value in the post-synaptic neuron. The rule defining how this conductance is modified has to be placed in the pre_spike argument of a Synapse object.\nThe default spiking synapse in ANNarchy is equivalent to:\ndefault_synapse = ann.Synapse(\n    pre_spike = \"\"\"\n        g_target += w\n    \"\"\"     \n) \nThe only thing it does is to increase the conductance g_target of the post-synaptic neuron (for example g_exc if the target is exc) every time a pre-synaptic spike arrives at the synapse, proportionally to the synaptic efficiency w of the synapse. Note that w is implicitely defined in all synapses, you do not need to define it explicitely.\nThe pre_spike argument can be a string, a multi-line string, a list of strings or a list of ann.Variable instances. It can only contain assignments (=, +=, etc.), no ODE.\nFor example, you may want to implement a \"fatigue\" mechanism for the synapse, transciently reducing the synaptic efficiency when the pre-synaptic neuron fires too strongly. One solution would be to decrease a synaptic variable everytime a pre-synaptic spike is received and increase the post-synaptic conductance proportionally to this value. When no spike is received, this trace variable should slowly return to its maximal value.\nFatigueSynapse = ann.Synapse(\n    parameters = dict(\n        tau = 1000,  # Time constant of the trace is 1 second\n        dec = 0.05,  # Decrement of the trace\n    ),\n    equations = [\n        ann.Variable('tau * dtrace/dt + trace = 1.0', min = 0.0),\n    ],\n    pre_spike = \"\"\"\n        g_target += w * trace\n        trace -= dec\n    \"\"\"     \n) \nEach time a pre-synaptic spike occurs, the post-synaptic conductance is increased from w*trace. As the baseline of trace is 1.0 (as defined in equations), this means that a \"fresh\" synapse will use the full synaptic efficiency. However, after each pre-synaptic spike, trace is decreased from dec = 0.05, meaning that the \"real\" synaptic efficiency can go down to 0.0 (the minimal value of trace) if the pre-synaptic neuron fires too often.\nIt is important here to restrict trace to positive values with the flag min=0.0, as it could otherwise transform an excitatory synapse into an inhibitory one.\n\nIt is obligatory to use the keyword g_target for the post-synaptic conductance. This value relates to the corresponding value in post-synaptic neuron: The target will be replaced with the projection’s target (for example exc or inh). So if you use this synapse in a projection with target = 'exc', the value of g_exc in post-synaptic neuron will be automatically replaced.\n\n\n\n\nIn spiking networks, there are usually two methods to implement event-driven synaptic plasticity (see the entry on STDP at Scholarpedia):\n\nby using the difference in spike times between the pre- and post-synaptic neurons;\nby using online implementations.\n\n\n\nA Synapse has access to two specific variables:\n\nt_pre corresponding to the time of the last pre-synaptic spike in milliseconds.\nt_post corresponding to the time of the last post-synaptic spike in milliseconds.\n\nThese times are relative to the creation of the network, so they only make sense when compared to each other or to t.\nSpike-timing dependent plasticity can for example be implemented the following way:\nSTDP = ann.Synapse(\n    parameters = dict(\n        tau_pre = 10.0,\n        tau_post = 10.0,\n        cApre = 0.01,\n        cApost = 0.0105,\n        wmax = 0.01,\n    ),\n    pre_spike = \"\"\"\n        g_target += w\n        w = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \n    \"\"\",                  \n    post_spike = \"\"\"\n        w = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax)\n    \"\"\"      \n) \npre_spike\nEvery time a pre-synaptic spike arrives at the synapse (pre_spike), the post-synaptic conductance is increased from the current value of the synaptic efficiency.\ng_target += w\nWhen a synapse object is defined, this behavior should be explicitely declared.\nThe value w is then decreased using a decreasing exponential function of the time elapsed since the last post-synaptic spike:\nw = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \nThe clip() global function is there to ensure that w is bounded between 0.0 and wmax. As t &gt;= t_post, the exponential part is smaller than 1.0. The pre_spike argument therefore ensures that the synapse is depressed is a pre-synaptic spike occurs shortly after a post-synaptic one. \"Shortly\" is quantified by the time constant tau_post, usually in the range of 10 ms.\npost_spike\nEvery time a post-synaptic spike is emitted (post_spike), the value w is increased proportionally to the time elapsed since the last pre-synaptic spike:\nw = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax)\nThis term defines the potentiation of a synapse when a pre-synaptic spike is followed immediately by a post-synaptic one: the inferred causality between the two events should be reinforced.\n\n\n\n\n\n\nWarning\n\n\n\nOnly the last pre- and post-synaptic spikes are accessible, not the whole history. Only nearest-neighbor spike-interactions are possible using ANNarchy, not temporal all-to-all interactions where the whole spike history is used for learning (see the entry on STDP at Scholarpedia).\nSome networks may not work properly when using this simulation mode. For example, whenever the pre-synaptic neurons fires twice in a very short interval and causes a post-synaptic spike, the corresponding weight should be reinforced twice. With the proposed STDP rule, it would be reinforced only once.\nIt is therefore generally advised to use online versions of STDP.\n\n\n\n\n\nThe online version of STDP requires two synaptic traces, which are increased whenever a pre- resp. post-synaptic spike is perceived, and decay with their own dynamics in between. Using the same vocabulary as Brian, such an implementation would be:\nSTDP_online = ann.Synapse(\n    parameters = dict(\n        tau_pre = 10.0,\n        tau_post = 10.0,\n        cApre = 0.01,\n        cApost = 0.0105,\n        wmax = 0.01,\n    ),\n    equations = [\n        ann.Variable('tau_pre * dApre/dt = - Apre', method='event-driven'),\n        ann.Variable('tau_post * dApost/dt = - Apost', method='event-driven'),\n    ],\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre * wmax\n        w = clip(w - Apost, 0.0 , wmax)\n    \"\"\",                  \n    post_spike = \"\"\"\n        Apost += cApost * wmax\n        w = clip(w + Apre, 0.0 , wmax)\n    \"\"\"      \n) \nThe variables Apre and Apost are exponentially decreasing traces of pre- and post-synaptic spikes, as shown by the leaky integration in equations. When a pre-synaptic spike is emitted, Apre is incremented, the conductance level of the post-synaptic neuron g_target too, and the synaptic efficiency is decreased proportionally to Apost (this means that if a post-synaptic spike was emitted shortly before, LTD will strongly be applied, while if it was longer ago, no major change will be observed). When a post-synaptic spike is observed, Apost increases and the synaptic efficiency is increased proportionally to Apre.\nThe effect of this online version is globally the same as the spike timing dependent version, except that the history of pre- and post-synaptic spikes is fully contained in the variables Apre and Apost.\nThe event-driven keyword allows event-driven integration of the variables Apre and Apost. This means the equations are not updated at each time step, but only when a pre- or post-synaptic spike occurs at the synapse. This is only possible because the two variables follow linear first-order ODEs. The event-driven integration method allows to spare a lot of computations if the number of spikes is not too high in the network.\n\n\n\nThree types of updates are potentially executed at every time step:\n\nPre-synaptic events, defined by pre_spike and triggered after each pre-synaptic spike, after a delay of at least dt.\nSynaptic variables defined by equations.\nPost-synaptic events, defined by post_spike and triggered after each post-synaptic spike, without delay.\n\nThese updates are conducted in that order at each time step. First, all spikes emitted in the previous step (or earlier if there are delays) are propagated to the corresponding synapses and influence variables there (especially conductance increases), then all synaptic variables are updated according to their ODE (after the neurons' equations are updated), then all neurons which have emitted a spike in the current step modify their synapses.\nA potential problem arises when a pre-synaptic and a post-synaptic spike are emitted at the same time. STDP-like plasticity rules are usually not defined when the spike time difference is 0, as the two spikes can not be correlated in that case (the pre-spike can not possibly be the cause of the post-spike).\nBy default, both event-driven updates (pre_spike leading to LTD, post_spike leading to LTP) will be conducted when the spikes are emitted at the same time. This can be problematic for some plastic models, for example the simple_stdp example provided in the source code.\nTo avoid this problem, the flag unless_post can be specified in pre_spike to indicate that the corresponding variable should be updated after each pre-synaptic spike, unless the post-synaptic neuron also fired at the previous time step. Without even-driven integration, the online STDP learning rule would become:\nSTDP_online = ann.Synapse(\n    parameters = dict(\n        tau_pre = 10.0,\n        tau_post = 10.0,\n        cApre = 0.01,\n        cApost = 0.0105,\n        wmax = 0.01,\n    ),\n    equations = [\n        ann.Variable('tau_pre * dApre/dt = - Apre'),\n        ann.Variable('tau_post * dApost/dt = - Apost'),\n    ],\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre : unless_post\n        w = clip(w - Apost, 0.0 , wmax) : unless_post\n    \"\"\",                  \n    post_spike = \"\"\"\n        Apost += cApost\n        w = clip(w + Apre, 0.0 , wmax)\n    \"\"\"      \n) \n\n\n\n\nIn some cases, synaptic transmission cannot be described in an event-driven framework. Synapses using the NMDA neurotransmitter are for example often modeled as non-linear synapses. Non-linear synapses can require the post-synaptic conductance to be a sum of synapse-specific variables, as for rate-coded neurons, and not simply incremented when a pre-synaptic spike occurs. NMDA synapses can be represented by two variables x(t) and g(t) following first-order ODEs:\n\\begin{aligned}\n\\begin{aligned}\n\\tau \\cdot \\frac{dx(t)}{dt} &= - x(t) \\\\\n\\tau \\cdot \\frac{dg(t)}{dt} &= - g(t) +  x(t) \\cdot (1 - g(t))\n\\end{aligned}\n\\end{aligned}\nWhen a pre-synaptic spike occurs, x(t) is incremented by the weight w(t). However, it does not influence directly the post-synaptic neuron, as the output of a synapse is the signal g(t). The post-synaptic conductance is defined at each time t as the sum over all synapses of the same type of their variable g(t):\ng_\\text{exc}(t) = \\sum_{i=1}^{N_\\text{exc}} g_i (t)\nSuch a synapse could be implemented the following way:\nNMDA = ann.Synapse(\n    parameters = dict(\n        tau = 10.0\n    ),\n    equations = [\n        'tau * dx/dt = -x',\n        'tau * dg/dt = -g +  x * (1 -g)'\n    ], \n    pre_spike = \"x += w\",\n    psp = \"g\"\n)\nThe synapse defines a psp argument which means that the output of this synapse is non-linear and the post-synaptic conductance should be summed over this value (g in this case). It is not possible to use the event-driven integration scheme for such non-linear synapses.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Spiking synapses"
    ]
  },
  {
    "objectID": "manual/SpikeSynapse.html#increase-of-conductance-after-a-pre-synaptic-spike",
    "href": "manual/SpikeSynapse.html#increase-of-conductance-after-a-pre-synaptic-spike",
    "title": "Spiking synapses",
    "section": "",
    "text": "In the simplest case, a pre-synaptic spike increases a target conductance value in the post-synaptic neuron. The rule defining how this conductance is modified has to be placed in the pre_spike argument of a Synapse object.\nThe default spiking synapse in ANNarchy is equivalent to:\ndefault_synapse = ann.Synapse(\n    pre_spike = \"\"\"\n        g_target += w\n    \"\"\"     \n) \nThe only thing it does is to increase the conductance g_target of the post-synaptic neuron (for example g_exc if the target is exc) every time a pre-synaptic spike arrives at the synapse, proportionally to the synaptic efficiency w of the synapse. Note that w is implicitely defined in all synapses, you do not need to define it explicitely.\nThe pre_spike argument can be a string, a multi-line string, a list of strings or a list of ann.Variable instances. It can only contain assignments (=, +=, etc.), no ODE.\nFor example, you may want to implement a \"fatigue\" mechanism for the synapse, transciently reducing the synaptic efficiency when the pre-synaptic neuron fires too strongly. One solution would be to decrease a synaptic variable everytime a pre-synaptic spike is received and increase the post-synaptic conductance proportionally to this value. When no spike is received, this trace variable should slowly return to its maximal value.\nFatigueSynapse = ann.Synapse(\n    parameters = dict(\n        tau = 1000,  # Time constant of the trace is 1 second\n        dec = 0.05,  # Decrement of the trace\n    ),\n    equations = [\n        ann.Variable('tau * dtrace/dt + trace = 1.0', min = 0.0),\n    ],\n    pre_spike = \"\"\"\n        g_target += w * trace\n        trace -= dec\n    \"\"\"     \n) \nEach time a pre-synaptic spike occurs, the post-synaptic conductance is increased from w*trace. As the baseline of trace is 1.0 (as defined in equations), this means that a \"fresh\" synapse will use the full synaptic efficiency. However, after each pre-synaptic spike, trace is decreased from dec = 0.05, meaning that the \"real\" synaptic efficiency can go down to 0.0 (the minimal value of trace) if the pre-synaptic neuron fires too often.\nIt is important here to restrict trace to positive values with the flag min=0.0, as it could otherwise transform an excitatory synapse into an inhibitory one.\n\nIt is obligatory to use the keyword g_target for the post-synaptic conductance. This value relates to the corresponding value in post-synaptic neuron: The target will be replaced with the projection’s target (for example exc or inh). So if you use this synapse in a projection with target = 'exc', the value of g_exc in post-synaptic neuron will be automatically replaced.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Spiking synapses"
    ]
  },
  {
    "objectID": "manual/SpikeSynapse.html#synaptic-plasticity",
    "href": "manual/SpikeSynapse.html#synaptic-plasticity",
    "title": "Spiking synapses",
    "section": "",
    "text": "In spiking networks, there are usually two methods to implement event-driven synaptic plasticity (see the entry on STDP at Scholarpedia):\n\nby using the difference in spike times between the pre- and post-synaptic neurons;\nby using online implementations.\n\n\n\nA Synapse has access to two specific variables:\n\nt_pre corresponding to the time of the last pre-synaptic spike in milliseconds.\nt_post corresponding to the time of the last post-synaptic spike in milliseconds.\n\nThese times are relative to the creation of the network, so they only make sense when compared to each other or to t.\nSpike-timing dependent plasticity can for example be implemented the following way:\nSTDP = ann.Synapse(\n    parameters = dict(\n        tau_pre = 10.0,\n        tau_post = 10.0,\n        cApre = 0.01,\n        cApost = 0.0105,\n        wmax = 0.01,\n    ),\n    pre_spike = \"\"\"\n        g_target += w\n        w = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \n    \"\"\",                  \n    post_spike = \"\"\"\n        w = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax)\n    \"\"\"      \n) \npre_spike\nEvery time a pre-synaptic spike arrives at the synapse (pre_spike), the post-synaptic conductance is increased from the current value of the synaptic efficiency.\ng_target += w\nWhen a synapse object is defined, this behavior should be explicitely declared.\nThe value w is then decreased using a decreasing exponential function of the time elapsed since the last post-synaptic spike:\nw = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \nThe clip() global function is there to ensure that w is bounded between 0.0 and wmax. As t &gt;= t_post, the exponential part is smaller than 1.0. The pre_spike argument therefore ensures that the synapse is depressed is a pre-synaptic spike occurs shortly after a post-synaptic one. \"Shortly\" is quantified by the time constant tau_post, usually in the range of 10 ms.\npost_spike\nEvery time a post-synaptic spike is emitted (post_spike), the value w is increased proportionally to the time elapsed since the last pre-synaptic spike:\nw = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax)\nThis term defines the potentiation of a synapse when a pre-synaptic spike is followed immediately by a post-synaptic one: the inferred causality between the two events should be reinforced.\n\n\n\n\n\n\nWarning\n\n\n\nOnly the last pre- and post-synaptic spikes are accessible, not the whole history. Only nearest-neighbor spike-interactions are possible using ANNarchy, not temporal all-to-all interactions where the whole spike history is used for learning (see the entry on STDP at Scholarpedia).\nSome networks may not work properly when using this simulation mode. For example, whenever the pre-synaptic neurons fires twice in a very short interval and causes a post-synaptic spike, the corresponding weight should be reinforced twice. With the proposed STDP rule, it would be reinforced only once.\nIt is therefore generally advised to use online versions of STDP.\n\n\n\n\n\nThe online version of STDP requires two synaptic traces, which are increased whenever a pre- resp. post-synaptic spike is perceived, and decay with their own dynamics in between. Using the same vocabulary as Brian, such an implementation would be:\nSTDP_online = ann.Synapse(\n    parameters = dict(\n        tau_pre = 10.0,\n        tau_post = 10.0,\n        cApre = 0.01,\n        cApost = 0.0105,\n        wmax = 0.01,\n    ),\n    equations = [\n        ann.Variable('tau_pre * dApre/dt = - Apre', method='event-driven'),\n        ann.Variable('tau_post * dApost/dt = - Apost', method='event-driven'),\n    ],\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre * wmax\n        w = clip(w - Apost, 0.0 , wmax)\n    \"\"\",                  \n    post_spike = \"\"\"\n        Apost += cApost * wmax\n        w = clip(w + Apre, 0.0 , wmax)\n    \"\"\"      \n) \nThe variables Apre and Apost are exponentially decreasing traces of pre- and post-synaptic spikes, as shown by the leaky integration in equations. When a pre-synaptic spike is emitted, Apre is incremented, the conductance level of the post-synaptic neuron g_target too, and the synaptic efficiency is decreased proportionally to Apost (this means that if a post-synaptic spike was emitted shortly before, LTD will strongly be applied, while if it was longer ago, no major change will be observed). When a post-synaptic spike is observed, Apost increases and the synaptic efficiency is increased proportionally to Apre.\nThe effect of this online version is globally the same as the spike timing dependent version, except that the history of pre- and post-synaptic spikes is fully contained in the variables Apre and Apost.\nThe event-driven keyword allows event-driven integration of the variables Apre and Apost. This means the equations are not updated at each time step, but only when a pre- or post-synaptic spike occurs at the synapse. This is only possible because the two variables follow linear first-order ODEs. The event-driven integration method allows to spare a lot of computations if the number of spikes is not too high in the network.\n\n\n\nThree types of updates are potentially executed at every time step:\n\nPre-synaptic events, defined by pre_spike and triggered after each pre-synaptic spike, after a delay of at least dt.\nSynaptic variables defined by equations.\nPost-synaptic events, defined by post_spike and triggered after each post-synaptic spike, without delay.\n\nThese updates are conducted in that order at each time step. First, all spikes emitted in the previous step (or earlier if there are delays) are propagated to the corresponding synapses and influence variables there (especially conductance increases), then all synaptic variables are updated according to their ODE (after the neurons' equations are updated), then all neurons which have emitted a spike in the current step modify their synapses.\nA potential problem arises when a pre-synaptic and a post-synaptic spike are emitted at the same time. STDP-like plasticity rules are usually not defined when the spike time difference is 0, as the two spikes can not be correlated in that case (the pre-spike can not possibly be the cause of the post-spike).\nBy default, both event-driven updates (pre_spike leading to LTD, post_spike leading to LTP) will be conducted when the spikes are emitted at the same time. This can be problematic for some plastic models, for example the simple_stdp example provided in the source code.\nTo avoid this problem, the flag unless_post can be specified in pre_spike to indicate that the corresponding variable should be updated after each pre-synaptic spike, unless the post-synaptic neuron also fired at the previous time step. Without even-driven integration, the online STDP learning rule would become:\nSTDP_online = ann.Synapse(\n    parameters = dict(\n        tau_pre = 10.0,\n        tau_post = 10.0,\n        cApre = 0.01,\n        cApost = 0.0105,\n        wmax = 0.01,\n    ),\n    equations = [\n        ann.Variable('tau_pre * dApre/dt = - Apre'),\n        ann.Variable('tau_post * dApost/dt = - Apost'),\n    ],\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre : unless_post\n        w = clip(w - Apost, 0.0 , wmax) : unless_post\n    \"\"\",                  \n    post_spike = \"\"\"\n        Apost += cApost\n        w = clip(w + Apre, 0.0 , wmax)\n    \"\"\"      \n)",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Spiking synapses"
    ]
  },
  {
    "objectID": "manual/SpikeSynapse.html#continuous-synaptic-transmission",
    "href": "manual/SpikeSynapse.html#continuous-synaptic-transmission",
    "title": "Spiking synapses",
    "section": "",
    "text": "In some cases, synaptic transmission cannot be described in an event-driven framework. Synapses using the NMDA neurotransmitter are for example often modeled as non-linear synapses. Non-linear synapses can require the post-synaptic conductance to be a sum of synapse-specific variables, as for rate-coded neurons, and not simply incremented when a pre-synaptic spike occurs. NMDA synapses can be represented by two variables x(t) and g(t) following first-order ODEs:\n\\begin{aligned}\n\\begin{aligned}\n\\tau \\cdot \\frac{dx(t)}{dt} &= - x(t) \\\\\n\\tau \\cdot \\frac{dg(t)}{dt} &= - g(t) +  x(t) \\cdot (1 - g(t))\n\\end{aligned}\n\\end{aligned}\nWhen a pre-synaptic spike occurs, x(t) is incremented by the weight w(t). However, it does not influence directly the post-synaptic neuron, as the output of a synapse is the signal g(t). The post-synaptic conductance is defined at each time t as the sum over all synapses of the same type of their variable g(t):\ng_\\text{exc}(t) = \\sum_{i=1}^{N_\\text{exc}} g_i (t)\nSuch a synapse could be implemented the following way:\nNMDA = ann.Synapse(\n    parameters = dict(\n        tau = 10.0\n    ),\n    equations = [\n        'tau * dx/dt = -x',\n        'tau * dg/dt = -g +  x * (1 -g)'\n    ], \n    pre_spike = \"x += w\",\n    psp = \"g\"\n)\nThe synapse defines a psp argument which means that the output of this synapse is non-linear and the post-synaptic conductance should be summed over this value (g in this case). It is not possible to use the event-driven integration scheme for such non-linear synapses.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Spiking synapses"
    ]
  },
  {
    "objectID": "manual/Random.html",
    "href": "manual/Random.html",
    "title": "Random distributions",
    "section": "",
    "text": "ANNarchy allows to sample values from various probability distributions. The list is also available in the reference section: RandomDistribution",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Random distributions"
    ]
  },
  {
    "objectID": "manual/Random.html#inside-equations",
    "href": "manual/Random.html#inside-equations",
    "title": "Random distributions",
    "section": "Inside equations",
    "text": "Inside equations\nThe probability distributions can be used inside neural or synaptic equations to add noise. The arguments to the random distributions can be either fixed values or global parameters.\nneuron = ann.Neuron(\n    parameters = dict(\n        noise_min = -0.1,\n        noise_max = 0.1,\n    ),\n    equations = [\n        ann.Variable('noise += Uniform(noise_min, noise_max)'),\n    ]\n)\nIt is not allowed to use local parameters (with different values per neuron) or variables, as the random number generators are initialized only once at network creation (doing otherwise would impair performance too much).\n\n\n\n\n\n\nCaution\n\n\n\nIf a global parameter is used, changing its value will not affect the generator after compilation (net.compile()).\nIt is therefore better practice to use normalized random generators and scale their outputs:\nneuron = ann.Neuron(\n    parameters = dict(\n        noise_min = -0.1,\n        noise_max = 0.1,\n    ),\n    equations = [\n        ann.Variable('noise += noise_min + (noise_max - noise_min) * Uniform(0, 1)'),\n    ]\n)",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Random distributions"
    ]
  },
  {
    "objectID": "manual/Random.html#python-classes",
    "href": "manual/Random.html#python-classes",
    "title": "Random distributions",
    "section": "Python classes",
    "text": "Python classes\nANNarchy also exposes these distributions as Python classes deriving from ann.RandomDistribution:\n\nrd = ann.Uniform(min=-1.0, max=1.0)\n\nvalues = rd.get_values(shape=(5, 5))\nprint(values)\n\n[[ 0.52496801  0.8304131   0.18981183  0.90584312  0.45013294]\n [-0.8496625   0.13878709 -0.71916338  0.65327206 -0.74278506]\n [-0.41181912  0.18992222 -0.71336558 -0.31109334  0.56593865]\n [ 0.95857898 -0.92678771  0.04872276 -0.80440061  0.91366882]\n [-0.43897458  0.53091326  0.5378137   0.64775663 -0.49223366]]\n\n\nThose classes are only thin wrappers around numpy’s random module. The code above is fully equivalent to:\n\nrng = np.random.default_rng()\n\nvalues = rng.uniform(-1.0, 1.0, (5,5))\nprint(values)\n\n[[ 0.18875554 -0.04882092 -0.45888849 -0.75761291  0.10636076]\n [-0.84540858  0.28957174 -0.42878277  0.6277162   0.08221608]\n [-0.86781869  0.95202885 -0.56962942 -0.95892803  0.03947233]\n [ 0.92376797 -0.89519452 -0.94021693 -0.77945232 -0.7258296 ]\n [-0.80374599  0.67538198  0.22838406  0.13981939  0.81150928]]\n\n\nThe main interest of using these objects instead of directly numpy is that the get_values() method can only be called when the network is instantiated (at the end of net.compile()).\nFor example, in the following code, the random values are only created after compilation and transferred to the C++ kernel. They do not comsume RAM unnecessarily on the Python side.\nnet = ann.Network()\npop = net.create(100000000, ann.Neuron(\"r = 0.0\"))\npop.r = ann.Uniform(min=0.0, max=1.0) \n# The values are not drawn yet\nnet.compile()\n# The values are now drawn and stored in the C++ kernel.\nIf you want to control the seed of the random distributions, you should pass the numpy default RNG to it, initialized with the network’s seed:\nnet = ann.Network(seed=42) # or leave the seed to None to have it automatically set\n\nrng = np.random.default_rng(seed=net.seed)\n\nrd = ann.Uniform(min=0.0, max=1.0, rng=rng)\nThis is especially important when running simulations in parallel with parallel_run().",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Random distributions"
    ]
  },
  {
    "objectID": "manual/Projections.html",
    "href": "manual/Projections.html",
    "title": "Projections",
    "section": "",
    "text": "Let’s suppose the BCM synapse should used to create a plastic projection between two populations :\nBCM = ann.Synapse(\n    parameters = dict(\n        eta = 0.01,\n        tau = 100.,\n    ),\n    equations = [\n        ann.Variable('tau * dtheta/dt + theta = post.r^2', locality='semiglobal'),\n        ann.Variable('dw/dt = eta * post.r * (post.r - theta) * pre.r', min=0.0),\n    ]\n)\nand two populations have been created:\nnet = ann.Network()\npop1 = net.create(10, ann.Neuron(equations=\"r=sum(exc)\"))\npop2 = net.create(10, ann.Neuron(equations=\"r=sum(exc)\"))",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#creating-the-projections",
    "href": "manual/Projections.html#creating-the-projections",
    "title": "Projections",
    "section": "Creating the projections",
    "text": "Creating the projections\nOnce the populations are created, one can connect them by creating Projection instances through the Network.connect() method:\n\nproj = net.connect(\n    pre = pop1,\n    post = pop2,\n    target = \"exc\",\n    synapse = BCM\n)\n\n\npre is either the name of the pre-synaptic population or the corresponding Population object.\npost is either the name of the post-synaptic population or the corresponding Population object.\ntarget is the type of the connection.\nsynapse is an optional argument requiring a Synapse instance.\n\nThe post-synaptic neuron type must use sum(exc) in the rate-coded case, respectively g_exc in the spiking case, otherwise the projection will be useless.\nIf the synapse argument is omitted, the default synapse will be used:\n\nthe default rate-coded synapse defines psp = w * pre.r,\nthe default spiking synapse defines g_target += w.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#instantiating-the-projections",
    "href": "manual/Projections.html#instantiating-the-projections",
    "title": "Projections",
    "section": "Instantiating the projections",
    "text": "Instantiating the projections\nCreating the Projection objects only defines the information that two populations are connected. The synapses must be explicitely created by applying a connector method on the Projection object.\nTo this end, ANNarchy already provides a set of predefined connector methods, but the user has also the possibility to define his own (see the following page Connectivity).\nThe connection pattern should be applied right after the creation of the Projection:\n\nproj.fixed_probability(probability=0.5, weights = 1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x7fb20c61b430&gt;\n\n\nThe connector method must be called before the network is compiled.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#projection-attributes",
    "href": "manual/Projections.html#projection-attributes",
    "title": "Projections",
    "section": "Projection attributes",
    "text": "Projection attributes\n\nGlobal attributes\nThe global parameters and variables of a projection (parameters by default, and variables defined with locality='global') can be accessed directly through attributes:\n\nproj.tau\n\n100.0\n\n\nSemi-global attributes have one value per post-synaptic neuron, so the result is a list:\n\nproj.theta\n\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n\n\nPost-synaptic variables can be modified by passing:\n\na single value, which will be the same for all post-synaptic neurons.\na list of values, with the same size as the number of neurons receiving synapses.\na list of lists, matching exactly the number of synapses in the projection.\n\nAfter compilation (and therefore creation of the synapses), you can access how many post-synaptic neurons receive actual synapses with:\n\nproj.size\n\n10\n\n\nThe list of ranks of the post-synaptic neurons receiving synapses is obtained with:\n\nproj.post_ranks\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n\nLocal attributes\nBeware: As projections are only instantiated after the call to compile(), local attributes of a Projection are only available then. Trying to access them before compilation will lead to an error!\nAt the projection level\nLocal attributes can also be accessed globally through attributes. It will return a list of lists containing the synapse-specific values.\n\nproj.w\n\n[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n [1.0, 1.0, 1.0, 1.0, 1.0],\n [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n [1.0, 1.0, 1.0, 1.0],\n [1.0, 1.0, 1.0, 1.0],\n [1.0, 1.0, 1.0, 1.0],\n [1.0, 1.0, 1.0, 1.0, 1.0],\n [1.0, 1.0, 1.0, 1.0, 1.0],\n [1.0, 1.0, 1.0, 1.0]]\n\n\nThe first index represents the post-synaptic neurons. It has the same length as proj.post_ranks. Beware that if some post-synaptic neurons do not receive any connection, this index will not correspond to the ranks of the post-synaptic population.\nThe second index represents the pre-synaptic neurons. The list of pre-synaptic ranks for each post-synaptic neuron is obtained with:\n\nproj.pre_ranks\n\n[[0, 1, 2, 3, 4, 7, 8, 9],\n [2, 3, 5, 6, 7, 8],\n [0, 1, 2, 4, 6],\n [1, 4, 5, 6, 7, 9],\n [2, 5, 8, 9],\n [3, 6, 8, 9],\n [1, 5, 6, 8],\n [0, 1, 3, 4, 6],\n [1, 3, 4, 7, 9],\n [0, 4, 5, 7]]\n\n\nproj.w and proj.pre_ranks have the same number of elements. In the general case, the connectivity matrix is not dense (different number of incoming synapses for each post-synaptic neuron), so projection attributes cannot be casted to 2D numpy arrays.\nThe connectivity matrix with weights w can be visualized as a 2D array using Projection.connectivity_matrix(), replacing non-existing synapse with 0.0 (or any other value of your choice).\n\nproj.connectivity_matrix(fill=0.0)\n\narray([[1., 1., 1., 1., 1., 0., 0., 1., 1., 1.],\n       [0., 0., 1., 1., 0., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 0., 1., 0., 1., 0., 0., 0.],\n       [0., 1., 0., 0., 1., 1., 1., 1., 0., 1.],\n       [0., 0., 1., 0., 0., 1., 0., 0., 1., 1.],\n       [0., 0., 0., 1., 0., 0., 1., 0., 1., 1.],\n       [0., 1., 0., 0., 0., 1., 1., 0., 1., 0.],\n       [1., 1., 0., 1., 1., 0., 1., 0., 0., 0.],\n       [0., 1., 0., 1., 1., 0., 0., 1., 0., 1.],\n       [1., 0., 0., 0., 1., 1., 0., 1., 0., 0.]])\n\n\n\n\n\n\n\n\nWarning\n\n\n\nModifying these lists of lists will not change the underlying connectivity, they are read-only. Only use them for analysis.\n\n\nAt the post-synaptic level\nTo minimize memory access, the local parameters and variables of a projection can be accessed through the Dendrite object, which gathers for a single post-synaptic neuron all synapses belonging to the projection.\nEach dendrite stores the parameters and variables of the corresponding synapses as attributes, as populations do for neurons. You can loop over all post-synaptic neurons receiving synapses with the dendrites iterator:\n\nfor dendrite in proj.dendrites:\n    print(dendrite.pre_ranks, dendrite.size)\n\n[0, 1, 2, 3, 4, 7, 8, 9] 8\n[2, 3, 5, 6, 7, 8] 6\n[0, 1, 2, 4, 6] 5\n[1, 4, 5, 6, 7, 9] 6\n[2, 5, 8, 9] 4\n[3, 6, 8, 9] 4\n[1, 5, 6, 8] 4\n[0, 1, 3, 4, 6] 5\n[1, 3, 4, 7, 9] 5\n[0, 4, 5, 7] 4\n\n\ndendrite.pre_ranks returns a list of pre-synaptic neuron ranks. dendrite.size returns the number of synapses for the considered post-synaptic neuron. Global parameters/variables return a single value (dendrite.tau) and local ones return a list (dendrite.w).\nYou can also access the dendrites individually, either by specifying the rank or the coordinates of the post-synaptic neuron:\n\ndendrite = proj.dendrite(3)\nprint(dendrite.w)\n\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou should make sure that the dendrite actually exists before accessing it through its rank, because it is otherwise a None object.\n\n\n\n\nFunctions\nIf you have defined a function inside a Synapse definition:\nBCM = ann.Synapse(\n    parameters = dict(\n        eta = 0.01,\n        tau = 100.,\n    ),\n    equations = [\n        ann.Variable('tau * dtheta/dt + theta = post.r^2', 'semiglobal'),\n        ann.Variable('dw/dt = eta * BCMRule(pre.r, post.r, theta)', min=0.0),\n    ]\n    functions = \"\"\"\n        BCMRule(pre, post, theta) = post * (post - theta) * pre\n    \"\"\"\n)\nyou can use this function in Python as if it were a method of the corresponding object:\nproj = net.connect(pop1, pop2, 'exc', BCM)\nproj.all_to_all(weights=1.0)\n\npre = np.linspace(0., 1., 100)\npost = np.linspace(0., 1., 100)\ntheta = 0.01 * np.ones(100)\n\nweight_change = proj.BCMRule(pre, post, theta)\nYou can pass either a list or a 1D Numpy array to each argument (not a single value, nor a multidimensional array!).\nThe size of the arrays passed for each argument is arbitrary (it must not match the projection’s size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#connecting-population-views",
    "href": "manual/Projections.html#connecting-population-views",
    "title": "Projections",
    "section": "Connecting population views",
    "text": "Connecting population views\nProjections are usually understood as a connectivity pattern between two populations. Complex connectivity patterns have to specifically designed (see Connectivity). In some cases, it can be much simpler to connect subsets of neurons directly, using built-in connector methods. To this end, the Projection object also accepts PopulationView objects for the pre and post arguments.\nLet’s suppose we want to connect the (8, 8) populations pop1 and pop2 in a all-to-all manner, but only for the (4,4) neurons in the center of these populations. The first step is to create the PopulationView objects using the slice operator:\npop1_center = pop1[2:7, 2:7]\npop2_center = pop2[2:7, 2:7]\nThey can then be simply used to create a projection:\nproj = net.connect(\n    pre = pop1_center,\n    post = pop2_center,\n    target = \"exc\",\n    synapse = BCM\n)\n\nproj.all_to_all(weights=1.0)\nEach neuron of pop2_center will receive synapses from all neurons of pop1_center, and only them. Neurons of pop2 which are not in pop2_center will not receive any synapse.\n\n\n\n\n\n\nWarning\n\n\n\nIf you define your own connector method and want to use PopulationViews, you will need to iterate over the ranks attribute of the PopulationView object.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#specifying-delays-in-synaptic-transmission",
    "href": "manual/Projections.html#specifying-delays-in-synaptic-transmission",
    "title": "Projections",
    "section": "Specifying delays in synaptic transmission",
    "text": "Specifying delays in synaptic transmission\nBy default, synaptic transmission is considered to be instantaneous (or more precisely, it takes one simulation step (dt) for a newly computed firing rate to be taken into account by post-synaptic neurons).\nIn order to take longer propagation times into account in the transmission of information between two populations, one has the possibility to define synaptic delays for a projection. All the built-in connector methods take an argument delays (default=dt), which can be a float (in milliseconds) or a random number generator.\nproj.all_to_all(weights = 1.0, delays = 10.0)\nproj.all_to_all(weights = 1.0, delays = Uniform(1.0, 10.0))\nIf the delay is not a multiple of the simulation time step (dt = 1.0 by default), it will be rounded to the closest multiple. The same is true for the values returned by a random number generator.\nNote: Per design, the minimal possible delay is equal to dt: values smaller than dt will be replaced by dt. Negative values do not make any sense and are ignored.\n\n\n\n\n\n\nWarning\n\n\n\nNon-uniform delays are not available on CUDA.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#controlling-projections",
    "href": "manual/Projections.html#controlling-projections",
    "title": "Projections",
    "section": "Controlling projections",
    "text": "Controlling projections\nSynaptic transmission, update and plasticity\nIt is possible to selectively control synaptic transmission and plasticity at the projection level. The boolean flags transmission, update and plasticity can be set for that purpose:\nproj.transmission = False\nproj.update = False\nproj.plasticity = False\n\nIf transmission is False, the projection is totally shut down: it does not transmit any information to the post-synaptic population (the corresponding weighted sums or conductances are constantly 0) and all synaptic variables are frozen to their current value (including the synaptic weights w).\nIf update is False, synaptic transmission occurs normally, but the synaptic variables are not updated. For spiking synapses, this includes traces when they are computed at each step, but not when they are integrated in an event-driven manner (flag event-driven). Beware: continous synaptic transmission as in NMDA synapses will not work in this mode, as internal variables are not updated.\nIf only plasticity is False, synaptic transmission and synaptic variable updates occur normally, but changes to the synaptic weight w are ignored.\n\nDisabling learning\nAlternatively, one can use the enable_learning() and disable_learning() methods of Projection. The effect of disable_learning() depends on the type of the projection:\n\nfor rate-coded projections, disable_learning() is equivalent to update=False: no synaptic variables is updated.\nfor spiking projections, it is equivalent to plasticity=False: only the weights are blocked.\n\nThe reason of this difference is to allow continuous synaptic transmission and computation of traces. Calling enable_learning() without arguments resumes the default learning behaviour.\nPeriodic learning\nenable_learning() also accepts two arguments period and offset. period defines the interval in ms between two evaluations of the synaptic variables. This can be useful when learning should only occur once at the end of a trial. It is recommended not to use ODEs in the equations in this case, as they are numerized according to a fixed time step. offset defines the time inside the period at which the evaluation should occur. By default, it is 0, so the variable updates will occur at the next step, then after period ms, and so on. Setting it to -1 will shift the update at the end of the period.\nNote that spiking synapses using online evaluation will not be affected by these parameters, as they are event-driven.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#multiple-targets",
    "href": "manual/Projections.html#multiple-targets",
    "title": "Projections",
    "section": "Multiple targets",
    "text": "Multiple targets\nFor spiking neurons, it may be desirable that a single synapses activates different currents (or conductances) in the post-synaptic neuron. One example are AMPA/NMDA synapses, where a single spike generates a “classical” AMPA current, plus a voltage-gated slower NMDA current. The following conductance-based Izhikevich is an example:\nRSNeuron = ann.Neuron(\n    parameters = dict(\n        a = 0.02,\n        b = 0.2,\n        c = -65.,\n        d = 8.,\n        tau_ampa = 5.,\n        tau_nmda = 150.,\n        vrev = 0.0,\n     ) ,\n    equations = [\n        'I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)',\n        'dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I',\n        'du/dt = a * (b*v - u)',\n        'tau_ampa * dg_ampa/dt = -g_ampa',\n        'tau_nmda * dg_nmda/dt = -g_nmda',\n    ],\n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\",\n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\"\n)\nHowever, g_ampa and g_nmda collect by default spikes from different projections, so the weights will not be shared between the \"ampa\" projection and the \"nmda\" one. It is therefore possible to specify a list of targets when building a projection, meaning that a single pre-synaptic spike will increase both g_ampa and g_nmda from the same weight:\nproj = net.connect(pop1, pop2, ['ampa', 'nmda'], STDP)\nAn example is provided in /notebooks/Ramp.ipynb.\n\n\n\n\n\n\nWarning\n\n\n\nMultiple targets are not available on CUDA yet.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Logging.html",
    "href": "manual/Logging.html",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "The tensorboard extension allows to visualize ANNarchy simulations using tensorboard. It requires the tensorboardX package:\npip install tensorboardX\nThe Logger class is a thin wrapper around tensorboardX.SummaryWriter, which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io. Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch.\nThe extension has to be imported explicitly:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.tensorboard import Logger\nFor detailed examples of how to use the extension, refer to the examples Basal Ganglia and Bayesian optimization, which are available as notebooks in the folder examples/tensorboard.\n\n\nThe Logger class has to be closed properly at the end of the script, so it is advised to use a context:\nwith ann.Logger() as logger:\n    logger.add_scalar(\"Accuracy\", acc, trial)\nYou can also make sure to close it:\nlogger = ann.Logger()\nlogger.add_scalar(\"Accuracy\", acc, trial)\nlogger.close()\nBy default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine. You can control these two elements by passing arguments to Logger():\nwith ann.Logger(logdir=\"/tmp/annarchy\", experiment=\"trial1\"): # logs in /tmp/annarchy/trial1\n\n\n\nAfter (or while) logging data within your simulation, run tensorboard in the terminal by specifying the path to the log directory:\ntensorboard --logdir runs\nYou will then be asked to open localhost:6006 in your browser and will see a page similar to this:\n\n\n\nStructure of a neural network\n\n\nThe information logged will be available in the different tabs (scalars, images...). You can also visualize and compare several experiments at the same time.\n\n\n\nThe add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc.\nThe simplest information to log is a scalar, for example the accuracy at the end of a trial:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        net.simulate(1000.0)\n        accuracy = ...\n        logger.add_scalar(\"Accuracy\", accuracy, trial)\nA tag should be given for each plot as the first argument. In the example above, the figure with the accuracy will be labelled \"Accuracy\" in tensorboard. You can also group plots together with 2-levels tags such as:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        net.simulate(1000.0)\n        train_accuracy = ...\n        test_accuracy = ...\n        logger.add_scalar(\"Accuracy/Train\", train_accuracy, trial)\n        logger.add_scalar(\"Accuracy/Test\", test_accuracy, trial)\nThe second argument is the scalar, obviously. The third is the index of x-axis of the plot. It can be the index of the trial, the current time or whatever you prefer.\nIf you want to display several scalars on the same plot, you can use the method add_scalars() and provide a dictionary:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        net.simulate(1000.0)\n        train_accuracy = ...\n        test_accuracy = ...\n        logger.add_scalars(\"Accuracy\", {'train': train_accuracy, 'test': test_accuracy}, trial)\n\n\n\nIt is also possible to log images, for example an input image or the firing rate of a 2D population, with the add_image() method:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        net.simulate(1000.0)\n        img = pop.r.reshape((10, 10))\n        logger.add_image(\"Population/Firing rate\", img, trial)\nThe image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images.\nThe values must be floats between 0 and 1 or integers between 0 and 255 in order to be displayed correctly. You can either do it yourself, or pass equalize=True to the add_image():\nlogger.add_image(\"Population/Firing rate\", img, trial, equalize=True)\nThe min/max values in the array are internally used to rescale the image:\nimg = (img - img.min())/(img.max() - img.min())\nTo display several images together, for example the receptive fields of a population, an array of size (number, height, width) or (number, height, width, 3) can be passed to add_images(), where number is the number of images to display:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        net.simulate(1000.0)\n        weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons\n        logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True)\nequalize=True applies the same scaling to all images, but you additionally pass equalize_per_image=True to have indepent scalings per image.\n\n\n\nHistograms can also be logged, for example to visualize the statistics of weights in a projection:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        net.simulate(1000.0)\n        weights= proj.w.flatten()\n        logger.add_histogram(\"Weight distribution\", weights, trial)\n\n\n\nMatplotlib figures can also be logged:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        fig = plt.figure()\n        plt.plot(pop.r)\n        logger.add_figure(\"Activity\", fig, trial)\nadd_figure() will automatically close the figure, no need to call show().\nBeware that this is very slow and requires a lot of space.\n\n\n\nThe previous methods can be called multiple times during a simulation, in order to visualize the changes during learning.\nadd_parameters() is more useful in the context of hyperparameter optimization, where the same network with different parameters is run multiple times.\nOnly once per simulation, typically at the end, you can log the value of some important parameters together with some metrics such as accuracy, error rate or so. This will allow tensorboard to display over multiple runs the relation between the parameters and the metrics in the tab “HPARAMS”:\nwith ann.Logger() as logger:\n    # ...\n    logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy}) \nRefer to Bayesian optimization for an example using the hyperopt library.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#creating-the-logger",
    "href": "manual/Logging.html#creating-the-logger",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "The Logger class has to be closed properly at the end of the script, so it is advised to use a context:\nwith ann.Logger() as logger:\n    logger.add_scalar(\"Accuracy\", acc, trial)\nYou can also make sure to close it:\nlogger = ann.Logger()\nlogger.add_scalar(\"Accuracy\", acc, trial)\nlogger.close()\nBy default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine. You can control these two elements by passing arguments to Logger():\nwith ann.Logger(logdir=\"/tmp/annarchy\", experiment=\"trial1\"): # logs in /tmp/annarchy/trial1",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#launching-tensorboard",
    "href": "manual/Logging.html#launching-tensorboard",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "After (or while) logging data within your simulation, run tensorboard in the terminal by specifying the path to the log directory:\ntensorboard --logdir runs\nYou will then be asked to open localhost:6006 in your browser and will see a page similar to this:\n\n\n\nStructure of a neural network\n\n\nThe information logged will be available in the different tabs (scalars, images...). You can also visualize and compare several experiments at the same time.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#logging-scalars",
    "href": "manual/Logging.html#logging-scalars",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "The add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc.\nThe simplest information to log is a scalar, for example the accuracy at the end of a trial:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        net.simulate(1000.0)\n        accuracy = ...\n        logger.add_scalar(\"Accuracy\", accuracy, trial)\nA tag should be given for each plot as the first argument. In the example above, the figure with the accuracy will be labelled \"Accuracy\" in tensorboard. You can also group plots together with 2-levels tags such as:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        net.simulate(1000.0)\n        train_accuracy = ...\n        test_accuracy = ...\n        logger.add_scalar(\"Accuracy/Train\", train_accuracy, trial)\n        logger.add_scalar(\"Accuracy/Test\", test_accuracy, trial)\nThe second argument is the scalar, obviously. The third is the index of x-axis of the plot. It can be the index of the trial, the current time or whatever you prefer.\nIf you want to display several scalars on the same plot, you can use the method add_scalars() and provide a dictionary:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        net.simulate(1000.0)\n        train_accuracy = ...\n        test_accuracy = ...\n        logger.add_scalars(\"Accuracy\", {'train': train_accuracy, 'test': test_accuracy}, trial)",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#logging-images",
    "href": "manual/Logging.html#logging-images",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "It is also possible to log images, for example an input image or the firing rate of a 2D population, with the add_image() method:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        net.simulate(1000.0)\n        img = pop.r.reshape((10, 10))\n        logger.add_image(\"Population/Firing rate\", img, trial)\nThe image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images.\nThe values must be floats between 0 and 1 or integers between 0 and 255 in order to be displayed correctly. You can either do it yourself, or pass equalize=True to the add_image():\nlogger.add_image(\"Population/Firing rate\", img, trial, equalize=True)\nThe min/max values in the array are internally used to rescale the image:\nimg = (img - img.min())/(img.max() - img.min())\nTo display several images together, for example the receptive fields of a population, an array of size (number, height, width) or (number, height, width, 3) can be passed to add_images(), where number is the number of images to display:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        net.simulate(1000.0)\n        weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons\n        logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True)\nequalize=True applies the same scaling to all images, but you additionally pass equalize_per_image=True to have indepent scalings per image.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#logging-histograms",
    "href": "manual/Logging.html#logging-histograms",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "Histograms can also be logged, for example to visualize the statistics of weights in a projection:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        net.simulate(1000.0)\n        weights= proj.w.flatten()\n        logger.add_histogram(\"Weight distribution\", weights, trial)",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#logging-figures",
    "href": "manual/Logging.html#logging-figures",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "Matplotlib figures can also be logged:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        fig = plt.figure()\n        plt.plot(pop.r)\n        logger.add_figure(\"Activity\", fig, trial)\nadd_figure() will automatically close the figure, no need to call show().\nBeware that this is very slow and requires a lot of space.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#logging-parameters",
    "href": "manual/Logging.html#logging-parameters",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "The previous methods can be called multiple times during a simulation, in order to visualize the changes during learning.\nadd_parameters() is more useful in the context of hyperparameter optimization, where the same network with different parameters is run multiple times.\nOnly once per simulation, typically at the end, you can log the value of some important parameters together with some metrics such as accuracy, error rate or so. This will allow tensorboard to display over multiple runs the relation between the parameters and the metrics in the tab “HPARAMS”:\nwith ann.Logger() as logger:\n    # ...\n    logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy}) \nRefer to Bayesian optimization for an example using the hyperopt library.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Populations.html",
    "href": "manual/Populations.html",
    "title": "Populations",
    "section": "",
    "text": "Once the Neuron objects have been defined, the populations can be created. Let’s suppose we have defined the following rate-coded neuron:\nimport numpy as np\nimport ANNarchy as ann\n\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.2\n    ),\n    equations = [\n        'tau * dv/dt  + v = baseline + sum(exc)',\n        'r = pos(v)',\n    ]\n)\n\nANNarchy 5.0 (5.0.0rc0) on linux (posix).",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Populations.html#creating-populations",
    "href": "manual/Populations.html#creating-populations",
    "title": "Populations",
    "section": "Creating populations",
    "text": "Creating populations\nPopulations of neurons are contained in an instance of the Population class, which can be created by calling the create() method of a network:\n\nnet = ann.Network()\n\npop1 = net.create(geometry=100, neuron=LeakyIntegratorNeuron)\npop2 = net.create(geometry=(8, 8), neuron=LeakyIntegratorNeuron, name=\"pop2\")\n\nThe rate-coded or spiking nature of the Neuron instance is irrelevant when creating the Population object. Population objects can also be created directly by providing the id of the network object, but it is not recommended.\nNetwork.create() takes different arguments:\n\ngeometry defines the number of neurons in the population, as well as its spatial structure (1D/2D/3D or more). For example, a two-dimensional population with 15*10 neurons takes the argument (15, 10), while a one-dimensional array of 100 neurons would take (100,) or simply 100.\nneuron indicates the neuron type to use for this population (which must have been defined before). It requires a Neuron class or instance.\nname is an unique string for each population in the network. If name is omitted, an internal name such as pop0 will be given (the number is incremented every time a new population is defined). Although this argument is optional, it is recommended to give an understandable name to each population: if you somehow “lose” the reference to the Population object in some portion of your code, you can always retrieve it using the net.get_population(name) method.\n\nAfter creation, each population has several attributes defined (corresponding to the parameters and variables of the Neuron type) and is assigned a fixed size (pop.size corresponding to the total number of neurons, here 100 for pop1 and 64 for pop2) and geometry (pop1.geometry, here (100, ) and (8, 8)).",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Populations.html#geometry-and-ranks",
    "href": "manual/Populations.html#geometry-and-ranks",
    "title": "Populations",
    "section": "Geometry and ranks",
    "text": "Geometry and ranks\nEach neuron in the population has a set of coordinates (expressed relative to pop1.geometry) and a rank (from 0 to pop1.size -1). The reason is that spatial coordinates are useful for visualization, or when defining a distance-dependent connection pattern, but that ANNarchy internally uses flat arrays for performance reasons.\nThe coordinates use the matrix notation for multi-dimensional arrays, which is also used by Numpy (for a 2D matrix, the first index represents the row, the second the column). You can therefore use safely the reshape() methods of Numpy to switch between coordinates-based and rank-based representations of an array.\nTo convert the rank of a neuron to its coordinates (and vice-versa), you can use the ravel_multi_index and unravel_index methods of Numpy, but they can be quite slow. The Population class provides two more efficient methods to do this conversion:\n\ncoordinates_from_rank returns a tuple representing the coordinates of neuron based on its rank (between 0 and size -1, otherwise an error is thrown).\nrank_from_coordinates returns the rank corresponding to the coordinates.\n\nFor example, with pop2 having a geometry (8, 8):\n\npop2.coordinates_from_rank(15)\n\n(1, 7)\n\n\n\npop2.rank_from_coordinates((4, 6))\n\n38",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Populations.html#population-attributes",
    "href": "manual/Populations.html#population-attributes",
    "title": "Populations",
    "section": "Population attributes",
    "text": "Population attributes\nThe value of the parameters and variables of all neurons in a population can be accessed and modified through population attributes.\nWith the previously defined populations, you can list all their parameters and variables with:\n\npop2.attributes\n\n['tau', 'baseline', 'v', 'r']\n\n\n\npop2.parameters\n\n['tau', 'baseline']\n\n\n\npop2.variables\n\n['v', 'r']\n\n\nReading their value is straightforward:\n\npop2.tau\n\n10.0\n\n\n\npop2.r\n\narray([[0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\nPopulation-wise parameters/variables have a single value for the population, while neuron-specific ones return a NumPy array with the same geometry has the population.\nSetting their value is also simple:\n\npop2.tau = 20.0\nprint(pop2.tau)\n\n20.0\n\n\n\npop2.r = 1.0\nprint(pop2.r) \n\n[[1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1.]]\n\n\n\npop2.v = 0.5 * np.ones(pop2.geometry)\nprint(pop2.v)\n\n[[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]]\n\n\n\npop2.r = ann.Uniform(0.0, 1.0)\nprint(pop2.r)\n\n[[0.88278574 0.28130904 0.43367077 0.50913694 0.06093721 0.39228607\n  0.55457071 0.84540543]\n [0.73271108 0.10024575 0.15139512 0.67225055 0.10787216 0.94030627\n  0.25132358 0.62874473]\n [0.5955857  0.6221603  0.19148943 0.18739162 0.33259606 0.27971882\n  0.14893079 0.34920703]\n [0.89394619 0.92644123 0.80385068 0.08887383 0.41098601 0.81618563\n  0.15934374 0.55867925]\n [0.12668823 0.76337302 0.48278515 0.94401798 0.88530427 0.76855142\n  0.91091673 0.53636214]\n [0.34179086 0.29020004 0.11615279 0.96351097 0.2509132  0.50810183\n  0.01549096 0.23028034]\n [0.10597681 0.06087826 0.53037316 0.32030348 0.48264167 0.07696145\n  0.37961175 0.58398803]\n [0.20115746 0.69142218 0.56428511 0.88247939 0.13806341 0.44426022\n  0.08976612 0.02018575]]\n\n\nFor population-wide attributes, you can only specify a single value (float, int or bool depending on the type of the parameter/variable). For neuron-specific attributes, you can provide either:\n\na single value which will be applied to all neurons of the population.\na list or a one-dimensional Numpy array of the same length as the number of neurons in the population. This information is provided by pop1.size.\na Numpy array of the same shape as the geometry of the population. This information is provided by pop1.geometry.\na random number generator object (Uniform, Normal...).\n\n\n\n\n\n\n\nNote\n\n\n\nIf you do not want to use the attributes of Python (for example when doing a loop over unknown attributes), you can also use the get(name) and set(values) methods of Population:\n\npop1.set({'v': 1.0, 'r': ann.Uniform(0.0, 1.0)})\nprint(pop1.get('v'))\nprint(pop1.get('r'))\n\n[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1.]\n[0.90182766 0.09259207 0.3726977  0.45817672 0.15112235 0.10496566\n 0.77518394 0.8830195  0.85959747 0.26498527 0.81807916 0.76340563\n 0.53191772 0.6958435  0.79753015 0.11778895 0.82693313 0.79647959\n 0.63865791 0.16748874 0.7563963  0.11848339 0.0985155  0.1435983\n 0.04366156 0.86150863 0.7020524  0.30669823 0.01796339 0.87956556\n 0.28805126 0.7378753  0.29682299 0.26393512 0.63680801 0.93853053\n 0.5639794  0.61534192 0.4516971  0.79543954 0.65003964 0.47950141\n 0.82133482 0.72023134 0.16565911 0.09568645 0.78650168 0.44425343\n 0.41165469 0.95472278 0.67191469 0.89705617 0.17705098 0.69071098\n 0.51507371 0.91905904 0.71851021 0.06783573 0.82166457 0.852719\n 0.25939713 0.75584488 0.38345775 0.15095496 0.88087791 0.53711281\n 0.10640781 0.73838776 0.1699055  0.29030966 0.05204202 0.21367352\n 0.4794544  0.53070043 0.59005974 0.4573081  0.70528585 0.23826384\n 0.44877338 0.8787035  0.55371882 0.96479858 0.71630609 0.83334776\n 0.74171017 0.13187172 0.30193958 0.94569879 0.11567426 0.98194533\n 0.03968325 0.01941603 0.51437854 0.28151292 0.25298667 0.13527596\n 0.70254359 0.68078721 0.65661374 0.4986296 ]",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Populations.html#accessing-individual-neurons",
    "href": "manual/Populations.html#accessing-individual-neurons",
    "title": "Populations",
    "section": "Accessing individual neurons",
    "text": "Accessing individual neurons\nThere exists a purely semantic access to individual neurons of a population. The IndividualNeuron class wraps population data for a specific neuron. It can be accessed through the Population.neuron() method using either the rank of the neuron (from 0 to pop1.size - 1) or its coordinates in the population’s geometry:\n\nprint(pop2.neuron(2, 2))\n\nNeuron of the population pop1 with rank 18 (coordinates (2, 2)).\nParameters:\n  tau = 20.0\n  baseline = -0.2\n\nVariables:\n  v = 0.5\n  r = 0.19148943158453047\n\n\n\nThe individual neurons can be manipulated individually:\n\nmy_neuron = pop2.neuron(2, 2)\nmy_neuron.r = 1.0\nprint(my_neuron)\n\nNeuron of the population pop1 with rank 18 (coordinates (2, 2)).\nParameters:\n  tau = 20.0\n  baseline = -0.2\n\nVariables:\n  v = 0.5\n  r = 1.0\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIndividualNeuron is only a wrapper for ease of use, the real data is stored in arrays for the whole population, so accessing individual neurons is much slower and should be reserved to specific cases (i.e. only from time to time and for a limited set of neurons).",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Populations.html#accessing-groups-of-neurons---populationview",
    "href": "manual/Populations.html#accessing-groups-of-neurons---populationview",
    "title": "Populations",
    "section": "Accessing groups of neurons - PopulationView",
    "text": "Accessing groups of neurons - PopulationView\nIndividual neurons can be grouped into PopulationView objects, which hold references to different neurons of the same population. One can create population views by “adding” several neurons together:\n\npopview = pop2.neuron(2, 2) + pop2.neuron(3, 3) + pop2.neuron(4, 4)\n\n\npopview.r = 1.0\nprint(pop2.r)\n\n[[0.88278574 0.28130904 0.43367077 0.50913694 0.06093721 0.39228607\n  0.55457071 0.84540543]\n [0.73271108 0.10024575 0.15139512 0.67225055 0.10787216 0.94030627\n  0.25132358 0.62874473]\n [0.5955857  0.6221603  1.         0.18739162 0.33259606 0.27971882\n  0.14893079 0.34920703]\n [0.89394619 0.92644123 0.80385068 1.         0.41098601 0.81618563\n  0.15934374 0.55867925]\n [0.12668823 0.76337302 0.48278515 0.94401798 1.         0.76855142\n  0.91091673 0.53636214]\n [0.34179086 0.29020004 0.11615279 0.96351097 0.2509132  0.50810183\n  0.01549096 0.23028034]\n [0.10597681 0.06087826 0.53037316 0.32030348 0.48264167 0.07696145\n  0.37961175 0.58398803]\n [0.20115746 0.69142218 0.56428511 0.88247939 0.13806341 0.44426022\n  0.08976612 0.02018575]]\n\n\nOne can also use the slice operators to create PopulationViews:\n\npopview = pop2[3, :]\npopview.r = 1.0\nprint(pop2.r)\n\n[[0.88278574 0.28130904 0.43367077 0.50913694 0.06093721 0.39228607\n  0.55457071 0.84540543]\n [0.73271108 0.10024575 0.15139512 0.67225055 0.10787216 0.94030627\n  0.25132358 0.62874473]\n [0.5955857  0.6221603  1.         0.18739162 0.33259606 0.27971882\n  0.14893079 0.34920703]\n [1.         1.         1.         1.         1.         1.\n  1.         1.        ]\n [0.12668823 0.76337302 0.48278515 0.94401798 1.         0.76855142\n  0.91091673 0.53636214]\n [0.34179086 0.29020004 0.11615279 0.96351097 0.2509132  0.50810183\n  0.01549096 0.23028034]\n [0.10597681 0.06087826 0.53037316 0.32030348 0.48264167 0.07696145\n  0.37961175 0.58398803]\n [0.20115746 0.69142218 0.56428511 0.88247939 0.13806341 0.44426022\n  0.08976612 0.02018575]]\n\n\nor:\n\npopview = pop2[2:5, 4] \npopview.r = 1.0\nprint(pop1.r)\n\n[0.90182766 0.09259207 0.3726977  0.45817672 0.15112235 0.10496566\n 0.77518394 0.8830195  0.85959747 0.26498527 0.81807916 0.76340563\n 0.53191772 0.6958435  0.79753015 0.11778895 0.82693313 0.79647959\n 0.63865791 0.16748874 0.7563963  0.11848339 0.0985155  0.1435983\n 0.04366156 0.86150863 0.7020524  0.30669823 0.01796339 0.87956556\n 0.28805126 0.7378753  0.29682299 0.26393512 0.63680801 0.93853053\n 0.5639794  0.61534192 0.4516971  0.79543954 0.65003964 0.47950141\n 0.82133482 0.72023134 0.16565911 0.09568645 0.78650168 0.44425343\n 0.41165469 0.95472278 0.67191469 0.89705617 0.17705098 0.69071098\n 0.51507371 0.91905904 0.71851021 0.06783573 0.82166457 0.852719\n 0.25939713 0.75584488 0.38345775 0.15095496 0.88087791 0.53711281\n 0.10640781 0.73838776 0.1699055  0.29030966 0.05204202 0.21367352\n 0.4794544  0.53070043 0.59005974 0.4573081  0.70528585 0.23826384\n 0.44877338 0.8787035  0.55371882 0.96479858 0.71630609 0.83334776\n 0.74171017 0.13187172 0.30193958 0.94569879 0.11567426 0.98194533\n 0.03968325 0.01941603 0.51437854 0.28151292 0.25298667 0.13527596\n 0.70254359 0.68078721 0.65661374 0.4986296 ]\n\n\nPopulationView objects can be used to create projections.\n\n\n\n\n\n\nWarning\n\n\n\nContrary to the equivalent in PyNN, PopulationViews in ANNarchy can only group neurons from the same population.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Populations.html#functions",
    "href": "manual/Populations.html#functions",
    "title": "Populations",
    "section": "Functions",
    "text": "Functions\nIf you have defined a function inside a Neuron definition:\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = dict(   \n        tau = 10.0,\n        slope = 1.0,\n        baseline = -0.2,\n    ),\n    equations = [\n        'tau * dv/dt + v = baseline + sum(exc)'\n        'r = sigmoid(v, slope)'\n    ],\n    functions = \"\"\"\n        sigmoid(x, k) = 1.0 / (1.0 + exp(-x*k))\n    \"\"\"\n)\nyou can use this function in Python as if it were a method of the corresponding object:\npop = net.create(1000, LeakyIntegratorNeuron)\n\nx = np.linspace(-1., 1., 100)\nk = np.ones(100)\nr = pop.sigmoid(x, k)\nYou can pass either a list or a 1D Numpy array to each argument (not a single value, nor a multidimensional array!).\nThe size of the arrays passed for each argument is arbitrary (it must not match the population’s size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/SpikeNeuron.html",
    "href": "manual/SpikeNeuron.html",
    "title": "Spiking neurons",
    "section": "",
    "text": "Contrary to rate-coded neurons, the use of spiking neurons requires the additional definition of a spike condition (the criteria defining the emission of a spike, typically when the membrane potential exceeds a threshold) and reset equations, governing the evolution of all variables after a spike is emitted.\n\n\nANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN. Their definition (parameters, equations) are described in Specific Neurons.\nThe classes can be used directly when creating the populations (no need to implement them). Example:\npop = net.create(geometry = 1000, neuron = ann.Izhikevich)\nThe model can also be instantiated with different parameter values to change the behavior of the neuron. For example for the Izhikevich neuron model:\npop = net.create(\n    geometry = 1000, \n    Izhikevich(\n        a=0.02,\n        b=0.2,\n        c=-65.0,\n        d=8.0,\n        v_thresh=30.0,\n        i_offset=0.0,\n        noise=0.0,\n    )\n)\n\n\n\nContrary to many other simulators, you can define your own neural models. Let’s consider a simple leaky integrate-and-fire spiking neuron model (LIF) using a voltage-gated excitatory conductance:\n\n    \\tau \\cdot  \\frac{ d v(t) }{ dt } = (E_r - v(t) ) + g_\\text{exc}(t) \\cdot (E_e -  v(t) )\n\nwhere v(t) is the membrane potential, \\tau is the membrane time constant (in milliseconds), E_r the resting potential, E_e the target potential for excitatory synapses and g_\\text{exc}(t) the total current induced by excitatory synapses.\nThis neural model can be defined in ANNarchy by:\nLIF = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        Er = -60.0,\n        Ee = 0.0,\n        T = -45.0,\n    ),\n    equations = [\n        ann.Variable('tau * dv/dt = (Er - v) + g_exc *(Ee- v)', init = 0.0),\n    ],\n    spike = \"v &gt; T\",\n    reset = \"v = Er\",\n    refractory = 5.0\n)\nAs for rate-coded neurons, the parameters are defined in the parameters description, here globally for the population. equations contains the description of the ODE followed by the membrane potential. The additional information to provide is:\n\nspike : a boolean condition on a single variable (typically the membrane potential) deciding when a spike is emitted.\nreset : the modifications to the neuron’s variables after a spike is emitted (typically, clamping the membrane potential to its reset potential).\nrefractory: optionally a refractory period in ms.\n\n\n\nThe spike condition is a single constraint definition. You may use the different available comparison operators (&gt;, &lt;, ==, etc) on a single neuron variable, using as many parameters as you want.\nThe use of assignment statements or ODEs will lead to an error. Conditional statements can be used. Example:\nneuron = ann.Neuron(\n    parameters = dict(\n        ...\n        T = -45.0 \n    ),\n    equations = [\n        'prev_v = v',\n        'noise = Uniform (-5.0, 5.0)',\n        'tau * dv/dt = E - v + g_exc',\n    ],\n    spike = \" (v &gt; T + noise) and (prev_v &lt; T + noise) \"\n)\n\n\n\nIn the reset attribute, you define the variables which should be set to certain values after a spike occured. Any assignment statements is allowed (=, +=, etc), but the use of ODEs is not possible, as the reset is performed only once at the end of the time step.\nExample:\nreset = [\n    'v = Er',\n    'u += 0.1',\n]\nA multiline string is also possible (and perhaps easier, as no other flag is allowed):\nreset = \"\"\"\n    v = Er\n    u += 0.1\n\"\"\"\n\n\n\nContrary to rate-coded neurons, spiking neurons use conductance variables to encode the received inputs, not weighted sums. In ANNarchy, the conductances are defined by g_ followed by the target name. For example, if a population receives excitatory input (target exc) from another one, you can access the total conductance provoked by exc spikes with:\ntau * dv/dt + v = g_exc\nThe dynamics of the conductance can be specified after its usage in the membrane potential equation.\nThe default behaviour for conductances is an instantaneous reset (or infinitely fast exponential decay). In practice, this means that all incoming spikes are summed up (weighted by the synaptic efficiency) at the beginning of a simulation step, and the resulting conductance is reset to 0.0 at the end of the step. This default behaviour is equivalent to :\nLIF = ann.Neuron(\n    equations = [\n        ann.Variable('tau * dv/dt = (Er - v) + g_exc * (Ee- v)', init = 0.0),\n        ann.Variable('g_exc = 0.0'),\n    ],\n)\nIncoming spikes increase g_exc and can provoke a post-synaptic spike at the next step, but leave no trace beyond that point.\nMost models however use exponentially decaying synapses, where the conductance decays with a short time constant after a spike is received. This behavior should be explicitely specified in the neuron’s equations:\nLIF = ann.Neuron(\n    equations = [\n        ann.Variable('tau * dv/dt = (Er - v) + g_exc * (Ee- v)', init = 0.0),\n        ann.Variable('tau_exc * dg_exc/dt = - g_exc'),\n    ],\n)\ng_exc is increased by incoming spikes, and slowly decays back to 0.0 until the next spikes arrive.\n\n\n\nThe refractory period in milliseconds is specified by the refractory parameter of Neuron.\nLIF = ann.Neuron (\n    refractory = 5.0\n)\nThe refractory argument can be a floating value or the name of a parameter/variable (string).\nIf dt = 0.1, this means that the equations will not be evaluated for 50 consecutive steps after a spike is emitted, except for the conductances (starting with g_) which are evaluated normally during the refractory period (the neuron is not \"deaf\", it only is frozen in a refractory state).\nrefractory becomes an attribute of a spiking Population object, so it can be set specifically for a population even when omitted in the neuron definition:\npop = ann.Population(geometry = 1000, neuron = LIF)\npop.refractory = Uniform(1.0, 10.0)\nIt can be either a single value, a RandomDistribution object or a Numpy array of the same size/geometry as the population.\n\n\n\nMethod 1: ISI\nSpiking neurons define an additional variable t_last which represents the timestamp (in ms) of the last emitted spike (updated at the end of the reset statement). The time elapsed since the last spike is then t - t_last.\nThis can be used to update the instantaneous firing rate of a neuron, by inverting the inter-spike interval (ISI) during the reset statement following the emission of a spike:\nneuron = ann.Neuron(\n    parameters = dict(tau = 20.0, tauf = 1000.),\n    equations = [\n        \"tau * dv/dt + v = g_exc\",\n        \"tauf * df/dt = -f \",\n    ],\n    spike = \"v &gt; 1.0\",\n    reset = \"\"\"\n        v = 0.0\n        f = 1000./(t - t_last)\n    \"\"\"\n)\nHere, a leaky integrator on f is needed to 1) smooth the firing rate and 2) slowly decay to 0 when the neuron stops firing. This method reflects very fast changes in the firing rate, but is also very sensible to noise.\nMethod 2: Window\nA more stable way to compute the firing rate of a neuron is to count at each time step the number of spikes emitted during a sliding temporal window (of 100 ms or 1s for example). By default, spiking neurons only record the time of the last spike they emitted (t_last), so this mechanism has to be explicitely enabled by calling the compute_firing_rate() method of the desired population:\npop = ann.Population(100, Izhikevich)\npop.compute_firing_rate(window=1000.0)\nThe window argument represents the period in milliseconds over which the spikes will be counted. The resulting firing rate (in Hz) will be stored in the local variable r (as for rate-coded neurons), which can be accessed by the neuron itself or by incoming and outgoing synapse (pre.r and post.r).\nIf the method has not been called, the variable r of a spiking neuron will be constantly 0.0.\n\n\n\n\n\n\nWarning\n\n\n\nThe window method is not available on CUDA yet.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Spiking neurons"
    ]
  },
  {
    "objectID": "manual/SpikeNeuron.html#built-in-neurons",
    "href": "manual/SpikeNeuron.html#built-in-neurons",
    "title": "Spiking neurons",
    "section": "",
    "text": "ANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN. Their definition (parameters, equations) are described in Specific Neurons.\nThe classes can be used directly when creating the populations (no need to implement them). Example:\npop = net.create(geometry = 1000, neuron = ann.Izhikevich)\nThe model can also be instantiated with different parameter values to change the behavior of the neuron. For example for the Izhikevich neuron model:\npop = net.create(\n    geometry = 1000, \n    Izhikevich(\n        a=0.02,\n        b=0.2,\n        c=-65.0,\n        d=8.0,\n        v_thresh=30.0,\n        i_offset=0.0,\n        noise=0.0,\n    )\n)",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Spiking neurons"
    ]
  },
  {
    "objectID": "manual/SpikeNeuron.html#user-defined-neurons",
    "href": "manual/SpikeNeuron.html#user-defined-neurons",
    "title": "Spiking neurons",
    "section": "",
    "text": "Contrary to many other simulators, you can define your own neural models. Let’s consider a simple leaky integrate-and-fire spiking neuron model (LIF) using a voltage-gated excitatory conductance:\n\n    \\tau \\cdot  \\frac{ d v(t) }{ dt } = (E_r - v(t) ) + g_\\text{exc}(t) \\cdot (E_e -  v(t) )\n\nwhere v(t) is the membrane potential, \\tau is the membrane time constant (in milliseconds), E_r the resting potential, E_e the target potential for excitatory synapses and g_\\text{exc}(t) the total current induced by excitatory synapses.\nThis neural model can be defined in ANNarchy by:\nLIF = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        Er = -60.0,\n        Ee = 0.0,\n        T = -45.0,\n    ),\n    equations = [\n        ann.Variable('tau * dv/dt = (Er - v) + g_exc *(Ee- v)', init = 0.0),\n    ],\n    spike = \"v &gt; T\",\n    reset = \"v = Er\",\n    refractory = 5.0\n)\nAs for rate-coded neurons, the parameters are defined in the parameters description, here globally for the population. equations contains the description of the ODE followed by the membrane potential. The additional information to provide is:\n\nspike : a boolean condition on a single variable (typically the membrane potential) deciding when a spike is emitted.\nreset : the modifications to the neuron’s variables after a spike is emitted (typically, clamping the membrane potential to its reset potential).\nrefractory: optionally a refractory period in ms.\n\n\n\nThe spike condition is a single constraint definition. You may use the different available comparison operators (&gt;, &lt;, ==, etc) on a single neuron variable, using as many parameters as you want.\nThe use of assignment statements or ODEs will lead to an error. Conditional statements can be used. Example:\nneuron = ann.Neuron(\n    parameters = dict(\n        ...\n        T = -45.0 \n    ),\n    equations = [\n        'prev_v = v',\n        'noise = Uniform (-5.0, 5.0)',\n        'tau * dv/dt = E - v + g_exc',\n    ],\n    spike = \" (v &gt; T + noise) and (prev_v &lt; T + noise) \"\n)\n\n\n\nIn the reset attribute, you define the variables which should be set to certain values after a spike occured. Any assignment statements is allowed (=, +=, etc), but the use of ODEs is not possible, as the reset is performed only once at the end of the time step.\nExample:\nreset = [\n    'v = Er',\n    'u += 0.1',\n]\nA multiline string is also possible (and perhaps easier, as no other flag is allowed):\nreset = \"\"\"\n    v = Er\n    u += 0.1\n\"\"\"\n\n\n\nContrary to rate-coded neurons, spiking neurons use conductance variables to encode the received inputs, not weighted sums. In ANNarchy, the conductances are defined by g_ followed by the target name. For example, if a population receives excitatory input (target exc) from another one, you can access the total conductance provoked by exc spikes with:\ntau * dv/dt + v = g_exc\nThe dynamics of the conductance can be specified after its usage in the membrane potential equation.\nThe default behaviour for conductances is an instantaneous reset (or infinitely fast exponential decay). In practice, this means that all incoming spikes are summed up (weighted by the synaptic efficiency) at the beginning of a simulation step, and the resulting conductance is reset to 0.0 at the end of the step. This default behaviour is equivalent to :\nLIF = ann.Neuron(\n    equations = [\n        ann.Variable('tau * dv/dt = (Er - v) + g_exc * (Ee- v)', init = 0.0),\n        ann.Variable('g_exc = 0.0'),\n    ],\n)\nIncoming spikes increase g_exc and can provoke a post-synaptic spike at the next step, but leave no trace beyond that point.\nMost models however use exponentially decaying synapses, where the conductance decays with a short time constant after a spike is received. This behavior should be explicitely specified in the neuron’s equations:\nLIF = ann.Neuron(\n    equations = [\n        ann.Variable('tau * dv/dt = (Er - v) + g_exc * (Ee- v)', init = 0.0),\n        ann.Variable('tau_exc * dg_exc/dt = - g_exc'),\n    ],\n)\ng_exc is increased by incoming spikes, and slowly decays back to 0.0 until the next spikes arrive.\n\n\n\nThe refractory period in milliseconds is specified by the refractory parameter of Neuron.\nLIF = ann.Neuron (\n    refractory = 5.0\n)\nThe refractory argument can be a floating value or the name of a parameter/variable (string).\nIf dt = 0.1, this means that the equations will not be evaluated for 50 consecutive steps after a spike is emitted, except for the conductances (starting with g_) which are evaluated normally during the refractory period (the neuron is not \"deaf\", it only is frozen in a refractory state).\nrefractory becomes an attribute of a spiking Population object, so it can be set specifically for a population even when omitted in the neuron definition:\npop = ann.Population(geometry = 1000, neuron = LIF)\npop.refractory = Uniform(1.0, 10.0)\nIt can be either a single value, a RandomDistribution object or a Numpy array of the same size/geometry as the population.\n\n\n\nMethod 1: ISI\nSpiking neurons define an additional variable t_last which represents the timestamp (in ms) of the last emitted spike (updated at the end of the reset statement). The time elapsed since the last spike is then t - t_last.\nThis can be used to update the instantaneous firing rate of a neuron, by inverting the inter-spike interval (ISI) during the reset statement following the emission of a spike:\nneuron = ann.Neuron(\n    parameters = dict(tau = 20.0, tauf = 1000.),\n    equations = [\n        \"tau * dv/dt + v = g_exc\",\n        \"tauf * df/dt = -f \",\n    ],\n    spike = \"v &gt; 1.0\",\n    reset = \"\"\"\n        v = 0.0\n        f = 1000./(t - t_last)\n    \"\"\"\n)\nHere, a leaky integrator on f is needed to 1) smooth the firing rate and 2) slowly decay to 0 when the neuron stops firing. This method reflects very fast changes in the firing rate, but is also very sensible to noise.\nMethod 2: Window\nA more stable way to compute the firing rate of a neuron is to count at each time step the number of spikes emitted during a sliding temporal window (of 100 ms or 1s for example). By default, spiking neurons only record the time of the last spike they emitted (t_last), so this mechanism has to be explicitely enabled by calling the compute_firing_rate() method of the desired population:\npop = ann.Population(100, Izhikevich)\npop.compute_firing_rate(window=1000.0)\nThe window argument represents the period in milliseconds over which the spikes will be counted. The resulting firing rate (in Hz) will be stored in the local variable r (as for rate-coded neurons), which can be accessed by the neuron itself or by incoming and outgoing synapse (pre.r and post.r).\nIf the method has not been called, the variable r of a spiking neuron will be constantly 0.0.\n\n\n\n\n\n\nWarning\n\n\n\nThe window method is not available on CUDA yet.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Spiking neurons"
    ]
  },
  {
    "objectID": "manual/Parser.html",
    "href": "manual/Parser.html",
    "title": "Equation parser",
    "section": "",
    "text": "A Neuron or Synapse instance is primarily defined by two sets of values which must be specified in its constructor:\n\nParameters are values such as time constants which are constant during the simulation. They can be the same throughout the population/projection (global), or take different values for each neuron/synapse (local). In projections, they can also be semiglobal: one value per post-synaptic neuron.\nVariables are neuronal variables (for example the membrane potential or firing rate) or synaptic variables (the synaptic efficiency) whose value evolve with time during the simulation. The equation (which can be an ordinary differential equation or not) ruling their evolution can be described using a specific equation-oriented language.\n\n\n\nParameters are provided to Neuron or Synapse models using dictionaries:\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.1,\n    )\n)\nA multiline string can also be passed (see below).\nAs a neuron/synapse type is likely to be reused in different populations/projections, it is good practice to set reasonable initial values in the neuron/synapse type, and eventually adapt them to the corresponding populations/projections later on.\nLocal vs. global parameters\nNeural parameters can be either local (each neuron has a different value of the parameter) or global (the value is the same for all neurons of a population - but can be different in two populations using the same neuron type).\nThe default locality of a parameter is global. The Parameter dataclass allows to tell ANNarchy that the parameter has to be local, i.e. it should take one value per neuron in the population.\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = ann.Parameter(10.0),\n        baseline = ann.Parameter(-0.1),\n    )\n)\nann.Parameter() also allows to define global parameters, but it is longer:\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = ann.Parameter(10.0, locality='global'), # or just: tau = 10.0\n        baseline = ann.Parameter(-0.1, locality='global'), # or just: baseline = -0.1\n    )\n)\nFor synapses, there are three different localities (see Locality):\n\n'local': one value per synapse in the projection (e.g. the synaptic efficiency).\n'semiglobal': one value per post-synaptic neuron in the projection.\n'global': one value per projection.\n\nThese strings can be passed to the parameter dataclass. 'global' is still the default.\nType of the parameter\nParameters have floating-point precision by default. If you want to force the parameter to be an integer or boolean, you can set the type argument of parameter:\nneuron = ann.Neuron(\n    parameters = dict(\n        number_spikes = ann.Parameter(0, type=int),\n        activated = ann.Parameter(False, locality='global', type=bool),\n    )\n)\n\"\"\"\nConstants\nAlternatively, it is possible to use external constants in the parameter definition by using their name (see later):\ntau_exc = ann.Constant('tau_exc', 10.0)\n\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 'tau_exc'\n    ),\n)\nThe advantage of this method is that if a parameter value is \"shared\" across several neuron/synapse types, you only need to change the value once, instead of in each neuron/synapse definition.\n\n\n\nTime-varying variables are defined using a list of strings (or ann.Variable instances):\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.1,\n    ),\n    equations = [\n        'noise += Uniform(-0.1, 0.1)',\n        'tau * dv/dt  + v = baseline + noise',\n        'r = pos(v)',\n    ]\nThe evolution of each variable with time can be described through a simple equation or an ordinary differential equation (ODE). ANNarchy provides a simple parser for mathematical expressions, whose role is to translate a high-level description of the equation into an optimized C++ code snippet.\nThe equation for one variable can depend on parameters, other variables (even when declared later) or constants. Variables are updated in the same order as their declaration in the multistring (see Numerical methods, as it influences how ODEs are solved).\nAs it is only a parser and not a solver, some limitations exist:\n\nSimple equations must hold only the name of the variable on the left sign of the equation. Variable definitions such as r + v = noise are forbidden, as it would be impossible to guess which variable should be updated.\nODEs are more free regarding the left side, but only one variable should hold the gradient: the one which will be updated. The following definitions are equivalent and will lead to the same C++ code:\n\ntau * dv/dt  = baseline - v\n\ntau * dv/dt  + v = baseline\n\ntau * dv/dt  + v -  baseline = 0\n\ndv/dt  = (baseline - v) / tau\nIn practice, ODEs are transformed using Sympy into the last form (only the gradient stays on the left) and numerized using the chosen numerical method (see Numerical methods).\n\n\nLocality and type\nLike the parameters, variables defined using the ann.Variable() class also accept the local, global and semiglobal attributes to define the locality of the variable, as well as the int or bool flags for their type.\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.1,\n    ),\n    equations = [\n        ann.Variable('noise += Uniform(-0.1, 0.1)', locality='global'),\n        ann.Variable('tau * dv/dt  + v = baseline + noise'),\n        ann.Variable('r = pos(v)'),\n    ]\nHere, there will be only one value of noise for the whole population.\nInitial value\nThe initial value of the variable (before the first simulation step starts) is 0.0 by default (or 0/bool, depending on the type). It can also be specified using the init attribute:\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.1,\n        init_v = -0.1,\n    ),\n    equations = [\n        ann.Variable('noise += Uniform(-0.1, 0.1)', init=0.1),\n        ann.Variable('tau * dv/dt  + v = baseline + noise', init='init_v'),\n        ann.Variable('r = pos(v)', init=ann.Uniform(0.0, 1.0)),\n    ]\n)\nAcceptables values are:\n\na float/int/bool matching the type of the variable, which will be the same for all neurons/synapses.\na RandomDistribution object (see Random Distributions), allowing to randomly initialize the variable fo each neuron/variable.\nthe name of a parameter of the same neuron model, or of a constant.\n\nWhen using a random variable, the size of the array will be sampled after creation of the object using its size (number of neurons/synapses) and the locality (a global variable only need one value).\nThe initial value can be changed after the Population or Projection objects are created (see Populations).\nMin and Max values of a variable\nUpper- and lower-bounds can be set using the min and max attributes:\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.1,\n        init_v = -0.1,\n    ),\n    equations = [\n        ann.Variable('noise += Uniform(-0.1, 0.1)', init=0.1),\n        ann.Variable('tau * dv/dt  + v = baseline + noise', init='init_v', min=0.0, max=10.0),\n        ann.Variable('r = v', init=ann.Uniform(0.0, 1.0)),\n    ]\n)\nAt each step of the simulation, after the update rule is calculated for v, the new value will be compared to the min and max value, and clamped if necessary.\nLike init, min and max can be either single values, constants or parameter names.\nNumerical method\nThe numerization method for a single ODEs can be explicitely set by specifying a flag:\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.1,\n        init_v = -0.1,\n    ),\n    equations = [\n        ann.Variable('noise += Uniform(-0.1, 0.1)', init=0.1),\n        ann.Variable('tau * dv/dt  + v = baseline + noise', init='init_v', min=0.0, max=10.0, method='exponential'),\n        ann.Variable('r = v', init=ann.Uniform(0.0, 1.0)),\n    ]\n)\nThe available numerical methods are described in Numerical methods.\n\n\n\n\nConstants can be created by the user and used inside any equation. They must define an unique name and a floating point value.\nConstants can either be declared at the global level:\ntau = ann.Constant('tau', 10.0)\n\nneuron = ann.Neuron(\n    equations = \"tau * dr/dt + r = sum(exc)\"\n)\nor from a Network instance:\nnet = ann.Network()\ntau = net.constant('tau', 10.0)\nneuron = ann.Neuron(\n    equations = \"tau * dr/dt + r = sum(exc)\"\n)\nNeuron or Synapse models does not have to define the parameter tau to use it: it is available everywhere. If the Neuron/Synapse redefines a parameter called tau, the constant is not visible anymore to that object.\nConstants can be manipulated as normal floats to define complex values:\ntau = ann.Constant('tau', 20)\nfactor = ann.Constant('factor', 0.1)\nreal_tau = ann.Constant('real_tau', tau*factor)\n\nneuron = ann.Neuron(\n    equations = [\n        'real_tau * dr/dt + r = 1.0'\n    ]\n)\nNote that changing the value of a global constant impacts all networks using them. When the constant was created at the network level, a change of its value only impacts this network.\nChanging the value of a constant can only be done through the set() method:\ntau = ann.Constant('tau', 20)\ntau.set(10.0)\nor:\ntau = net.constant('tau', 10.0)\ntau.set(10.0)\n\n\n\nThe mathematical parser relies heavily on the one provided by SymPy.\n\n\nAll parameters and variables use implicitly the floating-point double precision, except when stated otherwise with the int or bool keywords. You can use numerical constants within the equation, noting that they will be automatically converted to this precision:\ntau * dv / dt  = 1 / pos(v) + 1\nThe constant \\pi is available under the literal form pi.\n\n\n\n\nAdditions (+), substractions (-), multiplications (*), divisions (/) and power functions (^) are of course allowed.\nTemporal gradients are allowed only for the variable currently described. They take the form:\n\ndv / dt  = A\nwith a d preceding the variable’s name and terminated by /dt (with or without spaces). Gradients must be on the left side of the equation.\n\nTo update the value of a variable at each time step, the operators =, +=, -=, *=, and /= are allowed.\n\n\n\n\nAny parameter or variable defined in the same Neuron/Synapse can be used inside an equation. User-defined constants can also be used. Additionally, the following variables are pre-defined:\n\ndt : the discretization time step for the simulation. Using this variable, you can define the numerical method by yourself. For example:\n\ntau * dv / dt  + v = baseline\nwith backward Euler would be equivalent to:\nv += dt/tau * (baseline - v)\n\nt : the time in milliseconds elapsed since the creation of the network. This allows for example to generate oscillating variables:\n\nf = 10.0 # Frequency of 10 Hz\nphi = pi/4 # Phase\nts = t / 1000.0 # ts is in seconds\nr = 10.0 * (sin(2*pi*f*ts + phi) + 1.0)\n\n\n\nSeveral random generators are available and can be used within an equation, for example:\n\nUniform(min, max) generates random numbers from a uniform distribution in the range [\\text{min}, \\text{max}].\nNormal(mu, sigma) generates random numbers from a normal distribution with mean mu and standard deviation sigma.\n\nSee the manual and the reference for more details on the availab le distributions.\n\n\n\nMost mathematical functions of the cmath library are understood by the parser, for example:\ncos, sin, tan, acos, asin, atan, exp, abs, fabs, sqrt, log, ln\nThe positive and negative parts of a term are also defined, with short and long versions:\nr = pos(mp)\nr = positive(mp)\nr = neg(mp)\nr = negative(mp)\nA piecewise linear function is also provided (linear when x is between a and b, saturated at a or b otherwise):\nr = clip(x, a, b)\n\n\n\n\n\n\nNote\n\n\n\nIn such simple cases, it might be more readable to use the min/max attributes:\nneuron = ann.Neuron(\n    equations = [\n        ann.Variable('r = pos(v)'),\n        # is equivalent to:\n        ann.Variable('r = v', min=0.0),\n    ]\n)\n\n\nFor integer variables, the modulo operator is defined:\nx += 1 : int\ny = modulo(x, 10)\nWhen using the power function (r = x^2 or r = pow(x, 2)), the cmath pow(double, int) method is used. For small exponents (quadratic or cubic functions), it can be extremely slow, compared to r = x*x or r = x*x*x. Unfortunately, Sympy transforms automatically r = x*x into r = pow(x, 2). We therefore advise to use the built-in power(double, int) function instead:\nr = power(x, 3)\nThese functions must be followed by a set of matching brackets:\ntau * dmp / dt + mp = exp( - cos(2*pi*f*t + pi/4 ) + 1)\n\n\n\nPython-style\nIt is possible to use Python-style conditional statements as the right term of an equation or ODE. They follow the form:\nif condition : statement1 else : statement2\nFor example, to define a piecewise linear function, you can nest different conditionals:\nr = if mp &lt; 1. :\n        if mp &gt; 0.:\n            mp\n        else:\n            0.\n    else:\n        1.\nwhich is equivalent to:\nr = clip(mp, 0.0, 1.0)\nThe condition can use the following vocabulary:\nTrue, False, and, or, not, is, is not, ==, !=, &gt;, &lt;, &gt;=, &lt;=\n\n\n\n\n\n\nNote\n\n\n\nThe and, or and not logical operators must be used with parentheses around their terms. Example:\nvar = if (mp &gt; 0) and ( (noise &lt; 0.1) or (not(condition)) ):\n            1.0\n        else:\n            0.0\nis is equivalent to ==, is not is equivalent to !=.\n\n\nWhen a conditional statement is split over multiple lines, the flags must be set after the last line:\nrate = if mp &lt; 1.0 :\n          if mp &lt; 0.0 :\n              0.0\n          else:\n              mp\n       else:\n          1.0 : init = 0.6\nAn if a: b else: c statement must be exactly the right term of an equation. It is for example NOT possible to write:\nr = 1.0 + (if mp&gt; 0.0: mp else: 0.0) + b\nTernary operator\nThe ternary operator ite(cond, then, else) (ite stands for if-then-else) is available to ease the combination of conditionals with other terms:\nr = ite(mp &gt; 0.0, mp, 0.0)\n# is exactly the same as:\nr = if mp &gt; 0.0: mp else: 0.0\nThe advantage is that the conditional term is not restricted to the right term of the equation, and can be used multiple times:\nr = ite(mp &gt; 0.0, ite(mp &lt; 1.0, mp, 1.0), 0.0) + ite(stimulated, 1.0, 0.0)\n\n\n\nTo simplify the writing of equations, custom functions can be defined either globally (usable by all neurons and synapses) or locally (only for the particular type of neuron/synapse) using the same mathematical parser.\nGlobal functions can be defined using the add_function() method:\nann.add_function('sigmoid(x) = 1.0 / (1.0 + exp(-x))')\nWith this declaration, sigmoid() can be used in the declaration of any variable, for example:\nneuron = ann.Neuron(\n    equations = [\n        'r = sigmoid(sum(exc))'\n    ]\n)\nFunctions must be one-liners, i.e. they should have only one return value. They can use as many arguments as needed, but are totally unaware of the context: all the needed information should be passed as an argument (except constants which are visible to the function).\nThe types of the arguments (including the return value) are by default floating-point. If other types should be used, they should be specified at the end of the definition, after the : sign, with the type of the return value first, followed by the type of all arguments separated by commas:\nann.add_function('conditional_increment(c, v, t) = if v &gt; t : c + 1 else: c : int, int, float, float')\nLocal functions are specific to a Neuron or Synapse class and can only be used within this context (if they have the same name as global variables, they will override them). They can be passed as a multi-line argument to the constructor of a neuron or synapse (see later):\nfunctions == \"\"\"\n    sigmoid(x) = 1.0 / (1.0 + exp(-x))\n    conditional_increment(c, v, t) = if v &gt; t : c + 1 else: c : int, int, float, float\n\"\"\"\n\n\n\n\nPrior to version 5.0, the default interface was using multiline strings to define both parameters and variables. This interface still works, but might become deprecated in a future release.\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = \"\"\"\n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt  + v = baseline + sum(exc)\n        r = pos(v)\n    \"\"\"\n)\nWhen using strings, parameters and variables are by default local, i.e. unique to each neuron in a population or synapse in a projection. With parameters, this is the opposite to the new default behavior with dictionaries. To make the parameters global, you need to pass a flag\n\nIf population is passed after a colon ‘:’ in a neuron, the parameter will be common to all neurons of the population.\n\nparameters = \"\"\"\n    tau = 10.0                   # equivalent to local\n    baseline = 0.0 : population  # equivalent to global\n\"\"\"\n\nIf the postsynaptic flag is passed after a colon ‘:’ in a synapse, the parameter will be common to all synapses of a post-synaptic neuron, but can differ from one post-synaptic neuron to another. If the projection flag is passed, the parameter will be common to all synapses of a projection.\n\nparameters = \"\"\"\n    wmin = 0.0                # equivalent to local\n    wmax = 10.0 : projection  # equivalent to global\n    eta = 0.5 : postsynaptic  # equivalent to semiglobal\n\"\"\"\nVariables are also local by default, and can be modified by the same flags. The init, min, max flags and the numerical method (without method=) can be passed after the colon like in a regular ann.Variable().\nequations = \"\"\"\n    tau * dv/dt  + v = baseline + sum(exc) # init=-1.0, max=10.0, exponential\n    r = v : min=0.0\n\"\"\"",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Equation parser"
    ]
  },
  {
    "objectID": "manual/Parser.html#parameters",
    "href": "manual/Parser.html#parameters",
    "title": "Equation parser",
    "section": "",
    "text": "Parameters are provided to Neuron or Synapse models using dictionaries:\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.1,\n    )\n)\nA multiline string can also be passed (see below).\nAs a neuron/synapse type is likely to be reused in different populations/projections, it is good practice to set reasonable initial values in the neuron/synapse type, and eventually adapt them to the corresponding populations/projections later on.\nLocal vs. global parameters\nNeural parameters can be either local (each neuron has a different value of the parameter) or global (the value is the same for all neurons of a population - but can be different in two populations using the same neuron type).\nThe default locality of a parameter is global. The Parameter dataclass allows to tell ANNarchy that the parameter has to be local, i.e. it should take one value per neuron in the population.\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = ann.Parameter(10.0),\n        baseline = ann.Parameter(-0.1),\n    )\n)\nann.Parameter() also allows to define global parameters, but it is longer:\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = ann.Parameter(10.0, locality='global'), # or just: tau = 10.0\n        baseline = ann.Parameter(-0.1, locality='global'), # or just: baseline = -0.1\n    )\n)\nFor synapses, there are three different localities (see Locality):\n\n'local': one value per synapse in the projection (e.g. the synaptic efficiency).\n'semiglobal': one value per post-synaptic neuron in the projection.\n'global': one value per projection.\n\nThese strings can be passed to the parameter dataclass. 'global' is still the default.\nType of the parameter\nParameters have floating-point precision by default. If you want to force the parameter to be an integer or boolean, you can set the type argument of parameter:\nneuron = ann.Neuron(\n    parameters = dict(\n        number_spikes = ann.Parameter(0, type=int),\n        activated = ann.Parameter(False, locality='global', type=bool),\n    )\n)\n\"\"\"\nConstants\nAlternatively, it is possible to use external constants in the parameter definition by using their name (see later):\ntau_exc = ann.Constant('tau_exc', 10.0)\n\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 'tau_exc'\n    ),\n)\nThe advantage of this method is that if a parameter value is \"shared\" across several neuron/synapse types, you only need to change the value once, instead of in each neuron/synapse definition.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Equation parser"
    ]
  },
  {
    "objectID": "manual/Parser.html#variables",
    "href": "manual/Parser.html#variables",
    "title": "Equation parser",
    "section": "",
    "text": "Time-varying variables are defined using a list of strings (or ann.Variable instances):\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.1,\n    ),\n    equations = [\n        'noise += Uniform(-0.1, 0.1)',\n        'tau * dv/dt  + v = baseline + noise',\n        'r = pos(v)',\n    ]\nThe evolution of each variable with time can be described through a simple equation or an ordinary differential equation (ODE). ANNarchy provides a simple parser for mathematical expressions, whose role is to translate a high-level description of the equation into an optimized C++ code snippet.\nThe equation for one variable can depend on parameters, other variables (even when declared later) or constants. Variables are updated in the same order as their declaration in the multistring (see Numerical methods, as it influences how ODEs are solved).\nAs it is only a parser and not a solver, some limitations exist:\n\nSimple equations must hold only the name of the variable on the left sign of the equation. Variable definitions such as r + v = noise are forbidden, as it would be impossible to guess which variable should be updated.\nODEs are more free regarding the left side, but only one variable should hold the gradient: the one which will be updated. The following definitions are equivalent and will lead to the same C++ code:\n\ntau * dv/dt  = baseline - v\n\ntau * dv/dt  + v = baseline\n\ntau * dv/dt  + v -  baseline = 0\n\ndv/dt  = (baseline - v) / tau\nIn practice, ODEs are transformed using Sympy into the last form (only the gradient stays on the left) and numerized using the chosen numerical method (see Numerical methods).\n\n\nLocality and type\nLike the parameters, variables defined using the ann.Variable() class also accept the local, global and semiglobal attributes to define the locality of the variable, as well as the int or bool flags for their type.\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.1,\n    ),\n    equations = [\n        ann.Variable('noise += Uniform(-0.1, 0.1)', locality='global'),\n        ann.Variable('tau * dv/dt  + v = baseline + noise'),\n        ann.Variable('r = pos(v)'),\n    ]\nHere, there will be only one value of noise for the whole population.\nInitial value\nThe initial value of the variable (before the first simulation step starts) is 0.0 by default (or 0/bool, depending on the type). It can also be specified using the init attribute:\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.1,\n        init_v = -0.1,\n    ),\n    equations = [\n        ann.Variable('noise += Uniform(-0.1, 0.1)', init=0.1),\n        ann.Variable('tau * dv/dt  + v = baseline + noise', init='init_v'),\n        ann.Variable('r = pos(v)', init=ann.Uniform(0.0, 1.0)),\n    ]\n)\nAcceptables values are:\n\na float/int/bool matching the type of the variable, which will be the same for all neurons/synapses.\na RandomDistribution object (see Random Distributions), allowing to randomly initialize the variable fo each neuron/variable.\nthe name of a parameter of the same neuron model, or of a constant.\n\nWhen using a random variable, the size of the array will be sampled after creation of the object using its size (number of neurons/synapses) and the locality (a global variable only need one value).\nThe initial value can be changed after the Population or Projection objects are created (see Populations).\nMin and Max values of a variable\nUpper- and lower-bounds can be set using the min and max attributes:\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.1,\n        init_v = -0.1,\n    ),\n    equations = [\n        ann.Variable('noise += Uniform(-0.1, 0.1)', init=0.1),\n        ann.Variable('tau * dv/dt  + v = baseline + noise', init='init_v', min=0.0, max=10.0),\n        ann.Variable('r = v', init=ann.Uniform(0.0, 1.0)),\n    ]\n)\nAt each step of the simulation, after the update rule is calculated for v, the new value will be compared to the min and max value, and clamped if necessary.\nLike init, min and max can be either single values, constants or parameter names.\nNumerical method\nThe numerization method for a single ODEs can be explicitely set by specifying a flag:\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.1,\n        init_v = -0.1,\n    ),\n    equations = [\n        ann.Variable('noise += Uniform(-0.1, 0.1)', init=0.1),\n        ann.Variable('tau * dv/dt  + v = baseline + noise', init='init_v', min=0.0, max=10.0, method='exponential'),\n        ann.Variable('r = v', init=ann.Uniform(0.0, 1.0)),\n    ]\n)\nThe available numerical methods are described in Numerical methods.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Equation parser"
    ]
  },
  {
    "objectID": "manual/Parser.html#constants",
    "href": "manual/Parser.html#constants",
    "title": "Equation parser",
    "section": "",
    "text": "Constants can be created by the user and used inside any equation. They must define an unique name and a floating point value.\nConstants can either be declared at the global level:\ntau = ann.Constant('tau', 10.0)\n\nneuron = ann.Neuron(\n    equations = \"tau * dr/dt + r = sum(exc)\"\n)\nor from a Network instance:\nnet = ann.Network()\ntau = net.constant('tau', 10.0)\nneuron = ann.Neuron(\n    equations = \"tau * dr/dt + r = sum(exc)\"\n)\nNeuron or Synapse models does not have to define the parameter tau to use it: it is available everywhere. If the Neuron/Synapse redefines a parameter called tau, the constant is not visible anymore to that object.\nConstants can be manipulated as normal floats to define complex values:\ntau = ann.Constant('tau', 20)\nfactor = ann.Constant('factor', 0.1)\nreal_tau = ann.Constant('real_tau', tau*factor)\n\nneuron = ann.Neuron(\n    equations = [\n        'real_tau * dr/dt + r = 1.0'\n    ]\n)\nNote that changing the value of a global constant impacts all networks using them. When the constant was created at the network level, a change of its value only impacts this network.\nChanging the value of a constant can only be done through the set() method:\ntau = ann.Constant('tau', 20)\ntau.set(10.0)\nor:\ntau = net.constant('tau', 10.0)\ntau.set(10.0)",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Equation parser"
    ]
  },
  {
    "objectID": "manual/Parser.html#allowed-vocabulary",
    "href": "manual/Parser.html#allowed-vocabulary",
    "title": "Equation parser",
    "section": "",
    "text": "The mathematical parser relies heavily on the one provided by SymPy.\n\n\nAll parameters and variables use implicitly the floating-point double precision, except when stated otherwise with the int or bool keywords. You can use numerical constants within the equation, noting that they will be automatically converted to this precision:\ntau * dv / dt  = 1 / pos(v) + 1\nThe constant \\pi is available under the literal form pi.\n\n\n\n\nAdditions (+), substractions (-), multiplications (*), divisions (/) and power functions (^) are of course allowed.\nTemporal gradients are allowed only for the variable currently described. They take the form:\n\ndv / dt  = A\nwith a d preceding the variable’s name and terminated by /dt (with or without spaces). Gradients must be on the left side of the equation.\n\nTo update the value of a variable at each time step, the operators =, +=, -=, *=, and /= are allowed.\n\n\n\n\nAny parameter or variable defined in the same Neuron/Synapse can be used inside an equation. User-defined constants can also be used. Additionally, the following variables are pre-defined:\n\ndt : the discretization time step for the simulation. Using this variable, you can define the numerical method by yourself. For example:\n\ntau * dv / dt  + v = baseline\nwith backward Euler would be equivalent to:\nv += dt/tau * (baseline - v)\n\nt : the time in milliseconds elapsed since the creation of the network. This allows for example to generate oscillating variables:\n\nf = 10.0 # Frequency of 10 Hz\nphi = pi/4 # Phase\nts = t / 1000.0 # ts is in seconds\nr = 10.0 * (sin(2*pi*f*ts + phi) + 1.0)\n\n\n\nSeveral random generators are available and can be used within an equation, for example:\n\nUniform(min, max) generates random numbers from a uniform distribution in the range [\\text{min}, \\text{max}].\nNormal(mu, sigma) generates random numbers from a normal distribution with mean mu and standard deviation sigma.\n\nSee the manual and the reference for more details on the availab le distributions.\n\n\n\nMost mathematical functions of the cmath library are understood by the parser, for example:\ncos, sin, tan, acos, asin, atan, exp, abs, fabs, sqrt, log, ln\nThe positive and negative parts of a term are also defined, with short and long versions:\nr = pos(mp)\nr = positive(mp)\nr = neg(mp)\nr = negative(mp)\nA piecewise linear function is also provided (linear when x is between a and b, saturated at a or b otherwise):\nr = clip(x, a, b)\n\n\n\n\n\n\nNote\n\n\n\nIn such simple cases, it might be more readable to use the min/max attributes:\nneuron = ann.Neuron(\n    equations = [\n        ann.Variable('r = pos(v)'),\n        # is equivalent to:\n        ann.Variable('r = v', min=0.0),\n    ]\n)\n\n\nFor integer variables, the modulo operator is defined:\nx += 1 : int\ny = modulo(x, 10)\nWhen using the power function (r = x^2 or r = pow(x, 2)), the cmath pow(double, int) method is used. For small exponents (quadratic or cubic functions), it can be extremely slow, compared to r = x*x or r = x*x*x. Unfortunately, Sympy transforms automatically r = x*x into r = pow(x, 2). We therefore advise to use the built-in power(double, int) function instead:\nr = power(x, 3)\nThese functions must be followed by a set of matching brackets:\ntau * dmp / dt + mp = exp( - cos(2*pi*f*t + pi/4 ) + 1)\n\n\n\nPython-style\nIt is possible to use Python-style conditional statements as the right term of an equation or ODE. They follow the form:\nif condition : statement1 else : statement2\nFor example, to define a piecewise linear function, you can nest different conditionals:\nr = if mp &lt; 1. :\n        if mp &gt; 0.:\n            mp\n        else:\n            0.\n    else:\n        1.\nwhich is equivalent to:\nr = clip(mp, 0.0, 1.0)\nThe condition can use the following vocabulary:\nTrue, False, and, or, not, is, is not, ==, !=, &gt;, &lt;, &gt;=, &lt;=\n\n\n\n\n\n\nNote\n\n\n\nThe and, or and not logical operators must be used with parentheses around their terms. Example:\nvar = if (mp &gt; 0) and ( (noise &lt; 0.1) or (not(condition)) ):\n            1.0\n        else:\n            0.0\nis is equivalent to ==, is not is equivalent to !=.\n\n\nWhen a conditional statement is split over multiple lines, the flags must be set after the last line:\nrate = if mp &lt; 1.0 :\n          if mp &lt; 0.0 :\n              0.0\n          else:\n              mp\n       else:\n          1.0 : init = 0.6\nAn if a: b else: c statement must be exactly the right term of an equation. It is for example NOT possible to write:\nr = 1.0 + (if mp&gt; 0.0: mp else: 0.0) + b\nTernary operator\nThe ternary operator ite(cond, then, else) (ite stands for if-then-else) is available to ease the combination of conditionals with other terms:\nr = ite(mp &gt; 0.0, mp, 0.0)\n# is exactly the same as:\nr = if mp &gt; 0.0: mp else: 0.0\nThe advantage is that the conditional term is not restricted to the right term of the equation, and can be used multiple times:\nr = ite(mp &gt; 0.0, ite(mp &lt; 1.0, mp, 1.0), 0.0) + ite(stimulated, 1.0, 0.0)\n\n\n\nTo simplify the writing of equations, custom functions can be defined either globally (usable by all neurons and synapses) or locally (only for the particular type of neuron/synapse) using the same mathematical parser.\nGlobal functions can be defined using the add_function() method:\nann.add_function('sigmoid(x) = 1.0 / (1.0 + exp(-x))')\nWith this declaration, sigmoid() can be used in the declaration of any variable, for example:\nneuron = ann.Neuron(\n    equations = [\n        'r = sigmoid(sum(exc))'\n    ]\n)\nFunctions must be one-liners, i.e. they should have only one return value. They can use as many arguments as needed, but are totally unaware of the context: all the needed information should be passed as an argument (except constants which are visible to the function).\nThe types of the arguments (including the return value) are by default floating-point. If other types should be used, they should be specified at the end of the definition, after the : sign, with the type of the return value first, followed by the type of all arguments separated by commas:\nann.add_function('conditional_increment(c, v, t) = if v &gt; t : c + 1 else: c : int, int, float, float')\nLocal functions are specific to a Neuron or Synapse class and can only be used within this context (if they have the same name as global variables, they will override them). They can be passed as a multi-line argument to the constructor of a neuron or synapse (see later):\nfunctions == \"\"\"\n    sigmoid(x) = 1.0 / (1.0 + exp(-x))\n    conditional_increment(c, v, t) = if v &gt; t : c + 1 else: c : int, int, float, float\n\"\"\"",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Equation parser"
    ]
  },
  {
    "objectID": "manual/Parser.html#previous-string-based-interface-5.0",
    "href": "manual/Parser.html#previous-string-based-interface-5.0",
    "title": "Equation parser",
    "section": "",
    "text": "Prior to version 5.0, the default interface was using multiline strings to define both parameters and variables. This interface still works, but might become deprecated in a future release.\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = \"\"\"\n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt  + v = baseline + sum(exc)\n        r = pos(v)\n    \"\"\"\n)\nWhen using strings, parameters and variables are by default local, i.e. unique to each neuron in a population or synapse in a projection. With parameters, this is the opposite to the new default behavior with dictionaries. To make the parameters global, you need to pass a flag\n\nIf population is passed after a colon ‘:’ in a neuron, the parameter will be common to all neurons of the population.\n\nparameters = \"\"\"\n    tau = 10.0                   # equivalent to local\n    baseline = 0.0 : population  # equivalent to global\n\"\"\"\n\nIf the postsynaptic flag is passed after a colon ‘:’ in a synapse, the parameter will be common to all synapses of a post-synaptic neuron, but can differ from one post-synaptic neuron to another. If the projection flag is passed, the parameter will be common to all synapses of a projection.\n\nparameters = \"\"\"\n    wmin = 0.0                # equivalent to local\n    wmax = 10.0 : projection  # equivalent to global\n    eta = 0.5 : postsynaptic  # equivalent to semiglobal\n\"\"\"\nVariables are also local by default, and can be modified by the same flags. The init, min, max flags and the numerical method (without method=) can be passed after the colon like in a regular ann.Variable().\nequations = \"\"\"\n    tau * dv/dt  + v = baseline + sum(exc) # init=-1.0, max=10.0, exponential\n    r = v : min=0.0\n\"\"\"",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Equation parser"
    ]
  },
  {
    "objectID": "manual/Network.html",
    "href": "manual/Network.html",
    "title": "Network",
    "section": "",
    "text": "The main object in ANNarchy is the Network instance, which contains all data structures apart from the neuron / synapse models and the functions:\nimport ANNarchy as ann\n\nneuron = ann.Neuron(\n    parameters = dict(tau = 10.0),\n    equations = [\n        'tau * dv/dt  + v =  sum(exc)',\n        'r = pos(v)'\n    ]\n)\n\nsynapse = ann.Synapse(\n    parameters = dict(tau = 5000.0),\n    equations = 'tau * dw/dt = pre.r * post.r ',\n)\n\n# Create the empty network\nnet = Network()\n\n# Create two populations\npop1 = net.create(10, neuron)\npop2 = net.create(10, neuron)\n\n# Connect the two populations\nproj = net.connect(pop1, pop2, 'exc', synapse)\n\n# Monitor the second population\nm = net.monitor(pop2, 'r')\nNetwork.create() and Network.connect() are the main access points to create populations and projections. However, if you lose the references to pop1, pop2 and proj (for example if you create them inside a method but do not return them), it is difficult to access them.\nOne option is to iterate over the lists of populations and projections stored in the network, but you have to know what you are looking for:\nfor pop in net.get_populations():\n    print(pop.r)\n\nfor proj in net.get_projections():\n    print(proj.w)\nIt is also possible to provide a unique name to each population and projection at creation time, so they can be easily retrieved:\ndef create_network():\n    net = ann.Network()\n    pop1 = net.create(10, neuron, name='pop1')\n    pop2 = net.create(10, neuron, name='pop2')\n    proj = net.connect(pop1, pop2, 'exc', synapse, name='projection1')\n    m = net.monitor(pop2, 'r', name='monitor')\n    return net\n\nnet = create_network()\npop1 = net.get_population('pop1')\npop2 = net.get_population('pop2')\nproj = net.get_projection('projection1')\nm = net.get_monitor('monitor')\nAnother safer option is to create your own class inheriting from ann.Network and store all populations and projections as an attribute:\nclass SimpleNetwork (ann.Network):\n    def __init__(self, N)\n        self.pop1 = self.create(N, neuron)\n        self.pop2 = self.create(N, neuron)\n        self.proj = self.connect(self.pop1, self.pop2, 'exc', synapse)\n        self.m    = self.monitor(self.pop2, 'r')\n\nnet = SimpleNetwork(10)\nprint(net.pop1.r)\nYou do not need to explictly call the constructor of Network (Network.__init__(self)), it is done automatically. Creating a subclass of Network furthermore allows to use the parallel_run() method if the whole construction of the network is done through the constructor.\nApart from that, the two approaches are equivalent, pick the one you prefer. Subclasses are easier to re-use, especially across files.\n\n\n\nOnce all the relevant information has been defined, one needs to actually compile the network, by calling the Network.compile() method:\nnet.compile()\nThe optimized C++ code will be generated, compiled, the underlying objects created and made available to the Python interface.\nYou can specify several arguments to compile(), including:\n\ncompiler: to select which C++ compiler will be used.\ncompiler_flags: to select which flags are passed to the compiler.\ndirectory: absolute/relative path to the directory where files will be generated and compiled (default: annarchy/).\n\n\n\nANNarchy requires a C++ compiler. On GNU/Linux, the default choice is g++, while on MacOS it is clang++. You can change the compiler (and its flags) to use either during the call to net.compile() in your script:\nnet.compile(compiler=\"clang++\", compiler_flags=\"-march=native -O3\")\nor globally by modifying the configuration file located at ~/.config/ANNarchy/annarchy.json:\n{\n    \"openmp\": {\n        \"compiler\": \"clang++\",\n        \"flags\": \"-march=native -O3\"\n    }\n}\nBe careful with the flags: for example, the optimization level -O3 does not obligatorily produce faster code. But this is the case for most models, therefore it is the default in the ANNarchy 4.7.x releases.\nEven more caution is required when using the -ffast-math flag. It can increase the performance, in particular in combination with SIMD. However, the application of -ffast-math enables a set of optimizations which might violate IEEE 754 compliance (which might be okay in many cases, but it is important that the user verifies the result). For more details, see the g++ documentation: https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html\n\n\n\n\n\n\nNote\n\n\n\nIn rare cases, it may occur that the CPU architecture is not detectable for the used g++ compiler (e.g. Intel’s Tigerlake and g++ &lt;= 9.4). This will result in a compiler error which can be fixed by removing the ‘-march=native’ flag. To get access to AVX-512 SIMD instructions, you need to add -mavx512f instead, as well as -ftree-vectorize if -O3 is not already used.\n\n\n\n\n\nWhen calling compile(), the subfolder annarchy/ (or whatever is defined by `directory) will be created, and the generated code will be compiled. The first compilation may last a couple of seconds, but further runs of the script are much faster. If no modification to the network has been made except for parameter values, it will not be recompiled, sparing us this overhead.\nANNarchy tracks the changes in the script and re-generates the corresponding code. In some cases (a new version of ANNarchy has been installed, bugs), it may be necessary to perform a fresh compilation of the network. You can either delete the annarchy/ subfolder and restart the script:\nrm -rf annarchy/\npython MyNetwork.py\npass the --clean flag to the script:\npython MyNetwork.py --clean \nor tell compile() to start fresh:\nnet.compile(clean=True)\n\n\n\n\nAfter the network is compiled, the simulation can be run for the specified duration (in milliseconds) through the Network.simulate() method:\nnet.simulate(1000.0) # Simulate for 1 second\nThe provided duration should be a multiple of dt. If not, the number of simulation steps performed will be approximated.\nIn some cases, you may want to perform only one step of the simulation, instead of specifing the duration. The Network.step() method can then be used.\nnet.step() # Simulate for 1 step\n\n\n\nAn important value for the simulation is the discretization step dt. Its default value is 1 ms, which is usually fine for rate-coded networks, but may be too high for spiking networks, as the equations are stiffer. Taken too high, it can lead to high numerical errors. Too low, and the simulation will take an unnecessary amount of time.\nTo set the discretization step, just pass the desired value to the constructor of Network:\nnet = ann.Network(dt=0.1)\nIt can also be set using the config() method, before projections are created:\nnet.config(dt=0.1)\nHowever, changing its value after calling compile() will not have any effect.\nYou can always access the current value of dt with the attribute net.dt.\nIf you create a subclass of Network, you can also provide dt to its constructor, even if your network does not catch it.\nclass SimpleNetwork (ann.Network):\n    def __init__(self, N)\n        self.pop1 = net.create(N, neuron)\n        self.pop2 = net.create(N, neuron)\n        self.proj = net.connect(self.pop1, self.pop2, 'exc', synapse)\n\nnet = SimpleNetwork(N=10, dt=0.1)\n\n\n\nBy default, the random number generators (RNG) are seeded with secrets.randbits(32), so each simulation will be different from run to run. If you want to have deterministic simulations, you need to provide a fixed seed to the constructor of Network:\nnet = ann.Network(dt=0.1, seed=42)\nIf you define a subclass of Network, pass the seed without further processing, like for dt.\nclass SimpleNetwork (ann.Network):\n    def __init__(self, N)\n        self.pop1 = net.create(N, neuron)\n        self.pop2 = net.create(N, neuron)\n        self.proj = net.connect(self.pop1, self.pop2, 'exc', synapse)\n\nnet = SimpleNetwork(N=10, dt=0.1, seed=42)\nNote that this also sets the seed of the old RNG of numpy, which is used to initialize values produced by np.random.*.\nIf you use the new default RNG of numpy (rng = np.random.default_rng()) or ANNarchy’s random distributions (see Random Distributions), you will have to seed it yourself. The seed of network is accessible through the attribute net.seed.\nrng = np.random.default_rng(seed=net.seed)\n\npop.r = ann.Uniform(min=0.0, max=1.0, rng=rng)\n\n\n\n\n\n\nNote\n\n\n\nUsing the same seed with the OpenMP and CUDA backends will not lead to the same sequences of numbers!\n\n\n\n\n\nIn some cases, it is desired to stop the simulation whenever a criterion is fulfilled (for example, a neural integrator exceeds a certain threshold), not after a fixed amount of time.\nThere is the possibility to define a stop_condition when creating a Population:\npop1 = net.create( ... , stop_condition = \"r &gt; 1.0\")\nWhen calling the simulate_until() method instead of simulate():\nt = net.simulate_until(max_duration=1000.0, populations=pop1)\nthe simulation will be stopped whenever the stop_condition of pop1 is met, i.e. when the firing rate of any neuron of pop1 is above 1.0. If the condition is never met, the simulation will last maximally max_duration. The methods returns the effective duration of the simulation (to compute reaction times, for example).\nThe stop_condition can use any logical operation on the parameters and variables of the neuron associated to the population:\npop1 = net.create( ... , stop_condition = \"(r &gt; 1.0) and (mp &lt; 2.0)\")\nBy default, the simulation stops when at least one neuron in the population fulfills the criterion. If you want to stop the simulation when all neurons fulfill the condition, you can use the flag all after the condition:\npop1 = net.create( ... , stop_condition = \"r &gt; 1.0 : all\")\nThe flag any is the default behavior and can be omitted.\nThe stop criterion can depend on several populations, by providing a list of populations to the populations argument instead of a single population:\nt = net.simulate_until(max_duration=1000.0, populations=[pop1, pop2])\nThe simulation will then stop when the criterion is met in both populations at the same time. If you want that the simulation stops when at least one population meets its criterion, you can specify the operator argument:\nt = net.simulate_until(max_duration=1000.0, populations=[pop1, pop2], operator='or')\nThe default value of operator is a 'and' function between the populations’ criteria.\n\n\n\n\n\n\nWarning\n\n\n\nGlobal operations (min, max, mean) are not possible inside the stop_condition. If you need them, store them in a variable in the equations argument of the neuron and use it as the condition:\nequations = [\n    'r = ...',\n    'max_r = max(r)',\n]\n\n\n\n\n\nIn most cases, your simulation will be decomposed into a series of fixed-duration trials, where you basically set inputs at the beginning of the trial, run the simulation for a fixed duration, and possibly read out results at the end:\n# Iterate over 100 trials\nresult = []\nfor trial in range(100):\n    # Set inputs to the network\n    pop.I = ann.Uniform(0.0, 1.0)\n    # Simulate for 1 second\n    net.simulate(1000.)\n    # Save the output\n    result.append(pop.r)\nFor convenience, we provide the decorator every, which allows to register a python method and call it automatically during the simulation with a fixed period:\nresult = []\n\n@ann.every(period=1000.)\ndef set inputs(n):\n    # Set inputs to the network\n    pop.I = ann.Uniform(0.0, 1.0)\n    # Save the output of the previous step\n    if n &gt; 0:\n        result.append(pop.r)\n\nnet.simulate(100 * 1000.)\nIn this example, set_inputs() will be executed just before the steps corresponding to times t = 0., 1000., 2000., and so on until t = 100000.\nThe method can have any name, but must accept only one argument, the integer n which will be incremented at each call of the method (i.e. it will take the values 0, 1, 2 until 99). This can for example be used to access data in a numpy array:\nimages = np.random.random((100, 640, 480))\n\n@ann.every(period=1000.)\ndef set inputs(n):\n    # Set inputs to the network\n    pop.I = images[n, :, :]\n\nnet.simulate(100 * 1000.)\nOne can define several methods that will be called in the order of their definition:\n@ann.every(period=1000.)\ndef set inputs(n):\n    pop.I = 1.0\n\n@ann.every(period=1000.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this example, set_inputs() will be called first, followed by reset_inputs, so pop.I will finally be 0.0. The decorator every accepts an argument offset defining a delay within the period to call the method:\n@ann.every(period=1000.)\ndef set inputs(n):\n    pop.I = 1.0\n\n@ann.every(period=1000., offset=500.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this case, set_inputs() will be called at times 0, 1000, 2000... while reset_inputs() will be called at times 500, 1500, 2500..., allowing to structure a trial more effectively. The offset can be set negative, in which case it will be relative to the end of the trial:\n@every(period=1000., offset=-100.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this example, the method will be called at times 900, 1900, 2900 and so on. The offset value can not be longer than the period, by definition. If you try to do so, a modulo operation will anyway be applied (i.e. an offset of 1500 with a period of 1000 becomes 500).\nFinally, the wait argument allows to delay the first call to the method from a fixed interval:\n@every(period=1000., wait=5000.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this case, the method will be called at times 5000, 6000 and so on.\nBetween two calls to simulate(), the callbacks can be disabled or re-enabled using the following methods:\n@every(period=1000.)\ndef reset inputs(n):\n    pop.I = 0.0\n\n# Simulate with callbacks\nnet.simulate(10000.)\n\n# Disable callbacks\nnet.disable_callbacks()\n\n# Simulate without callbacks\nnet.simulate(10000.)\n\n# Re-enable callbacks\nnet.enable_callbacks()\n\n# Simulate with callbacks\nnet.simulate(10000.)\nNote that the period is always relative to the time when simulate() is called, so if no offset is defined, the callbacks will be called before the first step of a simulation, no matter how long the previous simulation lasted. In the current state, it is not possible yet to enable/disable callbacks selectively, it is all or none.\nCallbacks can only be used with simulate(), not with step() or simulate_until().\n\n\n\nThe default paradigm for an ANNarchy simulation is through openMP, which distributes automatically the computations over the available CPU cores.\nBy default, ANNarchy will use a single thread for your simulation. Automatically using all possible cores would not be optimal: small networks in particular tend to run faster with a smaller amount of cores. For this reason, the OMP_NUM_THREADS environment variable has no effect in ANNarchy.\nYou can control the number of cores by passing the -j flag to the Python script:\npython NeuralField.py -j2\nIt is the responsability of the user to find out which number of cores is optimal for his network, by comparing simulation times. When this optimal number is found, it can be hard-coded in the script by setting the num_threads argument to Network.config():\nnet.config(num_threads=2)\n\n\n\nTo run your network on GPUs, you need to declare to ANNarchy that you want to use CUDA. One way to do so is to pass the --gpu flag to the command line:\npython NeuralField.py --gpu\nYou can also set the paradigm argument of Network.config() to make it permanent:\nnet.config(paradigm=\"cuda\")\nIf there are multiple GPUs on your machine, you can select the ID of the device by specifying it to the --gpu flag on the command line:\npython NeuralField.py --gpu=2\nAlternatively, you can also pass the cuda_config dictionary argument to Network.compile():\nnet.compile(cuda_config={'device': 2})\nThe default GPU is defined in the configuration file ~/.config/ANNarchy/annarchy.json (0 unless you modify it).\n{\n    \"cuda\": {\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\n\nAs the current implementation is a development version, some of the features provided by ANNarchy are not supported yet with CUDA:\n\nweight sharing (convolutions),\nnon-uniform synaptic delays,\nstructural plasticity,\nspiking neurons: a) with mean firing rate and b) continous integration of inputs,\nSpikeSourceArray.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Network"
    ]
  },
  {
    "objectID": "manual/Network.html#network-class",
    "href": "manual/Network.html#network-class",
    "title": "Network",
    "section": "",
    "text": "The main object in ANNarchy is the Network instance, which contains all data structures apart from the neuron / synapse models and the functions:\nimport ANNarchy as ann\n\nneuron = ann.Neuron(\n    parameters = dict(tau = 10.0),\n    equations = [\n        'tau * dv/dt  + v =  sum(exc)',\n        'r = pos(v)'\n    ]\n)\n\nsynapse = ann.Synapse(\n    parameters = dict(tau = 5000.0),\n    equations = 'tau * dw/dt = pre.r * post.r ',\n)\n\n# Create the empty network\nnet = Network()\n\n# Create two populations\npop1 = net.create(10, neuron)\npop2 = net.create(10, neuron)\n\n# Connect the two populations\nproj = net.connect(pop1, pop2, 'exc', synapse)\n\n# Monitor the second population\nm = net.monitor(pop2, 'r')\nNetwork.create() and Network.connect() are the main access points to create populations and projections. However, if you lose the references to pop1, pop2 and proj (for example if you create them inside a method but do not return them), it is difficult to access them.\nOne option is to iterate over the lists of populations and projections stored in the network, but you have to know what you are looking for:\nfor pop in net.get_populations():\n    print(pop.r)\n\nfor proj in net.get_projections():\n    print(proj.w)\nIt is also possible to provide a unique name to each population and projection at creation time, so they can be easily retrieved:\ndef create_network():\n    net = ann.Network()\n    pop1 = net.create(10, neuron, name='pop1')\n    pop2 = net.create(10, neuron, name='pop2')\n    proj = net.connect(pop1, pop2, 'exc', synapse, name='projection1')\n    m = net.monitor(pop2, 'r', name='monitor')\n    return net\n\nnet = create_network()\npop1 = net.get_population('pop1')\npop2 = net.get_population('pop2')\nproj = net.get_projection('projection1')\nm = net.get_monitor('monitor')\nAnother safer option is to create your own class inheriting from ann.Network and store all populations and projections as an attribute:\nclass SimpleNetwork (ann.Network):\n    def __init__(self, N)\n        self.pop1 = self.create(N, neuron)\n        self.pop2 = self.create(N, neuron)\n        self.proj = self.connect(self.pop1, self.pop2, 'exc', synapse)\n        self.m    = self.monitor(self.pop2, 'r')\n\nnet = SimpleNetwork(10)\nprint(net.pop1.r)\nYou do not need to explictly call the constructor of Network (Network.__init__(self)), it is done automatically. Creating a subclass of Network furthermore allows to use the parallel_run() method if the whole construction of the network is done through the constructor.\nApart from that, the two approaches are equivalent, pick the one you prefer. Subclasses are easier to re-use, especially across files.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Network"
    ]
  },
  {
    "objectID": "manual/Network.html#compiling-the-network",
    "href": "manual/Network.html#compiling-the-network",
    "title": "Network",
    "section": "",
    "text": "Once all the relevant information has been defined, one needs to actually compile the network, by calling the Network.compile() method:\nnet.compile()\nThe optimized C++ code will be generated, compiled, the underlying objects created and made available to the Python interface.\nYou can specify several arguments to compile(), including:\n\ncompiler: to select which C++ compiler will be used.\ncompiler_flags: to select which flags are passed to the compiler.\ndirectory: absolute/relative path to the directory where files will be generated and compiled (default: annarchy/).\n\n\n\nANNarchy requires a C++ compiler. On GNU/Linux, the default choice is g++, while on MacOS it is clang++. You can change the compiler (and its flags) to use either during the call to net.compile() in your script:\nnet.compile(compiler=\"clang++\", compiler_flags=\"-march=native -O3\")\nor globally by modifying the configuration file located at ~/.config/ANNarchy/annarchy.json:\n{\n    \"openmp\": {\n        \"compiler\": \"clang++\",\n        \"flags\": \"-march=native -O3\"\n    }\n}\nBe careful with the flags: for example, the optimization level -O3 does not obligatorily produce faster code. But this is the case for most models, therefore it is the default in the ANNarchy 4.7.x releases.\nEven more caution is required when using the -ffast-math flag. It can increase the performance, in particular in combination with SIMD. However, the application of -ffast-math enables a set of optimizations which might violate IEEE 754 compliance (which might be okay in many cases, but it is important that the user verifies the result). For more details, see the g++ documentation: https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html\n\n\n\n\n\n\nNote\n\n\n\nIn rare cases, it may occur that the CPU architecture is not detectable for the used g++ compiler (e.g. Intel’s Tigerlake and g++ &lt;= 9.4). This will result in a compiler error which can be fixed by removing the ‘-march=native’ flag. To get access to AVX-512 SIMD instructions, you need to add -mavx512f instead, as well as -ftree-vectorize if -O3 is not already used.\n\n\n\n\n\nWhen calling compile(), the subfolder annarchy/ (or whatever is defined by `directory) will be created, and the generated code will be compiled. The first compilation may last a couple of seconds, but further runs of the script are much faster. If no modification to the network has been made except for parameter values, it will not be recompiled, sparing us this overhead.\nANNarchy tracks the changes in the script and re-generates the corresponding code. In some cases (a new version of ANNarchy has been installed, bugs), it may be necessary to perform a fresh compilation of the network. You can either delete the annarchy/ subfolder and restart the script:\nrm -rf annarchy/\npython MyNetwork.py\npass the --clean flag to the script:\npython MyNetwork.py --clean \nor tell compile() to start fresh:\nnet.compile(clean=True)",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Network"
    ]
  },
  {
    "objectID": "manual/Network.html#simulating-the-network",
    "href": "manual/Network.html#simulating-the-network",
    "title": "Network",
    "section": "",
    "text": "After the network is compiled, the simulation can be run for the specified duration (in milliseconds) through the Network.simulate() method:\nnet.simulate(1000.0) # Simulate for 1 second\nThe provided duration should be a multiple of dt. If not, the number of simulation steps performed will be approximated.\nIn some cases, you may want to perform only one step of the simulation, instead of specifing the duration. The Network.step() method can then be used.\nnet.step() # Simulate for 1 step",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Network"
    ]
  },
  {
    "objectID": "manual/Network.html#setting-the-discretization-step-dt",
    "href": "manual/Network.html#setting-the-discretization-step-dt",
    "title": "Network",
    "section": "",
    "text": "An important value for the simulation is the discretization step dt. Its default value is 1 ms, which is usually fine for rate-coded networks, but may be too high for spiking networks, as the equations are stiffer. Taken too high, it can lead to high numerical errors. Too low, and the simulation will take an unnecessary amount of time.\nTo set the discretization step, just pass the desired value to the constructor of Network:\nnet = ann.Network(dt=0.1)\nIt can also be set using the config() method, before projections are created:\nnet.config(dt=0.1)\nHowever, changing its value after calling compile() will not have any effect.\nYou can always access the current value of dt with the attribute net.dt.\nIf you create a subclass of Network, you can also provide dt to its constructor, even if your network does not catch it.\nclass SimpleNetwork (ann.Network):\n    def __init__(self, N)\n        self.pop1 = net.create(N, neuron)\n        self.pop2 = net.create(N, neuron)\n        self.proj = net.connect(self.pop1, self.pop2, 'exc', synapse)\n\nnet = SimpleNetwork(N=10, dt=0.1)",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Network"
    ]
  },
  {
    "objectID": "manual/Network.html#setting-the-seed-of-the-random-number-generators",
    "href": "manual/Network.html#setting-the-seed-of-the-random-number-generators",
    "title": "Network",
    "section": "",
    "text": "By default, the random number generators (RNG) are seeded with secrets.randbits(32), so each simulation will be different from run to run. If you want to have deterministic simulations, you need to provide a fixed seed to the constructor of Network:\nnet = ann.Network(dt=0.1, seed=42)\nIf you define a subclass of Network, pass the seed without further processing, like for dt.\nclass SimpleNetwork (ann.Network):\n    def __init__(self, N)\n        self.pop1 = net.create(N, neuron)\n        self.pop2 = net.create(N, neuron)\n        self.proj = net.connect(self.pop1, self.pop2, 'exc', synapse)\n\nnet = SimpleNetwork(N=10, dt=0.1, seed=42)\nNote that this also sets the seed of the old RNG of numpy, which is used to initialize values produced by np.random.*.\nIf you use the new default RNG of numpy (rng = np.random.default_rng()) or ANNarchy’s random distributions (see Random Distributions), you will have to seed it yourself. The seed of network is accessible through the attribute net.seed.\nrng = np.random.default_rng(seed=net.seed)\n\npop.r = ann.Uniform(min=0.0, max=1.0, rng=rng)\n\n\n\n\n\n\nNote\n\n\n\nUsing the same seed with the OpenMP and CUDA backends will not lead to the same sequences of numbers!",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Network"
    ]
  },
  {
    "objectID": "manual/Network.html#early-stopping",
    "href": "manual/Network.html#early-stopping",
    "title": "Network",
    "section": "",
    "text": "In some cases, it is desired to stop the simulation whenever a criterion is fulfilled (for example, a neural integrator exceeds a certain threshold), not after a fixed amount of time.\nThere is the possibility to define a stop_condition when creating a Population:\npop1 = net.create( ... , stop_condition = \"r &gt; 1.0\")\nWhen calling the simulate_until() method instead of simulate():\nt = net.simulate_until(max_duration=1000.0, populations=pop1)\nthe simulation will be stopped whenever the stop_condition of pop1 is met, i.e. when the firing rate of any neuron of pop1 is above 1.0. If the condition is never met, the simulation will last maximally max_duration. The methods returns the effective duration of the simulation (to compute reaction times, for example).\nThe stop_condition can use any logical operation on the parameters and variables of the neuron associated to the population:\npop1 = net.create( ... , stop_condition = \"(r &gt; 1.0) and (mp &lt; 2.0)\")\nBy default, the simulation stops when at least one neuron in the population fulfills the criterion. If you want to stop the simulation when all neurons fulfill the condition, you can use the flag all after the condition:\npop1 = net.create( ... , stop_condition = \"r &gt; 1.0 : all\")\nThe flag any is the default behavior and can be omitted.\nThe stop criterion can depend on several populations, by providing a list of populations to the populations argument instead of a single population:\nt = net.simulate_until(max_duration=1000.0, populations=[pop1, pop2])\nThe simulation will then stop when the criterion is met in both populations at the same time. If you want that the simulation stops when at least one population meets its criterion, you can specify the operator argument:\nt = net.simulate_until(max_duration=1000.0, populations=[pop1, pop2], operator='or')\nThe default value of operator is a 'and' function between the populations’ criteria.\n\n\n\n\n\n\nWarning\n\n\n\nGlobal operations (min, max, mean) are not possible inside the stop_condition. If you need them, store them in a variable in the equations argument of the neuron and use it as the condition:\nequations = [\n    'r = ...',\n    'max_r = max(r)',\n]",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Network"
    ]
  },
  {
    "objectID": "manual/Network.html#setting-inputs-periodically",
    "href": "manual/Network.html#setting-inputs-periodically",
    "title": "Network",
    "section": "",
    "text": "In most cases, your simulation will be decomposed into a series of fixed-duration trials, where you basically set inputs at the beginning of the trial, run the simulation for a fixed duration, and possibly read out results at the end:\n# Iterate over 100 trials\nresult = []\nfor trial in range(100):\n    # Set inputs to the network\n    pop.I = ann.Uniform(0.0, 1.0)\n    # Simulate for 1 second\n    net.simulate(1000.)\n    # Save the output\n    result.append(pop.r)\nFor convenience, we provide the decorator every, which allows to register a python method and call it automatically during the simulation with a fixed period:\nresult = []\n\n@ann.every(period=1000.)\ndef set inputs(n):\n    # Set inputs to the network\n    pop.I = ann.Uniform(0.0, 1.0)\n    # Save the output of the previous step\n    if n &gt; 0:\n        result.append(pop.r)\n\nnet.simulate(100 * 1000.)\nIn this example, set_inputs() will be executed just before the steps corresponding to times t = 0., 1000., 2000., and so on until t = 100000.\nThe method can have any name, but must accept only one argument, the integer n which will be incremented at each call of the method (i.e. it will take the values 0, 1, 2 until 99). This can for example be used to access data in a numpy array:\nimages = np.random.random((100, 640, 480))\n\n@ann.every(period=1000.)\ndef set inputs(n):\n    # Set inputs to the network\n    pop.I = images[n, :, :]\n\nnet.simulate(100 * 1000.)\nOne can define several methods that will be called in the order of their definition:\n@ann.every(period=1000.)\ndef set inputs(n):\n    pop.I = 1.0\n\n@ann.every(period=1000.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this example, set_inputs() will be called first, followed by reset_inputs, so pop.I will finally be 0.0. The decorator every accepts an argument offset defining a delay within the period to call the method:\n@ann.every(period=1000.)\ndef set inputs(n):\n    pop.I = 1.0\n\n@ann.every(period=1000., offset=500.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this case, set_inputs() will be called at times 0, 1000, 2000... while reset_inputs() will be called at times 500, 1500, 2500..., allowing to structure a trial more effectively. The offset can be set negative, in which case it will be relative to the end of the trial:\n@every(period=1000., offset=-100.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this example, the method will be called at times 900, 1900, 2900 and so on. The offset value can not be longer than the period, by definition. If you try to do so, a modulo operation will anyway be applied (i.e. an offset of 1500 with a period of 1000 becomes 500).\nFinally, the wait argument allows to delay the first call to the method from a fixed interval:\n@every(period=1000., wait=5000.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this case, the method will be called at times 5000, 6000 and so on.\nBetween two calls to simulate(), the callbacks can be disabled or re-enabled using the following methods:\n@every(period=1000.)\ndef reset inputs(n):\n    pop.I = 0.0\n\n# Simulate with callbacks\nnet.simulate(10000.)\n\n# Disable callbacks\nnet.disable_callbacks()\n\n# Simulate without callbacks\nnet.simulate(10000.)\n\n# Re-enable callbacks\nnet.enable_callbacks()\n\n# Simulate with callbacks\nnet.simulate(10000.)\nNote that the period is always relative to the time when simulate() is called, so if no offset is defined, the callbacks will be called before the first step of a simulation, no matter how long the previous simulation lasted. In the current state, it is not possible yet to enable/disable callbacks selectively, it is all or none.\nCallbacks can only be used with simulate(), not with step() or simulate_until().",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Network"
    ]
  },
  {
    "objectID": "manual/Network.html#parallel-computing-with-openmp",
    "href": "manual/Network.html#parallel-computing-with-openmp",
    "title": "Network",
    "section": "",
    "text": "The default paradigm for an ANNarchy simulation is through openMP, which distributes automatically the computations over the available CPU cores.\nBy default, ANNarchy will use a single thread for your simulation. Automatically using all possible cores would not be optimal: small networks in particular tend to run faster with a smaller amount of cores. For this reason, the OMP_NUM_THREADS environment variable has no effect in ANNarchy.\nYou can control the number of cores by passing the -j flag to the Python script:\npython NeuralField.py -j2\nIt is the responsability of the user to find out which number of cores is optimal for his network, by comparing simulation times. When this optimal number is found, it can be hard-coded in the script by setting the num_threads argument to Network.config():\nnet.config(num_threads=2)",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Network"
    ]
  },
  {
    "objectID": "manual/Network.html#parallel-computing-with-cuda",
    "href": "manual/Network.html#parallel-computing-with-cuda",
    "title": "Network",
    "section": "",
    "text": "To run your network on GPUs, you need to declare to ANNarchy that you want to use CUDA. One way to do so is to pass the --gpu flag to the command line:\npython NeuralField.py --gpu\nYou can also set the paradigm argument of Network.config() to make it permanent:\nnet.config(paradigm=\"cuda\")\nIf there are multiple GPUs on your machine, you can select the ID of the device by specifying it to the --gpu flag on the command line:\npython NeuralField.py --gpu=2\nAlternatively, you can also pass the cuda_config dictionary argument to Network.compile():\nnet.compile(cuda_config={'device': 2})\nThe default GPU is defined in the configuration file ~/.config/ANNarchy/annarchy.json (0 unless you modify it).\n{\n    \"cuda\": {\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\n\nAs the current implementation is a development version, some of the features provided by ANNarchy are not supported yet with CUDA:\n\nweight sharing (convolutions),\nnon-uniform synaptic delays,\nstructural plasticity,\nspiking neurons: a) with mean firing rate and b) continous integration of inputs,\nSpikeSourceArray.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Network"
    ]
  },
  {
    "objectID": "manual/Parallel.html",
    "href": "manual/Parallel.html",
    "title": "Parallel simulations",
    "section": "",
    "text": "A typical ANNarchy model is represented by a single network of populations and projections. Most of the work in computational neuroscience consists in running the same network again and again, varying some free parameters each time, until the fit to the data is publishable.\nIn order to simulate similar networks in paralle using the same script, the Network class can be used to make copies of the current network and simulate them in parallel using parallel_run(). This is demonstrated in the notebook MultipleNetworks.ipynb.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Parallel simulations"
    ]
  },
  {
    "objectID": "manual/Parallel.html#network-subclass",
    "href": "manual/Parallel.html#network-subclass",
    "title": "Parallel simulations",
    "section": "Network subclass",
    "text": "Network subclass\nLet’s define a pulse-coupled network of Izhikevich neurons using a subclass of Network:\nclass PulseNetwork(ann.Network):\n\n    def __init__(self, w_exc=0.5, w_inh=1.0):\n\n        # Create the population\n        self.P = self.create(geometry=1000, neuron=ann.Izhikevich)\n\n        # Create the projections\n        self.proj_exc = self.connect(self.P[:800], self.P, 'exc')\n        self.proj_inh = self.connect(self.P[800:], self.P, 'inh')\n\n        self.proj_exc.all_to_all(weights=ann.Uniform(0.0, w_exc))\n        self.proj_inh.all_to_all(weights=ann.Uniform(0.0, w_inh))\n\n        # Create a spike monitor\n        self.m = self.monitor(self.P, 'spike')\nRefer to MultipleNetworks.ipynb for the parameters allowing a more realistic simulation. Let’s suppose that we want to run several simulations in parallel with different values of the parameter w_exc and plot the corresponding raster plots.\n\n\n\n\n\n\nImportant\n\n\n\nparallel_run() does not work when using a Network instance directly, it must be a subclass.\nnet = ann.Network()\nnet.create(...)\nnet.compile()\n\nnet.parallel_run(...) # CRASH!",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Parallel simulations"
    ]
  },
  {
    "objectID": "manual/Parallel.html#simulation-method",
    "href": "manual/Parallel.html#simulation-method",
    "title": "Parallel simulations",
    "section": "Simulation method",
    "text": "Simulation method\nTo perform the simulation, we just need to write a method that takes a network instance as an input and returns what we want:\ndef simulation(net, duration=1000.):\n    # Run the simulation\n    net.simulate(duration)\n\n    # Compute the raster plot\n    t, n = net.m.raster_plot()\n\n    return t, n\n\n# Create a network\nnet = PulseNetwork(w_exc=0.5)\nnet.compile()\n\n# Run a single simulation\nt, n = simulation(net)",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Parallel simulations"
    ]
  },
  {
    "objectID": "manual/Parallel.html#parallel_run",
    "href": "manual/Parallel.html#parallel_run",
    "title": "Parallel simulations",
    "section": "parallel_run()",
    "text": "parallel_run()\nIf you want multiple instances of the same network to perform the simulation, all you need to call is:\nresults = net.parallel_run(method=simulation, number=4)\nThe simulation() method will be called over four copies of the network (in different processes). The first argument to this method must be an instance of a Network subclass (class Network), which will be instantiated by parallel_run() in each process.\nThis is why only subclasses of Network are allowed: parallel_run() must be able to re-create the network using only its constructor. Populations and projections created with `net.create() and net.connect() could not be re-created exactly the same way (up to random number generation).\nYou do not have access on the internally-created networks after the simulation (they are in a separate memory space and will be deleted). The method must return the data you want to analyze (here the raster plot) or write them to disk (in separate files).\nparallel_run() returns a list of the values returned by each run of the simulation method:\nt1, n1 = results[0]\nt2, n2 = results[1]\nt3, n3 = results[2]\n\nPassing arguments\nThere are two sorts of arguments that you may want to pass to the parallel simulations:\n\nConstructor arguments (w_exc, w_inh) for the network copies,\nMethod arguments (duration) for the simulation method.\n\nThe first obligatory argument of the simulation callback is net, the network object, but you do not have control over it.\nYou can provide these arguments to the simulation callback during the parallel_run() call, by passing either a single value (the same for all copies of the network), or a list of values (same size as the number of simulations).\nExample with w_exc and duration:\nresults = net.parallel_run(\n    method=simulation, \n    number=4, \n    w_exc=[0.5, 1.0, 1.5, 1.0]\n    duration=[500, 1000, 1500, 2000])\nThe first simulation would use w_exc=0.5 and duration=500, the second w_exc=1.0 and duration=1000, and so on.\nThe arguments can be passed in any order, but they must be named. parallel_run() does not explicitly distinguish between constructor and method arguments, so be sure to have different names!\n\n\n\n\n\n\nImportant\n\n\n\nWhen passing arguments to the constructor of the network in parallel_run(), make sure that this parameter does not force recompilation! The method would try to compile each copy of the network in the same folder, and terrible things would happen.\nFor example, never change the size of a population in parallel_run(), as this always leads to recompilation. The only thing you should vary is the value of global parameters.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Parallel simulations"
    ]
  },
  {
    "objectID": "manual/Parallel.html#controlling-the-seeds",
    "href": "manual/Parallel.html#controlling-the-seeds",
    "title": "Parallel simulations",
    "section": "Controlling the seeds",
    "text": "Controlling the seeds\nIf you do not vary any parameter (constructor or method) during the call to parallel_run(), the only variability would come from the network itself, for example from the initialization of the random weights:\nself.proj_exc.all_to_all(weights=ann.Uniform(0.0, w_exc))\nself.proj_inh.all_to_all(weights=ann.Uniform(0.0, w_inh))\nThe weights are going to be redrawn for each copy, so you get different simulations every time.\nHowever, you may want to control the seed of each network for reproducibility. You first need to make sure that the seed of a network is used everywhere in its constructor.\nEven at the beginning of the constructor, the attribute self.seed is set at a random value (if left to None in the call to constructor), or to a desired value. You should use it to seed all random objects, passing numpy’s default RNG to ANNarchy’s random distribution classes:\nclass PulseNetwork(ann.Network):\n\n    def __init__(self, w_exc=0.5, w_inh=1.0):\n\n        rng = np.random.default_rng(self.seed)\n\n        # Create the population\n        self.P = self.create(geometry=1000, neuron=ann.Izhikevich)\n        self.P.v = rng.random.uniform(0.0, 1.0, 1000)\n\n        # Create the projections\n        self.proj_exc = self.connect(self.P[:800], self.P, 'exc')\n        self.proj_inh = self.connect(self.P[800:], self.P, 'inh')\n\n        self.proj_exc.all_to_all(weights=ann.Uniform(0.0, w_exc, rng=rng))\n        self.proj_inh.all_to_all(weights=ann.Uniform(0.0, w_inh, rng=rng))\n\n        # Create a spike monitor\n        self.m = self.monitor(self.P, 'spike')\nIntrinsically random methods, such as proj.fixed_probability() are automatically seeded by the network, you do not need to worry about them.\nOnce the seed of a network is controlled, parallel_run() offers several possibilities:\nRandom seeds everywhere\nnet = PulseNetwork(seed=None)\nresults = net.parallel_run(\n    method=simulation, \n    number=4, \n    seeds=None\n)\nIf both the main network and its copies do not have seeds, all re-runs will be different.\nSeeded network but random copies\nnet = PulseNetwork(seed=42)\nresults = net.parallel_run(\n    method=simulation, number=4, \n    seeds=None\n)\nThe main network will always perform the same simulation, but not its copies.\nList of seeds\nnet = PulseNetwork(seed=whatever)\nresults = net.parallel_run(\n    method=simulation, number=4, \n    seeds=[23, 42, 73, 9382758568]\n)\nA list of seeds can be provided for each of the subnetworks, regardless how the main network was seeded.\nIncremental seeds\nnet = PulseNetwork(seed=42)\nresults = net.parallel_run(\n    method=simulation, number=4, \n    seeds='sequential' # equivalent to [43, 44, 45, 46]\n)\nIf set to 'sequential', the seeds will incrementally increase from the seed of the main network (which has to be set).",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Parallel simulations"
    ]
  },
  {
    "objectID": "notebooks/RC.html",
    "href": "notebooks/RC.html",
    "title": "Echo state networks",
    "section": "",
    "text": "If you run this notebook in colab, first uncomment and run this cell to install ANNarchy:\n\n#!pip install ANNarchy\n\nThis notebook demonstrates how to implement a simple Echo state network (ESN) with ANNarchy. It is a simple rate-coded network, with a population of recurrently-connected neurons and a readout layer which will be learned offline using scikit-learn. The task will be a simple univariate regression.\n\nLet’s start by importing ANNarchy.\nsetup() sets various parameters, such as the step size dt in milliseconds. By default, dt is 1.0, so the call is not necessary here.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nEach neuron in the reservoir follows the following equations:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + g \\,  \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nwhere \\xi(t) is a uniform noise.\nThe neuron has three parameters and two variables:\n\nESN_Neuron = ann.Neuron(\n    parameters = dict(\n        tau = 30.0,\n        g = 1.0,\n        noise = 0.01,\n    ),\n    equations = [\n        'tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1)',\n        'r = tanh(x)',\n    ]\n)\n\nLet’s create an empty network:\n\nnet = ann.Network()\n\nThe echo-state network will be a population of 400 neuron.\n\nN = 400\npop = net.create(N, ESN_Neuron)\n\nWe can specify the value of the parameters from Python, this will override the value defined in the neuron description. We can give single float values or numpy arrays of the correct shape:\n\npop.tau = 30.0\npop.g = 1.4\npop.noise = 0.01\n\nThe input to the reservoir is a single value, we create a special population InputArray that does nothing except storing a variable called r that can be set externally.\n\ninp = net.create(ann.InputArray(1))\ninp.r = 0.0\n\nInput weights are uniformly distributed between -1 and 1.\n\nWi = net.connect(inp, pop, 'in')\nWi.all_to_all(weights=ann.Uniform(-1.0, 1.0))\n\n&lt;ANNarchy.core.Projection.Projection at 0x11ede1580&gt;\n\n\nRecurrent weights are sampled from the normal distribution with mean 0 and variance g^2 / N. Here, we put the synaptic scaling g inside the neuron.\n\nWrec = net.connect(pop, pop, 'exc')\nWrec.all_to_all(weights=ann.Normal(0., 1/np.sqrt(N)))\n\n&lt;ANNarchy.core.Projection.Projection at 0x11e1be030&gt;\n\n\n\nnet.compile()\n\nWe create a monitor to record the evolution of the firing rates in the reservoir during the simulation.\n\nm = net.monitor(pop, 'r')\nn = net.monitor(inp, 'r')\n\nA single trial lasts 3 second by default, with a step input between 100 and 200 ms. We define the trial in a method, so we can run it multiple times.\n\ndef trial(T=3000.):\n    \"Runs two trials for a given spectral radius.\"\n\n    # Reset firing rates\n    inp.r = 0.0\n    pop.x = 0.0\n    pop.r = 0.0\n    \n    # Run the trial\n    net.simulate(100.)\n    inp.r = 1.0\n    net.simulate(100.0) # initial stimulation\n    inp.r = 0.0\n    net.simulate(T - 200.)\n    \n    return m.get('r')\n\nWe run two trials successively to look at the chaoticity depending on g.\n\npop.g = 1.5\ndata1 = trial()\ndata2 = trial()\n\n\nplt.figure(figsize=(12, 12))\nplt.subplot(311)\nplt.title(\"First trial\")\nfor i in range(5):\n    plt.plot(data1[:, i], lw=2)\nplt.subplot(312)\nplt.title(\"Second trial\")\nfor i in range(5):\n    plt.plot(data2[:, i], lw=2)\nplt.subplot(313)\nplt.title(\"Difference\")\nfor i in range(5):\n    plt.plot(data1[:, i] - data2[:, i], lw=2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can now train the readout neurons to reproduce a step signal after 2 seconds.\nFor simplicity, we just train a L1-regularized linear regression (LASSO) on the reservoir activity using scikit-learn.\n\ntarget = np.zeros(3000)\ntarget[2000:2500] = 1.0\n\n\nfrom sklearn import linear_model\n\nreg = linear_model.Lasso(alpha=0.001, max_iter=10000)\nreg.fit(data1, target)\npred = reg.predict(data2)\n\n\nplt.figure(figsize=(12, 8))\nplt.plot(pred, lw=3)\nplt.plot(target, lw=3)\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Echo-state networks"
    ]
  },
  {
    "objectID": "notebooks/BarLearning.html",
    "href": "notebooks/BarLearning.html",
    "title": "Bar Learning problem",
    "section": "",
    "text": "The bar learning problem describes the process of learning receptive fields on an artificial input pattern. Images consisting of independent bars are used. Those images are generated as following: an 8*8 image can filled randomly by eight horizontal or vertical bars, with a probability of 1/8 for each.\nThese input images are fed into a neural population, whose neurons should learn to extract the independent components of the input distribution, namely single horizontal or vertical bars.\n#!pip install ANNarchy",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Bar Learning"
    ]
  },
  {
    "objectID": "notebooks/BarLearning.html#model-overview",
    "href": "notebooks/BarLearning.html#model-overview",
    "title": "Bar Learning problem",
    "section": "Model overview",
    "text": "Model overview\nThe model consists of two populations inp and pop. The size of inp should be chosen to fit the input image size (here 8*8). The number of neurons in the pop population should be higher than the total number of independent bars (16, we choose here 32 neurons). The pop population gets excitory connections from inp through an all-to-all connection pattern. The same pattern is used for the inhibitory connections within pop.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Bar Learning"
    ]
  },
  {
    "objectID": "notebooks/BarLearning.html#defining-the-neurons-and-populations",
    "href": "notebooks/BarLearning.html#defining-the-neurons-and-populations",
    "title": "Bar Learning problem",
    "section": "Defining the neurons and populations",
    "text": "Defining the neurons and populations\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\n\nnet = ann.Network()\n\nInput population:\nThe input pattern will be clamped into this population by the main loop for every trial, so we just need an InputArray to store the values:\n\nN = 8\ninp = net.create(ann.InputArray(geometry=(N, N)))\n\nLearning population:\nThe neuron type composing this population sums up all the excitory inputs gain from inp and the lateral inhibition within pop.\n\\tau \\frac {dr_{j}}{dt} + r_{j} = \\sum_{i} w_{ij} \\cdot r_{i}^{\\text{inp}}  - \\sum_{k, k \\ne j} w_{kj} * r_{k}\ncould be implemented as the following:\n\nLeakyNeuron = ann.Neuron(\n    parameters = dict(tau = 10.0),\n    equations = ann.Variable(\"tau * dr/dt + r = sum(exc) - sum(inh)\", min=0.0),\n)\n\nThe firing rate is restricted to positive values with the min=0.0 attribute. The population of 32 neurons is created in the following way:\n\npop = net.create(geometry=(N, int(N/2)), neuron=LeakyNeuron)\n\nWe define the population with a (8, 4) geometry for visualization only, its 2D structure does not influence computations at all. We could also use geometry=32 and reshape the array afterwards.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Bar Learning"
    ]
  },
  {
    "objectID": "notebooks/BarLearning.html#defining-the-synapse-and-projections",
    "href": "notebooks/BarLearning.html#defining-the-synapse-and-projections",
    "title": "Bar Learning problem",
    "section": "Defining the synapse and projections",
    "text": "Defining the synapse and projections\nBoth feedforward (inp \\rightarrow pop) and lateral (pop \\rightarrow pop) projections are learned using the Oja learning rule (a regularized Hebbian learning rule ensuring the sum of all weights coming to a neuron is constant). Only some parameters will differ between the projections.\n\\tau \\frac{dw_{ij}}{dt} = r_{i} \\cdot r_{j} - \\alpha \\cdot r_{j}^{2} \\cdot w_{ij}\nwhere \\alpha is a parameter defining the strength of the regularization, r_i is the pre-synaptic firing rate and r_j the post-synaptic one. The implementation of this synapse type is straightforward:\n\nOja = ann.Synapse(\n    parameters = dict( \n        tau = 2000.0,\n        alpha = 8.0,\n        min_w = 0.0,\n    ),\n    equations = ann.Variable(\n        \"tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w\", \n        min='min_w'\n    )\n)  \n\nFor this network we need to create two projections, one excitory between the populations inp and pop and one inhibitory within the pop population itself:\n\nff = net.connect(\n    pre=inp, \n    post=pop, \n    target='exc', \n    synapse = Oja    \n)\nff.all_to_all(\n    weights = ann.Uniform(0.0, 0.5)\n)\n                     \nlat = net.connect(\n    pre=pop, \n    post=pop, \n    target='inh', \n    synapse = Oja\n)\nlat.all_to_all(\n    weights = ann.Uniform(0.0, 1.0)\n)\n\n&lt;ANNarchy.core.Projection.Projection at 0x1042e5010&gt;\n\n\nThe two projections are all-to-all and use the Oja synapse type. They only differ by the parameter alpha (lower in pop):\n\nlat.alpha = 0.3\n\nWe can now compile the network:\n\nnet.compile()\n\nCompiling network 1...  OK",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Bar Learning"
    ]
  },
  {
    "objectID": "notebooks/BarLearning.html#setting-inputs",
    "href": "notebooks/BarLearning.html#setting-inputs",
    "title": "Bar Learning problem",
    "section": "Setting inputs",
    "text": "Setting inputs\nOnce the network is defined, one has to specify how inputs are fed into the inp population. A simple solution is to define a method that sets the firing rate of inp according to the specified probabilities every time it is called, and runs the simulation for 50 ms:\n\ndef trial():\n\n    # Reset the firing rate for all neurons\n    inp.r = 0.0\n\n    # Clamp horizontal bars randomly\n    hbars = np.random.binomial(n=1, p=1./N, size=N) == 1\n    for i, exists in enumerate(hbars): \n        inp[i, :].r = 1.0 if exists else inp[i, :].r\n\n    # Clamp vertical bars randomly\n    vbars = np.random.binomial(n=1, p=1./N, size=N) == 1\n    for j, exists in enumerate(vbars): \n        inp[:, j].r = 1.0 if exists else inp[:, j].r\n\n    # Simulate for 50ms\n    net.simulate(50.)\n    \n    # Return firing rates and receptive fields for visualization\n    return inp.r, pop.r, ff.receptive_fields()\n\nOne can use here a single value or a Numpy array (e.g. np.zeros(inp.geometry))) to reset activity in inp, it does not matter.\nFor the random bars, we use the binomial distribution to decide for the existence of a vertical or horizontal bar with a probability of 1/8.\ninp[i, :] and inp[:, j] are PopulationViews, i.e. groups of neurons defined by the sub-indices (here the rows and columns of inp). Their attributes, such as r, can be accessed and modified as if it were a regular population.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Bar Learning"
    ]
  },
  {
    "objectID": "notebooks/BarLearning.html#running-the-simulation",
    "href": "notebooks/BarLearning.html#running-the-simulation",
    "title": "Bar Learning problem",
    "section": "Running the simulation",
    "text": "Running the simulation\nLet’s have a look at the activities and receptive fields after one trial:\n\ninput_array, activity_array, weights = trial()\n\n\nfrom matplotlib.gridspec import GridSpec\n\nfig = plt.figure(layout=\"constrained\", figsize=(8, 8))\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0]) \nax2 = fig.add_subplot(gs[0, 1]) \nax3 = fig.add_subplot(gs[1, :]) \n\nim1 = ax1.imshow(input_array.T, interpolation='nearest', cmap=plt.cm.gray)\nax1.set_title('Input')\nim2 = ax2.imshow(activity_array.T, interpolation='nearest', cmap=plt.cm.gray)\nax2.set_title('Feature')\nim3 = ax3.imshow(weights.T, interpolation='nearest', cmap=plt.cm.gray, vmin=0.0, vmax=0.5)\nax3.set_title('Receptive fields')\nfig.colorbar(im3)\n\n\n\n\n\n\n\n\nOne or a few bars are present in inp, a few neurons react in pop, but the receptive fields are all random.\nLet’s now define a for loop where the trial() method is called repetitively 10000 times:\n\ninputs = []; features = []; rfs = []\n\nT = 10000\nfor t in range(T):\n    \n    # Single trial\n    input_r, feature_r, weights = trial()\n\n    # Record every 10 trials\n    if t % 10 == 0:\n        inputs.append(input_r)\n        features.append(feature_r)\n        rfs.append(weights)\n\nWe can now visualize the activities and receptive fields after learning:\n\nfig = plt.figure(layout=\"constrained\", figsize=(8, 8))\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0]) \nax2 = fig.add_subplot(gs[0, 1]) \nax3 = fig.add_subplot(gs[1, :]) \n\nim1 = ax1.imshow(inputs[-1].T, interpolation='nearest', cmap=plt.cm.gray)\nax1.set_title('Input')\nim2 = ax2.imshow(features[-1].T, interpolation='nearest', cmap=plt.cm.gray)\nax2.set_title('Feature')\nim3 = ax3.imshow(rfs[-1].T, interpolation='nearest', cmap=plt.cm.gray, vmin=0.0, vmax=0.3)\nax3.set_title('Receptive fields')\nfig.colorbar(im3)\n\n\n\n\n\n\n\n\nAfter 10000 trials, most neurons have developed a receptive field specific for a single horizontal or vertical bar, although these were always presented together.\nLet’s now have a look at how these receptive fields develop over the course of training.\n\n%matplotlib inline\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nfig = plt.figure(layout=\"constrained\", figsize=(8, 8))\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0]) \nax2 = fig.add_subplot(gs[0, 1]) \nax3 = fig.add_subplot(gs[1, :]) \n\nim1 = ax1.imshow(inputs[-1].T, interpolation='nearest', cmap=plt.cm.gray)\nax1.set_title('Input')\nim2 = ax2.imshow(features[-1].T, interpolation='nearest', cmap=plt.cm.gray)\nax2.set_title('Feature')\nim3 = ax3.imshow(rfs[-1].T, interpolation='nearest', cmap=plt.cm.gray, vmin=0.0, vmax=0.3)\nax3.set_title('Receptive fields')\ncb = fig.colorbar(im3)\n\ndef drawframe(n):\n    im1.set_data(inputs[n].T)\n    im2.set_data(features[n].T) \n    im3.set_data(rfs[n].T) \n    return (im1, im2, im3)\n\nanim = animation.FuncAnimation(fig, drawframe, frames=int(T/10), interval=20, blit=True)\nplt.close()\nHTML(anim.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nMost neurons become selective for a single bar. However, due to the lateral inhibition, two neurons selective for the same bar will compete with each other. The “losing” neuron will have to modify its receptive field to let the winner have it. This explains the instability of some cells during learning.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Bar Learning"
    ]
  },
  {
    "objectID": "notebooks/BoldMonitoring1.html",
    "href": "notebooks/BoldMonitoring1.html",
    "title": "BOLD monitoring - Balloon model",
    "section": "",
    "text": "#!pip install ANNarchy\nThis notebook demonstrates the usage of the BOLD monitoring extension. The extension has to be explicitly imported if you use any specific model:\nimport numpy as np\n\nimport ANNarchy as ann\nimport ANNarchy.extensions.bold as bold\n\nANNarchy 5.0 (5.0.0) on darwin (posix).",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "BOLD monitor I"
    ]
  },
  {
    "objectID": "notebooks/BoldMonitoring1.html#background",
    "href": "notebooks/BoldMonitoring1.html#background",
    "title": "BOLD monitoring - Balloon model",
    "section": "Background",
    "text": "Background\nANNarchy pre-implements some model variants of the BOLD models presented in Stephan et al. (2007) which are variations of the Balloon Model originally designed by Buxton et al. (1998). The four balloon variants model pre-implemented in ANNarchy follow the naming scheme of Stephan et al. (2007). The model name starts with balloon_ followed by either C (i. e. classical coefficient) or R (i. e. revised coefficient) and then followed by either N which means non-linear BOLD equation or L which means linear bold equation.\nWe only provide here the equations without much explanations, for more details please refer to the literature:\n\nBuxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855–864. doi:10.1002/mrm.1910390602\n\n\nFriston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466–477\n\n\nBuxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220–S233. doi:10.1016/j.neuroimage.2004.07.013\n\n\nStephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387–401. doi:10.1016/j.neuroimage.2007.07.040\n\n\nMaith et al. (2021). A computational model-based analysis of basal ganglia pathway changes in Parkinson’s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278– 2295. doi:10.1111/ejn.14868\n\n\nMaith et al. (2022). BOLD Monitoring in the Neural Simulator ANNarchy. Frontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966.",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "BOLD monitor I"
    ]
  },
  {
    "objectID": "notebooks/BoldMonitoring1.html#single-input-balloon-model",
    "href": "notebooks/BoldMonitoring1.html#single-input-balloon-model",
    "title": "BOLD monitoring - Balloon model",
    "section": "Single input Balloon model",
    "text": "Single input Balloon model\nThis script shows a simple example for the application of one of the default models (the balloon_RN model) on a simple network. The balloon_RN model is described by the following equations:\n\n    \\tau \\cdot \\frac{ds}{dt} = \\phi \\cdot I_\\text{CBF} - \\kappa \\cdot s - \\gamma \\cdot (f_{in} - 1)\n\n\n    \\frac{df_{in}}{dt} = s\n\n\n    E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} }\n\n\n    \\tau \\cdot \\frac{dv}{dt} = \\frac{1}{\\tau_0} \\cdot (f_{in} - f_{out})\n\n\n    f_{out} = v^{\\frac{1}{\\alpha}}\n\n\n    \\tau \\cdot \\frac{dq}{dt} = \\frac{1}{\\tau_0} \\cdot ( f_{in} \\cdot \\frac{E}{E_0} - \\frac{q}{v} \\cdot f_{out} )\n\nwith revised coefficients and non-linear bold equation:\nk_1            = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\nk_2            = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\nk_3            = 1 - \\epsilon\n\n    BOLD = v_0 \\cdot ( k_1 \\cdot (1-q) + k_2 \\cdot (1 - \\dfrac{q}{v}) + k_3 \\cdot (1 - v) )\n\nThere are two important variables in that model: BOLD which is the output of the model and I_CBF which is the input signal, reflecting the mean firing rate of the input populations.\nAs the BOLD model reflects the change of activity levels in the recorded area, we will implement the following experiment. We record from two populations with 100 Izhikevich neurons each. After a short period of time we raise the activity level of one population for a fixed time window. For simplicity, we do not use connections between the neurons but simply increase the noise term in the Izhikevich model. This should lead to a noticeable BOLD signal.\n\nPopulations\nWe first create two populations of Izhikevich neurons:\n\nnet = ann.Network()\n\npop1 = net.create(100, neuron=ann.Izhikevich)\npop2 = net.create(100, neuron=ann.Izhikevich)\n\nAs we will not have any connections between the neurons, we need to increase the noise to create some baseline activity:\n\n# Set noise to create some baseline activity\npop1.noise = 5.0\npop2.noise = 5.0\n\nThe mean firing rate of the neurons will be used as an input to the BOLD model. This need to be activated explicitly as the computation of this value is quite expensive. In our example, the mean firing rate should be computed across a time window of 100 ms. The resulting value will be stored in the r attribute of the populations and can be easily recorded.\n\n# Compute mean firing rate in Hz on 100ms window\npop1.compute_firing_rate(window=100.0)\npop2.compute_firing_rate(window=100.0)\n\n# Record the mean firing rate\nmon_pop1 = net.monitor(pop1, [\"r\"], start=False)\nmon_pop2 = net.monitor(pop2, [\"r\"], start=False)\n\n\n\nBOLD Monitor definition\nThe BOLD monitor expects a list of populations which we want to record (in our case pop0 and pop1). A BOLD model should be specified, here we take balloon_RN which is the default. We then specify the mapping from the population variable that should influence the BOLD signal, in our case the mean firing rate r, to the input variable of the BOLD model I_CBF.\nThe mean firing rate is normalized over a time window of 2000 ms and we record both input and output signals:\n\nm_bold = net.boldmonitor(\n    # Recorded populations\n    populations = [pop1, pop2], \n    # BOLD model to use (default is balloon_RN)\n    bold_model = bold.balloon_RN(), \n    # Mapping from pop.r to I_CBF\n    mapping = {'I_CBF': 'r'}, \n    # Time window to compute the baseline.\n    normalize_input = 2000,  \n    # Variables to be recorded\n    recorded_variables = [\"I_CBF\", \"BOLD\"]  \n)\n\nNow we can compile and initialize the network:\n\nnet.compile()\n\nCompiling network 1...  OK \n\n\n\n\nSimulation\nWe first simulate 1 second biological time to ensure that the network reaches a stable firing rate:\n\nnet.simulate(1000)\n\nWe then enable the recording of all monitors:\n\nmon_pop1.start()\nmon_pop2.start()\nm_bold.start()\n\nWe simulate for 5 seconds with lower noise and we increase the noise in pop0 for 5 seconds before decreasing it again:\n\n# We manipulate the noise for the half of the neurons\nnet.simulate(5000)      # 5s with low noise\npop1.noise = 7.5\nnet.simulate(5000)      # 5s with higher noise (one population)\npop1.noise = 5\nnet.simulate(10000)     # 10s with low noise\n\n# Retrieve the recordings\nmean_fr1 = np.mean(mon_pop1.get(\"r\"), axis=1)\nmean_fr2 = np.mean(mon_pop2.get(\"r\"), axis=1)\n\ninput_data = m_bold.get(\"I_CBF\")\nbold_data = m_bold.get(\"BOLD\")\n\n\n\nEvaluation\nWe can now plot:\n\nthe mean firing rate in the input populations.\nthe recorded activity I which serves as an input to the BOLD model.\nthe resulting BOLD signal.\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 5))\n\n# mean firing rate\nax1 = plt.subplot(131)\nax1.plot(mean_fr1, label=\"pop1\")\nax1.plot(mean_fr2, label=\"pop2\")\nplt.legend()\nax1.set_ylabel(\"Average mean firing rate [Hz]\")\n\n# BOLD input signal\nax2 = plt.subplot(132)\nax2.plot(input_data)\nax2.set_ylabel(\"BOLD input I_CBF\")\n\n# BOLD output signal\nax3 = plt.subplot(133)\nax3.plot(bold_data*100.0)\nax3.set_ylabel(\"BOLD [%]\")\n\n# x-axis labels as seconds\nfor ax in [ax1, ax2, ax3]:\n    ax.set_xticks(np.arange(0,21,2)*1000)\n    ax.set_xticklabels(np.arange(0,21,2))\n    ax.set_xlabel(\"time [s]\")\n\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "BOLD monitor I"
    ]
  },
  {
    "objectID": "notebooks/StructuralPlasticity.html",
    "href": "notebooks/StructuralPlasticity.html",
    "title": "Structural plasticity",
    "section": "",
    "text": "#!pip install ANNarchy\n\nAs simple example showing how to use structural plasticity (creation/pruning of synapses) in a rate-coded network (spiking networks work similarly).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nWe define a leaky integrator rate-coded neuron and a small population:\n\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0 ,\n        baseline = ann.Parameter(0.0),\n    ),\n    equations = [\n        ann.Variable(\"tau * dr/dt + r = baseline\", min=0.0),\n    ]\n)\n\nnet = ann.Network()\n\npop = net.create(50, LeakyIntegratorNeuron)\n\nStructural plasticity has two components: creation of synapses and pruning (removal) under certain conditions. These conditions are defined in the synapse type itself in the pruning and creating arguments:\n\nStructuralPlasticSynapse = ann.Synapse(\n    parameters = dict(T = ann.Parameter(10000, type='int')),\n    \n    equations = ann.Variable(\n        \"age = if pre.r * post.r &gt; 1.0 : 0 else : age + 1\", \n        init = 0, \n        type=int\n    ),\n    \n    pruning = ann.Pruning(\"age &gt; T\", proba = 0.1),\n    \n    creating = ann.Creating(\"pre.r * post.r &gt; 1.0\", proba = 0.5, w = 0.01),\n)\n\nproj = net.connect(pop, pop, 'exc', StructuralPlasticSynapse)\nproj.fixed_probability(weights = 0.01, probability=0.1)\n\n&lt;ANNarchy.core.Projection.Projection at 0x13a4c2420&gt;\n\n\nThese conditions must be boolean values, which when True may trigger the creation/pruning of a synapse. The flag proba gives the probability by which the synapse will actually be created/pruned.\n\nWhen creating is True, a synapse that did not exist will be created with the provided probability. Its weight will take the value provided by the flag w (0.01), the other variables take their default value.\nWhen pruning is True, a synapse that exists will be deleted with the given probability.\n\nThe pruning condition can depend on any pre-synaptic, post-synaptic or synaptic variable. The creating condition can only depend on pre- or post-synaptic conditions, as the synapse does not exist yet.\nApart from these two fields, the synapse is a regular synapse, one could also define synaptic plasticity mechanisms and so on.\nWe finally create a sparse projection within the population, with 10% connectivity.\n\nnet.compile()\n\nThe creation and pruning have to be explicitly started before a simulation, as they are very expensive computationally. The period argument states how often the conditions will be checked (avoid using dt):\n\n# Start structural plasticity\nproj.start_creating(period=100.0)\nproj.start_pruning(period=100.0)\n\n# Save the initial connectivity matrix\ninitial_weights = proj.connectivity_matrix()\n\nTo see the effect of structural plasticity, one alternatively activates one half of the population by setting a high baseline (mimicking corrrelated inputs). As neurons in one half will be activated at the same time, they will create synapses between each other. Between the two halves, the neurons are never co-activated, so the existing synapses will slowly die out.\n\n# Let structural plasticity over several trials\nfor trial in range(20):\n    \n    # Activate the first subpopulation\n    pop[:25].baseline = ann.Uniform(0.5, 1.5)\n    \n    # Simulate for 1s\n    net.simulate(1000.)\n    \n    # Reset the population\n    pop.baseline = 0.0\n    net.simulate(100.)\n    \n    # Activate the second subpopulation\n    pop[25:].baseline = ann.Uniform(0.5, 1.5) \n    \n    # Simulate for 1s\n    net.simulate(1000.)\n    \n    # Reset the population\n    pop.baseline = 0.0\n    net.simulate(100.)\n\n# Inspect the final connectivity matrix\nfinal_weights = proj.connectivity_matrix()\n\nWe can check the effect of structural plasticity by looking at the connectivity matrix before and after the stimulation:\n\nplt.figure(figsize=(12, 8))\nplt.subplot(121)\nplt.imshow(initial_weights)\nplt.title('Connectivity matrix before')\nplt.subplot(122)\nplt.imshow(final_weights)\nplt.title('Connectivity matrix after')\nplt.show()\n\n\n\n\n\n\n\n\nSynapses can be also created/pruned externally from python, using Dendrite.create_synapse() and Dendrite.prune_synapse().\nHere we add a single synapse in a quadrant of the connectivity matrix where no synapses existed:\n\n# Add a synapse\nproj.dendrite(10).create_synapse(rank=30, w=0.01)\n\n# Inspect the final connectivity matrix\nweights_after_insertion = proj.connectivity_matrix()\n\nplt.figure(figsize=(12, 8))\nplt.subplot(121)\nplt.imshow(final_weights)\nplt.title('Connectivity matrix before')\nplt.subplot(122)\nplt.imshow(weights_after_insertion)\nplt.title('Connectivity matrix after')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Remove the synapse\nproj.dendrite(10).prune_synapse(rank=30)\n\n# Inspect the final connectivity matrix\nweights_after_deletion = proj.connectivity_matrix()\n\nplt.figure(figsize=(12, 8))\nplt.subplot(121)\nplt.imshow(weights_after_insertion)\nplt.title('Connectivity matrix before')\nplt.subplot(122)\nplt.imshow(weights_after_deletion)\nplt.title('Connectivity matrix after')\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Structural plasticity"
    ]
  },
  {
    "objectID": "notebooks/v4/RC.html",
    "href": "notebooks/v4/RC.html",
    "title": "Echo state networks",
    "section": "",
    "text": "If you run this notebook in colab, first uncomment and run this cell to install ANNarchy:\n\n#!pip install ANNarchy\n\nThis notebook demonstrates how to implement a simple Echo state network (ESN) with ANNarchy. It is a simple rate-coded network, with a population of recurrently-connected neurons and a readout layer which will be learned offline using scikit-learn. The task will be a simple univariate regression.\n\nLet’s start by importing ANNarchy.\nThe clear() command is necessary in notebooks when recreating a network. If you re-run the cells creating a network without calling clear() first, populations will add up, and the results may not be what you expect.\nsetup() sets various parameters, such as the step size dt in milliseconds. By default, dt is 1.0, so the call is not necessary here.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nann.clear()\nann.setup(dt=1.0)\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\n\n\nEach neuron in the reservoir follows the following equations:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + g \\,  \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nwhere \\xi(t) is a uniform noise.\nThe neuron has three parameters and two variables:\n\nESN_Neuron = ann.Neuron(\n    parameters = \"\"\"\n        tau = 30.0 : population\n        g = 1.0 : population\n        noise = 0.01\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1)\n\n        r = tanh(x)\n    \"\"\"\n)\n\nThe echo-state network will be a population of 400 neuron.\n\nN = 400\npop = ann.Population(N, ESN_Neuron)\n\nWe can specify the value of the parameters from Python, this will override the value defined in the neuron description. We can give single float values or numpy arrays of the correct shape:\n\npop.tau = 30.0\npop.g = 1.4\npop.noise = 0.01\n\nThe input to the reservoir is a single value, we create a special population InputArray that does nothing except storing a variable called r that can be set externally.\n\ninp = ann.InputArray(1)\ninp.r = 0.0\n\nInput weights are uniformly distributed between -1 and 1.\n\nWi = ann.Projection(inp, pop, 'in')\nWi.connect_all_to_all(weights=ann.Uniform(-1.0, 1.0))\n\n&lt;ANNarchy.core.Projection.Projection at 0x13163c710&gt;\n\n\nRecurrent weights are sampled from the normal distribution with mean 0 and variance g^2 / N. Here, we put the synaptic scaling g inside the neuron.\n\nWrec = ann.Projection(pop, pop, 'exc')\nWrec.connect_all_to_all(weights=ann.Normal(0., 1/np.sqrt(N)))\n\n&lt;ANNarchy.core.Projection.Projection at 0x13163d490&gt;\n\n\n\nann.compile()\n\nCompiling ...  OK \n\n\nWe create a monitor to record the evolution of the firing rates in the reservoir during the simulation.\n\nm = ann.Monitor(pop, 'r')\nn = ann.Monitor(inp, 'r')\n\nA single trial lasts 3 second by default, with a step input between 100 and 200 ms. We define the trial in a method, so we can run it multiple times.\n\ndef trial(T=3000.):\n    \"Runs two trials for a given spectral radius.\"\n\n    # Reset firing rates\n    inp.r = 0.0\n    pop.x = 0.0\n    pop.r = 0.0\n    \n    # Run the trial\n    ann.simulate(100.)\n    inp.r = 1.0\n    ann.simulate(100.0) # initial stimulation\n    inp.r = 0.0\n    ann.simulate(T - 200.)\n    \n    return m.get('r')\n\nWe run two trials successively to look at the chaoticity depending on g.\n\npop.g = 1.5\ndata1 = trial()\ndata2 = trial()\n\n\nplt.figure(figsize=(12, 12))\nplt.subplot(311)\nplt.title(\"First trial\")\nfor i in range(5):\n    plt.plot(data1[:, i], lw=2)\nplt.subplot(312)\nplt.title(\"Second trial\")\nfor i in range(5):\n    plt.plot(data2[:, i], lw=2)\nplt.subplot(313)\nplt.title(\"Difference\")\nfor i in range(5):\n    plt.plot(data1[:, i] - data2[:, i], lw=2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can now train the readout neurons to reproduce a step signal after 2 seconds.\nFor simplicity, we just train a L1-regularized linear regression (LASSO) on the reservoir activity using scikit-learn.\n\ntarget = np.zeros(3000)\ntarget[2000:2500] = 1.0\n\n\nfrom sklearn import linear_model\n\nreg = linear_model.Lasso(alpha=0.001, max_iter=10000)\nreg.fit(data1, target)\npred = reg.predict(data2)\n\n\nplt.figure(figsize=(12, 8))\nplt.plot(pred, lw=3)\nplt.plot(target, lw=3)\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/BarLearning.html",
    "href": "notebooks/v4/BarLearning.html",
    "title": "Bar Learning problem",
    "section": "",
    "text": "The bar learning problem describes the process of learning receptive fields on an artificial input pattern. Images consisting of independent bars are used. Those images are generated as following: an 8*8 image can filled randomly by eight horizontal or vertical bars, with a probability of 1/8 for each.\nThese input images are fed into a neural population, whose neurons should learn to extract the independent components of the input distribution, namely single horizontal or vertical bars.\n#!pip install ANNarchy"
  },
  {
    "objectID": "notebooks/v4/BarLearning.html#model-overview",
    "href": "notebooks/v4/BarLearning.html#model-overview",
    "title": "Bar Learning problem",
    "section": "Model overview",
    "text": "Model overview\nThe model consists of two populations inp and pop. The size of inp should be chosen to fit the input image size (here 8*8). The number of neurons in the pop population should be higher than the total number of independent bars (16, we choose here 32 neurons). The pop population gets excitory connections from inp through an all-to-all connection pattern. The same pattern is used for the inhibitory connections within pop."
  },
  {
    "objectID": "notebooks/v4/BarLearning.html#defining-the-neurons-and-populations",
    "href": "notebooks/v4/BarLearning.html#defining-the-neurons-and-populations",
    "title": "Bar Learning problem",
    "section": "Defining the neurons and populations",
    "text": "Defining the neurons and populations\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\nann.clear()\n\nANNarchy 4.8 (4.8.3) on darwin (posix).\n\n\nInput population:\nThe input pattern will be clamped into this population by the main loop for every trial, so we just need an InputArray to store the values:\n\nN = 8\ninp = ann.InputArray(geometry=(N, N))\n\nLearning population:\nThe neuron type composing this population sums up all the excitory inputs gain from inp and the lateral inhibition within pop.\n\\tau \\frac {dr_{j}}{dt} + r_{j} = \\sum_{i} w_{ij} \\cdot r_{i}^{\\text{inp}}  - \\sum_{k, k \\ne j} w_{kj} * r_{k}\ncould be implemented as the following:\n\nLeakyNeuron = ann.Neuron(\n    parameters=\"\"\" \n        tau = 10.0 : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dr/dt + r = sum(exc) - sum(inh) : min=0.0\n    \"\"\"\n)\n\nThe firing rate is restricted to positive values with the min=0.0 flag. The population of 32 neurons is created in the following way:\n\npop = ann.Population(geometry=(N, int(N/2)), neuron=LeakyNeuron)\n\nWe define the population with a (8, 4) geometry for visualization only, its 2D structure does not influence computations at all. We could also use geometry=32 and reshape the array afterwards."
  },
  {
    "objectID": "notebooks/v4/BarLearning.html#defining-the-synapse-and-projections",
    "href": "notebooks/v4/BarLearning.html#defining-the-synapse-and-projections",
    "title": "Bar Learning problem",
    "section": "Defining the synapse and projections",
    "text": "Defining the synapse and projections\nBoth feedforward (inp \\rightarrow pop) and lateral (pop \\rightarrow pop) projections are learned using the Oja learning rule (a regularized Hebbian learning rule ensuring the sum of all weights coming to a neuron is constant). Only some parameters will differ between the projections.\n\\tau \\frac{dw_{ij}}{dt} = r_{i} \\cdot r_{j} - \\alpha \\cdot r_{j}^{2} \\cdot w_{ij}\nwhere \\alpha is a parameter defining the strength of the regularization, r_i is the pre-synaptic firing rate and r_j the post-synaptic one. The implementation of this synapse type is straightforward:\n\nOja = ann.Synapse(\n    parameters=\"\"\" \n        tau = 2000.0 : postsynaptic\n        alpha = 8.0 : postsynaptic\n        min_w = 0.0 : postsynaptic\n    \"\"\",\n    equations=\"\"\"\n        tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=min_w\n    \"\"\"\n)  \n\nFor this network we need to create two projections, one excitory between the populations inp and pop and one inhibitory within the pop population itself:\n\nff = ann.Projection(\n    pre=inp, \n    post=pop, \n    target='exc', \n    synapse = Oja    \n)\nff.connect_all_to_all(\n    weights = ann.Uniform(0.0, 0.5)\n)\n                     \nlat = ann.Projection(\n    pre=pop, \n    post=pop, \n    target='inh', \n    synapse = Oja\n)\nlat.connect_all_to_all(\n    weights = ann.Uniform(0.0, 1.0)\n)\n\n&lt;ANNarchy.core.Projection.Projection at 0x134f82120&gt;\n\n\nThe two projections are all-to-all and use the Oja synapse type. They only differ by the parameter alpha (lower in pop):\n\nlat.alpha = 0.3\n\nWe can now compile the network:\n\nann.compile()\n\nCompiling ...  OK"
  },
  {
    "objectID": "notebooks/v4/BarLearning.html#setting-inputs",
    "href": "notebooks/v4/BarLearning.html#setting-inputs",
    "title": "Bar Learning problem",
    "section": "Setting inputs",
    "text": "Setting inputs\nOnce the network is defined, one has to specify how inputs are fed into the inp population. A simple solution is to define a method that sets the firing rate of inp according to the specified probabilities every time it is called, and runs the simulation for 50 ms:\n\ndef trial():\n\n    # Reset the firing rate for all neurons\n    inp.r = 0.0\n\n    # Clamp horizontal bars randomly\n    hbars = np.random.binomial(n=1, p=1./N, size=N) == 1\n    for i, exists in enumerate(hbars): inp[i, :].r = 1.0 if exists else inp[i, :].r\n\n    # Clamp vertical bars randomly\n    vbars = np.random.binomial(n=1, p=1./N, size=N) == 1\n    for j, exists in enumerate(vbars): inp[:, j].r = 1.0 if exists else inp[:, j].r\n\n    # Simulate for 50ms\n    ann.simulate(50.)\n    \n    # Return firing rates and receptive fields for visualization\n    return inp.r, pop.r, ff.receptive_fields()\n\nOne can use here a single value or a Numpy array (e.g. np.zeros(inp.geometry))) to reset activity in inp, it does not matter.\nFor the random bars, we use the binomial distribution to decide for the existence of a vertical or horizontal bar with a probability of 1/8.\ninp[i, :] and inp[:, j] are PopulationViews, i.e. groups of neurons defined by the sub-indices (here the rows and columns of inp). Their attributes, such as r, can be accessed and modified as if it were a regular population."
  },
  {
    "objectID": "notebooks/v4/BarLearning.html#running-the-simulation",
    "href": "notebooks/v4/BarLearning.html#running-the-simulation",
    "title": "Bar Learning problem",
    "section": "Running the simulation",
    "text": "Running the simulation\nLet’s have a look at the activities and receptive fields after one trial:\n\ninput_array, activity_array, weights = trial()\n\n\nfrom matplotlib.gridspec import GridSpec\n\nfig = plt.figure(layout=\"constrained\", figsize=(8, 8))\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0]) \nax2 = fig.add_subplot(gs[0, 1]) \nax3 = fig.add_subplot(gs[1, :]) \n\nim1 = ax1.imshow(input_array.T, interpolation='nearest', cmap=plt.cm.gray)\nax1.set_title('Input')\nim2 = ax2.imshow(activity_array.T, interpolation='nearest', cmap=plt.cm.gray)\nax2.set_title('Feature')\nim3 = ax3.imshow(weights.T, interpolation='nearest', cmap=plt.cm.gray, vmin=0.0, vmax=0.5)\nax3.set_title('Receptive fields')\nfig.colorbar(im3)\n\n\n\n\n\n\n\n\nOne or a few bars are present in inp, a few neurons react in pop, but the receptive fields are all random.\nLet’s now define a for loop where the trial() method is called repetitively 10000 times:\n\ninputs = []; features = []; rfs = []\n\nT = 10000\nfor t in range(T):\n    \n    # Single trial\n    input_r, feature_r, weights = trial()\n\n    # Record every 10 trials\n    if t % 10 == 0:\n        inputs.append(input_r)\n        features.append(feature_r)\n        rfs.append(weights)\n\nWe can now visualize the activities and receptive fields after learning:\n\nfig = plt.figure(layout=\"constrained\", figsize=(8, 8))\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0]) \nax2 = fig.add_subplot(gs[0, 1]) \nax3 = fig.add_subplot(gs[1, :]) \n\nim1 = ax1.imshow(inputs[-1].T, interpolation='nearest', cmap=plt.cm.gray)\nax1.set_title('Input')\nim2 = ax2.imshow(features[-1].T, interpolation='nearest', cmap=plt.cm.gray)\nax2.set_title('Feature')\nim3 = ax3.imshow(rfs[-1].T, interpolation='nearest', cmap=plt.cm.gray, vmin=0.0, vmax=0.3)\nax3.set_title('Receptive fields')\nfig.colorbar(im3)\n\n\n\n\n\n\n\n\nAfter 10000 trials, most neurons have developed a receptive field specific for a single horizontal or vertical bar, although these were always presented together.\nLet’s now have a look at how these receptive fields develop over the course of training.\n\n%matplotlib inline\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nfig = plt.figure(layout=\"constrained\", figsize=(8, 8))\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0]) \nax2 = fig.add_subplot(gs[0, 1]) \nax3 = fig.add_subplot(gs[1, :]) \n\nim1 = ax1.imshow(inputs[-1].T, interpolation='nearest', cmap=plt.cm.gray)\nax1.set_title('Input')\nim2 = ax2.imshow(features[-1].T, interpolation='nearest', cmap=plt.cm.gray)\nax2.set_title('Feature')\nim3 = ax3.imshow(rfs[-1].T, interpolation='nearest', cmap=plt.cm.gray, vmin=0.0, vmax=0.3)\nax3.set_title('Receptive fields')\ncb = fig.colorbar(im3)\n\ndef drawframe(n):\n    im1.set_data(inputs[n].T)\n    im2.set_data(features[n].T) \n    im3.set_data(rfs[n].T) \n    return (im1, im2, im3)\n\nanim = animation.FuncAnimation(fig, drawframe, frames=int(T/10), interval=20, blit=True)\nplt.close()\nHTML(anim.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nMost neurons become selective for a single bar. However, due to the lateral inhibition, two neurons selective for the same bar will compete with each other. The “losing” neuron will have to modify its receptive field to let the winner have it. This explains the instability of some cells during learning."
  },
  {
    "objectID": "notebooks/v4/BoldMonitoring1.html",
    "href": "notebooks/v4/BoldMonitoring1.html",
    "title": "BOLD monitoring - Balloon model",
    "section": "",
    "text": "#!pip install ANNarchy\nThis notebook demonstrates the usage of the BOLD monitoring extension. It has to be explicitly imported:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\nfrom ANNarchy.extensions.bold import *\n\nANNarchy 4.8 (4.8.3) on darwin (posix)."
  },
  {
    "objectID": "notebooks/v4/BoldMonitoring1.html#background",
    "href": "notebooks/v4/BoldMonitoring1.html#background",
    "title": "BOLD monitoring - Balloon model",
    "section": "Background",
    "text": "Background\nANNarchy pre-implements some model variants of the BOLD models presented in Stephan et al. (2007) which are variations of the Balloon Model originally designed by Buxton et al. (1998). The four balloon variants model pre-implemented in ANNarchy follow the naming scheme of Stephan et al. (2007). The model name starts with balloon_ followed by either C (i. e. classical coefficient) or R (i. e. revised coefficient) and then followed by either N which means non-linear BOLD equation or L which means linear bold equation.\nWe only provide here the equations without much explanations, for more details please refer to the literature:\n\nBuxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855–864. doi:10.1002/mrm.1910390602\n\n\nFriston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466–477\n\n\nBuxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220–S233. doi:10.1016/j.neuroimage.2004.07.013\n\n\nStephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387–401. doi:10.1016/j.neuroimage.2007.07.040\n\n\nMaith et al. (2021). A computational model-based analysis of basal ganglia pathway changes in Parkinson’s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278– 2295. doi:10.1111/ejn.14868\n\n\nMaith et al. (2022). BOLD Monitoring in the Neural Simulator ANNarchy. Frontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966."
  },
  {
    "objectID": "notebooks/v4/BoldMonitoring1.html#single-input-balloon-model",
    "href": "notebooks/v4/BoldMonitoring1.html#single-input-balloon-model",
    "title": "BOLD monitoring - Balloon model",
    "section": "Single input Balloon model",
    "text": "Single input Balloon model\nThis script shows a simple example for the application of one of the default models (the balloon_RN model) on a simple network. The balloon_RN model is described by the following equations:\n\n    \\tau \\cdot \\frac{ds}{dt} = \\phi \\cdot I_\\text{CBF} - \\kappa \\cdot s - \\gamma \\cdot (f_{in} - 1)\n\n\n    \\frac{df_{in}}{dt} = s\n\n\n    E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} }\n\n\n    \\tau \\cdot \\frac{dv}{dt} = \\frac{1}{\\tau_0} \\cdot (f_{in} - f_{out})\n\n\n    f_{out} = v^{\\frac{1}{\\alpha}}\n\n\n    \\tau \\cdot \\frac{dq}{dt} = \\frac{1}{\\tau_0} \\cdot ( f_{in} \\cdot \\frac{E}{E_0} - \\frac{q}{v} \\cdot f_{out} )\n\nwith revised coefficients and non-linear bold equation:\nk_1            = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\nk_2            = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\nk_3            = 1 - \\epsilon\n\n    BOLD = v_0 \\cdot ( k_1 \\cdot (1-q) + k_2 \\cdot (1 - \\dfrac{q}{v}) + k_3 \\cdot (1 - v) )\n\nThere are two important variables in that model: BOLD which is the output of the model and I_CBF which is the input signal, reflecting the mean firing rate of the input populations.\nAs the BOLD model reflects the change of activity levels in the recorded area, we will implement the following experiment. We record from two populations with 100 Izhikevich neurons each. After a short period of time we raise the activity level of one population for a fixed time window. For simplicity, we do not use connections between the neurons but simply increase the noise term in the Izhikevich model. This should lead to a noticeable BOLD signal.\n\nPopulations\nWe first create two populations of Izhikevich neurons:\n\nann.clear()\n\npop0 = ann.Population(100, neuron=ann.Izhikevich)\npop1 = ann.Population(100, neuron=ann.Izhikevich)\n\nAs we will not have any connections between the neurons, we need to increase the noise to create some baseline activity:\n\n# Set noise to create some baseline activity\npop0.noise = 5.0; pop1.noise = 5.0\n\nThe mean firing rate of the neurons will be used as an input to the BOLD model. This need to be activated explicitly as the computation of this value is quite expensive. In our example, the mean firing rate should be computed across a time window of 100 ms. The resulting value will be stored in the r attribute of the populations and can be easily recorded.\n\n# Compute mean firing rate in Hz on 100ms window\npop0.compute_firing_rate(window=100.0)\npop1.compute_firing_rate(window=100.0)\n\n# Record the mean firing rate\nmon_pop0 = ann.Monitor(pop0, [\"r\"], start=False)\nmon_pop1 = ann.Monitor(pop1, [\"r\"], start=False)\n\n\n\nBOLD Monitor definition\nThe BOLD monitor expects a list of populations which we want to record (in our case pop0 and pop1). A BOLD model should be specified, here we take balloon_RN which is the default. We then specify the mapping from the population variable that should influence the BOLD signal, in our case the mean firing rate r, to the input variable of the BOLD model I_CBF.\nThe mean firing rate is normalized over a time window of 2000 ms and we record both input and output signals:\n\nm_bold = BoldMonitor(\n    \n    populations = [pop0, pop1], # recorded populations\n    \n    bold_model = balloon_RN(), # BOLD model to use (default is balloon_RN)\n    \n    mapping = {'I_CBF': 'r'}, # mapping from pop.r to I_CBF\n    \n    normalize_input = 2000,  # time window to compute baseline.\n    \n    recorded_variables = [\"I_CBF\", \"BOLD\"]  # variables to be recorded\n)\n\nNow we can compile and initialize the network:\n\nann.compile()\n\n\n\nSimulation\nWe first simulate 1 second biological time to ensure that the network reaches a stable firing rate:\n\nann.simulate(1000)\n\nWe then enable the recording of all monitors:\n\nmon_pop0.start()\nmon_pop1.start()\nm_bold.start()\n\nWe simulate for 5 seconds with lower noise and we increase the noise in pop0 for 5 seconds before decreasing it again:\n\n# We manipulate the noise for the half of the neurons\nann.simulate(5000)      # 5s with low noise\npop0.noise = 7.5\nann.simulate(5000)      # 5s with higher noise (one population)\npop0.noise = 5\nann.simulate(10000)     # 10s with low noise\n\n# Retrieve the recordings\nmean_fr1 = np.mean(mon_pop0.get(\"r\"), axis=1)\nmean_fr2 = np.mean(mon_pop1.get(\"r\"), axis=1)\n\ninput_data = m_bold.get(\"I_CBF\")\nbold_data = m_bold.get(\"BOLD\")\n\n\n\nEvaluation\nWe can now plot:\n\nthe mean firing rate in the input populations.\nthe recorded activity I which serves as an input to the BOLD model.\nthe resulting BOLD signal.\n\n\nplt.figure(figsize=(15, 5))\n\n# mean firing rate\nax1 = plt.subplot(131)\nax1.plot(mean_fr1, label=\"pop0\")\nax1.plot(mean_fr2, label=\"pop1\")\nplt.legend()\nax1.set_ylabel(\"Average mean firing rate [Hz]\")\n\n# BOLD input signal\nax2 = plt.subplot(132)\nax2.plot(input_data)\nax2.set_ylabel(\"BOLD input I_CBF\")\n\n# BOLD output signal\nax3 = plt.subplot(133)\nax3.plot(bold_data*100.0)\nax3.set_ylabel(\"BOLD [%]\")\n\n# x-axis labels as seconds\nfor ax in [ax1, ax2, ax3]:\n    ax.set_xticks(np.arange(0,21,2)*1000)\n    ax.set_xticklabels(np.arange(0,21,2))\n    ax.set_xlabel(\"time [s]\")\n\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/StructuralPlasticity.html",
    "href": "notebooks/v4/StructuralPlasticity.html",
    "title": "Structural plasticity",
    "section": "",
    "text": "#!pip install ANNarchy\n\nAs simple example showing how to use structural plasticity (creation/pruning of synapses) in a rate-coded network (spiking networks work similarly).\nFirst, the structural plasticity mechanisms must be allowed in setup():\n\nimport numpy as np\n\nimport ANNarchy as ann\nann.clear()\n\n# Compulsory to allow structural plasticity\nann.setup(structural_plasticity=True)\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\n\n\nWe define a leaky integrator rate-coded neuron and a small population:\n\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters=\"\"\"\n        tau = 10.0 : population\n        baseline = 0.0 \n    \"\"\",\n    equations = \"\"\"\n        tau * dr/dt + r = baseline + sum(exc) : min=0.0\n    \"\"\"\n)\npop = ann.Population(100, LeakyIntegratorNeuron)\n\nStructural plasticity has two components: creation of synapses and pruning (removal) under certain conditions. These conditions are defined in the synapse type itself in the pruning and creating arguments:\n\nStructuralPlasticSynapse = ann.Synapse(\n    parameters = \" T = 10000 : int, projection \",\n    equations = \"\"\"\n        age = if pre.r * post.r &gt; 1.0 :\n                0\n              else :\n                age + 1 : init = 0, int\"\"\",\n    pruning = \"age &gt; T : proba = 0.2\",\n    creating = \"pre.r * post.r &gt; 1.0 : proba = 0.1, w = 0.01\",\n)\n\nproj = ann.Projection(pop, pop, 'exc', StructuralPlasticSynapse)\nproj.connect_fixed_probability(weights = 0.01, probability=0.1)\n\n&lt;ANNarchy.core.Projection.Projection at 0x129a6c980&gt;\n\n\nThese conditions must be boolean values, which when True may trigger the creation/pruning of a synapse. The flag proba gives the probability by which the synapse will actually be created/pruned.\n\nWhen creating is True, a synapse that did not exist will be created with the provided probability. Its weight will take the value provided by the flag w (0.01), the other variables take their default value.\nWhen pruning is True, a synapse that exists will be deleted with the given probability.\n\nThe pruning condition can depend on any pre-synaptic, post-synaptic or synaptic variable. The creating condition can only depend on pre- or post-synaptic conditions, as the synapse does not exist yet.\nApart from these two fields, the synapse is a regular synapse, one could also define synaptic plasticity mechanisms and so on.\nWe finally create a sparse projection within the population, with 10% connectivity.\n\nann.compile()\n\nCompiling ...  OK \n\n\nThe creation and pruning have to be explicitly started before a simulation, as they are very expensive computationally. The period argument states how often the conditions will be checked (avoid using dt):\n\nproj.start_creating(period=100.0)\nproj.start_pruning(period=100.0)\n\nTo see the effect of structural plasticity, one alternatively activates one half of the population by setting a high baseline (mimicking corrrelated inputs). As neurons in one half will be activated at the same time, they will create synapses between each other. Between the two halves, the neurons are never co-activated, so the existing synapses will slowly die out.\n\n# Save the initial connectivity matrix\ninitial_weights = proj.connectivity_matrix()\n\n# Let structural plasticity over several trials\nnum_trials = 100\nfor trial in range(num_trials):\n    # Activate the first subpopulation\n    pop[:50].baseline = 1.0\n    # Simulate for 1s\n    ann.simulate(1000.)\n    # Reset the population\n    pop.baseline = 0.0\n    ann.simulate(100.)\n    # Activate the second subpopulation\n    pop[50:].baseline = 1.0\n    # Simulate for 1s\n    ann.simulate(1000.)\n    # Reset the population\n    pop.baseline = 0.0\n    ann.simulate(100.)\n\n# Inspect the final connectivity matrix\nfinal_weights = proj.connectivity_matrix()\n\nWe can check the effect of structural plasticity by looking at the connectivity matrix before and after the stimulation:\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8))\nplt.subplot(121)\nplt.imshow(initial_weights)\nplt.title('Connectivity matrix before')\nplt.subplot(122)\nplt.imshow(final_weights)\nplt.title('Connectivity matrix after')\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/BasalGanglia.html",
    "href": "notebooks/v4/BasalGanglia.html",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "The tensorboard extension allows to log various information (scalars, images, etc) during training for visualization using tensorboard.\n\n#!pip install ANNarchy\n\nThe extension has to be explicitly imported after ANNarchy:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\nfrom ANNarchy.extensions.tensorboard import Logger\n\nANNarchy 4.8 (4.8.3) on darwin (posix).\n\n\nAs it is just for demonstration purposes, we will be an extremely simplified model of the basal ganglia learning to solve through reinforcement learning a stimulus-response task with 4 stimuli and 2 responses (left and right). The two first stimuli should be responded with left, the two others with right.\n\nstimuli = [\n    ([1, 0, 0, 0], 0), # A : left\n    ([0, 1, 0, 0], 0), # B : left\n    ([0, 0, 1, 0], 1), # C : right\n    ([0, 0, 0, 1], 1), # D : right\n]\n\nWe keep here the model as simple as possible. It is inspired from the rate-coded model described here:\nVitay J, Hamker FH. 2010. A computational model of Basal Ganglia and its role in memory retrieval in rewarded visual memory tasks. Frontiers in computational neuroscience 4. doi:10.3389/fncom.2010.00013\nThe input population is composed of 4 static neurons to represent the inputs:\n\ncortex = ann.Population(4, ann.Neuron(parameters=\"r=0.0\"))\n\nThe cortex projects on the striatum, which is composed of 10 neurons integrating excitatory and inhibitory inputs:\n\nmsn = ann.Neuron(\n    parameters=\"tau = 10.0 : population; noise = 0.1 : population\", \n    equations=\"\"\"\n        tau*dv/dt + v = sum(exc) - sum(inh) + noise * Uniform(-1, 1)\n        r = clip(v, 0.0, 1.0)\n        \"\"\")\nstriatum = ann.Population(10, msn)\n\nThe striatum projects inhibitorily on GPi, whose neurons are tonically active (high baseline). Normally, GPi would project on the thalamus and back to the cortex, but here we read the output of the network directly in GPi: if the first neuron (corresponding to the left action) is less active than the second neuron, the selected action is left.\n\ngp_neuron = ann.Neuron(\n    parameters=\"tau = 10.0 : population; B = 1.0\",\n    equations=\"tau*dv/dt + v = B - sum(inh); r= pos(v)\")\ngpi = ann.Population(2, gp_neuron)\n\nLearning occurs at the cortico-striatal synapses, using a reward-modulated Hebbian learning rule, with Oja regularization:\n\ncorticostriatal = ann.Synapse(\n    parameters=\"\"\"\n        eta = 0.1 : projection\n        alpha = 0.5 : projection\n        dopamine = 0.0 : projection\"\"\",\n    equations=\"w += eta*(dopamine * pre.r * post.r - alpha*w*post.r*post.r) : min=0.0\"\n)\ncx_str = ann.Projection(cortex, striatum, \"exc\", corticostriatal)\ncx_str.connect_all_to_all(weights=ann.Uniform(0.0, 0.5))\n\n&lt;ANNarchy.core.Projection.Projection at 0x1556fa750&gt;\n\n\nSome lateral competition between the striatal neurons:\n\nstr_str = ann.Projection(striatum, striatum, \"inh\")\nstr_str.connect_all_to_all(weights=0.6)\n\n&lt;ANNarchy.core.Projection.Projection at 0x1556fab70&gt;\n\n\nOne half of the striatal population is connected to the left GPi neuron, the other half to the right neuron:\n\nstr_gpi1 = ann.Projection(striatum[:int(striatum.size/2)], gpi[0], 'inh').connect_all_to_all(1.0)\nstr_gpi2 = ann.Projection(striatum[int(striatum.size/2):], gpi[1], 'inh').connect_all_to_all(1.0)\n\nWe add a monitor on GPi and compile:\n\nm = ann.Monitor(gpi, 'r')\n\nann.compile()\n\nCompiling ...  OK \n\n\nEach trial is very simple: we get a stimulus x from the stimuli array and a correct response t, reset the network for 40 ms, set the input and simulate for 50 ms, observe the activity in GPi to decide what the answer of the network is, provide reward accordingly to the corticostriatal projection and let learn for 10 ms.\nHere the “dopamine” signal is directly the reward (+1 for success, -1 for failure), not the reward prediction error, but it is just for demonstration.\n\ndef training_trial(x, t):\n    \n    # Delay period\n    cortex.r = 0.0\n    cx_str.dopamine = 0.0\n    ann.simulate(40.0)\n    \n    # Set inputs\n    cortex.r = np.array(x)\n    ann.simulate(50.0)\n    \n    # Read output\n    output = gpi.r\n    answer = np.argmin(output)\n    \n    # Provide reward\n    reward = 1.0 if answer == t else -1.0\n    cx_str.dopamine = reward\n    ann.simulate(10.0)\n    \n    # Get recordings\n    data = m.get('r')\n    \n    return reward, data\n\nThe whole training procedure will simply iterate over the four stimuli for 100 trials:\nfor trial in range(100):\n    # Get a stimulus\n    x, t = stimuli[trial%len(stimuli)]\n    # Perform a trial\n    reward, data = training_trial(x, t)\nWe use the Logger class of the tensorboard extension to keep track of various data:\nwith Logger() as logger:\n    for trial in range(100):\n        # Get a stimulus\n        x, t = stimuli[trial%len(stimuli)]\n        # Perform a trial\n        reward, data = training_trial(x, t)\n        # Log data...\nNote that it would be equivalent to manually close the Logger after training:\nlogger = Logger()\nfor trial in range(100):\n    # Get a stimulus\n    x, t = stimuli[trial%len(stimuli)]\n    # Perform a trial\n    reward, data = training_trial(x, t)\n        # Log data...\nlogger.close()\nWe log here different quantities, just to demonstrate the different methods of the Logger class:\n\nThe reward received after each trial:\n\nlogger.add_scalar(\"Reward\", reward, trial)\nThe tag “Reward” will be the name of the plot in tensorboard. reward is the value that will be displayed, while trial is the index of the current trial (x-axis).\n\nThe activity of the two GPi cells at the end of the trial, in separate plots depending on the stimulus:\n\nif trial%len(stimuli) == 0:\n    label = \"GPi activity/A\"\nelif trial%len(stimuli) == 1:\n    label = \"GPi activity/B\"\nelif trial%len(stimuli) == 2:\n    label = \"GPi activity/C\"\nelif trial%len(stimuli) == 3:\n    label = \"GPi activity/D\"\nlogger.add_scalars(label, {\"Left neuron\": gpi.r[0], \"Right neuron\": gpi.r[1]}, trial)\nThe four plots will be grouped under the label “GPi activity”, with a title A, B, C or D. Note that add_scalars() requires a dictionary of values that will plot together.\n\nThe activity in the striatum as a 2*5 image:\n\nlogger.add_image(\"Activity/Striatum\", striatum.r.reshape((2, 5)), trial)\nThe activity should be reshaped to the correct dimensions. Note that activity in the striatum is bounded between 0 and 1, so there is no need for equalization.\n\nAn histogram of the preference for the stimuli A and B of striatal cells:\n\nw = np.array(cx_str.w)\nlogger.add_histogram(\"Cortico-striatal weights/Left - AB/CD\", np.mean(w[:5, :2] - w[:5, 2:], axis=1), trial)\nlogger.add_histogram(\"Cortico-striatal weights/Right - AB/CD\", np.mean(w[5:, :2] - w[5:, 2:], axis=1), trial)\nWe make here two plots, one for the first 5 striatal cells, the other for the rest. We plot the difference between the mean weights of each cell for the stimuli A and B, and the mean weights for the stimuli C and D. If learning goes well, the first five striatal cells should have stronger weights for A and B than for C and D, as they project to the left GPi cell.\n\nA matplotlib figure showing the time course of the two GPi cells (as recorded by the monitor):\n\nfig = plt.figure(figsize=(10, 8))\nplt.plot(data[:, 0], label=\"left\")\nplt.plot(data[:, 1], label=\"right\")\nplt.legend()\nlogger.add_figure(\"Activity/GPi\", fig, trial)\nNote that the figure will be automatically closed by the logger, no need to call show(). Logging figures is extremely slow, use that feature wisely.\nBy default, the logs are saved in the subfolder runs/, but this can be changed when creating the Logger:\nwith Logger(\"/tmp/experiment\") as logger:\nEach run of the network will be saved in this folder. You may want to delete the folder before each run, in order to only visualize the last run:\n\n%rm -rf runs\n\n\nwith Logger() as logger:\n    \n    for trial in range(100):\n        \n        # Get a stimulus\n        x, t = stimuli[trial%len(stimuli)]\n    \n        # Perform a trial\n        reward, data = training_trial(x, t)\n        \n        # Log received rewards\n        logger.add_scalar(\"Reward\", reward, trial)\n\n        # Log outputs depending on the task\n        if trial%len(stimuli) == 0:\n            label = \"GPi activity/A\"\n        elif trial%len(stimuli) == 1:\n            label = \"GPi activity/B\"\n        elif trial%len(stimuli) == 2:\n            label = \"GPi activity/C\"\n        elif trial%len(stimuli) == 3:\n            label = \"GPi activity/D\"\n        logger.add_scalars(label, {\"Left neuron\": gpi.r[0], \"Right neuron\": gpi.r[1]}, trial)\n\n        # Log striatal activity as a 2*5 image\n        logger.add_image(\"Activity/Striatum\", striatum.r.reshape((2, 5)), trial)\n\n        # Log histogram of cortico-striatal weights\n        w = np.array(cx_str.w)\n        logger.add_histogram(\"Cortico-striatal weights/Left - AB/CD\", np.mean(w[:5, :2] - w[:5, 2:], axis=1), trial)\n        logger.add_histogram(\"Cortico-striatal weights/Right - AB/CD\", np.mean(w[5:, :2] - w[5:, 2:], axis=1), trial)\n\n        # Log matplotlib figure of GPi activity\n        fig = plt.figure(figsize=(10, 8))\n        plt.plot(data[:, 0], label=\"left\")\n        plt.plot(data[:, 1], label=\"right\")\n        plt.legend()\n        logger.add_figure(\"Activity/GPi\", fig, trial)\n\nLogging in runs/Feb04_12-09-02_e-10-57-0b27.local.tu-chemnitz.de\n\n\nYou can now visualize the logged information by running tensorboard in a separate terminal and opening the corresponding page:\ntensorboard --logdir runs\nor directly in the notebook if you have the tensorboard extension installed:\n\n%load_ext tensorboard\n%tensorboard --logdir runs --samples_per_plugin images=100\n\nYou should see a tensorboard page with four tabs Scalars, Images, Distributions and Histograms:\n\n\n\ntensorboard.png\n\n\nThe Reward plot shows that the network successfully learns to solve the task, as it consistently gets rewards of +1 (note that this may vary from run to run, depending on weight initialization):\n\n\n\nreward.png\n\n\nThe GPi activity tab shows that the two GPi cells quickly learn to be inhibited for the right stimuli.\n\n\n\ngpi.png\n\n\nIn the Images tab, the plot for the striatum allows to visualize activity at the end of each rtial, showing that only one cell in the correct subpopulation is active:\n\n\n\nstriatum.png\n\n\nThe matplotlib figure for the GPi activity shows what happens during a trial, especially at the end of the reset period:\n\n\n\ntimecourse.png\n\n\nIn the histograms tab, we can see that the left striatal population has acquired a preference (stronger weights) for the stimuli A and B, as the values are positive. The right population has negative values, so the neurons have stronger weights to the stimuli C and D. Note that some neurons in the right population still have stronger weights from A and B, but they are probably inhibited by the left population, so they do not impair performance.\n\n\n\nweights-left.png\n\n\n\n\n\nweights-right.png"
  },
  {
    "objectID": "notebooks/v4/Miconi.html",
    "href": "notebooks/v4/Miconi.html",
    "title": "Miconi network",
    "section": "",
    "text": "#!pip install ANNarchy\n\nReward-modulated recurrent network based on:\n\nMiconi T. (2017). Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks. eLife 6:e20899. doi:10.7554/eLife.20899\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nimport ANNarchy as ann\nann.clear()\nann.setup(dt=1.0)\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\n\n\nEach neuron in the reservoir follows the following equations:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nwhere \\xi(t) is a random perturbation at 3 Hz, with an amplitude randomly sampled between -A and +A.\nWe additionally keep track of the mean firing rate with a sliding average:\n\n    \\tilde{x}(t) = \\alpha \\, \\tilde{x}(t) + (1 - \\alpha) \\, x(t)\n\nThe three first neurons keep a constant rate throughout learning (1 or -1) to provide some bias to the other neurons.\n\nneuron = ann.Neuron(\n    parameters = \"\"\"\n        tau = 30.0 : population # Time constant\n        constant = 0.0 # The four first neurons have constant rates\n        alpha = 0.05 : population # To compute the sliding mean\n        f = 3.0 : population # Frequency of the perturbation\n        A = 16. : population # Perturbation amplitude. dt*A/tau should be 0.5...\n    \"\"\",\n    equations=\"\"\"\n        # Perturbation\n        perturbation = if Uniform(0.0, 1.0) &lt; f/1000.: 1.0 else: 0.0 \n        noise = if perturbation &gt; 0.5: A * Uniform(-1.0, 1.0) else: 0.0\n\n        # ODE for x\n        x += dt*(sum(in) + sum(exc) - x + noise)/tau\n\n        # Output r\n        rprev = r # store r at previous time step\n        r = if constant == 0.0: tanh(x) else: tanh(constant)\n\n        # Sliding mean\n        delta_x = x - x_mean\n        x_mean = alpha * x_mean + (1 - alpha) * x\n    \"\"\"\n)\n\nThe learning rule is defined by a trace e_{i, j}(t) for each synapse i \\rightarrow j incremented at each time step with:\n\n    e_{i, j}(t) = e_{i, j}(t-1) + (r_i (t) \\, x_j(t))^3\n\nAt the end T of a trial, the reward R is delivered and all weights are updated using:\n\n    \\Delta w_{i, j} = \\eta \\,  e_{i, j}(T) \\, |R_\\text{mean}| \\,  (R - R_\\text{mean})\n\nwhere R_\\text{mean} is the mean reward for the task. Here the reward is defined as the opposite of the prediction error.\nAll traces are then reset to 0 for the next trial. Weight changes are clamped between -0.0003 and 0.0003.\nAs ANNarchy applies the synaptic equations at each time step, we need to introduce a boolean learning_phase which performs trace integration when 0, weight update when 1.\n\nsynapse = ann.Synapse(\n    parameters=\"\"\"\n        eta = 0.5 : projection # Learning rate\n        learning_phase = 0.0 : projection # Flag to allow learning only at the end of a trial\n        reward = 0.0 : projection # Reward received\n        mean_reward = 0.0 : projection # Mean Reward received\n        max_weight_change = 0.0003 : projection # Clip the weight changes\n    \"\"\",\n    equations=\"\"\"\n        # Trace\n        trace += if learning_phase &lt; 0.5:\n                    power(pre.rprev * (post.delta_x), 3)\n                 else:\n                    0.0\n\n        # Weight update only at the end of the trial\n        delta_w = if learning_phase &gt; 0.5:\n                eta * trace * fabs(mean_reward) * (reward - mean_reward)\n             else:\n                 0.0 : min=-max_weight_change, max=max_weight_change\n        w += delta_w\n        \n    \"\"\"\n)\n\nWe model the DNMS task of Miconi. The RC network has two inputs A and B. The reservoir has 200 neurons, 3 of which having constant rates to serve as biases for the other neurons.\n\n# Input population\ninp = ann.Population(2, ann.Neuron(parameters=\"r=0.0\"))\n\n# Recurrent population\nN = 200\npop = ann.Population(N, neuron)\n\n# Biases\npop[0].constant = 1.0\npop[1].constant = 1.0\npop[2].constant = -1.0\n\nInput weights are uniformly distributed between -1 and 1.\nThe recurrent weights are normally distributed, with a coupling strength of g=1.5 (edge of chaos). In the original paper, the projection is fully connected (but self-connections are avoided). Using a sparse (0.1) connectivity matrix leads to similar results and is much faster.\n\n# Input weights\nWi = ann.Projection(inp, pop, 'in')\nWi.connect_all_to_all(weights=ann.Uniform(-1.0, 1.0))\n\n# Recurrent weights\ng = 1.5\nsparseness = 0.1\n\nWrec = ann.Projection(pop, pop, 'exc', synapse)\nif sparseness == 1.0:\n    Wrec.connect_all_to_all(weights=ann.Normal(0., g/np.sqrt(N)))\nelse:\n    Wrec.connect_fixed_probability(probability=sparseness, weights=ann.Normal(0., g/np.sqrt(sparseness*N)))\n\n\nann.compile()\n\nCompiling ...  OK \n\n\nThe output of the reservoir is chosen to be the neuron of index 100.\n\nOUTPUT_NEURON = 100\n\nWe record the rates inside the reservoir:\n\nm = ann.Monitor(pop, ['r'], start=False)\n\nParameters defining the task:\n\n# Durations\nd_stim = 200\nd_delay= 200\nd_response = 200\n\nDefinition of a DNMS trial (AA, AB, BA, BB):\n\ndef dnms_trial(trial_number, input, target, R_mean, record=False, perturbation=True):\n\n    # Switch off perturbations if needed\n    if not perturbation:\n        old_A = pop.A\n        pop.A = 0.0\n\n    # Reinitialize network\n    pop.x = ann.Uniform(-0.1, 0.1).get_values(N)\n    pop.r = np.tanh(pop.x)\n    pop[0].r = np.tanh(1.0)\n    pop[1].r = np.tanh(1.0)\n    pop[2].r = np.tanh(-1.0)\n\n    if record: m.resume()\n\n    # First input\n    inp[input[0]].r = 1.0\n    ann.simulate(d_stim)\n    \n    # Delay\n    inp.r = 0.0\n    ann.simulate(d_delay)\n    \n    # Second input\n    inp[input[1]].r = 1.0\n    ann.simulate(d_stim)\n    \n    # Delay\n    inp.r = 0.0\n    ann.simulate(d_delay)\n    \n    # Response\n    if not record: m.resume()\n    inp.r = 0.0\n    ann.simulate(d_response)\n    \n    # Read the output\n    m.pause()\n    recordings = m.get('r')\n    \n    # Response is over the last 200 ms\n    output = recordings[-int(d_response):, OUTPUT_NEURON] # neuron 100 over the last 200 ms\n    \n    # Compute the reward as the opposite of the absolute error\n    reward = - np.mean(np.abs(target - output))\n    \n    # The first 25 trial do not learn, to let R_mean get realistic values\n    if trial_number &gt; 25:\n\n        # Apply the learning rule\n        Wrec.learning_phase = 1.0\n        Wrec.reward = reward\n        Wrec.mean_reward = R_mean\n\n        # Learn for one step\n        ann.step()\n        \n        # Reset the traces\n        Wrec.learning_phase = 0.0\n        Wrec.trace = 0.0\n        #_ = m.get() # to flush the recording of the last step\n\n    # Switch back on perturbations if needed\n    if not perturbation:\n        pop.A = old_A\n\n    return recordings, reward\n\nLet’s visualize the activity of the output neuron during the first four trials.\n\n# Perform the four different trials successively\ninitialAA, errorAA = dnms_trial(0, [0, 0], -0.98, 0.0, record=True)\ninitialAB, errorAB = dnms_trial(0, [0, 1], +0.98, 0.0, record=True)\ninitialBA, errorBA = dnms_trial(0, [1, 0], +0.98, 0.0, record=True)\ninitialBB, errorBB = dnms_trial(0, [1, 1], -0.98, 0.0, record=True)\n\nplt.figure(figsize=(12, 10))\nax = plt.subplot(221)\nax.plot(initialAA[:, OUTPUT_NEURON])\nax.set_ylim((-1., 1.))\nax.set_title('Output AA -1')\nax = plt.subplot(222)\nax.plot(initialBA[:, OUTPUT_NEURON])\nax.set_ylim((-1., 1.))\nax.set_title('Output BA +1')\nax = plt.subplot(223)\nax.plot(initialAB[:, OUTPUT_NEURON])\nax.set_ylim((-1., 1.))\nax.set_title('Output AB +1')\nax = plt.subplot(224)\nax.plot(initialBB[:, OUTPUT_NEURON])\nax.set_ylim((-1., 1.))\nax.set_title('Output BB -1')\nplt.show()\n\n\n\n\n\n\n\n\nWe can now run the simulation for 1500 trials. Beware, this can take 15 to 20 minutes.\n\n# Compute the mean reward per trial\nR_mean = - np.ones((2, 2))\nalpha = 0.75\n\n# Many trials of each type\nrecord_rewards = []\n\nfor trial in (t := tqdm(range(10000))):\n\n    # Perform the four different trials successively\n    _, rewardAA = dnms_trial(trial, [0, 0], -0.98, R_mean[0, 0])\n\n    _, rewardAB = dnms_trial(trial, [0, 1], +0.98, R_mean[0, 1])\n\n    _, rewardBA = dnms_trial(trial, [1, 0], +0.98, R_mean[1, 0])\n\n    _, rewardBB = dnms_trial(trial, [1, 1], -0.98, R_mean[1, 1])\n\n    # Reward\n    reward = np.array([[rewardAA, rewardBA], [rewardBA, rewardBB]])\n\n    # Update mean reward\n    R_mean = alpha * R_mean + (1.- alpha) * reward\n\n    record_rewards.append(R_mean)\n    t.set_description(\n        f'AA: {R_mean[0, 0]:.2f} AB: {R_mean[0, 1]:.2f} BA: {R_mean[1, 0]:.2f} BB: {R_mean[1, 1]:.2f}'\n    )\n\nAA: -0.07 AB: -0.02 BA: -0.02 BB: -0.04: 100%|██████████| 10000/10000 [14:47&lt;00:00, 11.27it/s]\n\n\n\nrecord_rewards = np.array(record_rewards)\n\nplt.figure(figsize=(10, 6))\nplt.plot(record_rewards[:, 0, 0], label='AA')\nplt.plot(record_rewards[:, 0, 1], label='AB')\nplt.plot(record_rewards[:, 1, 0], label='BA')\nplt.plot(record_rewards[:, 1, 1], label='BB')\nplt.plot(record_rewards.mean(axis=(1,2)), label='mean')\nplt.xlabel(\"Trials\")\nplt.ylabel(\"Mean reward\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Perform the four different trials without perturbation for testing\nrecordsAA, errorAA = dnms_trial(0, [0, 0], -0.98, 0.0, record=True, perturbation=False)\nrecordsAB, errorAB = dnms_trial(0, [0, 1], +0.98, 0.0, record=True, perturbation=False)\nrecordsBA, errorBA = dnms_trial(0, [1, 0], +0.98, 0.0, record=True, perturbation=False)\nrecordsBB, errorBB = dnms_trial(0, [1, 1], -0.98, 0.0, record=True, perturbation=False)\n\nplt.figure(figsize=(12, 10))\nplt.subplot(221)\nplt.plot(initialAA[:, OUTPUT_NEURON], label='before')\nplt.plot(recordsAA[:, OUTPUT_NEURON], label='after')\nplt.legend()\nplt.ylim((-1., 1.))\nplt.title('Trial AA : t=-1')\nplt.subplot(222)\nplt.plot(initialBA[:, OUTPUT_NEURON], label='before')\nplt.plot(recordsBA[:, OUTPUT_NEURON], label='after')\nplt.ylim((-1., 1.))\nplt.title('Trial BA : t=+1')\nplt.subplot(223)\nplt.plot(initialAB[:, OUTPUT_NEURON], label='before')\nplt.plot(recordsAB[:, OUTPUT_NEURON], label='after')\nplt.ylim((-1., 1.))\nplt.title('Trial AB : t=+1')\nplt.subplot(224)\nplt.plot(initialBB[:, OUTPUT_NEURON], label='before')\nplt.plot(recordsBB[:, OUTPUT_NEURON], label='after')\nplt.ylim((-1., 1.))\nplt.title('Trial BB : t=-1')\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/HodgkinHuxley.html",
    "href": "notebooks/v4/HodgkinHuxley.html",
    "title": "Hodgkin Huxley neuron",
    "section": "",
    "text": "#!pip install ANNarchy\n\nSimple Hodgkin-Huxley neuron.\n\nimport numpy as np\nimport ANNarchy as ann\nann.clear()\n\ndt=0.01\nann.setup(dt=dt)\n\nHH = ann.Neuron(\n\n    parameters = \"\"\"\n    C = 1.0 # Capacitance\n    VL = -59.387 # Leak voltage\n    VK = -82.0 # Potassium reversal voltage\n    VNa = 45.0 # Sodium reveral voltage\n    gK = 36.0 # Maximal Potassium conductance\n    gNa = 120.0 # Maximal Sodium conductance\n    gL = 0.3 # Leak conductance\n    vt = 30.0 # Threshold for spike emission\n    I = 0.0 # External current\n    \"\"\",\n\n    equations = \"\"\"\n    # Previous membrane potential\n    prev_V = V\n\n    # Voltage-dependency parameters\n    an = 0.01 * (V + 60.0) / (1.0 - exp(-0.1* (V + 60.0) ) )\n    am = 0.1 * (V + 45.0) / (1.0 - exp (- 0.1 * ( V + 45.0 )))\n    ah = 0.07 * exp(- 0.05 * ( V + 70.0 ))\n\n    bn = 0.125 * exp (- 0.0125 * (V + 70.0))\n    bm = 4.0 *  exp (- (V + 70.0) / 80.0)\n    bh = 1.0/(1.0 + exp (- 0.1 * ( V + 40.0 )) )\n\n    # Alpha/Beta functions\n    dn/dt = an * (1.0 - n) - bn * n : init = 0.3, midpoint\n    dm/dt = am * (1.0 - m) - bm * m : init = 0.0, midpoint\n    dh/dt = ah * (1.0 - h) - bh * h : init = 0.6, midpoint\n\n    # Membrane equation\n    C * dV/dt = gL * (VL - V ) + gK * n**4 * (VK - V) + gNa * m**3 * h * (VNa - V) + I : midpoint\n\n    \"\"\",\n\n    spike = \"\"\"\n    # Spike is emitted when the membrane potential crosses the threshold from below\n    (V &gt; vt) and (prev_V &lt;= vt)    \n    \"\"\",\n\n    reset = \"\"\"\n    # Nothing to do, it is built-in...\n    \"\"\"\n)\n\npop = ann.Population(neuron=HH, geometry=1)\npop.V = -50.0\n\nann.compile()\n\nm = ann.Monitor(pop, ['spike', 'V', 'n', 'm', 'h'])\n\n# Preparation\nann.simulate(100.0)\n# Current impulse for 1 ms\npop.I = 200.0\nann.simulate(1.0)\n# Reset\npop.I = 0.0\nann.simulate(100.0)\n\ndata = m.get()\n\ntstart = int(90.0/dt)\ntstop  = int(120.0/dt)\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\nplt.subplot(2,2,1)\nplt.plot(90.0 + dt*np.arange(tstop-tstart), data['V'][tstart:tstop, 0])\nplt.title('V')\nplt.subplot(2,2,2)\nplt.plot(90.0 + dt*np.arange(tstop-tstart), data['n'][tstart:tstop, 0])\nplt.title('n')\nplt.ylim((0.0, 1.0))\nplt.subplot(2,2,3)\nplt.plot(90.0 + dt*np.arange(tstop-tstart), data['m'][tstart:tstop, 0])\nplt.title('m')\nplt.ylim((0.0, 1.0))\nplt.subplot(2,2,4)\nplt.plot(90.0 + dt*np.arange(tstop-tstart), data['h'][tstart:tstop, 0])\nplt.title('h')\nplt.ylim((0.0, 1.0))\nplt.show()\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\nCompiling ...  OK"
  },
  {
    "objectID": "notebooks/v4/BCM.html",
    "href": "notebooks/v4/BCM.html",
    "title": "BCM learning rule",
    "section": "",
    "text": "The goal of this notebook is to investigate the Intrator & Cooper BCM learning rule for rate-coded networks.\n\\Delta w = \\eta \\, r^\\text{pre} \\, r^\\text{post}  \\,  (r^\\text{post} - \\mathbb{E}[(r^\\text{post})^2])\n\nIntrator, N., & Cooper, L. N. (1992). Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. Neural Networks, 5(1), 3–17. https://doi.org/10.1016/S0893-6080(05)80003-6\n\n\n#!pip install ANNarchy\n\nWe first import ANNarchy:\n\nimport numpy as np\nimport ANNarchy as ann\n\nann.clear()\nann.setup(dt=1.0)\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\n\n\nWe will keep a minimal experimental setup, with two input neurons connected to a single output neuron. Note how the input neurons are defined by setting r as a parameter that can be set externally.\n\n# Input\ninput_neuron = ann.Neuron(\n    parameters = \"\"\"\n        r = 0.0\n    \"\"\"\n)\npre = ann.Population(2, input_neuron)\n\n# Output\nneuron = ann.Neuron(\n    equations = \"\"\"\n        r = sum(exc)\n    \"\"\"\n)\npost = ann.Population(1, neuron)\n\nWe can now define a synapse model implementing the Intrator and Cooper version of the BCM learning rule.\nThe synapse has two parameters: The learning rate eta and the time constant tau of the moving average theta. Both are defined as projection parameters, as we only need one value for the whole projection. If you omit this flag, there will be one value per synapse, which would be a waste of RAM.\nThe moving average theta tracks the square of the post-synaptic firing rate post.r. It has the flag postsynaptic, as we need to compute only one variable per post-synaptic neuron (it does not really matter in our example as have only one output neuron…). It uses the exponential numerical method, as it is a first-order linear ODE that can be solved exactly. However, the default explicit Euler method would work just as well here.\nThe weight change dw/dt follows the BCM learning rule. min=0.0 ensures that the weight w stays positive throughout learning. The explicit Euler method is the default and could be omitted.\nThe psp argument w * pre.r (what is summed by the post-synaptic neuron over its incoming connections) is also the default value and could be omitted.\n\nIBCM = ann.Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = (post.r)^2 : postsynaptic, exponential\n\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit\n    \"\"\",\n    psp = \"w * pre.r\"\n)\n\nWe can now create a projection between the two populations using the synapse type. The connection method is all-to-all, initialozing the two weights to 1.\n\nproj = ann.Projection(pre, post, 'exc', IBCM)\nproj.connect_all_to_all(1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x10e3b8ec0&gt;\n\n\nWe can now compile the network and record the post-synaptic firing rate as well as the evolution of the weights and thresholds during learning.\n\nann.compile()\n\nm = ann.Monitor(post, 'r')\nn = ann.Monitor(proj, ['w', 'theta'])\n\nCompiling ...  OK \nWARNING: Monitor(): it is a bad idea to record synaptic variables of a projection at each time step! \n\n\nThe simulation protocol is kept simple, as it consists of setting constant firing rates for the two input neurons and simulating for one second.\n\npre.r = np.array([1.0, 0.1])\nann.simulate(1000.)\n\nWe can now retrieve the recordings and plot the evolution of the various variables.\n\nr = m.get('r')\nw = n.get('w')\ntheta = n.get('theta')\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.subplot(211)\nplt.plot(r[:, 0], label='r')\nplt.plot(theta[:, 0], label='theta')\nplt.legend()\nplt.subplot(212)\nplt.plot(w[:, 0, 0], label=\"$w_1$\")\nplt.plot(w[:, 0, 1], label=\"$w_2$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNotice how the first weight increases when r is higher than theta (LTP), but decreases afterwards (LTD). Unintuitively, the input neuron with the highest activity sees its weight decreased at the end of the stimulation."
  },
  {
    "objectID": "notebooks/v4/Ramp.html",
    "href": "notebooks/v4/Ramp.html",
    "title": "Homeostatic STDP",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis example is a reimplementation of the mechanism described in:\n\nCarlson, K.D.; Richert, M.; Dutt, N.; Krichmar, J.L., “Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks,” in Neural Networks (IJCNN), The 2013 International Joint Conference on , vol., no., pp.1-8, 4-9 Aug. 2013. doi: 10.1109/IJCNN.2013.6706961\n\nIt is based on the corresponding Carlsim tutorial:\nhttp://www.socsci.uci.edu/~jkrichma/CARLsim/doc/tut3_plasticity.html\nThis notebook focuses on the simple “Ramp” experiment, but the principle is similar for the self-organizing receptive fields (SORF) in the next notebook.\n\nimport numpy as np\n\nimport ANNarchy as ann\nann.clear()\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\n\n\nThe network uses regular-spiking Izhikevich neurons (see the Izhikevich notebook), but using exponentially-decaying conductances and NMDA synapses:\n\nRSNeuron = ann.Neuron(\n    parameters = \"\"\"\n        a = 0.02 : population\n        b = 0.2 : population\n        c = -65. : population\n        d = 8. : population\n        tau_ampa = 5. : population\n        tau_nmda = 150. : population\n        vrev = 0.0 : population\n    \"\"\" ,\n    equations=\"\"\"\n        # Inputs\n        I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)  \n        # Membrane potential and recovery variable are solved using the midpoint method for stability     \n        dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint\n        du/dt = a * (b*v - u) : init=-13., midpoint\n        # AMPA and NMDA conductances\n        tau_ampa * dg_ampa/dt = -g_ampa : exponential\n        tau_nmda * dg_nmda/dt = -g_nmda : exponential\n    \"\"\" , \n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\", \n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\"\n)\n\nThe main particularity about NMDA synaptic models is that a single synaptic connection influences two conductances:\n\nThe AMPA conductance, which primarily drives the post-synaptic neuron:\n\n\n    I_\\text{AMPA} = g_\\text{AMPA} \\times (V_\\text{rev} - V)\n\n\nThe NMDA conductance, which is non-linearly dependent on the membrane potential:\n\n\n    I_\\text{NMDA} = g_\\text{NMDA} \\times \\frac{(\\frac{V - V_\\text{NMDA}}{\\sigma})^2}{1 + (\\frac{V - V_\\text{NMDA}}{\\sigma})^2} \\times (V_\\text{rev} - V)\n\nIn short, the NMDA conductance only increases if the post-synaptic neuron is already depolarized.\nThe nmda function is defined in the functions argument for readability. The parameters V_\\text{NMDA} =-80 \\text{mV} and \\sigma = 60 \\text{mV} are here hardcoded in the equation, but they could be defined as global parameters.\nThe AMPA and NMDA conductances are exponentially decreasing with different time constants:\n\n    \\tau_\\text{AMPA} \\frac{dg_\\text{AMPA}(t)}{dt} + g_\\text{AMPA}(t) = 0\n \n    \\tau_\\text{NMDA} \\frac{dg_\\text{NMDA}(t)}{dt} + g_\\text{NMDA}(t) = 0\n\nAnother thing to notice in this neuron model is that the differential equations for the membrane potential and recovery variable are solved concurrently using the midpoint numerical method for stability: the semi-implicit method initially proposed by Izhikevich would fail.\nThe input of the network is a population of 100 Poisson neurons, whose firing rate vary linearly from 0.2 to 20 Hz:\n\n# Input population\ninp = ann.PoissonPopulation(100, rates=np.linspace(0.2, 20., 100))\n\nWe will consider two RS neurons, one learning inputs from the Poisson population using the regular STDP, the other learning using the proposed homeostatic STDP:\n\n# RS neuron without homeostatic mechanism\npop1 = ann.Population(1, RSNeuron)\n\n# RS neuron with homeostatic mechanism\npop2 = ann.Population(1, RSNeuron)\n\nThe regular STDP used in the article is a nearest-neighbour variant, which integrates LTP and LTD traces triggered after each pre- or post-synaptic spikes, respectively.\nContrary to the STDP synapse provided by ANNarchy, weight changes occur at each each time step:\n\nIn a post-pre interval, weight changes follow the LTP trace,\nIn a pre-post interval, weight changes follow the LTD trace.\n\nThe weights are clipped between 0 and w_\\text{max}.\n\nnearest_neighbour_stdp = ann.Synapse(\n    parameters=\"\"\"\n        tau_plus = 20. : projection\n        tau_minus = 60. : projection\n        A_plus = 0.0002 : projection\n        A_minus = 0.000066 : projection\n        w_max = 0.03 : projection\n    \"\"\",\n    equations = \"\"\"\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n        # Nearest-neighbour\n        w += if t_post &gt;= t_pre: ltp else: - ltd : min=0.0, max=w_max\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\"\n)\n\nThe homeostatic STDP rule proposed by Carlson et al. is more complex. It has a regular STDP part (the nearest-neighbour variant above) and a homeostatic regularization part, ensuring that the post-synaptic firing rate R does not exceed a target firing rate R_\\text{target} = 35 Hz.\nThe firing rate of a spiking neuron can be automatically computed by ANNarchy (see later). It is then accessible as the variable r of the neuron (as if it were a regular rate-coded neuron).\nThe homeostatic STDP rule is defined by:\n\n    \\Delta w = K \\, (\\alpha  \\, (1 - \\frac{R}{R_\\text{target}}) \\, w + \\beta \\, \\text{stdp} )\n\nwhere stdp is the regular STDP weight change, and K is a firing rate-dependent learning rate:\n\n    K =  \\frac{R}{ T \\, (1 + |1 - \\gamma \\, \\frac{R}{R_\\text{target}}|})\n\nwith T being the window over which the mean firing rate is computed (5 seconds) and \\alpha, \\beta, \\gamma are parameters.\n\nhomeo_stdp = ann.Synapse(\n    parameters=\"\"\"\n        # STDP\n        tau_plus = 20. : projection\n        tau_minus = 60. : projection\n        A_plus = 0.0002 : projection\n        A_minus = 0.000066 : projection\n        w_min = 0.0 : projection\n        w_max = 0.03 : projection\n\n        # Homeostatic regulation\n        alpha = 0.1 : projection\n        beta = 1.0 : projection\n        gamma = 50. : projection\n        Rtarget = 35. : projection\n        T = 5000. : projection\n    \"\"\",\n    equations = \"\"\"\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n        # Homeostatic values\n        R = post.r : postsynaptic\n        K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic\n        # Nearest-neighbour\n        stdp = if t_post &gt;= t_pre: ltp else: - ltd \n        w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\" \n)\n\nThis rule necessitates that the post-synaptic neurons compute their average firing rate over a 5 seconds window. This has to be explicitely enabled, as it would be computationally too expensive to allow it by default:\n\npop1.compute_firing_rate(5000.)\npop2.compute_firing_rate(5000.)\n\nWe can now fully connect the input population to the two neurons with random weights:\n\n# Projection without homeostatic mechanism\nproj1 = ann.Projection(inp, pop1, ['ampa', 'nmda'], synapse=nearest_neighbour_stdp)\nproj1.connect_all_to_all(ann.Uniform(0.01, 0.03))\n\n# Projection with homeostatic mechanism\nproj2 = ann.Projection(inp, pop2, ['ampa', 'nmda'], synapse=homeo_stdp)\nproj2.connect_all_to_all(weights=ann.Uniform(0.01, 0.03))\n\n&lt;ANNarchy.core.Projection.Projection at 0x10f1ea850&gt;\n\n\nNote that the same weights will target both AMPA and NMDA conductances in the post-synaptic neurons. By default, the argument target of Projection should be a string, but you can also pass a list of strings to reach several conductances with the same weights.\nWe can now compile and simulate for 1000 seconds while recording the relevat information:\n\nann.compile()\n\nCompiling ...  OK \n\n\n\n# Record\nm1 = ann.Monitor(pop1, 'r')\nm2 = ann.Monitor(pop2, 'r')\nm3 = ann.Monitor(proj1[0], 'w', period=1000.)\nm4 = ann.Monitor(proj2[0], 'w', period=1000.)\n\n# Simulate\nT = 1000 # 1000s\nann.simulate(T*1000., True)\n\n# Get the data\ndata1 = m1.get('r')\ndata2 = m2.get('r')\ndata3 = m3.get('w')\ndata4 = m4.get('w')\n\nprint('Mean Firing Rate without homeostasis:', np.mean(data1[:, 0]))\nprint('Mean Firing Rate with homeostasis:', np.mean(data2[:, 0]))\n\nSimulating 1000.0 seconds of the network took 1.0502750873565674 seconds. \nMean Firing Rate without homeostasis: 55.75053480000002\nMean Firing Rate with homeostasis: 35.2554836\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\n\nplt.subplot(311)\nplt.plot(np.linspace(0, T, len(data1[:, 0])), data1[:, 0], 'r-', label=\"Without homeostasis\")\nplt.plot(np.linspace(0, T, len(data2[:, 0])), data2[:, 0], 'b-', label=\"With homeostasis\")\nplt.xlabel('Time (s)')\nplt.ylabel('Firing rate (Hz)')\n\nplt.subplot(312)\nplt.plot(data3[-1, :], 'r-')\nplt.plot(data4[-1, :], 'bx')\naxes = plt.gca()\naxes.set_ylim([0., 0.035])\nplt.xlabel('# neuron')\nplt.ylabel('Weights after 1000s')\n\nplt.subplot(313)\nplt.imshow(np.array(data4, dtype='float').T, aspect='auto', cmap='hot')\nplt.xlabel('Time (s)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\n\n\n\n\n\nWe see that without homeostasis, the post-synaptic neuron reaches quickly a firing of 55 Hz, with all weights saturating at their maximum value 0.03. This is true even for inputs as low as 0.2Hz.\nMeanwhile, with homeostasis, the post-synaptic neuron gets a firing rate of 35 Hz (its desired value), and the weights from the input population are proportional to the underlying activity."
  },
  {
    "objectID": "notebooks/v4/SORF.html",
    "href": "notebooks/v4/SORF.html",
    "title": "Homeostatic STDP: SORF model",
    "section": "",
    "text": "#!pip install ANNarchy\n\nReimplementation of the SORF model published in:\n\nCarlson, K.D.; Richert, M.; Dutt, N.; Krichmar, J.L., “Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks,” in Neural Networks (IJCNN), The 2013 International Joint Conference on , vol., no., pp.1-8, 4-9 Aug. 2013. doi: 10.1109/IJCNN.2013.6706961\n\n\nimport numpy as np\nimport ANNarchy as ann\n\n\nnb_neuron = 4 # Number of exc and inh neurons\nsize = (32, 32) # input size\nfreq = 1.2 # nb_cycles/half-image\nnb_stim = 40 # Number of grating per epoch\nnb_epochs = 20 # Number of epochs\nmax_freq = 28. # Max frequency of the poisson neurons\nT = 10000. # Period for averaging the firing rate\n\n\n# Izhikevich Coba neuron with AMPA, NMDA and GABA receptors\nRSNeuron = ann.Neuron(\n    parameters = \"\"\"\n        a = 0.02 : population\n        b = 0.2 : population\n        c = -65. : population\n        d = 8. : population\n        tau_ampa = 5. : population\n        tau_nmda = 150. : population\n        tau_gabaa = 6. : population\n        tau_gabab = 150. : population\n        vrev_ampa = 0.0 : population\n        vrev_nmda = 0.0 : population\n        vrev_gabaa = -70.0 : population\n        vrev_gabab = -90.0 : population\n    \"\"\" ,\n    equations=\"\"\"\n        # Inputs\n        I = g_ampa * (vrev_ampa - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev_nmda -v) + g_gabaa * (vrev_gabaa - v) + g_gabab * (vrev_gabab -v)\n        # Midpoint scheme\n        dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., min=-90., midpoint\n        du/dt = a * (b*v - u) : init=-13., midpoint\n        # Conductances\n        tau_ampa * dg_ampa/dt = -g_ampa : exponential\n        tau_nmda * dg_nmda/dt = -g_nmda : exponential\n        tau_gabaa * dg_gabaa/dt = -g_gabaa : exponential\n        tau_gabab * dg_gabab/dt = -g_gabab : exponential\n    \"\"\" ,\n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\",\n    reset = \"\"\"\n        v = c\n        u += d\n        g_ampa = 0.0\n        g_nmda = 0.0\n        g_gabaa = 0.0\n        g_gabab = 0.0\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\",\n    refractory=1.0\n)\n\n\n# STDP with homeostatic regulation\nhomeo_stdp = ann.Synapse(\n    parameters=\"\"\"\n        # STDP\n        tau_plus  = 60. : projection\n        tau_minus = 90. : projection\n        A_plus  = 0.000045 : projection\n        A_minus = 0.00003 : projection\n\n        # Homeostatic regulation\n        alpha = 0.1 : projection\n        beta = 50.0 : projection # &lt;- Difference with the original implementation\n        gamma = 50.0 : projection\n        Rtarget = 10. : projection\n        T = 10000. : projection\n    \"\"\",\n    equations = \"\"\"\n        # Homeostatic values\n        R = post.r : postsynaptic\n        K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic\n        # Nearest-neighbour\n        stdp = if t_post &gt;= t_pre: ltp else: - ltd \n        w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=0.0, max=10.0\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",\n    post_spike=\"\"\"\n        ltd = A_minus\n    \"\"\"\n)\n\n\n# Input population\nOnPoiss = ann.PoissonPopulation(size, rates=1.0)\nOffPoiss = ann.PoissonPopulation(size, rates=1.0)\n\n# RS neuron for the input buffers\nOnBuffer = ann.Population(size, RSNeuron)\nOffBuffer = ann.Population(size, RSNeuron)\n\n# Connect the buffers\nOnPoissBuffer = ann.Projection(OnPoiss, OnBuffer, ['ampa', 'nmda'])\nOnPoissBuffer.connect_one_to_one(ann.Uniform(0.2, 0.6))\nOffPoissBuffer = ann.Projection(OffPoiss, OffBuffer, ['ampa', 'nmda'])\nOffPoissBuffer.connect_one_to_one(ann.Uniform(0.2, 0.6))\n\n# Excitatory and inhibitory neurons\nExc = ann.Population(nb_neuron, RSNeuron)\nInh = ann.Population(nb_neuron, RSNeuron)\nExc.compute_firing_rate(T)\nInh.compute_firing_rate(T)\n\n# Input connections\nOnBufferExc = ann.Projection(OnBuffer, Exc, ['ampa', 'nmda'], homeo_stdp)\nOnBufferExc.connect_all_to_all(ann.Uniform(0.004, 0.015))\nOffBufferExc = ann.Projection(OffBuffer, Exc, ['ampa', 'nmda'], homeo_stdp)\nOffBufferExc.connect_all_to_all(ann.Uniform(0.004, 0.015))\n\n# Competition\nExcInh = ann.Projection(Exc, Inh, ['ampa', 'nmda'], homeo_stdp)\nExcInh.connect_all_to_all(ann.Uniform(0.116, 0.403))\n\nExcInh.Rtarget = 75.\nExcInh.tau_plus = 51.\nExcInh.tau_minus = 78.\nExcInh.A_plus = -0.000041\nExcInh.A_minus = -0.000015\n\nInhExc = ann.Projection(Inh, Exc, ['gabaa', 'gabab'])\nInhExc.connect_all_to_all(ann.Uniform(0.065, 0.259))\n\nann.compile()\n\nCompiling ...  OK \n\n\n\n# Inputs\ndef get_grating(theta):\n    x = np.linspace(-1., 1., size[0])\n    y = np.linspace(-1., 1., size[1])\n    xx, yy = np.meshgrid(x, y)\n    z = np.sin(2.*np.pi*(np.cos(theta)*xx + np.sin(theta)*yy)*freq)\n    return np.maximum(z, 0.), -np.minimum(z, 0.0)\n\n# Initial weights\nw_on_start = OnBufferExc.w\nw_off_start = OffBufferExc.w\n\n# Monitors\nm = ann.Monitor(Exc, 'r')\nn = ann.Monitor(Inh, 'r')\no = ann.Monitor(OnBufferExc[0], 'w', period=1000.)\np = ann.Monitor(ExcInh[0], 'w', period=1000.)\n\n# Learning procedure\nfrom time import time\nimport random\ntstart = time()\nstim_order = list(range(nb_stim))\ntry:\n    for epoch in range(nb_epochs):\n        random.shuffle(stim_order)\n        for stim in stim_order:\n            # Generate a grating randomly\n            rates_on, rates_off = get_grating(np.pi*stim/float(nb_stim))\n            # Set it as input to the poisson neurons\n            OnPoiss.rates  = max_freq * rates_on\n            OffPoiss.rates = max_freq * rates_off\n            # Simulate for 2s\n            ann.simulate(2000.)\n            # Relax the Poisson inputs\n            OnPoiss.rates  = 1.\n            OffPoiss.rates = 1.\n            # Simulate for 500ms\n            ann.simulate(500.)\n        print('Epoch', epoch+1, 'done.')\nexcept KeyboardInterrupt:\n    print('Simulation stopped')\n    \nprint('Done in ', time()-tstart)\n\n# Recordings\ndatae = m.get('r')\ndatai = n.get('r')\ndataw = o.get('w')\ndatal = p.get('w')\n\nEpoch 1 done.\nEpoch 2 done.\nEpoch 3 done.\nEpoch 4 done.\nEpoch 5 done.\nEpoch 6 done.\nEpoch 7 done.\nEpoch 8 done.\nEpoch 9 done.\nEpoch 10 done.\nEpoch 11 done.\nEpoch 12 done.\nEpoch 13 done.\nEpoch 14 done.\nEpoch 15 done.\nEpoch 16 done.\nEpoch 17 done.\nEpoch 18 done.\nEpoch 19 done.\nEpoch 20 done.\nDone in  122.07915997505188\n\n\n\n# Final weights\nw_on_end = OnBufferExc.w\nw_off_end = OffBufferExc.w\n\n# Plot\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 12))\nplt.title('Feedforward weights before and after learning')\nfor i in range(nb_neuron):\n    plt.subplot(3, nb_neuron, i+1)\n    plt.imshow((np.array(w_on_start[i])).reshape((32,32)), aspect='auto', cmap='hot')\n    plt.subplot(3, nb_neuron, nb_neuron + i +1)\n    plt.imshow((np.array(w_on_end[i])).reshape((32,32)), aspect='auto', cmap='hot')\n    plt.subplot(3, nb_neuron, 2*nb_neuron + i +1)\n    plt.imshow((np.array(w_off_end[i])).reshape((32,32)), aspect='auto', cmap='hot')\n\nplt.figure(figsize=(12, 8))\nplt.plot(datae[:, 0], label='Exc')\nplt.plot(datai[:, 0], label='Inh')\nplt.title('Mean FR of the Exc and Inh neurons')\nplt.legend()\n\nplt.figure(figsize=(12, 8))\nplt.subplot(121)\nplt.imshow(np.array(dataw, dtype='float').T, aspect='auto', cmap='hot')\nplt.title('Timecourse of feedforward weights')\nplt.colorbar()\nplt.subplot(122)\nplt.imshow(np.array(datal, dtype='float').T, aspect='auto', cmap='hot')\nplt.title('Timecourse of inhibitory weights')\nplt.colorbar()\nplt.show()\n\n/var/folders/6w/6msx49ws7k13cc0bbys0tt4m0000gn/T/ipykernel_8956/2229657718.py:11: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(3, nb_neuron, i+1)"
  },
  {
    "objectID": "notebooks/v4/ANN2SNN2.html",
    "href": "notebooks/v4/ANN2SNN2.html",
    "title": "ANN-to-SNN conversion - CNN",
    "section": "",
    "text": "This notebook demonstrates how to transform a CNN trained using tensorflow/keras into an SNN network usable in ANNarchy.\nThe CNN is adapted from the original model used in:\n#!pip install ANNarchy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nprint(f\"Tensorflow {tf.__version__}\")\n\nTensorflow 2.16.2\n# Download data\n(X_train, t_train), (X_test, t_test) = tf.keras.datasets.mnist.load_data()\n\n# Normalize inputs\nX_train = X_train.astype('float32') / 255.\nX_test = X_test.astype('float32') / 255.\n\n# One-hot output vectors\nT_train = tf.keras.utils.to_categorical(t_train, 10)\nT_test = tf.keras.utils.to_categorical(t_test, 10)"
  },
  {
    "objectID": "notebooks/v4/ANN2SNN2.html#training-an-ann-in-tensorflowkeras",
    "href": "notebooks/v4/ANN2SNN2.html#training-an-ann-in-tensorflowkeras",
    "title": "ANN-to-SNN conversion - CNN",
    "section": "Training an ANN in tensorflow/keras",
    "text": "Training an ANN in tensorflow/keras\nThe tensorflow.keras convolutional network is built using the functional API.\nThe CNN has three 5*5 convolutional layers with ReLU, each followed by 2*2 max-pooling, no bias, dropout at 0.25, and a softmax output layer with 10 neurons. We use the standard SGD optimizer and the categorical crossentropy loss for classification.\n\ndef create_cnn():\n    \n    inputs = tf.keras.Input(shape = (28, 28, 1))\n    x = tf.keras.layers.Conv2D(\n        16, \n        kernel_size=(5,5),\n        activation='relu',\n        padding = 'same',\n        use_bias=False)(inputs)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Conv2D(\n        64,\n        kernel_size=(5,5),\n        activation='relu',\n        padding = 'same',\n        use_bias=False)(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Conv2D(\n        64,\n        kernel_size=(5,5),\n        activation='relu',\n        padding = 'same',\n        use_bias=False)(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Dropout(0.25)(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(\n        10,\n        activation='softmax',\n        use_bias=False)(x)\n\n    # Create functional model\n    model= tf.keras.Model(inputs, x)\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n\n    # Loss function\n    model.compile(\n        loss='categorical_crossentropy', # loss function\n        optimizer=optimizer, # learning rule\n        metrics=['accuracy'] # show accuracy\n    )\n    print(model.summary())\n\n    return model\n\n\n# Create model\nmodel = create_cnn()\n\n# Train model\nhistory = model.fit(\n    X_train, T_train,       # training data\n    batch_size=128,          # batch size\n    epochs=20,              # Maximum number of epochs\n    validation_split=0.1,   # Percentage of training data used for validation\n)\n\nmodel.save(\"runs/cnn.keras\")\n\n# Test model\npredictions_keras = model.predict(X_test, verbose=0)\ntest_loss, test_accuracy = model.evaluate(X_test, T_test, verbose=0)\nprint(f\"Test accuracy: {test_accuracy}\")\n\nModel: \"functional\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)        │ (None, 28, 28, 1)      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d (Conv2D)                 │ (None, 28, 28, 16)     │           400 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (MaxPooling2D)    │ (None, 14, 14, 16)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (Conv2D)               │ (None, 14, 14, 64)     │        25,600 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (MaxPooling2D)  │ (None, 7, 7, 64)       │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (Conv2D)               │ (None, 7, 7, 64)       │       102,400 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (MaxPooling2D)  │ (None, 3, 3, 64)       │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 3, 3, 64)       │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (Flatten)               │ (None, 576)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 10)             │         5,760 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 134,160 (524.06 KB)\n\n\n\n Trainable params: 134,160 (524.06 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nNone\n\nEpoch 1/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 37ms/step - accuracy: 0.3226 - loss: 2.0190 - val_accuracy: 0.9087 - val_loss: 0.3620\n\nEpoch 2/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 17s 40ms/step - accuracy: 0.8745 - loss: 0.4172 - val_accuracy: 0.9560 - val_loss: 0.1607\n\nEpoch 3/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9303 - loss: 0.2364 - val_accuracy: 0.9662 - val_loss: 0.1238\n\nEpoch 4/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9459 - loss: 0.1799 - val_accuracy: 0.9745 - val_loss: 0.0967\n\nEpoch 5/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 17s 40ms/step - accuracy: 0.9554 - loss: 0.1512 - val_accuracy: 0.9742 - val_loss: 0.0908\n\nEpoch 6/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 17s 41ms/step - accuracy: 0.9601 - loss: 0.1332 - val_accuracy: 0.9775 - val_loss: 0.0792\n\nEpoch 7/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9636 - loss: 0.1189 - val_accuracy: 0.9788 - val_loss: 0.0732\n\nEpoch 8/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 39ms/step - accuracy: 0.9671 - loss: 0.1061 - val_accuracy: 0.9783 - val_loss: 0.0713\n\nEpoch 9/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 17s 40ms/step - accuracy: 0.9692 - loss: 0.0988 - val_accuracy: 0.9797 - val_loss: 0.0690\n\nEpoch 10/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 18s 43ms/step - accuracy: 0.9712 - loss: 0.0937 - val_accuracy: 0.9825 - val_loss: 0.0615\n\nEpoch 11/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 17s 39ms/step - accuracy: 0.9724 - loss: 0.0876 - val_accuracy: 0.9820 - val_loss: 0.0617\n\nEpoch 12/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 18s 42ms/step - accuracy: 0.9745 - loss: 0.0821 - val_accuracy: 0.9833 - val_loss: 0.0569\n\nEpoch 13/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 19s 44ms/step - accuracy: 0.9760 - loss: 0.0780 - val_accuracy: 0.9810 - val_loss: 0.0594\n\nEpoch 14/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 39ms/step - accuracy: 0.9763 - loss: 0.0768 - val_accuracy: 0.9832 - val_loss: 0.0581\n\nEpoch 15/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 18s 42ms/step - accuracy: 0.9777 - loss: 0.0730 - val_accuracy: 0.9818 - val_loss: 0.0633\n\nEpoch 16/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 17s 40ms/step - accuracy: 0.9783 - loss: 0.0688 - val_accuracy: 0.9830 - val_loss: 0.0535\n\nEpoch 17/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 19s 46ms/step - accuracy: 0.9783 - loss: 0.0680 - val_accuracy: 0.9840 - val_loss: 0.0540\n\nEpoch 18/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 17s 39ms/step - accuracy: 0.9810 - loss: 0.0623 - val_accuracy: 0.9853 - val_loss: 0.0500\n\nEpoch 19/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9792 - loss: 0.0634 - val_accuracy: 0.9850 - val_loss: 0.0517\n\nEpoch 20/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9812 - loss: 0.0615 - val_accuracy: 0.9850 - val_loss: 0.0500\n\nTest accuracy: 0.9857000112533569\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/ANN2SNN2.html#initialize-the-ann-to-snn-converter",
    "href": "notebooks/v4/ANN2SNN2.html#initialize-the-ann-to-snn-converter",
    "title": "ANN-to-SNN conversion - CNN",
    "section": "Initialize the ANN-to-SNN converter",
    "text": "Initialize the ANN-to-SNN converter\nWe now create an instance of the ANN-to-SNN conversion object.\n\nfrom ANNarchy.extensions.ann_to_snn_conversion import ANNtoSNNConverter\n\nsnn_converter = ANNtoSNNConverter(\n    input_encoding='IB', \n    hidden_neuron='IaF',\n    read_out='spike_count',\n)\n\nANNarchy 4.8 (4.8.1) on darwin (posix).\n\n\n\nnet = snn_converter.load_keras_model(\"runs/cnn.keras\", show_info=True)\n\nWARNING: Dense representation is an experimental feature for spiking models, we greatly appreciate bug reports. \n* Input layer: input_layer, (28, 28, 1)\n* InputLayer skipped.\n* Conv2D layer: conv2d, (28, 28, 16) \n* MaxPooling2D layer: max_pooling2d, (14, 14, 16) \n* Conv2D layer: conv2d_1, (14, 14, 64) \n* MaxPooling2D layer: max_pooling2d_1, (7, 7, 64) \n* Conv2D layer: conv2d_2, (7, 7, 64) \n* MaxPooling2D layer: max_pooling2d_2, (3, 3, 64) \n* Dropout skipped.\n* Flatten skipped.\n* Dense layer: dense, 10 \n    weights: (10, 576)\n    mean 5.032593981013633e-05, std 0.06920499354600906\n    min -0.26682066917419434, max 0.22082802653312683\n\n\n\n\npredictions_snn = snn_converter.predict(X_test[:300], duration_per_sample=200)\n\n300/300\n\n\nUsing the recorded predictions, we can now compute the accuracy using scikit-learn for all presented samples.\n\nfrom sklearn.metrics import classification_report, accuracy_score\n\nprint(classification_report(t_test[:300], predictions_snn))\nprint(\"Test accuracy of the SNN:\", accuracy_score(t_test[:300], predictions_snn))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        24\n           1       1.00      0.95      0.97        41\n           2       0.97      1.00      0.98        32\n           3       1.00      1.00      1.00        24\n           4       1.00      0.95      0.97        37\n           5       1.00      1.00      1.00        29\n           6       1.00      1.00      1.00        24\n           7       1.00      1.00      1.00        34\n           8       0.91      1.00      0.95        21\n           9       0.97      1.00      0.99        34\n\n    accuracy                           0.99       300\n   macro avg       0.99      0.99      0.99       300\nweighted avg       0.99      0.99      0.99       300\n\nTest accuracy of the SNN: 0.9866666666666667"
  },
  {
    "objectID": "notebooks/v4/Hybrid.html",
    "href": "notebooks/v4/Hybrid.html",
    "title": "Hybrid network",
    "section": "",
    "text": "#!pip install ANNarchy\n\nSimple example showing hybrid spike/rate-coded networks.\nReproduces Fig.4 of (Vitay, Dinkelbach and Hamker, 2015)\n\nimport numpy as np\nimport ANNarchy as ann\n\nann.clear()\nann.setup(dt=0.1)\n\n# Rate-coded input neuron\ninput_neuron = ann.Neuron(\n    parameters = \"baseline = 0.0\",\n    equations = \"r = baseline\"\n)\n# Rate-coded output neuron\nsimple_neuron = ann.Neuron(\n    equations = \"r = sum(exc)\"\n)\n\n# Rate-coded population for input\npop1 = ann.Population(geometry=1, neuron=input_neuron)\n\n# Poisson Population to encode\npop2 = ann.PoissonPopulation(geometry=1000, target=\"exc\")\nproj = ann.Projection(pop1, pop2, 'exc').connect_all_to_all(weights=1.)\n\n# Rate-coded population to decode\npop3 = ann.Population(geometry=1000, neuron =simple_neuron)\nproj = ann.DecodingProjection(pop2, pop3, 'exc', window=10.0)\n\ndef diagonal(pre, post, weights):\n    \"\"\"\n    Simple connector pattern to progressively connect each post-synaptic neuron to a growing number of pre-synaptic neurons.\n    \"\"\"\n    lil = ann.LILConnectivity()\n    for rk_post in range(post.size):\n        lil.add(rk_post, range((rk_post+1)), [weights], [0] )\n    return lil\nproj.connect_with_func(method=diagonal, weights=1.)\n\nann.compile()\n\n# Monitors\nm1 = ann.Monitor(pop1, 'r')\nm2 = ann.Monitor(pop2, 'spike')\nm3 = ann.Monitor(pop3, 'r')\n\n# Simulate\nduration = 250.\n# 0 Hz\npop1.baseline = 0.0\nann.simulate(duration)\n# 10 Hz\npop1.baseline = 10.0\nann.simulate(duration)\n# 50 Hz\npop1.baseline = 50.0\nann.simulate(duration)\n# 100 Hz\npop1.baseline = 100.0\nann.simulate(duration)\n\n# Get recordings\ndata1 = m1.get()\ndata2 = m2.get()\ndata3 = m3.get()\n\n# Raster plot of the spiking population\nt, n = m2.raster_plot(data2['spike'])\n\n# Variance of the the decoded firing rate\ndata_10 = data3['r'][int(1.0*duration/ann.dt()):int(2*duration/ann.dt()), :]\ndata_50 = data3['r'][int(2.0*duration/ann.dt()):int(3*duration/ann.dt()), :]\ndata_100 = data3['r'][int(3.0*duration/ann.dt()):int(4*duration/ann.dt()), :]\nvar_10 = np.mean(np.abs((data_10 - 10.)/10.), axis=0)\nvar_50 = np.mean(np.abs((data_50 - 50.)/50.), axis=0)\nvar_100 = np.mean(np.abs((data_100 - 100.)/100.), axis=0)\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\nCompiling ...  OK \n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 15))\nplt.subplot(3,1,1)\nplt.plot(t, n, '.', markersize=0.5)\nplt.title('a) Raster plot')\nplt.xlabel('Time (ms)')\nplt.ylabel('# neurons')\nplt.xlim((0, 4*duration))\n\nplt.subplot(3,1,2)\nplt.plot(np.arange(0, 4*duration, 0.1), data1['r'][:, 0], label='Original firing rate')\nplt.plot(np.arange(0, 4*duration, 0.1), data3['r'][:, 999], label='Decoded firing rate')\nplt.legend(frameon=False, loc=2)\nplt.title('b) Decoded firing rate')\nplt.xlabel('Time (ms)')\nplt.ylabel('Activity (Hz)')\n\nplt.subplot(3,1,3)\nplt.plot(var_10, label='10 Hz')\nplt.plot(var_50, label='50 Hz')\nplt.plot(var_100, label='100 Hz')\nplt.legend(frameon=False)\nplt.title('c) Precision')\nplt.xlabel('# neurons used for decoding')\nplt.ylabel('Normalized error')\nplt.ylim((0,1))\n\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/STP.html",
    "href": "notebooks/v4/STP.html",
    "title": "Short-term Plasticity and Synchrony",
    "section": "",
    "text": "#!pip install ANNarchy\n\nImplementation of the recurrent network with short-term plasticity (STP) proposed in:\n\nTsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses, The Journal of Neuroscience, 20(50).\n\n\nimport numpy as np\nimport ANNarchy as ann\nann.clear()\n\ndt=0.25\nann.setup(dt=dt)\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\n\n\nThis network uses simple leaky integrate-and-fire (LIF) neurons:\n\nLIF = ann.Neuron(\n    parameters = \"\"\"\n        tau = 30.0 : population\n        I = 15.0\n        tau_I = 3.0 : population\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt = -v + g_exc - g_inh + I : init=13.5\n        \n        tau_I * dg_exc/dt = -g_exc\n        tau_I * dg_inh/dt = -g_inh\n    \"\"\",\n    spike = \"v &gt; 15.0\",\n    reset = \"v = 13.5\",\n    refractory = 3.0\n)\n\nP = ann.Population(geometry=500, neuron=LIF)\nP.I = np.sort(ann.Uniform(14.625, 15.375).get_values(500))\nP.v = ann.Uniform(0.0, 15.0)\nExc = P[:400]\nInh = P[400:]\n\nShort-term plasticity can be defined by dynamical changes of synaptic efficiency, based on pre- or post-synaptic activity.\nWe define a STP synapse, whose post-pynaptic potential (psp, define by g_target) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u:\n\nSTP = ann.Synapse(\n    parameters = \"\"\"\n        tau_rec = 1.0\n        tau_facil = 1.0\n        U = 0.1\n    \"\"\",\n    equations = \"\"\"\n        dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n        du/dt = (U - u)/tau_facil : init = 0.1, event-driven   \n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w * u * x\n        x *= (1 - u)\n        u += U * (1 - u)\n    \"\"\"\n)\n\nCreating the projection between the excitatory and inhibitory is straightforward when the right parameters are chosen:\n\n# Parameters for the synapses\nAee = 1.8\nAei = 5.4\nAie = 7.2\nAii = 7.2\n\nUee = 0.5\nUei = 0.5\nUie = 0.04\nUii = 0.04\n\ntau_rec_ee = 800.0\ntau_rec_ei = 800.0\ntau_rec_ie = 100.0\ntau_rec_ii = 100.0\n\ntau_facil_ie = 1000.0\ntau_facil_ii = 1000.0\n\n# Create projections\nproj_ee = ann.Projection(pre=Exc, post=Exc, target='exc', synapse=STP)\nproj_ee.connect_fixed_probability(probability=0.1, weights=ann.Normal(Aee, (Aee/2.0), min=0.2*Aee, max=2.0*Aee)) \nproj_ee.U = ann.Normal(Uee, (Uee/2.0), min=0.1, max=0.9)\nproj_ee.tau_rec = ann.Normal(tau_rec_ee, (tau_rec_ee/2.0), min=5.0)\nproj_ee.tau_facil = dt # Cannot be 0!\n\nproj_ei = ann.Projection(pre=Inh, post=Exc, target='inh', synapse=STP)\nproj_ei.connect_fixed_probability(probability=0.1, weights=ann.Normal(Aei, (Aei/2.0), min=0.2*Aei, max=2.0*Aei))\nproj_ei.U = ann.Normal(Uei, (Uei/2.0), min=0.1, max=0.9)\nproj_ei.tau_rec = ann.Normal(tau_rec_ei, (tau_rec_ei/2.0), min=5.0)\nproj_ei.tau_facil = dt # Cannot be 0!\n\nproj_ie = ann.Projection(pre=Exc, post=Inh, target='exc', synapse=STP)\nproj_ie.connect_fixed_probability(probability=0.1, weights=ann.Normal(Aie, (Aie/2.0), min=0.2*Aie, max=2.0*Aie))\nproj_ie.U = ann.Normal(Uie, (Uie/2.0), min=0.001, max=0.07)\nproj_ie.tau_rec = ann.Normal(tau_rec_ie, (tau_rec_ie/2.0), min=5.0)\nproj_ie.tau_facil = ann.Normal(tau_facil_ie, (tau_facil_ie/2.0), min=5.0)\n\nproj_ii = ann.Projection(pre=Inh, post=Inh, target='inh', synapse=STP)\nproj_ii.connect_fixed_probability(probability=0.1, weights=ann.Normal(Aii, (Aii/2.0), min=0.2*Aii, max=2.0*Aii))\nproj_ii.U = ann.Normal(Uii, (Uii/2.0), min=0.001, max=0.07)\nproj_ii.tau_rec = ann.Normal(tau_rec_ii, (tau_rec_ii/2.0), min=5.0)\nproj_ii.tau_facil = ann.Normal(tau_facil_ii, (tau_facil_ii/2.0), min=5.0)\n\nWe compile and simulate for 10 seconds:\n\n# Compile\nann.compile()\n\n# Record\nMe = ann.Monitor(Exc, 'spike')\nMi = ann.Monitor(Inh, 'spike')\n\n# Simulate\nduration = 10000.0\nann.simulate(duration, measure_time=True)\n\nCompiling ...  OK \nSimulating 10.0 seconds of the network took 0.07543301582336426 seconds. \n\n\nWe retrieve the recordings and plot them:\n\n# Retrieve recordings\ndata_exc = Me.get()\ndata_inh = Mi.get()\nte, ne = Me.raster_plot(data_exc['spike'])\nti, ni = Mi.raster_plot(data_inh['spike'])\n\n# Histogram of the exc population\nh = Me.histogram(data_exc['spike'], bins=1.0)\n\n# Mean firing rate of each excitatory neuron\nrates = []\nfor neur in data_exc['spike'].keys():\n    rates.append(len(data_exc['spike'][neur])/duration*1000.0)\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 10))\nplt.subplot(3,1,1)\nplt.plot(te, ne, 'b.', markersize=1.0)\nplt.plot(ti, ni, 'b.', markersize=1.0)\nplt.xlim((0, duration)); plt.ylim((0,500))\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\n\nplt.subplot(3,1,2)\nplt.plot(h/400.)\nplt.xlabel('Time (ms)')\nplt.ylabel('Net activity')\n\nplt.subplot(3,1,3)\nplt.plot(sorted(rates))\nplt.ylabel('Spikes / sec')\nplt.xlabel('# neuron')\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/SynapticTransmission.html",
    "href": "notebooks/v4/SynapticTransmission.html",
    "title": "Synaptic transmission",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis notebook simply demonstrates the three main type of synaptic transmission for spiking neurons:\n\nInstantaneous\nExponentially-decreasing\nAlpha-shaped\n\n\nimport numpy as anp\n\nimport ANNarchy as ann\nann.clear()\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\n\n\nWe use here a simple LIF neuron receving three types of projections (a, b, c). The conductance g_a uses instantaneous transmission, as it is reset to 0 after each step. g_b decreases exponentially with time following a first order ODE. g_c is integrated twice in alpha_c, leading to the alpha shape.\nAll methods use the exponential numerical method, as they are first order linear ODEs and can be solved exactly.\n\nLIF = ann.Neuron(\n    parameters=\"\"\"\n        tau = 20.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        tau_b = 10.0\n        tau_c = 10.0\n    \"\"\",\n    equations=\"\"\"\n        # Membrane potential\n        tau * dv/dt = (E_L - v) + g_a + g_b + alpha_c : init=-70.\n        \n        # Exponentially decreasing\n        tau_b * dg_b/dt = -g_b : exponential\n        \n        # Alpha-shaped\n        tau_c * dg_c/dt = -g_c : exponential\n        tau_c * dalpha_c/dt = exp((tau_c - dt/2.0)/tau_c) * g_c - alpha_c  : exponential\n    \"\"\",\n    spike=\"v &gt;= v_T\",\n    reset=\"v = v_r\",\n    refractory = 2.0\n)\n\nThe LIF neuron will receive a single spike at t = 10 ms, using the SpikeSourceArray specific population.\n\ninp = ann.SpikeSourceArray([10.])\npop = ann.Population(1, LIF)\n\nWe implement three different projections between the same neurons, to highlight the three possible transmission mechanisms.\n\nproj = ann.Projection(inp, pop, 'a')\nproj.connect_all_to_all(weights=1.0)\n\nproj = ann.Projection(inp, pop, 'b')\nproj.connect_all_to_all(weights=1.0)\n\nproj = ann.Projection(inp, pop, 'c')\nproj.connect_all_to_all(weights=1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x10d5ea0d0&gt;\n\n\n\nann.compile()\n\nCompiling ...  OK \n\n\nWe monitor the three conductances:\n\nm = ann.Monitor(pop, ['g_a', 'g_b', 'alpha_c'])\n\n\ninp.clear()\nann.simulate(100.)\n\n\ndata = m.get()\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nplt.subplot(311)\nplt.plot(data['g_a'][:, 0])\nplt.ylabel(\"Instantaneous\")\nplt.subplot(312)\nplt.plot(data['g_b'][:, 0])\nplt.ylabel(\"Exponential\")\nplt.subplot(313)\nplt.plot(data['alpha_c'][:, 0])\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"Alpha\")\nplt.show()"
  },
  {
    "objectID": "notebooks/BasalGanglia.html",
    "href": "notebooks/BasalGanglia.html",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "The tensorboard extension allows to log various information (scalars, images, etc) during training for visualization using tensorboard.\n\n#!pip install ANNarchy\n\nThe extension has to be explicitly imported after ANNarchy:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tqdm\n\nimport ANNarchy as ann\nfrom ANNarchy.extensions.tensorboard import Logger\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nAs it is just for demonstration purposes, we will be an extremely simplified model of the basal ganglia learning to solve through reinforcement learning a stimulus-response task with 4 stimuli and 2 responses (left and right). The two first stimuli should be responded with left, the two others with right.\n\nstimuli = [\n    ([1, 0, 0, 0], 0), # A : left\n    ([0, 1, 0, 0], 0), # B : left\n    ([0, 0, 1, 0], 1), # C : right\n    ([0, 0, 0, 1], 1), # D : right\n]\n\nWe keep here the model as simple as possible. It is inspired from the rate-coded model described here:\n\nVitay J, Hamker FH. 2010. A computational model of Basal Ganglia and its role in memory retrieval in rewarded visual memory tasks. Frontiers in computational neuroscience 4. doi:10.3389/fncom.2010.00013\n\n\nnet = ann.Network()\n\nThe input population is composed of 4 static neurons to represent the inputs:\n\ncortex = net.create(4, ann.Neuron(parameters=\"r=0.0\"))\n\nThe cortex projects on the striatum, which is composed of 10 neurons integrating excitatory and inhibitory inputs:\n\nmsn = ann.Neuron(\n    parameters = dict(tau = 10.0, noise = 0.1), \n    equations = [\n        'tau * dv/dt + v = sum(exc) - sum(inh) + noise * Uniform(-1, 1)',\n        'r = clip(v, 0.0, 1.0)',\n    ]\n)\nstriatum = net.create(10, msn)\n\nThe striatum projects inhibitorily on GPi, whose neurons are tonically active (high baseline). Normally, GPi would project on the thalamus and back to the cortex, but here we read the output of the network directly in GPi: if the first neuron (corresponding to the left action) is less active than the second neuron, the selected action is left.\n\ngp_neuron = ann.Neuron(\n    parameters = dict(tau = 10.0, B = 1.0),\n    equations = [\n        \"tau * dv/dt + v = B - sum(inh)\",\n        \"r= pos(v)\"\n    ]\n)\ngpi = net.create(2, gp_neuron)\n\nLearning occurs at the cortico-striatal synapses, using a reward-modulated Hebbian learning rule, with Oja regularization:\n\ncorticostriatal = ann.Synapse(\n    parameters = dict(\n        eta = 0.1,\n        alpha = 0.5,\n        dopamine = 0.0\n    ),\n    equations = ann.Variable(\"w += eta * (dopamine * pre.r * post.r - alpha*w*post.r*post.r)\", min=0.0)\n)\ncx_str = net.connect(cortex, striatum, \"exc\", corticostriatal)\ncx_str.all_to_all(weights=ann.Uniform(0.0, 0.5))\n\n&lt;ANNarchy.core.Projection.Projection at 0x157416990&gt;\n\n\nSome lateral competition between the striatal neurons:\n\nstr_str = net.connect(striatum, striatum, \"inh\")\nstr_str.all_to_all(weights=0.6)\n\n&lt;ANNarchy.core.Projection.Projection at 0x156e329f0&gt;\n\n\nOne half of the striatal population is connected to the left GPi neuron, the other half to the right neuron:\n\nstr_gpi1 = net.connect(striatum[:int(striatum.size/2)], gpi[0], 'inh')\nstr_gpi1.all_to_all(1.0)\n\nstr_gpi2 = net.connect(striatum[int(striatum.size/2):], gpi[1], 'inh')\nstr_gpi2.all_to_all(1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x15743a570&gt;\n\n\nWe add a monitor on GPi and compile:\n\nm = net.monitor(gpi, 'r')\n\nnet.compile()\n\nCompiling network 1...  OK \n\n\nEach trial is very simple: we get a stimulus x from the stimuli array and a correct response t, reset the network for 40 ms, set the input and simulate for 50 ms, observe the activity in GPi to decide what the answer of the network is, provide reward accordingly to the corticostriatal projection and let learn for 10 ms.\nHere the “dopamine” signal is directly the reward (+1 for success, -1 for failure), not the reward prediction error, but it is just for demonstration.\n\ndef training_trial(x, t):\n    \n    # Delay period\n    cortex.r = 0.0\n    cx_str.dopamine = 0.0\n    net.simulate(40.0)\n    \n    # Set inputs\n    cortex.r = np.array(x)\n    net.simulate(50.0)\n    \n    # Read output\n    output = gpi.r\n    answer = np.argmin(output)\n    \n    # Provide reward\n    reward = 1.0 if answer == t else -1.0\n    cx_str.dopamine = reward\n    net.simulate(10.0)\n    \n    # Get recordings\n    data = m.get('r')\n    \n    return reward, data\n\nThe whole training procedure will simply iterate over the four stimuli for 100 trials:\nfor trial in range(100):\n    # Get a stimulus\n    x, t = stimuli[trial%len(stimuli)]\n    # Perform a trial\n    reward, data = training_trial(x, t)\nWe use the Logger class of the tensorboard extension to keep track of various data:\nwith Logger() as logger:\n    for trial in range(100):\n        # Get a stimulus\n        x, t = stimuli[trial%len(stimuli)]\n        # Perform a trial\n        reward, data = training_trial(x, t)\n        # Log data...\nNote that it would be equivalent to manually close the Logger after training:\nlogger = Logger()\nfor trial in range(100):\n    # Get a stimulus\n    x, t = stimuli[trial%len(stimuli)]\n    # Perform a trial\n    reward, data = training_trial(x, t)\n        # Log data...\nlogger.close()\nWe log here different quantities, just to demonstrate the different methods of the Logger class:\n\nThe reward received after each trial:\n\nlogger.add_scalar(\"Reward\", reward, trial)\nThe tag “Reward” will be the name of the plot in tensorboard. reward is the value that will be displayed, while trial is the index of the current trial (x-axis).\n\nThe activity of the two GPi cells at the end of the trial, in separate plots depending on the stimulus:\n\nif trial%len(stimuli) == 0:\n    label = \"GPi activity/A\"\nelif trial%len(stimuli) == 1:\n    label = \"GPi activity/B\"\nelif trial%len(stimuli) == 2:\n    label = \"GPi activity/C\"\nelif trial%len(stimuli) == 3:\n    label = \"GPi activity/D\"\nlogger.add_scalars(label, {\"Left neuron\": gpi.r[0], \"Right neuron\": gpi.r[1]}, trial)\nThe four plots will be grouped under the label “GPi activity”, with a title A, B, C or D. Note that add_scalars() requires a dictionary of values that will plot together.\n\nThe activity in the striatum as a 2*5 image:\n\nlogger.add_image(\"Activity/Striatum\", striatum.r.reshape((2, 5)), trial)\nThe activity should be reshaped to the correct dimensions. Note that activity in the striatum is bounded between 0 and 1, so there is no need for equalization.\n\nAn histogram of the preference for the stimuli A and B of striatal cells:\n\nw = np.array(cx_str.w)\nlogger.add_histogram(\"Cortico-striatal weights/Left - AB/CD\", np.mean(w[:5, :2] - w[:5, 2:], axis=1), trial)\nlogger.add_histogram(\"Cortico-striatal weights/Right - AB/CD\", np.mean(w[5:, :2] - w[5:, 2:], axis=1), trial)\nWe make here two plots, one for the first 5 striatal cells, the other for the rest. We plot the difference between the mean weights of each cell for the stimuli A and B, and the mean weights for the stimuli C and D. If learning goes well, the first five striatal cells should have stronger weights for A and B than for C and D, as they project to the left GPi cell.\n\nA matplotlib figure showing the time course of the two GPi cells (as recorded by the monitor):\n\nfig = plt.figure(figsize=(10, 8))\nplt.plot(data[:, 0], label=\"left\")\nplt.plot(data[:, 1], label=\"right\")\nplt.legend()\nlogger.add_figure(\"Activity/GPi\", fig, trial)\nNote that the figure will be automatically closed by the logger, no need to call show(). Logging figures is extremely slow, use that feature wisely.\nBy default, the logs are saved in the subfolder runs/, but this can be changed when creating the Logger:\nwith Logger(\"/tmp/experiment\") as logger:\nEach run of the network will be saved in this folder. You may want to delete the folder before each run, in order to only visualize the last run:\n\n%rm -rf runs\n\n\nwith Logger() as logger:\n    \n    for trial in (pbar := tqdm.tqdm(range(100))):\n        \n        # Get a stimulus\n        x, t = stimuli[trial%len(stimuli)]\n    \n        # Perform a trial\n        reward, data = training_trial(x, t)\n        \n        # Log received rewards\n        logger.add_scalar(\"Reward\", reward, trial)\n        pbar.set_description(f\"Reward: {reward}\")\n\n        # Log outputs depending on the task\n        if trial%len(stimuli) == 0:\n            label = \"GPi activity/A\"\n        elif trial%len(stimuli) == 1:\n            label = \"GPi activity/B\"\n        elif trial%len(stimuli) == 2:\n            label = \"GPi activity/C\"\n        elif trial%len(stimuli) == 3:\n            label = \"GPi activity/D\"\n        logger.add_scalars(label, {\"Left neuron\": gpi.r[0], \"Right neuron\": gpi.r[1]}, trial)\n\n        # Log striatal activity as a 2*5 image\n        logger.add_image(\"Activity/Striatum\", striatum.r.reshape((2, 5)), trial)\n\n        # Log histogram of cortico-striatal weights\n        w = np.array(cx_str.w)\n        logger.add_histogram(\"Cortico-striatal weights/Left - AB/CD\", np.mean(w[:5, :2] - w[:5, 2:], axis=1), trial)\n        logger.add_histogram(\"Cortico-striatal weights/Right - AB/CD\", np.mean(w[5:, :2] - w[5:, 2:], axis=1), trial)\n\n        # Log matplotlib figure of GPi activity\n        fig = plt.figure(figsize=(10, 8))\n        plt.plot(data[:, 0], label=\"left\")\n        plt.plot(data[:, 1], label=\"right\")\n        plt.legend()\n        logger.add_figure(\"Activity/GPi\", fig, trial)\n\nLogging in runs/Mar13_12-17-15_Juliens-MacBook-Pro.local\n\n\nReward: 1.0: 100%|██████████| 100/100 [00:08&lt;00:00, 12.16it/s]\n\n\nYou can now visualize the logged information by running tensorboard in a separate terminal and opening the corresponding page:\ntensorboard --logdir runs\nor directly in the notebook if you have the tensorboard extension installed:\n\n%load_ext tensorboard\n%tensorboard --logdir runs --samples_per_plugin images=100\n\nYou should see a tensorboard page with four tabs Scalars, Images, Distributions and Histograms:\n\n\n\ntensorboard.png\n\n\nThe Reward plot shows that the network successfully learns to solve the task, as it consistently gets rewards of +1 (note that this may vary from run to run, depending on weight initialization):\n\n\n\nreward.png\n\n\nThe GPi activity tab shows that the two GPi cells quickly learn to be inhibited for the right stimuli.\n\n\n\ngpi.png\n\n\nIn the Images tab, the plot for the striatum allows to visualize activity at the end of each rtial, showing that only one cell in the correct subpopulation is active:\n\n\n\nstriatum.png\n\n\nThe matplotlib figure for the GPi activity shows what happens during a trial, especially at the end of the reset period:\n\n\n\ntimecourse.png\n\n\nIn the histograms tab, we can see that the left striatal population has acquired a preference (stronger weights) for the stimuli A and B, as the values are positive. The right population has negative values, so the neurons have stronger weights to the stimuli C and D. Note that some neurons in the right population still have stronger weights from A and B, but they are probably inhibited by the left population, so they do not impair performance.\n\n\n\nweights-left.png\n\n\n\n\n\nweights-right.png",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "Tensorboard"
    ]
  },
  {
    "objectID": "notebooks/Miconi.html",
    "href": "notebooks/Miconi.html",
    "title": "Miconi network",
    "section": "",
    "text": "#!pip install ANNarchy\n\nReward-modulated recurrent network based on:\n\nMiconi T. (2017). Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks. eLife 6:e20899. doi:10.7554/eLife.20899\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nEach neuron in the reservoir follows the following equations:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nwhere \\xi(t) is a random perturbation at 3 Hz, with an amplitude randomly sampled between -A and +A.\nWe additionally keep track of the mean firing rate with a sliding average:\n\n    \\tilde{x}(t) = \\alpha \\, \\tilde{x}(t) + (1 - \\alpha) \\, x(t)\n\nThe three first neurons keep a constant rate throughout learning (1 or -1) to provide some bias to the other neurons.\n\nneuron = ann.Neuron(\n    parameters = dict(\n        tau = 30.0, # Time constant\n        constant = ann.Parameter(0.0), # The four first neurons have constant rates\n        alpha = 0.05, # To compute the sliding mean\n        f = 3.0, # Frequency of the perturbation\n        A = 16., # Perturbation amplitude. dt*A/tau should be 0.5...\n    ),\n    equations = [\n        # Perturbation\n        'perturbation = if Uniform(0.0, 1.0) &lt; f/1000.: 1.0 else: 0.0',\n        'noise = if perturbation &gt; 0.5: A * Uniform(-1.0, 1.0) else: 0.0',\n\n        # ODE for x\n        'x += dt*(sum(in) + sum(exc) - x + noise)/tau',\n\n        # Output r\n        'rprev = r', # store r at previous time step\n        'r = if constant == 0.0: tanh(x) else: tanh(constant)',\n\n        # Sliding mean\n        'delta_x = x - x_mean',\n        'x_mean = alpha * x_mean + (1 - alpha) * x',\n    ]\n)\n\nThe learning rule is defined by a trace e_{i, j}(t) for each synapse i \\rightarrow j incremented at each time step with:\n\n    e_{i, j}(t) = e_{i, j}(t-1) + (r_i (t) \\, x_j(t))^3\n\nAt the end T of a trial, the reward R is delivered and all weights are updated using:\n\n    \\Delta w_{i, j} = \\eta \\,  e_{i, j}(T) \\, |R_\\text{mean}| \\,  (R - R_\\text{mean})\n\nwhere R_\\text{mean} is the mean reward for the task. Here the reward is defined as the opposite of the prediction error.\nAll traces are then reset to 0 for the next trial. Weight changes are clamped between -0.0003 and 0.0003.\nAs ANNarchy applies the synaptic equations at each time step, we need to introduce a global boolean learning_phase which performs trace integration when false, and allows weight update when true.\n\nsynapse = ann.Synapse(\n    parameters=dict(\n        eta = 0.5, # Learning rate\n        max_weight_change = 0.0003, # Clip the weight changes\n\n        # Flag to allow learning only at the end of a trial\n        learning_phase = ann.Parameter(False, 'global', 'bool'), \n        reward = 0.0, # Reward received\n        mean_reward = 0.0, # Mean Reward received\n    ),\n    equations = [\n        # Trace\n        \"\"\"\n        trace += if not(learning_phase):\n                    power(pre.rprev * (post.delta_x), 3)\n                 else:\n                    0.0\n        \"\"\",\n\n        # Weight update only at the end of the trial\n        ann.Variable(\"\"\"\n            delta_w = if learning_phase:\n                    eta * trace * fabs(mean_reward) * (reward - mean_reward)\n                else:\n                    0.0 \n        \"\"\", \n        min='-max_weight_change', max='max_weight_change'),\n\n        # Weight update\n        \"w += delta_w\",\n        \n    ]\n)\n\nWe implement the network as a class deriving from ann.Network. The network has two inputs A and B, so we create the corresponding static population. The reservoir has 200 neurons, 3 of which having constant rates to serve as biases for the other neurons.\nInput weights are uniformly distributed between -1 and 1.\nThe recurrent weights are normally distributed, with a coupling strength of g=1.5 (edge of chaos). In the original paper, the projection is fully connected (but self-connections are avoided). Using a sparse (0.1) connectivity matrix leads to similar results and is much faster.\n\nclass MiconiNetwork (ann.Network):\n\n    def __init__(self, N, g, sparseness):\n\n        # Input population\n        self.inp = self.create(2, ann.Neuron(\"r=0.0\"))\n\n        # Recurrent population\n        self.pop = self.create(N, neuron)\n\n        # Biases\n        self.pop[0].constant = 1.0\n        self.pop[1].constant = 1.0\n        self.pop[2].constant = -1.0\n\n        # Input weights\n        self.Wi = self.connect(self.inp, self.pop, 'in')\n        self.Wi.all_to_all(weights=ann.Uniform(-1.0, 1.0))\n\n        # Recurrent weights\n        self.Wrec = self.connect(self.pop, self.pop, 'exc', synapse)\n        if sparseness == 1.0:\n            self.Wrec.all_to_all(weights=ann.Normal(0., g/np.sqrt(N)))\n        else:\n            self.Wrec.fixed_probability(\n                probability=sparseness, \n                weights=ann.Normal(0., g/np.sqrt(sparseness*N))\n            )\n\n        # Monitor\n        self.m = self.monitor(self.pop, ['r'], start=False)\n\n\nnet = MiconiNetwork(N=200, g=1.5, sparseness=0.1)\nnet.compile()\n\nCompiling network 1...  OK \n\n\nThe output of the reservoir is chosen to be the neuron of index 100.\n\nOUTPUT_NEURON = 100\n\nParameters defining the task:\n\n# Durations\nd_stim = 200\nd_delay= 200\nd_response = 200\n\nDefinition of a DNMS trial (AA, AB, BA, BB):\n\ndef dnms_trial(trial_number, input, target, R_mean, record=False, perturbation=True):\n\n    # Switch off perturbations if needed\n    if not perturbation:\n        old_A = net.pop.A\n        net.pop.A = 0.0\n\n    # Reinitialize network\n    net.pop.x = ann.Uniform(-0.1, 0.1).get_values(net.pop.size)\n    net.pop.r = np.tanh(net.pop.x)\n    net.pop[0].r = np.tanh(1.0)\n    net.pop[1].r = np.tanh(1.0)\n    net.pop[2].r = np.tanh(-1.0)\n\n    if record: net.m.resume()\n\n    # First input\n    net.inp[input[0]].r = 1.0\n    net.simulate(d_stim)\n    \n    # Delay\n    net.inp.r = 0.0\n    net.simulate(d_delay)\n    \n    # Second input\n    net.inp[input[1]].r = 1.0\n    net.simulate(d_stim)\n    \n    # Delay\n    net.inp.r = 0.0\n    net.simulate(d_delay)\n    \n    # Response\n    if not record: net.m.resume()\n    net.inp.r = 0.0\n    net.simulate(d_response)\n    \n    # Read the output\n    net.m.pause()\n    recordings = net.m.get('r')\n    \n    # Response is over the last 200 ms\n    output = recordings[-int(d_response):, OUTPUT_NEURON] # neuron 100 over the last 200 ms\n    \n    # Compute the reward as the opposite of the absolute error\n    reward = - np.mean(np.abs(target - output))\n    \n    # The first 25 trial do not learn, to let R_mean get realistic values\n    if trial_number &gt; 25:\n\n        # Apply the learning rule\n        net.Wrec.learning_phase = True\n        net.Wrec.reward = reward\n        net.Wrec.mean_reward = R_mean\n\n        # Learn for one step\n        net.step()\n        \n        # Reset the traces\n        net.Wrec.learning_phase = False\n        net.Wrec.trace = 0.0\n        #_ = m.get() # to flush the recording of the last step\n\n    # Switch back on perturbations if needed\n    if not perturbation:\n        net.pop.A = old_A\n\n    return recordings, reward\n\nLet’s visualize the activity of the output neuron during the first four trials.\n\n# Perform the four different trials successively\ninitialAA, errorAA = dnms_trial(0, [0, 0], -0.98, 0.0, record=True)\ninitialAB, errorAB = dnms_trial(0, [0, 1], +0.98, 0.0, record=True)\ninitialBA, errorBA = dnms_trial(0, [1, 0], +0.98, 0.0, record=True)\ninitialBB, errorBB = dnms_trial(0, [1, 1], -0.98, 0.0, record=True)\n\nplt.figure(figsize=(12, 10))\nax = plt.subplot(221)\nax.plot(initialAA[:, OUTPUT_NEURON])\nax.set_ylim((-1., 1.))\nax.set_title('Output AA -1')\nax = plt.subplot(222)\nax.plot(initialBA[:, OUTPUT_NEURON])\nax.set_ylim((-1., 1.))\nax.set_title('Output BA +1')\nax = plt.subplot(223)\nax.plot(initialAB[:, OUTPUT_NEURON])\nax.set_ylim((-1., 1.))\nax.set_title('Output AB +1')\nax = plt.subplot(224)\nax.plot(initialBB[:, OUTPUT_NEURON])\nax.set_ylim((-1., 1.))\nax.set_title('Output BB -1')\nplt.show()\n\n\n\n\n\n\n\n\nWe can now run the simulation for 1500 trials. Beware, this can take 15 to 20 minutes.\n\n# Compute the mean reward per trial\nR_mean = - np.ones((2, 2))\nalpha = 0.75\n\n# Many trials of each type\nrecord_rewards = []\n\nfor trial in (t := tqdm(range(10000))):\n\n    # Perform the four different trials successively\n    _, rewardAA = dnms_trial(trial, [0, 0], -0.98, R_mean[0, 0])\n\n    _, rewardAB = dnms_trial(trial, [0, 1], +0.98, R_mean[0, 1])\n\n    _, rewardBA = dnms_trial(trial, [1, 0], +0.98, R_mean[1, 0])\n\n    _, rewardBB = dnms_trial(trial, [1, 1], -0.98, R_mean[1, 1])\n\n    # Reward\n    reward = np.array([[rewardAA, rewardBA], [rewardBA, rewardBB]])\n\n    # Update mean reward\n    R_mean = alpha * R_mean + (1.- alpha) * reward\n\n    record_rewards.append(R_mean)\n    t.set_description(\n        f'AA: {R_mean[0, 0]:.2f} AB: {R_mean[0, 1]:.2f} BA: {R_mean[1, 0]:.2f} BB: {R_mean[1, 1]:.2f}'\n    )\n\nAA: -0.07 AB: -0.28 BA: -0.28 BB: -0.06: 100%|██████████| 10000/10000 [13:32&lt;00:00, 12.31it/s]\n\n\n\nrecord_rewards = np.array(record_rewards)\n\nplt.figure(figsize=(10, 6))\nplt.plot(record_rewards[:, 0, 0], label='AA')\nplt.plot(record_rewards[:, 0, 1], label='AB')\nplt.plot(record_rewards[:, 1, 0], label='BA')\nplt.plot(record_rewards[:, 1, 1], label='BB')\nplt.plot(record_rewards.mean(axis=(1,2)), label='mean')\n\nplt.xlabel(\"Trials\")\nplt.ylabel(\"Mean reward\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Perform the four different trials without perturbation for testing\nrecordsAA, errorAA = dnms_trial(0, [0, 0], -0.98, 0.0, record=True, perturbation=False)\nrecordsAB, errorAB = dnms_trial(0, [0, 1], +0.98, 0.0, record=True, perturbation=False)\nrecordsBA, errorBA = dnms_trial(0, [1, 0], +0.98, 0.0, record=True, perturbation=False)\nrecordsBB, errorBB = dnms_trial(0, [1, 1], -0.98, 0.0, record=True, perturbation=False)\n\nplt.figure(figsize=(12, 10))\nplt.subplot(221)\nplt.plot(initialAA[:, OUTPUT_NEURON], label='before')\nplt.plot(recordsAA[:, OUTPUT_NEURON], label='after')\nplt.legend()\nplt.ylim((-1., 1.))\nplt.title('Trial AA : t=-1')\nplt.subplot(222)\nplt.plot(initialBA[:, OUTPUT_NEURON], label='before')\nplt.plot(recordsBA[:, OUTPUT_NEURON], label='after')\nplt.ylim((-1., 1.))\nplt.title('Trial BA : t=+1')\nplt.subplot(223)\nplt.plot(initialAB[:, OUTPUT_NEURON], label='before')\nplt.plot(recordsAB[:, OUTPUT_NEURON], label='after')\nplt.ylim((-1., 1.))\nplt.title('Trial AB : t=+1')\nplt.subplot(224)\nplt.plot(initialBB[:, OUTPUT_NEURON], label='before')\nplt.plot(recordsBB[:, OUTPUT_NEURON], label='after')\nplt.ylim((-1., 1.))\nplt.title('Trial BB : t=-1')\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Miconi network"
    ]
  },
  {
    "objectID": "notebooks/HodgkinHuxley.html",
    "href": "notebooks/HodgkinHuxley.html",
    "title": "Hodgkin Huxley neuron",
    "section": "",
    "text": "#!pip install ANNarchy\n\nSimple Hodgkin-Huxley neuron.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ANNarchy as ann\n\nHH = ann.Neuron(\n\n    parameters = dict(\n        C = 1.0, # Capacitance\n        VL = -59.387, # Leak voltage\n        VK = -82.0, # Potassium reversal voltage\n        VNa = 45.0, # Sodium reveral voltage\n        gK = 36.0, # Maximal Potassium conductance\n        gNa = 120.0, # Maximal Sodium conductance\n        gL = 0.3, # Leak conductance\n        vt = 30.0, # Threshold for spike emission\n        I = 0.0, # External current\n    ),\n\n    equations = [\n        # Previous membrane potential\n        'prev_V = V',\n\n        # Voltage-dependency parameters\n        \"\"\"\n        an = 0.01 * (V + 60.0) / (1.0 - exp(-0.1* (V + 60.0) ) )\n        am = 0.1 * (V + 45.0) / (1.0 - exp (- 0.1 * ( V + 45.0 )))\n        ah = 0.07 * exp(- 0.05 * ( V + 70.0 ))\n\n        bn = 0.125 * exp (- 0.0125 * (V + 70.0))\n        bm = 4.0 *  exp (- (V + 70.0) / 80.0)\n        bh = 1.0/(1.0 + exp (- 0.1 * ( V + 40.0 )) )\n        \"\"\",\n\n        # Alpha/Beta functions\n        ann.Variable('dn/dt = an * (1.0 - n) - bn * n', init = 0.3, method='midpoint'),\n        ann.Variable('dm/dt = am * (1.0 - m) - bm * m', init = 0.0, method='midpoint'),\n        ann.Variable('dh/dt = ah * (1.0 - h) - bh * h', init = 0.6, method='midpoint'),\n\n        # Membrane equation\n        ann.Variable('C * dV/dt = gL * (VL - V ) + gK * n**4 * (VK - V) + gNa * m**3 * h * (VNa - V) + I', method='midpoint'),\n    ],\n\n    spike = \"\"\"\n        # Spike is emitted when the membrane potential crosses the threshold from below\n        (V &gt; vt) and (prev_V &lt;= vt)    \n    \"\"\",\n\n    reset = \"\"\"\n        # Nothing to do, it is built-in...\n    \"\"\"\n)\n\nnet = ann.Network(dt=0.01)\n\npop = net.create(neuron=HH, geometry=1)\npop.V = -50.0\n\nnet.compile()\n\nm = net.monitor(pop, ['spike', 'V', 'n', 'm', 'h'])\n\n# Preparation\nnet.simulate(100.0)\n# Current impulse for 1 ms\npop.I = 200.0\nnet.simulate(1.0)\n# Reset\npop.I = 0.0\nnet.simulate(100.0)\n\ndata = m.get()\n\ntstart = int(90.0/net.dt)\ntstop  = int(120.0/net.dt)\n\nplt.figure(figsize=(15, 10))\nplt.subplot(2,2,1)\nplt.plot(90.0 + net.dt*np.arange(tstop-tstart), data['V'][tstart:tstop, 0])\nplt.title('V')\nplt.subplot(2,2,2)\nplt.plot(90.0 + net.dt*np.arange(tstop-tstart), data['n'][tstart:tstop, 0])\nplt.title('n')\nplt.ylim((0.0, 1.0))\nplt.subplot(2,2,3)\nplt.plot(90.0 + net.dt*np.arange(tstop-tstart), data['m'][tstart:tstop, 0])\nplt.title('m')\nplt.ylim((0.0, 1.0))\nplt.subplot(2,2,4)\nplt.plot(90.0 + net.dt*np.arange(tstop-tstart), data['h'][tstart:tstop, 0])\nplt.title('h')\nplt.ylim((0.0, 1.0))\nplt.show()\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\nCompiling network 1...  OK",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Hodgkin-Huxley"
    ]
  },
  {
    "objectID": "notebooks/BCM.html",
    "href": "notebooks/BCM.html",
    "title": "BCM learning rule",
    "section": "",
    "text": "The goal of this notebook is to investigate the Intrator & Cooper BCM learning rule for rate-coded networks.\n\\Delta w = \\eta \\, r^\\text{pre} \\, r^\\text{post}  \\,  (r^\\text{post} - \\mathbb{E}[(r^\\text{post})^2])\n\nIntrator, N., & Cooper, L. N. (1992). Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. Neural Networks, 5(1), 3–17. https://doi.org/10.1016/S0893-6080(05)80003-6\n\n\n#!pip install ANNarchy\n\nWe first import ANNarchy:\n\nimport numpy as np\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nWe will keep a minimal experimental setup, with two input neurons connected to a single output neuron. Note how the input neurons are defined by using the InputArrayclass, allowing to set r externally.\n\n# Network\nnet = ann.Network()\n\n# Input\npre = net.create(ann.InputArray(2))\n\n# Output\nneuron = ann.Neuron(\n    equations = \"\"\"\n        r = sum(exc)\n    \"\"\"\n)\npost = net.create(1, neuron)\n\nWe can now define a synapse model implementing the Intrator and Cooper version of the BCM learning rule.\nThe synapse has two parameters: The learning rate eta and the time constant tau of the moving average theta. Both are defined as global parameters, as we only need one value for the whole projection.\nThe moving average theta tracks the square of the post-synaptic firing rate post.r. It has the locality semiglobal, as we need to compute only one variable per post-synaptic neuron (it does not really matter in our example as we have only one output neuron…). It uses the exponential numerical method, as it is a first-order linear ODE that can be solved exactly. However, the default explicit Euler method would work just as well here.\nThe weight change dw/dt follows the BCM learning rule. min=0.0 ensures that the weight w stays positive throughout learning. The explicit Euler method is the default and could be omitted.\nThe psp argument w * pre.r (what is summed by the post-synaptic neuron over its incoming connections) is also the default value and could be omitted.\n\nIBCM = ann.Synapse(\n    parameters = dict(\n        eta = 0.01,\n        tau = 100.0,\n    ),\n    equations = [\n        ann.Variable('tau * dtheta/dt + theta = (post.r)^2', locality='semiglobal', method='exponential'),\n\n        ann.Variable('dw/dt = eta * post.r * (post.r - theta) * pre.r', min=0.0, method='explicit')\n    ],\n    psp = \"w * pre.r\"\n)\n\nWe can now create a projection between the two populations using the synapse type. The connection method is all-to-all, initialozing the two weights to 1.\n\nproj = net.connect(pre, post, 'exc', IBCM)\nproj.all_to_all(1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x13df0d400&gt;\n\n\nWe can now compile the network and record the post-synaptic firing rate as well as the evolution of the weights and thresholds during learning.\nWhen recording synaptic variables, you will always get a warning saying that this is a bad idea. It is indeed for bigger networks, but here it should be fine.\n\nnet.compile()\n\nm = net.monitor(post, 'r')\nn = net.monitor(proj, ['w', 'theta'])\n\nCompiling network 1...  OK \nWARNING: Monitor(): it is a bad idea to record synaptic variables of a projection at each time step! \n\n\nThe simulation protocol is kept simple, as it consists of setting constant firing rates for the two input neurons and simulating for one second.\n\npre.r = np.array([1.0, 0.1])\nnet.simulate(1000.)\n\nWe can now retrieve the recordings and plot the evolution of the various variables.\n\nr = m.get('r')\nw = n.get('w')\ntheta = n.get('theta')\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.subplot(211)\nplt.plot(r[:, 0], label='r')\nplt.plot(theta[:, 0], label='theta')\nplt.legend()\nplt.subplot(212)\nplt.plot(w[:, 0, 0], label=\"$w_1$\")\nplt.plot(w[:, 0, 1], label=\"$w_2$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNotice how the first weight increases when r is higher than theta (LTP), but decreases afterwards (LTD). Unintuitively, the input neuron with the highest activity sees its weight decreased at the end of the stimulation."
  },
  {
    "objectID": "notebooks/Ramp.html",
    "href": "notebooks/Ramp.html",
    "title": "Homeostatic STDP",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis example is a reimplementation of the mechanism described in:\n\nCarlson, K.D.; Richert, M.; Dutt, N.; Krichmar, J.L., “Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks,” in Neural Networks (IJCNN), The 2013 International Joint Conference on , vol., no., pp.1-8, 4-9 Aug. 2013. doi: 10.1109/IJCNN.2013.6706961\n\nIt is based on the corresponding Carlsim tutorial:\nhttp://www.socsci.uci.edu/~jkrichma/CARLsim/doc/tut3_plasticity.html\nThis notebook focuses on the simple “Ramp” experiment, but the principle is similar for the self-organizing receptive fields (SORF) in the next notebook.\n\nimport numpy as np\n\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nThe network uses regular-spiking Izhikevich neurons (see the Izhikevich notebook), but using exponentially-decaying conductances and NMDA synapses:\n\nRSNeuron = ann.Neuron(\n    parameters = dict(\n        a = 0.02,\n        b = 0.2,\n        c = -65.,\n        d = 8.,\n        tau_ampa = 5.,\n        tau_nmda = 150.,\n        vrev = 0.0,\n    ),\n    equations = [\n        # Inputs\n        ann.Variable('I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)'),\n\n        # Membrane potential and recovery variable are solved using the midpoint method for stability     \n        ann.Variable('dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I', init=-65., method='midpoint'),\n        ann.Variable('du/dt = a * (b*v - u)', init=-13., method='midpoint'),\n\n        # AMPA and NMDA conductances\n        ann.Variable('tau_ampa * dg_ampa/dt = -g_ampa', method='exponential'),\n        ann.Variable('tau_nmda * dg_nmda/dt = -g_nmda', method='exponential'),\n    ], \n    spike = \"v &gt;= 30.\", \n    reset = [\n        \"v = c\",\n        \"u += d\",\n    ],\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\"\n)\n\nThe main particularity about NMDA synaptic models is that a single synaptic connection influences two conductances:\n\nThe AMPA conductance, which primarily drives the post-synaptic neuron:\n\n\n    I_\\text{AMPA} = g_\\text{AMPA} \\times (V_\\text{rev} - V)\n\n\nThe NMDA conductance, which is non-linearly dependent on the membrane potential:\n\n\n    I_\\text{NMDA} = g_\\text{NMDA} \\times \\frac{(\\frac{V - V_\\text{NMDA}}{\\sigma})^2}{1 + (\\frac{V - V_\\text{NMDA}}{\\sigma})^2} \\times (V_\\text{rev} - V)\n\nIn short, the NMDA conductance only increases if the post-synaptic neuron is already depolarized.\nThe nmda function is defined in the functions argument for readability. The parameters V_\\text{NMDA} =-80 \\text{mV} and \\sigma = 60 \\text{mV} are here hardcoded in the equation, but they could be defined as global parameters.\nThe AMPA and NMDA conductances are exponentially decreasing with different time constants:\n\n    \\tau_\\text{AMPA} \\frac{dg_\\text{AMPA}(t)}{dt} + g_\\text{AMPA}(t) = 0\n \n    \\tau_\\text{NMDA} \\frac{dg_\\text{NMDA}(t)}{dt} + g_\\text{NMDA}(t) = 0\n\nAnother thing to notice in this neuron model is that the differential equations for the membrane potential and recovery variable are solved concurrently using the midpoint numerical method for stability: the semi-implicit method initially proposed by Izhikevich would fail.\n\nnet = ann.Network()\n\nThe input of the network is a population of 100 Poisson neurons, whose firing rate vary linearly from 0.2 to 20 Hz:\n\n# Input population\ninp = net.create(ann.PoissonPopulation(100, rates=np.linspace(0.2, 20., 100)))\n\nWe will consider two RS neurons, one learning inputs from the Poisson population using the regular STDP, the other learning using the proposed homeostatic STDP:\n\n# RS neuron without homeostatic mechanism\npop1 = net.create(ann.Population(1, RSNeuron))\n\n# RS neuron with homeostatic mechanism\npop2 = net.create(ann.Population(1, RSNeuron))\n\nThe regular STDP used in the article is a nearest-neighbour variant, which integrates LTP and LTD traces triggered after each pre- or post-synaptic spikes, respectively.\nContrary to the STDP synapse provided by ANNarchy, weight changes occur at each each time step:\n\nIn a post-pre interval, weight changes follow the LTP trace,\nIn a pre-post interval, weight changes follow the LTD trace.\n\nThe weights are clipped between 0 and w_\\text{max}.\n\nnearest_neighbour_stdp = ann.Synapse(\n    parameters = dict(\n        tau_plus = 20.,\n        tau_minus = 60.,\n        A_plus = 0.0002,\n        A_minus = 0.000066,\n        w_max = 0.03,\n    ),\n    equations = [\n        # Traces\n        ann.Variable('tau_plus  * dltp/dt = -ltp', method='exponential'),\n        ann.Variable('tau_minus * dltd/dt = -ltd', method='exponential'),\n        \n        # Nearest-neighbour\n        ann.Variable('w += if t_post &gt;= t_pre: ltp else: - ltd',  min=0.0, max='w_max')\n    ],\n    pre_spike = [\n        'g_target += w',\n        'ltp = A_plus',\n    ],         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\"\n)\n\nThe homeostatic STDP rule proposed by Carlson et al. is more complex. It has a regular STDP part (the nearest-neighbour variant above) and a homeostatic regularization part, ensuring that the post-synaptic firing rate R does not exceed a target firing rate R_\\text{target} = 35 Hz.\nThe firing rate of a spiking neuron can be automatically computed by ANNarchy (see later). It is then accessible as the variable r of the neuron (as if it were a regular rate-coded neuron).\nThe homeostatic STDP rule is defined by:\n\n    \\Delta w = K \\, (\\alpha  \\, (1 - \\frac{R}{R_\\text{target}}) \\, w + \\beta \\, \\text{stdp} )\n\nwhere stdp is the regular STDP weight change, and K is a firing rate-dependent learning rate:\n\n    K =  \\frac{R}{ T \\, (1 + |1 - \\gamma \\, \\frac{R}{R_\\text{target}}|})\n\nwith T being the window over which the mean firing rate is computed (5 seconds) and \\alpha, \\beta, \\gamma are parameters.\n\nhomeo_stdp = ann.Synapse(\n    parameters=dict(\n        # STDP\n        tau_plus = 20.,\n        tau_minus = 60.,\n        A_plus = 0.0002,\n        A_minus = 0.000066,\n        w_min = 0.0,\n        w_max = 0.03,\n\n        # Homeostatic regulation\n        alpha = 0.1,\n        beta = 1.0,\n        gamma = 50.,\n        Rtarget = 35.,\n        T = 5000.,\n    ),\n    equations = [\n        # Traces\n        ann.Variable('tau_plus  * dltp/dt = -ltp', method='exponential'),\n        ann.Variable('tau_minus * dltd/dt = -ltd', method='exponential'),\n\n        # Homeostatic values\n        ann.Variable('R = post.r', locality = 'semiglobal'),\n        ann.Variable('K = R/(T * (1. + fabs(1. - R / Rtarget) * gamma))', locality = 'semiglobal'),\n\n        # Nearest-neighbour\n        ann.Variable('stdp = if t_post &gt;= t_pre: ltp else: - ltd'),\n        ann.Variable('w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K', min='w_min', max='w_max'),\n    ],\n    pre_spike = [\n        'g_target += w',\n        'ltp = A_plus',\n    ],         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\"\n)\n\nThis rule necessitates that the post-synaptic neurons compute their average firing rate over a 5 seconds window. This has to be explicitely enabled, as it would be computationally too expensive to allow it by default:\n\npop1.compute_firing_rate(5000.)\npop2.compute_firing_rate(5000.)\n\nWe can now fully connect the input population to the two neurons with random weights:\n\n# Projection without homeostatic mechanism\nproj1 = net.connect(inp, pop1, ['ampa', 'nmda'], synapse=nearest_neighbour_stdp)\nproj1.all_to_all(ann.Uniform(0.01, 0.03))\n\n# Projection with homeostatic mechanism\nproj2 = net.connect(inp, pop2, ['ampa', 'nmda'], synapse=homeo_stdp)\nproj2.all_to_all(weights=ann.Uniform(0.01, 0.03))\n\n&lt;ANNarchy.core.Projection.Projection at 0x1567e87a0&gt;\n\n\nNote that the same weights will target both AMPA and NMDA conductances in the post-synaptic neurons. By default, the argument target of Projection should be a string, but you can also pass a list of strings to reach several conductances with the same weights.\nWe can now compile and simulate for 1000 seconds while recording the relevat information:\n\nnet.compile()\n\nCompiling network 1...  OK \n\n\n\n# Record\nm1 = net.monitor(pop1, 'r')\nm2 = net.monitor(pop2, 'r')\nm3 = net.monitor(proj1[0], 'w', period=1000.)\nm4 = net.monitor(proj2[0], 'w', period=1000.)\n\n# Simulate\nT = 1000 # 1000s\nnet.simulate(T*1000., True)\n\n# Get the data\ndata1 = m1.get('r')\ndata2 = m2.get('r')\ndata3 = m3.get('w')\ndata4 = m4.get('w')\n\nprint('Mean Firing Rate without homeostasis:', np.mean(data1[:, 0]))\nprint('Mean Firing Rate with homeostasis:', np.mean(data2[:, 0]))\n\nSimulating 1000.0 seconds of the network 1 took 1.4701051712036133 seconds. \nMean Firing Rate without homeostasis: 55.676368399999994\nMean Firing Rate with homeostasis: 35.23116840000001\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\n\nplt.subplot(311)\nplt.plot(np.linspace(0, T, len(data1[:, 0])), data1[:, 0], 'r-', label=\"Without homeostasis\")\nplt.plot(np.linspace(0, T, len(data2[:, 0])), data2[:, 0], 'b-', label=\"With homeostasis\")\nplt.xlabel('Time (s)')\nplt.ylabel('Firing rate (Hz)')\n\nplt.subplot(312)\nplt.plot(data3[-1, :], 'r-')\nplt.plot(data4[-1, :], 'bx')\naxes = plt.gca()\naxes.set_ylim([0., 0.035])\nplt.xlabel('# neuron')\nplt.ylabel('Weights after 1000s')\n\nplt.subplot(313)\nplt.imshow(np.array(data4, dtype='float').T, aspect='auto', cmap='hot')\nplt.xlabel('Time (s)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\n\n\n\n\n\nWe see that without homeostasis, the post-synaptic neuron reaches quickly a firing of 55 Hz, with all weights saturating at their maximum value 0.03. This is true even for inputs as low as 0.2Hz.\nMeanwhile, with homeostasis, the post-synaptic neuron gets a firing rate of 35 Hz (its desired value), and the weights from the input population are proportional to the underlying activity.",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Homeostatic STDP - Ramp"
    ]
  },
  {
    "objectID": "notebooks/SORF.html",
    "href": "notebooks/SORF.html",
    "title": "Homeostatic STDP: SORF model",
    "section": "",
    "text": "#!pip install ANNarchy\n\nReimplementation of the SORF model published in:\n\nCarlson, K.D.; Richert, M.; Dutt, N.; Krichmar, J.L., “Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks,” in Neural Networks (IJCNN), The 2013 International Joint Conference on , vol., no., pp.1-8, 4-9 Aug. 2013. doi: 10.1109/IJCNN.2013.6706961\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ANNarchy as ann\n\nfrom tqdm import tqdm\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nHyperparameters:\n\nnb_neuron = 4 # Number of exc and inh neurons\nsize = (32, 32) # input size\nfreq = 1.2 # nb_cycles/half-image\nnb_stim = 40 # Number of grating per epoch\nnb_epochs = 20 # Number of epochs\nmax_freq = 28. # Max frequency of the poisson neurons\nT = 10000. # Period for averaging the firing rate\n\nNeuron type:\n\n# Izhikevich Coba neuron with AMPA, NMDA and GABA receptors\nRSNeuron = ann.Neuron(\n    parameters = dict(\n        a = 0.02,\n        b = 0.2,\n        c = -65.,\n        d = 8.,\n\n        tau_ampa = 5.,\n        tau_nmda = 150.,\n        tau_gabaa = 6.,\n        tau_gabab = 150.,\n\n        vrev_ampa = 0.0,\n        vrev_nmda = 0.0,\n        vrev_gabaa = -70.0,\n        vrev_gabab = -90.0,\n     ) ,\n    equations = [\n        # Inputs\n        ann.Variable(\"\"\"\n            I = g_ampa * (vrev_ampa - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev_nmda -v) + g_gabaa * (vrev_gabaa - v) + g_gabab * (vrev_gabab -v)\n        \"\"\"),\n\n        # Midpoint scheme\n        ann.Variable(\"dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I\", init=-65., min=-90., method='midpoint'),\n        ann.Variable(\"du/dt = a * (b*v - u)\", init=-13., method='midpoint'),\n        \n        # Conductances\n        ann.Variable(\"tau_ampa * dg_ampa/dt = -g_ampa\", method='exponential'),\n        ann.Variable(\"tau_nmda * dg_nmda/dt = -g_nmda\", method='exponential'),\n        ann.Variable(\"tau_gabaa * dg_gabaa/dt = -g_gabaa\", method='exponential'),\n        ann.Variable(\"tau_gabab * dg_gabab/dt = -g_gabab\", method='exponential'),\n    ],\n    spike = \"v &gt;= 30.\",\n    reset = \"\"\"\n        v = c\n        u += d\n        g_ampa = 0.0\n        g_nmda = 0.0\n        g_gabaa = 0.0\n        g_gabab = 0.0\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\",\n    refractory=1.0\n)\n\nSynapse:\n\n# STDP with homeostatic regulation\nhomeo_stdp = ann.Synapse(\n    parameters=dict(\n        # STDP\n        tau_plus  = 60.,\n        tau_minus = 90.,\n        A_plus  = 0.000045,\n        A_minus = 0.00003,\n\n        # Homeostatic regulation\n        alpha = 0.1,\n        beta = 50.0, # &lt;- Difference with the original implementation\n        gamma = 50.0,\n        Rtarget = 10.,\n        T = 10000.,\n    ),\n    equations = [\n        # Homeostatic values\n        ann.Variable(\"R = post.r\", locality='semiglobal'),\n        ann.Variable(\"K = R/(T * (1. + fabs(1. - R / Rtarget) * gamma))\", locality='semiglobal'),\n\n        # Nearest-neighbour\n        ann.Variable(\"stdp = if t_post &gt;= t_pre: ltp else: - ltd\"),\n        ann.Variable(\"w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K\", min=0.0, max=10.0),\n        \n        # Traces\n        ann.Variable(\"tau_plus  * dltp/dt = -ltp\", method=\"exponential\"),\n        ann.Variable(\"tau_minus * dltd/dt = -ltd\", method=\"exponential\"),\n    ],\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",\n    post_spike=\"ltd = A_minus\"\n)\n\nNetwork:\n\n# Network\nnet = ann.Network()\n\n# Input population\nOnPoiss = net.create(ann.PoissonPopulation(size, rates=1.0))\nOffPoiss = net.create(ann.PoissonPopulation(size, rates=1.0))\n\n# RS neuron for the input buffers\nOnBuffer = net.create(size, RSNeuron)\nOffBuffer = net.create(size, RSNeuron)\n\n# Connect the buffers\nOnPoissBuffer = net.connect(OnPoiss, OnBuffer, ['ampa', 'nmda'])\nOnPoissBuffer.one_to_one(ann.Uniform(0.2, 0.6))\nOffPoissBuffer = net.connect(OffPoiss, OffBuffer, ['ampa', 'nmda'])\nOffPoissBuffer.one_to_one(ann.Uniform(0.2, 0.6))\n\n# Excitatory and inhibitory neurons\nExc = net.create(nb_neuron, RSNeuron)\nInh = net.create(nb_neuron, RSNeuron)\nExc.compute_firing_rate(T)\nInh.compute_firing_rate(T)\n\n# Input connections\nOnBufferExc = net.connect(OnBuffer, Exc, ['ampa', 'nmda'], homeo_stdp)\nOnBufferExc.all_to_all(ann.Uniform(0.004, 0.015))\nOffBufferExc = net.connect(OffBuffer, Exc, ['ampa', 'nmda'], homeo_stdp)\nOffBufferExc.all_to_all(ann.Uniform(0.004, 0.015))\n\n# Competition\nExcInh = net.connect(Exc, Inh, ['ampa', 'nmda'], homeo_stdp)\nExcInh.all_to_all(ann.Uniform(0.116, 0.403))\n\nExcInh.Rtarget = 75.\nExcInh.tau_plus = 51.\nExcInh.tau_minus = 78.\nExcInh.A_plus = -0.000041\nExcInh.A_minus = -0.000015\n\nInhExc = net.connect(Inh, Exc, ['gabaa', 'gabab'])\nInhExc.all_to_all(ann.Uniform(0.065, 0.259))\n\nnet.compile()\n\nCompiling network 1...  OK \n\n\n\n# Inputs\ndef get_grating(theta):\n    x = np.linspace(-1., 1., size[0])\n    y = np.linspace(-1., 1., size[1])\n    xx, yy = np.meshgrid(x, y)\n    z = np.sin(2.*np.pi*(np.cos(theta)*xx + np.sin(theta)*yy)*freq)\n    return np.maximum(z, 0.), -np.minimum(z, 0.0)\n\n# Initial weights\nw_on_start = OnBufferExc.w\nw_off_start = OffBufferExc.w\n\n# Monitors\nm = net.monitor(Exc, 'r')\nn = net.monitor(Inh, 'r')\no = net.monitor(OnBufferExc[0], 'w', period=1000.)\np = net.monitor(ExcInh[0], 'w', period=1000.)\n\n# Learning procedure\nfrom time import time\nimport random\ntstart = time()\nstim_order = list(range(nb_stim))\n\nfor epoch in tqdm(range(nb_epochs)):\n\n    random.shuffle(stim_order)\n    \n    for stim in stim_order:\n    \n        # Generate a grating randomly\n        rates_on, rates_off = get_grating(np.pi*stim/float(nb_stim))\n    \n        # Set it as input to the poisson neurons\n        OnPoiss.rates  = max_freq * rates_on\n        OffPoiss.rates = max_freq * rates_off\n    \n        # Simulate for 2s\n        net.simulate(2000.)\n    \n        # Relax the Poisson inputs\n        OnPoiss.rates  = 1.\n        OffPoiss.rates = 1.\n    \n        # Simulate for 500ms\n        net.simulate(500.)\n    \nprint('Done in ', time()-tstart)\n\n# Recordings\ndatae = m.get('r')\ndatai = n.get('r')\ndataw = o.get('w')\ndatal = p.get('w')\n\n100%|██████████| 20/20 [01:40&lt;00:00,  5.05s/it]\n\n\nDone in  101.01462006568909\n\n\n\n# Final weights\nw_on_end = OnBufferExc.w\nw_off_end = OffBufferExc.w\n\n# Plot\nplt.figure(figsize=(12, 12))\nplt.title('Feedforward weights before and after learning')\nfor i in range(nb_neuron):\n    plt.subplot(3, nb_neuron, i+1)\n    plt.imshow((np.array(w_on_start[i])).reshape((32,32)), aspect='auto', cmap='hot')\n    plt.subplot(3, nb_neuron, nb_neuron + i +1)\n    plt.imshow((np.array(w_on_end[i])).reshape((32,32)), aspect='auto', cmap='hot')\n    plt.subplot(3, nb_neuron, 2*nb_neuron + i +1)\n    plt.imshow((np.array(w_off_end[i])).reshape((32,32)), aspect='auto', cmap='hot')\n\nplt.figure(figsize=(12, 8))\nplt.plot(datae[:, 0], label='Exc')\nplt.plot(datai[:, 0], label='Inh')\nplt.title('Mean FR of the Exc and Inh neurons')\nplt.legend()\n\nplt.figure(figsize=(12, 8))\nplt.subplot(121)\nplt.imshow(np.array(dataw, dtype='float').T, aspect='auto', cmap='hot')\nplt.title('Timecourse of feedforward weights')\nplt.colorbar()\nplt.subplot(122)\nplt.imshow(np.array(datal, dtype='float').T, aspect='auto', cmap='hot')\nplt.title('Timecourse of inhibitory weights')\nplt.colorbar()\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Homeostatic STDP - SORF"
    ]
  },
  {
    "objectID": "notebooks/ANN2SNN2.html",
    "href": "notebooks/ANN2SNN2.html",
    "title": "ANN-to-SNN conversion - CNN",
    "section": "",
    "text": "This notebook demonstrates how to transform a CNN trained using tensorflow/keras into an SNN network usable in ANNarchy.\nThe CNN is adapted from the original model used in:\n#!pip install ANNarchy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nprint(f\"Tensorflow {tf.__version__}\")\n\nTensorflow 2.16.2\n# Download data\n(X_train, t_train), (X_test, t_test) = tf.keras.datasets.mnist.load_data()\n\n# Normalize inputs\nX_train = X_train.astype('float32') / 255.\nX_test = X_test.astype('float32') / 255.\n\n# One-hot output vectors\nT_train = tf.keras.utils.to_categorical(t_train, 10)\nT_test = tf.keras.utils.to_categorical(t_test, 10)",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "ANN to SNN II"
    ]
  },
  {
    "objectID": "notebooks/ANN2SNN2.html#training-an-ann-in-tensorflowkeras",
    "href": "notebooks/ANN2SNN2.html#training-an-ann-in-tensorflowkeras",
    "title": "ANN-to-SNN conversion - CNN",
    "section": "Training an ANN in tensorflow/keras",
    "text": "Training an ANN in tensorflow/keras\nThe tensorflow.keras convolutional network is built using the functional API.\nThe CNN has three 5*5 convolutional layers with ReLU, each followed by 2*2 max-pooling, no bias, dropout at 0.25, and a softmax output layer with 10 neurons. We use the standard SGD optimizer and the categorical crossentropy loss for classification.\n\ndef create_cnn():\n    \n    inputs = tf.keras.Input(shape = (28, 28, 1))\n    x = tf.keras.layers.Conv2D(\n        16, \n        kernel_size=(5,5),\n        activation='relu',\n        padding = 'same',\n        use_bias=False)(inputs)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Conv2D(\n        64,\n        kernel_size=(5,5),\n        activation='relu',\n        padding = 'same',\n        use_bias=False)(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Conv2D(\n        64,\n        kernel_size=(5,5),\n        activation='relu',\n        padding = 'same',\n        use_bias=False)(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Dropout(0.25)(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(\n        10,\n        activation='softmax',\n        use_bias=False)(x)\n\n    # Create functional model\n    model= tf.keras.Model(inputs, x)\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n\n    # Loss function\n    model.compile(\n        loss='categorical_crossentropy', # loss function\n        optimizer=optimizer, # learning rule\n        metrics=['accuracy'] # show accuracy\n    )\n    print(model.summary())\n\n    return model\n\n\n# Create model\nmodel = create_cnn()\n\n# Train model\nhistory = model.fit(\n    X_train, T_train,       # training data\n    batch_size=128,          # batch size\n    epochs=20,              # Maximum number of epochs\n    validation_split=0.1,   # Percentage of training data used for validation\n)\n\nmodel.save(\"runs/cnn.keras\")\n\n# Test model\npredictions_keras = model.predict(X_test, verbose=0)\ntest_loss, test_accuracy = model.evaluate(X_test, T_test, verbose=0)\nprint(f\"Test accuracy: {test_accuracy}\")\n\nModel: \"functional\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)        │ (None, 28, 28, 1)      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d (Conv2D)                 │ (None, 28, 28, 16)     │           400 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (MaxPooling2D)    │ (None, 14, 14, 16)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (Conv2D)               │ (None, 14, 14, 64)     │        25,600 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (MaxPooling2D)  │ (None, 7, 7, 64)       │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (Conv2D)               │ (None, 7, 7, 64)       │       102,400 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (MaxPooling2D)  │ (None, 3, 3, 64)       │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 3, 3, 64)       │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (Flatten)               │ (None, 576)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 10)             │         5,760 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 134,160 (524.06 KB)\n\n\n\n Trainable params: 134,160 (524.06 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nNone\n\nEpoch 1/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 17s 39ms/step - accuracy: 0.3815 - loss: 1.9014 - val_accuracy: 0.9252 - val_loss: 0.3057\n\nEpoch 2/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.8914 - loss: 0.3637 - val_accuracy: 0.9595 - val_loss: 0.1558\n\nEpoch 3/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 17s 39ms/step - accuracy: 0.9317 - loss: 0.2245 - val_accuracy: 0.9687 - val_loss: 0.1192\n\nEpoch 4/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 39ms/step - accuracy: 0.9496 - loss: 0.1717 - val_accuracy: 0.9720 - val_loss: 0.0966\n\nEpoch 5/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 39ms/step - accuracy: 0.9555 - loss: 0.1487 - val_accuracy: 0.9770 - val_loss: 0.0871\n\nEpoch 6/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9612 - loss: 0.1273 - val_accuracy: 0.9783 - val_loss: 0.0772\n\nEpoch 7/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9655 - loss: 0.1168 - val_accuracy: 0.9790 - val_loss: 0.0766\n\nEpoch 8/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9694 - loss: 0.1055 - val_accuracy: 0.9797 - val_loss: 0.0718\n\nEpoch 9/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9706 - loss: 0.0977 - val_accuracy: 0.9813 - val_loss: 0.0665\n\nEpoch 10/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9698 - loss: 0.0945 - val_accuracy: 0.9820 - val_loss: 0.0626\n\nEpoch 11/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 17s 39ms/step - accuracy: 0.9738 - loss: 0.0834 - val_accuracy: 0.9815 - val_loss: 0.0603\n\nEpoch 12/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9754 - loss: 0.0787 - val_accuracy: 0.9828 - val_loss: 0.0568\n\nEpoch 13/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9754 - loss: 0.0790 - val_accuracy: 0.9842 - val_loss: 0.0547\n\nEpoch 14/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 37ms/step - accuracy: 0.9771 - loss: 0.0737 - val_accuracy: 0.9848 - val_loss: 0.0529\n\nEpoch 15/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 37ms/step - accuracy: 0.9792 - loss: 0.0686 - val_accuracy: 0.9852 - val_loss: 0.0511\n\nEpoch 16/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 37ms/step - accuracy: 0.9789 - loss: 0.0684 - val_accuracy: 0.9852 - val_loss: 0.0502\n\nEpoch 17/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9780 - loss: 0.0670 - val_accuracy: 0.9857 - val_loss: 0.0527\n\nEpoch 18/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9807 - loss: 0.0629 - val_accuracy: 0.9845 - val_loss: 0.0509\n\nEpoch 19/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9823 - loss: 0.0565 - val_accuracy: 0.9863 - val_loss: 0.0490\n\nEpoch 20/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 16s 38ms/step - accuracy: 0.9827 - loss: 0.0574 - val_accuracy: 0.9863 - val_loss: 0.0472\n\nTest accuracy: 0.9871000051498413\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "ANN to SNN II"
    ]
  },
  {
    "objectID": "notebooks/ANN2SNN2.html#initialize-the-ann-to-snn-converter",
    "href": "notebooks/ANN2SNN2.html#initialize-the-ann-to-snn-converter",
    "title": "ANN-to-SNN conversion - CNN",
    "section": "Initialize the ANN-to-SNN converter",
    "text": "Initialize the ANN-to-SNN converter\nWe now create an instance of the ANN-to-SNN conversion object.\n\nfrom ANNarchy.extensions.ann_to_snn_conversion import ANNtoSNNConverter\n\nsnn_converter = ANNtoSNNConverter(\n    input_encoding='IB', \n    hidden_neuron='IaF',\n    read_out='spike_count',\n)\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\n\nnet = snn_converter.load_keras_model(\"runs/cnn.keras\", show_info=True)\n\nWARNING: Dense representation is an experimental feature for spiking models, we greatly appreciate bug reports. \n* Input layer: input_layer, (28, 28, 1)\n* InputLayer skipped.\n* Conv2D layer: conv2d, (28, 28, 16) \n* MaxPooling2D layer: max_pooling2d, (14, 14, 16) \n* Conv2D layer: conv2d_1, (14, 14, 64) \n* MaxPooling2D layer: max_pooling2d_1, (7, 7, 64) \n* Conv2D layer: conv2d_2, (7, 7, 64) \n* MaxPooling2D layer: max_pooling2d_2, (3, 3, 64) \n* Dropout skipped.\n* Flatten skipped.\n* Dense layer: dense, 10 \n    weights: (10, 576)\n    mean -0.0003130047698505223, std 0.069309763610363\n    min -0.23668308556079865, max 0.20495571196079254\n\n\n\n\npredictions_snn = snn_converter.predict(X_test[:300], duration_per_sample=200)\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]100%|██████████| 300/300 [09:32&lt;00:00,  1.91s/it]\n\n\nUsing the recorded predictions, we can now compute the accuracy using scikit-learn for all presented samples.\n\nfrom sklearn.metrics import classification_report, accuracy_score\n\nprint(classification_report(t_test[:300], predictions_snn))\nprint(\"Test accuracy of the SNN:\", accuracy_score(t_test[:300], predictions_snn))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        24\n           1       1.00      0.90      0.95        41\n           2       1.00      1.00      1.00        32\n           3       0.96      1.00      0.98        24\n           4       1.00      0.97      0.99        37\n           5       1.00      1.00      1.00        29\n           6       0.96      1.00      0.98        24\n           7       1.00      0.97      0.99        34\n           8       0.75      1.00      0.86        21\n           9       1.00      0.91      0.95        34\n\n    accuracy                           0.97       300\n   macro avg       0.97      0.98      0.97       300\nweighted avg       0.98      0.97      0.97       300\n\nTest accuracy of the SNN: 0.97",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "ANN to SNN II"
    ]
  },
  {
    "objectID": "notebooks/Hybrid.html",
    "href": "notebooks/Hybrid.html",
    "title": "Hybrid network",
    "section": "",
    "text": "#!pip install ANNarchy\n\nSimple example showing hybrid spike/rate-coded networks.\nReproduces Fig.4 of (Vitay, Dinkelbach and Hamker, 2015)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ANNarchy as ann\n\n# Rate-coded output neuron\nsimple_neuron = ann.Neuron(\n    equations = \"r = sum(exc)\"\n)\n\n# Network\nnet = ann.Network(dt=0.1)\n\n# Rate-coded population for input\npop1 = net.create(ann.InputArray(geometry=1))\n\n# Poisson Population to encode\npop2 = net.create(ann.PoissonPopulation(geometry=1000, target=\"exc\"))\nproj = net.connect(pop1, pop2, 'exc')\nproj.all_to_all(weights=1.)\n\n# Rate-coded population to decode\npop3 = net.create(geometry=1000, neuron=simple_neuron)\nproj = net.connect(ann.DecodingProjection(pop2, pop3, 'exc', window=10.0))\n\ndef diagonal(pre, post, weights):\n    \"\"\"\n    Simple connector pattern to progressively connect each post-synaptic neuron to a growing number of pre-synaptic neurons.\n    \"\"\"\n    lil = ann.LILConnectivity()\n    for rk_post in range(post.size):\n        lil.add(rk_post, range((rk_post+1)), [weights], [0] )\n    return lil\n\nproj.from_function(method=diagonal, weights=1.)\n\nnet.compile()\n\n# Monitors\nm1 = net.monitor(pop1, 'r')\nm2 = net.monitor(pop2, 'spike')\nm3 = net.monitor(pop3, 'r')\n\n# Simulate\nduration = 250.\n# 0 Hz\npop1.r = 0.0\nnet.simulate(duration)\n# 10 Hz\npop1.r = 10.0\nnet.simulate(duration)\n# 50 Hz\npop1.r = 50.0\nnet.simulate(duration)\n# 100 Hz\npop1.r = 100.0\nnet.simulate(duration)\n\n# Get recordings\ndata1 = m1.get()\ndata2 = m2.get()\ndata3 = m3.get()\n\n# Raster plot of the spiking population\nt, n = m2.raster_plot(data2['spike'])\n\n# Variance of the the decoded firing rate\ndata_10 = data3['r'][int(1.0*duration/net.dt):int(2*duration/net.dt), :]\ndata_50 = data3['r'][int(2.0*duration/net.dt):int(3*duration/net.dt), :]\ndata_100 = data3['r'][int(3.0*duration/net.dt):int(4*duration/net.dt), :]\nvar_10 = np.mean(np.abs((data_10 - 10.)/10.), axis=0)\nvar_50 = np.mean(np.abs((data_50 - 50.)/50.), axis=0)\nvar_100 = np.mean(np.abs((data_100 - 100.)/100.), axis=0)\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\nCompiling network 1...  OK \n\n\n\nplt.figure(figsize=(12, 15))\nplt.subplot(3,1,1)\nplt.plot(t, n, '.', markersize=0.5)\nplt.title('a) Raster plot')\nplt.xlabel('Time (ms)')\nplt.ylabel('# neurons')\nplt.xlim((0, 4*duration))\n\nplt.subplot(3,1,2)\nplt.plot(np.arange(0, 4*duration, 0.1), data1['r'][:, 0], label='Original firing rate')\nplt.plot(np.arange(0, 4*duration, 0.1), data3['r'][:, 999], label='Decoded firing rate')\nplt.legend(frameon=False, loc=2)\nplt.title('b) Decoded firing rate')\nplt.xlabel('Time (ms)')\nplt.ylabel('Activity (Hz)')\n\nplt.subplot(3,1,3)\nplt.plot(var_10, label='10 Hz')\nplt.plot(var_50, label='50 Hz')\nplt.plot(var_100, label='100 Hz')\nplt.legend(frameon=False)\nplt.title('c) Precision')\nplt.xlabel('# neurons used for decoding')\nplt.ylabel('Normalized error')\nplt.ylim((0,1))\n\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Advanced features**",
      "Hybrid networks"
    ]
  },
  {
    "objectID": "notebooks/STP.html",
    "href": "notebooks/STP.html",
    "title": "Short-term Plasticity and Synchrony",
    "section": "",
    "text": "#!pip install ANNarchy\n\nImplementation of the recurrent network with short-term plasticity (STP) proposed in:\n\nTsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses, The Journal of Neuroscience, 20(50).\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nThis network uses simple leaky integrate-and-fire (LIF) neurons:\n\nLIF = ann.Neuron(\n    parameters = dict(\n        tau = 30.0,\n        I = ann.Parameter(15.0),\n        tau_I = 3.0,\n    ),\n    equations = [\n        ann.Variable('tau * dv/dt = -v + g_exc - g_inh + I', init=13.5),\n        ann.Variable('tau_I * dg_exc/dt = -g_exc'),\n        ann.Variable('tau_I * dg_inh/dt = -g_inh'),\n    ],\n    spike = \"v &gt; 15.0\",\n    reset = \"v = 13.5\",\n    refractory = 3.0\n)\n\nnet = ann.Network(dt=0.25)\n\nP = net.create(geometry=500, neuron=LIF)\nP.I = np.sort(ann.Uniform(14.625, 15.375).get_values(500))\nP.v = ann.Uniform(0.0, 15.0)\nExc = P[:400]\nInh = P[400:]\n\nShort-term plasticity can be defined by dynamical changes of synaptic efficiency, based on pre- or post-synaptic activity.\nWe define a STP synapse, whose post-pynaptic potential (psp, define by g_target) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u:\n\nSTP = ann.Synapse(\n    parameters = dict(\n        tau_rec = ann.Parameter(1.0),\n        tau_facil = ann.Parameter(1.0),\n        U = ann.Parameter(0.1),\n    ),\n    equations = [\n        ann.Variable('dx/dt = (1 - x)/tau_rec', init = 1.0, method='event-driven'),\n        ann.Variable('du/dt = (U - u)/tau_facil', init = 0.1, method='event-driven'),\n    ],\n    pre_spike=\"\"\"\n        g_target += w * u * x\n        x *= (1 - u)\n        u += U * (1 - u)\n    \"\"\"\n)\n\nCreating the projection between the excitatory and inhibitory is straightforward when the right parameters are chosen:\n\n# Parameters for the synapses\nAee = 1.8\nAei = 5.4\nAie = 7.2\nAii = 7.2\n\nUee = 0.5\nUei = 0.5\nUie = 0.04\nUii = 0.04\n\ntau_rec_ee = 800.0\ntau_rec_ei = 800.0\ntau_rec_ie = 100.0\ntau_rec_ii = 100.0\n\ntau_facil_ie = 1000.0\ntau_facil_ii = 1000.0\n\n# Create projections\nproj_ee = net.connect(pre=Exc, post=Exc, target='exc', synapse=STP)\nproj_ee.fixed_probability(probability=0.1, weights=ann.Normal(Aee, (Aee/2.0), min=0.2*Aee, max=2.0*Aee)) \nproj_ee.U = ann.Normal(Uee, (Uee/2.0), min=0.1, max=0.9)\nproj_ee.tau_rec = ann.Normal(tau_rec_ee, (tau_rec_ee/2.0), min=5.0)\nproj_ee.tau_facil = net.dt # Cannot be 0!\n\nproj_ei = net.connect(pre=Inh, post=Exc, target='inh', synapse=STP)\nproj_ei.fixed_probability(probability=0.1, weights=ann.Normal(Aei, (Aei/2.0), min=0.2*Aei, max=2.0*Aei))\nproj_ei.U = ann.Normal(Uei, (Uei/2.0), min=0.1, max=0.9)\nproj_ei.tau_rec = ann.Normal(tau_rec_ei, (tau_rec_ei/2.0), min=5.0)\nproj_ei.tau_facil = net.dt # Cannot be 0!\n\nproj_ie = net.connect(pre=Exc, post=Inh, target='exc', synapse=STP)\nproj_ie.fixed_probability(probability=0.1, weights=ann.Normal(Aie, (Aie/2.0), min=0.2*Aie, max=2.0*Aie))\nproj_ie.U = ann.Normal(Uie, (Uie/2.0), min=0.001, max=0.07)\nproj_ie.tau_rec = ann.Normal(tau_rec_ie, (tau_rec_ie/2.0), min=5.0)\nproj_ie.tau_facil = ann.Normal(tau_facil_ie, (tau_facil_ie/2.0), min=5.0)\n\nproj_ii = net.connect(pre=Inh, post=Inh, target='inh', synapse=STP)\nproj_ii.fixed_probability(probability=0.1, weights=ann.Normal(Aii, (Aii/2.0), min=0.2*Aii, max=2.0*Aii))\nproj_ii.U = ann.Normal(Uii, (Uii/2.0), min=0.001, max=0.07)\nproj_ii.tau_rec = ann.Normal(tau_rec_ii, (tau_rec_ii/2.0), min=5.0)\nproj_ii.tau_facil = ann.Normal(tau_facil_ii, (tau_facil_ii/2.0), min=5.0)\n\nWe compile and simulate for 10 seconds:\n\n# Compile\nnet.compile()\n\n# Record\nMe = net.monitor(Exc, 'spike')\nMi = net.monitor(Inh, 'spike')\n\n# Simulate\nduration = 10000.0\nnet.simulate(duration, measure_time=True)\n\nCompiling network 1...  OK \nSimulating 10.0 seconds of the network 1 took 0.07660222053527832 seconds. \n\n\nWe retrieve the recordings and plot them:\n\n# Retrieve recordings\ndata_exc = Me.get()\ndata_inh = Mi.get()\nte, ne = Me.raster_plot(data_exc['spike'])\nti, ni = Mi.raster_plot(data_inh['spike'])\n\n# Histogram of the exc population\nh = Me.histogram(data_exc['spike'], bins=1.0)\n\n# Mean firing rate of each excitatory neuron\nrates = []\nfor neur in data_exc['spike'].keys():\n    rates.append(len(data_exc['spike'][neur])/duration*1000.0)\n\n\nplt.figure(figsize=(12, 10))\nplt.subplot(3,1,1)\nplt.plot(te, ne, 'b.', markersize=1.0)\nplt.plot(ti, ni, 'b.', markersize=1.0)\nplt.xlim((0, duration)); plt.ylim((0,500))\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\n\nplt.subplot(3,1,2)\nplt.plot(h/400.)\nplt.xlabel('Time (ms)')\nplt.ylabel('Net activity')\n\nplt.subplot(3,1,3)\nplt.plot(sorted(rates))\nplt.ylabel('Spikes / sec')\nplt.xlabel('# neuron')\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "STP"
    ]
  },
  {
    "objectID": "notebooks/SynapticTransmission.html",
    "href": "notebooks/SynapticTransmission.html",
    "title": "Synaptic transmission",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis notebook simply demonstrates the three main type of synaptic transmission for spiking neurons:\n\nInstantaneous\nExponentially-decreasing\nAlpha-shaped\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nWe use here a simple LIF neuron receving three types of projections (a, b, c). The conductance g_a uses instantaneous transmission, as it is reset to 0 after each step. g_b decreases exponentially with time following a first order ODE. g_c is integrated twice in alpha_c, leading to the alpha shape.\nAll methods use the exponential numerical method, as they are first order linear ODEs and can be solved exactly.\n\nLIF = ann.Neuron(\n    parameters = dict(\n        tau = 20.,\n        E_L = -70.,\n        v_T = 0.,\n        v_r = -58.,\n        tau_b = 10.0,\n        tau_c = 10.0,\n    ),\n    equations = [\n        # Membrane potential\n        ann.Variable('tau * dv/dt = (E_L - v) + g_a + g_b + alpha_c', init=-70.),\n        \n        # Exponentially decreasing\n        ann.Variable('tau_b * dg_b/dt = -g_b', method='exponential'),\n        \n        # Alpha-shaped\n        ann.Variable('tau_c * dg_c/dt = -g_c', method='exponential'),\n        ann.Variable('tau_c * dalpha_c/dt = exp((tau_c - dt/2.0)/tau_c) * g_c - alpha_c', method='exponential'),\n    ],\n    spike=\"v &gt;= v_T\",\n    reset=\"v = v_r\",\n    refractory = 2.0\n)\n\nThe LIF neuron will receive a single spike at t = 10 ms, using the SpikeSourceArray specific population.\n\nnet = ann.Network()\ninp = net.create(ann.SpikeSourceArray([10.]))\npop = net.create(1, LIF)\n\nWe implement three different projections between the same neurons, to highlight the three possible transmission mechanisms.\n\nproj = net.connect(inp, pop, 'a')\nproj.all_to_all(weights=1.0)\n\nproj = net.connect(inp, pop, 'b')\nproj.all_to_all(weights=1.0)\n\nproj = net.connect(inp, pop, 'c')\nproj.all_to_all(weights=1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x11fb47fe0&gt;\n\n\n\nnet.compile()\n\nCompiling network 1...  OK \n\n\nWe monitor the three conductances:\n\nm = net.monitor(pop, ['g_a', 'g_b', 'alpha_c'])\n\n\ninp.clear()\nnet.simulate(100.)\n\n\ndata = m.get()\n\n\nplt.figure(figsize=(10, 10))\nplt.subplot(311)\nplt.plot(data['g_a'][:, 0])\nplt.ylabel(\"Instantaneous\")\nplt.subplot(312)\nplt.plot(data['g_b'][:, 0])\nplt.ylabel(\"Exponential\")\nplt.subplot(313)\nplt.plot(data['alpha_c'][:, 0])\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"Alpha\")\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Synaptic transmission"
    ]
  },
  {
    "objectID": "reference/Constant.html",
    "href": "reference/Constant.html",
    "title": "Constant",
    "section": "",
    "text": "Constant(self, name, value, net_id=0)\nConstant parameter that can be used by all neurons and synapses.\nThe class Constant derives from float, so any legal operation on floats (addition, multiplication) can be used, but it returns a float.\nIf a Neuron/Synapse already defines a parameter with the same name, the constant will not be visible.\nThe constant can be declared at the global level, usually before the neuron/synapse definition:\ntau = ann.Constant('tau', 20)\nfactor = ann.Constant('factor', 0.1)\nreal_tau = ann.Constant('real_tau', tau*factor)\n\nneuron = ann.Neuron(\n    equations=[\n        'real_tau * dr/dt + r = 1.0'\n    ]\n)\n\nnet = ann.Network()\nnet.create(10, neuron)\nnet.compile()\nThe value of the constant can be changed anytime with the set() method.\ntau.set(30.0)\nIf tau was defined at the global level, ALL networks using that constant will see the change. If you want the change to impact only one network, you should first retrieve the local Constant instance from the network:\ntau = net.get_constant('tau')\ntau.set(30.0)\nAssignments will have no effect (e.g. tau = 10.0 creates a new float and erases the Constant object).\nThe value of constants defined as combination of other constants (real_tau) is not updated if the value of these constants changes (changing tau with tau.set(10.0) will not modify the value of real_tau).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the constant (unique), which can be used in equations.\nrequired\n\n\nvalue\nfloat\nthe value of the constant, which must be a float, or a combination of Constants.\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nname\nName.\n\n\nvalue\nValue.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nset\nChanges the value of the constant.\n\n\n\n\n\nset(value)\nChanges the value of the constant.\nIf the constant was declared globally, this impacts all networks. Call Network.get_constant(name) if you want to impact a single network.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nfloat\nValue.\nrequired",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Constant"
    ]
  },
  {
    "objectID": "reference/Constant.html#parameters",
    "href": "reference/Constant.html#parameters",
    "title": "Constant",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the constant (unique), which can be used in equations.\nrequired\n\n\nvalue\nfloat\nthe value of the constant, which must be a float, or a combination of Constants.\nrequired",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Constant"
    ]
  },
  {
    "objectID": "reference/Constant.html#attributes",
    "href": "reference/Constant.html#attributes",
    "title": "Constant",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nname\nName.\n\n\nvalue\nValue.",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Constant"
    ]
  },
  {
    "objectID": "reference/Constant.html#methods",
    "href": "reference/Constant.html#methods",
    "title": "Constant",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nset\nChanges the value of the constant.\n\n\n\n\n\nset(value)\nChanges the value of the constant.\nIf the constant was declared globally, this impacts all networks. Call Network.get_constant(name) if you want to impact a single network.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nfloat\nValue.\nrequired",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Constant"
    ]
  },
  {
    "objectID": "reference/STDP.html",
    "href": "reference/STDP.html",
    "title": "STDP",
    "section": "",
    "text": "STDP(\n    self,\n    tau_plus=20.0,\n    tau_minus=20.0,\n    A_plus=0.01,\n    A_minus=0.01,\n    w_min=0.0,\n    w_max=1.0,\n)\nSpike-timing dependent plasticity, online version.\n\nSong, S., and Abbott, L.F. (2001). Cortical development and remapping through spike timing-dependent plasticity. Neuron 32, 339-350.\n\nEquivalent code:\nSTDP = ann.Synapse(\n    parameters = dict(\n        tau_plus = 20.0,\n        tau_minus = 20.0,\n        A_plus = 0.01,\n        A_minus = 0.01,\n        w_min = 0.0,\n        w_max = 1.0,\n    ),\n    equations = [\n        ann.Variable('tau_plus  * dx/dt = -x', method='event-driven'),\n        ann.Variable('tau_minus * dy/dt = -y', method='event-driven'),\n    ],\n    pre_spike=\"\"\"\n        g_target += w\n        x += A_plus * w_max\n        w = clip(w + y, w_min , w_max)\n    \"\"\",\n    post_spike=\"\"\"\n        y -= A_minus * w_max\n        w = clip(w + x, w_min , w_max)\n    \"\"\"\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntau_plus\n\ntime constant of the pre-synaptic trace (ms)\n20.0\n\n\ntau_minus\n\ntime constant of the pre-synaptic trace (ms)\n20.0\n\n\nA_plus\n\nincrease of the pre-synaptic trace after a spike.\n0.01\n\n\nA_minus\n\ndecrease of the post-synaptic trace after a spike.\n0.01\n\n\nw_min\n\nminimal value of the weight w.\n0.0\n\n\nw_max\n\nmaximal value of the weight w.\n1.0",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "STDP"
    ]
  },
  {
    "objectID": "reference/STDP.html#parameters",
    "href": "reference/STDP.html#parameters",
    "title": "STDP",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntau_plus\n\ntime constant of the pre-synaptic trace (ms)\n20.0\n\n\ntau_minus\n\ntime constant of the pre-synaptic trace (ms)\n20.0\n\n\nA_plus\n\nincrease of the pre-synaptic trace after a spike.\n0.01\n\n\nA_minus\n\ndecrease of the post-synaptic trace after a spike.\n0.01\n\n\nw_min\n\nminimal value of the weight w.\n0.0\n\n\nw_max\n\nmaximal value of the weight w.\n1.0",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "STDP"
    ]
  },
  {
    "objectID": "reference/Gamma.html",
    "href": "reference/Gamma.html",
    "title": "Gamma",
    "section": "",
    "text": "Gamma(self, alpha, beta=1.0, min=None, max=None, rng=None)\nGamma distribution.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nalpha\nfloat\nShape of the gamma distribution.\nrequired\n\n\nbeta\nfloat\nScale of the gamma distribution.\n1.0\n\n\nmin\nfloat\nMinimum value returned (default: unlimited).\nNone\n\n\nmax\nfloat\nMaximum value returned (default: unlimited).\nNone\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Gamma"
    ]
  },
  {
    "objectID": "reference/Gamma.html#parameters",
    "href": "reference/Gamma.html#parameters",
    "title": "Gamma",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nalpha\nfloat\nShape of the gamma distribution.\nrequired\n\n\nbeta\nfloat\nScale of the gamma distribution.\n1.0\n\n\nmin\nfloat\nMinimum value returned (default: unlimited).\nNone\n\n\nmax\nfloat\nMaximum value returned (default: unlimited).\nNone\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Gamma"
    ]
  },
  {
    "objectID": "reference/Gamma.html#methods",
    "href": "reference/Gamma.html#methods",
    "title": "Gamma",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Gamma"
    ]
  },
  {
    "objectID": "reference/functions.html",
    "href": "reference/functions.html",
    "title": "functions",
    "section": "",
    "text": "functions(name, network=None)\nAllows to access a global function declared with add_function() and use it from Python using arrays after compilation of the magic network.\nThe name of the function is not added to the global namespace to avoid overloading.\nadd_function(\"logistic(x) = 1. / (1. + exp(-x))\") \n\nmagic_network().compile()  \n\nresult = functions('logistic')([0., 1., 2., 3., 4.])\nOnly lists or 1D Numpy arrays can be passed as arguments, not single values nor multidimensional arrays.\nWhen passing several arguments, make sure they have the same size.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the function.\nrequired",
    "crumbs": [
      "Reference",
      "**Functions**",
      "functions"
    ]
  },
  {
    "objectID": "reference/functions.html#parameters",
    "href": "reference/functions.html#parameters",
    "title": "functions",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the function.\nrequired",
    "crumbs": [
      "Reference",
      "**Functions**",
      "functions"
    ]
  },
  {
    "objectID": "reference/PopulationView.html",
    "href": "reference/PopulationView.html",
    "title": "PopulationView",
    "section": "",
    "text": "PopulationView(self, population, ranks, geometry=None)\nSubset of a Population.\nA list of ranks can be passed, but population views are usually created through slicing:\npop = net.create(1000, neuron)\n\nE = pop[:800] # ann.PopulationView(pop, range(800))\nI = pop[800:] # ann.PopulationView(pop, range(800, 1000))\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npopulation\n\npopulation object\nrequired\n\n\nranks\n\nlist or numpy array containing the ranks of the selected neurons.\nrequired\n\n\ngeometry\n\na geometry for the Populationview (optional)\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\npopulation\nOriginal population.\n\n\nranks\nArray of ranks in the PopulationView.\n\n\ngeometry\nGeometry of the PopulationView (optional).\n\n\nsize\nSize of the PopulationView (number of neurons).\n\n\ntargets\nList of targets connected to the population.\n\n\nname\nName of the original population.\n\n\nattributes\nÄist of attributes of the original population.\n\n\nvariables\nList of variables of the original population.\n\n\nparameters\nList of constants of the original population.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nrank_from_coordinates\nReturns the rank of a neuron based on coordinates.\n\n\ncoordinates_from_rank\nReturns the coordinates of a neuron based on its rank.\n\n\nget\nReturns the parameter/variable value.\n\n\nset\nUpdates the neurons’ parameter/variable values.\n\n\nsum\nReturns the array of weighted sums corresponding to the target:\n\n\n\n\n\nrank_from_coordinates(coord, local=False)\nReturns the rank of a neuron based on coordinates.\nWhen local is False (default), the coordinates are relative to the ORIGINAL population, not the PopulationView.\nWhen local is True, the coordinates are interpreted relative to the geometry of the PopulationView if available. When you add two population views, the geometry is lost and the method will return an error.\nThe rank is relative to the original population. Iterate over len(pop) otherwise.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncoord\ntuple\ncoordinate tuple, can be multidimensional.\nrequired\n\n\nlocal\nbool\nwhether the coordinates are local to the PopulationView or not.\nFalse\n\n\n\n\n\n\n\ncoordinates_from_rank(rank, local=False)\nReturns the coordinates of a neuron based on its rank.\nWhen local is False (default), the coordinates are relative to the ORIGINAL population, not the PopulationView.\nWhen local is True, the coordinates are interpreted relative to the geometry of the PopulationView if available. When you add two population views, the geometry is lost and the method will return an error.\nThe rank is relative to the original population. Iterate over len(pop) otherwise.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the neuron in the original population.\nrequired\n\n\nlocal\nbool\nwhether the coordinates are local to the PopulationView or not.\nFalse\n\n\n\n\n\n\n\nget(name)\nReturns the parameter/variable value.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nname of the parameter/variable.\nrequired\n\n\n\n\n\n\n\nset(value)\nUpdates the neurons’ parameter/variable values.\nWarning: If you modify the value of a global parameter, this will be the case for ALL neurons of the population, not only the subset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\ndict\ndictionary of parameters/variables to be updated for the corresponding subset of neurons. It can be a single value or a list/1D array of the same size as the PopulationView.\nrequired\n\n\n\n\n\n\n\nsum(target)\nReturns the array of weighted sums corresponding to the target:\nexcitatory = pop[:50].sum('exc')\nFor spiking networks, this is equivalent to accessing the conductances directly::\nexcitatory = pop[:50].g_exc\nIf no incoming projection has the given target, the method returns zeros.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntarget\nstr\nthe target.\nrequired",
    "crumbs": [
      "Reference",
      "**Core components**",
      "PopulationView"
    ]
  },
  {
    "objectID": "reference/PopulationView.html#parameters",
    "href": "reference/PopulationView.html#parameters",
    "title": "PopulationView",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npopulation\n\npopulation object\nrequired\n\n\nranks\n\nlist or numpy array containing the ranks of the selected neurons.\nrequired\n\n\ngeometry\n\na geometry for the Populationview (optional)\nNone",
    "crumbs": [
      "Reference",
      "**Core components**",
      "PopulationView"
    ]
  },
  {
    "objectID": "reference/PopulationView.html#attributes",
    "href": "reference/PopulationView.html#attributes",
    "title": "PopulationView",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npopulation\nOriginal population.\n\n\nranks\nArray of ranks in the PopulationView.\n\n\ngeometry\nGeometry of the PopulationView (optional).\n\n\nsize\nSize of the PopulationView (number of neurons).\n\n\ntargets\nList of targets connected to the population.\n\n\nname\nName of the original population.\n\n\nattributes\nÄist of attributes of the original population.\n\n\nvariables\nList of variables of the original population.\n\n\nparameters\nList of constants of the original population.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "PopulationView"
    ]
  },
  {
    "objectID": "reference/PopulationView.html#methods",
    "href": "reference/PopulationView.html#methods",
    "title": "PopulationView",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nrank_from_coordinates\nReturns the rank of a neuron based on coordinates.\n\n\ncoordinates_from_rank\nReturns the coordinates of a neuron based on its rank.\n\n\nget\nReturns the parameter/variable value.\n\n\nset\nUpdates the neurons’ parameter/variable values.\n\n\nsum\nReturns the array of weighted sums corresponding to the target:\n\n\n\n\n\nrank_from_coordinates(coord, local=False)\nReturns the rank of a neuron based on coordinates.\nWhen local is False (default), the coordinates are relative to the ORIGINAL population, not the PopulationView.\nWhen local is True, the coordinates are interpreted relative to the geometry of the PopulationView if available. When you add two population views, the geometry is lost and the method will return an error.\nThe rank is relative to the original population. Iterate over len(pop) otherwise.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncoord\ntuple\ncoordinate tuple, can be multidimensional.\nrequired\n\n\nlocal\nbool\nwhether the coordinates are local to the PopulationView or not.\nFalse\n\n\n\n\n\n\n\ncoordinates_from_rank(rank, local=False)\nReturns the coordinates of a neuron based on its rank.\nWhen local is False (default), the coordinates are relative to the ORIGINAL population, not the PopulationView.\nWhen local is True, the coordinates are interpreted relative to the geometry of the PopulationView if available. When you add two population views, the geometry is lost and the method will return an error.\nThe rank is relative to the original population. Iterate over len(pop) otherwise.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the neuron in the original population.\nrequired\n\n\nlocal\nbool\nwhether the coordinates are local to the PopulationView or not.\nFalse\n\n\n\n\n\n\n\nget(name)\nReturns the parameter/variable value.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nname of the parameter/variable.\nrequired\n\n\n\n\n\n\n\nset(value)\nUpdates the neurons’ parameter/variable values.\nWarning: If you modify the value of a global parameter, this will be the case for ALL neurons of the population, not only the subset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\ndict\ndictionary of parameters/variables to be updated for the corresponding subset of neurons. It can be a single value or a list/1D array of the same size as the PopulationView.\nrequired\n\n\n\n\n\n\n\nsum(target)\nReturns the array of weighted sums corresponding to the target:\nexcitatory = pop[:50].sum('exc')\nFor spiking networks, this is equivalent to accessing the conductances directly::\nexcitatory = pop[:50].g_exc\nIf no incoming projection has the given target, the method returns zeros.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntarget\nstr\nthe target.\nrequired",
    "crumbs": [
      "Reference",
      "**Core components**",
      "PopulationView"
    ]
  },
  {
    "objectID": "reference/get_projection.html",
    "href": "reference/get_projection.html",
    "title": "get_projection",
    "section": "",
    "text": "get_projection(name, net_id=0)\nReturns the projection with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the projection.\nrequired",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "get_projection"
    ]
  },
  {
    "objectID": "reference/get_projection.html#parameters",
    "href": "reference/get_projection.html#parameters",
    "title": "get_projection",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the projection.\nrequired",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "get_projection"
    ]
  },
  {
    "objectID": "reference/IF_curr_alpha.html",
    "href": "reference/IF_curr_alpha.html",
    "title": "IF_curr_alpha",
    "section": "",
    "text": "IF_curr_alpha(\n    self,\n    v_rest=-65.0,\n    cm=1.0,\n    tau_m=20.0,\n    tau_refrac=0.0,\n    tau_syn_E=5.0,\n    tau_syn_I=5.0,\n    v_thresh=-50.0,\n    v_reset=-65.0,\n    i_offset=0.0,\n)\nLeaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents.\nSeparate synaptic currents for excitatory and inhibitory synapses.\nThe alpha currents are calculated through a system of two linears ODEs. After a spike is received at t_spike, it peaks at t_spike + tau_syn_X, with a maximum equal to the synaptic efficiency.\nThe ODEs are solved using the exponential Euler method.\nEquivalent code:\nIF_curr_alpha = Neuron(\n    parameters = dict(\n        v_rest = ann.Parameter(-65.0),\n        cm  = ann.Parameter(1.0),\n        tau_m  = ann.Parameter(20.0),\n        tau_syn_E = ann.Parameter(5.0),\n        tau_syn_I = ann.Parameter(5.0),\n        v_thresh = ann.Parameter(-50.0),\n        v_reset = ann.Parameter(-65.0),\n        i_offset = ann.Parameter(0.0),\n    ), \n    equations = [\n        # Scaling\n        'gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)',\n        'gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)',\n\n        # Membrane potential\n        ann.Variable(\n            'cm * dv/dt = cm/tau_m*(v_rest -v)   + alpha_exc - alpha_inh + i_offset',\n            method='exponential', init=-65.0),\n\n        # Alpha-shaped conductance\n        ann.Variable('tau_syn_E * dg_exc/dt = - g_exc', method='exponential'),\n        ann.Variable('tau_syn_I * dg_inh/dt = - g_inh', method='exponential'),\n\n        ann.Variable('tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc', method='exponential'),\n        ann.Variable('tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh', method='exponential'),\n    ],\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = v_reset\",\n    refractory = 0.0\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv_rest\n\nResting membrane potential (mV)\n-65.0\n\n\ncm\n\nCapacity of the membrane (nF)\n1.0\n\n\ntau_m\n\nMembrane time constant (ms)\n20.0\n\n\ntau_refrac\n\nDuration of refractory period (ms)\n0.0\n\n\ntau_syn_E\n\nRise time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\n\nRise time of inhibitory synaptic current (ms)\n5.0\n\n\nv_thresh\n\nSpike threshold (mV)\n-50.0\n\n\nv_reset\n\nReset potential after a spike (mV)\n-65.0\n\n\ni_offset\n\nOffset current (nA)\n0.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_curr_alpha"
    ]
  },
  {
    "objectID": "reference/IF_curr_alpha.html#parameters",
    "href": "reference/IF_curr_alpha.html#parameters",
    "title": "IF_curr_alpha",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nv_rest\n\nResting membrane potential (mV)\n-65.0\n\n\ncm\n\nCapacity of the membrane (nF)\n1.0\n\n\ntau_m\n\nMembrane time constant (ms)\n20.0\n\n\ntau_refrac\n\nDuration of refractory period (ms)\n0.0\n\n\ntau_syn_E\n\nRise time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\n\nRise time of inhibitory synaptic current (ms)\n5.0\n\n\nv_thresh\n\nSpike threshold (mV)\n-50.0\n\n\nv_reset\n\nReset potential after a spike (mV)\n-65.0\n\n\ni_offset\n\nOffset current (nA)\n0.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_curr_alpha"
    ]
  },
  {
    "objectID": "reference/clear.html",
    "href": "reference/clear.html",
    "title": "clear",
    "section": "",
    "text": "clear(functions=True, neurons=True, synapses=True)\nClears all variables (erasing already defined populations, projections, monitors), as if you had just imported ANNarchy.\nUseful when re-running Jupyter/IPython notebooks multiple times:\nimport ANNarchy as ann\nann.clear()\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunctions\nbool\nif True (default), all functions defined with add_function are erased.\nTrue\n\n\nneurons\nbool\nif True (default), all neurons defined with Neuron are erased.\nTrue\n\n\nsynapses\nbool\nif True (default), all synapses defined with Synapse are erased.\nTrue",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "clear"
    ]
  },
  {
    "objectID": "reference/clear.html#parameters",
    "href": "reference/clear.html#parameters",
    "title": "clear",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfunctions\nbool\nif True (default), all functions defined with add_function are erased.\nTrue\n\n\nneurons\nbool\nif True (default), all neurons defined with Neuron are erased.\nTrue\n\n\nsynapses\nbool\nif True (default), all synapses defined with Synapse are erased.\nTrue",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "clear"
    ]
  },
  {
    "objectID": "reference/Pruning.html",
    "href": "reference/Pruning.html",
    "title": "Pruning",
    "section": "",
    "text": "Pruning(self, equation, proba=1.0)\nDataclass to represent a pruning condition for structural plasticity.\nWhen the condition is true, a synapse is pruned with the specified probability.\nPruningSynapse = ann.Synapse(\n    parameters = dict(T = ann.Parameter(10000, 'global', int),\n    equations = ann.Variable('''\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n            else :\n                age + 1 : init = 0, int\n    ''', init=0, type=int)\n    pruning = ann.Pruning(\"age &gt; T\", proba = 0.5),\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nequation\nstr\nstring representing the equation.\nrequired\n\n\nproba\nfloat\nprobability of pruning of the synapse.\n1.0",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Pruning"
    ]
  },
  {
    "objectID": "reference/Pruning.html#parameters",
    "href": "reference/Pruning.html#parameters",
    "title": "Pruning",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nequation\nstr\nstring representing the equation.\nrequired\n\n\nproba\nfloat\nprobability of pruning of the synapse.\n1.0",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Pruning"
    ]
  },
  {
    "objectID": "reference/get_population.html",
    "href": "reference/get_population.html",
    "title": "get_population",
    "section": "",
    "text": "get_population(name, net_id=0)\nReturns the population with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the population.\nrequired",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "get_population"
    ]
  },
  {
    "objectID": "reference/get_population.html#parameters",
    "href": "reference/get_population.html#parameters",
    "title": "get_population",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the population.\nrequired",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "get_population"
    ]
  },
  {
    "objectID": "reference/STP.html",
    "href": "reference/STP.html",
    "title": "STP",
    "section": "",
    "text": "STP(self, tau_rec=100.0, tau_facil=0.01, U=0.5)\nSynapse exhibiting short-term facilitation and depression.\nImplemented using the model of Tsodyks, Markram et al.:\n\nTsodyks, Uziel and Markram (2000) Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses. Journal of Neuroscience 20:RC50\n\nNote that the time constant of the post-synaptic current is set in the neuron model, not here.\nEquivalent code:\nSTP = ann.Synapse(\n    parameters = dict(\n        tau_rec = 100.0,\n        tau_facil = 0.01,\n        U = 0.5,\n    ),\n    equations = [\n        ann.Variable('dx/dt = (1 - x)/tau_rec', init = 1.0, method='event-driven'),\n        ann.Variable('du/dt = (U - u)/tau_facil', init = 0.5, method='event-driven'),\n    ],\n    pre_spike=\"\"\"\n        g_target += w * u * x\n        x *= (1 - u)\n        u += U * (1 - u)\n    \"\"\"\n)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntau_rec\n\ndepression time constant (ms).\n100.0\n\n\ntau_facil\n\nfacilitation time constant (ms).\n0.01\n\n\nU\n\nuse parameter.\n0.5",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "STP"
    ]
  },
  {
    "objectID": "reference/STP.html#parameters",
    "href": "reference/STP.html#parameters",
    "title": "STP",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntau_rec\n\ndepression time constant (ms).\n100.0\n\n\ntau_facil\n\nfacilitation time constant (ms).\n0.01\n\n\nU\n\nuse parameter.\n0.5",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "STP"
    ]
  },
  {
    "objectID": "reference/Exponential.html",
    "href": "reference/Exponential.html",
    "title": "Exponential",
    "section": "",
    "text": "Exponential(self, Lambda, min=None, max=None, rng=None)\nExponential distribution, according to the density function:\nP(x | \\lambda) = \\lambda e^{(-\\lambda x )}\nNote: Lambda is capitalized, otherwise it would be a reserved Python keyword.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nLambda\nfloat\nrate parameter.\nrequired\n\n\nmin\nfloat\nminimum value (default: unlimited).\nNone\n\n\nmax\nfloat\nmaximum value (default: unlimited).\nNone\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Exponential"
    ]
  },
  {
    "objectID": "reference/Exponential.html#parameters",
    "href": "reference/Exponential.html#parameters",
    "title": "Exponential",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nLambda\nfloat\nrate parameter.\nrequired\n\n\nmin\nfloat\nminimum value (default: unlimited).\nNone\n\n\nmax\nfloat\nmaximum value (default: unlimited).\nNone\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Exponential"
    ]
  },
  {
    "objectID": "reference/Exponential.html#methods",
    "href": "reference/Exponential.html#methods",
    "title": "Exponential",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Exponential"
    ]
  },
  {
    "objectID": "reference/clear_all_callbacks.html",
    "href": "reference/clear_all_callbacks.html",
    "title": "clear_all_callbacks",
    "section": "",
    "text": "clear_all_callbacks\nclear_all_callbacks(net_id=0)\nClears the list of declared callbacks for the network.\nCannot be undone!",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "clear_all_callbacks"
    ]
  },
  {
    "objectID": "reference/simulate_until.html",
    "href": "reference/simulate_until.html",
    "title": "simulate_until",
    "section": "",
    "text": "simulate_until(\n    max_duration,\n    population,\n    operator='and',\n    measure_time=False,\n    net_id=0,\n)\nRuns the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped.\nOne can specify several populations. If the stop condition is true for any of the populations, the simulation will stop (‘or’ function).\nReturns the actual duration of the simulation in milliseconds.\nExample:\npop1 = ann.Population( ..., stop_condition = \"r &gt; 1.0 : any\")\nann.compile()\nduration = ann.simulate_until(max_duration=1000.0, population=pop1)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmax_duration\nfloat\nMaximum duration of the simulation in milliseconds.\nrequired\n\n\npopulation\nPopulation | list[Population]\n(list of) population(s) whose stop_condition should be checked to stop the simulation.\nrequired\n\n\noperator\n\nOperator to be used (‘and’ or ‘or’) when multiple populations are provided (default: ‘and’).\n'and'\n\n\nmeasure_time\nbool\nDefines whether the simulation time should be printed (default=False).\nFalse",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "simulate_until"
    ]
  },
  {
    "objectID": "reference/simulate_until.html#parameters",
    "href": "reference/simulate_until.html#parameters",
    "title": "simulate_until",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmax_duration\nfloat\nMaximum duration of the simulation in milliseconds.\nrequired\n\n\npopulation\nPopulation | list[Population]\n(list of) population(s) whose stop_condition should be checked to stop the simulation.\nrequired\n\n\noperator\n\nOperator to be used (‘and’ or ‘or’) when multiple populations are provided (default: ‘and’).\n'and'\n\n\nmeasure_time\nbool\nDefines whether the simulation time should be printed (default=False).\nFalse",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "simulate_until"
    ]
  },
  {
    "objectID": "reference/monitors.html",
    "href": "reference/monitors.html",
    "title": "monitors",
    "section": "",
    "text": "monitors\nmonitors(net_id=0, obj=None)\nReturns a list of declared monitors.\nBy default, all monitors are returned.\nBy setting obj, only monitors recording from this object, either Population or Projection, will be returned.",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "monitors"
    ]
  },
  {
    "objectID": "reference/disable_learning.html",
    "href": "reference/disable_learning.html",
    "title": "disable_learning",
    "section": "",
    "text": "disable_learning(projections=None, net_id=0)\nDisables learning for all projections.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojections\n\nthe projections whose learning should be disabled. By default, all the existing projections are disabled.\nNone",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "disable_learning"
    ]
  },
  {
    "objectID": "reference/disable_learning.html#parameters",
    "href": "reference/disable_learning.html#parameters",
    "title": "disable_learning",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nprojections\n\nthe projections whose learning should be disabled. By default, all the existing projections are disabled.\nNone",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "disable_learning"
    ]
  },
  {
    "objectID": "reference/Monitor.html",
    "href": "reference/Monitor.html",
    "title": "Monitor",
    "section": "",
    "text": "Monitor(\n    self,\n    obj,\n    variables=[],\n    period=None,\n    period_offset=None,\n    start=True,\n    name=None,\n    net_id=0,\n)\nObject allowing to record variables from Population, PopulationView, Dendrite or Projection instances.\nThis object should not be created directly, but returned by Network.monitor():\nm = net.monitor(pop, ['g_exc', 'v', 'spike'], period=10.0, start=False)\nMonitors are started by default after compile(). You can control their recording behavior with the start(), stop(), pause() and resume() methods.\nm.start() # Start recording\nnet.simulate(T)\nm.pause() # Pause recording\nnet.simulate(T)\nm.resume() # Resume recording\nnet.simulate(T)\n\ndata = m.get() # Get the data\nFor spiking networks recording 'spike', some utilities allow to easily compute raster plots /other statistics or mean firing rates over time/neuron axes:\nspikes = m.get('spike')\n\nt, n = m.raster_plot(spikes)\nhisto = m.histogram()\nisi = m.inter_spike_interval(spikes)\ncov = m.coefficient_of_variation(spikes)\nfr = m.mean_fr(spikes)\nr = m.smoothed_rate(spikes, smooth=100.)\nr_mean = m.population_rate(spikes, smooth=100.)\n\n\n\n\n\nName\nDescription\n\n\n\n\nperiod\nPeriod of recording in milliseconds.\n\n\nperiod_offset\nOffset of recording within a period, in milliseconds.\n\n\nvariables\nCurrent list of recorded variables.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget\nReturns the recorded variables and empties the buffer.\n\n\nstart\nStarts recording the variable.\n\n\npause\nPauses the recording.\n\n\nresume\nResumes the recording.\n\n\nstop\nStops the recording.\n\n\nreset\nReset the monitor to its initial state.\n\n\nsave\nSaves the recorded variables as a Numpy array (first dimension is time, second is neuron index).\n\n\ntimes\nReturns the start and stop times (in ms) of the recorded variables as a dictionary.\n\n\nraster_plot\nReturns two numpy arrays representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.\n\n\nhistogram\nReturns a histogram for the recorded spikes in the population.\n\n\ninter_spike_interval\nComputes the inter-spike intervals (ISI) for the recorded spikes in the population.\n\n\ncoefficient_of_variation\nComputes the coefficient of variation for the recorded spikes in the population.\n\n\nmean_fr\nComputes the mean firing rate in the population during the recordings.\n\n\nsmoothed_rate\nComputes the smoothed firing rate of the recorded spiking neurons.\n\n\npopulation_rate\nComputes a smoothed firing rate for the population of recorded neurons.\n\n\n\n\n\nget(variables=None, keep=False, reshape=False, force_dict=False)\nReturns the recorded variables and empties the buffer.\nThe recorded data is returned as a Numpy array (first dimension is time, second is neuron index).\nIf a single variable name is provided, the recorded values for this variable are directly returned as an array. If a list is provided or the argument left empty, a dictionary with all recorded variables is returned.\nThe spike variable of a population will be returned as a dictionary of lists, where the key is the neuron index, and the list contains the spike times (in steps; multiply by net.dt to get spike times in milliseconds) for each recorded neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nstr | list[str]\n(list of) variables. By default, a dictionary with all variables is returned.\nNone\n\n\nkeep\nbool\ndefines if the content in memory for each variable should be kept (default: False).\nFalse\n\n\nreshape\nbool\ntransforms the second axis of the array to match the population’s geometry (default: False).\nFalse\n\n\n\n\n\n\n\nstart(variables=None, period=None)\nStarts recording the variable.\nIt is called automatically after Network.compile() if the flag start=False was not passed to the constructor.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nlist\nsingle variable name or list of variable names to start recording (default: the variables argument passed to the constructor).\nNone\n\n\nperiod\nfloat\ndelay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View).\nNone\n\n\n\n\n\n\n\npause()\nPauses the recording.\n\n\n\nresume()\nResumes the recording.\n\n\n\nstop()\nStops the recording.\nWarning: This will delete the content of the C++ object and all data not previously retrieved is lost.\n\n\n\nreset()\nReset the monitor to its initial state.\n\n\n\nsave(filename, variables=None, keep=False, reshape=False, force_dict=False)\nSaves the recorded variables as a Numpy array (first dimension is time, second is neuron index).\nIf a single variable name is provided, the recorded values for this variable are directly saved. If a list is provided or the argument left empty, a dictionary with all recorded variables is saved.\nThe spike variable of a population will be returned as a dictionary of lists containing the spike times (in steps; multiply by net.dt to get spike times in milliseconds) for each recorded neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nname of the save file.\nrequired\n\n\nvariables\nstr | list[str]\n(list of) variables. By default, a dictionary with all variables is returned.\nNone\n\n\nkeep\nbool\ndefines if the content in memory for each variable should be kept (default: False).\nFalse\n\n\nreshape\nbool\ntransforms the second axis of the array to match the population’s geometry (default: False).\nFalse\n\n\n\n\n\n\n\ntimes(variables=None)\nReturns the start and stop times (in ms) of the recorded variables as a dictionary.\nIt should only be called after a call to get(), so that it describes when the variables have been recorded.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nlist[str]\n(list of) variables. By default, the times for all variables is returned.\nNone\n\n\n\n\n\n\n\nraster_plot(spikes=None)\nReturns two numpy arrays representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.\nExample:\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nt, n = m.raster_plot()\nplt.plot(t, n, '.')\nor:\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\nt, n = m.raster_plot(spikes)\nplt.plot(t, n, '.')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\nhistogram(spikes=None, bins=None, per_neuron=False, recording_window=None)\nReturns a histogram for the recorded spikes in the population.\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\nhisto = m.histogram(spikes)\nplt.plot(histo)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\nbins\n\nthe bin size in ms (default: dt).\nNone\n\n\n\n\n\n\n\ninter_spike_interval(spikes=None, ranks=None, per_neuron=False)\nComputes the inter-spike intervals (ISI) for the recorded spikes in the population.\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\nisi = m.inter_spike_interval(spikes)\nplt.hist(isi)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\nranks\nlist[int]\na list of neurons that should be evaluated. By default None, all neurons are evaluated.\nNone\n\n\nper_neuron\nbool\nif set to True, the computed inter-spike intervals are stored per neuron (analog to spikes), otherwise all values are stored in one huge vector (default: False).\nFalse\n\n\n\n\n\n\n\ncoefficient_of_variation(spikes=None, ranks=None)\nComputes the coefficient of variation for the recorded spikes in the population.\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\ncov = m.coefficient_of_variation(spikes)\nplt.hist(isi)\n:ranks: a list of neurons that should be evaluated. By default (None), all neurons are evaluated.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\nmean_fr(spikes=None)\nComputes the mean firing rate in the population during the recordings.\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\nfr = m.mean_fr(spikes)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\nsmoothed_rate(spikes=None, smooth=0.0)\nComputes the smoothed firing rate of the recorded spiking neurons.\nThe first axis is the neuron index, the second is time.\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\nr = m.smoothed_rate(spikes, smooth=100.)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\nsmooth\nfloat\nsmoothing time constant. Default: 0.0 (no smoothing).\n0.0\n\n\n\n\n\n\n\npopulation_rate(spikes=None, smooth=0.0)\nComputes a smoothed firing rate for the population of recorded neurons.\nThis method is faster than calling smoothed_rate and then averaging.\nIf spikes is left empty, get('spike') will be called. Beware: this erases the data from memory.\nExample:\nm = net.monitor(P[:1000], 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\nr = m.population_rate(spikes, smooth=100.)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike').\nNone\n\n\nsmooth\nfloat\nsmoothing time constant. Default: 0.0 (no smoothing).\n0.0",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Monitor"
    ]
  },
  {
    "objectID": "reference/Monitor.html#attributes",
    "href": "reference/Monitor.html#attributes",
    "title": "Monitor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nperiod\nPeriod of recording in milliseconds.\n\n\nperiod_offset\nOffset of recording within a period, in milliseconds.\n\n\nvariables\nCurrent list of recorded variables.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Monitor"
    ]
  },
  {
    "objectID": "reference/Monitor.html#methods",
    "href": "reference/Monitor.html#methods",
    "title": "Monitor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget\nReturns the recorded variables and empties the buffer.\n\n\nstart\nStarts recording the variable.\n\n\npause\nPauses the recording.\n\n\nresume\nResumes the recording.\n\n\nstop\nStops the recording.\n\n\nreset\nReset the monitor to its initial state.\n\n\nsave\nSaves the recorded variables as a Numpy array (first dimension is time, second is neuron index).\n\n\ntimes\nReturns the start and stop times (in ms) of the recorded variables as a dictionary.\n\n\nraster_plot\nReturns two numpy arrays representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.\n\n\nhistogram\nReturns a histogram for the recorded spikes in the population.\n\n\ninter_spike_interval\nComputes the inter-spike intervals (ISI) for the recorded spikes in the population.\n\n\ncoefficient_of_variation\nComputes the coefficient of variation for the recorded spikes in the population.\n\n\nmean_fr\nComputes the mean firing rate in the population during the recordings.\n\n\nsmoothed_rate\nComputes the smoothed firing rate of the recorded spiking neurons.\n\n\npopulation_rate\nComputes a smoothed firing rate for the population of recorded neurons.\n\n\n\n\n\nget(variables=None, keep=False, reshape=False, force_dict=False)\nReturns the recorded variables and empties the buffer.\nThe recorded data is returned as a Numpy array (first dimension is time, second is neuron index).\nIf a single variable name is provided, the recorded values for this variable are directly returned as an array. If a list is provided or the argument left empty, a dictionary with all recorded variables is returned.\nThe spike variable of a population will be returned as a dictionary of lists, where the key is the neuron index, and the list contains the spike times (in steps; multiply by net.dt to get spike times in milliseconds) for each recorded neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nstr | list[str]\n(list of) variables. By default, a dictionary with all variables is returned.\nNone\n\n\nkeep\nbool\ndefines if the content in memory for each variable should be kept (default: False).\nFalse\n\n\nreshape\nbool\ntransforms the second axis of the array to match the population’s geometry (default: False).\nFalse\n\n\n\n\n\n\n\nstart(variables=None, period=None)\nStarts recording the variable.\nIt is called automatically after Network.compile() if the flag start=False was not passed to the constructor.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nlist\nsingle variable name or list of variable names to start recording (default: the variables argument passed to the constructor).\nNone\n\n\nperiod\nfloat\ndelay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View).\nNone\n\n\n\n\n\n\n\npause()\nPauses the recording.\n\n\n\nresume()\nResumes the recording.\n\n\n\nstop()\nStops the recording.\nWarning: This will delete the content of the C++ object and all data not previously retrieved is lost.\n\n\n\nreset()\nReset the monitor to its initial state.\n\n\n\nsave(filename, variables=None, keep=False, reshape=False, force_dict=False)\nSaves the recorded variables as a Numpy array (first dimension is time, second is neuron index).\nIf a single variable name is provided, the recorded values for this variable are directly saved. If a list is provided or the argument left empty, a dictionary with all recorded variables is saved.\nThe spike variable of a population will be returned as a dictionary of lists containing the spike times (in steps; multiply by net.dt to get spike times in milliseconds) for each recorded neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nname of the save file.\nrequired\n\n\nvariables\nstr | list[str]\n(list of) variables. By default, a dictionary with all variables is returned.\nNone\n\n\nkeep\nbool\ndefines if the content in memory for each variable should be kept (default: False).\nFalse\n\n\nreshape\nbool\ntransforms the second axis of the array to match the population’s geometry (default: False).\nFalse\n\n\n\n\n\n\n\ntimes(variables=None)\nReturns the start and stop times (in ms) of the recorded variables as a dictionary.\nIt should only be called after a call to get(), so that it describes when the variables have been recorded.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nlist[str]\n(list of) variables. By default, the times for all variables is returned.\nNone\n\n\n\n\n\n\n\nraster_plot(spikes=None)\nReturns two numpy arrays representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.\nExample:\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nt, n = m.raster_plot()\nplt.plot(t, n, '.')\nor:\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\nt, n = m.raster_plot(spikes)\nplt.plot(t, n, '.')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\nhistogram(spikes=None, bins=None, per_neuron=False, recording_window=None)\nReturns a histogram for the recorded spikes in the population.\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\nhisto = m.histogram(spikes)\nplt.plot(histo)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\nbins\n\nthe bin size in ms (default: dt).\nNone\n\n\n\n\n\n\n\ninter_spike_interval(spikes=None, ranks=None, per_neuron=False)\nComputes the inter-spike intervals (ISI) for the recorded spikes in the population.\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\nisi = m.inter_spike_interval(spikes)\nplt.hist(isi)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\nranks\nlist[int]\na list of neurons that should be evaluated. By default None, all neurons are evaluated.\nNone\n\n\nper_neuron\nbool\nif set to True, the computed inter-spike intervals are stored per neuron (analog to spikes), otherwise all values are stored in one huge vector (default: False).\nFalse\n\n\n\n\n\n\n\ncoefficient_of_variation(spikes=None, ranks=None)\nComputes the coefficient of variation for the recorded spikes in the population.\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\ncov = m.coefficient_of_variation(spikes)\nplt.hist(isi)\n:ranks: a list of neurons that should be evaluated. By default (None), all neurons are evaluated.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\nmean_fr(spikes=None)\nComputes the mean firing rate in the population during the recordings.\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\nfr = m.mean_fr(spikes)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\nsmoothed_rate(spikes=None, smooth=0.0)\nComputes the smoothed firing rate of the recorded spiking neurons.\nThe first axis is the neuron index, the second is time.\nm = net.monitor(pop, 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\nr = m.smoothed_rate(spikes, smooth=100.)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\nsmooth\nfloat\nsmoothing time constant. Default: 0.0 (no smoothing).\n0.0\n\n\n\n\n\n\n\npopulation_rate(spikes=None, smooth=0.0)\nComputes a smoothed firing rate for the population of recorded neurons.\nThis method is faster than calling smoothed_rate and then averaging.\nIf spikes is left empty, get('spike') will be called. Beware: this erases the data from memory.\nExample:\nm = net.monitor(P[:1000], 'spike')\nnet.simulate(1000.0)\n\nspikes = m.get('spike')\nr = m.population_rate(spikes, smooth=100.)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike').\nNone\n\n\nsmooth\nfloat\nsmoothing time constant. Default: 0.0 (no smoothing).\n0.0",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Monitor"
    ]
  },
  {
    "objectID": "reference/DiscreteUniform.html",
    "href": "reference/DiscreteUniform.html",
    "title": "DiscreteUniform",
    "section": "",
    "text": "DiscreteUniform(self, min, max, rng=None)\nDiscrete uniform distribution between min and max.\nThe returned values are integers in the range [min, max].\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmin\nint\nminimum value.\nrequired\n\n\nmax\nint\nmaximum value.\nrequired\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "DiscreteUniform"
    ]
  },
  {
    "objectID": "reference/DiscreteUniform.html#parameters",
    "href": "reference/DiscreteUniform.html#parameters",
    "title": "DiscreteUniform",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmin\nint\nminimum value.\nrequired\n\n\nmax\nint\nmaximum value.\nrequired\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "DiscreteUniform"
    ]
  },
  {
    "objectID": "reference/DiscreteUniform.html#methods",
    "href": "reference/DiscreteUniform.html#methods",
    "title": "DiscreteUniform",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "DiscreteUniform"
    ]
  },
  {
    "objectID": "reference/Copy.html",
    "href": "reference/Copy.html",
    "title": "Copy",
    "section": "",
    "text": "Copy(\n    self,\n    pre,\n    post,\n    target,\n    psp='pre.r * w',\n    operation='sum',\n    name=None,\n    copied=False,\n    net_id=0,\n)\nCreates a virtual projection reusing the weights and delays of an already-defined projection.\nAlthough the original projection can be learnable, this one can not. Changes in the original weights will be reflected in this projection. The only possible modifications are psp and operation.\nThe pre- and post-synaptic populations of both projections must have the same geometry.\nExample:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.convolution import Copy\n\nnet = ann.Network()\n\npop1 = net.create(1000, ann.Izhikevich)\npop2 = net.create(1000, ann.Izhikevich)\npop3 = net.create(1000, ann.Izhikevich)\n\nproj = ann.Projection(pop1, pop2, \"exc\")\nproj.fixed_probability(0.1, 0.5)\n\ncopy_proj = Copy(pop1, pop3, \"exc\")\ncopy_proj.copy(proj)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\n\npre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\n\npost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\n\ntype of the connection\nrequired\n\n\npsp\n\ncontinuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r).\n'pre.r * w'\n\n\noperation\n\noperation (sum, max, min, mean) performed by the kernel (default: sum).\n'sum'\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncopy\nInstantiates the projection.\n\n\nsave_connectivity\nNot available.\n\n\nsave\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nconnectivity_matrix\nNot available.\n\n\n\n\n\ncopy(projection)\nInstantiates the projection.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojection\n\nExisting projection to copy.\nrequired\n\n\n\n\n\n\n\nsave_connectivity(filename)\nNot available.\n\n\n\nsave(filename)\nNot available.\n\n\n\nload(filename)\nNot available.\n\n\n\nreceptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nconnectivity_matrix(fill=0.0)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Copy"
    ]
  },
  {
    "objectID": "reference/Copy.html#parameters",
    "href": "reference/Copy.html#parameters",
    "title": "Copy",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\n\npre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\n\npost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\n\ntype of the connection\nrequired\n\n\npsp\n\ncontinuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r).\n'pre.r * w'\n\n\noperation\n\noperation (sum, max, min, mean) performed by the kernel (default: sum).\n'sum'",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Copy"
    ]
  },
  {
    "objectID": "reference/Copy.html#methods",
    "href": "reference/Copy.html#methods",
    "title": "Copy",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncopy\nInstantiates the projection.\n\n\nsave_connectivity\nNot available.\n\n\nsave\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nconnectivity_matrix\nNot available.\n\n\n\n\n\ncopy(projection)\nInstantiates the projection.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojection\n\nExisting projection to copy.\nrequired\n\n\n\n\n\n\n\nsave_connectivity(filename)\nNot available.\n\n\n\nsave(filename)\nNot available.\n\n\n\nload(filename)\nNot available.\n\n\n\nreceptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nconnectivity_matrix(fill=0.0)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Copy"
    ]
  },
  {
    "objectID": "reference/Neuron.html",
    "href": "reference/Neuron.html",
    "title": "Neuron",
    "section": "",
    "text": "Neuron(\n    self,\n    parameters='',\n    equations='',\n    spike=None,\n    axon_spike=None,\n    reset=None,\n    axon_reset=None,\n    refractory=None,\n    functions=None,\n    name='',\n    description='',\n    extra_values={},\n)\nBase class to define a neuron model.\nNeurons are rate-coded by default (in which case they must define the variable r). parameters expects a dictionary of parameter values, equations expects a list of variables.\nSpiking neurons must define the spike condition (and usually also reset). They do not need to define r.\nLIF = ann.Neuron(\n    parameters = dict(\n        tau = 10.0\n    ),\n    equations = [\n        \"tau * dv/dt + v = g_exc\",\n    ],\n    spike = \"v &gt; 30.0\",\n    reset = \"v = 0.0\",\n    refractory = 5.0,\n    name = \"LIF\",\n    description = \"Leaky Integrate-and-Fire spiking neuron with time constant $\\tau$.\" \n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameters\nstr | dict\nparameters of the neuron and their initial value.\n''\n\n\nequations\nstr | list\nequations defining the temporal evolution of variables.\n''\n\n\nfunctions\nstr\nadditional functions used in the variables’ equations.\nNone\n\n\nspike\nstr\ncondition to emit a spike (only for spiking neurons).\nNone\n\n\naxon_spike\nstr\ncondition to emit an axonal spike (only for spiking neurons and optional). The axonal spike can appear additional to the spike and is independent from refractoriness of a neuron.\nNone\n\n\nreset\nstr | list\nchanges to the variables after a spike (only for spiking neurons).\nNone\n\n\naxon_reset\nstr | list\nchanges to the variables after an axonal spike (only for spiking neurons).\nNone\n\n\nrefractory\nstr\nrefractory period of a neuron after a spike (only for spiking neurons).\nNone\n\n\nname\nstr\nname of the neuron type (used for reporting only).\n''\n\n\ndescription\nstr\nshort description of the neuron type (used for reporting).\n''",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Neuron"
    ]
  },
  {
    "objectID": "reference/Neuron.html#parameters",
    "href": "reference/Neuron.html#parameters",
    "title": "Neuron",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nparameters\nstr | dict\nparameters of the neuron and their initial value.\n''\n\n\nequations\nstr | list\nequations defining the temporal evolution of variables.\n''\n\n\nfunctions\nstr\nadditional functions used in the variables’ equations.\nNone\n\n\nspike\nstr\ncondition to emit a spike (only for spiking neurons).\nNone\n\n\naxon_spike\nstr\ncondition to emit an axonal spike (only for spiking neurons and optional). The axonal spike can appear additional to the spike and is independent from refractoriness of a neuron.\nNone\n\n\nreset\nstr | list\nchanges to the variables after a spike (only for spiking neurons).\nNone\n\n\naxon_reset\nstr | list\nchanges to the variables after an axonal spike (only for spiking neurons).\nNone\n\n\nrefractory\nstr\nrefractory period of a neuron after a spike (only for spiking neurons).\nNone\n\n\nname\nstr\nname of the neuron type (used for reporting only).\n''\n\n\ndescription\nstr\nshort description of the neuron type (used for reporting).\n''",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Neuron"
    ]
  },
  {
    "objectID": "reference/Dendrite.html",
    "href": "reference/Dendrite.html",
    "title": "Dendrite",
    "section": "",
    "text": "Dendrite(self, proj, post_rank, idx)\nSub-group of a Projection for a single post-synaptic neuron.\nIt can not be created directly, only through a call to Projection.dendrite(rank):\ndendrite = proj.dendrite(6)\n\n\n\n\n\nName\nDescription\n\n\n\n\npost_rank\nRank of the post-synaptic neuron.\n\n\nproj\nParent projection.\n\n\nsize\nNumber of synapses reaching the post-synaptic neuron.\n\n\npre_ranks\nList of ranks of pre-synaptic neurons.\n\n\nsynapses\nIteratively returns the synapses corresponding to this dendrite.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nsynapse\nReturns the synapse coming from the corresponding presynaptic neuron.\n\n\nset\nSets the value of a parameter/variable on all synapses in the dendrite.\n\n\nget\nReturns the value of a parameter/variable.\n\n\nreceptive_field\nReturns the given variable as a receptive field.\n\n\ncreate_synapse\nCreates a single synapse for this dendrite with the given pre-synaptic neuron.\n\n\ncreate_synapses\nCreates a set of synapses for this dendrite with the given pre-synaptic neurons.\n\n\nprune_synapse\nRemoves the synapse with the given pre-synaptic neuron from the dendrite.\n\n\nprune_synapses\nRemoves the synapses which belong to the provided pre-synaptic neurons from the dendrite.\n\n\n\n\n\nsynapse(pos)\nReturns the synapse coming from the corresponding presynaptic neuron.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npos\nint | tuple[int]\ncan be either the rank or the coordinates of the presynaptic neuron\nrequired\n\n\n\n\n\n\n\nset(value)\nSets the value of a parameter/variable on all synapses in the dendrite.\ndendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } )\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\ndict\na dictionary containing the parameter/variable names as keys.\nrequired\n\n\n\n\n\n\n\nget(name)\nReturns the value of a parameter/variable.\ndendrite.get('w')\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the parameter/variable.\nrequired\n\n\n\n\n\n\n\nreceptive_field(variable='w', fill=0.0)\nReturns the given variable as a receptive field.\nA numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value fill).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr\nname of the variable (default = ‘w’)\n'w'\n\n\nfill\nfloat\nvalue to use when a synapse does not exist (default: 0.0).\n0.0\n\n\n\n\n\n\n\ncreate_synapse(rank, w=0.0, delay=0)\nCreates a single synapse for this dendrite with the given pre-synaptic neuron.\nThe configuration key 'structural_plasticity' must be set to True before compile() for this method to work.\nnet = ann.Network()\nnet.config(structural_plasticity=True)\nnet.compile()\n\ntry:\n    proj.dendrite(10).create_synapse(rank=20, w=0.1, delay=0.0)\nexcept Exception as e:\n    print(e)\nIf the synapse already exists, an error is thrown, so make sure to catch the exception.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the pre-synaptic neuron\nrequired\n\n\nw\nfloat\nsynaptic weight.\n0.0\n\n\ndelay\nfloat\nsynaptic delay in milliseconds that should be a multiple of dt.\n0\n\n\n\n\n\n\n\ncreate_synapses(ranks, weights=None, delays=None)\nCreates a set of synapses for this dendrite with the given pre-synaptic neurons.\nThe configuration key 'structural_plasticity' must be set to True before compile() for this method to work.\nnet = ann.Network()\nnet.config(structural_plasticity=True)\nnet.compile()\n\ntry:\n    proj.dendrite(10).create_synapses(ranks=[20, 30, 40], weights=0.1, delay=0.0)\nexcept Exception as e:\n    print(e)\nIf the synapses already exist, an error is thrown, so make sure to catch the exception.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nranks\nlist[int]\nlist of ranks for the pre-synaptic neurons.\nrequired\n\n\nweights\nfloat | list[float]\nlist of synaptic weights (default: 0.0).\nNone\n\n\ndelays\nfloat | list[float]\nlist of synaptic delays (default = dt).\nNone\n\n\n\n\n\n\n\nprune_synapse(rank)\nRemoves the synapse with the given pre-synaptic neuron from the dendrite.\nThe configuration key 'structural_plasticity' must be set to True before compile() for this method to work.\nnet = ann.Network()\nnet.config(structural_plasticity=True)\nnet.compile()\n\ntry:\n    proj.dendrite(10).prune_synapse(rank=20)\nexcept Exception as e:\n    print(e)\nIf the synapse does not exist, an error is thrown, so make sure to catch the exception.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the pre-synaptic neuron\nrequired\n\n\n\n\n\n\n\nprune_synapses(ranks)\nRemoves the synapses which belong to the provided pre-synaptic neurons from the dendrite.\nThe configuration key 'structural_plasticity' must be set to True before compile() for this method to work.\nnet = ann.Network()\nnet.config(structural_plasticity=True)\nnet.compile()\n\ntry:\n    proj.dendrite(10).prune_synapses(ranks=[20, 30, 40])\nexcept Exception as e:\n    print(e)\nIf the synapses do not exist, an error is thrown, so make sure to catch the exception.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nranks\nlist[int]\nlist of ranks of the pre-synaptic neurons.\nrequired",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Dendrite"
    ]
  },
  {
    "objectID": "reference/Dendrite.html#attributes",
    "href": "reference/Dendrite.html#attributes",
    "title": "Dendrite",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npost_rank\nRank of the post-synaptic neuron.\n\n\nproj\nParent projection.\n\n\nsize\nNumber of synapses reaching the post-synaptic neuron.\n\n\npre_ranks\nList of ranks of pre-synaptic neurons.\n\n\nsynapses\nIteratively returns the synapses corresponding to this dendrite.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Dendrite"
    ]
  },
  {
    "objectID": "reference/Dendrite.html#methods",
    "href": "reference/Dendrite.html#methods",
    "title": "Dendrite",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nsynapse\nReturns the synapse coming from the corresponding presynaptic neuron.\n\n\nset\nSets the value of a parameter/variable on all synapses in the dendrite.\n\n\nget\nReturns the value of a parameter/variable.\n\n\nreceptive_field\nReturns the given variable as a receptive field.\n\n\ncreate_synapse\nCreates a single synapse for this dendrite with the given pre-synaptic neuron.\n\n\ncreate_synapses\nCreates a set of synapses for this dendrite with the given pre-synaptic neurons.\n\n\nprune_synapse\nRemoves the synapse with the given pre-synaptic neuron from the dendrite.\n\n\nprune_synapses\nRemoves the synapses which belong to the provided pre-synaptic neurons from the dendrite.\n\n\n\n\n\nsynapse(pos)\nReturns the synapse coming from the corresponding presynaptic neuron.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npos\nint | tuple[int]\ncan be either the rank or the coordinates of the presynaptic neuron\nrequired\n\n\n\n\n\n\n\nset(value)\nSets the value of a parameter/variable on all synapses in the dendrite.\ndendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } )\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\ndict\na dictionary containing the parameter/variable names as keys.\nrequired\n\n\n\n\n\n\n\nget(name)\nReturns the value of a parameter/variable.\ndendrite.get('w')\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the parameter/variable.\nrequired\n\n\n\n\n\n\n\nreceptive_field(variable='w', fill=0.0)\nReturns the given variable as a receptive field.\nA numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value fill).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr\nname of the variable (default = ‘w’)\n'w'\n\n\nfill\nfloat\nvalue to use when a synapse does not exist (default: 0.0).\n0.0\n\n\n\n\n\n\n\ncreate_synapse(rank, w=0.0, delay=0)\nCreates a single synapse for this dendrite with the given pre-synaptic neuron.\nThe configuration key 'structural_plasticity' must be set to True before compile() for this method to work.\nnet = ann.Network()\nnet.config(structural_plasticity=True)\nnet.compile()\n\ntry:\n    proj.dendrite(10).create_synapse(rank=20, w=0.1, delay=0.0)\nexcept Exception as e:\n    print(e)\nIf the synapse already exists, an error is thrown, so make sure to catch the exception.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the pre-synaptic neuron\nrequired\n\n\nw\nfloat\nsynaptic weight.\n0.0\n\n\ndelay\nfloat\nsynaptic delay in milliseconds that should be a multiple of dt.\n0\n\n\n\n\n\n\n\ncreate_synapses(ranks, weights=None, delays=None)\nCreates a set of synapses for this dendrite with the given pre-synaptic neurons.\nThe configuration key 'structural_plasticity' must be set to True before compile() for this method to work.\nnet = ann.Network()\nnet.config(structural_plasticity=True)\nnet.compile()\n\ntry:\n    proj.dendrite(10).create_synapses(ranks=[20, 30, 40], weights=0.1, delay=0.0)\nexcept Exception as e:\n    print(e)\nIf the synapses already exist, an error is thrown, so make sure to catch the exception.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nranks\nlist[int]\nlist of ranks for the pre-synaptic neurons.\nrequired\n\n\nweights\nfloat | list[float]\nlist of synaptic weights (default: 0.0).\nNone\n\n\ndelays\nfloat | list[float]\nlist of synaptic delays (default = dt).\nNone\n\n\n\n\n\n\n\nprune_synapse(rank)\nRemoves the synapse with the given pre-synaptic neuron from the dendrite.\nThe configuration key 'structural_plasticity' must be set to True before compile() for this method to work.\nnet = ann.Network()\nnet.config(structural_plasticity=True)\nnet.compile()\n\ntry:\n    proj.dendrite(10).prune_synapse(rank=20)\nexcept Exception as e:\n    print(e)\nIf the synapse does not exist, an error is thrown, so make sure to catch the exception.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the pre-synaptic neuron\nrequired\n\n\n\n\n\n\n\nprune_synapses(ranks)\nRemoves the synapses which belong to the provided pre-synaptic neurons from the dendrite.\nThe configuration key 'structural_plasticity' must be set to True before compile() for this method to work.\nnet = ann.Network()\nnet.config(structural_plasticity=True)\nnet.compile()\n\ntry:\n    proj.dendrite(10).prune_synapses(ranks=[20, 30, 40])\nexcept Exception as e:\n    print(e)\nIf the synapses do not exist, an error is thrown, so make sure to catch the exception.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nranks\nlist[int]\nlist of ranks of the pre-synaptic neurons.\nrequired",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Dendrite"
    ]
  },
  {
    "objectID": "reference/balloon_CL.html",
    "href": "reference/balloon_CL.html",
    "title": "balloon_CL",
    "section": "",
    "text": "balloon_CL(\n    self,\n    phi=1.0,\n    kappa=1 / 1.54,\n    gamma=1 / 2.46,\n    E_0=0.34,\n    tau=0.98,\n    alpha=0.33,\n    V_0=0.02,\n    v_0=40.3,\n    TE=40 / 1000.0,\n    epsilon=1.43,\n)\nA balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007).\nEquivalent code:\nballoon_CL = BoldModel(\n    parameters = dict(\n        second    = 1000.0,\n        phi       = 1.0,\n        kappa     = 1/1.54,\n        gamma     = 1/2.46,\n        E_0       = 0.34,\n        tau       = 0.98,\n        alpha     = 0.33,\n        V_0       = 0.02,\n        v_0       = 40.3,\n        TE        = 40/1000.,\n        epsilon   = 1.43,\n    ),\n    equations = [\n        # Single input\n        ann.Variable('I_CBF = sum(I_CBF)', init=0.0),\n        ann.Variable('ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second', init=0.0),\n        ann.Variable('df_in/dt = s / second', init=1.0, min=0.01),\n\n        ann.Variable('E = 1 - (1 - E_0)**(1 / f_in)', init=0.3424),\n        ann.Variable('dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)', init=1.0, min=0.01),\n        ann.Variable('dv/dt = (f_in - f_out)/(tau*second), init=1.0, min=0.01),\n        ann.Variable('f_out = v**(1 / alpha)', init=1, min=0.01),\n\n        # Classic coefficients\n        ann.Variable('k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE'),\n        ann.Variable('k_2 = 2 * E_0'),\n        ann.Variable('k_3 = 1 - epsilon'),\n\n        # Non-linear equation\n        ann.Variable('BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v))'),\n    ],\n    inputs=\"I_CBF\",\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_CL"
    ]
  },
  {
    "objectID": "reference/balloon_CL.html#parameters",
    "href": "reference/balloon_CL.html#parameters",
    "title": "balloon_CL",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_CL"
    ]
  },
  {
    "objectID": "reference/CurrentInjection.html",
    "href": "reference/CurrentInjection.html",
    "title": "CurrentInjection",
    "section": "",
    "text": "CurrentInjection(self, pre, post, target, name=None, copied=False, net_id=0)\nInject current from a rate-coded population into a spiking population.\nThe pre-synaptic population must be be rate-coded, the post-synaptic one must be spiking, both must have the same size and no plasticity is allowed.\nFor each post-synaptic neuron, the current g_target (e.g. g_excor g_inh) will be set at each time step to the firing rate r of the pre-synaptic neuron with the same rank.\nThe projection must be connected with connect_current(), which takes no parameter and does not accept delays. It is equivalent to one_to_one(weights=1.0).\nExample:\ninp = net.create(100, ann.Neuron(equations=\"r = 5*sin(t/1000)\"))\npop = net.create(100, ann.Izhikevich)\n\nproj = net.connect(ann.CurrentInjection(inp, pop, 'exc'))\nproj.connect_current()\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\nPopulation\npre-synaptic population.\nrequired\n\n\npost\nPopulation\npost-synaptic population.\nrequired\n\n\ntarget\nstr\ntype of the connection.\nrequired\n\n\nname\nstr\noptional name.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "CurrentInjection"
    ]
  },
  {
    "objectID": "reference/CurrentInjection.html#parameters",
    "href": "reference/CurrentInjection.html#parameters",
    "title": "CurrentInjection",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\nPopulation\npre-synaptic population.\nrequired\n\n\npost\nPopulation\npost-synaptic population.\nrequired\n\n\ntarget\nstr\ntype of the connection.\nrequired\n\n\nname\nstr\noptional name.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "CurrentInjection"
    ]
  },
  {
    "objectID": "reference/Population.html",
    "href": "reference/Population.html",
    "title": "Population",
    "section": "",
    "text": "Population(\n    self,\n    geometry,\n    neuron,\n    name=None,\n    stop_condition=None,\n    storage_order='post_to_pre',\n    copied=False,\n    net_id=0,\n)\nPopulation of neurons.\nThe object is returned by Network.create() and should not be created directly.\nnet = ann.Network()\npop = net.create(100, neuron=ann.Izhikevich, name=\"Excitatory population\")\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple | int\npopulation geometry as tuple. If an integer is given, it is the size of the population.\nrequired\n\n\nneuron\nNeuron\nNeuron instance. It can be user-defined or a built-in model.\nrequired\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\nstop_condition\nstr\na single condition on a neural variable which can stop the simulation whenever it is true.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ngeometry\nGeometry of the population.\n\n\nwidth\nWidth of the population.\n\n\nheight\nHeight of the population.\n\n\ndepth\nDepth of the population.\n\n\ndimension\nNumber of dimensions of the population.\n\n\nsize\nSize of the population (total number of neurons).\n\n\nranks\nArray of ranks in the population (between 0 and size - 1).\n\n\nparameters\nList of parameter names.\n\n\nvariables\nList of variable names.\n\n\nattributes\nList of attribute names (parameters + variables).\n\n\nfunctions\nList of functions defined by the neuron model.\n\n\ntargets\nList of connected targets.\n\n\nrefractory\nRefractory period (in ms).\n\n\nneurons\nReturns iteratively each neuron in the population.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nreset\nResets all parameters and variables of the population to the value they had before the call to net.compile().\n\n\nclear\nClears all spiking events previously emitted (history of spikes, delayed spikes).\n\n\nenable\n(Re)-enables computations in this population, after they were disabled by the disable() method.\n\n\ndisable\nTemporarily disables computations in this population (including the projections leading to it).\n\n\nset\nSets the value of neural variables and parameters from a dictionary.\n\n\nget\nReturns the value of a neural variable or parameter based on its name.\n\n\nsum\nReturns the array of weighted sums corresponding to the target:\n\n\ncompute_firing_rate\nTells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable r.\n\n\nneuron\nReturns an IndividualNeuron object wrapping the neuron with the provided rank or coordinates.\n\n\nrank_from_coordinates\nReturns the rank of a neuron based on coordinates.\n\n\ncoordinates_from_rank\nReturns the coordinates of a neuron based on its rank.\n\n\nnormalized_coordinates_from_rank\nReturns normalized coordinates of a neuron based on its rank.\n\n\nsave\nSaves all information about the population (structure, current value of parameters and variables) into a file.\n\n\nload\nLoad the saved state of the population by Population.save().\n\n\n\n\n\nreset(attributes=None)\nResets all parameters and variables of the population to the value they had before the call to net.compile().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattributes\nlist\nlist of attributes (parameter or variable) which should be reinitialized. Default: all attributes.\nNone\n\n\n\n\n\n\n\nclear()\nClears all spiking events previously emitted (history of spikes, delayed spikes).\nCan be useful if you do not want to totally reset a population (i.e. all variables), only to clear the spiking history between two trials.\nNote: does nothing for rate-coded networks.\n\n\n\nenable()\n(Re)-enables computations in this population, after they were disabled by the disable() method.\nThe status of the population is accessible through the enabled flag.\n\n\n\ndisable()\nTemporarily disables computations in this population (including the projections leading to it).\nYou can re-enable it with the enable() method.\n\n\n\nset(values)\nSets the value of neural variables and parameters from a dictionary.\nExample:\npop.set({'tau': 20.0, 'r': np.random.rand((8,8)) } )\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\ndict\ndictionary of attributes to be updated.\nrequired\n\n\n\n\n\n\n\nget(name)\nReturns the value of a neural variable or parameter based on its name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nattribute name as a string.\nrequired\n\n\n\n\n\n\n\nsum(target)\nReturns the array of weighted sums corresponding to the target:\nexcitatory = pop.sum('exc')\nFor spiking networks, this is equivalent to accessing the conductances directly:\nexcitatory = pop.g_exc\nIf no incoming projection has the given target, the method returns zeros.\nNote: it is not possible to distinguish the original population when the same target is used.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntarget\nstr\nthe desired projection target.\nrequired\n\n\n\n\n\n\n\ncompute_firing_rate(window)\nTells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable r.\nThis method has an effect on spiking neurons only.\nIf this method is not called, r will always be 0.0. r can of course be accessed and recorded as any other variable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwindow\nfloat\nwindow in ms over which the spikes will be counted.\nrequired\n\n\n\n\n\n\n\nneuron(*coord)\nReturns an IndividualNeuron object wrapping the neuron with the provided rank or coordinates.\n\n\n\nrank_from_coordinates(coord)\nReturns the rank of a neuron based on coordinates.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncoord\ntuple\ncoordinate tuple, can be multidimensional.\nrequired\n\n\n\n\n\n\n\ncoordinates_from_rank(rank)\nReturns the coordinates of a neuron based on its rank.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the neuron.\nrequired\n\n\n\n\n\n\n\nnormalized_coordinates_from_rank(rank, norm=1.0)\nReturns normalized coordinates of a neuron based on its rank.\nThe geometry of the population is mapped to the hypercube [0, 1]^d\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the neuron.\nrequired\n\n\nnorm\nfloat\nnorm of the hypercube (default = 1.0).\n1.0\n\n\n\n\n\n\n\nsave(filename)\nSaves all information about the population (structure, current value of parameters and variables) into a file.\n\nIf the file name is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the file name ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nIf the file name is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nOtherwise, the data will be pickled into a simple binary text file using pickle.\n\nWarning: The ‘.mat’ data will not be loadable by ANNarchy, it is only for external analysis purpose.\nExample:\npop.save('pop1.npz')\npop.save('pop1.txt')\npop.save('pop1.txt.gz')\npop.save('pop1.mat')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nFilename, may contain relative or absolute path.\nrequired\n\n\n\n\n\n\n\nload(filename, pickle_encoding=None)\nLoad the saved state of the population by Population.save().\nWarning: Matlab data can not be loaded.\nExample:\npop.load('pop1.npz')\npop.load('pop1.txt')\npop.load('pop1.txt.gz')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\n\nthe filename with relative or absolute path.\nrequired",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Population"
    ]
  },
  {
    "objectID": "reference/Population.html#parameters",
    "href": "reference/Population.html#parameters",
    "title": "Population",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple | int\npopulation geometry as tuple. If an integer is given, it is the size of the population.\nrequired\n\n\nneuron\nNeuron\nNeuron instance. It can be user-defined or a built-in model.\nrequired\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\nstop_condition\nstr\na single condition on a neural variable which can stop the simulation whenever it is true.\nNone",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Population"
    ]
  },
  {
    "objectID": "reference/Population.html#attributes",
    "href": "reference/Population.html#attributes",
    "title": "Population",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ngeometry\nGeometry of the population.\n\n\nwidth\nWidth of the population.\n\n\nheight\nHeight of the population.\n\n\ndepth\nDepth of the population.\n\n\ndimension\nNumber of dimensions of the population.\n\n\nsize\nSize of the population (total number of neurons).\n\n\nranks\nArray of ranks in the population (between 0 and size - 1).\n\n\nparameters\nList of parameter names.\n\n\nvariables\nList of variable names.\n\n\nattributes\nList of attribute names (parameters + variables).\n\n\nfunctions\nList of functions defined by the neuron model.\n\n\ntargets\nList of connected targets.\n\n\nrefractory\nRefractory period (in ms).\n\n\nneurons\nReturns iteratively each neuron in the population.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Population"
    ]
  },
  {
    "objectID": "reference/Population.html#methods",
    "href": "reference/Population.html#methods",
    "title": "Population",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nreset\nResets all parameters and variables of the population to the value they had before the call to net.compile().\n\n\nclear\nClears all spiking events previously emitted (history of spikes, delayed spikes).\n\n\nenable\n(Re)-enables computations in this population, after they were disabled by the disable() method.\n\n\ndisable\nTemporarily disables computations in this population (including the projections leading to it).\n\n\nset\nSets the value of neural variables and parameters from a dictionary.\n\n\nget\nReturns the value of a neural variable or parameter based on its name.\n\n\nsum\nReturns the array of weighted sums corresponding to the target:\n\n\ncompute_firing_rate\nTells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable r.\n\n\nneuron\nReturns an IndividualNeuron object wrapping the neuron with the provided rank or coordinates.\n\n\nrank_from_coordinates\nReturns the rank of a neuron based on coordinates.\n\n\ncoordinates_from_rank\nReturns the coordinates of a neuron based on its rank.\n\n\nnormalized_coordinates_from_rank\nReturns normalized coordinates of a neuron based on its rank.\n\n\nsave\nSaves all information about the population (structure, current value of parameters and variables) into a file.\n\n\nload\nLoad the saved state of the population by Population.save().\n\n\n\n\n\nreset(attributes=None)\nResets all parameters and variables of the population to the value they had before the call to net.compile().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattributes\nlist\nlist of attributes (parameter or variable) which should be reinitialized. Default: all attributes.\nNone\n\n\n\n\n\n\n\nclear()\nClears all spiking events previously emitted (history of spikes, delayed spikes).\nCan be useful if you do not want to totally reset a population (i.e. all variables), only to clear the spiking history between two trials.\nNote: does nothing for rate-coded networks.\n\n\n\nenable()\n(Re)-enables computations in this population, after they were disabled by the disable() method.\nThe status of the population is accessible through the enabled flag.\n\n\n\ndisable()\nTemporarily disables computations in this population (including the projections leading to it).\nYou can re-enable it with the enable() method.\n\n\n\nset(values)\nSets the value of neural variables and parameters from a dictionary.\nExample:\npop.set({'tau': 20.0, 'r': np.random.rand((8,8)) } )\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\ndict\ndictionary of attributes to be updated.\nrequired\n\n\n\n\n\n\n\nget(name)\nReturns the value of a neural variable or parameter based on its name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nattribute name as a string.\nrequired\n\n\n\n\n\n\n\nsum(target)\nReturns the array of weighted sums corresponding to the target:\nexcitatory = pop.sum('exc')\nFor spiking networks, this is equivalent to accessing the conductances directly:\nexcitatory = pop.g_exc\nIf no incoming projection has the given target, the method returns zeros.\nNote: it is not possible to distinguish the original population when the same target is used.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntarget\nstr\nthe desired projection target.\nrequired\n\n\n\n\n\n\n\ncompute_firing_rate(window)\nTells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable r.\nThis method has an effect on spiking neurons only.\nIf this method is not called, r will always be 0.0. r can of course be accessed and recorded as any other variable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwindow\nfloat\nwindow in ms over which the spikes will be counted.\nrequired\n\n\n\n\n\n\n\nneuron(*coord)\nReturns an IndividualNeuron object wrapping the neuron with the provided rank or coordinates.\n\n\n\nrank_from_coordinates(coord)\nReturns the rank of a neuron based on coordinates.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncoord\ntuple\ncoordinate tuple, can be multidimensional.\nrequired\n\n\n\n\n\n\n\ncoordinates_from_rank(rank)\nReturns the coordinates of a neuron based on its rank.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the neuron.\nrequired\n\n\n\n\n\n\n\nnormalized_coordinates_from_rank(rank, norm=1.0)\nReturns normalized coordinates of a neuron based on its rank.\nThe geometry of the population is mapped to the hypercube [0, 1]^d\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the neuron.\nrequired\n\n\nnorm\nfloat\nnorm of the hypercube (default = 1.0).\n1.0\n\n\n\n\n\n\n\nsave(filename)\nSaves all information about the population (structure, current value of parameters and variables) into a file.\n\nIf the file name is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the file name ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nIf the file name is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nOtherwise, the data will be pickled into a simple binary text file using pickle.\n\nWarning: The ‘.mat’ data will not be loadable by ANNarchy, it is only for external analysis purpose.\nExample:\npop.save('pop1.npz')\npop.save('pop1.txt')\npop.save('pop1.txt.gz')\npop.save('pop1.mat')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nFilename, may contain relative or absolute path.\nrequired\n\n\n\n\n\n\n\nload(filename, pickle_encoding=None)\nLoad the saved state of the population by Population.save().\nWarning: Matlab data can not be loaded.\nExample:\npop.load('pop1.npz')\npop.load('pop1.txt')\npop.load('pop1.txt.gz')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\n\nthe filename with relative or absolute path.\nrequired",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Population"
    ]
  },
  {
    "objectID": "reference/set_current_step.html",
    "href": "reference/set_current_step.html",
    "title": "set_current_step",
    "section": "",
    "text": "set_current_step\nset_current_step(t, net_id=0)\nSets the current simulation step (integer).\nWarning: can be dangerous for some spiking models.",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "set_current_step"
    ]
  },
  {
    "objectID": "reference/dt.html",
    "href": "reference/dt.html",
    "title": "dt",
    "section": "",
    "text": "dt\ndt(net_id=0)\nReturns the simulation step size dt used in the simulation.",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "dt"
    ]
  },
  {
    "objectID": "reference/load.html",
    "href": "reference/load.html",
    "title": "load",
    "section": "",
    "text": "load(\n    filename,\n    populations=True,\n    projections=True,\n    pickle_encoding=None,\n    net_id=0,\n)\nLoads a saved state of the network.\nWarning: Matlab data can not be loaded.\nExample:\nann.load('results/network.npz')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nthe filename with relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be loaded (by default True)\nTrue\n\n\nprojections\nbool\nif True, projection data will be loaded (by default True)\nTrue\n\n\npickle_encoding\nstr\noptional parameter provided to the pickle.load() method. If set to None the default is used.\nNone",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "load"
    ]
  },
  {
    "objectID": "reference/load.html#parameters",
    "href": "reference/load.html#parameters",
    "title": "load",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nthe filename with relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be loaded (by default True)\nTrue\n\n\nprojections\nbool\nif True, projection data will be loaded (by default True)\nTrue\n\n\npickle_encoding\nstr\noptional parameter provided to the pickle.load() method. If set to None the default is used.\nNone",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "load"
    ]
  },
  {
    "objectID": "reference/Variable.html",
    "href": "reference/Variable.html",
    "title": "Variable",
    "section": "",
    "text": "Variable(\n    self,\n    equation,\n    init=None,\n    min=None,\n    max=None,\n    method=None,\n    locality='local',\n    type='float',\n)\nDataclass to represent a variable in a Neuron or Synapse definition.\nneuron = ann.Neuron(\n    equations = [\n        ann.Variable('C * dv/dt = - gL * (v - E_L) +  gL * delta_T * exp((v-v_T)/delta_T) + I - w', init=-70.0),\n\n        ann.Variable('tau_w * dw/dt = a * (v - E_L) - w', min=0.0),\n    ]\n)\nVariables are local by default. Set locality to global or semiglobal otherwise.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nequation\nstr\nstring representing the equation.\nrequired\n\n\ninit\nfloat | int | bool | RandomDistribution\ninitial value of the variable. It can be defined as a RandomDistribution, which will be sampled with the correct shape when the population/projection is created, or a float/int/bool, depending on type.\nNone\n\n\nmin\nfloat\nminimum value that the variable can take.\nNone\n\n\nmax\nfloat\nmaximum value that the variable can take.\nNone\n\n\nmethod\nstr\nnumerical method to be used when the equation is an ODE. Must be in [‘explicit’, ‘implicit’, ‘semiimplicit’, ‘exponential’,‘midpoint’, ‘rk4’, ‘event-driven’]\nNone\n\n\nlocality\nstr\nLocality of the parameter. Must be in [‘global’, ‘semiglobal’, ‘local’].\n'local'\n\n\ntype\nstr\nData type of the parameter. Must be in [float, int, bool] (or [‘float’, ‘int’, ‘bool’]).\n'float'",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Variable"
    ]
  },
  {
    "objectID": "reference/Variable.html#parameters",
    "href": "reference/Variable.html#parameters",
    "title": "Variable",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nequation\nstr\nstring representing the equation.\nrequired\n\n\ninit\nfloat | int | bool | RandomDistribution\ninitial value of the variable. It can be defined as a RandomDistribution, which will be sampled with the correct shape when the population/projection is created, or a float/int/bool, depending on type.\nNone\n\n\nmin\nfloat\nminimum value that the variable can take.\nNone\n\n\nmax\nfloat\nmaximum value that the variable can take.\nNone\n\n\nmethod\nstr\nnumerical method to be used when the equation is an ODE. Must be in [‘explicit’, ‘implicit’, ‘semiimplicit’, ‘exponential’,‘midpoint’, ‘rk4’, ‘event-driven’]\nNone\n\n\nlocality\nstr\nLocality of the parameter. Must be in [‘global’, ‘semiglobal’, ‘local’].\n'local'\n\n\ntype\nstr\nData type of the parameter. Must be in [float, int, bool] (or [‘float’, ‘int’, ‘bool’]).\n'float'",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Variable"
    ]
  },
  {
    "objectID": "reference/reset.html",
    "href": "reference/reset.html",
    "title": "reset",
    "section": "",
    "text": "reset(\n    populations=True,\n    projections=False,\n    synapses=False,\n    monitors=True,\n    net_id=0,\n)\nReinitialises the network to its state before the call to compile. The network time will be set to 0ms.\nAll monitors are emptied.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npopulations\nbool\nif True (default), the neural parameters and variables will be reset to their initial value.\nTrue\n\n\nprojections\nbool\nif True, the synaptic parameters and variables (except the connections) will be reset (default=False).\nFalse\n\n\nsynapses\nbool\nif True, the synaptic weights will be erased and recreated (default=False).\nFalse\n\n\nmonitors\nbool\nif True, the monitors will be emptied and reset (default=True).\nTrue",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "reset"
    ]
  },
  {
    "objectID": "reference/reset.html#parameters",
    "href": "reference/reset.html#parameters",
    "title": "reset",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npopulations\nbool\nif True (default), the neural parameters and variables will be reset to their initial value.\nTrue\n\n\nprojections\nbool\nif True, the synaptic parameters and variables (except the connections) will be reset (default=False).\nFalse\n\n\nsynapses\nbool\nif True, the synaptic weights will be erased and recreated (default=False).\nFalse\n\n\nmonitors\nbool\nif True, the monitors will be emptied and reset (default=True).\nTrue",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "reset"
    ]
  },
  {
    "objectID": "reference/set_seed.html",
    "href": "reference/set_seed.html",
    "title": "set_seed",
    "section": "",
    "text": "set_seed(seed, use_seed_seq=True, net_id=0)\nSets the seed of the random number generators, both in ANNarchy.RandomDistributions and in the C++ library when it is created.\nNumpy still has to be seeded explicitly when using the default RNG, for example:\nann.set_seed(seed=42)\nrng = np.random.default_rng(seed=42)\nA = rng.uniform(0.0, 1.0, (10, 10))\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nseed\nint\ninteger value used to seed the C++ and Numpy RNG\nrequired\n\n\nuse_seed_seq\nbool\nfor openMP and parallel RNGs, we use either the STL SeedSequence (True, default) or a specialized implementation proposed by Melissa O’Neil (False, see _optimization_flags for more details).\nTrue",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "set_seed"
    ]
  },
  {
    "objectID": "reference/set_seed.html#parameters",
    "href": "reference/set_seed.html#parameters",
    "title": "set_seed",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nseed\nint\ninteger value used to seed the C++ and Numpy RNG\nrequired\n\n\nuse_seed_seq\nbool\nfor openMP and parallel RNGs, we use either the STL SeedSequence (True, default) or a specialized implementation proposed by Melissa O’Neil (False, see _optimization_flags for more details).\nTrue",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "set_seed"
    ]
  },
  {
    "objectID": "reference/save.html",
    "href": "reference/save.html",
    "title": "save",
    "section": "",
    "text": "save(filename, populations=True, projections=True, net_id=0)\nSave the current network state (parameters and variables) to a file.\n\nIf the extension is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the extension is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nIf the extension ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nOtherwise, the data will be pickled into a simple binary text file using cPickle.\n\nWarning: The ‘.mat’ data will not be loadable by ANNarchy, it is only for external analysis purpose.\nExample:\nann.save('results/init.npz')\n\nann.save('results/init.data')\n\nann.save('results/init.txt.gz')\n\nann.save('1000_trials.mat')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfilename, may contain relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be saved (by default True)\nTrue\n\n\nprojections\nbool\nif True, projection data will be saved (by default True)\nTrue",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "save"
    ]
  },
  {
    "objectID": "reference/save.html#parameters",
    "href": "reference/save.html#parameters",
    "title": "save",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfilename, may contain relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be saved (by default True)\nTrue\n\n\nprojections\nbool\nif True, projection data will be saved (by default True)\nTrue",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "save"
    ]
  },
  {
    "objectID": "reference/every.html",
    "href": "reference/every.html",
    "title": "every",
    "section": "",
    "text": "every(self, network=None, period=1.0, offset=0.0, wait=0.0)\nDecorator to declare a callback method that will be called periodically during the simulation.\nExample of setting increasing inputs to a population every 100 ms, with an offset of 90 ms (or -10 ms relative to the period):\nnet = ann.Network()\n\n@ann.every(network=net, period=100., offset=-10.)\ndef step_input(n):\n    pop.I = float(n) / 100.\n\nnet.simulate(10000.)\nstep_input() will be called at times 90, 190, …, 9990 ms during the call to simulate().\nThe method must accept only n as parameter (an integer being 0 the first time the method is called, and incremented afterwards) and can not return anything.\nThe times at which the method is called are relative to the time when simulate() is called (if t is already 150 before calling simulate(), the first call will then be made at t=240 with the previous example).\nIf multiple callbacks are defined, they will be called in the order of their declaration if they occur at the same time.\nwait can be combined with offset, so if period=100., offset=50. and wait=500., the first call will be made 550 ms after the call to `simulate()\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnetwork\nNetwork\nthe network instance that will catch the callbacks. By default it is the top-level network of id 0.\nNone\n\n\nperiod\nfloat\ninterval in ms between two calls to the function. If less than dt, will be called every step.\n1.0\n\n\noffset\nfloat\nby default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period.\n0.0\n\n\nwait\nfloat\nallows to wait for a certain amount of time (in ms) before starting to call the method.\n0.0",
    "crumbs": [
      "Reference",
      "**Callbacks**",
      "every"
    ]
  },
  {
    "objectID": "reference/every.html#parameters",
    "href": "reference/every.html#parameters",
    "title": "every",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nnetwork\nNetwork\nthe network instance that will catch the callbacks. By default it is the top-level network of id 0.\nNone\n\n\nperiod\nfloat\ninterval in ms between two calls to the function. If less than dt, will be called every step.\n1.0\n\n\noffset\nfloat\nby default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period.\n0.0\n\n\nwait\nfloat\nallows to wait for a certain amount of time (in ms) before starting to call the method.\n0.0",
    "crumbs": [
      "Reference",
      "**Callbacks**",
      "every"
    ]
  },
  {
    "objectID": "reference/save_parameters.html",
    "href": "reference/save_parameters.html",
    "title": "save_parameters",
    "section": "",
    "text": "save_parameters(filename, net_id=0)\nSaves the global parameters of a network (flag population for neurons, projection for synapses) to a JSON file.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\npath to the JSON file.\nrequired",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "save_parameters"
    ]
  },
  {
    "objectID": "reference/save_parameters.html#parameters",
    "href": "reference/save_parameters.html#parameters",
    "title": "save_parameters",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\npath to the JSON file.\nrequired",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "save_parameters"
    ]
  },
  {
    "objectID": "reference/populations.html",
    "href": "reference/populations.html",
    "title": "populations",
    "section": "",
    "text": "populations\npopulations(net_id=0)\nReturns a list of all declared populations.\n:retruns: a list of all populations.",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "populations"
    ]
  },
  {
    "objectID": "reference/Creating.html",
    "href": "reference/Creating.html",
    "title": "Creating",
    "section": "",
    "text": "Creating(self, equation, proba=1.0, w=0.0, d=None)\nDataclass to represent a creation condition for structural plasticity.\nWhen the condition is true, a synapse is created with the specified probability, using the weight w and delay d.\nCreatingSynapse = ann.Synapse(\n\n    parameters = dict(eta = 0.1, T = 1.0),\n\n    equations = ann.Variable(\"dw/dt = eta * pre.r * post.r\"),\n    \n    creating = ann.Creating(\"pre.r * post.r &gt; T\", proba = 0.1, w = 0.01),\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nequation\nstr\nstring representing the equation.\nrequired\n\n\nproba\nfloat\nprobability of creation of the synapse.\n1.0\n\n\nw\nfloat\nweight when the synapse is created.\n0.0\n\n\nd\nfloat\ndelay when the synapse is created.\nNone",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Creating"
    ]
  },
  {
    "objectID": "reference/Creating.html#parameters",
    "href": "reference/Creating.html#parameters",
    "title": "Creating",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nequation\nstr\nstring representing the equation.\nrequired\n\n\nproba\nfloat\nprobability of creation of the synapse.\n1.0\n\n\nw\nfloat\nweight when the synapse is created.\n0.0\n\n\nd\nfloat\ndelay when the synapse is created.\nNone",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Creating"
    ]
  },
  {
    "objectID": "reference/Izhikevich.html",
    "href": "reference/Izhikevich.html",
    "title": "Izhikevich",
    "section": "",
    "text": "Izhikevich(\n    self,\n    a=0.02,\n    b=0.2,\n    c=-65.0,\n    d=8.0,\n    v_thresh=30.0,\n    i_offset=0.0,\n    noise=0.0,\n    tau_refrac=0.0,\n    conductance='g_exc - g_inh',\n)\nIzhikevich quadratic spiking neuron.\n\nIzhikevich, E.M. (2003). Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks, 14:6. http://dx.doi.org/10.1109/TNN.2003.820440\n\nThe neural equations are:\n\\frac{dv}{dt} = 0.04 * v^2 + 5.0 * v + 140.0 - u + I\n\\frac{du}{dt} = a * (b * v - u)\nBy default, the conductance is “g_exc - g_inh”, but this can be changed by setting the conductance argument:\nneuron = ann.Izhikevich(conductance='g_ampa * (1 + g_nmda) - g_gaba')\nThe synapses are instantaneous, i.e the corresponding conductance is increased from the synaptic efficiency w at the time step when a spike is received.\nThe ODEs are solved using the explicit Euler method.\nEquivalent code:\nIzhikevich = ann.Neuron(\n    parameters = dict(\n        noise = ann.Parameter(0.0),\n        a = ann.Parameter(0.02),\n        b = ann.Parameter(0.2),\n        c = ann.Parameter(-65.0),\n        d = ann.Parameter(8.0),\n        v_thresh = ann.Parameter(30.0),\n        i_offset = ann.Parameter(0.0),\n    ), \n    equations = [\n        'I = g_exc - g_inh + noise * Normal(0.0, 1.0) + i_offset',\n        ann.Variable('dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I', init = -65.0),\n        ann.Variable('du/dt = a * (b*v - u)', init= -13.0),\n    ],\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = c; u += d\",\n    refractory = 0.0\n)\nThe default parameters are for a regular spiking (RS) neuron. They are defined as local parameters, so their value can be set at the neuron-level. Here is the neuron definition using global parameters:\nIzhikevich = ann.Neuron(\n    parameters = dict(\n        noise = 0.0,\n        a = 0.02,\n        b = 0.2,\n        c = -65.0,\n        d = 8.0,\n        v_thresh = 30.0,\n        i_offset = 0.0,\n    ), \n    equations = [\n        'I = g_exc - g_inh + noise * Normal(0.0, 1.0) + i_offset',\n        ann.Variable('dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I', init = -65.0),\n        ann.Variable('du/dt = a * (b*v - u)', init= -13.0),\n    ],\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = c; u += d\",\n    refractory = 0.0\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nfloat\nSpeed of the recovery variable\n0.02\n\n\nb\nfloat\nScaling of the recovery variable\n0.2\n\n\nc\nfloat\nReset potential.\n-65.0\n\n\nd\nfloat\nIncrement of the recovery variable after a spike.\n8.0\n\n\nv_thresh\nfloat\nSpike threshold (mV).\n30.0\n\n\ni_offset\nfloat\nexternal current (nA).\n0.0\n\n\nnoise\nfloat\nAmplitude of the normal additive noise.\n0.0\n\n\ntau_refrac\nfloat\nDuration of refractory period (ms).\n0.0\n\n\nconductance\nstr\nConductances used as inputs.\n'g_exc - g_inh'",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "reference/Izhikevich.html#parameters",
    "href": "reference/Izhikevich.html#parameters",
    "title": "Izhikevich",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\na\nfloat\nSpeed of the recovery variable\n0.02\n\n\nb\nfloat\nScaling of the recovery variable\n0.2\n\n\nc\nfloat\nReset potential.\n-65.0\n\n\nd\nfloat\nIncrement of the recovery variable after a spike.\n8.0\n\n\nv_thresh\nfloat\nSpike threshold (mV).\n30.0\n\n\ni_offset\nfloat\nexternal current (nA).\n0.0\n\n\nnoise\nfloat\nAmplitude of the normal additive noise.\n0.0\n\n\ntau_refrac\nfloat\nDuration of refractory period (ms).\n0.0\n\n\nconductance\nstr\nConductances used as inputs.\n'g_exc - g_inh'",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "reference/projections.html",
    "href": "reference/projections.html",
    "title": "projections",
    "section": "",
    "text": "projections\nprojections(net_id=0, post=None, pre=None, target=None, suppress_error=False)\nReturns a list of all declared populations.",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "projections"
    ]
  },
  {
    "objectID": "reference/InputArray.html",
    "href": "reference/InputArray.html",
    "title": "InputArray",
    "section": "",
    "text": "InputArray(self, geometry=None, name=None, copied=False, net_id=0)\nPopulation holding static inputs for a rate-coded network.\nThe input values are stored in the recordable attribute r, without any further processing.\ninp = net.create(ann.InputArray(geometry=10))\ninp.r = np.linspace(1, 10, 10)\n\npop = net.create(100, ann.LeakyIntegrator)\n\nproj = net.connect(inp, pop, 'exc')\nproj.all_to_all(1.0)\nNote that this population is functionally equivalent to:\ninp = net.create(geometry, ann.Neuron(parameters=\"r=0.0\"))\nbut r is recordable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\nint | tuple\nshape of the population, either an integer or a tuple.\nNone\n\n\nname\nstr\noptional name of the population.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "InputArray"
    ]
  },
  {
    "objectID": "reference/InputArray.html#parameters",
    "href": "reference/InputArray.html#parameters",
    "title": "InputArray",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngeometry\nint | tuple\nshape of the population, either an integer or a tuple.\nNone\n\n\nname\nstr\noptional name of the population.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "InputArray"
    ]
  },
  {
    "objectID": "reference/IBCM.html",
    "href": "reference/IBCM.html",
    "title": "IBCM",
    "section": "",
    "text": "IBCM(self, eta=0.01, tau=2000.0)\nRate-coded synapse with Intrator & Cooper (1992) plasticity.\nEquivalent code:\nIBCM = ann.Synapse(\n    parameters = dict(\n        eta = 0.01,\n        tau = 2000.0,\n    ),\n    equations = [\n        ann.Variable('tau * dtheta/dt + theta = post.r^2', locality='semiglobal', method='exponential'),\n        ann.Variable('dw/dt = eta * post.r * (post.r - theta) * pre.r',' min=0.0),\n    ]\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\neta\n\nlearning rate.\n0.01\n\n\ntau\n\ntime constant of the sliding threshold.\n2000.0",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "IBCM"
    ]
  },
  {
    "objectID": "reference/IBCM.html#parameters",
    "href": "reference/IBCM.html#parameters",
    "title": "IBCM",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\neta\n\nlearning rate.\n0.01\n\n\ntau\n\ntime constant of the sliding threshold.\n2000.0",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "IBCM"
    ]
  },
  {
    "objectID": "reference/Convolution.html",
    "href": "reference/Convolution.html",
    "title": "Convolution",
    "section": "",
    "text": "Convolution(\n    self,\n    pre,\n    post,\n    target,\n    psp='pre.r * w',\n    operation='sum',\n    name=None,\n    copied=False,\n    net_id=0,\n)\nPerforms a convolution of a weight kernel on the pre-synaptic population.\nDespite its name, the operation performed is actually a cross-correlation, as is usual in computer vision and convolutional neural networks:\ng(x) = \\sum_{k=-n}^n h(k) \\, f(x + k)\nThe convolution operation benefits from giving a multi-dimensional geometry to the populations and filters, for example in 2D:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.convolution import Convolution\n\nnet = ann.Network()\n\ninp = net.create(geometry=(100, 100), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npop = net.create(geometry=(100, 100), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = net.connect(Convolution(inp, pop, 'exc'))\nproj.connect_filter(\n    [\n        [-1., 0., 1.],\n        [-1., 0., 1.],\n        [-1., 0., 1.]\n    ])\n\nnet.compile()\nThe maximum number of dimensions for populations and filters is 4, an error is thrown otherwise.\nDepending on the number of dimensions of the pre- and post-synaptic populations, as well as of the kernel, the convolution is implemented differentely.\nMethod connect_filter()\n\nIf the pre- and post-populations have the same dimension as the kernel, the convolution is regular. Example:\n(100, 100) * (3, 3) -&gt; (100, 100)\nIf the post-population has one dimension less than the pre-synaptic one, the last dimension of the kernel must match the last one of the pre-synaptic population. Example:\n(100, 100, 3) * (3, 3, 3) -&gt; (100, 100)\nIf the kernel has less dimensions than the two populations, the number of neurons in the last dimension of the populations must be the same. The convolution will be calculated for each feature map in the last dimension. In this case, you must set keep_last_dimension to True. Example:\n(100, 100, 16) * (3, 3) -&gt; (100, 100, 16)\n\nMethod connect_filters()\n\nIf the kernel has more dimensions than the pre-synaptic population, this means a bank of different filters will be applied on the pre-synaptic population (like a convolutional layer in a CNN). Attention: the first index of weights corresponds to the different filters, while the result will be accessible in the last dimension of the post-synaptic population. You must set the multiple argument to True. Example:\n(100, 100) * (16, 3, 3) -&gt; (100, 100, 16)\n\nThe convolution always uses padding for elements that would be outside the array (no equivalent of valid in tensorflow). It is 0.0 by default, but can be changed using the padding argument. Setting padding to the string border will repeat the value of the border elements.\nSub-sampling will be automatically performed according to the populations’ geometry. If these geometries do not match, an error will be thrown. Example:\n(100, 100) * (3, 3) -&gt; (50, 50)\nYou can redefine the sub-sampling by providing a list subsampling as argument, defining for each post-synaptic neuron the coordinates of the pre-synaptic neuron which will be the center of the filter/kernel.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\n\npre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\n\npost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\n\ntype of the connection\nrequired\n\n\npsp\n\ncontinuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r).\n'pre.r * w'\n\n\noperation\n\noperation (sum, max, min, mean) performed by the kernel (default: sum).\n'sum'\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nconnect_filter\nApplies a single filter on the pre-synaptic population.\n\n\nconnect_filters\nApplies a set of different filters on the pre-synaptic population.\n\n\nsave_connectivity\nNot available.\n\n\nsave\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nconnectivity_matrix\nNot available.\n\n\n\n\n\nconnect_filter(\n    weights,\n    delays=0.0,\n    keep_last_dimension=False,\n    padding=0.0,\n    subsampling=None,\n)\nApplies a single filter on the pre-synaptic population.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\n\nnumpy array or list of lists representing the matrix of weights for the filter.\nrequired\n\n\ndelays\n\ndelay in synaptic transmission (default: dt). Can only be the same value for all neurons.\n0.0\n\n\nkeep_last_dimension\n\ndefines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.\nFalse\n\n\npadding\n\nvalue to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to ‘border’, the values on the boundaries are repeated. Default: 0.0.\n0.0\n\n\nsubsampling\n\nlist for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.\nNone\n\n\n\n\n\n\n\nconnect_filters(\n    weights,\n    delays=0.0,\n    keep_last_dimension=False,\n    padding=0.0,\n    subsampling=None,\n)\nApplies a set of different filters on the pre-synaptic population.\nThe weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\n\nnumpy array or list of lists representing the matrix of weights for the filter.\nrequired\n\n\ndelays\n\ndelay in synaptic transmission (default: dt). Can only be the same value for all neurons.\n0.0\n\n\nkeep_last_dimension\n\ndefines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.\nFalse\n\n\npadding\n\nvalue to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to ‘border’, the values on the boundaries are repeated. Default: 0.0.\n0.0\n\n\nsubsampling\n\nlist for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.\nNone\n\n\n\n\n\n\n\nsave_connectivity(filename)\nNot available.\n\n\n\nsave(filename)\nNot available.\n\n\n\nload(filename)\nNot available.\n\n\n\nreceptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nconnectivity_matrix(fill=0.0)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Convolution"
    ]
  },
  {
    "objectID": "reference/Convolution.html#parameters",
    "href": "reference/Convolution.html#parameters",
    "title": "Convolution",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\n\npre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\n\npost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\n\ntype of the connection\nrequired\n\n\npsp\n\ncontinuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r).\n'pre.r * w'\n\n\noperation\n\noperation (sum, max, min, mean) performed by the kernel (default: sum).\n'sum'",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Convolution"
    ]
  },
  {
    "objectID": "reference/Convolution.html#methods",
    "href": "reference/Convolution.html#methods",
    "title": "Convolution",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nconnect_filter\nApplies a single filter on the pre-synaptic population.\n\n\nconnect_filters\nApplies a set of different filters on the pre-synaptic population.\n\n\nsave_connectivity\nNot available.\n\n\nsave\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nconnectivity_matrix\nNot available.\n\n\n\n\n\nconnect_filter(\n    weights,\n    delays=0.0,\n    keep_last_dimension=False,\n    padding=0.0,\n    subsampling=None,\n)\nApplies a single filter on the pre-synaptic population.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\n\nnumpy array or list of lists representing the matrix of weights for the filter.\nrequired\n\n\ndelays\n\ndelay in synaptic transmission (default: dt). Can only be the same value for all neurons.\n0.0\n\n\nkeep_last_dimension\n\ndefines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.\nFalse\n\n\npadding\n\nvalue to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to ‘border’, the values on the boundaries are repeated. Default: 0.0.\n0.0\n\n\nsubsampling\n\nlist for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.\nNone\n\n\n\n\n\n\n\nconnect_filters(\n    weights,\n    delays=0.0,\n    keep_last_dimension=False,\n    padding=0.0,\n    subsampling=None,\n)\nApplies a set of different filters on the pre-synaptic population.\nThe weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\n\nnumpy array or list of lists representing the matrix of weights for the filter.\nrequired\n\n\ndelays\n\ndelay in synaptic transmission (default: dt). Can only be the same value for all neurons.\n0.0\n\n\nkeep_last_dimension\n\ndefines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.\nFalse\n\n\npadding\n\nvalue to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to ‘border’, the values on the boundaries are repeated. Default: 0.0.\n0.0\n\n\nsubsampling\n\nlist for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.\nNone\n\n\n\n\n\n\n\nsave_connectivity(filename)\nNot available.\n\n\n\nsave(filename)\nNot available.\n\n\n\nload(filename)\nNot available.\n\n\n\nreceptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nconnectivity_matrix(fill=0.0)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Convolution"
    ]
  },
  {
    "objectID": "reference/HomogeneousCorrelatedSpikeTrains.html",
    "href": "reference/HomogeneousCorrelatedSpikeTrains.html",
    "title": "HomogeneousCorrelatedSpikeTrains",
    "section": "",
    "text": "HomogeneousCorrelatedSpikeTrains(\n    self,\n    geometry,\n    rates,\n    corr,\n    tau,\n    schedule=None,\n    period=-1.0,\n    name=None,\n    refractory=None,\n    copied=False,\n    net_id=0,\n)\nPopulation of spiking neurons following a homogeneous distribution with correlated spike trains.\nThe method describing the generation of homogeneous correlated spike trains is described in:\n\nBrette, R. (2009). Generation of correlated spike trains. Neural Computation 21(1). http://romainbrette.fr/WordPress3/wp-content/uploads/2014/06/Brette2008NC.pdf\n\nThe implementation is based on the one provided by Brian http://briansimulator.org.\nTo generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation:\n\\frac{dx}{dt} = \\frac{\\mu - x}{\\tau} + \\sigma \\, \\frac{\\xi}{\\sqrt{\\tau}}\nwhere \\xi is a random sample from the standard normal distribution. In short, x will randomly vary around \\mu over time, with an amplitude determined by \\sigma and a speed determined by \\tau.\nThis doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process.\nTo avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates, the desired correlation strength corr and the time constant tau. See Brette’s paper for details.\nIn short, you should only define the parameters rates, corr and tau, and let the class compute mu and sigma for you. Changing rates, corr or tau after initialization automatically recomputes mu and sigma.\nExample:\nimport ANNarchy as ann\n\nnet = ann.Network(dt=0.1)\n\npop_corr = net.create(\n    ann.HomogeneousCorrelatedSpikeTrains(\n        geometry=200, \n        rates=10., \n        corr=0.3, \n        tau=10.)\n)\n\nnet.compile()\n\nnet.simulate(1000.)\n\npop_corr.rates=30.\n\nnet.simulate(1000.)\nAlternatively, a schedule can be provided to change automatically the value of rates and corr (but not tau) at the required times (as in TimedArray or TimedPoissonPopulation):\npop_corr = net.create(\n    ann.HomogeneousCorrelatedSpikeTrains(\n        geometry = 200, \n        rates = [10., 30.], \n        corr = [0.3, 0.5], \n        tau = 10.,\n        schedule = [0., 1000.]\n    )\n)\nEven when using a schedule, corr accepts a single constant value. The first value of schedule must be 0. period specifies when the schedule “loops” back to its initial value.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\nint | tuple[int]\npopulation geometry as tuple.\nrequired\n\n\nrates\nfloat | list[float]\nrate in Hz of the population (must be a float or a list of float)\nrequired\n\n\ncorr\nfloat | list[float]\ntotal correlation strength (float in [0, 1], or a list)\nrequired\n\n\ntau\nfloat\ncorrelation time constant in ms.\nrequired\n\n\nschedule\nlist[float]\nlist of times where new values of ratesand corrwill be used to computre mu and sigma.\nNone\n\n\nperiod\nfloat\ntime when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.)\n-1.0\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\nrefractory\nfloat\nrefractory period in ms (careful: may break the correlation)\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "HomogeneousCorrelatedSpikeTrains"
    ]
  },
  {
    "objectID": "reference/HomogeneousCorrelatedSpikeTrains.html#parameters",
    "href": "reference/HomogeneousCorrelatedSpikeTrains.html#parameters",
    "title": "HomogeneousCorrelatedSpikeTrains",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngeometry\nint | tuple[int]\npopulation geometry as tuple.\nrequired\n\n\nrates\nfloat | list[float]\nrate in Hz of the population (must be a float or a list of float)\nrequired\n\n\ncorr\nfloat | list[float]\ntotal correlation strength (float in [0, 1], or a list)\nrequired\n\n\ntau\nfloat\ncorrelation time constant in ms.\nrequired\n\n\nschedule\nlist[float]\nlist of times where new values of ratesand corrwill be used to computre mu and sigma.\nNone\n\n\nperiod\nfloat\ntime when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.)\n-1.0\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\nrefractory\nfloat\nrefractory period in ms (careful: may break the correlation)\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "HomogeneousCorrelatedSpikeTrains"
    ]
  },
  {
    "objectID": "reference/step.html",
    "href": "reference/step.html",
    "title": "step",
    "section": "",
    "text": "step\nstep(net_id=0)\nPerforms a single simulation step (duration = dt).",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "step"
    ]
  },
  {
    "objectID": "reference/ImagePopulation.html",
    "href": "reference/ImagePopulation.html",
    "title": "ImagePopulation",
    "section": "",
    "text": "ImagePopulation(self, geometry, name=None, copied=False, net_id=0)\nRate-coded Population allowing to represent images (png, jpg…) as the firing rate of a population (each neuron represents one pixel).\nThis extension requires the Python Image Library (pip install Pillow).\nThe extensions has to be explicitly imported:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.image import ImagePopulation\n\nnet = ann.Network()\npop = net.create(ImagePopulation(geometry=(480, 640)))\npop.set_image('image.jpg')\nAbout the geometry:\n\nIf the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image).\nIf the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color).\n\nIf the third dimension is 3, each will correspond to the RGB values of the pixels.\nWarning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple\npopulation geometry as tuple. It must correspond to the image size and be fixed through the whole simulation.\nrequired\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nset_image\nSets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population.\n\n\n\n\n\nset_image(filename)\nSets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population.\nIf the image has a different size from the population, it will be resized.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nname of the image file.\nrequired",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "ImagePopulation"
    ]
  },
  {
    "objectID": "reference/ImagePopulation.html#parameters",
    "href": "reference/ImagePopulation.html#parameters",
    "title": "ImagePopulation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple\npopulation geometry as tuple. It must correspond to the image size and be fixed through the whole simulation.\nrequired\n\n\nname\nstr\nunique name of the population (optional).\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "ImagePopulation"
    ]
  },
  {
    "objectID": "reference/ImagePopulation.html#methods",
    "href": "reference/ImagePopulation.html#methods",
    "title": "ImagePopulation",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nset_image\nSets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population.\n\n\n\n\n\nset_image(filename)\nSets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population.\nIf the image has a different size from the population, it will be resized.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nname of the image file.\nrequired",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "ImagePopulation"
    ]
  },
  {
    "objectID": "reference/PoissonPopulation.html",
    "href": "reference/PoissonPopulation.html",
    "title": "PoissonPopulation",
    "section": "",
    "text": "PoissonPopulation(\n    self,\n    geometry,\n    name=None,\n    rates=None,\n    target=None,\n    parameters={},\n    refractory=None,\n    copied=False,\n    net_id=0,\n)\nPopulation of spiking neurons following a Poisson distribution.\nEach neuron of the population will randomly emit spikes, with a mean firing rate defined by the rates argument.\nThe mean firing rate in Hz can be a fixed value for all neurons:\npop = net.create(ann.PoissonPopulation(geometry=100, rates=100.0))\nbut it can be modified later as a normal parameter:\npop.rates = np.linspace(10, 150, 100)\nIt is also possible to define a temporal equation for the rates, by passing a string to the argument:\npop = net.create(\n    ann.PoissonPopulation(\n        geometry=100, \n        rates=\"100.0 * (1.0 + sin(2*pi*t/1000.0) )/2.0\"\n    )\n)\nThe syntax of this equation follows the same structure as neural variables.\nIt is also possible to add parameters to the population which can be used in the equation of rates:\npop = net.create(\n    ann.PoissonPopulation(\n        geometry=100,\n        parameters = dict(\n            amp = 100.0,\n            frequency = 1.0,\n        ),\n        rates=\"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\"\n    )\n)\nNote: The preceding definition is fully equivalent to the definition of this neuron:\npoisson = ann.Neuron(\n    parameters = dict(\n        amp = 100.0,\n        frequency = 1.0,\n    ),\n    equations = [\n        'rates = amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0',\n        'p = Uniform(0.0, 1.0) * 1000.0 / dt',\n    ],\n    spike = \"p &lt; rates\"\n)\nThe refractory period can also be set, so that a neuron can not emit two spikes too close from each other.\nIf the rates argument is not set, the population can be used as an interface from a rate-coded population. The target argument specifies which incoming projections will be summed to determine the instantaneous firing rate of each neuron.\nnet = ann.Network()\n\nrates = 10.*np.ones((2, 100))\nrates[0, :50] = 100.\nrates[1, 50:] = 100.\ninp = net.create(ann.TimedArray(rates = rates, schedule=50.))\n\npop = net.create(ann.PoissonPopulation(100, target=\"exc\"))\n\nproj = net.connect(inp, pop, 'exc')\nproj.one_to_one(1.0)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\nint | tuple[int]\npopulation geometry as tuple.\nrequired\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\nrates\nfloat | str\nmean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string).\nNone\n\n\ntarget\nstr\nthe mean firing rate will be the weighted sum of inputs having this target name (e.g. “exc”).\nNone\n\n\nparameters\ndict\nadditional parameters which can be used in the rates equation.\n{}\n\n\nrefractory\nfloat\nrefractory period in ms.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "PoissonPopulation"
    ]
  },
  {
    "objectID": "reference/PoissonPopulation.html#parameters",
    "href": "reference/PoissonPopulation.html#parameters",
    "title": "PoissonPopulation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngeometry\nint | tuple[int]\npopulation geometry as tuple.\nrequired\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\nrates\nfloat | str\nmean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string).\nNone\n\n\ntarget\nstr\nthe mean firing rate will be the weighted sum of inputs having this target name (e.g. “exc”).\nNone\n\n\nparameters\ndict\nadditional parameters which can be used in the rates equation.\n{}\n\n\nrefractory\nfloat\nrefractory period in ms.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "PoissonPopulation"
    ]
  },
  {
    "objectID": "reference/sparse_random_matrix.html",
    "href": "reference/sparse_random_matrix.html",
    "title": "sparse_random_matrix",
    "section": "",
    "text": "sparse_random_matrix(pre, post, proba, weights)\nReturns a sparse lil-matrix for use in Projection.from_sparse().\nCreates a scipy.sparse.lil_matrix connectivity matrix that connects the pre and post populations with the probability p and the value weight, which can be either a constant or an ANNarchy random distribution object.\npop1 = net.create(100, neuron)\npop2 = net.create(100, neuron)\nproj = net.connect(pre=pop1, post=pop2, target='exc')\n\nmatrix = sparse_random_matrix(pre=pop1, post=pop2, p=0.1, w=ann.Uniform(0.0, 1.0))\nproj.from_sparse(matrix)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\nPopulation\npre-synaptic population.\nrequired\n\n\npost\nPopulation\npost-synaptic population.\nrequired\n\n\nproba\nfloat\nconnection probability.\nrequired\n\n\nweights\nfloat | RandomDistribution\nweight values (constant or random).\nrequired",
    "crumbs": [
      "Reference",
      "**Utilities**",
      "sparse_random_matrix"
    ]
  },
  {
    "objectID": "reference/sparse_random_matrix.html#parameters",
    "href": "reference/sparse_random_matrix.html#parameters",
    "title": "sparse_random_matrix",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\nPopulation\npre-synaptic population.\nrequired\n\n\npost\nPopulation\npost-synaptic population.\nrequired\n\n\nproba\nfloat\nconnection probability.\nrequired\n\n\nweights\nfloat | RandomDistribution\nweight values (constant or random).\nrequired",
    "crumbs": [
      "Reference",
      "**Utilities**",
      "sparse_random_matrix"
    ]
  },
  {
    "objectID": "reference/balloon_CN.html",
    "href": "reference/balloon_CN.html",
    "title": "balloon_CN",
    "section": "",
    "text": "balloon_CN(\n    self,\n    phi=1.0,\n    kappa=1 / 1.54,\n    gamma=1 / 2.46,\n    E_0=0.34,\n    tau=0.98,\n    alpha=0.33,\n    V_0=0.02,\n    v_0=40.3,\n    TE=40 / 1000.0,\n    epsilon=1.43,\n)\nA balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\nEquivalent code:\nballoon_CN = BoldModel(\n    parameters = dict(\n        second    = 1000.0,\n        phi       = 1.0,\n        kappa     = 1/1.54,\n        gamma     = 1/2.46,\n        E_0       = 0.34,\n        tau       = 0.98,\n        alpha     = 0.33,\n        V_0       = 0.02,\n        v_0       = 40.3,\n        TE        = 40/1000.,\n        epsilon   = 1.43,\n    ),\n    equations = [\n        # Single input\n        ann.Variable('I_CBF = sum(I_CBF)', init=0.0),\n        ann.Variable('ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second', init=0.0),\n        ann.Variable('df_in/dt = s / second', init=1.0, min=0.01),\n\n        ann.Variable('E = 1 - (1 - E_0)**(1 / f_in)', init=0.3424),\n        ann.Variable('dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)', init=1.0, min=0.01),\n        ann.Variable('dv/dt = (f_in - f_out)/(tau*second), init=1.0, min=0.01),\n        ann.Variable('f_out = v**(1 / alpha)', init=1, min=0.01),\n\n        # Classic coefficients\n        ann.Variable('k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE'),\n        ann.Variable('k_2 = 2 * E_0'),\n        ann.Variable('k_3 = 1 - epsilon'),\n\n        # Non-linear equation\n        ann.Variable('BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))'),\n    ],\n    inputs=\"I_CBF\",\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_CN"
    ]
  },
  {
    "objectID": "reference/balloon_CN.html#parameters",
    "href": "reference/balloon_CN.html#parameters",
    "title": "balloon_CN",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_CN"
    ]
  },
  {
    "objectID": "reference/IF_cond_exp.html",
    "href": "reference/IF_cond_exp.html",
    "title": "IF_cond_exp",
    "section": "",
    "text": "IF_cond_exp(\n    self,\n    v_rest=-65.0,\n    cm=1.0,\n    tau_m=20.0,\n    tau_refrac=0.0,\n    tau_syn_E=5.0,\n    tau_syn_I=5.0,\n    e_rev_E=0.0,\n    e_rev_I=-70.0,\n    v_thresh=-50.0,\n    v_reset=-65.0,\n    i_offset=0.0,\n)\nLeaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance.\nThe ODEs are solved using the exponential Euler method.\nEquivalent code:\nIF_cond_exp = Neuron(\n    parameters = dict(\n        v_rest = ann.Parameter(-65.0),\n        cm  = ann.Parameter(1.0),\n        tau_m  = ann.Parameter(20.0),\n        tau_syn_E = ann.Parameter(5.0),\n        tau_syn_I = ann.Parameter(5.0),\n        e_rev_E = ann.Parameter(0.0),\n        e_rev_I = ann.Parameter(-70.0),\n        v_thresh = ann.Parameter(-50.0),\n        v_reset = ann.Parameter(-65.0),\n        i_offset = ann.Parameter(0.0),\n    ), \n    equations = [\n        ann.Variable(\n            'cm * dv/dt = cm/tau_m * (v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset',\n            method='exponential', init=-65.0\n        ),\n        ann.Variable('tau_syn_E * dg_exc/dt = - g_exc', method='exponential'),\n        ann.Variable('tau_syn_I * dg_inh/dt = - g_inh', method='exponential'),\n    ],\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = v_reset\",\n    refractory = 0.0\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv_rest\n\nResting membrane potential (mV)\n-65.0\n\n\ncm\n\nCapacity of the membrane (nF)\n1.0\n\n\ntau_m\n\nMembrane time constant (ms)\n20.0\n\n\ntau_refrac\n\nDuration of refractory period (ms)\n0.0\n\n\ntau_syn_E\n\nDecay time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\n\nDecay time of inhibitory synaptic current (ms)\n5.0\n\n\ne_rev_E\n\nReversal potential of excitatory conductance (mV)\n0.0\n\n\ne_rev_I\n\nReversal potential of inhibitory conductance (mV)\n-70.0\n\n\nv_thresh\n\nSpike threshold (mV)\n-50.0\n\n\nv_reset\n\nReset potential after a spike (mV)\n-65.0\n\n\ni_offset\n\nOffset current (nA)\n0.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_cond_exp"
    ]
  },
  {
    "objectID": "reference/IF_cond_exp.html#parameters",
    "href": "reference/IF_cond_exp.html#parameters",
    "title": "IF_cond_exp",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nv_rest\n\nResting membrane potential (mV)\n-65.0\n\n\ncm\n\nCapacity of the membrane (nF)\n1.0\n\n\ntau_m\n\nMembrane time constant (ms)\n20.0\n\n\ntau_refrac\n\nDuration of refractory period (ms)\n0.0\n\n\ntau_syn_E\n\nDecay time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\n\nDecay time of inhibitory synaptic current (ms)\n5.0\n\n\ne_rev_E\n\nReversal potential of excitatory conductance (mV)\n0.0\n\n\ne_rev_I\n\nReversal potential of inhibitory conductance (mV)\n-70.0\n\n\nv_thresh\n\nSpike threshold (mV)\n-50.0\n\n\nv_reset\n\nReset potential after a spike (mV)\n-65.0\n\n\ni_offset\n\nOffset current (nA)\n0.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_cond_exp"
    ]
  },
  {
    "objectID": "reference/disable_callbacks.html",
    "href": "reference/disable_callbacks.html",
    "title": "disable_callbacks",
    "section": "",
    "text": "disable_callbacks\ndisable_callbacks(net_id=0)\nDisables all callbacks for the network.",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "disable_callbacks"
    ]
  },
  {
    "objectID": "Installation.html",
    "href": "Installation.html",
    "title": "Installation",
    "section": "",
    "text": "ANNarchy is designed to run on GNU/Linux and OSX. It relies mostly on a C++ compiler (g++ or clang++), Cython (C for Python extension) and Python (Numpy, Sympy) libraries.\nInstallation on Windows is only possible using the WSL (Windows Subsystem for Linux) or a VM.\n\n\nThe source code of ANNarchy can be downloaded on github:\ngit clone https://github.com/ANNarchy/ANNarchy.git\n\n\n\n\n\nANNarchy depends on a number of packages that should be easily accessible on recent GNU/Linux distributions. The classical way to install these dependencies is through your package manager, or using full Python distributions such as Anaconda. Older versions of these packages may work but have not been tested.\n\npython &gt;= 3.10 (with the development files, e.g. python-dev or python-devel)\ng++ &gt;= 7.4 or clang++ &gt;= 3.4\ncmake &gt;= 3.16\nsetuptools &gt;= 65.0\nnanobind &gt;= 2.4.0\ncython &gt;= 3.0\nnumpy &gt;= 1.21\nsympy &gt;= 1.11\nscipy &gt;= 1.9\nmatplotlib &gt;= 3.0\ntqdm &gt;= 4.60\n\nAdditionally, the following packages are optional but strongly recommended:\n\nlxml (to save the networks in .xml format).\nh5py (to export data in .h5 format).\npandoc (for report()).\ntensorflow (for the ann_to_snn_conversion extension)\ntensorboardX (for the logging extension).\n\nThe CUDA-SDK is available on the official website (we recommend to use at least a SDK version &gt; 6.x). For further details on installation etc., please consider the corresponding Quickstart guides (Quickstart_8.0 for the SDK 8.x).\nANNarchy works with full Python distributions such as Anaconda, as well as in virtual environments and Jupyter notebooks.\n\n\n\n\n\n\nNote\n\n\n\nOn a fresh install of Ubuntu 22.04, here are the minimal system packages to install before ANNarchy:\nsudo apt install build-essential git python3-dev python3-setuptools python3-pip \n\n\n\n\n\n\n\nStable releases of ANNarchy are available on PyPi:\npip install ANNarchy\nor:\npip install ANNarchy --user\nif you do not have administrator permissions. Omit --user in a virtual environment.\nYou may also install directly the latest commit in the master (stable) or develop branches with:\npip install git+https://github.com/ANNarchy/ANNarchy.git@master\n\n\n\nInstallation of ANNarchy from source is possible using pip in the top-level directory:\npip install .\nor in development mode:\npip install -e .\nUsing python setup.py install is deprecated, but still works.\n\n\n\nBy default, ANNarchy will use the GNU C++ compiler g++, which should be in your PATH. If you want to use another compiler (clang++, icc), you can edit the configuration file located at $HOME/.config/ANNarchy/annarchy.json (created during installation) accordingly. By default, it is:\n{\n    \"openmp\": {\n        \"compiler\": \"g++\",\n        \"flags\": \"-march=native -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\nThe (path to the) compiler can be changed in the openmp section (ignore the cuda section if you do not have a GPU).\nYou can also change the compiler flags if you know what you are doing. -O3 does not always lead to faster simulation times, nor to exact simulations.\n\n\n\nIf ANNarchy detects the CUDA SDK during installation, it will prepare the required modules. You need to make sure that the CUDA compiler nvcc is accessible in your path.\nThe main problem with CUDA is that the binaries, headers and libraries are installed at different locations depending on the version: /usr/local/cuda, /usr/local/cuda-7.0 or /usr/local/cuda-8.0. There is unfortunately no way for ANNarchy to guess the installation path.\nA first thing to help ANNarchy find the CUDA libraries is to define the LD_LIBRARY_PATH environment variable and have point at the lib64/ subfolder:\nexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64/:$LD_LIBRARY_PATH\nThis should in most cases work if you have only one CUDA installation. Otherwise, it is needed that you indicate where the CUDA libraries are, by modifying the ANNarchy configuration file located at $HOME/.config/ANNarchy/annarchy.json:\n{\n    \"openmp\": {\n        \"compiler\": \"g++\",\n        \"flags\": \"-march=native -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\nSimply point the ['cuda']['path'] field to the right location (without lib64/). If the nvcc compiler binary is at a different location, the absolute path to the nvcc can be provided by ['cuda']['compiler'] field.\nIt can happen that the detection of CUDA fails during installation, as some environment variables are not set. In this case try:\nenv \"PATH=$PATH\" \"LIBRARY_PATH=$LIBRARY_PATH\" pip install .\n\n\n\n\n\nInstallation on MacOS X is in principle similar to GNU/Linux:\npip install ANNarchy\nWe strongly advise using a full Python distribution like Miniforge, or a package manager like Homebrew rather than the default python provided by Apple.\nThe main issue is the choice of the C++ compiler:\n\n\nIf not done already, you should first install the Xcode Command Line Tools and Homebrew (see https://mac.install.guide/commandlinetools/ for a guide) to get the LLVM clang++ compiler.\nThe major drawback of Apple’s clang++ is that it still does not support OpenMP for parallel computing. Any attempt to use OpenMP with ANNarchy using this compiler will crash.\nIt is strongly advised to use a more recent version of clang++ such as the one offered by homebrew:\nbrew install llvm\nand update your shell config to use this version of clang++ instead of Apple’s (adapt the paths accordingly):\nexport PATH=\"/opt/homebrew/opt/llvm/bin:$PATH\"\nexport LDFLAGS=\"-L/opt/homebrew/opt/llvm/lib:$LDFLAGS\"\nexport CPPFLAGS=\"-I/opt/homebrew/opt/llvm/include:$CPPFLAGS\"\nIf you have a MX arm64 processor, it might be beneficial to tell clang++ to use optimizations for that hardware. Open the configuration file at $HOME/.config/ANNarchy/annarchy.json and add the following compiler flag (adapt it to your processor):\n{\n    \"openmp\": {\n        \"compiler\": \"clang++\",\n        \"flags\": \"-mcpu=apple-m1 -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\n\n\n\nIn order to benefit from OpenMP parallelization, you can also install gcc, the GNU C compiler, using Homebrew:\nbrew install gcc\nYou will get the command-line C++ compiler with a version number, e.g.:\ng++-11\nThe g++ executable is a symlink to Apple’s clang++, do not use it…\nYou now have to tell ANNarchy which compiler to use, even if it is in your PATH. After installing ANNarchy, a config file is created in $HOME/.config/ANNarchy/annarchy.json. Open it and change the openmp entry to:\n{\n    \"openmp\": {\n        \"compiler\": \"g++-11\",\n        \"flags\": \"-march=native -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\n\n\n\n\n\n\nWarning\n\n\n\nThe CUDA backend is not available on OS X.",
    "crumbs": [
      "ANNarchy",
      "Installation"
    ]
  },
  {
    "objectID": "Installation.html#download",
    "href": "Installation.html#download",
    "title": "Installation",
    "section": "",
    "text": "The source code of ANNarchy can be downloaded on github:\ngit clone https://github.com/ANNarchy/ANNarchy.git",
    "crumbs": [
      "ANNarchy",
      "Installation"
    ]
  },
  {
    "objectID": "Installation.html#installation-on-gnulinux",
    "href": "Installation.html#installation-on-gnulinux",
    "title": "Installation",
    "section": "",
    "text": "ANNarchy depends on a number of packages that should be easily accessible on recent GNU/Linux distributions. The classical way to install these dependencies is through your package manager, or using full Python distributions such as Anaconda. Older versions of these packages may work but have not been tested.\n\npython &gt;= 3.10 (with the development files, e.g. python-dev or python-devel)\ng++ &gt;= 7.4 or clang++ &gt;= 3.4\ncmake &gt;= 3.16\nsetuptools &gt;= 65.0\nnanobind &gt;= 2.4.0\ncython &gt;= 3.0\nnumpy &gt;= 1.21\nsympy &gt;= 1.11\nscipy &gt;= 1.9\nmatplotlib &gt;= 3.0\ntqdm &gt;= 4.60\n\nAdditionally, the following packages are optional but strongly recommended:\n\nlxml (to save the networks in .xml format).\nh5py (to export data in .h5 format).\npandoc (for report()).\ntensorflow (for the ann_to_snn_conversion extension)\ntensorboardX (for the logging extension).\n\nThe CUDA-SDK is available on the official website (we recommend to use at least a SDK version &gt; 6.x). For further details on installation etc., please consider the corresponding Quickstart guides (Quickstart_8.0 for the SDK 8.x).\nANNarchy works with full Python distributions such as Anaconda, as well as in virtual environments and Jupyter notebooks.\n\n\n\n\n\n\nNote\n\n\n\nOn a fresh install of Ubuntu 22.04, here are the minimal system packages to install before ANNarchy:\nsudo apt install build-essential git python3-dev python3-setuptools python3-pip \n\n\n\n\n\n\n\nStable releases of ANNarchy are available on PyPi:\npip install ANNarchy\nor:\npip install ANNarchy --user\nif you do not have administrator permissions. Omit --user in a virtual environment.\nYou may also install directly the latest commit in the master (stable) or develop branches with:\npip install git+https://github.com/ANNarchy/ANNarchy.git@master\n\n\n\nInstallation of ANNarchy from source is possible using pip in the top-level directory:\npip install .\nor in development mode:\npip install -e .\nUsing python setup.py install is deprecated, but still works.\n\n\n\nBy default, ANNarchy will use the GNU C++ compiler g++, which should be in your PATH. If you want to use another compiler (clang++, icc), you can edit the configuration file located at $HOME/.config/ANNarchy/annarchy.json (created during installation) accordingly. By default, it is:\n{\n    \"openmp\": {\n        \"compiler\": \"g++\",\n        \"flags\": \"-march=native -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\nThe (path to the) compiler can be changed in the openmp section (ignore the cuda section if you do not have a GPU).\nYou can also change the compiler flags if you know what you are doing. -O3 does not always lead to faster simulation times, nor to exact simulations.\n\n\n\nIf ANNarchy detects the CUDA SDK during installation, it will prepare the required modules. You need to make sure that the CUDA compiler nvcc is accessible in your path.\nThe main problem with CUDA is that the binaries, headers and libraries are installed at different locations depending on the version: /usr/local/cuda, /usr/local/cuda-7.0 or /usr/local/cuda-8.0. There is unfortunately no way for ANNarchy to guess the installation path.\nA first thing to help ANNarchy find the CUDA libraries is to define the LD_LIBRARY_PATH environment variable and have point at the lib64/ subfolder:\nexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64/:$LD_LIBRARY_PATH\nThis should in most cases work if you have only one CUDA installation. Otherwise, it is needed that you indicate where the CUDA libraries are, by modifying the ANNarchy configuration file located at $HOME/.config/ANNarchy/annarchy.json:\n{\n    \"openmp\": {\n        \"compiler\": \"g++\",\n        \"flags\": \"-march=native -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\nSimply point the ['cuda']['path'] field to the right location (without lib64/). If the nvcc compiler binary is at a different location, the absolute path to the nvcc can be provided by ['cuda']['compiler'] field.\nIt can happen that the detection of CUDA fails during installation, as some environment variables are not set. In this case try:\nenv \"PATH=$PATH\" \"LIBRARY_PATH=$LIBRARY_PATH\" pip install .",
    "crumbs": [
      "ANNarchy",
      "Installation"
    ]
  },
  {
    "objectID": "Installation.html#installation-on-macos-x",
    "href": "Installation.html#installation-on-macos-x",
    "title": "Installation",
    "section": "",
    "text": "Installation on MacOS X is in principle similar to GNU/Linux:\npip install ANNarchy\nWe strongly advise using a full Python distribution like Miniforge, or a package manager like Homebrew rather than the default python provided by Apple.\nThe main issue is the choice of the C++ compiler:\n\n\nIf not done already, you should first install the Xcode Command Line Tools and Homebrew (see https://mac.install.guide/commandlinetools/ for a guide) to get the LLVM clang++ compiler.\nThe major drawback of Apple’s clang++ is that it still does not support OpenMP for parallel computing. Any attempt to use OpenMP with ANNarchy using this compiler will crash.\nIt is strongly advised to use a more recent version of clang++ such as the one offered by homebrew:\nbrew install llvm\nand update your shell config to use this version of clang++ instead of Apple’s (adapt the paths accordingly):\nexport PATH=\"/opt/homebrew/opt/llvm/bin:$PATH\"\nexport LDFLAGS=\"-L/opt/homebrew/opt/llvm/lib:$LDFLAGS\"\nexport CPPFLAGS=\"-I/opt/homebrew/opt/llvm/include:$CPPFLAGS\"\nIf you have a MX arm64 processor, it might be beneficial to tell clang++ to use optimizations for that hardware. Open the configuration file at $HOME/.config/ANNarchy/annarchy.json and add the following compiler flag (adapt it to your processor):\n{\n    \"openmp\": {\n        \"compiler\": \"clang++\",\n        \"flags\": \"-mcpu=apple-m1 -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\n\n\n\nIn order to benefit from OpenMP parallelization, you can also install gcc, the GNU C compiler, using Homebrew:\nbrew install gcc\nYou will get the command-line C++ compiler with a version number, e.g.:\ng++-11\nThe g++ executable is a symlink to Apple’s clang++, do not use it…\nYou now have to tell ANNarchy which compiler to use, even if it is in your PATH. After installing ANNarchy, a config file is created in $HOME/.config/ANNarchy/annarchy.json. Open it and change the openmp entry to:\n{\n    \"openmp\": {\n        \"compiler\": \"g++-11\",\n        \"flags\": \"-march=native -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\n\n\n\n\n\n\nWarning\n\n\n\nThe CUDA backend is not available on OS X.",
    "crumbs": [
      "ANNarchy",
      "Installation"
    ]
  },
  {
    "objectID": "reference/balloon_two_inputs.html",
    "href": "reference/balloon_two_inputs.html",
    "title": "balloon_two_inputs",
    "section": "",
    "text": "balloon_two_inputs\nballoon_two_inputs(self)\nBOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007).",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_two_inputs"
    ]
  },
  {
    "objectID": "reference/Normal.html",
    "href": "reference/Normal.html",
    "title": "Normal",
    "section": "",
    "text": "Normal(self, mu, sigma, min=None, max=None, rng=None)\nNormal distribution.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmu\nfloat\nMean of the distribution.\nrequired\n\n\nsigma\nfloat\nStandard deviation of the distribution.\nrequired\n\n\nmin\nfloat\nMinimum value (default: unlimited).\nNone\n\n\nmax\nfloat\nMaximum value (default: unlimited).\nNone\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Normal"
    ]
  },
  {
    "objectID": "reference/Normal.html#parameters",
    "href": "reference/Normal.html#parameters",
    "title": "Normal",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmu\nfloat\nMean of the distribution.\nrequired\n\n\nsigma\nfloat\nStandard deviation of the distribution.\nrequired\n\n\nmin\nfloat\nMinimum value (default: unlimited).\nNone\n\n\nmax\nfloat\nMaximum value (default: unlimited).\nNone\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Normal"
    ]
  },
  {
    "objectID": "reference/Normal.html#methods",
    "href": "reference/Normal.html#methods",
    "title": "Normal",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Normal"
    ]
  },
  {
    "objectID": "reference/TimedArray.html",
    "href": "reference/TimedArray.html",
    "title": "TimedArray",
    "section": "",
    "text": "TimedArray(\n    self,\n    rates=None,\n    geometry=None,\n    schedule=0.0,\n    period=-1.0,\n    name=None,\n    copied=False,\n    net_id=0,\n)\nData structure holding sequential inputs for a rate-coded network.\nThe input values are stored in the (recordable) attribute r, without any further processing.\nBy default, the firing rate of this population will iterate over the different values step by step:\ninputs = np.array(\n    [\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n    ]\n)\n\nnet = ann.Network()\ninp = net.create(ann.TimedArray(rates=inputs))\npop = net.create(10, ann.LeakyIntegrator)\n\nproj = net.connect(inp, pop, 'exc')\nproj.one_to_one(1.0)\n\nnet.compile()\n\nnet.simulate(10.)\nThis creates a population of 10 neurons whose activity will change during the first 10*dt milliseconds of the simulation. After that delay, the last input will be kept (i.e. 1 for the last neuron).\nIf you want the TimedArray to “loop” over the different input vectors, you can specify a period for the inputs:\ninp = net.create(ann.TimedArray(rates=inputs, period=10.))\nIf the period is smaller than the length of the rates, the last inputs will not be set.\nIf you do not want the inputs to be set at every step, but every 10 ms for example, you can use the schedule argument:\ninp = net.create(ann.TimedArray(rates=inputs, schedule=10.))\nThe input [1, 0, 0,…] will stay for 10 ms, then [0, 1, 0, …] for the next 10 ms, etc…\nIf you need a less regular schedule, you can specify it as a list of times:\ninp = ann.TimedArray(rates=inputs, schedule=[10., 20., 50., 60., 100., 110.])\nThe first input is set at t = 10 ms (r = 0.0 in the first 10 ms), the second at t = 20 ms, the third at t = 50 ms, etc.\nIf you specify less times than in the array of rates, the last ones will be ignored.\nScheduling can be combined with periodic cycling. Note that you can use the reset() method to manually reinitialize the TimedArray, times becoming relative to that call:\nann.simulate(100.) # ten inputs are shown with a schedule of 10 ms\ninp.reset()\nann.simulate(100.) # the same ten inputs are presented again.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrates\nnp.ndarray\narray of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population.\nNone\n\n\ngeometry\nint | tuple\ndesired dimensions of the population. This argument will be considered if rates is None.\nNone\n\n\nschedule\nfloat\neither a single value or a list of time points where inputs should be set. Default: every timestep.\n0.0\n\n\nperiod\nfloat\ntime when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.).\n-1.0\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nupdate\nSet a new array of inputs.\n\n\n\n\n\nupdate(rates, schedule=0.0, period=-1)\nSet a new array of inputs.\nThe first axis corresponds to time, the others to the desired dimensions of the population. Note, the geometry is set during construction phase of the object.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrates\nnp.ndarray\narray of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population.\nrequired\n\n\nschedule\nfloat\neither a single value or a list of time points where inputs should be set. Default: every timestep.\n0.0\n\n\nperiod\nfloat\ntime when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.).\n-1",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "TimedArray"
    ]
  },
  {
    "objectID": "reference/TimedArray.html#parameters",
    "href": "reference/TimedArray.html#parameters",
    "title": "TimedArray",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nrates\nnp.ndarray\narray of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population.\nNone\n\n\ngeometry\nint | tuple\ndesired dimensions of the population. This argument will be considered if rates is None.\nNone\n\n\nschedule\nfloat\neither a single value or a list of time points where inputs should be set. Default: every timestep.\n0.0\n\n\nperiod\nfloat\ntime when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.).\n-1.0",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "TimedArray"
    ]
  },
  {
    "objectID": "reference/TimedArray.html#methods",
    "href": "reference/TimedArray.html#methods",
    "title": "TimedArray",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nupdate\nSet a new array of inputs.\n\n\n\n\n\nupdate(rates, schedule=0.0, period=-1)\nSet a new array of inputs.\nThe first axis corresponds to time, the others to the desired dimensions of the population. Note, the geometry is set during construction phase of the object.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrates\nnp.ndarray\narray of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population.\nrequired\n\n\nschedule\nfloat\neither a single value or a list of time points where inputs should be set. Default: every timestep.\n0.0\n\n\nperiod\nfloat\ntime when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.).\n-1",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "TimedArray"
    ]
  },
  {
    "objectID": "reference/Logger.html",
    "href": "reference/Logger.html",
    "title": "Logger",
    "section": "",
    "text": "Logger(self, logdir='runs/', experiment=None)\nLogger class to use tensorboard to visualize ANNarchy simulations. Requires the tensorboardX package (pip install tensorboardX).\nThe Logger class is a thin wrapper around tensorboardX.SummaryWriter, which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io/. Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch.\nThe extension has to be imported explictly:\nfrom ANNarchy.extensions.tensorboard import Logger\nThe Logger class has to be closed properly at the end of the script, so it is advised to use a context:\nwith Logger() as logger:\n    logger.add_scalar(\"Accuracy\", acc, trial)\nYou can also make sure to close it:\nlogger = Logger()\nlogger.add_scalar(\"Accuracy\", acc, trial)\nlogger.close()\nBy default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine. You can control these two elements by passing arguments to Logger():\nwith Logger(logdir=\"/tmp/annarchy\", experiment=\"trial1\"): # logs in /tmp/annarchy/trial1\nThe add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc.\nA tag should be given to each plot. In the example above, the figure with the accuracy will be labelled “Accuracy” in tensorboard. You can also group plots together with tags such as “Global performance/Accuracy”, “Global performance/Error rate”, “Neural activity/Population 1”, etc.\nAfter (or while) logging data within your simulation, run tensorboard in the terminal by specifying the log directory:\ntensorboard --logdir runs\nTensorboardX enqueues the data in memory before writing to disk. You can force flushing with:\nlogger.flush()\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlogdir\nstr\npath (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is “runs/”\n'runs/'\n\n\nexperiment\nstr\nname of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_scalar\nLogs a single scalar value, e.g. a success rate at various stages of learning.\n\n\nadd_scalars\nLogs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities.\n\n\nadd_image\nLogs an image.\n\n\nadd_images\nLogs a set of images (e.g. receptive fields).\n\n\nadd_parameters\nLogs parameters of a simulation.\n\n\nadd_histogram\nLogs an histogram.\n\n\nadd_figure\nLogs a Matplotlib figure.\n\n\nflush\nForces the logged data to be flushed to disk.\n\n\nclose\nCloses the logger.\n\n\n\n\n\nadd_scalar(tag, value, step=None)\nLogs a single scalar value, e.g. a success rate at various stages of learning.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        accuracy = ...\n        logger.add_scalar(\"Accuracy\", accuracy, trial)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nvalue\nfloat\nvalue.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\n\n\n\n\n\nadd_scalars(tag, value, step=None)\nLogs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        act1 = pop.r[0]\n        act2 = pop.r[1]\n        logger.add_scalars(\n            \"Accuracy\", \n            {'First neuron': act1, 'Second neuron': act2}, \n            trial)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nvalue\ndict\ndictionary of values.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\n\n\n\n\n\nadd_image(tag, img, step=None, equalize=False)\nLogs an image.\nThe image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values.\nExample::\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        img = pop.r.reshape((10, 10))\n        logger.add_image(\"Population / Firing rate\", img, trial, equalize=True)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nimg\nnp.ndarray\narray for the image.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\nequalize\nbool\nrescales the pixels between 0 and 1 using the min and max values of the array.\nFalse\n\n\n\n\n\n\n\nadd_images(tag, img, step=None, equalize=False, equalize_per_image=False)\nLogs a set of images (e.g. receptive fields).\nThe numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons\n        logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nimg\nnp.array\narray for the images.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\nequalize\nbool\nrescales the pixels between 0 and 1 using the min and max values of the array.\nFalse\n\n\nequalize_per_image\nbool\nwhether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False.\nFalse\n\n\n\n\n\n\n\nadd_parameters(params, metrics)\nLogs parameters of a simulation.\nThis should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc.\nExample:\nwith Logger() as logger:\n    # ...\n    logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy})\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparams\ndict\ndictionary of parameters.\nrequired\n\n\nmetrics\ndict\ndictionary of metrics.\nrequired\n\n\n\n\n\n\n\nadd_histogram(tag, hist, step=None)\nLogs an histogram.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        weights= proj.w.flatten()\n        logger.add_histogram(\"Weight distribution\", weights, trial)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nhist\nlist | np.ndarray\na list or 1D numpy array of values.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\n\n\n\n\n\nadd_figure(tag, figure, step=None, close=True)\nLogs a Matplotlib figure.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        fig = plt.figure()\n        plt.plot(pop.r)\n        logger.add_figure(\"Activity\", fig, trial)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the image in tensorboard.\nrequired\n\n\nfigure\nlist | np.ndarray\na list or 1D numpy array of values.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\nclose\nbool\nwhether the logger will close the figure when done (default: True).\nTrue\n\n\n\n\n\n\n\nflush()\nForces the logged data to be flushed to disk.\n\n\n\nclose()\nCloses the logger.",
    "crumbs": [
      "Reference",
      "**Tensorboard logging**",
      "Logger"
    ]
  },
  {
    "objectID": "reference/Logger.html#parameters",
    "href": "reference/Logger.html#parameters",
    "title": "Logger",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nlogdir\nstr\npath (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is “runs/”\n'runs/'\n\n\nexperiment\nstr\nname of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended.\nNone",
    "crumbs": [
      "Reference",
      "**Tensorboard logging**",
      "Logger"
    ]
  },
  {
    "objectID": "reference/Logger.html#methods",
    "href": "reference/Logger.html#methods",
    "title": "Logger",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nadd_scalar\nLogs a single scalar value, e.g. a success rate at various stages of learning.\n\n\nadd_scalars\nLogs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities.\n\n\nadd_image\nLogs an image.\n\n\nadd_images\nLogs a set of images (e.g. receptive fields).\n\n\nadd_parameters\nLogs parameters of a simulation.\n\n\nadd_histogram\nLogs an histogram.\n\n\nadd_figure\nLogs a Matplotlib figure.\n\n\nflush\nForces the logged data to be flushed to disk.\n\n\nclose\nCloses the logger.\n\n\n\n\n\nadd_scalar(tag, value, step=None)\nLogs a single scalar value, e.g. a success rate at various stages of learning.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        accuracy = ...\n        logger.add_scalar(\"Accuracy\", accuracy, trial)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nvalue\nfloat\nvalue.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\n\n\n\n\n\nadd_scalars(tag, value, step=None)\nLogs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        act1 = pop.r[0]\n        act2 = pop.r[1]\n        logger.add_scalars(\n            \"Accuracy\", \n            {'First neuron': act1, 'Second neuron': act2}, \n            trial)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nvalue\ndict\ndictionary of values.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\n\n\n\n\n\nadd_image(tag, img, step=None, equalize=False)\nLogs an image.\nThe image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values.\nExample::\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        img = pop.r.reshape((10, 10))\n        logger.add_image(\"Population / Firing rate\", img, trial, equalize=True)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nimg\nnp.ndarray\narray for the image.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\nequalize\nbool\nrescales the pixels between 0 and 1 using the min and max values of the array.\nFalse\n\n\n\n\n\n\n\nadd_images(tag, img, step=None, equalize=False, equalize_per_image=False)\nLogs a set of images (e.g. receptive fields).\nThe numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons\n        logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nimg\nnp.array\narray for the images.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\nequalize\nbool\nrescales the pixels between 0 and 1 using the min and max values of the array.\nFalse\n\n\nequalize_per_image\nbool\nwhether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False.\nFalse\n\n\n\n\n\n\n\nadd_parameters(params, metrics)\nLogs parameters of a simulation.\nThis should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc.\nExample:\nwith Logger() as logger:\n    # ...\n    logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy})\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparams\ndict\ndictionary of parameters.\nrequired\n\n\nmetrics\ndict\ndictionary of metrics.\nrequired\n\n\n\n\n\n\n\nadd_histogram(tag, hist, step=None)\nLogs an histogram.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        weights= proj.w.flatten()\n        logger.add_histogram(\"Weight distribution\", weights, trial)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nhist\nlist | np.ndarray\na list or 1D numpy array of values.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\n\n\n\n\n\nadd_figure(tag, figure, step=None, close=True)\nLogs a Matplotlib figure.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        fig = plt.figure()\n        plt.plot(pop.r)\n        logger.add_figure(\"Activity\", fig, trial)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the image in tensorboard.\nrequired\n\n\nfigure\nlist | np.ndarray\na list or 1D numpy array of values.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\nclose\nbool\nwhether the logger will close the figure when done (default: True).\nTrue\n\n\n\n\n\n\n\nflush()\nForces the logged data to be flushed to disk.\n\n\n\nclose()\nCloses the logger.",
    "crumbs": [
      "Reference",
      "**Tensorboard logging**",
      "Logger"
    ]
  },
  {
    "objectID": "reference/ANNtoSNNConverter.html",
    "href": "reference/ANNtoSNNConverter.html",
    "title": "ANNtoSNNConverter",
    "section": "",
    "text": "ANNtoSNNConverter(\n    self,\n    input_encoding='CPN',\n    hidden_neuron='IaF',\n    read_out='spike_count',\n    **kwargs,\n)\nConverts a pre-trained Keras model .keras into an ANNarchy spiking neural network.\nThe implementation of the present module is inspired by the SNNToolbox (Rueckauer et al. 2017), and is largely based on the work of Diehl et al. (2015). We provide several input encodings, as suggested in the work of Park et al. (2019) and Auge et al. (2021).\nConstraints on the ANN\nIt is not possible to convert any keras model into an ANNarchy SNN: some constraints ahve to be respected.\n\nThe only allowed layers in the ANN are:\n\nDense\nConv2D\nMaxPooling2D\nAveragePooling2D\nas well as non-neural layers such as Dropout, Activation, BatchNorm, etc.\n\nThe layers must not contain any bias, even the conv layers:\n\nx = tf.keras.layers.Dense(128, use_bias=False, activation='relu')(x)\nx = tf.keras.layers.Conv2D(64, kernel_size=(5,5), activation='relu', padding='same', use_bias=False)(x)\n\nThe first layer of the network must be an Input:\n\ninputs = tf.keras.Input(shape = (28, 28, 1))\n\nPooling must explicitly be done by MaxPooling2D/AveragePooling2D, strides are ignored.\n\nPlease be aware that the module is very experimental and the conversion may not work for many different reasons. Feel free to submit issues.\nProcessing Queue\nThe pre-trained ANN model to be converted should be saved in keras (extension .keras). The saved model is transformed layer by layer into a feed-forward ANNarchy spiking network. The structure of the network remains the same as in the original ANN, while the weights are normalised. Please note that the current implementation focuses primarily on the correctness of the conversion. Computational performance, especially of the converted CNNs, will be improved in future releases.\n\n\n\n\n\n\nNote\n\n\n\nWhile the neurons are conceptually spiking neurons, there is one specialty: next to the spike event (stored automatically in ANNarchy), each event will be stored in an additional mask array. This mask value decays in absence of further spike events exponentially. The decay can be controlled by the mask_tau parameter of the population. The projections (either dense or convolution) will use this mask as pre-synaptic input, not the generated list of spike events.\n\n\nInput Encoding\n\nPoisson (“CPN”)\n\nThis encoding uses a Poisson distribution where the pixel values of the image will be used as probability for each individual neuron.\n\nIntrinsically Bursting (“IB”)\n\nThis encoding is based on the Izhikevich (2003) model that comprises two ODEs:\n\n\\begin{cases}\n\\frac{dv}{dt} = 0.04 \\cdot v^2 + 5.0 \\cdot v + 140.0 - u + I \\\\\n\\\\\n\\frac{du}{dt} = a \\cdot (b \\cdot v - u) \\\\\n\\end{cases}\n\nThe parameters for a - d are selected accordingly to Izhikevich (2003). The provided input images will be set as I.\n\nPhase Shift Oscillation (“PSO”)\n\nBased on the description by Park et al. (2019), the spiking threshold v_\\text{th} is modulated by a oscillation function \\Pi, whereas the membrane potential follows simply the input current.\n\n\\begin{cases}\n\\Pi(t) = 2^{-(1+ \\text{mod}(t,k))}\\\\\n\\\\\nv_\\text{th}(t) = \\Pi(t) \\, v_\\text{th}(t)\\\\\n\\end{cases}\n\n\nUser-defined input encodings\n\nIn addition to the pre-defined models, one can opt for individual models using the Neuron class of ANNarchy. Please note that a mask variable need to be defined, which is fed into the subsequent projections.\nRead-out Methods\nIn a classification task, the neuron with the highest activity corresponds corresponds to the decision to which class the presented input belongs. However, the highest activity can be determined in different ways. We support currently three methods, defined by the read_out parameter of the constructor:\n\nMaximum Spike Count\n\nread_out = 'spike_count' : the number of spikes emitted by each neuron is recorded and the index of the neuron(s) with the maximum number is returned.\n\nTime to Number of Spikes\n\nread_out = 'time_to_first_spike' or read_out = 'time_to_k_spikes': when the first or first k spikes are emitted by a single neuron, the simulation is stopped and the neuron rank(s) is returned. For the second mode, an additional k argument need to be also provided.\n\nMembrane potential\n\nread_out = 'membrane_potential': pre-synaptic events are accumulated in the membrane potential of each output neuron. The index of the neuron(s) with the highest membrane potential is returned.\n\nIzhikevich (2003) Simple Model of Spiking Neurons. IEEE transactions on neural networks 14(6). doi: 10.1109/TNN.2003.820440\n\n\nDiehl PU, Neil D, Binas J,et al. (2015) Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing, 2015 International Joint Conference on Neural Networks (IJCNN), 1-8, doi: 10.1109/IJCNN.2015.7280696.\n\n\nRueckauer B, Lungu I, Hu Y, et al. (2017) Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification., Front. Neurosci., 2017, 11. doi: 10.3389/fnins.2017.00682\n\n\nPark S, Kim S, Choe H, et al. (2019) Fast and Efficient Information Transmission with Burst Spikes in Deep Spiking Neural Networks.\n\n\nAuge D, Hille J, Mueller E et al. (2021) A Survey of Encoding Techniques for Signal Processing in Spiking Neural Networks. Neural Processing Letters. 2021; 53:4963-4710. doi:10.1007/s11063-021-10562-2\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput_encoding\nstr\na string representing which input encoding should be used: ‘CPN’, ‘IB’ or ‘PSO’.\n'CPN'\n\n\nhidden_neuron\nstr\nneuron model used in the hidden layers. Either the default integrate-and-fire (‘IaF’) or an ANNarchy Neuron object.\n'IaF'\n\n\nread_out\nstr\na string which of the following read-out method should be used: spike_count, time_to_first_spike, membrane_potential.\n'spike_count'\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nload_keras_model\nLoads the pre-trained model provided as a .keras file.\n\n\nget_annarchy_network\nReturns the ANNarchy.Network instance.\n\n\npredict\nPerforms the prediction for a given input array.\n\n\n\n\n\nload_keras_model(\n    filename,\n    directory='annarchy',\n    scale_factor=None,\n    show_info=True,\n)\nLoads the pre-trained model provided as a .keras file.\nIn tf.keras, the weights can be saved using:\nmodel.save(\"model.keras\")\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\npath to the .keras file.\nrequired\n\n\ndirectory\nstr\nsub-directory where the generated code should be stored (default: “annarchy”)\n'annarchy'\n\n\nscale_factor\nfloat\nallows a fine-grained control of the weight scale factor. By default (None), with each layer-depth the factor increases by one. If a scalar value is provided the same value is used for each layer. Otherwise a list can be provided to assign the scale factors individually.\nNone\n\n\nshow_info\nbool\nwhether the network structure should be printed on console (default: True)\nTrue\n\n\n\n\n\n\n\nget_annarchy_network()\nReturns the ANNarchy.Network instance.\n\n\n\npredict(samples, duration_per_sample=1000, multiple=False)\nPerforms the prediction for a given input array.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsamples\nnp.ndarray\nset of inputs to present to the network. The function expects a 2-dimensional array (num_samples, input_size).\nrequired\n\n\nduration_per_sample\nint\nthe number of simulation steps for one input sample (default: 1000, 1 second biological time)\n1000\n\n\nmultiple\nbool\nif several output neurons reach the criteria, return the full list instead of randomly chosing one.\nFalse",
    "crumbs": [
      "Reference",
      "**ANN-to-SNN conversion**",
      "ANNtoSNNConverter"
    ]
  },
  {
    "objectID": "reference/ANNtoSNNConverter.html#parameters",
    "href": "reference/ANNtoSNNConverter.html#parameters",
    "title": "ANNtoSNNConverter",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ninput_encoding\nstr\na string representing which input encoding should be used: ‘CPN’, ‘IB’ or ‘PSO’.\n'CPN'\n\n\nhidden_neuron\nstr\nneuron model used in the hidden layers. Either the default integrate-and-fire (‘IaF’) or an ANNarchy Neuron object.\n'IaF'\n\n\nread_out\nstr\na string which of the following read-out method should be used: spike_count, time_to_first_spike, membrane_potential.\n'spike_count'",
    "crumbs": [
      "Reference",
      "**ANN-to-SNN conversion**",
      "ANNtoSNNConverter"
    ]
  },
  {
    "objectID": "reference/ANNtoSNNConverter.html#methods",
    "href": "reference/ANNtoSNNConverter.html#methods",
    "title": "ANNtoSNNConverter",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nload_keras_model\nLoads the pre-trained model provided as a .keras file.\n\n\nget_annarchy_network\nReturns the ANNarchy.Network instance.\n\n\npredict\nPerforms the prediction for a given input array.\n\n\n\n\n\nload_keras_model(\n    filename,\n    directory='annarchy',\n    scale_factor=None,\n    show_info=True,\n)\nLoads the pre-trained model provided as a .keras file.\nIn tf.keras, the weights can be saved using:\nmodel.save(\"model.keras\")\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\npath to the .keras file.\nrequired\n\n\ndirectory\nstr\nsub-directory where the generated code should be stored (default: “annarchy”)\n'annarchy'\n\n\nscale_factor\nfloat\nallows a fine-grained control of the weight scale factor. By default (None), with each layer-depth the factor increases by one. If a scalar value is provided the same value is used for each layer. Otherwise a list can be provided to assign the scale factors individually.\nNone\n\n\nshow_info\nbool\nwhether the network structure should be printed on console (default: True)\nTrue\n\n\n\n\n\n\n\nget_annarchy_network()\nReturns the ANNarchy.Network instance.\n\n\n\npredict(samples, duration_per_sample=1000, multiple=False)\nPerforms the prediction for a given input array.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsamples\nnp.ndarray\nset of inputs to present to the network. The function expects a 2-dimensional array (num_samples, input_size).\nrequired\n\n\nduration_per_sample\nint\nthe number of simulation steps for one input sample (default: 1000, 1 second biological time)\n1000\n\n\nmultiple\nbool\nif several output neurons reach the criteria, return the full list instead of randomly chosing one.\nFalse",
    "crumbs": [
      "Reference",
      "**ANN-to-SNN conversion**",
      "ANNtoSNNConverter"
    ]
  },
  {
    "objectID": "reference/Pooling.html",
    "href": "reference/Pooling.html",
    "title": "Pooling",
    "section": "",
    "text": "Pooling(\n    self,\n    pre,\n    post,\n    target,\n    psp='pre.r',\n    operation='max',\n    name=None,\n    copied=False,\n    net_id=0,\n)\nPerforms a pooling operation (e.g. max.pooling) on the pre-synaptic population.\nEach post-synaptic neuron covers a specific region (extent) of the pre-synaptic population, over which the result of the operation on firing rates will be assigned to sum(target).\nThe extent is automatically computed using the geometry of the populations, but can be specified in the `pooling()`` methods.\nExample:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.convolution import Pooling\n\nnet = ann.Network()\ninp = net.create(geometry=(100, 100), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npop = net.create(geometry=(50, 50), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = net.connect(Pooling(inp, pop, 'exc', operation='max')) # max-pooling\nproj.pooling() # extent=(2, 2) is implicit\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\n\npre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\n\npost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\n\ntype of the connection\nrequired\n\n\noperation\n\npooling function to be applied (“max”, “min”, “mean”)\n'max'\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\npooling\nConnects the Pooling projection.\n\n\nsave_connectivity\nNot available.\n\n\nsave\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nconnectivity_matrix\nNot available.\n\n\n\n\n\npooling(extent=None, delays=0.0)\nConnects the Pooling projection.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nextent\ntuple\nextent of the pooling area expressed in the geometry of the pre-synaptic population (e.g (2, 2)). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None.\nNone\n\n\ndelays\nfloat\nsynaptic delay in ms\n0.0\n\n\n\n\n\n\n\nsave_connectivity(filename)\nNot available.\n\n\n\nsave(filename)\nNot available.\n\n\n\nload(filename)\nNot available.\n\n\n\nreceptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nconnectivity_matrix(fill=0.0)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Pooling"
    ]
  },
  {
    "objectID": "reference/Pooling.html#parameters",
    "href": "reference/Pooling.html#parameters",
    "title": "Pooling",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\n\npre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\n\npost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\n\ntype of the connection\nrequired\n\n\noperation\n\npooling function to be applied (“max”, “min”, “mean”)\n'max'",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Pooling"
    ]
  },
  {
    "objectID": "reference/Pooling.html#methods",
    "href": "reference/Pooling.html#methods",
    "title": "Pooling",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npooling\nConnects the Pooling projection.\n\n\nsave_connectivity\nNot available.\n\n\nsave\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nconnectivity_matrix\nNot available.\n\n\n\n\n\npooling(extent=None, delays=0.0)\nConnects the Pooling projection.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nextent\ntuple\nextent of the pooling area expressed in the geometry of the pre-synaptic population (e.g (2, 2)). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None.\nNone\n\n\ndelays\nfloat\nsynaptic delay in ms\n0.0\n\n\n\n\n\n\n\nsave_connectivity(filename)\nNot available.\n\n\n\nsave(filename)\nNot available.\n\n\n\nload(filename)\nNot available.\n\n\n\nreceptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nconnectivity_matrix(fill=0.0)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Pooling"
    ]
  },
  {
    "objectID": "reference/DecodingProjection.html",
    "href": "reference/DecodingProjection.html",
    "title": "DecodingProjection",
    "section": "",
    "text": "DecodingProjection(\n    self,\n    pre,\n    post,\n    target,\n    window=0.0,\n    name=None,\n    copied=False,\n    net_id=0,\n)\nDecoding projection to transform spike trains into firing rates.\nThe pre-synaptic population must be a spiking population, while the post-synaptic one must be rate-coded.\nPre-synaptic spikes are accumulated for each post-synaptic neuron. A sliding window can be used to smoothen the results with the window parameter.\nThe decoded firing rate is accessible in the post-synaptic neurons with sum(target).\nThe projection can be connected using any method available in Projection (although all-to-all or many-to-one makes mostly sense). Delays are ignored.\nThe weight value allows to scale the firing rate: if you want a pre-synaptic firing rate of 100 Hz to correspond to a post-synaptic rate of 1.0, use w = 1./100..\nExample:\nnet = ann.Network()\n\npop1 = net.create(ann.PoissonPopulation(1000, rates=100.))\npop2 = net.create(1, ann.Neuron(equations=\"r=sum(exc)\"))\n\nproj = net.connect(ann.DecodingProjection(pop1, pop2, 'exc', window=10.0))\nproj.all_to_all(1.0)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\nPopulation\npre-synaptic population.\nrequired\n\n\npost\nPopulation\npost-synaptic population.\nrequired\n\n\ntarget\nstr\ntype of the connection.\nrequired\n\n\nwindow\nfloat\nduration of the time window to collect spikes (default: dt).\n0.0\n\n\nname\nstr\noptional name.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "DecodingProjection"
    ]
  },
  {
    "objectID": "reference/DecodingProjection.html#parameters",
    "href": "reference/DecodingProjection.html#parameters",
    "title": "DecodingProjection",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\nPopulation\npre-synaptic population.\nrequired\n\n\npost\nPopulation\npost-synaptic population.\nrequired\n\n\ntarget\nstr\ntype of the connection.\nrequired\n\n\nwindow\nfloat\nduration of the time window to collect spikes (default: dt).\n0.0\n\n\nname\nstr\noptional name.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "DecodingProjection"
    ]
  },
  {
    "objectID": "reference/callbacks_enabled.html",
    "href": "reference/callbacks_enabled.html",
    "title": "callbacks_enabled",
    "section": "",
    "text": "callbacks_enabled\ncallbacks_enabled(net_id=0)\nReturns True if callbacks are enabled for the network.",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "callbacks_enabled"
    ]
  },
  {
    "objectID": "reference/report.html",
    "href": "reference/report.html",
    "title": "report",
    "section": "",
    "text": "report(\n    network,\n    filename='./report.tex',\n    standalone=True,\n    gather_subprojections=False,\n    title=None,\n    author=None,\n    date=None,\n)\nGenerates a report describing the network.\nIf the filename ends with .tex, the TeX file is generated according to:\n\nNordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8): e1000456.\n\nIf the filename ends with .md, a (more complete) Markdown file is generated, which can be converted to pdf or html by pandoc::\npandoc report.md -o report.pdf\npandoc report.md -o report.html\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnetwork\nNetwork\nNetwork instance.\nrequired\n\n\nfilename\nstr\nname of the file where the report will be written.\n'./report.tex'\n\n\nstandalone\nbool\ntells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown.\nTrue\n\n\ngather_subprojections\nbool\nif a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary.\nFalse\n\n\ntitle\nstr\ntitle of the document (Markdown only).\nNone\n\n\nauthor\nstr\nauthor of the document (Markdown only).\nNone\n\n\ndate\nstr\ndate of the document (Markdown only).\nNone",
    "crumbs": [
      "Reference",
      "**Utilities**",
      "report"
    ]
  },
  {
    "objectID": "reference/report.html#parameters",
    "href": "reference/report.html#parameters",
    "title": "report",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nnetwork\nNetwork\nNetwork instance.\nrequired\n\n\nfilename\nstr\nname of the file where the report will be written.\n'./report.tex'\n\n\nstandalone\nbool\ntells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown.\nTrue\n\n\ngather_subprojections\nbool\nif a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary.\nFalse\n\n\ntitle\nstr\ntitle of the document (Markdown only).\nNone\n\n\nauthor\nstr\nauthor of the document (Markdown only).\nNone\n\n\ndate\nstr\ndate of the document (Markdown only).\nNone",
    "crumbs": [
      "Reference",
      "**Utilities**",
      "report"
    ]
  },
  {
    "objectID": "reference/Network.html",
    "href": "reference/Network.html",
    "title": "Network",
    "section": "",
    "text": "Network(self, dt=None, seed=None)\nA network creates the populations, projections and monitors, and controls the simulation.\nnet = ann.Network(dt=1.0, seed=42)\nTo create a population:\npop = net.create(100, neuron=ann.Izhikevich)\nTo connect two populations:\nproj = net.connect(pre=pop1, post=pop2, target='exc', synapse=ann.STDP)\nTo monitor a population or projection:\npop = net.monitor(pop, ['spike', 'v'])\nTo compile the network:\nnet.compile()\nTo simulate for one second:\nnet.simulate(1000.)\nRefer to the manual for more functionalities. dt and seed must be passed with keywords.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndt\nfloat\nstep size in milliseconds.\nNone\n\n\nseed\nint\nseed for the random number generators.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndt\nStep size in milliseconds for the integration of the ODEs.\n\n\nseed\nSeed for the random number generator (Python and C++).\n\n\ncompiled\nBoolean indicating whether the network has been compiled.\n\n\ndirectory\nDirectory in which the network has been compiled.\n\n\ntime\nCurrent time t in milliseconds.\n\n\ncurrent_step\nCurrent simulation step.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncreate\nAdds a population of neurons to the network.\n\n\nconnect\nConnects two populations by creating a projection.\n\n\nmonitor\nCreates a Monitor on variables of the specified object.\n\n\nboldmonitor\nMonitors the BOLD signal of several populations using a computational model.\n\n\nconstant\nAdds a constant to the network.\n\n\ncompile\nCompiles the network.\n\n\nsimulate\nRuns the network for the given duration in milliseconds.\n\n\nsimulate_until\nRuns the network for the maximal duration in milliseconds until a stop_condition is met.\n\n\nstep\nPerforms a single simulation step (duration = dt).\n\n\nreset\nReinitialises the network to its state before the call to compile().\n\n\nenable_learning\nEnables learning for all projections.\n\n\ndisable_learning\nDisables learning for all projections.\n\n\nclear\nEmpties the network to prevent a memory leak until the garbage collector wakes up.\n\n\nparallel_run\nRuns the provided method for multiple copies of the network.\n\n\ncopy\nReturns a new instance of the Network class, using the provided arguments to the constructor.\n\n\nconfig\nConfiguration of the network.\n\n\nload\nLoads parameters and variables from a file created with Network.save().\n\n\nsave\nSaves the parameters and variables of the networkin a file.\n\n\nget_population\nReturns the population with the given name.\n\n\nget_projection\nReturns the projection with the given name.\n\n\nget_monitor\nReturns the monitor with the given name.\n\n\nget_extension\nReturns the extension with the given name.\n\n\nget_constant\nReturns the constant with the given name.\n\n\nget_populations\nReturns a list of all declared populations in this network.\n\n\nget_projections\nReturns a list of all declared projections for the current network.\n\n\nget_monitors\nReturns a list of declared monitors.\n\n\nget_extensions\nReturns a list of declared extensions (e.g. BOLD monitors).\n\n\nget_constants\nReturns a list of declared constants.\n\n\ncallbacks_enabled\nReturns True if callbacks are enabled for the network.\n\n\ndisable_callbacks\nDisables all callbacks for the network.\n\n\nenable_callbacks\nEnables all declared callbacks for the network.\n\n\nclear_all_callbacks\nClears the list of declared callbacks for the network.\n\n\n\n\n\ncreate(\n    geometry,\n    neuron=None,\n    stop_condition=None,\n    name=None,\n    population=None,\n    storage_order='post_to_pre',\n)\nAdds a population of neurons to the network.\nnet = ann.Network()\npop = net.create(geometry=100, neuron=ann.Izhikevich, name=\"Excitatory population\")\nSpecific populations (e.g. PoissonPopulation()) can also be passed to the population argument, or simply as the first argument:\npop = net.create(population=ann.PoissonPopulation(100, rates=20.))\n# or\npop = net.create(ann.PoissonPopulation(100, rates=20.))\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple | int\npopulation geometry as tuple. If an integer is given, it is the size of the population.\nrequired\n\n\nneuron\nNeuron\nNeuron instance. It can be user-defined or a built-in model.\nNone\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\nstop_condition\nstr\na single condition on a neural variable which can stop the simulation whenever it is true.\nNone\n\n\npopulation\nPopulation\ninstance of a SpecificPopulation.\nNone\n\n\n\n\n\n\n\nconnect(\n    pre,\n    post=None,\n    target='',\n    synapse=None,\n    name=None,\n    projection=None,\n    disable_omp=True,\n)\nConnects two populations by creating a projection.\nnet.connect(pre=pop1, post=pop2, target=\"exc\", synapse=STDP)\nIf the synapse argument is omitted, defaults synapses without plastivity will be used (psp = \"w * pre.r\" for rate-coded projections, pre_spike=\"g_target += w\" for spiking ones.).\nSpecific projections can be passed to the projection argument, or as the first unnamed argument.\nnet.connect(projection=ann.DecodingProjection(pre, post, 'exc))\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\nstr | Population\npre-synaptic population.\nrequired\n\n\npost\nstr | Population\npost-synaptic population.\nNone\n\n\ntarget\nstr\ntype of the connection.\n''\n\n\nsynapse\nSynapse\nSynapse class or instance.\nNone\n\n\nname\nstr\n(optional) name of the Projection.\nNone\n\n\nprojection\nProjection\nspecific projection.\nNone\n\n\n\n\n\n\n\nmonitor(\n    obj,\n    variables=[],\n    period=None,\n    period_offset=None,\n    start=True,\n    name=None,\n)\nCreates a Monitor on variables of the specified object.\nThe object can be an instance of either Population, PopulationView, Dendrite or Projection.\nThe variables must be declared by the neuron or synapse type. For spiking neurons, 'spike' can be also recorded.\nm = net.monitor(pop, ['v', 'spike'])\nm = net.monitor(proj.dendrite(0), 'w')\nBy default, the monitors start recording right after Network.compile(), but you can hold them with start=False. Starting the recordings necessitates then to call the start()mehtod. Pausing/resuming the recordings is cheived through the pause() and resume().\nm.start() # Start recording\nnet.simulate(T)\nm.pause() # Pause recording\nnet.simulate(T)\nm.resume() # Resume recording\nnet.simulate(T)\n\ndata = m.get() # Get the data\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobj\nPopulation | PopulationView | Projection\nobject to monitor. Must be a Population, PopulationView, Dendrite or Projection object.\nrequired\n\n\nvariables\nlist\nsingle variable name or list of variable names to record (default: []).\n[]\n\n\nperiod\nfloat\n\nNone\n\n\nperiod_offset\nfloat\n\nNone\n\n\nstart\nbool\n\nTrue\n\n\nname\nstr\n\nNone\n\n\n\n\n\n\n\nboldmonitor(\n    populations=None,\n    bold_model=None,\n    mapping={'I_CBF': 'r'},\n    scale_factor=None,\n    normalize_input=None,\n    recorded_variables=None,\n    start=False,\n)\nMonitors the BOLD signal of several populations using a computational model.\nThe BOLD monitor transforms one or two input population variables (such as the mean firing rate) into a recordable BOLD signal according to a computational model (for example a variation of the Balloon model).\nSeveral models are available or can be created with a bold.BoldModel instance. These models must be explicitly imported:\nimport ANNarchy.extensions.bold as bold\n\nm_bold = net.boldmonitor(\n    # Recorded populations\n    populations = [pop1, pop2], \n    # BOLD model to use (default is balloon_RN)\n    bold_model = bold.balloon_RN(), \n    # Mapping from pop.r to I_CBF\n    mapping = {'I_CBF': 'r'}, \n    # Time window to compute the baseline.\n    normalize_input = 2000,  \n    # Variables to be recorded\n    recorded_variables = [\"I_CBF\", \"BOLD\"]  \n)\n\n# Unlike regular monitors, BOLD monitors must be explicitly started\nm_bold.start()\nnet.simulate(5000) \nbold_data = m_bold.get(\"BOLD\")\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npopulations\nlist\nlist of recorded populations.\nNone\n\n\nbold_model\nbold.BoldModel\ncomputational model for BOLD signal defined as a BoldModel object. Default is bold.balloon_RN.\nNone\n\n\nmapping\ndict\nmapping dictionary between the inputs of the BOLD model (I_CBF for single inputs, I_CBF and I_CMRO2 for double inputs in the provided examples) and the variables of the input populations. By default, {'I_CBF': 'r'} maps the firing rate r of the input population(s) to the variable I_CBF of the BOLD model.\n{'I_CBF': 'r'}\n\n\nscale_factor\nlist[float]\nlist of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region.\nNone\n\n\nnormalize_input\nlist[int]\nlist of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time).\nNone\n\n\nrecorded_variables\nlist[str]\nwhich variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [“BOLD”] for the provided examples).\nNone\n\n\nstart\nbool\nwhether to start recording directly.\nFalse\n\n\n\n\n\n\n\nconstant(name, value)\nAdds a constant to the network.\nc = net.constant('c', 2.0)\n\n# `c` can be used in a neuron/synapse definition\nneuron = ann.Neuron(equations=\"r = c * sum(exc)\")\n\n# Change the value of the constant in the network.\nc.set(10.0) \n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nname of the constant.\nrequired\n\n\nvalue\n\ninitial value of the constant.\nrequired\n\n\n\n\n\n\n\ncompile(\n    directory='annarchy',\n    clean=False,\n    compiler='default',\n    compiler_flags='default',\n    add_sources='',\n    extra_libs='',\n    cuda_config={'device': 0},\n    annarchy_json='',\n    silent=False,\n    debug_build=False,\n    profile_enabled=False,\n)\nCompiles the network.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndirectory\nstr\nname of the subdirectory where the code will be generated and compiled. Default: “./annarchy/”.\n'annarchy'\n\n\nclean\nbool\nboolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False).\nFalse\n\n\ncompiler\nstr\nC++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++].\n'default'\n\n\ncompiler_flags\nlist[str]\nplatform-specific flags to pass to the compiler. Default: “-march=native -O2”. Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended.\n'default'\n\n\ncuda_config\ndict\ndictionary defining the CUDA configuration for each population and projection.\n{'device': 0}\n\n\nannarchy_json\nstr\ncompiler flags etc are stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location.\n''\n\n\nsilent\nbool\ndefines if the “Compiling… OK” should be printed.\nFalse\n\n\n\n\n\n\n\nsimulate(duration, measure_time=False)\nRuns the network for the given duration in milliseconds.\nThe number of simulation steps is computed relative to the discretization step dt declared in the constructor (default: 1 ms):\nnet.simulate(1000.0)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nduration\nfloat\nthe duration in milliseconds.\nrequired\n\n\nmeasure_time\nbool\ndefines whether the simulation time should be printed.\nFalse\n\n\n\n\n\n\n\nsimulate_until(max_duration, population, operator='and', measure_time=False)\nRuns the network for the maximal duration in milliseconds until a stop_condition is met.\nWhenever the stop_condition defined in population becomes true, the simulation is stopped.\nThe method returns the actual duration of the simulation in milliseconds.\nOne can specify several populations. If the stop condition is true for any of the populations, the simulation will stop (‘or’ function).\nExample:\npop1 = net.create( ..., stop_condition = \"r &gt; 1.0 : any\")\n\nnet.compile()\n\nnet.simulate_until(max_duration=1000.0. population=pop1)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmax_duration\nfloat\nthe maximum duration of the simulation in milliseconds.\nrequired\n\n\npopulation\nPopulation\nthe (list of) population whose stop_condition should be checked to stop the simulation.\nrequired\n\n\noperator\nstr\noperator to be used (‘and’ or ‘or’) when multiple populations are provided (default: ‘and’).\n'and'\n\n\nmeasure_time\nbool\ndefines whether the simulation time should be printed (default=False).\nFalse\n\n\n\n\n\n\n\nstep()\nPerforms a single simulation step (duration = dt).\n\n\n\nreset(populations=True, projections=False, monitors=True, synapses=False)\nReinitialises the network to its state before the call to compile().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npopulations\nbool\nif True (default), the neural parameters and variables will be reset to their initial value.\nTrue\n\n\nprojections\nbool\nif True, the synaptic parameters and variables (except the connections) will be reset (default=False).\nFalse\n\n\nsynapses\nbool\nif True, the synaptic weights will be erased and recreated (default=False).\nFalse\n\n\n\n\n\n\n\nenable_learning(projections=None, period=None, offset=None)\nEnables learning for all projections.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojections\nlist\nthe projections whose learning should be enabled. By default, all the existing projections are disabled.\nNone\n\n\n\n\n\n\n\ndisable_learning(projections=None)\nDisables learning for all projections.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojections\nlist\nthe projections whose learning should be disabled. By default, all the existing projections are disabled.\nNone\n\n\n\n\n\n\n\nclear()\nEmpties the network to prevent a memory leak until the garbage collector wakes up.\n\n\n\nparallel_run(\n    method,\n    number,\n    max_processes=-1,\n    seeds=None,\n    measure_time=False,\n    **kwargs,\n)\nRuns the provided method for multiple copies of the network.\nImportant: The network must have defined as a subclass of Network, not a Network instance where create() and connect() where sequentially called.\nSee the manual for more explanations:\nclass PulseNetwork(ann.Network):\n    def __init__(self):\n        self.create(...)\n\n# Simulation method\ndef simulation(net, duration=1000.):\n    net.simulate(duration)\n    t, n = net.m.raster_plot()\n    return t, n\n\n# Create a network\nnet = PulseNetwork()\nnet.compile()\n\n# Parallel simulation\nresults = net.parallel_run(method=simulation, number=4)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmethod\n\nmethod invoked for each copy of the network.\nrequired\n\n\nnumber\nint\nnumber of simulations.\nrequired\n\n\nmax_processes\nint\nmaximum number of concurrent processes. Defaults to the number of cores.\n-1\n\n\nseeds\nint | str | list[int]\nlist of seeds for each network. If None, the seeds will be be randomly set. If 'same', it will be the same as the current network. If 'sequential', the seeds will be incremented for each network (42, 43, 44, etc).\nNone\n\n\nmeasure_time\nbool\nif the total duration of the simulation should be reported at the end.\nFalse\n\n\n\n\n\n\n\ncopy(*args, **kwargs)\nReturns a new instance of the Network class, using the provided arguments to the constructor.\nBeware, Network.compile() is not called, only the instantiation of the data structures. Nothing in the constructor should induce a recompilation.\n\n\n\nconfig(*args, **kwargs)\nConfiguration of the network.\nThis method is equivalent to calling setup() at the global level, but only influences the current network. The initial configuration of the network copies the values set in setup() at the time of the creation of the network.\nIt can be called multiple times until compile() is called, new values of keys erasing older ones.\nThe only functional difference with setup() is the seed, which should be passed to the constructor of Network, otherwise any random number generation in the constructor might be unseeded. dt can also be passed to the constructor, but setting it in config() is also fine.\nIt takes various optional arguments. The most useful ones are:\n\ndt: simulation step size in milliseconds (default: 1.0).\nparadigm: parallel framework for code generation. Accepted values: “openmp” or “cuda” (default: “openmp”).\nmethod: default method to numerize the ODEs. Default is the explicit forward Euler method (‘explicit’).\nprecision: default floating precision for variables in ANNarchy. Accepted values: “float” or “double” (default: “double”)\nstructural_plasticity: allows synapses to be dynamically added/removed during the simulation (default: False).\nseed: the seed (integer) to be used in the random number generators (default = None is equivalent to time(NULL)).\nnum_threads: number of treads used by openMP (overrides the environment variable OMP_NUM_THREADS when set, default = None).\n\nFlags related to the optimization of the simulation kernels are:\n\nsparse_matrix_format: the default matrix format for projections in ANNarchy (by default: List-In-List for CPUs and Compressed Sparse Row). Note that this affects only the C++ data structures.\nsparse_matrix_storage_order: encodes whether the row in a connectivity matrix encodes pre-synaptic neurons (post_to_pre, default) or post-synaptic neurons (pre_to_post). Note that affects only the C++ data structures.\nonly_int_idx_type: if set to True (default) only signed integers are used to store pre-/post-synaptic ranks which was default until 4.7. If set to False, the index type used in a single projection is selected based on the size of the corresponding populations.\nvisible_cores: allows a fine-grained control which cores are useable for the created threads (default = [] for no limitation). It can be used to limit created openMP threads to a physical socket.\n\nThe following parameters are mainly for debugging and profiling, and should be ignored by most users:\n\nverbose: shows details about compilation process on console (by default False). Additional some information of the network construction will be shown.\nsuppress_warnings: if True, warnings (e. g. from the mathematical parser) are suppressed.\nshow_time: if True, initialization times are shown. Attention: verbose should be set to True additionally.\ndisable_shared_library_time_offset: by default False. If set to True, the shared library generated by ANNarchy will not be extended by time offset.\n\n\n\n\nload(filename, populations=True, projections=True, pickle_encoding=None)\nLoads parameters and variables from a file created with Network.save().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfilename, may contain relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be saved.\nTrue\n\n\nprojections\nbool\nif True, projection data will be saved.\nTrue\n\n\npickle_encoding\nstr\noptional parameter provided to the pickle.load() method. If set to None the default is used.\nNone\n\n\n\n\n\n\n\nsave(filename, populations=True, projections=True)\nSaves the parameters and variables of the networkin a file.\n\nIf the extension is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the extension is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nIf the extension ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nOtherwise, the data will be pickled into a simple binary text file using cPickle.\n\nWarning: The ‘.mat’ data will not be loadable by ANNarchy, it is only for external analysis purpose.\nExample:\nnet.save('results/init.npz')\n\nnet.save('results/init.data')\n\nnet.save('results/init.txt.gz')\n\nnet.save('1000_trials.mat')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfilename, may contain relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be saved.\nTrue\n\n\nprojections\nbool\nif True, projection data will be saved.\nTrue\n\n\n\n\n\n\n\nget_population(name)\nReturns the population with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the population\nrequired\n\n\n\n\n\n\n\nget_projection(name)\nReturns the projection with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the projection\nrequired\n\n\n\n\n\n\n\nget_monitor(name)\nReturns the monitor with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the monitor\nrequired\n\n\n\n\n\n\n\nget_extension(name)\nReturns the extension with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the extension\nrequired\n\n\n\n\n\n\n\nget_constant(name)\nReturns the constant with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the constant\nrequired\n\n\n\n\n\n\n\nget_populations()\nReturns a list of all declared populations in this network.\n\n\n\nget_projections()\nReturns a list of all declared projections for the current network.\n\n\n\nget_monitors()\nReturns a list of declared monitors.\n\n\n\nget_extensions()\nReturns a list of declared extensions (e.g. BOLD monitors).\n\n\n\nget_constants()\nReturns a list of declared constants.\n\n\n\ncallbacks_enabled()\nReturns True if callbacks are enabled for the network.\n\n\n\ndisable_callbacks()\nDisables all callbacks for the network.\n\n\n\nenable_callbacks()\nEnables all declared callbacks for the network.\n\n\n\nclear_all_callbacks()\nClears the list of declared callbacks for the network.\nCannot be undone!",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Network"
    ]
  },
  {
    "objectID": "reference/Network.html#parameters",
    "href": "reference/Network.html#parameters",
    "title": "Network",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndt\nfloat\nstep size in milliseconds.\nNone\n\n\nseed\nint\nseed for the random number generators.\nNone",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Network"
    ]
  },
  {
    "objectID": "reference/Network.html#attributes",
    "href": "reference/Network.html#attributes",
    "title": "Network",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndt\nStep size in milliseconds for the integration of the ODEs.\n\n\nseed\nSeed for the random number generator (Python and C++).\n\n\ncompiled\nBoolean indicating whether the network has been compiled.\n\n\ndirectory\nDirectory in which the network has been compiled.\n\n\ntime\nCurrent time t in milliseconds.\n\n\ncurrent_step\nCurrent simulation step.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Network"
    ]
  },
  {
    "objectID": "reference/Network.html#methods",
    "href": "reference/Network.html#methods",
    "title": "Network",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncreate\nAdds a population of neurons to the network.\n\n\nconnect\nConnects two populations by creating a projection.\n\n\nmonitor\nCreates a Monitor on variables of the specified object.\n\n\nboldmonitor\nMonitors the BOLD signal of several populations using a computational model.\n\n\nconstant\nAdds a constant to the network.\n\n\ncompile\nCompiles the network.\n\n\nsimulate\nRuns the network for the given duration in milliseconds.\n\n\nsimulate_until\nRuns the network for the maximal duration in milliseconds until a stop_condition is met.\n\n\nstep\nPerforms a single simulation step (duration = dt).\n\n\nreset\nReinitialises the network to its state before the call to compile().\n\n\nenable_learning\nEnables learning for all projections.\n\n\ndisable_learning\nDisables learning for all projections.\n\n\nclear\nEmpties the network to prevent a memory leak until the garbage collector wakes up.\n\n\nparallel_run\nRuns the provided method for multiple copies of the network.\n\n\ncopy\nReturns a new instance of the Network class, using the provided arguments to the constructor.\n\n\nconfig\nConfiguration of the network.\n\n\nload\nLoads parameters and variables from a file created with Network.save().\n\n\nsave\nSaves the parameters and variables of the networkin a file.\n\n\nget_population\nReturns the population with the given name.\n\n\nget_projection\nReturns the projection with the given name.\n\n\nget_monitor\nReturns the monitor with the given name.\n\n\nget_extension\nReturns the extension with the given name.\n\n\nget_constant\nReturns the constant with the given name.\n\n\nget_populations\nReturns a list of all declared populations in this network.\n\n\nget_projections\nReturns a list of all declared projections for the current network.\n\n\nget_monitors\nReturns a list of declared monitors.\n\n\nget_extensions\nReturns a list of declared extensions (e.g. BOLD monitors).\n\n\nget_constants\nReturns a list of declared constants.\n\n\ncallbacks_enabled\nReturns True if callbacks are enabled for the network.\n\n\ndisable_callbacks\nDisables all callbacks for the network.\n\n\nenable_callbacks\nEnables all declared callbacks for the network.\n\n\nclear_all_callbacks\nClears the list of declared callbacks for the network.\n\n\n\n\n\ncreate(\n    geometry,\n    neuron=None,\n    stop_condition=None,\n    name=None,\n    population=None,\n    storage_order='post_to_pre',\n)\nAdds a population of neurons to the network.\nnet = ann.Network()\npop = net.create(geometry=100, neuron=ann.Izhikevich, name=\"Excitatory population\")\nSpecific populations (e.g. PoissonPopulation()) can also be passed to the population argument, or simply as the first argument:\npop = net.create(population=ann.PoissonPopulation(100, rates=20.))\n# or\npop = net.create(ann.PoissonPopulation(100, rates=20.))\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple | int\npopulation geometry as tuple. If an integer is given, it is the size of the population.\nrequired\n\n\nneuron\nNeuron\nNeuron instance. It can be user-defined or a built-in model.\nNone\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\nstop_condition\nstr\na single condition on a neural variable which can stop the simulation whenever it is true.\nNone\n\n\npopulation\nPopulation\ninstance of a SpecificPopulation.\nNone\n\n\n\n\n\n\n\nconnect(\n    pre,\n    post=None,\n    target='',\n    synapse=None,\n    name=None,\n    projection=None,\n    disable_omp=True,\n)\nConnects two populations by creating a projection.\nnet.connect(pre=pop1, post=pop2, target=\"exc\", synapse=STDP)\nIf the synapse argument is omitted, defaults synapses without plastivity will be used (psp = \"w * pre.r\" for rate-coded projections, pre_spike=\"g_target += w\" for spiking ones.).\nSpecific projections can be passed to the projection argument, or as the first unnamed argument.\nnet.connect(projection=ann.DecodingProjection(pre, post, 'exc))\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\nstr | Population\npre-synaptic population.\nrequired\n\n\npost\nstr | Population\npost-synaptic population.\nNone\n\n\ntarget\nstr\ntype of the connection.\n''\n\n\nsynapse\nSynapse\nSynapse class or instance.\nNone\n\n\nname\nstr\n(optional) name of the Projection.\nNone\n\n\nprojection\nProjection\nspecific projection.\nNone\n\n\n\n\n\n\n\nmonitor(\n    obj,\n    variables=[],\n    period=None,\n    period_offset=None,\n    start=True,\n    name=None,\n)\nCreates a Monitor on variables of the specified object.\nThe object can be an instance of either Population, PopulationView, Dendrite or Projection.\nThe variables must be declared by the neuron or synapse type. For spiking neurons, 'spike' can be also recorded.\nm = net.monitor(pop, ['v', 'spike'])\nm = net.monitor(proj.dendrite(0), 'w')\nBy default, the monitors start recording right after Network.compile(), but you can hold them with start=False. Starting the recordings necessitates then to call the start()mehtod. Pausing/resuming the recordings is cheived through the pause() and resume().\nm.start() # Start recording\nnet.simulate(T)\nm.pause() # Pause recording\nnet.simulate(T)\nm.resume() # Resume recording\nnet.simulate(T)\n\ndata = m.get() # Get the data\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobj\nPopulation | PopulationView | Projection\nobject to monitor. Must be a Population, PopulationView, Dendrite or Projection object.\nrequired\n\n\nvariables\nlist\nsingle variable name or list of variable names to record (default: []).\n[]\n\n\nperiod\nfloat\n\nNone\n\n\nperiod_offset\nfloat\n\nNone\n\n\nstart\nbool\n\nTrue\n\n\nname\nstr\n\nNone\n\n\n\n\n\n\n\nboldmonitor(\n    populations=None,\n    bold_model=None,\n    mapping={'I_CBF': 'r'},\n    scale_factor=None,\n    normalize_input=None,\n    recorded_variables=None,\n    start=False,\n)\nMonitors the BOLD signal of several populations using a computational model.\nThe BOLD monitor transforms one or two input population variables (such as the mean firing rate) into a recordable BOLD signal according to a computational model (for example a variation of the Balloon model).\nSeveral models are available or can be created with a bold.BoldModel instance. These models must be explicitly imported:\nimport ANNarchy.extensions.bold as bold\n\nm_bold = net.boldmonitor(\n    # Recorded populations\n    populations = [pop1, pop2], \n    # BOLD model to use (default is balloon_RN)\n    bold_model = bold.balloon_RN(), \n    # Mapping from pop.r to I_CBF\n    mapping = {'I_CBF': 'r'}, \n    # Time window to compute the baseline.\n    normalize_input = 2000,  \n    # Variables to be recorded\n    recorded_variables = [\"I_CBF\", \"BOLD\"]  \n)\n\n# Unlike regular monitors, BOLD monitors must be explicitly started\nm_bold.start()\nnet.simulate(5000) \nbold_data = m_bold.get(\"BOLD\")\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npopulations\nlist\nlist of recorded populations.\nNone\n\n\nbold_model\nbold.BoldModel\ncomputational model for BOLD signal defined as a BoldModel object. Default is bold.balloon_RN.\nNone\n\n\nmapping\ndict\nmapping dictionary between the inputs of the BOLD model (I_CBF for single inputs, I_CBF and I_CMRO2 for double inputs in the provided examples) and the variables of the input populations. By default, {'I_CBF': 'r'} maps the firing rate r of the input population(s) to the variable I_CBF of the BOLD model.\n{'I_CBF': 'r'}\n\n\nscale_factor\nlist[float]\nlist of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region.\nNone\n\n\nnormalize_input\nlist[int]\nlist of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time).\nNone\n\n\nrecorded_variables\nlist[str]\nwhich variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [“BOLD”] for the provided examples).\nNone\n\n\nstart\nbool\nwhether to start recording directly.\nFalse\n\n\n\n\n\n\n\nconstant(name, value)\nAdds a constant to the network.\nc = net.constant('c', 2.0)\n\n# `c` can be used in a neuron/synapse definition\nneuron = ann.Neuron(equations=\"r = c * sum(exc)\")\n\n# Change the value of the constant in the network.\nc.set(10.0) \n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nname of the constant.\nrequired\n\n\nvalue\n\ninitial value of the constant.\nrequired\n\n\n\n\n\n\n\ncompile(\n    directory='annarchy',\n    clean=False,\n    compiler='default',\n    compiler_flags='default',\n    add_sources='',\n    extra_libs='',\n    cuda_config={'device': 0},\n    annarchy_json='',\n    silent=False,\n    debug_build=False,\n    profile_enabled=False,\n)\nCompiles the network.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndirectory\nstr\nname of the subdirectory where the code will be generated and compiled. Default: “./annarchy/”.\n'annarchy'\n\n\nclean\nbool\nboolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False).\nFalse\n\n\ncompiler\nstr\nC++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++].\n'default'\n\n\ncompiler_flags\nlist[str]\nplatform-specific flags to pass to the compiler. Default: “-march=native -O2”. Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended.\n'default'\n\n\ncuda_config\ndict\ndictionary defining the CUDA configuration for each population and projection.\n{'device': 0}\n\n\nannarchy_json\nstr\ncompiler flags etc are stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location.\n''\n\n\nsilent\nbool\ndefines if the “Compiling… OK” should be printed.\nFalse\n\n\n\n\n\n\n\nsimulate(duration, measure_time=False)\nRuns the network for the given duration in milliseconds.\nThe number of simulation steps is computed relative to the discretization step dt declared in the constructor (default: 1 ms):\nnet.simulate(1000.0)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nduration\nfloat\nthe duration in milliseconds.\nrequired\n\n\nmeasure_time\nbool\ndefines whether the simulation time should be printed.\nFalse\n\n\n\n\n\n\n\nsimulate_until(max_duration, population, operator='and', measure_time=False)\nRuns the network for the maximal duration in milliseconds until a stop_condition is met.\nWhenever the stop_condition defined in population becomes true, the simulation is stopped.\nThe method returns the actual duration of the simulation in milliseconds.\nOne can specify several populations. If the stop condition is true for any of the populations, the simulation will stop (‘or’ function).\nExample:\npop1 = net.create( ..., stop_condition = \"r &gt; 1.0 : any\")\n\nnet.compile()\n\nnet.simulate_until(max_duration=1000.0. population=pop1)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmax_duration\nfloat\nthe maximum duration of the simulation in milliseconds.\nrequired\n\n\npopulation\nPopulation\nthe (list of) population whose stop_condition should be checked to stop the simulation.\nrequired\n\n\noperator\nstr\noperator to be used (‘and’ or ‘or’) when multiple populations are provided (default: ‘and’).\n'and'\n\n\nmeasure_time\nbool\ndefines whether the simulation time should be printed (default=False).\nFalse\n\n\n\n\n\n\n\nstep()\nPerforms a single simulation step (duration = dt).\n\n\n\nreset(populations=True, projections=False, monitors=True, synapses=False)\nReinitialises the network to its state before the call to compile().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npopulations\nbool\nif True (default), the neural parameters and variables will be reset to their initial value.\nTrue\n\n\nprojections\nbool\nif True, the synaptic parameters and variables (except the connections) will be reset (default=False).\nFalse\n\n\nsynapses\nbool\nif True, the synaptic weights will be erased and recreated (default=False).\nFalse\n\n\n\n\n\n\n\nenable_learning(projections=None, period=None, offset=None)\nEnables learning for all projections.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojections\nlist\nthe projections whose learning should be enabled. By default, all the existing projections are disabled.\nNone\n\n\n\n\n\n\n\ndisable_learning(projections=None)\nDisables learning for all projections.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojections\nlist\nthe projections whose learning should be disabled. By default, all the existing projections are disabled.\nNone\n\n\n\n\n\n\n\nclear()\nEmpties the network to prevent a memory leak until the garbage collector wakes up.\n\n\n\nparallel_run(\n    method,\n    number,\n    max_processes=-1,\n    seeds=None,\n    measure_time=False,\n    **kwargs,\n)\nRuns the provided method for multiple copies of the network.\nImportant: The network must have defined as a subclass of Network, not a Network instance where create() and connect() where sequentially called.\nSee the manual for more explanations:\nclass PulseNetwork(ann.Network):\n    def __init__(self):\n        self.create(...)\n\n# Simulation method\ndef simulation(net, duration=1000.):\n    net.simulate(duration)\n    t, n = net.m.raster_plot()\n    return t, n\n\n# Create a network\nnet = PulseNetwork()\nnet.compile()\n\n# Parallel simulation\nresults = net.parallel_run(method=simulation, number=4)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmethod\n\nmethod invoked for each copy of the network.\nrequired\n\n\nnumber\nint\nnumber of simulations.\nrequired\n\n\nmax_processes\nint\nmaximum number of concurrent processes. Defaults to the number of cores.\n-1\n\n\nseeds\nint | str | list[int]\nlist of seeds for each network. If None, the seeds will be be randomly set. If 'same', it will be the same as the current network. If 'sequential', the seeds will be incremented for each network (42, 43, 44, etc).\nNone\n\n\nmeasure_time\nbool\nif the total duration of the simulation should be reported at the end.\nFalse\n\n\n\n\n\n\n\ncopy(*args, **kwargs)\nReturns a new instance of the Network class, using the provided arguments to the constructor.\nBeware, Network.compile() is not called, only the instantiation of the data structures. Nothing in the constructor should induce a recompilation.\n\n\n\nconfig(*args, **kwargs)\nConfiguration of the network.\nThis method is equivalent to calling setup() at the global level, but only influences the current network. The initial configuration of the network copies the values set in setup() at the time of the creation of the network.\nIt can be called multiple times until compile() is called, new values of keys erasing older ones.\nThe only functional difference with setup() is the seed, which should be passed to the constructor of Network, otherwise any random number generation in the constructor might be unseeded. dt can also be passed to the constructor, but setting it in config() is also fine.\nIt takes various optional arguments. The most useful ones are:\n\ndt: simulation step size in milliseconds (default: 1.0).\nparadigm: parallel framework for code generation. Accepted values: “openmp” or “cuda” (default: “openmp”).\nmethod: default method to numerize the ODEs. Default is the explicit forward Euler method (‘explicit’).\nprecision: default floating precision for variables in ANNarchy. Accepted values: “float” or “double” (default: “double”)\nstructural_plasticity: allows synapses to be dynamically added/removed during the simulation (default: False).\nseed: the seed (integer) to be used in the random number generators (default = None is equivalent to time(NULL)).\nnum_threads: number of treads used by openMP (overrides the environment variable OMP_NUM_THREADS when set, default = None).\n\nFlags related to the optimization of the simulation kernels are:\n\nsparse_matrix_format: the default matrix format for projections in ANNarchy (by default: List-In-List for CPUs and Compressed Sparse Row). Note that this affects only the C++ data structures.\nsparse_matrix_storage_order: encodes whether the row in a connectivity matrix encodes pre-synaptic neurons (post_to_pre, default) or post-synaptic neurons (pre_to_post). Note that affects only the C++ data structures.\nonly_int_idx_type: if set to True (default) only signed integers are used to store pre-/post-synaptic ranks which was default until 4.7. If set to False, the index type used in a single projection is selected based on the size of the corresponding populations.\nvisible_cores: allows a fine-grained control which cores are useable for the created threads (default = [] for no limitation). It can be used to limit created openMP threads to a physical socket.\n\nThe following parameters are mainly for debugging and profiling, and should be ignored by most users:\n\nverbose: shows details about compilation process on console (by default False). Additional some information of the network construction will be shown.\nsuppress_warnings: if True, warnings (e. g. from the mathematical parser) are suppressed.\nshow_time: if True, initialization times are shown. Attention: verbose should be set to True additionally.\ndisable_shared_library_time_offset: by default False. If set to True, the shared library generated by ANNarchy will not be extended by time offset.\n\n\n\n\nload(filename, populations=True, projections=True, pickle_encoding=None)\nLoads parameters and variables from a file created with Network.save().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfilename, may contain relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be saved.\nTrue\n\n\nprojections\nbool\nif True, projection data will be saved.\nTrue\n\n\npickle_encoding\nstr\noptional parameter provided to the pickle.load() method. If set to None the default is used.\nNone\n\n\n\n\n\n\n\nsave(filename, populations=True, projections=True)\nSaves the parameters and variables of the networkin a file.\n\nIf the extension is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the extension is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nIf the extension ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nOtherwise, the data will be pickled into a simple binary text file using cPickle.\n\nWarning: The ‘.mat’ data will not be loadable by ANNarchy, it is only for external analysis purpose.\nExample:\nnet.save('results/init.npz')\n\nnet.save('results/init.data')\n\nnet.save('results/init.txt.gz')\n\nnet.save('1000_trials.mat')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfilename, may contain relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be saved.\nTrue\n\n\nprojections\nbool\nif True, projection data will be saved.\nTrue\n\n\n\n\n\n\n\nget_population(name)\nReturns the population with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the population\nrequired\n\n\n\n\n\n\n\nget_projection(name)\nReturns the projection with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the projection\nrequired\n\n\n\n\n\n\n\nget_monitor(name)\nReturns the monitor with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the monitor\nrequired\n\n\n\n\n\n\n\nget_extension(name)\nReturns the extension with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the extension\nrequired\n\n\n\n\n\n\n\nget_constant(name)\nReturns the constant with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the constant\nrequired\n\n\n\n\n\n\n\nget_populations()\nReturns a list of all declared populations in this network.\n\n\n\nget_projections()\nReturns a list of all declared projections for the current network.\n\n\n\nget_monitors()\nReturns a list of declared monitors.\n\n\n\nget_extensions()\nReturns a list of declared extensions (e.g. BOLD monitors).\n\n\n\nget_constants()\nReturns a list of declared constants.\n\n\n\ncallbacks_enabled()\nReturns True if callbacks are enabled for the network.\n\n\n\ndisable_callbacks()\nDisables all callbacks for the network.\n\n\n\nenable_callbacks()\nEnables all declared callbacks for the network.\n\n\n\nclear_all_callbacks()\nClears the list of declared callbacks for the network.\nCannot be undone!",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Network"
    ]
  },
  {
    "objectID": "reference/Transpose.html",
    "href": "reference/Transpose.html",
    "title": "Transpose",
    "section": "",
    "text": "Transpose(self, projection, target, name=None, copied=False, net_id=0)\nTransposed projection reusing the weights of an already-defined projection.\nEven though the original projection can be learnable, this one can not. The computed post-synaptic potential is the default case for rate-coded projections: “w * pre.r”\nThe proposed target can differ from the target of the forward projection.\nExample:\nproj_ff = net.connect input, output, target=\"exc\" )\nproj_ff.all_to_all(weights=Uniform(0,1)\n\nproj_fb = net.connect(Transpose(proj_ff, target=\"inh\"))\nproj_fb.transpose()\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojection\n\noriginal projection.\nrequired\n\n\ntarget\n\ntype of the connection (can differ from the original one).\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nsave_connectivity\nNot available.\n\n\nsave\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nconnectivity_matrix\nNot available.\n\n\n\n\n\nsave_connectivity(filename)\nNot available.\n\n\n\nsave(filename)\nNot available.\n\n\n\nload(filename)\nNot available.\n\n\n\nreceptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nconnectivity_matrix(fill=0.0)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Transpose"
    ]
  },
  {
    "objectID": "reference/Transpose.html#parameters",
    "href": "reference/Transpose.html#parameters",
    "title": "Transpose",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nprojection\n\noriginal projection.\nrequired\n\n\ntarget\n\ntype of the connection (can differ from the original one).\nrequired",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Transpose"
    ]
  },
  {
    "objectID": "reference/Transpose.html#methods",
    "href": "reference/Transpose.html#methods",
    "title": "Transpose",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nsave_connectivity\nNot available.\n\n\nsave\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nconnectivity_matrix\nNot available.\n\n\n\n\n\nsave_connectivity(filename)\nNot available.\n\n\n\nsave(filename)\nNot available.\n\n\n\nload(filename)\nNot available.\n\n\n\nreceptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nconnectivity_matrix(fill=0.0)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Transpose"
    ]
  },
  {
    "objectID": "reference/Oja.html",
    "href": "reference/Oja.html",
    "title": "Oja",
    "section": "",
    "text": "Oja(self, eta=0.01, alpha=1.0)\nRate-coded synapse with regularized Hebbian plasticity (Oja).\nEquivalent code:\nOja = ann.Synapse(\n    parameters = dict(\n        eta = 0.01,\n        alpha = 1.0,\n    ),\n    equations = [\n        ann.Variable('dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w )', min=0.0),\n    ]\n)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\neta\nfloat\nlearning rate.\n0.01\n\n\nalpha\nfloat\nregularization coefficient.\n1.0",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "Oja"
    ]
  },
  {
    "objectID": "reference/Oja.html#parameters",
    "href": "reference/Oja.html#parameters",
    "title": "Oja",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\neta\nfloat\nlearning rate.\n0.01\n\n\nalpha\nfloat\nregularization coefficient.\n1.0",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "Oja"
    ]
  },
  {
    "objectID": "reference/LeakyIntegrator.html",
    "href": "reference/LeakyIntegrator.html",
    "title": "LeakyIntegrator",
    "section": "",
    "text": "LeakyIntegrator(\n    self,\n    tau=10.0,\n    B=0.0,\n    T=0.0,\n    sum='sum(exc) - sum(inh)',\n    noise=None,\n)\nLeaky-integrator rate-coded neuron, optionally noisy.\nThis simple rate-coded neuron defines an internal variable v(t) which integrates the inputs I(t) with a global time constant \\tau and a local baseline B. An additive noise N(t) can be optionally defined:\n\\tau \\cdot \\frac{dv(t)}{dt} + v(t) = I(t) + B + N(t)\nThe transfer function is the positive (or rectified linear ReLU) function with a threshold T:\nr(t) = (v(t) - T)^+\nBy default, the input I(t) to this neuron is \"sum(exc) - sum(inh)\", but this can be changed by setting the sum argument:\nneuron = ann.LeakyIntegrator(sum=\"sum(ampa)\")\nBy default, there is no additive noise, but the noise argument can be passed with a specific distribution:\nneuron = ann.LeakyIntegrator(noise=\"Normal(0.0, 1.0)\")\nEquivalent code:\nLeakyIntegrator = Neuron(\n    parameters=dict(\n        tau = 10.0,\n        B = ann.Parameter(0.0),\n        T = 0.0,\n    ), \n    equations=[\n        ann.Variable()'tau * dv/dt + v = sum(exc) - sum(inh) + B', method=exponential),\n        'r = pos(v - T)',\n    ]\n)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntau\nfloat\nTime constant (global).\n10.0\n\n\nB\nfloat\nBaseline (local).\n0.0\n\n\nT\nfloat\nThreshold (global).\n0.0\n\n\nsum\nstr\nInput sums.\n'sum(exc) - sum(inh)'",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "LeakyIntegrator"
    ]
  },
  {
    "objectID": "reference/LeakyIntegrator.html#parameters",
    "href": "reference/LeakyIntegrator.html#parameters",
    "title": "LeakyIntegrator",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntau\nfloat\nTime constant (global).\n10.0\n\n\nB\nfloat\nBaseline (local).\n0.0\n\n\nT\nfloat\nThreshold (global).\n0.0\n\n\nsum\nstr\nInput sums.\n'sum(exc) - sum(inh)'",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "LeakyIntegrator"
    ]
  },
  {
    "objectID": "reference/LogNormal.html",
    "href": "reference/LogNormal.html",
    "title": "LogNormal",
    "section": "",
    "text": "LogNormal(self, mu, sigma, min=None, max=None, rng=None)\nLog-normal distribution.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmu\nfloat\nMean of the distribution.\nrequired\n\n\nsigma\nfloat\nStandard deviation of the distribution.\nrequired\n\n\nmin\nfloat\nMinimum value (default: unlimited).\nNone\n\n\nmax\nfloat\nMaximum value (default: unlimited).\nNone\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "LogNormal"
    ]
  },
  {
    "objectID": "reference/LogNormal.html#parameters",
    "href": "reference/LogNormal.html#parameters",
    "title": "LogNormal",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmu\nfloat\nMean of the distribution.\nrequired\n\n\nsigma\nfloat\nStandard deviation of the distribution.\nrequired\n\n\nmin\nfloat\nMinimum value (default: unlimited).\nNone\n\n\nmax\nfloat\nMaximum value (default: unlimited).\nNone\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "LogNormal"
    ]
  },
  {
    "objectID": "reference/LogNormal.html#methods",
    "href": "reference/LogNormal.html#methods",
    "title": "LogNormal",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "LogNormal"
    ]
  },
  {
    "objectID": "reference/enable_learning.html",
    "href": "reference/enable_learning.html",
    "title": "enable_learning",
    "section": "",
    "text": "enable_learning(projections=None, period=None, offset=None, net_id=0)\nEnables learning for all projections. Optionally period and offset can be changed for all projections.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojections\nlist\nthe projections whose learning should be enabled. By default, all the existing projections are enabled.\nNone\n\n\nperiod\nlist\ndetermines how often the synaptic variables will be updated.\nNone\n\n\noffset\nfloat\ndetermines the offset at which the synaptic variables will be updated relative to the current time.\nNone",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "enable_learning"
    ]
  },
  {
    "objectID": "reference/enable_learning.html#parameters",
    "href": "reference/enable_learning.html#parameters",
    "title": "enable_learning",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nprojections\nlist\nthe projections whose learning should be enabled. By default, all the existing projections are enabled.\nNone\n\n\nperiod\nlist\ndetermines how often the synaptic variables will be updated.\nNone\n\n\noffset\nfloat\ndetermines the offset at which the synaptic variables will be updated relative to the current time.\nNone",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "enable_learning"
    ]
  },
  {
    "objectID": "reference/get_current_step.html",
    "href": "reference/get_current_step.html",
    "title": "get_current_step",
    "section": "",
    "text": "get_current_step\nget_current_step(net_id=0)\nReturns the current simulation step.",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "get_current_step"
    ]
  },
  {
    "objectID": "reference/IF_cond_alpha.html",
    "href": "reference/IF_cond_alpha.html",
    "title": "IF_cond_alpha",
    "section": "",
    "text": "IF_cond_alpha(\n    self,\n    v_rest=-65.0,\n    cm=1.0,\n    tau_m=20.0,\n    tau_refrac=0.0,\n    tau_syn_E=5.0,\n    tau_syn_I=5.0,\n    e_rev_E=0.0,\n    e_rev_I=-70.0,\n    v_thresh=-50.0,\n    v_reset=-65.0,\n    i_offset=0.0,\n)\nLeaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance.\nSeparate synaptic currents for excitatory and inhibitory synapses.\nThe alpha currents are calculated through a system of two linears ODEs. After a spike is received at t_spike, it peaks at t_spike + tau_syn_X, with a maximum equal to the synaptic efficiency.\nThe ODEs are solved using the exponential Euler method.\nEquivalent code:\nIF_cond_alpha = Neuron(\n    parameters = dict(\n        v_rest = ann.Parameter(-65.0),\n        cm  = ann.Parameter(1.0),\n        tau_m  = ann.Parameter(20.0),\n        tau_syn_E = ann.Parameter(5.0),\n        tau_syn_I = ann.Parameter(5.0),\n        v_thresh = ann.Parameter(-50.0),\n        v_reset = ann.Parameter(-65.0),\n        i_offset = ann.Parameter(0.0),\n    ), \n    equations = [\n        # Scaling\n        'gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)',\n        'gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)',\n\n        # Membrane potential\n        ann.Variable(\n            'cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc  * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset',\n            method='exponential', init=-65.0),\n\n        # Alpha-shaped conductance\n        ann.Variable('tau_syn_E * dg_exc/dt = - g_exc', method='exponential'),\n        ann.Variable('tau_syn_I * dg_inh/dt = - g_inh', method='exponential'),\n\n        ann.Variable('tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc', method='exponential'),\n        ann.Variable('tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh', method='exponential'),\n    ],\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = v_reset\",\n    refractory = 0.0\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv_rest\n\nResting membrane potential (mV)\n-65.0\n\n\ncm\n\nCapacity of the membrane (nF)\n1.0\n\n\ntau_m\n\nMembrane time constant (ms)\n20.0\n\n\ntau_refrac\n\nDuration of refractory period (ms)\n0.0\n\n\ntau_syn_E\n\nRise time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\n\nRise time of inhibitory synaptic current (ms)\n5.0\n\n\nv_thresh\n\nSpike threshold (mV)\n-50.0\n\n\nv_reset\n\nReset potential after a spike (mV)\n-65.0\n\n\ni_offset\n\nOffset current (nA)\n0.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_cond_alpha"
    ]
  },
  {
    "objectID": "reference/IF_cond_alpha.html#parameters",
    "href": "reference/IF_cond_alpha.html#parameters",
    "title": "IF_cond_alpha",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nv_rest\n\nResting membrane potential (mV)\n-65.0\n\n\ncm\n\nCapacity of the membrane (nF)\n1.0\n\n\ntau_m\n\nMembrane time constant (ms)\n20.0\n\n\ntau_refrac\n\nDuration of refractory period (ms)\n0.0\n\n\ntau_syn_E\n\nRise time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\n\nRise time of inhibitory synaptic current (ms)\n5.0\n\n\nv_thresh\n\nSpike threshold (mV)\n-50.0\n\n\nv_reset\n\nReset potential after a spike (mV)\n-65.0\n\n\ni_offset\n\nOffset current (nA)\n0.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_cond_alpha"
    ]
  },
  {
    "objectID": "reference/sparse_delays_from_weights.html",
    "href": "reference/sparse_delays_from_weights.html",
    "title": "sparse_delays_from_weights",
    "section": "",
    "text": "sparse_delays_from_weights(weights, delays)\nReturns a sparse delay matrix with the same connectivity as the sparse matrix weight_matrix.\nmatrix = sparse_random_matrix(pre=pop1, post=pop2, p=0.1, w=ann.Uniform(0.0, 1.0))\ndelays = sparse_delays_from_weights(matrix, ann.Uniform(5.0, 10.0))\nproj.from_sparse(matrix, delays)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nscipy.sparse.lil_matrix\nscipy sparse matrix to use for the connectivity.\nrequired\n\n\ndelays\nfloat | RandomDistribution\ndelay value (constant or random).\nrequired",
    "crumbs": [
      "Reference",
      "**Utilities**",
      "sparse_delays_from_weights"
    ]
  },
  {
    "objectID": "reference/sparse_delays_from_weights.html#parameters",
    "href": "reference/sparse_delays_from_weights.html#parameters",
    "title": "sparse_delays_from_weights",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nweights\nscipy.sparse.lil_matrix\nscipy sparse matrix to use for the connectivity.\nrequired\n\n\ndelays\nfloat | RandomDistribution\ndelay value (constant or random).\nrequired",
    "crumbs": [
      "Reference",
      "**Utilities**",
      "sparse_delays_from_weights"
    ]
  },
  {
    "objectID": "reference/magic_network.html",
    "href": "reference/magic_network.html",
    "title": "magic_network",
    "section": "",
    "text": "magic_network\nmagic_network()\nReturns the magic network of ID 0.\nThe magic network collects populations and projections created directly, like in the old (&lt;5.0) API.\npop = ann.Population(1000, ann.Izhikevich)\nproj = ann.Projection(pop, pop, 'exc)\n\nnet = magic_network()\nnet.compile()",
    "crumbs": [
      "Reference",
      "**Utilities**",
      "magic_network"
    ]
  },
  {
    "objectID": "reference/balloon_maith2021.html",
    "href": "reference/balloon_maith2021.html",
    "title": "balloon_maith2021",
    "section": "",
    "text": "balloon_maith2021\nballoon_maith2021(self)\nThe balloon model as used in Maith et al. (2021).",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_maith2021"
    ]
  },
  {
    "objectID": "reference/balloon_RN.html",
    "href": "reference/balloon_RN.html",
    "title": "balloon_RN",
    "section": "",
    "text": "balloon_RN(\n    self,\n    phi=1.0,\n    kappa=1 / 1.54,\n    gamma=1 / 2.46,\n    E_0=0.34,\n    tau=0.98,\n    alpha=0.33,\n    V_0=0.02,\n    v_0=40.3,\n    TE=40 / 1000.0,\n    epsilon=1.43,\n    r_0=25,\n)\nA balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\nEquivalent code:\nballoon_RN = BoldModel(\n    parameters = dict(\n        second    = 1000.0,\n        phi       = 1.0,\n        kappa     = 1/1.54,\n        gamma     = 1/2.46,\n        E_0       = 0.34,\n        tau       = 0.98,\n        alpha     = 0.33,\n        V_0       = 0.02,\n        v_0       = 40.3,\n        TE        = 40/1000.,\n        epsilon   = 1.43,\n        r_0       = 25.,\n    ),\n    equations = [\n        # Single input\n        ann.Variable('I_CBF = sum(I_CBF)', init=0.0),\n        ann.Variable('ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second', init=0.0),\n        ann.Variable('df_in/dt = s / second', init=1.0, min=0.01),\n\n        ann.Variable('E = 1 - (1 - E_0)**(1 / f_in)', init=0.3424),\n        ann.Variable('dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)', init=1.0, min=0.01),\n        ann.Variable('dv/dt = (f_in - f_out)/(tau*second), init=1.0, min=0.01),\n        ann.Variable('f_out = v**(1 / alpha)', init=1, min=0.01),\n\n        # Revised coefficients\n        ann.Variable('k_1 = 4.3 * v_0 * E_0 * TE'),\n        ann.Variable('k_2 = epsilon * r_0 * E_0 * TE'),\n        ann.Variable('k_3 = 1 - epsilon'),\n\n        # Non-linear equation\n        ann.Variable('BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))'),\n    ],\n    inputs=\"I_CBF\",\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43\n\n\nr_0\n\nslope of the relation between the intravascular relaxation rate and oxygen saturation\n25",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_RN"
    ]
  },
  {
    "objectID": "reference/balloon_RN.html#parameters",
    "href": "reference/balloon_RN.html#parameters",
    "title": "balloon_RN",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43\n\n\nr_0\n\nslope of the relation between the intravascular relaxation rate and oxygen saturation\n25",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_RN"
    ]
  },
  {
    "objectID": "reference/simulate.html",
    "href": "reference/simulate.html",
    "title": "simulate",
    "section": "",
    "text": "simulate(duration, measure_time=False, callbacks=True, net_id=0)\nSimulates the network for the given duration in milliseconds.\nThe number of simulation steps is computed relative to the discretization step dt declared in setup() (default: 1ms):\nann.simulate(1000.0)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nduration\nfloat\nthe duration in milliseconds.\nrequired\n\n\nmeasure_time\nbool\ndefines whether the simulation time should be printed.\nFalse\n\n\ncallbacks\nbool\ndefines if the callback methods (decorator every) should be called.\nTrue",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "simulate"
    ]
  },
  {
    "objectID": "reference/simulate.html#parameters",
    "href": "reference/simulate.html#parameters",
    "title": "simulate",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nduration\nfloat\nthe duration in milliseconds.\nrequired\n\n\nmeasure_time\nbool\ndefines whether the simulation time should be printed.\nFalse\n\n\ncallbacks\nbool\ndefines if the callback methods (decorator every) should be called.\nTrue",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "simulate"
    ]
  },
  {
    "objectID": "reference/timeit.html",
    "href": "reference/timeit.html",
    "title": "timeit",
    "section": "",
    "text": "timeit\ntimeit(func)\nDecorator to measure the execution time of a method.\n@ann.timeit\ndef run(net, T):\n    net.simulate(T)\n    return net.m.get()\n\nnet = ann.Network()\ndata = run(net, 1000)",
    "crumbs": [
      "Reference",
      "**Utilities**",
      "timeit"
    ]
  },
  {
    "objectID": "reference/SpikeSourceArray.html",
    "href": "reference/SpikeSourceArray.html",
    "title": "SpikeSourceArray",
    "section": "",
    "text": "SpikeSourceArray(self, spike_times, name=None, copied=False, net_id=0)\nSpike source generating spikes at the times given in the spike_times array.\nDepending on the initial array provided, the population will have one or several neurons, but the geometry can only be one-dimensional.\nYou can later modify the spike_times attribute of the population, but it must have the same number of neurons as the initial one.\nThe spike times are by default relative to the start of a simulation. If you call the reset() method of a SpikeSourceArray, this will set the spike times relative to the current time. You can then repeat a stimulation many times.\n# 2 neurons firing at 100Hz with a 1 ms delay\ntimes = [\n    [ 10, 20, 30, 40],\n    [ 11, 21, 31, 41]\n]\ninp = net.create(ann.SpikeSourceArray(spike_times=times))\n\nnet.compile()\n\n# Spikes at 10/11, 20/21, etc\nnet.simulate(50)\n\n# Reset the internal time of the SpikeSourceArray\ninp.reset()\n\n# Spikes at 60/61, 70/71, etc\nnet.simulate(50)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspike_times\nlist[float]\na list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt.\nrequired\n\n\nname\nstr\noptional name for the population.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "SpikeSourceArray"
    ]
  },
  {
    "objectID": "reference/SpikeSourceArray.html#parameters",
    "href": "reference/SpikeSourceArray.html#parameters",
    "title": "SpikeSourceArray",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nspike_times\nlist[float]\na list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt.\nrequired\n\n\nname\nstr\noptional name for the population.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "SpikeSourceArray"
    ]
  },
  {
    "objectID": "reference/load_parameters.html",
    "href": "reference/load_parameters.html",
    "title": "load_parameters",
    "section": "",
    "text": "load_parameters(filename, global_only=True, verbose=False, net_id=0)\nLoads the global parameters of a network (flag population for neurons, projection for synapses) from a JSON file.\nIt returns a dictionary of additional parameters not related to populations or projections (keyword network in the JSON file).\nIt is advised to generate the JSON file first with save_parameters() and later edit it manually.\nA strong restriction is that population/projection names cannot change between saving and loading. By default, they take names such as pop0 or proj2, we advise setting explicitly a name in their constructor for readability.\nIf you add a parameter name to the JSON file but it does not exist in te neuron/synapse, it will be silently skipped. Enable verbose=True to see which parameters are effectively changed.\nIf you set global_only to True, you will be able to set values for non-global parameters (e.g. synapse-specific), but a single value will be loaded for all. The JSON file cannot contain arrays.\nIf you want to save/load the value of variables after a simulation, please refer to save() or load().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\npath to the JSON file.\nrequired\n\n\nglobal_only\nbool\nTrue if only global parameters (flags population and projection) should be loaded, the other values are ignored. (default: True)\nTrue\n\n\nverbose\nbool\nTrue if the old and new values of the parameters should be printed (default: False).\nFalse",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "load_parameters"
    ]
  },
  {
    "objectID": "reference/load_parameters.html#parameters",
    "href": "reference/load_parameters.html#parameters",
    "title": "load_parameters",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\npath to the JSON file.\nrequired\n\n\nglobal_only\nbool\nTrue if only global parameters (flags population and projection) should be loaded, the other values are ignored. (default: True)\nTrue\n\n\nverbose\nbool\nTrue if the old and new values of the parameters should be printed (default: False).\nFalse",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "load_parameters"
    ]
  },
  {
    "objectID": "reference/Parameter.html",
    "href": "reference/Parameter.html",
    "title": "Parameter",
    "section": "",
    "text": "Parameter(self, value, locality='local', type='float')\nDataclass to represent a parameter in a Neuron or Synapse definition.\nneuron = ann.Neuron(\n    parameters = dict(\n\n        # Global parameter\n        tau = 10.0 # or ann.Parameter(value=10.0, locality='global')\n\n        # Local parameter\n        baseline = ann.Parameter(value=ann.Uniform(-1., 1.)),\n\n        # Boolean global parameter\n        activated = ann.Parameter(value=True, locality='global', type=bool),\n    )\n)\nIn a neuron or synapse model, parameters are global and use the float type if the Parameter class is not used.\nIf you need a local parameter (one value per neuron or synapse), the Parameter class allows to specify it. Note that you can also define global parameters by passing locality='global'.\nSemi-global synaptic parameters (one value per post-synaptic neuron) can be defined using locality='semiglobalglobal'.\nIf the parameter is an int or a bool, pass it to the type attribute.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nfloat | int | bool | RandomDistribution\nInitial value of the parameter. It can be defined as a RandomDistribution, which will be sampled with the correct shape when the population/projection is created, or a float/int/bool, depending on type.\nrequired\n\n\nlocality\nstr\nLocality of the parameter. Must be in [‘global’, ‘semiglobal’, ‘local’].\n'local'\n\n\ntype\nstr\nData type of the parameter. Must be in [float, int, bool] (or [‘float’, ‘int’, ‘bool’]).\n'float'",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Parameter"
    ]
  },
  {
    "objectID": "reference/Parameter.html#parameters",
    "href": "reference/Parameter.html#parameters",
    "title": "Parameter",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nvalue\nfloat | int | bool | RandomDistribution\nInitial value of the parameter. It can be defined as a RandomDistribution, which will be sampled with the correct shape when the population/projection is created, or a float/int/bool, depending on type.\nrequired\n\n\nlocality\nstr\nLocality of the parameter. Must be in [‘global’, ‘semiglobal’, ‘local’].\n'local'\n\n\ntype\nstr\nData type of the parameter. Must be in [float, int, bool] (or [‘float’, ‘int’, ‘bool’]).\n'float'",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Parameter"
    ]
  },
  {
    "objectID": "reference/get_time.html",
    "href": "reference/get_time.html",
    "title": "get_time",
    "section": "",
    "text": "get_time\nget_time(net_id=0)\nReturns the current time in ms.",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "get_time"
    ]
  },
  {
    "objectID": "reference/enable_callbacks.html",
    "href": "reference/enable_callbacks.html",
    "title": "enable_callbacks",
    "section": "",
    "text": "enable_callbacks\nenable_callbacks(net_id=0)\nEnables all declared callbacks for the network.",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "enable_callbacks"
    ]
  },
  {
    "objectID": "reference/TimedPoissonPopulation.html",
    "href": "reference/TimedPoissonPopulation.html",
    "title": "TimedPoissonPopulation",
    "section": "",
    "text": "TimedPoissonPopulation(\n    self,\n    geometry,\n    rates,\n    schedule,\n    period=-1.0,\n    name=None,\n    copied=False,\n    net_id=0,\n)\nPoisson population whose rate vary with the provided schedule.\nExample:\ninp = net.create(\n    TimedPoissonPopulation(\n        geometry = 100,\n        rates = [10., 20., 100., 20., 5.],\n        schedule = [0., 100., 200., 500., 600.],\n    )\n)\nThis creates a population of 100 Poisson neurons whose rate will be:\n\n10 Hz during the first 100 ms.\n20 HZ during the next 100 ms.\n100 Hz during the next 300 ms.\n20 Hz during the next 100 ms.\n5 Hz until the end of the simulation.\n\nIf you want the TimedPoissonPopulation to “loop” over the schedule, you can specify a period:\ninp = net.create(\n    TimedPoissonPopulation(\n        geometry = 100,\n        rates = [10., 20., 100., 20., 5.],\n        schedule = [0., 100., 200., 500., 600.],\n        period = 1000.,\n    )\n)\nHere the rate will become 10Hz again every 1 second of simulation. If the period is smaller than the schedule, the remaining rates will not be set.\nYou can use the reset() method to manually reinitialize the schedule, times becoming relative to that call:\nnet.simulate(1200.) # Should switch to 100 Hz due to the period of 1000.\ninp.reset()\nnet.simulate(1000.) # Starts at 10 Hz again.\nNote that the rates are reset to the value they had before compile().\nThe rates are here common to all neurons of the population. If you want each neuron to have a different rate, rates must have additional dimensions corresponding to the geometry of the population. The first dimension still corresponds to the schedule.\ninp = net.create(\n    TimedPoissonPopulation(\n        geometry = 100,\n        rates = [ \n            [10. + 0.05*i for i in range(100)], # First 100 ms\n            [20. + 0.05*i for i in range(100)], # After 100 ms\n        ],\n        schedule = [0., 100.],\n        period = 1000.,\n    )\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrates\n\narray of firing rates (list of floats or lists of numpy arrays). The first axis corresponds to the times where the firing rate should change and have the same length as schedule, if used. The other dimensions must match the geometry of the population.\nrequired\n\n\nschedule\n\nlist of times (in ms) where the firing rate should change.\nrequired\n\n\nperiod\n\ntime when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1).\n-1.0\n\n\nname\n\noptional name for the population.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "TimedPoissonPopulation"
    ]
  },
  {
    "objectID": "reference/TimedPoissonPopulation.html#parameters",
    "href": "reference/TimedPoissonPopulation.html#parameters",
    "title": "TimedPoissonPopulation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nrates\n\narray of firing rates (list of floats or lists of numpy arrays). The first axis corresponds to the times where the firing rate should change and have the same length as schedule, if used. The other dimensions must match the geometry of the population.\nrequired\n\n\nschedule\n\nlist of times (in ms) where the firing rate should change.\nrequired\n\n\nperiod\n\ntime when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1).\n-1.0\n\n\nname\n\noptional name for the population.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "TimedPoissonPopulation"
    ]
  },
  {
    "objectID": "reference/EIF_cond_exp_isfa_ista.html",
    "href": "reference/EIF_cond_exp_isfa_ista.html",
    "title": "EIF_cond_exp_isfa_ista",
    "section": "",
    "text": "EIF_cond_exp_isfa_ista(\n    self,\n    v_rest=-70.6,\n    cm=0.281,\n    tau_m=9.3667,\n    tau_refrac=0.1,\n    tau_syn_E=5.0,\n    tau_syn_I=5.0,\n    e_rev_E=0.0,\n    e_rev_I=-80.0,\n    tau_w=144.0,\n    a=4.0,\n    b=0.0805,\n    i_offset=0.0,\n    delta_T=2.0,\n    v_thresh=-50.4,\n    v_reset=-70.6,\n    v_spike=-40.0,\n)\nExponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.), decaying-exponential post-synaptic conductances.\nDefinition according to:\n\nBrette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642\n\nEquivalent code:\n\nEIF_cond_exp_isfa_ista = Neuron(\n    parameters = dict(\n        v_rest = ann.Parameter(-70.6),\n        cm = ann.Parameter(0.281), \n        tau_m = ann.Parameter(9.3667), \n        tau_syn_E = ann.Parameter(5.0),\n        tau_syn_I = ann.Parameter(5.0), \n        e_rev_E = ann.Parameter(0.0),\n        e_rev_I = ann.Parameter(-80.0),\n        tau_w = ann.Parameter(144.0), \n        a = ann.Parameter(4.0),\n        b = ann.Parameter(0.0805),\n        i_offset = ann.Parameter(0.0),\n        delta_T = ann.Parameter(2.0),\n        v_thresh = ann.Parameter(-50.4),\n        v_reset = ann.Parameter(-70.6),\n        v_spike = ann.Parameter(-40.0),\n    ), \n    equations = [\n        # Input current\n        'I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset',     \n        # Membrane potential     \n        ann.Variable('tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w)', init=-70.6),     \n        # Recovery variable \n        'tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w',       \n        # Conductances\n        ann.Variable('tau_syn_E * dg_exc/dt = - g_exc', method='exponential'),\n        ann.Variable('tau_syn_I * dg_inh/dt = - g_inh', method='exponential'),\n    ],\n    spike = \"v &gt; v_spike\",\n    reset = \"\"\"\n        v = v_reset\n        w += b\n    \"\"\",\n    refractory = 0.1\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv_rest\n\nResting membrane potential (mV)\n-70.6\n\n\ncm\n\nCapacity of the membrane (nF)\n0.281\n\n\ntau_m\n\nMembrane time constant (ms)\n9.3667\n\n\ntau_refrac\n\nDuration of refractory period (ms)\n0.1\n\n\ntau_syn_E\n\nDecay time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\n\nDecay time of inhibitory synaptic current (ms)\n5.0\n\n\ne_rev_E\n\nReversal potential for excitatory input (mV)\n0.0\n\n\ne_rev_I\n\nReversal potential for inhibitory input (mv)\n-80.0\n\n\ntau_w\n\nTime constant of the adaptation variable (ms)\n144.0\n\n\na\n\nScaling of the adaptation variable\n4.0\n\n\nb\n\nIncrement on the adaptation variable after a spike\n0.0805\n\n\ni_offset\n\nOffset current (nA)\n0.0\n\n\ndelta_T\n\nSpeed of the exponential (mV)\n2.0\n\n\nv_thresh\n\nSpike threshold for the exponential (mV)\n-50.4\n\n\nv_reset\n\nReset potential after a spike (mV)\n-70.6\n\n\nv_spike\n\nSpike threshold (mV)\n-40.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "EIF_cond_exp_isfa_ista"
    ]
  },
  {
    "objectID": "reference/EIF_cond_exp_isfa_ista.html#parameters",
    "href": "reference/EIF_cond_exp_isfa_ista.html#parameters",
    "title": "EIF_cond_exp_isfa_ista",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nv_rest\n\nResting membrane potential (mV)\n-70.6\n\n\ncm\n\nCapacity of the membrane (nF)\n0.281\n\n\ntau_m\n\nMembrane time constant (ms)\n9.3667\n\n\ntau_refrac\n\nDuration of refractory period (ms)\n0.1\n\n\ntau_syn_E\n\nDecay time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\n\nDecay time of inhibitory synaptic current (ms)\n5.0\n\n\ne_rev_E\n\nReversal potential for excitatory input (mV)\n0.0\n\n\ne_rev_I\n\nReversal potential for inhibitory input (mv)\n-80.0\n\n\ntau_w\n\nTime constant of the adaptation variable (ms)\n144.0\n\n\na\n\nScaling of the adaptation variable\n4.0\n\n\nb\n\nIncrement on the adaptation variable after a spike\n0.0805\n\n\ni_offset\n\nOffset current (nA)\n0.0\n\n\ndelta_T\n\nSpeed of the exponential (mV)\n2.0\n\n\nv_thresh\n\nSpike threshold for the exponential (mV)\n-50.4\n\n\nv_reset\n\nReset potential after a spike (mV)\n-70.6\n\n\nv_spike\n\nSpike threshold (mV)\n-40.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "EIF_cond_exp_isfa_ista"
    ]
  },
  {
    "objectID": "reference/IF_curr_exp.html",
    "href": "reference/IF_curr_exp.html",
    "title": "IF_curr_exp",
    "section": "",
    "text": "IF_curr_exp(\n    self,\n    v_rest=-65.0,\n    cm=1.0,\n    tau_m=20.0,\n    tau_refrac=0.0,\n    tau_syn_E=5.0,\n    tau_syn_I=5.0,\n    v_thresh=-50.0,\n    v_reset=-65.0,\n    i_offset=0.0,\n)\nLeaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current.\n(Separate synaptic currents for excitatory and inhibitory synapses).\nThe ODEs are solved using the exponential Euler method.\nEquivalent code:\nIF_curr_exp = Neuron(\n    parameters = dict(\n        v_rest = ann.Parameter(-65.0),\n        cm  = ann.Parameter(1.0),\n        tau_m  = ann.Parameter(20.0),\n        tau_syn_E = ann.Parameter(5.0),\n        tau_syn_I = ann.Parameter(5.0),\n        v_thresh = ann.Parameter(-50.0),\n        v_reset = ann.Parameter(-65.0),\n        i_offset = ann.Parameter(0.0),\n    ), \n    equations = [\n        ann.Variable(\n            'cm * dv/dt = cm/tau_m*(v_rest -v)   + g_exc - g_inh + i_offset',\n            method='exponential', init=-65.0\n        ),\n        ann.Variable('tau_syn_E * dg_exc/dt = - g_exc', method='exponential'),\n        ann.Variable('tau_syn_I * dg_inh/dt = - g_inh', method='exponential'),\n    ],\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = v_reset\",\n    refractory = 0.0\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv_rest\nfloat\nResting membrane potential (mV)\n-65.0\n\n\ncm\nfloat\nCapacity of the membrane (nF)\n1.0\n\n\ntau_m\nfloat\nMembrane time constant (ms)\n20.0\n\n\ntau_refrac\nfloat\nDuration of refractory period (ms)\n0.0\n\n\ntau_syn_E\nfloat\nDecay time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\nfloat\nDecay time of inhibitory synaptic current (ms)\n5.0\n\n\nv_thresh\nfloat\nSpike threshold (mV)\n-50.0\n\n\nv_reset\nfloat\nReset potential after a spike (mV)\n-65.0\n\n\ni_offset\nfloat\nOffset current (nA)\n0.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_curr_exp"
    ]
  },
  {
    "objectID": "reference/IF_curr_exp.html#parameters",
    "href": "reference/IF_curr_exp.html#parameters",
    "title": "IF_curr_exp",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nv_rest\nfloat\nResting membrane potential (mV)\n-65.0\n\n\ncm\nfloat\nCapacity of the membrane (nF)\n1.0\n\n\ntau_m\nfloat\nMembrane time constant (ms)\n20.0\n\n\ntau_refrac\nfloat\nDuration of refractory period (ms)\n0.0\n\n\ntau_syn_E\nfloat\nDecay time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\nfloat\nDecay time of inhibitory synaptic current (ms)\n5.0\n\n\nv_thresh\nfloat\nSpike threshold (mV)\n-50.0\n\n\nv_reset\nfloat\nReset potential after a spike (mV)\n-65.0\n\n\ni_offset\nfloat\nOffset current (nA)\n0.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_curr_exp"
    ]
  },
  {
    "objectID": "reference/balloon_RL.html",
    "href": "reference/balloon_RL.html",
    "title": "balloon_RL",
    "section": "",
    "text": "balloon_RL(\n    self,\n    phi=1.0,\n    kappa=1 / 1.54,\n    gamma=1 / 2.46,\n    E_0=0.34,\n    tau=0.98,\n    alpha=0.33,\n    V_0=0.02,\n    v_0=40.3,\n    TE=40 / 1000.0,\n    epsilon=1.43,\n    r_0=25,\n)\nA balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007).\nEquivalent code:\nballoon_RL = BoldModel(\n    parameters = dict(\n        second    = 1000.0,\n        phi       = 1.0,\n        kappa     = 1/1.54,\n        gamma     = 1/2.46,\n        E_0       = 0.34,\n        tau       = 0.98,\n        alpha     = 0.33,\n        V_0       = 0.02,\n        v_0       = 40.3,\n        TE        = 40/1000.,\n        epsilon   = 1.43,\n        r_0       = 25.,\n    ),\n    equations = [\n        # Single input\n        ann.Variable('I_CBF = sum(I_CBF)', init=0.0),\n        ann.Variable('ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second', init=0.0),\n        ann.Variable('df_in/dt = s / second', init=1.0, min=0.01),\n\n        ann.Variable('E = 1 - (1 - E_0)**(1 / f_in)', init=0.3424),\n        ann.Variable('dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)', init=1.0, min=0.01),\n        ann.Variable('dv/dt = (f_in - f_out)/(tau*second), init=1.0, min=0.01),\n        ann.Variable('f_out = v**(1 / alpha)', init=1, min=0.01),\n\n        # Revised coefficients\n        ann.Variable('k_1 = 4.3 * v_0 * E_0 * TE'),\n        ann.Variable('k_2 = epsilon * r_0 * E_0 * TE'),\n        ann.Variable('k_3 = 1 - epsilon'),\n\n        # Linear equation\n        ann.Variable('BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v))'),\n    ],\n    inputs=\"I_CBF\",\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43\n\n\nr_0\n\nslope of the relation between the intravascular relaxation rate and oxygen saturation\n25",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_RL"
    ]
  },
  {
    "objectID": "reference/balloon_RL.html#parameters",
    "href": "reference/balloon_RL.html#parameters",
    "title": "balloon_RL",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43\n\n\nr_0\n\nslope of the relation between the intravascular relaxation rate and oxygen saturation\n25",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_RL"
    ]
  },
  {
    "objectID": "reference/BoldModel.html",
    "href": "reference/BoldModel.html",
    "title": "BoldModel",
    "section": "",
    "text": "BoldModel(\n    self,\n    parameters,\n    equations,\n    inputs,\n    output=['BOLD'],\n    name='Custom BOLD model',\n    description='',\n)\nBase class to define a BOLD model to be used in a BOLD monitor.\nA BOLD model is quite similar to a regular rate-coded neuron. It gets a weighted sum of inputs with a specific target (e.g. I_CBF) and compute a single output variable (called BOLD in the predefined models, but it could be anything).\nThe main difference is that a BOLD model should also declare which targets are used for the input signal:\nbold_model = BoldModel(\n    parameters = dict(\n        tau = 1000.\n    ),\n    equations = [\n        'I_CBF = sum(I_CBF)',\n        # ...\n        'tau * dBOLD/dt = I_CBF - BOLD',\n    ],\n    inputs = \"I_CBF\"\n)\nThe provided BOLD models follow the Balloon model (Buxton et al., 1998) with the different variations studied in (Stephan et al., 2007). Those models all compute the vascular response to neural activity through a dampened oscillator:\n\n    \\frac{ds}{dt} = \\phi \\, I_\\text{CBF} - \\kappa \\, s - \\gamma \\, (f_{in} - 1)\n\n\n    \\frac{df_{in}}{dt} = s\n\nThis allows to compute the oxygen extraction fraction:\n\n    E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} }\n\nThe (normalized) venous blood volume is computed as:\n\n    \\tau_0 \\, \\frac{dv}{dt} = (f_{in} - f_{out})\n\n\n    f_{out} = v^{\\frac{1}{\\alpha}}\n\nThe level of deoxyhemoglobin into the venous compartment is computed by:\n\n    \\tau_0 \\, \\frac{dq}{dt} = f_{in} \\, \\frac{E}{E_0} - \\frac{q}{v} \\, f_{out}\n\nUsing the two signals v and q, there are two ways to compute the corresponding BOLD signal:\n\nN: Non-linear BOLD equation:\n\n\n    BOLD = v_0 \\, ( k_1 \\, (1-q) + k_2 \\, (1- \\dfrac{q}{v}) + k_3 \\, (1 - v) )\n\n\nL: Linear BOLD equation:\n\n\n    BOLD = v_0 \\, ((k_1 + k_2) \\, (1 - q) + (k_3 - k_2) \\, (1 - v))\n\nAdditionally, the three coefficients k_1, k_2, k_3 can be computed in two different ways:\n\nC: classical coefficients from (Buxton et al., 1998):\n\nk_1            = (1 - v_0) \\, 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\nk_2            = 2 \\, E_0\nk_3            = 1 - \\epsilon\n\nR: revised coefficients from (Obata et al., 2004):\n\nk_1            = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\nk_2            = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\nk_3            = 1 - \\epsilon\nThis makes a total of four different BOLD model (balloon_RN, balloon_RL, balloon_CN, balloon_CL) which are provided by the extension. The different parameters can be modified in the constructor. Additionally, we also provide the model that was used in (Maith et al., 2021) and the two-inputs model of (Maith et al, 2022).\n\nBuxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855-864. doi:10.1002/mrm.1910390602\n\n\nFriston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466-477\n\n\nBuxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, 220-233. doi:10.1016/j.neuroimage.2004.07.013\n\n\nStephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387-401. doi:10.1016/j.neuroimage.2007.07.040\n\n\nMaith et al. (2021) A computational model-based analysis of basal ganglia pathway changes in Parkinson’s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278-2295. doi:10.1111/ejn.14868\n\n\nMaith et al. (2022) BOLD monitoring in the neural simulator ANNarchy. Frontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameters\n\nparameters of the model and their initial value.\nrequired\n\n\nequations\n\nequations defining the temporal evolution of variables.\nrequired\n\n\ninputs\n\nsingle variable or list of input signals (e.g. ‘I_CBF’ or [‘I_CBF’, ‘I_CMRO2’]).\nrequired\n\n\noutput\n\noutput variable of the model (default is ‘BOLD’).\n['BOLD']\n\n\nname\n\noptional model name.\n'Custom BOLD model'\n\n\ndescription\n\noptional model description.\n''",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "BoldModel"
    ]
  },
  {
    "objectID": "reference/BoldModel.html#parameters",
    "href": "reference/BoldModel.html#parameters",
    "title": "BoldModel",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nparameters\n\nparameters of the model and their initial value.\nrequired\n\n\nequations\n\nequations defining the temporal evolution of variables.\nrequired\n\n\ninputs\n\nsingle variable or list of input signals (e.g. ‘I_CBF’ or [‘I_CBF’, ‘I_CMRO2’]).\nrequired\n\n\noutput\n\noutput variable of the model (default is ‘BOLD’).\n['BOLD']\n\n\nname\n\noptional model name.\n'Custom BOLD model'\n\n\ndescription\n\noptional model description.\n''",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "BoldModel"
    ]
  },
  {
    "objectID": "reference/EIF_cond_alpha_isfa_ista.html",
    "href": "reference/EIF_cond_alpha_isfa_ista.html",
    "title": "EIF_cond_alpha_isfa_ista",
    "section": "",
    "text": "EIF_cond_alpha_isfa_ista(\n    self,\n    v_rest=-70.6,\n    cm=0.281,\n    tau_m=9.3667,\n    tau_refrac=0.1,\n    tau_syn_E=5.0,\n    tau_syn_I=5.0,\n    e_rev_E=0.0,\n    e_rev_I=-80.0,\n    tau_w=144.0,\n    a=4.0,\n    b=0.0805,\n    i_offset=0.0,\n    delta_T=2.0,\n    v_thresh=-50.4,\n    v_reset=-70.6,\n    v_spike=-40.0,\n)\nExponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.), alpha post-synaptic conductances.\nDefinition according to:\n\nBrette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642\n\nEquivalent code:\n\nEIF_cond_alpha_isfa_ista = Neuron(\n    parameters = dict(\n        v_rest = ann.Parameter(-70.6),\n        cm = ann.Parameter(0.281), \n        tau_m = ann.Parameter(9.3667), \n        tau_syn_E = ann.Parameter(5.0),\n        tau_syn_I = ann.Parameter(5.0), \n        e_rev_E = ann.Parameter(0.0),\n        e_rev_I = ann.Parameter(-80.0),\n        tau_w = ann.Parameter(144.0), \n        a = ann.Parameter(4.0),\n        b = ann.Parameter(0.0805),\n        i_offset = ann.Parameter(0.0),\n        delta_T = ann.Parameter(2.0),\n        v_thresh = ann.Parameter(-50.4),\n        v_reset = ann.Parameter(-70.6),\n        v_spike = ann.Parameter(-40.0),\n    ), \n    equations = [\n        # Scaling\n        'gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)',\n        'gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)',\n\n        # Input current\n        'I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset',    \n\n        # Membrane potential     \n        ann.Variable('tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w)', init=-70.6),   \n\n        # Recovery variable \n        'tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w',    \n\n        # Alpha-shaped conductance\n        ann.Variable('tau_syn_E * dg_exc/dt = - g_exc', method='exponential'),\n        ann.Variable('tau_syn_I * dg_inh/dt = - g_inh', method='exponential'),\n\n        ann.Variable('tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc', method='exponential'),\n        ann.Variable('tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh', method='exponential'),\n    ],\n    spike = \"v &gt; v_spike\",\n    reset = \"\"\"\n        v = v_reset\n        w += b\n    \"\"\",\n    refractory = 0.1\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv_rest\n\nResting membrane potential (mV)\n-70.6\n\n\ncm\n\nCapacity of the membrane (nF)\n0.281\n\n\ntau_m\n\nMembrane time constant (ms)\n9.3667\n\n\ntau_refrac\n\nDuration of refractory period (ms)\n0.1\n\n\ntau_syn_E\n\nDecay time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\n\nDecay time of inhibitory synaptic current (ms)\n5.0\n\n\ne_rev_E\n\nReversal potential for excitatory input (mV)\n0.0\n\n\ne_rev_I\n\nReversal potential for inhibitory input (mv)\n-80.0\n\n\ntau_w\n\nTime constant of the adaptation variable (ms)\n144.0\n\n\na\n\nScaling of the adaptation variable\n4.0\n\n\nb\n\nIncrement on the adaptation variable after a spike\n0.0805\n\n\ni_offset\n\nOffset current (nA)\n0.0\n\n\ndelta_T\n\nSpeed of the exponential (mV)\n2.0\n\n\nv_thresh\n\nSpike threshold for the exponential (mV)\n-50.4\n\n\nv_reset\n\nReset potential after a spike (mV)\n-70.6\n\n\nv_spike\n\nSpike threshold (mV)\n-40.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "EIF_cond_alpha_isfa_ista"
    ]
  },
  {
    "objectID": "reference/EIF_cond_alpha_isfa_ista.html#parameters",
    "href": "reference/EIF_cond_alpha_isfa_ista.html#parameters",
    "title": "EIF_cond_alpha_isfa_ista",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nv_rest\n\nResting membrane potential (mV)\n-70.6\n\n\ncm\n\nCapacity of the membrane (nF)\n0.281\n\n\ntau_m\n\nMembrane time constant (ms)\n9.3667\n\n\ntau_refrac\n\nDuration of refractory period (ms)\n0.1\n\n\ntau_syn_E\n\nDecay time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\n\nDecay time of inhibitory synaptic current (ms)\n5.0\n\n\ne_rev_E\n\nReversal potential for excitatory input (mV)\n0.0\n\n\ne_rev_I\n\nReversal potential for inhibitory input (mv)\n-80.0\n\n\ntau_w\n\nTime constant of the adaptation variable (ms)\n144.0\n\n\na\n\nScaling of the adaptation variable\n4.0\n\n\nb\n\nIncrement on the adaptation variable after a spike\n0.0805\n\n\ni_offset\n\nOffset current (nA)\n0.0\n\n\ndelta_T\n\nSpeed of the exponential (mV)\n2.0\n\n\nv_thresh\n\nSpike threshold for the exponential (mV)\n-50.4\n\n\nv_reset\n\nReset potential after a spike (mV)\n-70.6\n\n\nv_spike\n\nSpike threshold (mV)\n-40.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "EIF_cond_alpha_isfa_ista"
    ]
  },
  {
    "objectID": "reference/set_time.html",
    "href": "reference/set_time.html",
    "title": "set_time",
    "section": "",
    "text": "set_time\nset_time(t, net_id=0)\nSets the current time in ms.\nWarning: can be dangerous for some spiking models.",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "set_time"
    ]
  },
  {
    "objectID": "reference/setup.html",
    "href": "reference/setup.html",
    "title": "setup",
    "section": "",
    "text": "setup\nsetup(**keyValueArgs)\nThe setup function is used to configure ANNarchy simulations.\nIt takes various optional arguments. The most useful ones are:\n\ndt: simulation step size in milliseconds (default: 1.0).\nparadigm: parallel framework for code generation. Accepted values: “openmp” or “cuda” (default: “openmp”).\nmethod: default method to numerize ODEs. Default is the explicit forward Euler method (‘explicit’).\nprecision: default floating precision for variables in ANNarchy. Accepted values: “float” or “double” (default: “double”)\nstructural_plasticity: allows synapses to be dynamically added/removed during the simulation (default: False).\nseed: the seed (integer) to be used in the random number generators (default = None is equivalent to time(NULL)).\nnum_threads: number of treads used by openMP (overrides the environment variable OMP_NUM_THREADS when set, default = None).\n\nFlags related to the optimization of the simulation kernels are:\n\nsparse_matrix_format: the default matrix format for projections in ANNarchy (by default: List-In-List for CPUs and Compressed Sparse Row). Note that this affects only the C++ data structures.\nsparse_matrix_storage_order: encodes whether the row in a connectivity matrix encodes pre-synaptic neurons (post_to_pre, default) or post-synaptic neurons (pre_to_post). Note that affects only the C++ data structures.\nonly_int_idx_type: if set to True (default) only signed integers are used to store pre-/post-synaptic ranks which was default until 4.7.If set to False, the index type used in a single projection is selected based on the size of the corresponding populations.\nvisible_cores: allows a fine-grained control which cores are useable for the created threads (default = [] for no limitation). It can be used to limit created openMP threads to a physical socket.\n\nThe following parameters are mainly for debugging and profiling, and should be ignored by most users:\n\nverbose: shows details about compilation process on console (by default False). Additional some information of the network construction will be shown.\nsuppress_warnings: if True, warnings (e. g. from the mathematical parser) are suppressed.\nshow_time: if True, initialization times are shown. Attention: verbose should be set to True additionally.\ndisable_shared_library_time_offset: by default False. If set to True, the shared library generated by ANNarchy will not be extended by time offset.\n\nNote:\nThis function should be used before any other functions of ANNarchy (including importing a network definition), right after import ANNarchy:\nimport ANNarchy as ann\nann.setup(dt=1.0, method='midpoint', num_threads=2)",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "setup"
    ]
  },
  {
    "objectID": "reference/Hebb.html",
    "href": "reference/Hebb.html",
    "title": "Hebb",
    "section": "",
    "text": "Hebb(self, eta=0.01)\nRate-coded synapse with Hebbian plasticity.\nEquivalent code:\nHebb = ann.Synapse(\n    parameters = dict(\n        eta = 0.01,\n    ),\n    equations = [\n        ann.Variable('dw/dt = eta * pre.r * post.r', min=0.0),\n    ]\n)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\neta\n\nlearning rate.\n0.01",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "Hebb"
    ]
  },
  {
    "objectID": "reference/Hebb.html#parameters",
    "href": "reference/Hebb.html#parameters",
    "title": "Hebb",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\neta\n\nlearning rate.\n0.01",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "Hebb"
    ]
  },
  {
    "objectID": "reference/Uniform.html",
    "href": "reference/Uniform.html",
    "title": "Uniform",
    "section": "",
    "text": "Uniform(self, min, max, rng=None)\nUniform distribution between min and max.\nThe returned values are floats in the range [min, max].\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmin\nfloat\nminimum value.\nrequired\n\n\nmax\nfloat\nmaximum value.\nrequired\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Uniform"
    ]
  },
  {
    "objectID": "reference/Uniform.html#parameters",
    "href": "reference/Uniform.html#parameters",
    "title": "Uniform",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmin\nfloat\nminimum value.\nrequired\n\n\nmax\nfloat\nmaximum value.\nrequired\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Uniform"
    ]
  },
  {
    "objectID": "reference/Uniform.html#methods",
    "href": "reference/Uniform.html#methods",
    "title": "Uniform",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Uniform"
    ]
  },
  {
    "objectID": "reference/Projection.html",
    "href": "reference/Projection.html",
    "title": "Projection",
    "section": "",
    "text": "Projection(\n    self,\n    pre,\n    post,\n    target,\n    synapse=None,\n    name=None,\n    disable_omp=True,\n    copied=False,\n    net_id=0,\n)\nProjection between two populations.\nThe object is returned by Network.connect() and should not be created directly:\nproj = net.connect(pre=pop1, post=pop2, target=\"exc\", synapse=STDP)\nThe projection still has to be instantiated, by calling a connector method such as all_to_all() or fixed_probability().\nIf not specified, the default synapse only ensures linear synaptic transmission:\n\nFor rate-coded populations: psp = w * pre.r\nFor spiking populations: g_target += w\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\nstr | Population\nPre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\nstr | Population\nPost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\nstr\nType of the connection.\nrequired\n\n\nsynapse\nSynapse\nA Synapse instance.\nNone\n\n\nname\nstr\nUnique name of the projection (optional, it defaults to proj0, proj1, etc).\nNone\n\n\ndisable_omp\nbool\nEspecially for small- and mid-scale sparse spiking networks, the parallelization of spike propagation is not scalable and disabled by default. It can be re-enabled by setting this parameter to False.\nTrue\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nparameters\nList of parameter names.\n\n\nvariables\nList of variable names.\n\n\nattributes\nList of attribute names (parameters + variables).\n\n\nfunctions\nList of functions defined by the synapse model.\n\n\nsize\nNumber of post-synaptic neurons receiving synapses.\n\n\nnb_synapses\nTotal number of synapses in the projection.\n\n\nnb_synapses_per_dendrite\nTotal number of synapses for each dendrite as a list.\n\n\npost_ranks\nList of ranks of post-synaptic neurons that receive connections. Read-only.\n\n\npre_ranks\nList of lists of pre-synaptic ranks, for each post-synaptic neuron. Read-only.\n\n\ndendrites\nIteratively returns the dendrites corresponding to this projection.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nall_to_all\nall-to-all (fully-connected) connection pattern.\n\n\nfixed_probability\nProbabilistic sparse connection pattern.\n\n\none_to_one\none-to-one connection pattern.\n\n\nfixed_number_pre\nConnection pattern where each post-synaptic neuron receives a fixed number of pre-synaptic neurons.\n\n\nfixed_number_post\nEach pre-synaptic neuron randomly sends a fixed number of connections to the post-synaptic neurons.\n\n\ngaussian\nGaussian connection pattern.\n\n\ndog\nDifference-Of-Gaussians connection pattern.\n\n\nfrom_function\nConnection pattern based on a user-defined function.\n\n\nfrom_matrix\nBuilds a connection pattern according to a dense connectivity matrix.\n\n\nfrom_matrix_market\nLoads a weight matrix encoded in the Matrix Market format. This connector is intended for benchmarking purposes.\n\n\nfrom_sparse\nBuilds a connectivity pattern using a Scipy sparse matrix for the weights and (optionally) delays.\n\n\nfrom_file\nBuilds the connectivity matrix using data saved using Projection.save_connectivity() (not save()!).\n\n\nreset\nResets all parameters and variables of the projection to their initial value (before the call to compile()).\n\n\ndendrite\nReturns the dendrite of a postsynaptic neuron according to its rank.\n\n\nsynapse\nReturns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise.\n\n\nget\nReturns a list of parameters/variables values for each dendrite in the projection.\n\n\nset\nSets the parameters/variables values for each dendrite in the projection.\n\n\nenable_learning\nEnables learning for all the synapses of this projection.\n\n\ndisable_learning\nDisables learning for all synapses of this projection.\n\n\nsave_connectivity\nSaves the connectivity of the projection into a file.\n\n\nconnectivity_matrix\nReturns a dense connectivity matrix (2D Numpy array) representing the connectivity matrix between the pre- and post-populations.\n\n\nreceptive_fields\nGathers all receptive fields within this projection.\n\n\nsave\nSaves all information about the projection (connectivity, current value of parameters and variables) into a file.\n\n\nload\nLoads the saved state of the projection by Projection.save().\n\n\nstart_pruning\nStarts pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n\n\nstop_pruning\nStops pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n\n\nstart_creating\nStarts creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n\n\nstop_creating\nStops creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n\n\nupdate_launch_config\nAllows the adjustment of the CUDA launch config (since 4.7.2).\n\n\n\n\n\nall_to_all(\n    self,\n    weights,\n    delays=0.0,\n    allow_self_connections=False,\n    force_multiple_weights=False,\n    storage_format=None,\n    storage_order=None,\n)\nall-to-all (fully-connected) connection pattern.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nfloat | RandomDistribution\nSynaptic values, either a single value or a random distribution object.\nrequired\n\n\ndelays\nfloat | RandomDistribution\nSynaptic delays, either a single value or a random distribution object (default=dt).\n0.0\n\n\nallow_self_connections\nbool\nIf True, self-connections between a neuron and itself are allowed (default = False if the pre- and post-populations are identical, True otherwise).\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\nfixed_probability(\n    self,\n    probability,\n    weights,\n    delays=0.0,\n    allow_self_connections=False,\n    force_multiple_weights=False,\n    storage_format=None,\n    storage_order=None,\n)\nProbabilistic sparse connection pattern.\nEach neuron in the postsynaptic population is connected to neurons of the presynaptic population with the given probability. Self-connections are avoided by default.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprobability\nfloat\nProbability that a synapse is created.\nrequired\n\n\nweights\nfloat | RandomDistribution\nEither a single value for all synapses or a RandomDistribution object.\nrequired\n\n\ndelays\nfloat | RandomDistribution\nEither a single value for all synapses or a RandomDistribution object (default = dt)\n0.0\n\n\nallow_self_connections\nbool\nDefines if self-connections are allowed (default=False).\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\none_to_one(\n    self,\n    weights=1.0,\n    delays=0.0,\n    force_multiple_weights=False,\n    storage_format=None,\n    storage_order=None,\n)\none-to-one connection pattern.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nfloat | RandomDistribution\nInitial synaptic values, either a single value (float) or a random distribution object.\n1.0\n\n\ndelays\nfloat | RandomDistribution\nSynaptic delays, either a single value or a random distribution object (default=dt).\n0.0\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\nfixed_number_pre(\n    self,\n    number,\n    weights,\n    delays=0.0,\n    allow_self_connections=False,\n    force_multiple_weights=False,\n    storage_format=None,\n    storage_order=None,\n)\nConnection pattern where each post-synaptic neuron receives a fixed number of pre-synaptic neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnumber\nint\nNumber of synapses per postsynaptic neuron.\nrequired\n\n\nweights\nfloat | RandomDistribution\nEither a single value for all synapses or a RandomDistribution object.\nrequired\n\n\ndelays\nfloat | RandomDistribution\nEither a single value for all synapses or a RandomDistribution object (default = dt)\n0.0\n\n\nallow_self_connections\nbool\nDefines if self-connections are allowed (default=False).\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\nfixed_number_post(\n    self,\n    number,\n    weights=1.0,\n    delays=0.0,\n    allow_self_connections=False,\n    force_multiple_weights=False,\n    storage_format=None,\n    storage_order=None,\n)\nEach pre-synaptic neuron randomly sends a fixed number of connections to the post-synaptic neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnumber\nint\nNumber of synapses per pre-synaptic neuron.\nrequired\n\n\nweights\nfloat | RandomDistribution\nEither a single value for all synapses or a RandomDistribution object.\n1.0\n\n\ndelays\nfloat | RandomDistribution\nEither a single value for all synapses or a RandomDistribution object (default = dt)\n0.0\n\n\nallow_self_connections\nbool\nDefines if self-connections are allowed (default=False)\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\ngaussian(\n    self,\n    amp,\n    sigma,\n    delays=0.0,\n    limit=0.01,\n    allow_self_connections=False,\n    storage_format=None,\n)\nGaussian connection pattern.\nEach neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Gaussian profile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\namp\nfloat\nAmplitude of the Gaussian function\nrequired\n\n\nsigma\nfloat\nWidth of the Gaussian function\nrequired\n\n\ndelays\nfloat | RandomDistribution\nSynaptic delay, either a single value or a random distribution object (default=dt).\n0.0\n\n\nlimit\nfloat\nProportion of amp below which synapses are not created\n0.01\n\n\nallow_self_connections\nbool\nAllows connections between a neuron and itself.\nFalse\n\n\n\n\n\n\n\ndog(\n    self,\n    amp_pos,\n    sigma_pos,\n    amp_neg,\n    sigma_neg,\n    delays=0.0,\n    limit=0.01,\n    allow_self_connections=False,\n    storage_format=None,\n)\nDifference-Of-Gaussians connection pattern.\nEach neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Difference-Of-Gaussians profile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\namp_pos\nfloat\nAmplitude of the positive Gaussian function\nrequired\n\n\nsigma_pos\nfloat\nWidth of the positive Gaussian function\nrequired\n\n\namp_neg\nfloat\nAmplitude of the negative Gaussian function\nrequired\n\n\nsigma_neg\nfloat\nWidth of the negative Gaussian function\nrequired\n\n\ndelays\nfloat | RandomDistribution\nSynaptic delay, either a single value or a random distribution object (default=dt).\n0.0\n\n\nlimit\nfloat\nProportion of amp below which synapses are not created (default: 0.01)\n0.01\n\n\nallow_self_connections\nbool\nAllows connections between a neuron and itself.\nFalse\n\n\n\n\n\n\n\nfrom_function(self, method, storage_format=None, storage_order=None, **args)\nConnection pattern based on a user-defined function.\nThe two first arguments of the function must be the pre and post populations. Additional arguments can be passed at creation time.\nThe function must return a ann.LILConnectivity object.\nExample:\ndef probabilistic_pattern(pre, post, weight, probability):\n    # Create a LIL structure for the connectivity matrix\n    synapses = ann.LILConnectivity()\n    # For all neurons in the post-synaptic population\n    for post_rank in xrange(post.size):\n        # Decide which pre-synaptic neurons should form synapses\n        ranks = []\n        for pre_rank in xrange(pre.size):\n            if random.random() &lt; probability:\n                ranks.append(pre_rank)\n        # Create weights and delays arrays of the same size\n        values = [weight for i in xrange(len(ranks)) ]\n        delays = [0 for i in xrange(len(ranks)) ]\n        # Add this information to the LIL matrix\n        synapses.add(post_rank, ranks, values, delays)\n\n    return synapses\n\nproj = ann.Projection(pop1, pop2, target = 'inh')\nproj.connect_with_func(\n    method=probabilistic_pattern, \n    weight=1.0, \n    probability=0.3\n) \n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmethod\n\nMethod to call. The method must return a LILConnectivity object.\nrequired\n\n\nargs\n\nList of additional arguments needed by the function.\n{}\n\n\n\n\n\n\n\nfrom_matrix(\n    self,\n    weights,\n    delays=0.0,\n    pre_post=False,\n    storage_format=None,\n    storage_order=None,\n)\nBuilds a connection pattern according to a dense connectivity matrix.\nThe matrix must be N*M, where N is the number of neurons in the post-synaptic population and M in the pre-synaptic one. Lists of lists must have the same size.\nIf a synapse should not be created, the weight value should be None.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nnp.array\nNumpy array (or list of lists of equal size) representing the weights. If a value is None, the corresponding synapse will not be created.\nrequired\n\n\ndelays\n\nNumpy array representing the delays. Must represent the same synapses as the weights argument. If omitted, the delays are considered 0.\n0.0\n\n\npre_post\n\nStates which index is first. By default, the first dimension is related to the post-synaptic population. If pre_post is True, the first dimension is the pre-synaptic population.\nFalse\n\n\n\n\n\n\n\nfrom_matrix_market(self, filename, storage_format=None, storage_order=None)\nLoads a weight matrix encoded in the Matrix Market format. This connector is intended for benchmarking purposes.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nFilename of the Matrix Market (.mtx) file.\nrequired\n\n\n\n\n\n\n\nfrom_sparse(self, weights, delays=0.0, storage_format=None, storage_order=None)\nBuilds a connectivity pattern using a Scipy sparse matrix for the weights and (optionally) delays.\nWarning: a sparse matrix has pre-synaptic ranks as first dimension.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nscipy.sparse.lil_matrix\na sparse lil_matrix object created from scipy.\nrequired\n\n\ndelays\nint | float\nthe value of the constant delay (default: dt). Variable delays are not allowed.\n0.0\n\n\n\n\n\n\n\nfrom_file(\n    self,\n    filename,\n    pickle_encoding=None,\n    storage_format=None,\n    storage_order=None,\n)\nBuilds the connectivity matrix using data saved using Projection.save_connectivity() (not save()!).\nAdmissible file formats are compressed Numpy files (.npz), gunzipped binary text files (.gz) or binary text files.\nNote: Only the ranks, weights and delays are loaded, not the other variables.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfile where the connections were saved.\nrequired\n\n\n\n\n\n\n\nreset(attributes=-1, synapses=False)\nResets all parameters and variables of the projection to their initial value (before the call to compile()).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattributes\n\nlist of attributes (parameter or variable) which should be reinitialized. Default: all attributes (-1).\n-1\n\n\nsynapses\n\ndefines if the weights and delays should also be recreated. Default: False\nFalse\n\n\n\n\n\n\n\ndendrite(post)\nReturns the dendrite of a postsynaptic neuron according to its rank.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npost\nint\ncan be either the rank or the coordinates of the post-synaptic neuron.\nrequired\n\n\n\n\n\n\n\nsynapse(pre, post)\nReturns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\nint\nrank of the pre-synaptic neuron.\nrequired\n\n\npost\nint\nrank of the post-synaptic neuron.\nrequired\n\n\n\n\n\n\n\nget(name)\nReturns a list of parameters/variables values for each dendrite in the projection.\nThe list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nthe name of the parameter or variable\nrequired\n\n\n\n\n\n\n\nset(value)\nSets the parameters/variables values for each dendrite in the projection.\nFor parameters, you can provide:\n\na single value, which will be the same for all dendrites.\na list or 1D numpy array of the same length as the number of actual dendrites (self.size).\n\nFor variables, you can provide:\n\na single value, which will be the same for all synapses of all dendrites.\na list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value.\n\nWarning: it is not possible to set different values to each synapse using this method. One should iterate over the dendrites:\nfor dendrite in proj.dendrites:\n    dendrite.w = np.ones(dendrite.size)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\n\na dictionary with the name of the parameter/variable as key.\nrequired\n\n\n\n\n\n\n\nenable_learning(period=None, offset=None)\nEnables learning for all the synapses of this projection.\nFor example, providing the following parameters at time 10 ms:\nproj.enable_learning(period=10., offset=5.)\nwould call the updating methods at times 15, 25, 35, etc…\nThe default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of dt.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nperiod\nfloat\ndetermines how often the synaptic variables will be updated.\nNone\n\n\noffset\nfloat\ndetermines the offset at which the synaptic variables will be updated relative to the current time.\nNone\n\n\n\n\n\n\n\ndisable_learning()\nDisables learning for all synapses of this projection.\nThe effect depends on the rate-coded or spiking nature of the projection:\n\nRate-coded: the updating of all synaptic variables is disabled (including the weights w). This is equivalent to proj.update = False.\nSpiking: the updating of the weights w is disabled, but all other variables are updated. This is equivalent to proj.plasticity = False.\n\nThis method is useful when performing some tests on a trained network without messing with the learned weights.\n\n\n\nsave_connectivity(filename)\nSaves the connectivity of the projection into a file.\nOnly the connectivity matrix, the weights and delays are saved, not the other synaptic variables (use save() if you want these).\nfilename = 'data.npz'\nproj.save_connectivity(filename)\nThe generated data can be used to create a projection in another network:\nproj.from_file(filename)\n\nIf the file name is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the file name ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nIf the file name is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nOtherwise, the data will be pickled into a simple binary text file using pickle.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\n\nfile name, may contain relative or absolute path.\nrequired\n\n\n\n\n\n\n\nconnectivity_matrix(fill=0.0)\nReturns a dense connectivity matrix (2D Numpy array) representing the connectivity matrix between the pre- and post-populations.\nThe first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones.\nIf PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfill\nfloat\nvalue to put in the matrix when there is no connection (default: 0.0).\n0.0\n\n\n\n\n\n\n\nreceptive_fields(variable='w', in_post_geometry=True)\nGathers all receptive fields within this projection.\nThe method only works when the pre- and post-synaptic populations have a 2d geometry. It concatenates all receptive fields of the post population into a 2d array.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr\nName of the variable.\n'w'\n\n\nin_post_geometry\nbool\nIf False, the data will be plotted as square grid.\nTrue\n\n\n\n\n\n\n\nsave(filename)\nSaves all information about the projection (connectivity, current value of parameters and variables) into a file.\n\nIf the file name is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the file name ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nIf the file name is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nOtherwise, the data will be pickled into a simple binary text file using pickle.\n\nWarning: the ‘.mat’ data will not be loadable by ANNarchy, it is only for external analysis purpose.\nExample:\nproj.save('proj1.npz')\nproj.save('proj1.txt')\nproj.save('proj1.txt.gz')\nproj.save('proj1.mat')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfile name, may contain relative or absolute path.\nrequired\n\n\n\n\n\n\n\nload(filename, pickle_encoding=None)\nLoads the saved state of the projection by Projection.save().\nWarning: Matlab data can not be loaded.\nExample:\nproj.load('proj1.npz')\nproj.load('proj1.txt')\nproj.load('proj1.txt.gz')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nthe file name with relative or absolute path.\nrequired\n\n\npickle_encoding\nstr\nWhat encoding to use when reading Python 2 strings. Only useful when loading Python 2 generated pickled files in Python 3, which includes npy/npz files containing object arrays. Values other than latin1, ASCII, and bytes are not allowed, as they can corrupt numerical data.\nNone\n\n\n\n\n\n\n\nstart_pruning(period=None)\nStarts pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nperiod\nfloat\nhow often pruning should be evaluated (default: dt, i.e. each step)\nNone\n\n\n\n\n\n\n\nstop_pruning()\nStops pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\nstart_creating(period=None)\nStarts creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nperiod\nfloat\nhow often creating should be evaluated (default: dt, i.e. each step)\nNone\n\n\n\n\n\n\n\nstop_creating()\nStops creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\nupdate_launch_config(nb_blocks=-1, threads_per_block=32)\nAllows the adjustment of the CUDA launch config (since 4.7.2).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnb_blocks\nint\nnumber of CUDA blocks which can be 65535 at maximum. If set to -1, the number of launched blocks is computed by ANNarchy.\n-1\n\n\nthreads_per_block\nint\nnumber of CUDA threads for one block which can be maximally 1024.\n32",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Projection"
    ]
  },
  {
    "objectID": "reference/Projection.html#parameters",
    "href": "reference/Projection.html#parameters",
    "title": "Projection",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\nstr | Population\nPre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\nstr | Population\nPost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\nstr\nType of the connection.\nrequired\n\n\nsynapse\nSynapse\nA Synapse instance.\nNone\n\n\nname\nstr\nUnique name of the projection (optional, it defaults to proj0, proj1, etc).\nNone\n\n\ndisable_omp\nbool\nEspecially for small- and mid-scale sparse spiking networks, the parallelization of spike propagation is not scalable and disabled by default. It can be re-enabled by setting this parameter to False.\nTrue",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Projection"
    ]
  },
  {
    "objectID": "reference/Projection.html#attributes",
    "href": "reference/Projection.html#attributes",
    "title": "Projection",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nparameters\nList of parameter names.\n\n\nvariables\nList of variable names.\n\n\nattributes\nList of attribute names (parameters + variables).\n\n\nfunctions\nList of functions defined by the synapse model.\n\n\nsize\nNumber of post-synaptic neurons receiving synapses.\n\n\nnb_synapses\nTotal number of synapses in the projection.\n\n\nnb_synapses_per_dendrite\nTotal number of synapses for each dendrite as a list.\n\n\npost_ranks\nList of ranks of post-synaptic neurons that receive connections. Read-only.\n\n\npre_ranks\nList of lists of pre-synaptic ranks, for each post-synaptic neuron. Read-only.\n\n\ndendrites\nIteratively returns the dendrites corresponding to this projection.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Projection"
    ]
  },
  {
    "objectID": "reference/Projection.html#methods",
    "href": "reference/Projection.html#methods",
    "title": "Projection",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nall_to_all\nall-to-all (fully-connected) connection pattern.\n\n\nfixed_probability\nProbabilistic sparse connection pattern.\n\n\none_to_one\none-to-one connection pattern.\n\n\nfixed_number_pre\nConnection pattern where each post-synaptic neuron receives a fixed number of pre-synaptic neurons.\n\n\nfixed_number_post\nEach pre-synaptic neuron randomly sends a fixed number of connections to the post-synaptic neurons.\n\n\ngaussian\nGaussian connection pattern.\n\n\ndog\nDifference-Of-Gaussians connection pattern.\n\n\nfrom_function\nConnection pattern based on a user-defined function.\n\n\nfrom_matrix\nBuilds a connection pattern according to a dense connectivity matrix.\n\n\nfrom_matrix_market\nLoads a weight matrix encoded in the Matrix Market format. This connector is intended for benchmarking purposes.\n\n\nfrom_sparse\nBuilds a connectivity pattern using a Scipy sparse matrix for the weights and (optionally) delays.\n\n\nfrom_file\nBuilds the connectivity matrix using data saved using Projection.save_connectivity() (not save()!).\n\n\nreset\nResets all parameters and variables of the projection to their initial value (before the call to compile()).\n\n\ndendrite\nReturns the dendrite of a postsynaptic neuron according to its rank.\n\n\nsynapse\nReturns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise.\n\n\nget\nReturns a list of parameters/variables values for each dendrite in the projection.\n\n\nset\nSets the parameters/variables values for each dendrite in the projection.\n\n\nenable_learning\nEnables learning for all the synapses of this projection.\n\n\ndisable_learning\nDisables learning for all synapses of this projection.\n\n\nsave_connectivity\nSaves the connectivity of the projection into a file.\n\n\nconnectivity_matrix\nReturns a dense connectivity matrix (2D Numpy array) representing the connectivity matrix between the pre- and post-populations.\n\n\nreceptive_fields\nGathers all receptive fields within this projection.\n\n\nsave\nSaves all information about the projection (connectivity, current value of parameters and variables) into a file.\n\n\nload\nLoads the saved state of the projection by Projection.save().\n\n\nstart_pruning\nStarts pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n\n\nstop_pruning\nStops pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n\n\nstart_creating\nStarts creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n\n\nstop_creating\nStops creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n\n\nupdate_launch_config\nAllows the adjustment of the CUDA launch config (since 4.7.2).\n\n\n\n\n\nall_to_all(\n    self,\n    weights,\n    delays=0.0,\n    allow_self_connections=False,\n    force_multiple_weights=False,\n    storage_format=None,\n    storage_order=None,\n)\nall-to-all (fully-connected) connection pattern.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nfloat | RandomDistribution\nSynaptic values, either a single value or a random distribution object.\nrequired\n\n\ndelays\nfloat | RandomDistribution\nSynaptic delays, either a single value or a random distribution object (default=dt).\n0.0\n\n\nallow_self_connections\nbool\nIf True, self-connections between a neuron and itself are allowed (default = False if the pre- and post-populations are identical, True otherwise).\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\nfixed_probability(\n    self,\n    probability,\n    weights,\n    delays=0.0,\n    allow_self_connections=False,\n    force_multiple_weights=False,\n    storage_format=None,\n    storage_order=None,\n)\nProbabilistic sparse connection pattern.\nEach neuron in the postsynaptic population is connected to neurons of the presynaptic population with the given probability. Self-connections are avoided by default.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprobability\nfloat\nProbability that a synapse is created.\nrequired\n\n\nweights\nfloat | RandomDistribution\nEither a single value for all synapses or a RandomDistribution object.\nrequired\n\n\ndelays\nfloat | RandomDistribution\nEither a single value for all synapses or a RandomDistribution object (default = dt)\n0.0\n\n\nallow_self_connections\nbool\nDefines if self-connections are allowed (default=False).\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\none_to_one(\n    self,\n    weights=1.0,\n    delays=0.0,\n    force_multiple_weights=False,\n    storage_format=None,\n    storage_order=None,\n)\none-to-one connection pattern.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nfloat | RandomDistribution\nInitial synaptic values, either a single value (float) or a random distribution object.\n1.0\n\n\ndelays\nfloat | RandomDistribution\nSynaptic delays, either a single value or a random distribution object (default=dt).\n0.0\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\nfixed_number_pre(\n    self,\n    number,\n    weights,\n    delays=0.0,\n    allow_self_connections=False,\n    force_multiple_weights=False,\n    storage_format=None,\n    storage_order=None,\n)\nConnection pattern where each post-synaptic neuron receives a fixed number of pre-synaptic neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnumber\nint\nNumber of synapses per postsynaptic neuron.\nrequired\n\n\nweights\nfloat | RandomDistribution\nEither a single value for all synapses or a RandomDistribution object.\nrequired\n\n\ndelays\nfloat | RandomDistribution\nEither a single value for all synapses or a RandomDistribution object (default = dt)\n0.0\n\n\nallow_self_connections\nbool\nDefines if self-connections are allowed (default=False).\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\nfixed_number_post(\n    self,\n    number,\n    weights=1.0,\n    delays=0.0,\n    allow_self_connections=False,\n    force_multiple_weights=False,\n    storage_format=None,\n    storage_order=None,\n)\nEach pre-synaptic neuron randomly sends a fixed number of connections to the post-synaptic neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnumber\nint\nNumber of synapses per pre-synaptic neuron.\nrequired\n\n\nweights\nfloat | RandomDistribution\nEither a single value for all synapses or a RandomDistribution object.\n1.0\n\n\ndelays\nfloat | RandomDistribution\nEither a single value for all synapses or a RandomDistribution object (default = dt)\n0.0\n\n\nallow_self_connections\nbool\nDefines if self-connections are allowed (default=False)\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\ngaussian(\n    self,\n    amp,\n    sigma,\n    delays=0.0,\n    limit=0.01,\n    allow_self_connections=False,\n    storage_format=None,\n)\nGaussian connection pattern.\nEach neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Gaussian profile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\namp\nfloat\nAmplitude of the Gaussian function\nrequired\n\n\nsigma\nfloat\nWidth of the Gaussian function\nrequired\n\n\ndelays\nfloat | RandomDistribution\nSynaptic delay, either a single value or a random distribution object (default=dt).\n0.0\n\n\nlimit\nfloat\nProportion of amp below which synapses are not created\n0.01\n\n\nallow_self_connections\nbool\nAllows connections between a neuron and itself.\nFalse\n\n\n\n\n\n\n\ndog(\n    self,\n    amp_pos,\n    sigma_pos,\n    amp_neg,\n    sigma_neg,\n    delays=0.0,\n    limit=0.01,\n    allow_self_connections=False,\n    storage_format=None,\n)\nDifference-Of-Gaussians connection pattern.\nEach neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Difference-Of-Gaussians profile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\namp_pos\nfloat\nAmplitude of the positive Gaussian function\nrequired\n\n\nsigma_pos\nfloat\nWidth of the positive Gaussian function\nrequired\n\n\namp_neg\nfloat\nAmplitude of the negative Gaussian function\nrequired\n\n\nsigma_neg\nfloat\nWidth of the negative Gaussian function\nrequired\n\n\ndelays\nfloat | RandomDistribution\nSynaptic delay, either a single value or a random distribution object (default=dt).\n0.0\n\n\nlimit\nfloat\nProportion of amp below which synapses are not created (default: 0.01)\n0.01\n\n\nallow_self_connections\nbool\nAllows connections between a neuron and itself.\nFalse\n\n\n\n\n\n\n\nfrom_function(self, method, storage_format=None, storage_order=None, **args)\nConnection pattern based on a user-defined function.\nThe two first arguments of the function must be the pre and post populations. Additional arguments can be passed at creation time.\nThe function must return a ann.LILConnectivity object.\nExample:\ndef probabilistic_pattern(pre, post, weight, probability):\n    # Create a LIL structure for the connectivity matrix\n    synapses = ann.LILConnectivity()\n    # For all neurons in the post-synaptic population\n    for post_rank in xrange(post.size):\n        # Decide which pre-synaptic neurons should form synapses\n        ranks = []\n        for pre_rank in xrange(pre.size):\n            if random.random() &lt; probability:\n                ranks.append(pre_rank)\n        # Create weights and delays arrays of the same size\n        values = [weight for i in xrange(len(ranks)) ]\n        delays = [0 for i in xrange(len(ranks)) ]\n        # Add this information to the LIL matrix\n        synapses.add(post_rank, ranks, values, delays)\n\n    return synapses\n\nproj = ann.Projection(pop1, pop2, target = 'inh')\nproj.connect_with_func(\n    method=probabilistic_pattern, \n    weight=1.0, \n    probability=0.3\n) \n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmethod\n\nMethod to call. The method must return a LILConnectivity object.\nrequired\n\n\nargs\n\nList of additional arguments needed by the function.\n{}\n\n\n\n\n\n\n\nfrom_matrix(\n    self,\n    weights,\n    delays=0.0,\n    pre_post=False,\n    storage_format=None,\n    storage_order=None,\n)\nBuilds a connection pattern according to a dense connectivity matrix.\nThe matrix must be N*M, where N is the number of neurons in the post-synaptic population and M in the pre-synaptic one. Lists of lists must have the same size.\nIf a synapse should not be created, the weight value should be None.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nnp.array\nNumpy array (or list of lists of equal size) representing the weights. If a value is None, the corresponding synapse will not be created.\nrequired\n\n\ndelays\n\nNumpy array representing the delays. Must represent the same synapses as the weights argument. If omitted, the delays are considered 0.\n0.0\n\n\npre_post\n\nStates which index is first. By default, the first dimension is related to the post-synaptic population. If pre_post is True, the first dimension is the pre-synaptic population.\nFalse\n\n\n\n\n\n\n\nfrom_matrix_market(self, filename, storage_format=None, storage_order=None)\nLoads a weight matrix encoded in the Matrix Market format. This connector is intended for benchmarking purposes.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nFilename of the Matrix Market (.mtx) file.\nrequired\n\n\n\n\n\n\n\nfrom_sparse(self, weights, delays=0.0, storage_format=None, storage_order=None)\nBuilds a connectivity pattern using a Scipy sparse matrix for the weights and (optionally) delays.\nWarning: a sparse matrix has pre-synaptic ranks as first dimension.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nscipy.sparse.lil_matrix\na sparse lil_matrix object created from scipy.\nrequired\n\n\ndelays\nint | float\nthe value of the constant delay (default: dt). Variable delays are not allowed.\n0.0\n\n\n\n\n\n\n\nfrom_file(\n    self,\n    filename,\n    pickle_encoding=None,\n    storage_format=None,\n    storage_order=None,\n)\nBuilds the connectivity matrix using data saved using Projection.save_connectivity() (not save()!).\nAdmissible file formats are compressed Numpy files (.npz), gunzipped binary text files (.gz) or binary text files.\nNote: Only the ranks, weights and delays are loaded, not the other variables.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfile where the connections were saved.\nrequired\n\n\n\n\n\n\n\nreset(attributes=-1, synapses=False)\nResets all parameters and variables of the projection to their initial value (before the call to compile()).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattributes\n\nlist of attributes (parameter or variable) which should be reinitialized. Default: all attributes (-1).\n-1\n\n\nsynapses\n\ndefines if the weights and delays should also be recreated. Default: False\nFalse\n\n\n\n\n\n\n\ndendrite(post)\nReturns the dendrite of a postsynaptic neuron according to its rank.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npost\nint\ncan be either the rank or the coordinates of the post-synaptic neuron.\nrequired\n\n\n\n\n\n\n\nsynapse(pre, post)\nReturns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\nint\nrank of the pre-synaptic neuron.\nrequired\n\n\npost\nint\nrank of the post-synaptic neuron.\nrequired\n\n\n\n\n\n\n\nget(name)\nReturns a list of parameters/variables values for each dendrite in the projection.\nThe list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nthe name of the parameter or variable\nrequired\n\n\n\n\n\n\n\nset(value)\nSets the parameters/variables values for each dendrite in the projection.\nFor parameters, you can provide:\n\na single value, which will be the same for all dendrites.\na list or 1D numpy array of the same length as the number of actual dendrites (self.size).\n\nFor variables, you can provide:\n\na single value, which will be the same for all synapses of all dendrites.\na list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value.\n\nWarning: it is not possible to set different values to each synapse using this method. One should iterate over the dendrites:\nfor dendrite in proj.dendrites:\n    dendrite.w = np.ones(dendrite.size)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\n\na dictionary with the name of the parameter/variable as key.\nrequired\n\n\n\n\n\n\n\nenable_learning(period=None, offset=None)\nEnables learning for all the synapses of this projection.\nFor example, providing the following parameters at time 10 ms:\nproj.enable_learning(period=10., offset=5.)\nwould call the updating methods at times 15, 25, 35, etc…\nThe default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of dt.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nperiod\nfloat\ndetermines how often the synaptic variables will be updated.\nNone\n\n\noffset\nfloat\ndetermines the offset at which the synaptic variables will be updated relative to the current time.\nNone\n\n\n\n\n\n\n\ndisable_learning()\nDisables learning for all synapses of this projection.\nThe effect depends on the rate-coded or spiking nature of the projection:\n\nRate-coded: the updating of all synaptic variables is disabled (including the weights w). This is equivalent to proj.update = False.\nSpiking: the updating of the weights w is disabled, but all other variables are updated. This is equivalent to proj.plasticity = False.\n\nThis method is useful when performing some tests on a trained network without messing with the learned weights.\n\n\n\nsave_connectivity(filename)\nSaves the connectivity of the projection into a file.\nOnly the connectivity matrix, the weights and delays are saved, not the other synaptic variables (use save() if you want these).\nfilename = 'data.npz'\nproj.save_connectivity(filename)\nThe generated data can be used to create a projection in another network:\nproj.from_file(filename)\n\nIf the file name is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the file name ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nIf the file name is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nOtherwise, the data will be pickled into a simple binary text file using pickle.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\n\nfile name, may contain relative or absolute path.\nrequired\n\n\n\n\n\n\n\nconnectivity_matrix(fill=0.0)\nReturns a dense connectivity matrix (2D Numpy array) representing the connectivity matrix between the pre- and post-populations.\nThe first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones.\nIf PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfill\nfloat\nvalue to put in the matrix when there is no connection (default: 0.0).\n0.0\n\n\n\n\n\n\n\nreceptive_fields(variable='w', in_post_geometry=True)\nGathers all receptive fields within this projection.\nThe method only works when the pre- and post-synaptic populations have a 2d geometry. It concatenates all receptive fields of the post population into a 2d array.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr\nName of the variable.\n'w'\n\n\nin_post_geometry\nbool\nIf False, the data will be plotted as square grid.\nTrue\n\n\n\n\n\n\n\nsave(filename)\nSaves all information about the projection (connectivity, current value of parameters and variables) into a file.\n\nIf the file name is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the file name ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nIf the file name is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nOtherwise, the data will be pickled into a simple binary text file using pickle.\n\nWarning: the ‘.mat’ data will not be loadable by ANNarchy, it is only for external analysis purpose.\nExample:\nproj.save('proj1.npz')\nproj.save('proj1.txt')\nproj.save('proj1.txt.gz')\nproj.save('proj1.mat')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfile name, may contain relative or absolute path.\nrequired\n\n\n\n\n\n\n\nload(filename, pickle_encoding=None)\nLoads the saved state of the projection by Projection.save().\nWarning: Matlab data can not be loaded.\nExample:\nproj.load('proj1.npz')\nproj.load('proj1.txt')\nproj.load('proj1.txt.gz')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nthe file name with relative or absolute path.\nrequired\n\n\npickle_encoding\nstr\nWhat encoding to use when reading Python 2 strings. Only useful when loading Python 2 generated pickled files in Python 3, which includes npy/npz files containing object arrays. Values other than latin1, ASCII, and bytes are not allowed, as they can corrupt numerical data.\nNone\n\n\n\n\n\n\n\nstart_pruning(period=None)\nStarts pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nperiod\nfloat\nhow often pruning should be evaluated (default: dt, i.e. each step)\nNone\n\n\n\n\n\n\n\nstop_pruning()\nStops pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\nstart_creating(period=None)\nStarts creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nperiod\nfloat\nhow often creating should be evaluated (default: dt, i.e. each step)\nNone\n\n\n\n\n\n\n\nstop_creating()\nStops creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\nupdate_launch_config(nb_blocks=-1, threads_per_block=32)\nAllows the adjustment of the CUDA launch config (since 4.7.2).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnb_blocks\nint\nnumber of CUDA blocks which can be 65535 at maximum. If set to -1, the number of launched blocks is computed by ANNarchy.\n-1\n\n\nthreads_per_block\nint\nnumber of CUDA threads for one block which can be maximally 1024.\n32",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Projection"
    ]
  },
  {
    "objectID": "reference/compile.html",
    "href": "reference/compile.html",
    "title": "compile",
    "section": "",
    "text": "compile(\n    directory='annarchy',\n    clean=False,\n    compiler='default',\n    compiler_flags='default',\n    add_sources='',\n    extra_libs='',\n    cuda_config={'device': 0},\n    annarchy_json='',\n    silent=False,\n    debug_build=False,\n    trace_calls=None,\n    profile_enabled=False,\n    net_id=0,\n)\nThis method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation.\nThe compiler, compiler_flags and part of cuda_config take their default value from the configuration file ~/.config/ANNarchy/annarchy.json.\nThe following arguments are for internal development use only:\n\ndebug_build: creates a debug version of ANNarchy, which logs the creation of objects and some other data (default: False).\ntrace_calls: if set to init, simulate, or both simulation calls inside of the C++ kernel are logged to console (default: None)\nprofile_enabled: creates a profilable version of ANNarchy, which logs several computation timings (default: False).\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndirectory\n\nname of the subdirectory where the code will be generated and compiled. Default: “annarchy/”.\n'annarchy'\n\n\nclean\n\nboolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False).\nFalse\n\n\ncompiler\n\nC++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++].\n'default'\n\n\ncompiler_flags\n\nplatform-specific flags to pass to the compiler. Defaults are defined in annarchy.json: “-march=native -O3”.\n'default'\n\n\ncuda_config\n\ndictionary defining the CUDA configuration for each population and projection.\n{'device': 0}\n\n\nannarchy_json\n\ncompiler flags etc can be stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location.\n''\n\n\nsilent\n\ndefines if status message like “Compiling… OK” should be printed.\nFalse",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "compile"
    ]
  },
  {
    "objectID": "reference/compile.html#parameters",
    "href": "reference/compile.html#parameters",
    "title": "compile",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndirectory\n\nname of the subdirectory where the code will be generated and compiled. Default: “annarchy/”.\n'annarchy'\n\n\nclean\n\nboolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False).\nFalse\n\n\ncompiler\n\nC++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++].\n'default'\n\n\ncompiler_flags\n\nplatform-specific flags to pass to the compiler. Defaults are defined in annarchy.json: “-march=native -O3”.\n'default'\n\n\ncuda_config\n\ndictionary defining the CUDA configuration for each population and projection.\n{'device': 0}\n\n\nannarchy_json\n\ncompiler flags etc can be stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location.\n''\n\n\nsilent\n\ndefines if status message like “Compiling… OK” should be printed.\nFalse",
    "crumbs": [
      "Reference",
      "**DEPRECATED Top-level API**",
      "compile"
    ]
  },
  {
    "objectID": "reference/BoldMonitor.html",
    "href": "reference/BoldMonitor.html",
    "title": "BoldMonitor",
    "section": "",
    "text": "BoldMonitor(\n    self,\n    populations=None,\n    bold_model=None,\n    mapping={'I_CBF': 'r'},\n    scale_factor=None,\n    normalize_input=None,\n    recorded_variables=None,\n    start=False,\n    copied=False,\n    net_id=0,\n)\nMonitors the BOLD signal for several populations using a computational model.\nReturned by Network.boldmonitor().\nThe monitor can be started and stopped with start() and stop(). The recorded data is retrieved with get().\n\n\n\n\n\nName\nDescription\n\n\n\n\nstart\nStarts recording as in ANNarchy.core.Monitor.start().\n\n\nstop\nStops recording as in ANNarchy.core.Monitor.stop().\n\n\nget\nRetrieves recordings as in ANNarchy.core.Monitor.get().\n\n\n\n\n\nstart()\nStarts recording as in ANNarchy.core.Monitor.start().\n\n\n\nstop()\nStops recording as in ANNarchy.core.Monitor.stop().\n\n\n\nget(variable)\nRetrieves recordings as in ANNarchy.core.Monitor.get().",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "BoldMonitor"
    ]
  },
  {
    "objectID": "reference/BoldMonitor.html#methods",
    "href": "reference/BoldMonitor.html#methods",
    "title": "BoldMonitor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nstart\nStarts recording as in ANNarchy.core.Monitor.start().\n\n\nstop\nStops recording as in ANNarchy.core.Monitor.stop().\n\n\nget\nRetrieves recordings as in ANNarchy.core.Monitor.get().\n\n\n\n\n\nstart()\nStarts recording as in ANNarchy.core.Monitor.start().\n\n\n\nstop()\nStops recording as in ANNarchy.core.Monitor.stop().\n\n\n\nget(variable)\nRetrieves recordings as in ANNarchy.core.Monitor.get().",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "BoldMonitor"
    ]
  },
  {
    "objectID": "reference/add_function.html",
    "href": "reference/add_function.html",
    "title": "add_function",
    "section": "",
    "text": "add_function(function)\nDefines a global function which can be used by all neurons and synapses.\nThe function must have only one return value and use only the passed arguments.\nExamples of valid functions:\nann.add_function('logistic(x) = 1 / (1 + exp(-x))')\n\nann.add_function('''\n    piecewise(x, a, b) = if x &lt; a:\n                            a\n                         else: \n                            if x &gt; b :\n                                b\n                            else:\n                                x\n''')\nPlease refer to the manual to know the allowed mathematical functions.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunction\nstr\n(multi)string representing the function.\nrequired",
    "crumbs": [
      "Reference",
      "**Functions**",
      "add_function"
    ]
  },
  {
    "objectID": "reference/add_function.html#parameters",
    "href": "reference/add_function.html#parameters",
    "title": "add_function",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfunction\nstr\n(multi)string representing the function.\nrequired",
    "crumbs": [
      "Reference",
      "**Functions**",
      "add_function"
    ]
  },
  {
    "objectID": "reference/Binomial.html",
    "href": "reference/Binomial.html",
    "title": "Binomial",
    "section": "",
    "text": "Binomial(self, n, p, rng=None)\nBinomial distribution.\nParameters: n trials and p probability of success where n an integer &gt;= 0 and p is in the interval [0,1].\nThe returned values are the number of successes over the n trials.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of trials.\nrequired\n\n\np\nfloat\nProbability of success.\nrequired\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Binomial"
    ]
  },
  {
    "objectID": "reference/Binomial.html#parameters",
    "href": "reference/Binomial.html#parameters",
    "title": "Binomial",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of trials.\nrequired\n\n\np\nfloat\nProbability of success.\nrequired\n\n\nrng\nnp.random.Generator\n(optional) random number generator. If left None, np.random.default_rng() is used.\nNone",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Binomial"
    ]
  },
  {
    "objectID": "reference/Binomial.html#methods",
    "href": "reference/Binomial.html#methods",
    "title": "Binomial",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\nget_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nshape of the array.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Binomial"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "ANNarchy",
    "section": "",
    "text": "Basic objects composing a network. The Network class is the main access point for all functionalities.\n\n\n\nNetwork\nA network creates the populations, projections and monitors, and controls the simulation.\n\n\nPopulation\nPopulation of neurons.\n\n\nProjection\nProjection between two populations.\n\n\nMonitor\nObject allowing to record variables from Population, PopulationView, Dendrite or Projection instances.\n\n\nPopulationView\nSubset of a Population.\n\n\nDendrite\nSub-group of a Projection for a single post-synaptic neuron.\n\n\n\n\n\n\nObjects allowing to design neuron and synapse models.\n\n\n\nNeuron\nBase class to define a neuron model.\n\n\nSynapse\nBase class to define a synapse model.\n\n\nParameter\nDataclass to represent a parameter in a Neuron or Synapse definition.\n\n\nVariable\nDataclass to represent a variable in a Neuron or Synapse definition.\n\n\nCreating\nDataclass to represent a creation condition for structural plasticity.\n\n\nPruning\nDataclass to represent a pruning condition for structural plasticity.\n\n\nConstant\nConstant parameter that can be used by all neurons and synapses.\n\n\n\n\n\n\nDefault neuron models that can be used directly. The naming follows the PyNN convention.\n\n\n\nLeakyIntegrator\nLeaky-integrator rate-coded neuron, optionally noisy.\n\n\nIzhikevich\nIzhikevich quadratic spiking neuron.\n\n\nIF_curr_exp\nLeaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current.\n\n\nIF_cond_exp\nLeaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance.\n\n\nIF_curr_alpha\nLeaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents.\n\n\nIF_cond_alpha\nLeaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance.\n\n\nHH_cond_exp\nSingle-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub.\n\n\nEIF_cond_alpha_isfa_ista\nExponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.), alpha post-synaptic conductances.\n\n\nEIF_cond_exp_isfa_ista\nExponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.), decaying-exponential post-synaptic conductances.\n\n\n\n\n\n\nDefault synapse models that can be used directly.\n\n\n\nHebb\nRate-coded synapse with Hebbian plasticity.\n\n\nOja\nRate-coded synapse with regularized Hebbian plasticity (Oja).\n\n\nIBCM\nRate-coded synapse with Intrator & Cooper (1992) plasticity.\n\n\nSTP\nSynapse exhibiting short-term facilitation and depression.\n\n\nSTDP\nSpike-timing dependent plasticity, online version.\n\n\n\n\n\n\nInput populations that can be used to stimulate the networks.\nInput populations can be passed to Network.create() directly, e.g.:\npop1 = net.create(ann.PoissonPopulation(100, rates=15.))\nInput projections (CurrentInjection and DecodingProjection) can be passed to Network.connect():\nnet.connect(ann.DecodingProjection(pop1, pop2, 'exc', window=10.0))\n\n\n\nInputArray\nPopulation holding static inputs for a rate-coded network.\n\n\nTimedArray\nData structure holding sequential inputs for a rate-coded network.\n\n\nPoissonPopulation\nPopulation of spiking neurons following a Poisson distribution.\n\n\nTimedPoissonPopulation\nPoisson population whose rate vary with the provided schedule.\n\n\nSpikeSourceArray\nSpike source generating spikes at the times given in the spike_times array.\n\n\nHomogeneousCorrelatedSpikeTrains\nPopulation of spiking neurons following a homogeneous distribution with correlated spike trains.\n\n\nCurrentInjection\nInject current from a rate-coded population into a spiking population.\n\n\nDecodingProjection\nDecoding projection to transform spike trains into firing rates.\n\n\nImagePopulation\nRate-coded Population allowing to represent images (png, jpg…) as the firing rate of a population (each neuron represents one pixel).\n\n\nVideoPopulation\nRate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel).\n\n\n\n\n\n\nRandom distributions that can be used to generate numpy arrays.\n\n\n\nUniform\nUniform distribution between min and max.\n\n\nDiscreteUniform\nDiscrete uniform distribution between min and max.\n\n\nNormal\nNormal distribution.\n\n\nLogNormal\nLog-normal distribution.\n\n\nExponential\nExponential distribution, according to the density function:\n\n\nGamma\nGamma distribution.\n\n\nBinomial\nBinomial distribution.\n\n\n\n\n\n\nFunctions declared at the global level.\n\n\n\nadd_function\nDefines a global function which can be used by all neurons and synapses.\n\n\nfunctions\nAllows to access a global function declared with add_function() and use it from Python using arrays after compilation of the magic network.\n\n\n\n\n\n\nDecorator used to interrupt the simulation at the desired time.\n@every(period=100., offset=-10.)\ndef step_input(n):\n    pop.I = float(n) / 100.\n\nsimulate(10000.)\n\n\n\nevery\nDecorator to declare a callback method that will be called periodically during the simulation.\n\n\n\n\n\n\nVarious additional utilities.\n\n\n\nreport\nGenerates a report describing the network.\n\n\ntimeit\nDecorator to measure the execution time of a method.\n\n\nsparse_random_matrix\nReturns a sparse lil-matrix for use in Projection.from_sparse().\n\n\nsparse_delays_from_weights\nReturns a sparse delay matrix with the same connectivity as the sparse matrix weight_matrix.\n\n\nmagic_network\nReturns the magic network of ID 0.\n\n\n\n\n\n\nExtension for convolution and pooling projections. The extension has to be explicitly imported:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.convolution import Convolution, Pooling, Copy, Transpose\n\n\n\nConvolution\nPerforms a convolution of a weight kernel on the pre-synaptic population.\n\n\nPooling\nPerforms a pooling operation (e.g. max.pooling) on the pre-synaptic population.\n\n\nTranspose\nTransposed projection reusing the weights of an already-defined projection.\n\n\nCopy\nCreates a virtual projection reusing the weights and delays of an already-defined projection.\n\n\n\n\n\n\nExtension for monitoring BOLD signals in a population.\nimport ANNarchy as ann\nimport ANNarchy.extensions.bold as bold\n\n\n\nBoldMonitor\nMonitors the BOLD signal for several populations using a computational model.\n\n\nBoldModel\nBase class to define a BOLD model to be used in a BOLD monitor.\n\n\nballoon_RN\nA balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_RL\nA balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_CN\nA balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_CL\nA balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_maith2021\nThe balloon model as used in Maith et al. (2021).\n\n\nballoon_two_inputs\nBOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007).\n\n\n\n\n\n\nLogging utilities based on tensorboard are provided in the module ANNarchy.extensions.tensorboard, which must be explicitly imported:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.tensorboard import Logger\nThe main object is the Logger class.\n\n\n\nLogger\nLogger class to use tensorboard to visualize ANNarchy simulations. Requires the tensorboardX package (pip install tensorboardX).\n\n\n\n\n\n\nModule allowing to convert an ANN trained with keras into a spiking neural network:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.ann_to_snn_conversion import ANNtoSNNConverter\n\n\n\nANNtoSNNConverter\nConverts a pre-trained Keras model .keras into an ANNarchy spiking neural network.\n\n\n\n\n\n\nAccessing objects at the global level is deprecated since ANNarchy 5.0.\n\n\n\nsetup\nThe setup function is used to configure ANNarchy simulations.\n\n\ncompile\nThis method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation.\n\n\nclear\nClears all variables (erasing already defined populations, projections, monitors), as if you had just imported ANNarchy.\n\n\nreset\nReinitialises the network to its state before the call to compile. The network time will be set to 0ms.\n\n\nset_seed\nSets the seed of the random number generators, both in ANNarchy.RandomDistributions and in the C++ library when it is created.\n\n\nget_population\nReturns the population with the given name.\n\n\nget_projection\nReturns the projection with the given name.\n\n\npopulations\nReturns a list of all declared populations.\n\n\nprojections\nReturns a list of all declared populations.\n\n\nmonitors\nReturns a list of declared monitors.\n\n\nsimulate\nSimulates the network for the given duration in milliseconds.\n\n\nsimulate_until\nRuns the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped.\n\n\nstep\nPerforms a single simulation step (duration = dt).\n\n\nenable_learning\nEnables learning for all projections. Optionally period and offset can be changed for all projections.\n\n\ndisable_learning\nDisables learning for all projections.\n\n\nget_time\nReturns the current time in ms.\n\n\nset_time\nSets the current time in ms.\n\n\nget_current_step\nReturns the current simulation step.\n\n\nset_current_step\nSets the current simulation step (integer).\n\n\ndt\nReturns the simulation step size dt used in the simulation.\n\n\nsave\nSave the current network state (parameters and variables) to a file.\n\n\nload\nLoads a saved state of the network.\n\n\nsave_parameters\nSaves the global parameters of a network (flag population for neurons, projection for synapses) to a JSON file.\n\n\nload_parameters\nLoads the global parameters of a network (flag population for neurons, projection for synapses) from a JSON file.\n\n\ncallbacks_enabled\nReturns True if callbacks are enabled for the network.\n\n\ndisable_callbacks\nDisables all callbacks for the network.\n\n\nenable_callbacks\nEnables all declared callbacks for the network.\n\n\nclear_all_callbacks\nClears the list of declared callbacks for the network.",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#core-components",
    "href": "reference/index.html#core-components",
    "title": "ANNarchy",
    "section": "",
    "text": "Basic objects composing a network. The Network class is the main access point for all functionalities.\n\n\n\nNetwork\nA network creates the populations, projections and monitors, and controls the simulation.\n\n\nPopulation\nPopulation of neurons.\n\n\nProjection\nProjection between two populations.\n\n\nMonitor\nObject allowing to record variables from Population, PopulationView, Dendrite or Projection instances.\n\n\nPopulationView\nSubset of a Population.\n\n\nDendrite\nSub-group of a Projection for a single post-synaptic neuron.",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#neuron-and-synapse-models",
    "href": "reference/index.html#neuron-and-synapse-models",
    "title": "ANNarchy",
    "section": "",
    "text": "Objects allowing to design neuron and synapse models.\n\n\n\nNeuron\nBase class to define a neuron model.\n\n\nSynapse\nBase class to define a synapse model.\n\n\nParameter\nDataclass to represent a parameter in a Neuron or Synapse definition.\n\n\nVariable\nDataclass to represent a variable in a Neuron or Synapse definition.\n\n\nCreating\nDataclass to represent a creation condition for structural plasticity.\n\n\nPruning\nDataclass to represent a pruning condition for structural plasticity.\n\n\nConstant\nConstant parameter that can be used by all neurons and synapses.",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#neuron-models",
    "href": "reference/index.html#neuron-models",
    "title": "ANNarchy",
    "section": "",
    "text": "Default neuron models that can be used directly. The naming follows the PyNN convention.\n\n\n\nLeakyIntegrator\nLeaky-integrator rate-coded neuron, optionally noisy.\n\n\nIzhikevich\nIzhikevich quadratic spiking neuron.\n\n\nIF_curr_exp\nLeaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current.\n\n\nIF_cond_exp\nLeaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance.\n\n\nIF_curr_alpha\nLeaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents.\n\n\nIF_cond_alpha\nLeaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance.\n\n\nHH_cond_exp\nSingle-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub.\n\n\nEIF_cond_alpha_isfa_ista\nExponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.), alpha post-synaptic conductances.\n\n\nEIF_cond_exp_isfa_ista\nExponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.), decaying-exponential post-synaptic conductances.",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#synapse-models",
    "href": "reference/index.html#synapse-models",
    "title": "ANNarchy",
    "section": "",
    "text": "Default synapse models that can be used directly.\n\n\n\nHebb\nRate-coded synapse with Hebbian plasticity.\n\n\nOja\nRate-coded synapse with regularized Hebbian plasticity (Oja).\n\n\nIBCM\nRate-coded synapse with Intrator & Cooper (1992) plasticity.\n\n\nSTP\nSynapse exhibiting short-term facilitation and depression.\n\n\nSTDP\nSpike-timing dependent plasticity, online version.",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#inputs",
    "href": "reference/index.html#inputs",
    "title": "ANNarchy",
    "section": "",
    "text": "Input populations that can be used to stimulate the networks.\nInput populations can be passed to Network.create() directly, e.g.:\npop1 = net.create(ann.PoissonPopulation(100, rates=15.))\nInput projections (CurrentInjection and DecodingProjection) can be passed to Network.connect():\nnet.connect(ann.DecodingProjection(pop1, pop2, 'exc', window=10.0))\n\n\n\nInputArray\nPopulation holding static inputs for a rate-coded network.\n\n\nTimedArray\nData structure holding sequential inputs for a rate-coded network.\n\n\nPoissonPopulation\nPopulation of spiking neurons following a Poisson distribution.\n\n\nTimedPoissonPopulation\nPoisson population whose rate vary with the provided schedule.\n\n\nSpikeSourceArray\nSpike source generating spikes at the times given in the spike_times array.\n\n\nHomogeneousCorrelatedSpikeTrains\nPopulation of spiking neurons following a homogeneous distribution with correlated spike trains.\n\n\nCurrentInjection\nInject current from a rate-coded population into a spiking population.\n\n\nDecodingProjection\nDecoding projection to transform spike trains into firing rates.\n\n\nImagePopulation\nRate-coded Population allowing to represent images (png, jpg…) as the firing rate of a population (each neuron represents one pixel).\n\n\nVideoPopulation\nRate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel).",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#random-distributions",
    "href": "reference/index.html#random-distributions",
    "title": "ANNarchy",
    "section": "",
    "text": "Random distributions that can be used to generate numpy arrays.\n\n\n\nUniform\nUniform distribution between min and max.\n\n\nDiscreteUniform\nDiscrete uniform distribution between min and max.\n\n\nNormal\nNormal distribution.\n\n\nLogNormal\nLog-normal distribution.\n\n\nExponential\nExponential distribution, according to the density function:\n\n\nGamma\nGamma distribution.\n\n\nBinomial\nBinomial distribution.",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#functions",
    "href": "reference/index.html#functions",
    "title": "ANNarchy",
    "section": "",
    "text": "Functions declared at the global level.\n\n\n\nadd_function\nDefines a global function which can be used by all neurons and synapses.\n\n\nfunctions\nAllows to access a global function declared with add_function() and use it from Python using arrays after compilation of the magic network.",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#callbacks",
    "href": "reference/index.html#callbacks",
    "title": "ANNarchy",
    "section": "",
    "text": "Decorator used to interrupt the simulation at the desired time.\n@every(period=100., offset=-10.)\ndef step_input(n):\n    pop.I = float(n) / 100.\n\nsimulate(10000.)\n\n\n\nevery\nDecorator to declare a callback method that will be called periodically during the simulation.",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#utilities",
    "href": "reference/index.html#utilities",
    "title": "ANNarchy",
    "section": "",
    "text": "Various additional utilities.\n\n\n\nreport\nGenerates a report describing the network.\n\n\ntimeit\nDecorator to measure the execution time of a method.\n\n\nsparse_random_matrix\nReturns a sparse lil-matrix for use in Projection.from_sparse().\n\n\nsparse_delays_from_weights\nReturns a sparse delay matrix with the same connectivity as the sparse matrix weight_matrix.\n\n\nmagic_network\nReturns the magic network of ID 0.",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#convolution",
    "href": "reference/index.html#convolution",
    "title": "ANNarchy",
    "section": "",
    "text": "Extension for convolution and pooling projections. The extension has to be explicitly imported:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.convolution import Convolution, Pooling, Copy, Transpose\n\n\n\nConvolution\nPerforms a convolution of a weight kernel on the pre-synaptic population.\n\n\nPooling\nPerforms a pooling operation (e.g. max.pooling) on the pre-synaptic population.\n\n\nTranspose\nTransposed projection reusing the weights of an already-defined projection.\n\n\nCopy\nCreates a virtual projection reusing the weights and delays of an already-defined projection.",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#bold-monitoring",
    "href": "reference/index.html#bold-monitoring",
    "title": "ANNarchy",
    "section": "",
    "text": "Extension for monitoring BOLD signals in a population.\nimport ANNarchy as ann\nimport ANNarchy.extensions.bold as bold\n\n\n\nBoldMonitor\nMonitors the BOLD signal for several populations using a computational model.\n\n\nBoldModel\nBase class to define a BOLD model to be used in a BOLD monitor.\n\n\nballoon_RN\nA balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_RL\nA balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_CN\nA balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_CL\nA balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_maith2021\nThe balloon model as used in Maith et al. (2021).\n\n\nballoon_two_inputs\nBOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007).",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#tensorboard-logging",
    "href": "reference/index.html#tensorboard-logging",
    "title": "ANNarchy",
    "section": "",
    "text": "Logging utilities based on tensorboard are provided in the module ANNarchy.extensions.tensorboard, which must be explicitly imported:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.tensorboard import Logger\nThe main object is the Logger class.\n\n\n\nLogger\nLogger class to use tensorboard to visualize ANNarchy simulations. Requires the tensorboardX package (pip install tensorboardX).",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#ann-to-snn-conversion",
    "href": "reference/index.html#ann-to-snn-conversion",
    "title": "ANNarchy",
    "section": "",
    "text": "Module allowing to convert an ANN trained with keras into a spiking neural network:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.ann_to_snn_conversion import ANNtoSNNConverter\n\n\n\nANNtoSNNConverter\nConverts a pre-trained Keras model .keras into an ANNarchy spiking neural network.",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/index.html#deprecated-top-level-api",
    "href": "reference/index.html#deprecated-top-level-api",
    "title": "ANNarchy",
    "section": "",
    "text": "Accessing objects at the global level is deprecated since ANNarchy 5.0.\n\n\n\nsetup\nThe setup function is used to configure ANNarchy simulations.\n\n\ncompile\nThis method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation.\n\n\nclear\nClears all variables (erasing already defined populations, projections, monitors), as if you had just imported ANNarchy.\n\n\nreset\nReinitialises the network to its state before the call to compile. The network time will be set to 0ms.\n\n\nset_seed\nSets the seed of the random number generators, both in ANNarchy.RandomDistributions and in the C++ library when it is created.\n\n\nget_population\nReturns the population with the given name.\n\n\nget_projection\nReturns the projection with the given name.\n\n\npopulations\nReturns a list of all declared populations.\n\n\nprojections\nReturns a list of all declared populations.\n\n\nmonitors\nReturns a list of declared monitors.\n\n\nsimulate\nSimulates the network for the given duration in milliseconds.\n\n\nsimulate_until\nRuns the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped.\n\n\nstep\nPerforms a single simulation step (duration = dt).\n\n\nenable_learning\nEnables learning for all projections. Optionally period and offset can be changed for all projections.\n\n\ndisable_learning\nDisables learning for all projections.\n\n\nget_time\nReturns the current time in ms.\n\n\nset_time\nSets the current time in ms.\n\n\nget_current_step\nReturns the current simulation step.\n\n\nset_current_step\nSets the current simulation step (integer).\n\n\ndt\nReturns the simulation step size dt used in the simulation.\n\n\nsave\nSave the current network state (parameters and variables) to a file.\n\n\nload\nLoads a saved state of the network.\n\n\nsave_parameters\nSaves the global parameters of a network (flag population for neurons, projection for synapses) to a JSON file.\n\n\nload_parameters\nLoads the global parameters of a network (flag population for neurons, projection for synapses) from a JSON file.\n\n\ncallbacks_enabled\nReturns True if callbacks are enabled for the network.\n\n\ndisable_callbacks\nDisables all callbacks for the network.\n\n\nenable_callbacks\nEnables all declared callbacks for the network.\n\n\nclear_all_callbacks\nClears the list of declared callbacks for the network.",
    "crumbs": [
      "Reference",
      "**ANNarchy**"
    ]
  },
  {
    "objectID": "reference/Synapse.html",
    "href": "reference/Synapse.html",
    "title": "Synapse",
    "section": "",
    "text": "Synapse(\n    self,\n    parameters='',\n    equations='',\n    psp=None,\n    operation='sum',\n    pre_spike=None,\n    post_spike=None,\n    pre_axon_spike=None,\n    functions=None,\n    pruning=None,\n    creating=None,\n    name=None,\n    description=None,\n    extra_values={},\n)\nBase class to define a synapse model.\nSynapses expect parameters as a dictionary and equations as a list of variable updates (including w if there is synaptic plasticity).\nRate-coded synapses can define psp and operation to modify synaptic transmission:\nnonlinear_synapse = ann.Synapse( \n    psp = \"log( (pre.r * w + 1 ) / (pre.r * w - 1) )\",\n    operation = 'max',\n)\nSpiking synapses can define event-based rules, such as pre_spike (a pre-synaptic spike arrives at the synapse) and post_spike (the post-synaptic neuron emits a spike):\nSTDP = ann.Synapse(\n    parameters = dict(\n        tau_pre = 10.0,\n        tau_post = 10.0,\n        cApre = 0.01,\n        cApost = 0.0105,\n        wmax = 0.01,\n    ),\n    equations = [\n        ann.Variable('tau_pre * dApre/dt = - Apre', method='event-driven'),\n        ann.Variable('tau_post * dApost/dt = - Apost', method='event-driven'),\n    ],\n    pre_spike = '''\n        g_target += w\n        Apre += cApre * wmax\n        w = clip(w - Apost, 0.0 , wmax)\n    ''',                  \n    post_spike = '''\n        Apost += cApost * wmax\n        w = clip(w + Apre, 0.0 , wmax)\n    ''' \n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameters\nstr | dict\ndictionary of parameters and their initial value.\n''\n\n\nequations\nstr | list\nlist of equations defining the temporal evolution of variables.\n''\n\n\npsp\nstr\ncontinuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r). Synaptic transmission in spiking synapses occurs in pre_spike.\nNone\n\n\noperation\nstr\noperation (sum, max, min, mean) performed by the post-synaptic neuron on the individual psp (rate-coded only).\n'sum'\n\n\npre_spike\nstr | list\nupdating of variables when a pre-synaptic spike is received (spiking only).\nNone\n\n\npost_spike\nstr | list\nupdating of variables when a post-synaptic spike is emitted (spiking only).\nNone\n\n\npre_axon_spike\nstr\nupdating of variables when an axonal spike was emitted (spiking only, default None). The usage of this arguments prevents the application of learning rules.\nNone\n\n\nfunctions\nstr\nadditional functions used in the equations.\nNone\n\n\npruning\nstr\nCondition for pruning the synapse.\nNone\n\n\ncreating\nstr\nCondition for creating the synapse.\nNone\n\n\nname\nstr\nname of the synapse type (used for reporting only).\nNone\n\n\ndescription\nstr\nshort description of the synapse type (used for reporting).\nNone",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Synapse"
    ]
  },
  {
    "objectID": "reference/Synapse.html#parameters",
    "href": "reference/Synapse.html#parameters",
    "title": "Synapse",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nparameters\nstr | dict\ndictionary of parameters and their initial value.\n''\n\n\nequations\nstr | list\nlist of equations defining the temporal evolution of variables.\n''\n\n\npsp\nstr\ncontinuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r). Synaptic transmission in spiking synapses occurs in pre_spike.\nNone\n\n\noperation\nstr\noperation (sum, max, min, mean) performed by the post-synaptic neuron on the individual psp (rate-coded only).\n'sum'\n\n\npre_spike\nstr | list\nupdating of variables when a pre-synaptic spike is received (spiking only).\nNone\n\n\npost_spike\nstr | list\nupdating of variables when a post-synaptic spike is emitted (spiking only).\nNone\n\n\npre_axon_spike\nstr\nupdating of variables when an axonal spike was emitted (spiking only, default None). The usage of this arguments prevents the application of learning rules.\nNone\n\n\nfunctions\nstr\nadditional functions used in the equations.\nNone\n\n\npruning\nstr\nCondition for pruning the synapse.\nNone\n\n\ncreating\nstr\nCondition for creating the synapse.\nNone\n\n\nname\nstr\nname of the synapse type (used for reporting only).\nNone\n\n\ndescription\nstr\nshort description of the synapse type (used for reporting).\nNone",
    "crumbs": [
      "Reference",
      "**Neuron and Synapse models**",
      "Synapse"
    ]
  },
  {
    "objectID": "reference/VideoPopulation.html",
    "href": "reference/VideoPopulation.html",
    "title": "VideoPopulation",
    "section": "",
    "text": "VideoPopulation(\n    self,\n    geometry,\n    opencv_version='4',\n    name=None,\n    copied=False,\n    net_id=0,\n)\nRate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel).\nThis extension requires the C++ library OpenCV &gt;= 4.0 (apt-get/yum install opencv). pkg-config opencv4 --cflags --libs should not return an error. vtk might additionally have to be installed.\nThe extensions has to be explicitly imported:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.image import VideoPopulation\n\nnet = ann.Network()\npop = net.create(VideoPopulation(geometry=(480, 640)))\n\nnet.compile()\n\npop.start_camera(0)\n\nwhile(True):\n    pop.grab_image()\n    net.simulate(10.0)\nAbout the geometry:\n\nIf the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image).\nIf the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color).\n\nIf the third dimension is 3, each will correspond to the RGB values of the pixels.\nWarning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple\npopulation geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized.\nrequired\n\n\nopencv_version\nstr\nOpenCV version (default=4).\n'4'\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nstart_camera\nStarts the webcam with the corresponding device (default = 0).\n\n\ngrab_image\nGrabs one image from the camera and feeds it into the population.\n\n\nrelease\nReleases the camera:\n\n\n\n\n\nstart_camera(camera_port=0)\nStarts the webcam with the corresponding device (default = 0).\nOn linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc.\n\n\n\ngrab_image()\nGrabs one image from the camera and feeds it into the population.\nThe camera must be first started with:\npop.start_camera(0)\n\n\n\nrelease()\nReleases the camera:\npop.release()",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "VideoPopulation"
    ]
  },
  {
    "objectID": "reference/VideoPopulation.html#parameters",
    "href": "reference/VideoPopulation.html#parameters",
    "title": "VideoPopulation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple\npopulation geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized.\nrequired\n\n\nopencv_version\nstr\nOpenCV version (default=4).\n'4'\n\n\nname\nstr\nunique name of the population (optional).\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "VideoPopulation"
    ]
  },
  {
    "objectID": "reference/VideoPopulation.html#methods",
    "href": "reference/VideoPopulation.html#methods",
    "title": "VideoPopulation",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nstart_camera\nStarts the webcam with the corresponding device (default = 0).\n\n\ngrab_image\nGrabs one image from the camera and feeds it into the population.\n\n\nrelease\nReleases the camera:\n\n\n\n\n\nstart_camera(camera_port=0)\nStarts the webcam with the corresponding device (default = 0).\nOn linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc.\n\n\n\ngrab_image()\nGrabs one image from the camera and feeds it into the population.\nThe camera must be first started with:\npop.start_camera(0)\n\n\n\nrelease()\nReleases the camera:\npop.release()",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "VideoPopulation"
    ]
  },
  {
    "objectID": "reference/HH_cond_exp.html",
    "href": "reference/HH_cond_exp.html",
    "title": "HH_cond_exp",
    "section": "",
    "text": "HH_cond_exp(\n    self,\n    gbar_Na=20.0,\n    gbar_K=6.0,\n    gleak=0.01,\n    cm=0.2,\n    v_offset=-63.0,\n    e_rev_Na=50.0,\n    e_rev_K=-90.0,\n    e_rev_leak=-65.0,\n    e_rev_E=0.0,\n    e_rev_I=-80.0,\n    tau_syn_E=0.2,\n    tau_syn_I=2.0,\n    i_offset=0.0,\n    v_thresh=0.0,\n)\nSingle-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub.\nThe ODEs for n, m, h and v are solved using the midpoint method, while the conductances g_exc and g_inh are solved using the exponential Euler method.\nEquivalent code:\nHH_cond_exp = Neuron(\n    parameters = dict(\n        gbar_Na = 20.0\n        gbar_K = 6.0\n        gleak = 0.01\n        cm = 0.2 \n        v_offset = -63.0 \n        e_rev_Na = 50.0\n        e_rev_K = -90.0 \n        e_rev_leak = -65.0\n        e_rev_E = 0.0\n        e_rev_I = -80.0 \n        tau_syn_E = 0.2\n        tau_syn_I = 2.0\n        i_offset = 0.0\n        v_thresh = 0.0\n    ), \n    equations = [\n        # Previous membrane potential\n        'prev_v = v',\n\n        # Voltage-dependent rate constants\n        'an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0)',\n        'am = 0.32  * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0)',\n        'ah = 0.128 * exp((17.0 - v + v_offset)/18.0)',\n\n        'bn = 0.5   * exp ((10.0 - v + v_offset)/40.0)',\n        'bm = 0.28  * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0)',\n        'bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) )',\n\n        # Activation variables\n        ann.Variable('dn/dt = an * (1.0 - n) - bn * n', init = 0.0, method=\"exponential\")\n        ann.Variable('dm/dt = am * (1.0 - m) - bm * m', init = 0.0, method=\"exponential\")\n        ann.Variable('dh/dt = ah * (1.0 - h) - bh * h', init = 1.0, method=\"exponential\")\n\n        # Membrane equation\n        ann.Variable('cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v)\n                        + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset', method=\"exponential\", init=-65.0),\n\n        # Exponentially-decaying conductances\n        ann.Variable('tau_syn_E * dg_exc/dt = - g_exc', method=\"exponential\")\n        ann.Variable('tau_syn_I * dg_inh/dt = - g_inh', method=\"exponential\")\n    ],\n    spike = \"(v &gt; v_thresh) and (prev_v &lt;= v_thresh)\",\n    reset = \"\"\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngbar_Na\n\nMaximal conductance of the Sodium current.\n20.0\n\n\ngbar_K\n\nMaximal conductance of the Potassium current.\n6.0\n\n\ngleak\n\nConductance of the leak current (nF)\n0.01\n\n\ncm\n\nCapacity of the membrane (nF)\n0.2\n\n\nv_offset\n\nThreshold for the rate constants (mV)\n-63.0\n\n\ne_rev_Na\n\nReversal potential for the Sodium current (mV)\n50.0\n\n\ne_rev_K\n\nReversal potential for the Potassium current (mV)\n-90.0\n\n\ne_rev_leak\n\nReversal potential for the leak current (mV)\n-65.0\n\n\ne_rev_E\n\nReversal potential for excitatory input (mV)\n0.0\n\n\ne_rev_I\n\nReversal potential for inhibitory input (mV)\n-80.0\n\n\ntau_syn_E\n\nDecay time of excitatory synaptic current (ms)\n0.2\n\n\ntau_syn_I\n\nDecay time of inhibitory synaptic current (ms)\n2.0\n\n\ni_offset\n\nOffset current (nA)\n0.0\n\n\nv_thresh\n\nThreshold for spike emission\n0.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "HH_cond_exp"
    ]
  },
  {
    "objectID": "reference/HH_cond_exp.html#parameters",
    "href": "reference/HH_cond_exp.html#parameters",
    "title": "HH_cond_exp",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngbar_Na\n\nMaximal conductance of the Sodium current.\n20.0\n\n\ngbar_K\n\nMaximal conductance of the Potassium current.\n6.0\n\n\ngleak\n\nConductance of the leak current (nF)\n0.01\n\n\ncm\n\nCapacity of the membrane (nF)\n0.2\n\n\nv_offset\n\nThreshold for the rate constants (mV)\n-63.0\n\n\ne_rev_Na\n\nReversal potential for the Sodium current (mV)\n50.0\n\n\ne_rev_K\n\nReversal potential for the Potassium current (mV)\n-90.0\n\n\ne_rev_leak\n\nReversal potential for the leak current (mV)\n-65.0\n\n\ne_rev_E\n\nReversal potential for excitatory input (mV)\n0.0\n\n\ne_rev_I\n\nReversal potential for inhibitory input (mV)\n-80.0\n\n\ntau_syn_E\n\nDecay time of excitatory synaptic current (ms)\n0.2\n\n\ntau_syn_I\n\nDecay time of inhibitory synaptic current (ms)\n2.0\n\n\ni_offset\n\nOffset current (nA)\n0.0\n\n\nv_thresh\n\nThreshold for spike emission\n0.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "HH_cond_exp"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html",
    "href": "notebooks/NeuralField.html",
    "title": "Neural Field",
    "section": "",
    "text": "#!pip install ANNarchy\nThis notebook demonstrates a simple rate-coded model using Neural Fields. It consists of two 2D populations inp and pop, with one-to-one connections between inp and pop, and Difference-of-Gaussians (DoG) lateral connections within pop.\nIt is is based on the following paper:",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#model-overview",
    "href": "notebooks/NeuralField.html#model-overview",
    "title": "Neural Field",
    "section": "Model overview",
    "text": "Model overview\nEach population consists of N*N neurons, with N=20. The inp population is solely used to represent inputs for pop. The pop population implements a discretized neural field, with neurons following the ODE:\n\\tau \\frac{d r_i(t)}{dt} + r_i(t) = r^\\text{input}_i(t) + \\sum_{j=1}^{N} w_{j, i} \\cdot r_j(t) + \\eta(t)\nwhere r_i(t) is the neuron’s firing rate, \\tau a time constant and w_{j, i} the weight value (synaptic efficiency) of the synapse between the neurons j and i. \\eta(t) is an additive noise uniformly taken in [-0.5, 0.5]. f() is a semi-linear function, ensuring the firing rate is bounded between 0 and 1.\nEach neuron in pop takes inputs from the neuron of inp which has the same position, leading to a one_to_one connection pattern.\nThe lateral connections within pop follow a difference-of-Gaussians (dog) connection pattern, with the connection weights w_{i,j} depending on the normalized euclidian distance between the neurons in the N*N population:\nw_{j, i} = A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_+^2}) -  A^- \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_-^2})\nIf i and j have coordinates (x_i, y_i) and (x_j, y_j) in the N*N space, the distance between them is computed as:\nd(i, j)^2 = (\\frac{x_i - x_j}{N})^2 + (\\frac{y_i - y_j}{N})^2\nInputs are given to the network by changing the firing rate of inp neurons.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#importing-annarchy",
    "href": "notebooks/NeuralField.html#importing-annarchy",
    "title": "Neural Field",
    "section": "Importing ANNarchy",
    "text": "Importing ANNarchy\nWe first start by importing the numpy and ANNarchy libraries:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nThe setup() method allows to configure ANNarchy to run in different modes, such as chosing the parallel framework (omp or cuda), setting the simulation step dt, the numerical method method or the seed of the random number generators.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#defining-the-neuron",
    "href": "notebooks/NeuralField.html#defining-the-neuron",
    "title": "Neural Field",
    "section": "Defining the neuron",
    "text": "Defining the neuron\n\nNeuralFieldNeuron = ann.Neuron(\n    parameters = dict(tau = 10.0),\n    equations = ann.Variable(\n        'tau * dr/dt + r = sum(exc) + sum(inh) + Uniform(-0.5, 0.5)', min=0.0, max=1.0\n    )\n)\n\nThe NeuralField neuron is governed by an ODE and considers inputs from other neurons. It has a non-linear activation function, which is linear when the firing rate is between 0.0 and 1.0, and constant otherwise.\ntau is a population-wise parameter, whose value will be the same for all neurons of the population.\nr is the firing rate of he neuron, whose dynamics are governed by a first-order linear ODE integrating the sums of excitatory and inhibitory inputs with noise.\nsum(exc) retrieves the weighted sum of pre-synaptic firing rates for the synapses having the connection type exc, here the one_to_one connections between inp and pop. sum(inh) does the same for inh type connections, here the lateral connections within pop.\nThe firing rate is restricted to the range [0, 1] by setting the min and max accordingly in the variable definition. This means that after evaluating the ODE and getting a new value for r, its value will be clamped if it outside these values. One can define both min and max, only one of them, or none.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#creating-the-populations",
    "href": "notebooks/NeuralField.html#creating-the-populations",
    "title": "Neural Field",
    "section": "Creating the populations",
    "text": "Creating the populations\nWe first create a network that will hold all the populations and projections.\n\nnet = ann.Network()\n\nThe two populations have a geometry of (20, 20), therefore 400 neurons each. They are created simply by calling create() of the network:\n\nN = 20\n\n# Input population will be an input array\ninp = net.create(ann.InputArray(geometry = (N, N), name='Input'))\n\n# Main population uses NeuralFieldNeuron\npop = net.create(geometry = (N, N), neuron = NeuralFieldNeuron, name='Focus')\n\nThe populations can be assigned a unique name (here ‘Input’ and ‘Focus’) in order to be be able to retrieve them if the references inp and focus are lost, but it is not obligatory.\nThey are given a 2D geometry and associated to the corresponding Neuron instance.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#creating-the-projections",
    "href": "notebooks/NeuralField.html#creating-the-projections",
    "title": "Neural Field",
    "section": "Creating the projections",
    "text": "Creating the projections\nThe first projection is a one-to-one projection from Input to Focus with the type ‘exc’. This connection pattern pattern is possible because the two populations have the same geometry. The weights are initialized to 1.0, and this value will not change with time (no learning), so it is not necessary to define a synapse type:\n\nff = net.connect(pre=inp, post=pop, target='exc')\nff.one_to_one(weights=1.0, delays = 20.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x12492a030&gt;\n\n\nThe references to the pre- and post-synaptic population (or their names), as well as the target type, are passed to the constructor of Projection. The connector method one_to_one() is immediately applied to the Projection, defining how many synapses will be created. The weights are initialized uniformly to 1.0.\nThe second projection is a difference of gaussians (DoG) for the lateral connections within pop. The connector method is already provided by ANNarchy, so there is nothing more to do than to call it with the right parameters:\n\nlat = net.connect(pre=pop, post=pop, target='inh')\nlat.dog(amp_pos=0.2, sigma_pos=0.1, amp_neg=0.1, sigma_neg=0.7)\n\n&lt;ANNarchy.core.Projection.Projection at 0x1258a36b0&gt;\n\n\nWe set two monitors recording the firing rate of the two populations:\n\nm = net.monitor(inp, 'r')\nn = net.monitor(pop, 'r')",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#compiling-the-network-and-simulating",
    "href": "notebooks/NeuralField.html#compiling-the-network-and-simulating",
    "title": "Neural Field",
    "section": "Compiling the network and simulating",
    "text": "Compiling the network and simulating\nOnce the populations and projections are created, the network is ready to be generated, compiled and simulated. Compilation is simply done by calling net.compile():\n\nnet.compile()\n\nCompiling network 1...  OK \n\n\nThis generates optimized C++ code from the neurons’ definition and network structure, compiles it with gcc/clang and instantiates all objects, particularly the synapses. If some errors were made in the neuron definition, they will be signaled at this point.\nHint: The call to compile() is mandatory. After it is called, populations and projections can not be added anymore.\nOnce the compilation is successful, the network can be simulated by calling net.simulate(). As no input has been fed into the network yet, calling net.simulate() now won’t lead to anything interesting. The next step is to clamp inputs into the input population’s baseline.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#setting-inputs",
    "href": "notebooks/NeuralField.html#setting-inputs",
    "title": "Neural Field",
    "section": "Setting inputs",
    "text": "Setting inputs\n\nTight loop\nIn this example, we use a moving bubble of activity rotating along a circle in the 20*20 input space in 1 second (i.e. 1000 steps). The simplest way of setting such inputs is to access population attributes (namely inp.r) in a tight loop in Python:\n\ndef gaussian(angle):\n    \"Unnormalized Gaussian at the specified angle.\"\n    # 20*20 matrices with XY coordinates\n    xx, yy = np.meshgrid(np.linspace(0, N-1, N), np.linspace(0, N-1, N))\n\n    # Compute the center of the bubble\n    cx = N / 2. * ( 1.0 + 0.5 * np.cos(2.0 * np.pi * angle ) )\n    cy = N / 2. * ( 1.0 + 0.5 * np.sin(2.0 * np.pi * angle ) )\n\n    # Gaussian\n    bubble = np.exp(-((xx-cx)**2 + (yy-cy)**2)/8.0)\n    return bubble\n\n# Main loop\nT = 1000\nangle = 0.0\nfor t in range(T):\n\n    # Update the angle\n    angle += 1.0/T\n    \n    # Clamp the firing rate of inp with the Gaussian and some noise\n    inp.r = gaussian(angle) + np.random.uniform(-0.5, 0.5, (N, N))\n    \n    # Simulate for one step = 1 ms\n    net.step()  \n\nangle represents the angle made by the bubble with respect to the center of the input population. At each iteration of the simulation (i.e. every millisecond of simulation), the bubble is slightly rotated (angle is incremented) so as to make a complete revolution in 1 seconds (1000 steps).\nA Gaussian function (in the form of the Numpy array returned by the gaussian() methpd) is then clamped into the activity of inp. Some uniform noise is then added.\nLast, a single simulation step is performed using step(). step() is equivalent to simulate(dt), although a little bit faster as it does not check anything.\nLet’s plot the firing rates of the two populations at the end of the simulation:\n\nplt.figure(figsize=(15, 10))\nplt.subplot(121)\nplt.imshow(inp.r, interpolation='nearest', cmap=plt.cm.gray)\nplt.subplot(122)\nplt.imshow(pop.r, interpolation='nearest', cmap=plt.cm.gray)\nplt.show()\n\n\n\n\n\n\n\n\nWe see a noisy bubble of activity in inp and a clean one in pop, demonstrating the noise-filtering capacities of neural fields.\nLet’s retrieve the data recorded by the monitors, and use Matplotlib animations to show how the firing rates changed during the simulation (other methods are possible):\n\ninp_data = m.get('r')\npop_data = n.get('r')\n\n\n%matplotlib inline\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nfig = plt.figure(figsize=(10,5))\nax1 = plt.subplot(1,2,1)   \nax2 = plt.subplot(1,2,2)\n\nim1 = ax1.imshow(inp.r, interpolation='nearest', cmap=plt.cm.gray)\nim2 = ax2.imshow(pop.r, interpolation='nearest', cmap=plt.cm.gray)\n\ndef drawframe(n):\n    im1.set_data(inp_data[n].reshape((N, N)))\n    im2.set_data(pop_data[n].reshape((N, N)))    \n    return (im1, im2)\n\nanim = animation.FuncAnimation(fig, drawframe, frames=T, interval=2, blit=True)\nplt.close()\nHTML(anim.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nEverything works as expected. However, the simulation is actually quite slow (even if you do not notice it here), as Python is bad at tight loops like this one. For longer simulations, the overhead of python might become too damaging.\n\n\nTimedArray\nA much more efficient variant is to precompute the input rates and store them in an array that will be iteratively read by the TimedArrayobject. This is a new population that cannot be added to the network anymore, as we have already compiled it. Let’s create a new network and redefine everything, replacing the InputArray with a TimedArray of the same shape:\n\nnet = ann.Network()\n\ninp = net.create(ann.TimedArray(geometry = (N, N), name='Input'))\npop = net.create(geometry = (N, N), neuron = NeuralFieldNeuron, name='Focus')\n\nff = net.connect(pre=inp, post=pop, target='exc')\nff.one_to_one(weights=1.0, delays = 20.0)\n\nlat = net.connect(pre=pop, post=pop, target='inh')\nlat.dog(amp_pos=0.2, sigma_pos=0.1, amp_neg=0.1, sigma_neg=0.7)\n\nm = net.monitor(inp, 'r')\nn = net.monitor(pop, 'r')\n\nnet.compile()\n\nCompiling network 2...  OK \n\n\nWe now create a numpy array of shape (T, N, N) where T is the duration of the simulation (1000 steps) and N the dimension of the neural field. We make sure to add noise to the inputs.\nWe then set the rates parameter of the TimedArrayto that array by using update(). At each step of the simulation, the timed array will “read” a new input activity in that array and store it as r. We simply need to call simulate()for the whole duration of the simulation, instead of using a slow tight loop in Python.\n\ndef gaussian(angle):\n    \"Unnormalized Gaussian at the specified angle.\"\n    # 20*20 matrices with XY coordinates\n    xx, yy = np.meshgrid(np.linspace(0, N-1, N), np.linspace(0, N-1, N))\n\n    # Compute the center of the bubble\n    cx = N / 2. * ( 1.0 + 0.5 * np.cos(2.0 * np.pi * angle ) )\n    cy = N / 2. * ( 1.0 + 0.5 * np.sin(2.0 * np.pi * angle ) )\n\n    # Gaussian\n    bubble = np.exp(-((xx-cx)**2 + (yy-cy)**2)/8.0)\n    return bubble\n\n# Precompute the angles for 1000 steps = 1 cycle\nT = 1000\nangles = np.linspace(0, 1, T)\n\n# Accumulate the 1000 inputs with time as the first dimension\ninput_rates = np.array([gaussian(angle) + np.random.uniform(-0.5, 0.5, (N, N)) for angle in angles])\n\n# Set it as the rates argument of the input population\ninp.update(input_rates)\n\n# Simulate for 1 second\nnet.simulate(T)\n\n# Retrieve the recordings\ninp_data = m.get('r')\npop_data = n.get('r')\n\n\n%matplotlib inline\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nfig = plt.figure(figsize=(10,5))\nax1 = plt.subplot(1,2,1)   \nax2 = plt.subplot(1,2,2)\n\nim1 = ax1.imshow(inp.r, interpolation='nearest', cmap=plt.cm.gray)\nim2 = ax2.imshow(pop.r, interpolation='nearest', cmap=plt.cm.gray)\n\ndef drawframe(n):\n    im1.set_data(inp_data[n].reshape((N, N)))\n    im2.set_data(pop_data[n].reshape((N, N)))    \n    return (im1, im2)\n\nanim = animation.FuncAnimation(fig, drawframe, frames=T, interval=2, blit=True)\nplt.close()\nHTML(anim.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/COBA.html",
    "href": "notebooks/COBA.html",
    "title": "COBA and CUBA networks",
    "section": "",
    "text": "#!pip install ANNarchy\nThis notebook reproduces the benchmarks used in:\nThey are based on the balanced network proposed by:\nEach network is composed of 4000 neurons (3200 excitatory and 800 inhibitory), reciprocally connected with a probability of 0.02 (sparse connection).\nThe CUBA network uses a current-based integrate-and-fire neuron model:\n\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) - g_\\text{inh} (t)\nwhile the COBA model uses conductance-based IF neurons:\n\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) * (E_\\text{exc} - v(t)) + g_\\text{inh} (t) * (E_\\text{inh} - v(t)) + I(t)\nApart from the neuron model and synaptic weights, both networks are equal, so we’ll focus on the COBA network here.\nThe discretization step has to be set to 0.1 ms:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "COBA/CUBA"
    ]
  },
  {
    "objectID": "notebooks/COBA.html#neuron-definition",
    "href": "notebooks/COBA.html#neuron-definition",
    "title": "COBA and CUBA networks",
    "section": "Neuron definition",
    "text": "Neuron definition\n\nCOBA = ann.Neuron(\n    parameters = dict(\n        El = -60.0,\n        Vr = -60.0,\n        Erev_exc = 0.0,\n        Erev_inh = -80.0,\n        Vt = -50.0,\n        tau = 20.0,\n        tau_exc = 5.0,\n        tau_inh = 10.0,\n        I = 20.0,\n    ),\n    equations=[\n        'tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I',\n        'tau_exc * dg_exc/dt = - g_exc',\n        'tau_inh * dg_inh/dt = - g_inh',\n    ],\n    spike = \"v &gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\n\nCUBA = ann.Neuron(\n    parameters = dict(\n        El = -49.0,\n        Vr = -60.0,\n        Vt = -50.0,\n        tau_m = 20.0,\n        tau_exc = 5.0,\n        tau_inh = 10.0,\n    ),\n    equations = [\n        'tau_m * dv/dt = (El - v) + g_exc - g_inh',\n        'tau_exc * dg_exc/dt = - g_exc',\n        'tau_inh * dg_inh/dt = - g_inh',\n    ],\n    spike = \"v &gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\nThe neurons define exponentially-decreasing conductance g_exc and g_inh for the excitatory and inhibitory conductances/currents, respectively. They also define a refractory period of 5 ms.",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "COBA/CUBA"
    ]
  },
  {
    "objectID": "notebooks/COBA.html#networks",
    "href": "notebooks/COBA.html#networks",
    "title": "COBA and CUBA networks",
    "section": "Networks",
    "text": "Networks\nEach network creates a population P of 4000 homogeneous neurons. It is implicitly split into excitatory (80%) and inhibitory (20%) subpopulations using PopulationViews and slicing.\nThe neurons are randomly connected with a probability of 0.02. Excitatory neurons project on all other neurons with the target “exc”, while the inhibitory neurons have the target “inh”. The weights values are uniform in all projections.\nA monitor records the spikes in the whole population.\n\nclass COBANetwork (ann.Network):\n    \n    def __init__(self, w_exc, w_inh):\n\n        self.P = self.create(geometry=4000, neuron=COBA)\n        self.P.v     = ann.Normal(-55.0, 5.0)\n        self.P.g_exc = ann.Normal(4.0,   1.5)\n        self.P.g_inh = ann.Normal(20.0, 12.0)\n\n        self.Ce = self.connect(pre=self.P[:3200], post=self.P, target='exc')\n        self.Ce.fixed_probability(weights=w_exc, probability=0.02)\n\n        self.Ci = self.connect(pre=self.P[3200:], post=self.P, target='inh')\n        self.Ci.fixed_probability(weights=w_inh, probability=0.02)\n\n        self.m = self.monitor(self.P, ['spike'])\n\n\nclass CUBANetwork (ann.Network):\n    \n    def __init__(self, w_exc, w_inh):\n\n        self.P = self.create(geometry=4000, neuron=CUBA)\n        self.P.v = ann.Uniform(-60.0, -50.0)\n\n        self.Ce = self.connect(pre=self.P[:3200], post=self.P, target='exc')\n        self.Ce.fixed_probability(weights=w_exc, probability=0.02)\n\n        self.Ci = self.connect(pre=self.P[3200:], post=self.P, target='inh')\n        self.Ci.fixed_probability(weights=w_inh, probability=0.02)\n\n        self.m = self.monitor(self.P, ['spike'])\n\n\n# COBA network\nwe_COBA = 0.6\nwi_COBA = 6.7\nnet_coba = COBANetwork(w_exc=we_COBA, w_inh=wi_COBA, dt=0.1)\nnet_coba.compile()\n\n# CUBA network\nwe_CUBA = 0.27 * 60.0 / 10.0 # 0.7 * (Vmean - E_rev_exc) / gL (mV)\nwi_CUBA = 4.5 * 20.0 / 10.0 # 4.5 * (Vmean - E_rev_inh) / gL (mV)\nnet_cuba = CUBANetwork(w_exc = we_CUBA, w_inh=wi_CUBA, dt=0.1)\nnet_cuba.compile()\n\nCompiling network 2...  OK",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "COBA/CUBA"
    ]
  },
  {
    "objectID": "notebooks/COBA.html#simulation",
    "href": "notebooks/COBA.html#simulation",
    "title": "COBA and CUBA networks",
    "section": "Simulation",
    "text": "Simulation\nWe can now simulate each network for 1 second:\n\nnet_coba.simulate(1000., measure_time=True)\nnet_cuba.simulate(1000., measure_time=True)\n\nSimulating 1.0 seconds of the network 1 took 0.12989521026611328 seconds. \nSimulating 1.0 seconds of the network 2 took 0.08427762985229492 seconds. \n\n\nWe retrieve the recorded spikes from the monitor:\n\ndata_COBA = net_coba.m.get('spike')\ndata_CUBA = net_cuba.m.get('spike')\n\nand compute a raster plot from the data:\n\nt_COBA, n_COBA = net_coba.m.raster_plot(data_COBA)\nt_CUBA, n_CUBA = net_cuba.m.raster_plot(data_CUBA)\n\nt and n are lists representing for each spike emitted during the simulation the time at which it was emitted and the index the neuron which fired. The length of this list represents the total number of spikes in the popultion, so we can compute the population mean firing rate:\n\nprint('Mean firing rate in the COBA population: ' + str(len(t_COBA) / 4000.) + 'Hz')\nprint('Mean firing rate in the CUBA population: ' + str(len(t_CUBA) / 4000.) + 'Hz')\n\nMean firing rate in the COBA population: 20.62775Hz\nMean firing rate in the CUBA population: 5.71275Hz\n\n\nFinally, we can show the raster plot with matplotlib:\n\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.title(\"COBA\")\nplt.plot(t_COBA, n_COBA, '.', markersize=0.5)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.subplot(122)\nplt.title(\"CUBA\")\nplt.plot(t_CUBA, n_CUBA, '.', markersize=0.5)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\n\n\n\n\n\nMore detailed information about the activity of the population is provided by the inter-spike interval and the coefficient of variation, for both of which values we offer methods provided by the Monitor class.\n\nisi_COBA = net_coba.m.inter_spike_interval(data_COBA)\nisi_CUBA = net_cuba.m.inter_spike_interval(data_CUBA)\n\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.title(\"COBA\")\nplt.hist(isi_COBA)\nplt.xlabel('ISI (ms)')\nplt.ylabel('n in bin')\nplt.subplot(122)\nplt.title(\"CUBA\")\nplt.hist(isi_CUBA)\nplt.xlabel('ISI (ms)')\nplt.ylabel('n in bin')\nplt.show()\n\n\n\n\n\n\n\n\n\ncov_COBA = net_coba.m.coefficient_of_variation(data_COBA)\ncov_CUBA = net_cuba.m.coefficient_of_variation(data_CUBA)\n\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.title(\"COBA\")\nplt.hist(cov_COBA)\nplt.xlabel('ISI CV')\nplt.ylabel('n in bin')\nplt.subplot(122)\nplt.title(\"CUBA\")\nplt.hist(cov_CUBA)\nplt.xlabel('ISI CV')\nplt.ylabel('n in bin')\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "COBA/CUBA"
    ]
  },
  {
    "objectID": "notebooks/Image.html",
    "href": "notebooks/Image.html",
    "title": "Convolutions and pooling",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis simple example in examples/image demonstrates how to load images directly into the firing rates of a population and apply basic linear filters on it.\nIt relies on the ANNarchy extensions image and convolution which must be explicitly imported:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ANNarchy as ann\n\nfrom ANNarchy.extensions.image import ImagePopulation\nfrom ANNarchy.extensions.convolution import Convolution, Pooling\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nANNarchy.extensions.image depends on the Python bindings of OpenCV, they must be installed before running the script.\nWe first create an ImagePopulation that will load images:\n\nnet = ann.Network()\nimage = net.create(ImagePopulation(geometry=(480, 640, 3)))\n\nIts geometry specifies the size of the images that can be loaded, here 640x480 RGB images. Note the geometry must be of the form (height, width, channels), where channels is 1 for grayscale images and 3 for color images.\nThe next step is to reduce the size of the image, what can be done by using the Pooling class of the convolution extension.\nWe define a dummy artificial neuron, whose firing rate r will simply be the sum of excitatory connections /ensured to be positive, but this should always be the case). We then create a smaller population pooled with this neuron type, and connect it to the ImagePopulation using mean-pooling:\n\n# Simple ANN\nLinearNeuron = ann.Neuron(equations=ann.Variable(\"r=sum(exc)\",min=0.0))\n\n# Subsampling population\npooled = net.create(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Mean-pooling projection\npool_proj = net.connect(Pooling(pre=image, post=pooled, target='exc', operation='mean'))\npool_proj.pooling()\n\n&lt;ANNarchy.extensions.convolution.Pooling.Pooling at 0x11f22da00&gt;\n\n\nThe pooled population reduces the size of the image by a factor ten (defined by the size of the population) by averaging the pixels values over 10x10 regions (operation is set to 'mean', but one could use 'max' or 'min'). The pooling() connector creates the “fake” connection pattern (as no weights are involved).\nLet’s apply now a 3x3 box filter on each channel of the pooled population:\n\n# Smoothing population\nsmoothed = net.create(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Box filter projection\nbox_filter = np.ones((3, 3, 1))/9.\nsmooth_proj = net.connect(Convolution(pre=pooled, post=smoothed, target='exc'))\nsmooth_proj.connect_filter(weights=box_filter)\n\n&lt;ANNarchy.extensions.convolution.Convolve.Convolution at 0x11df4ba40&gt;\n\n\nTo perform a convolution operation on the population (or more precisely a cross-correlation), we call the connect_filter() connector method of the Convolution projection. It requires to define a kernel (weights) that will be convolved over the input population. Here we use a simple box filter, but any filter can be used.\nAs the pooled population has three dimensions and we want to smooth the activities per color channel, we need to define a (3, 3, 1) kernel. If we wanted to smooth also over the color channels, we could have used a (3, 3) filter: the resulting population would have the shape (48, 64).\nWe now apply a bank of three filters, each selective to a particular color (red/green/blue). This filters do not have a spatial extent (1x1 convolution), but sum over the third dimension (the color channels):\n\n# Convolution population    \nfiltered = net.create(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Red/Green/Blue filter bank\nfilter_bank = np.array([ \n    [[ [2.0, -1.0, -1.0] ]] , # Red filter \n    [[ [-1.0, 2.0, -1.0] ]] , # Blue filter\n    [[ [-1.0, -1.0, 2.0] ]]   # Green filter\n])\nfilter_proj = net.connect(Convolution(pre=smoothed, post=filtered, target='exc'))\nfilter_proj.connect_filters(weights=filter_bank)\n\n&lt;ANNarchy.extensions.convolution.Convolve.Convolution at 0x103fb4ad0&gt;\n\n\nEach of the three filter has the shape (1, 1, 3). The result of each convolution would then be (48, 64), but as there are three filters, the output population is (48, 64, 3). The last dimension does not correspond to the number of color channels, but to the number of filters in the bank: if you add a filter, the population will have to be (48, 64, 4).\nBanks of filters require to use connect_filters() instead of connect_filter().\n\nnet.compile()\n\nCompiling network 1...  OK \n\n\nAfter compilation, we can load an image into the input population:\n\nimage.set_image('test.jpg')\n\nTo see the result, we need to simulate for four time steps (4 milliseconds, as dt=1.0).\n\nStep 1: The image population loads the image.\nStep 2: The pooled population subsamples the image.\nStep 3: The smoothed population filters the pooled image.\nStep 4: The bank of filters are applied by filtered.\n\n\nnet.simulate(4.0)\n\n\nfig = plt.figure(figsize=(15, 20))\n\nplt.subplot(532)\nplt.imshow(image.r)\nplt.title('Original')\n\nplt.subplot(534)\nplt.imshow(image.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('image R')\nplt.subplot(535)\nplt.imshow(image.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('image G')\nplt.subplot(536)\nplt.imshow(image.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('image B')\n\nplt.subplot(537)\nplt.imshow(pooled.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('pooled R')\nplt.subplot(538)\nplt.imshow(pooled.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('pooled G')\nplt.subplot(539)\nplt.imshow(pooled.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('pooled B')\n\nplt.subplot(5, 3, 10)\nplt.imshow(smoothed.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('smoothed R')\nplt.subplot(5, 3, 11)\nplt.imshow(smoothed.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('smoothed G')\nplt.subplot(5, 3, 12)\nplt.imshow(smoothed.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('smoothed B')\n\nplt.subplot(5, 3, 13)\nplt.imshow(filtered.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('filtered R')\nplt.subplot(5, 3, 14)\nplt.imshow(filtered.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('filtered G')\nplt.subplot(5, 3, 15)\nplt.imshow(filtered.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('filtered B')\n\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "Image"
    ]
  },
  {
    "objectID": "notebooks/Izhikevich.html",
    "href": "notebooks/Izhikevich.html",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "",
    "text": "#!pip install ANNarchy\nThis script reproduces the simple pulse-coupled network proposed by Eugene Izhikevich in the article:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\nThe original Matlab code is provided below:",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "notebooks/Izhikevich.html#neuron-type",
    "href": "notebooks/Izhikevich.html#neuron-type",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "Neuron type",
    "text": "Neuron type\nThe network is composed of parameterized quadratic integrate-and-fire neurons, known as Izhikevich neurons. They are simply defined by the following equations:\n\n    \\frac{dv(t)}{dt} = 0.04 \\, v^2(t) + 5 \\, v(t) + 140 - u(t) + I(t)\n\n\n    \\frac{du(t)}{dt} = a \\, (b \\, v(t) - u)\n\nwith v(t) representing the membrane potential and u(t) the recovery variable. The spiking mechanism is defined by a threshold on v(t), so that it emits a spike when it exceeds V_T = 30 mV. The membrane potential is reset to c and the recovery variable is incremented from d\n\n    \\text{if} \\; v(t) &gt; V_T : \\; \\begin{cases}\n        \\text{emit a spike} \\\\\n        v(t) \\leftarrow c \\\\\n        u(t) \\leftarrow u(t) + d \\\\\n    \\end{cases}\n\na, b, c, d are parameters allowing to reproduce many types of neural firing.\nI(t) is the input voltage to a neuron at each time step t. For the desired network, it is the sum of a random value taken from a normal distribution with mean 0.0 and variance 1.0 (multiplied by a scaling factor) and the net effect of incoming spikes (excitatory and inhibitory).\nImplementing such a neuron in ANNarchy is straightforward:\n\nIzhikevich = ann.Neuron(\n    parameters=dict(\n        noise = ann.Parameter(5.0), \n        a = ann.Parameter(0.02), \n        b = ann.Parameter(0.2), \n        c = ann.Parameter(-65.0), \n        d = ann.Parameter(2.0), \n        v_thresh = 30.0\n    ),\n    equations = [\n        # Input current\n        'I = g_exc - g_inh + noise * Normal(0.0, 1.0)',\n\n        # Membrane potential\n        'dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I',\n\n        # Recovery variable\n        'du/dt = a * (b*v - u)',\n    ],\n    spike = \" v &gt;= v_thresh \",\n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n)\n\nThe parameters a, b, c, d as well as the noise amplitude noise are declared in the parameters argument, as their value is constant during the simulation. However, each neuron will have a different value for these parameters, so they are declared using ann.Parameter().\nThe equations for v and u are direct translations of their mathematical counterparts. Note the use of dx/dt for the time derivative and ^2 for the square function.\nThe input voltage I is defined as the sum of:\n\nthe total conductance of excitatory synapses g_exc,\nthe total conductance of inhibitory synapses -g_inh (in this example, we consider all weights to be positive, so we need to invert g_inh in order to model inhibitory synapses),\na random number taken from the normal distribution N(0,1) and multiplied by the noise scale noise.\n\nIn the pulse-coupled network, synapses are considered as instantaneous, i.e. a pre-synaptic spikes increases immediately the post-synaptic conductance proportionally to the weight of the synapse, but does not leave further trace. As this is the default behavior in ANNarchy, nothing has to be specified in the neuron’s equations.\nThe spike argument specifies the condition for when a spike should be emitted (here the membrane potential v should be greater than v_thresh). The reset argument specifies the changes to neural variables that should occur after a spike is emitted: here, the membrane potential is reset to the resting potential c and the membrane recovery variable u is increased from d.",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "notebooks/Izhikevich.html#defining-the-populations",
    "href": "notebooks/Izhikevich.html#defining-the-populations",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "Defining the populations",
    "text": "Defining the populations\nWe start by defining a population of 1000 Izhikevich neurons and split it into 800 excitatory neurons and 200 inhibitory ones:\n\nnet = ann.Network()\n\npop = net.create(geometry=1000, neuron=Izhikevich)\n\nExc = pop[:800]\nInh = pop[800:]\n\nExc and Inh are subsets of pop, which have the same properties as a population. We can then set parameters differently for each population:\n\nre = np.random.random(800)      ; ri = np.random.random(200)\nExc.noise = 5.0                 ; Inh.noise = 2.0\nExc.a = 0.02                    ; Inh.a = 0.02 + 0.08 * ri\nExc.b = 0.2                     ; Inh.b = 0.25 - 0.05 * ri\nExc.c = -65.0 + 15.0 * re**2    ; Inh.c = -65.0\nExc.d = 8.0 - 6.0 * re**2       ; Inh.d = 2.0\nExc.v = -65.0                   ; Inh.v = -65.0\nExc.u = Exc.v * Exc.b           ; Inh.u = Inh.v * Inh.b",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "notebooks/Izhikevich.html#defining-the-projections",
    "href": "notebooks/Izhikevich.html#defining-the-projections",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "Defining the projections",
    "text": "Defining the projections\nWe can now define the connections within the network:\n\nThe excitatory neurons are connected to all neurons with a weight randomly chosen in [0, 0.5]\nThe inhibitory neurons are connected to all neurons with a weight randomly chosen in [0, 1]\n\n\nexc_proj = net.connect(pre=Exc, post=pop, target='exc')\nexc_proj.all_to_all(weights=ann.Uniform(0.0, 0.5))\n   \ninh_proj = net.connect(pre=Inh, post=pop, target='inh')\ninh_proj.all_to_all(weights=ann.Uniform(0.0, 1.0))\n\n&lt;ANNarchy.core.Projection.Projection at 0x122ce2f90&gt;\n\n\nThe network is now ready, we can compile:\n\nnet.compile()\n\nCompiling network 1...  OK",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "notebooks/Izhikevich.html#running-the-simulation",
    "href": "notebooks/Izhikevich.html#running-the-simulation",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "Running the simulation",
    "text": "Running the simulation\nWe start by monitoring the spikes and membrane potential in the whole population:\n\nm = net.monitor(pop, ['spike', 'v'])\n\nWe run the simulation for 1000 milliseconds:\n\nnet.simulate(1000.0, measure_time=True)\n\nSimulating 1.0 seconds of the network 1 took 0.08657598495483398 seconds. \n\n\nWe retrieve the recordings, generate a raster plot and the population firing rate:\n\nspikes = m.get('spike')\nv = m.get('v')\nt, n = m.raster_plot(spikes)\nfr = m.histogram(spikes)\n\nWe plot:\n\nThe raster plot of population\nThe evolution of the membrane potential of a single excitatory neuron\nThe population firing rate\n\n\nplt.figure(figsize=(12, 12))\n\n# First plot: raster plot\nplt.subplot(311)\nplt.plot(t, n, 'b.')\nplt.title('Raster plot')\n\n# Second plot: membrane potential of a single excitatory cell\nplt.subplot(312)\nplt.plot(v[:, 15]) # for example\nplt.title('Membrane potential')\n\n# Third plot: number of spikes per step in the population.\nplt.subplot(313)\nplt.plot(fr)\nplt.title('Number of spikes')\nplt.xlabel('Time (ms)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can observe how the pusle-coupled network oscillates first at a low frequency, before emitting higher-frequency oscillations.",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "notebooks/BoldMonitoring2.html",
    "href": "notebooks/BoldMonitoring2.html",
    "title": "Recording BOLD signals - Davis model",
    "section": "",
    "text": "#!pip install ANNarchy\nimport numpy as np\n\nimport ANNarchy as ann\nimport ANNarchy.extensions.bold as bold\n\nANNarchy 5.0 (5.0.0) on darwin (posix).",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "BOLD monitor II"
    ]
  },
  {
    "objectID": "notebooks/BoldMonitoring2.html#davis-model",
    "href": "notebooks/BoldMonitoring2.html#davis-model",
    "title": "Recording BOLD signals - Davis model",
    "section": "Davis model",
    "text": "Davis model\nLet’s now demonstrate how to define a custom BOLD model. The default Ballon model is defined by the following code:\nballoon_RN = BoldModel(\n    parameters = dict(\n        phi       = 1.0         ,   kappa     = 1/1.54  ,\n        gamma     = 1/2.46      ,   E_0       = 0.34    ,\n        tau       = 0.98        ,   alpha     = 0.33    ,\n        V_0       = 0.02        ,   v_0       = 40.3    ,\n        TE        = 40/1000.    ,   epsilon   = 1.43    ,\n        r_0       = 25.         ,   second    = 1000.0  ,\n    ),\n    equations = [\n        # CBF input\n        'I_CBF = sum(I_CBF)',       \n        'ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second', \n        ann.Variable('df_in/dt = s / second', init=1, min=0.01),\n\n        # Balloon model\n        ann.Variable('E  = 1 - (1 - E_0)**(1 / f_in)', init=0.3424),\n        ann.Variable('dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)', init=1, min=0.01),\n        ann.Variable('dv/dt = (f_in - f_out)/(tau*second)', init=1, min=0.01),\n        ann.Variable('f_out = v**(1 / alpha)', init=1, min=0.01),\n\n        # Revised coefficients\n        'k_1 = 4.3 * v_0 * E_0 * TE',\n        'k_2 = epsilon * r_0 * E_0 * TE',\n        'k_3 = 1.0 - epsilon',\n\n        # Non-linear BOLD equation\n        'BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))',\n    ],\n    inputs=['I_CBF']\n)\nIt is very similar to the interface of a Neuron model, with parameters and equations defined in two multi-line strings. The input signal I_CBF has to be explicitly defined in the inputs argument to help the BOLD monitor create the mapping.\nTo demonstrate how to create a custom BOLD model, let’s suppose we want a model that computes both the BOLD signal of the Balloon model and the one of the Davis model:\n\nDavis, T. L., Kwong, K. K., Weisskoff, R. M., and Rosen, B. R. (1998). Calibrated functional MRI: mapping the dynamics of oxidative metabolism. Proceedings of the National Academy of Sciences 95, 1834–1839\n\nWithout going into too many details, the Davis model computes the BOLD signal directly using f_in and E, without introducing a differential equation for the BOLD signal. Its implementation using the BOLD model would be:\nDavisModel = BoldModel(\n    parameters = dict(\n        second = 1000.0,\n        \n        phi    = 1.0,    # Friston et al. (2000)\n        kappa  = 1/1.54,\n        gamma  = 1/2.46,\n        E_0    = 0.34,\n        \n        M      = 0.149,   # Griffeth & Buxton (2011)\n        alpha  = 0.14,\n        beta   = 0.91,\n    ),\n    equations = [\n        \n        # CBF-driving input as in Friston et al. (2000)\n        'I_CBF = sum(I_CBF)',\n        'ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second',\n        ann.Variable('df_in/dt = s  / second', init=1, min=0.01),\n    ​\n        # Using part of the Balloon model to calculate r (normalized CMRO2) as in Buxton et al. (2004)\n        ann.Variable('E = 1 - (1 - E_0)**(1 / f_in)', init=0.34),\n        ann.Variable('r = f_in * E / E_0'),\n        \n        # Davis model\n        'BOLD = M * (1 - f_in**alpha * (r / f_in)**beta)',\n    ],\n    inputs=['I_CBF']\n)\nNote that we could simply define two BOLD monitors using different models, but let’s create a complex model that does both for the sake of demonstration.\nLet’s first redefine the populations of the previous notebook:\n\n# Network\nnet = ann.Network()\n\n# Two populations of 100 izhikevich neurons\npop1 = net.create(100, neuron=ann.Izhikevich)\npop2 = net.create(100, neuron=ann.Izhikevich)\n\n# Set noise to create some baseline activity\npop1.noise = 5.0; pop2.noise = 5.0\n\n# Compute mean firing rate in Hz on 100ms window\npop1.compute_firing_rate(window=100.0)\npop2.compute_firing_rate(window=100.0)\n\n# Create required monitors\nmon_pop1 = net.monitor(pop1, [\"r\"], start=False)\nmon_pop2 = net.monitor(pop2, [\"r\"], start=False)\n\nWe can now create a hybrid model computing both the Balloon RN model of Stephan et al. (2007) and the Davis model:\n\nballoon_Davis = bold.BoldModel(\n    parameters = dict(\n        phi       = 1.0         ,   kappa     = 1/1.54 ,\n        gamma     = 1/2.46      ,   E_0       = 0.34   ,\n        tau       = 0.98        ,   alpha     = 0.33   ,\n        V_0       = 0.02        ,   v_0       = 40.3   ,\n        TE        = 40/1000.    ,   epsilon   = 1.43   ,\n        r_0       = 25.         ,   second    = 1000.0 ,\n        M         = 0.062       ,   alpha2    = 0.14   ,\n        beta      = 0.91\n    ),\n    equations = [\n\n        # CBF input\n        'I_CBF = sum(I_CBF)',       \n        'ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second', \n        ann.Variable('df_in/dt = s / second', init=1, min=0.01),\n\n        # Balloon model\n        ann.Variable('E  = 1 - (1 - E_0)**(1 / f_in)', init=0.3424),\n        ann.Variable('dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)', init=1, min=0.01),\n        ann.Variable('dv/dt = (f_in - f_out)/(tau*second)', init=1, min=0.01),\n        ann.Variable('f_out = v**(1 / alpha)', init=1, min=0.01),\n\n        # Revised coefficients\n        'k_1 = 4.3 * v_0 * E_0 * TE',\n        'k_2 = epsilon * r_0 * E_0 * TE',\n        'k_3 = 1.0 - epsilon',\n\n        # Non-linear BOLD equation\n        'BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))',\n        \n        # Davis model\n        ann.Variable('r = f_in * E / E_0', init=1, min=0.01),\n        'BOLD_Davis =  M * (1 - f_in**alpha2 * (r / f_in)**beta)',\n    ],\n    inputs=['I_CBF']\n)\n\nWe now only need to pass that new object to the BOLD monitor, and specify that we want to record both BOLD and BOLD_Davis:\n\nm_bold = net.boldmonitor(\n    # Recorded populations\n    populations = [pop1, pop2], \n    # BOLD model to use\n    bold_model = balloon_Davis, \n    # Mapping from pop.r to I_CBF\n    mapping = {'I_CBF': 'r'}, \n    # Time window to compute the baseline.\n    normalize_input = 2000,  \n    # Variables to be recorded\n    recorded_variables = [\"I_CBF\", \"BOLD\", \"BOLD_Davis\"],\n)\n\nnet.compile()\n\nCompiling network 1...  OK \n\n\nWe run the same simulation protocol and compare the two BOLD signals. Note that the value of M has been modified to give a similar amplitude to both signals:\n\n# Ramp up time\nnet.simulate(1000)\n\n# Start recording\nmon_pop1.start()\nmon_pop2.start()\nm_bold.start()\n\n# we manipulate the noise for the half of the neurons\nnet.simulate(5000)      # 5s with low noise\npop1.noise = 7.5\nnet.simulate(5000)      # 5s with higher noise (one population)\npop1.noise = 5\nnet.simulate(10000)     # 10s with low noise\n\n# retrieve the recordings\nmean_fr1 = np.mean(mon_pop1.get(\"r\"), axis=1)\nmean_fr2 = np.mean(mon_pop2.get(\"r\"), axis=1)\n\nIf_data = m_bold.get(\"I_CBF\")\nbold_data = m_bold.get(\"BOLD\")\ndavis_data = m_bold.get(\"BOLD_Davis\")\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 5))\n\n# mean firing rate\nax1 = plt.subplot(121)\nax1.plot(mean_fr1, label=\"pop0\")\nax1.plot(mean_fr2, label=\"pop1\")\nplt.legend()\nax1.set_ylabel(\"Average firing rate [Hz]\")\n\n# BOLD input signal as percent\nax2 = plt.subplot(122)\nax2.plot(bold_data*100.0, label=\"Balloon_RN\")\nax2.plot(davis_data*100.0, label=\"Davis\")\nplt.legend()\nax2.set_ylabel(\"BOLD [%]\")\n\n# x-axis labels as seconds\nfor ax in [ax1, ax2]:\n    ax.set_xticks(np.arange(0,21,2)*1000)\n    ax.set_xticklabels(np.arange(0,21,2))\n    ax.set_xlabel(\"time [s]\")\n\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "BOLD monitor II"
    ]
  },
  {
    "objectID": "notebooks/MultipleNetworks.html",
    "href": "notebooks/MultipleNetworks.html",
    "title": "Parallel simulations",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis example demonstrates the use of parallel_run() to simulate the same network multiple times in parallel.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ANNarchy as ann\n\nimport time\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nParallel simulations use the multiprocessing module to start parallel processes. On Linux, it should work directly, but there is an issue on OSX. Since Python 3.8, the ‘spawn’ method is the default way to start processes, but it does not work on MacOS. The following cell should fix the issue, but it should only be ran once.\n\nimport platform\nif platform.system() == \"Darwin\":\n    import multiprocessing as mp\n    mp.set_start_method('fork')\n\nWe start by creating the Izhikevich pulse-coupled network defined in Izhikevich.ipynb, but using a class inheriting from ann.Network:\n\nclass PulseNetwork(ann.Network):\n\n    def __init__(self, size=1000):\n\n        self.rng = np.random.default_rng(seed=self.seed)\n\n        # Create the population\n        self.P = self.create(geometry=size, neuron=ann.Izhikevich)\n\n        # Create the excitatory population\n        nb_exc = int(0.8*size)\n        self.Exc = self.P[:nb_exc]\n        re = self.rng.random(nb_exc)\n        self.Exc.noise = 5.0\n        self.Exc.a = 0.02\n        self.Exc.b = 0.2\n        self.Exc.c = -65.0 + 15.0 * re**2\n        self.Exc.d = 8.0 - 6.0 * re**2\n        self.Exc.v = -65.0\n        self.Exc.u = self.Exc.v * self.Exc.b\n\n        # Create the Inh population\n        self.Inh = self.P[nb_exc:]\n        ri = self.rng.random(size - nb_exc)\n        self.Inh.noise = 2.0\n        self.Inh.a = 0.02 + 0.08 * ri\n        self.Inh.b = 0.25 - 0.05 * ri\n        self.Inh.c = -65.0\n        self.Inh.d = 2.0\n        self.Inh.v = -65.0\n        self.Inh.u = self.Inh.v * self.Inh.b\n\n        # Create the projections\n        self.proj_exc = self.connect(self.Exc, self.P, 'exc')\n        self.proj_inh = self.connect(self.Inh, self.P, 'inh')\n\n        self.proj_exc.all_to_all(weights=ann.Uniform(0.0, 0.5, self.rng))\n        self.proj_inh.all_to_all(weights=ann.Uniform(0.0, 1.0, self.rng))\n\n        # Create a spike monitor\n        self.M = self.monitor(self.P, 'spike')\n\nWe create such a network, compile it and define a simulation method. Here, it is only a simple simulation, while the methods returns the recorded spike trains.\n\n# Create network\nnet = PulseNetwork(1000)\nnet.config(dt=1.0)\nnet.compile()\n\n# Single trial\ndef run(net, duration=1000):\n    # Simulate\n    net.simulate(duration)\n    # Recordings\n    t, n = net.M.raster_plot()\n    return t, n\n\nt, n = run(net, 1000)\n\nplt.figure()\nplt.plot(t, n, '.')\nplt.show()\n\nCompiling network 1...  OK \n\n\n\n\n\n\n\n\n\nUsing the parallel_run methods of Network allows to reinitialize the network and run the simulation multiple times in parallel, with different durations. The results of each simulation are concatenated and returned.\nWe make sure that the seeds of each network are different by setting the value 'sequential'. None or a list of seeds would also work.\nNote how, in the network definition, the rng of the random distributions was set using the seed of the network.\n\nresults = net.parallel_run(\n    method=run, \n    number=8, \n    seeds='sequential', \n    duration=[200 * (i+1) for i in range(8)]\n)\n\nWe can then plot the 8 simulations\n\nplt.figure(figsize=(15, 15))\nfor i in range(8):\n    t, n = results[i]\n    plt.subplot(4, 2, i+1)\n    plt.plot(t, n, '.')\nplt.show()\n\n\n\n\n\n\n\n\nAs a side note, it is important to define the network as a subclass of ann.Network, not just as an instance of ann.Network() where populations are created manually. parallel_run() needs to create copies of the network instance, which is only possible if the constructor of the class has all the required information to build those instances.\nThe following code should crash:\n\nnet = ann.Network()\n\npop = net.create(geometry=1000, neuron=ann.Izhikevich)\nExc = pop[:800]\nInh = pop[800:]\n\nre = np.random.random(800)      ; ri = np.random.random(200)\nExc.noise = 5.0                 ; Inh.noise = 2.0\nExc.a = 0.02                    ; Inh.a = 0.02 + 0.08 * ri\nExc.b = 0.2                     ; Inh.b = 0.25 - 0.05 * ri\nExc.c = -65.0 + 15.0 * re**2    ; Inh.c = -65.0\nExc.d = 8.0 - 6.0 * re**2       ; Inh.d = 2.0\nExc.v = -65.0                   ; Inh.v = -65.0\nExc.u = Exc.v * Exc.b           ; Inh.u = Inh.v * Inh.b\n\nexc_proj = net.connect(pre=Exc, post=pop, target='exc')\nexc_proj.all_to_all(weights=ann.Uniform(0.0, 0.5))\n   \ninh_proj = net.connect(pre=Inh, post=pop, target='inh')\ninh_proj.all_to_all(weights=ann.Uniform(0.0, 1.0))\n\nm = net.monitor(pop, 'spike')\n\nnet.compile()\n\nnet.parallel_run(number=2, method=run)\n\nCompiling network 2...  OK \n\n\n\n---------------------------------------------------------------------------\nANNarchyException                         Traceback (most recent call last)\nCell In[8], line 26\n     22 m = net.monitor(pop, 'spike')\n     24 net.compile()\n---&gt; 26 net.parallel_run(number=2, method=run)\n\nFile ~/Research/ANNarchy/source/ANNarchy/core/Network.py:656, in Network.parallel_run(self, method, number, max_processes, seeds, measure_time, **kwargs)\n    653 Messages._debug(\"Network was created with \", self._init_args, \"and\", self._init_kwargs)\n    655 if type(self) is Network:\n--&gt; 656     Messages._error(\"Network.parallel_run(): the network must be an instance of a class deriving from Network, not Network itself.\")\n    658 if measure_time:\n    659     tstart = time.time()\n\nFile ~/Research/ANNarchy/source/ANNarchy/intern/Messages.py:102, in _error(*var_text, **args)\n     99     exit = True\n    101 if exit:\n--&gt; 102     raise ANNarchyException(text)\n    103 else:\n    104     print('ERROR:' + text)\n\nANNarchyException: Network.parallel_run(): the network must be an instance of a class deriving from Network, not Network itself.",
    "crumbs": [
      "Notebooks",
      "**Advanced features**",
      "Parallel run"
    ]
  },
  {
    "objectID": "notebooks/BayesianOptimization.html",
    "href": "notebooks/BayesianOptimization.html",
    "title": "Hyperparameter optimization",
    "section": "",
    "text": "#!pip install ANNarchy\n\nMost of the work in computational neuroscience is to guess the values of parameters which are not constrained by the biology. The most basic approach is to simply try out different values, run the simulation, reason about why the results are not what you want, change some parameters, run again, etc. It is very easy to get lost in this process and it requires a great deal of intuition about how the model works.\nIf you are able to define an objective function for your model (a single number that tells how well your model performs), you can use search algorithms to find this hyperparameters automatically, at the cost of running your model multiple times.\nLet’s take the example of a rate-coded model depending on two hyperparameters a and b, where is the objective is to have a minimal activity after 1 s of simulation (dummy example):\nnet = MyNetwork()\nnet.compile()\n\ndef run(a, b):\n    net.pop.a = a\n    net.pop.b = b\n    \n    net.simulate(1000.)\n    \n    return (net.pop.r)**2\nGrid search would iterate over all possible values of the parameters to perform the search:\nmin_loss = 1000.\nfor a in np.linspace(0.0, 1.0, 100):\n    for b in np.linspace(0.0, 1.0, 100):\n        loss = run(a, b)\n        if loss &lt; min_loss:\n            min_loss = loss\n            a_best = a ; b_best = b\nIf you try 100 values for each parameters, you need 10000 simulations to find your parameters. The number of simulations explodes with the number of free parameters. Moreover, you cannot stop the search before the end, as you could miss the interesting region.\nRandom search samples blindly values for the hyperparameters:\nmin_loss = 1000.\nfor _ in range(1000):\n    a = np.random.uniform(0.0, 1.0)\n    b = np.random.uniform(0.0, 1.0)\n    loss = run(a, b)\n    if loss &lt; min_loss:\n        min_loss = loss\n        a_best = a ; b_best = b\nIf you are lucky, you may find a good solution quite early in the search, so you can stop it when the loss is below a desired threshold. The main drawback is that the search may spend a lot of time in uninteresting regions: it does not learn anything between two samples.\nAn often much more efficient search method is Bayesian optimization (also called sequential model-based optimization - SMBO). It is a form of random search that updates beliefs on the hyperparameters. In short, if some parameter values do not lead to good values of the objective function in early samples, they will not be used in later samples. The search becomes more and more focused on the interesting regions of the hyperparameter space.\nAs always with Python, there are many libraries for that, including:\n\nhyperopt https://github.com/hyperopt/hyperopt\noptuna https://github.com/pfnet/optuna\ntalos (for keras models) https://github.com/autonomio/talos\n\nThis notebook demonstrates how to use hyperopt to find some hyperparameters of the COBA models already included in the ANNarchy examples:\nhttps://annarchy.github.io/notebooks/COBA.html\nAdditionally, we will use the tensorboard extension to visualize the dependency between the parameters and the objective function.\n\nimport numpy as np\n\nimport ANNarchy as ann\nfrom ANNarchy.extensions.tensorboard import Logger\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\n\nCOBA = ann.Neuron(\n    parameters = dict(\n        El = -60.0 ,\n        Vr = -60.0 ,\n        Erev_exc = 0.0 ,\n        Erev_inh = -80.0 ,\n        Vt = -50.0 ,\n        tau = 20.0 ,\n        tau_exc = 5.0 ,\n        tau_inh = 10.0 ,\n        I = 20.0 ,\n    ),\n    equations = [\n        'tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I',\n        'tau_exc * dg_exc/dt = - g_exc',\n        'tau_inh * dg_inh/dt = - g_inh',\n    ],\n    spike = \"v &gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\nclass COBANetwork (ann.Network):\n    \n    def __init__(self, w_exc:float = 0.6, w_inh:float = 6.7):\n\n        self.P = self.create(geometry=4000, neuron=COBA)\n        self.P.v = ann.Normal(-55.0, 5.0)\n        self.P.g_exc = ann.Normal(4.0, 1.5)\n        self.P.g_inh = ann.Normal(20.0, 12.0)\n\n        self.Ce = self.connect(pre=self.P[:3200], post=self.P, target='exc')\n        self.Ce.fixed_probability(weights=w_exc, probability=0.02)\n\n        self.Ci = self.connect(pre=self.P[3200:], post=self.P, target='inh')\n        self.Ci.fixed_probability(weights=w_inh, probability=0.02)\n\n        self.m = self.monitor(self.P, ['spike'])\n\nnet = COBANetwork(w_exc=0.6, w_inh=6.7, dt=0.1)\nnet.compile()\n\nWith the default parameters, the COBA network fires at around 20 Hz:\n\nnet.simulate(1000.0)\n\ndata = net.m.get('spike')\nfr = net.m.mean_fr(data)\nprint(\"Firing rate:\", fr)\n\nFiring rate: 20.132\n\n\nLet’s suppose we now want the network to fire at 30 Hz. Which parameters should we change to obtain that value?\nMany parameters might influence the firing rate of the network (if not all). Here, we make the assumption that the weight values for the excitatory connections (0.6) and inhibitory ones (6.7) are the most critical ones.\nLet’s start by importing hyperopt (after installing it with pip install hyperopt):\n\nfrom hyperopt import fmin, tpe, hp, STATUS_OK\n\nWe define a trial() method taking values for the two hyperparameters as inputs. It starts by resetting the network, sets the excitatory and inhibitory weights to the desired value, simulates for one second, computes the mean firing rate of the population, logs the parameters and finally returns the objective function: the squared error between the recorded firing rate and 30 Hz.\n\nlogger = Logger()\n\ndef trial(args):\n    \n    # Retrieve the parameters\n    w_exc = args[0]\n    w_inh = args[1]\n    \n    # Create the network as a copy of net with different parameters (avoids recompilation)\n    new_net = net.copy(w_exc, w_inh)\n    \n    # Simulate 1 second\n    new_net.simulate(1000.0)\n\n    # Retrieve the spike recordings\n    spikes = new_net.m.get('spike')\n\n    # Compute the population firing rate\n    fr = new_net.m.mean_fr(spikes)\n    \n    # Compute a quadratic loss around 30 Hz\n    loss = 0.001 * (fr - 30.0)**2   \n    \n    # Log the parameters\n    logger.add_parameters({'w_exc': w_exc, 'w_inh': w_inh},\n                         {'loss': loss, 'firing_rate': fr})\n    \n    # Delete the network to avoid memory leak\n    new_net.clear()\n    \n    return {\n        'loss': loss,\n        'status': STATUS_OK,\n        # -- store other results like this\n        'fr': fr,\n        }\n\nLogging in runs/Mar13_12-22-06_Juliens-MacBook-Pro.local\n\n\nWe can check that the default parameters indeed lead to a firing rate of 20 Hz:\n\ntrial([0.6, 6.7])\n\n{'loss': 0.10801444900000001, 'status': 'ok', 'fr': 19.607}\n\n\nWe can now use hyperopt to find the hyperparameters making the network fire at 30 Hz.\nThe fmin() function takes:\n\nfn: the objective function for a set of parameters.\nspace: the search space for the hyperparameters (the prior).\nalgo: which algorithm to use, either tpe.suggest or random.suggest\nmax_evals: number of samples (simulations) to make.\n\nHere, we will sample the excitatory weights between 0.1 and 1, the inhibitory ones between 1 and 10. Of course, the smaller the range, the better. Refer to the doc of hyperopt for other sampling priors.\n\nbest = fmin(\n    fn=trial,\n    space=[\n        hp.uniform('w_exc', 0.1, 1.0), \n        hp.uniform('w_inh', 1.0, 10.0)\n    ],\n    algo=tpe.suggest,\n    max_evals=100)\nprint(best)\n\n100%|██████████| 100/100 [00:36&lt;00:00,  2.75trial/s, best loss: 1.3875624999997522e-06]\n{'w_exc': 0.8522950103771161, 'w_inh': 6.307488074280366}\n\n\nAfter 100 simulations, hyperopt returns a set of hyperparameter values that make the network fire at 30Hz. We can check that it is true with:\n\ntrial([best['w_exc'], best['w_inh']])\n\n{'loss': 0.015135990249999967, 'status': 'ok', 'fr': 33.890499999999996}\n\n\nThere are plenty of options to hyperopt (check Trials or the parallel search using MongoDB), but this simple example should get you started.\n\n\n\nScreen Capture_select-area_20200529161854.png",
    "crumbs": [
      "Notebooks",
      "**Advanced features**",
      "Bayesian optimization"
    ]
  },
  {
    "objectID": "notebooks/STDP1.html",
    "href": "notebooks/STDP1.html",
    "title": "STDP - single synapse",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis notebook demonstrates the online implementation of the spike time-dependent plasticity (STDP) rule for a pair of neurons.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nThe STDP learning rule maintains exponentially-decaying traces for the pre-synaptic and post-synaptic spikes.\n\\tau^+ \\, \\frac{d x(t)}{dt} = -x (t)\n\\tau^- \\, \\frac{d y(t)}{dt} = -x (t)\nLTP and LTD occur at spike times depending on the corresponding traces.\n\nWhen a pre-synaptic spike occurs, x(t) is incremented and LTD is applied proportionally to y(t).\nWhen a post-synaptic spike occurs, y(t) is incremented and LTP is applied proportionally to x(t).\n\n\nSTDP = ann.Synapse(\n    parameters = dict(\n        tau_plus = 20.0,\n        tau_minus = 20.0, \n        A_plus = 0.01,\n        A_minus = 0.01,\n        w_min = 0.0,\n        w_max = 2.0,\n    ),\n    equations = [\n        # Pre-synaptic trace\n        ann.Variable('tau_plus * dx/dt = -x', method='event-driven'),\n        # Post-synaptic trace\n        ann.Variable('tau_minus * dy/dt = -y', method='event-driven'),\n    ],\n    pre_spike=\"\"\"\n        g_target += w\n        x += A_plus * w_max\n        w = clip(w - y, w_min , w_max) # LTD\n    \"\"\",\n    post_spike=\"\"\"\n        y += A_minus * w_max\n        w = clip(w + x, w_min , w_max) # LTP\n    \"\"\"\n)\n\nWe create two dummy populations with one neuron each, whose spike times we can control.\n\nnet = ann.Network()\npre = net.create(ann.SpikeSourceArray([[0.]]))\npost = net.create(ann.SpikeSourceArray([[50.]]))\n\nWe connect the population using a STDP synapse.\n\nproj = net.connect(pre, post, 'exc', STDP)\nproj.all_to_all(1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x1125f0e30&gt;\n\n\n\nnet.compile()\n\nCompiling network 1...  OK \n\n\nThe presynaptic neuron will fire at various times between 0 and 100 ms, while the postsynaptic neuron keeps firing at 50 ms.\n\npre_times = np.linspace(100.0, 0.0, 101)\n\n\nweight_changes = []\n\nfor t_pre in pre_times:\n    \n    # Reset the populations\n    pre.clear()\n    post.clear()\n    pre.spike_times = [[t_pre]]\n    post.spike_times = [[50.0]]\n    \n    # Reset the traces\n    proj.x = 0.0\n    proj.y = 0.0\n    \n    # Weight before the simulation\n    w_before = proj[0].w[0]\n    \n    # Simulate long enough\n    net.simulate(105.0)\n    \n    # Record weight change\n    delta_w = proj[0].w[0] - w_before\n    weight_changes.append(delta_w)\n\nWe can now plot the classical STDP figure:\n\nplt.figure(figsize=(10, 8))\nplt.plot(50. - pre_times, weight_changes, \"*\")\nplt.plot([-50, 50], [0, 0], 'k')\nplt.plot([0, 0], [min(weight_changes), max(weight_changes)], 'k')\nplt.xlabel(\"t_post - t_pre\")\nplt.ylabel(\"delta_w\")\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "STDP I"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html",
    "href": "notebooks/PyNN.html",
    "title": "PyNN and Brian examples",
    "section": "",
    "text": "#!pip install ANNarchy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html#if_curr_alpha",
    "href": "notebooks/PyNN.html#if_curr_alpha",
    "title": "PyNN and Brian examples",
    "section": "IF_curr_alpha",
    "text": "IF_curr_alpha\nSimple network with a Poisson spike source projecting to a pair of IF_curr_alpha neurons.\nThis is a reimplementation of the PyNN example:\nhttp://www.neuralensemble.org/trac/PyNN/wiki/Examples/simpleNetwork\n\nclass IFCurrAlpha (ann.Network):\n\n    def __init__(self, spike_times):\n\n        # Input population\n        inp  = self.create(ann.SpikeSourceArray(spike_times))\n\n        # Output population\n        pop = self.create(2, ann.IF_curr_alpha)\n        pop.tau_refrac = 2.0\n        pop.v_thresh = -50.0\n        pop.tau_syn_E = 2.0\n        pop.tau_syn_I = 2.0\n\n        # Excitatory projection\n        proj = self.connect(inp, pop, 'exc')\n        proj.all_to_all(weights=1.0)\n\n        # Monitor\n        self.m = self.monitor(pop, ['spike', 'v'])\n\n\n# Parameters\ntstop = 1000.0\nrate = 100.0\n\n# Create the Poisson spikes\nnumber = int(2 * tstop * rate / 1000.0)\nnp.random.seed(26278342)\nspike_times = list(np.add.accumulate(np.random.exponential(1000.0/rate, size=number)))\n\n# Create the network\nnet = IFCurrAlpha(spike_times, dt=0.1)\n\n# Compile the network\nnet.compile()\n\n# Simulate\nnet.simulate(tstop)\ndata = net.m.get()\n\n# Plot the results\nplt.figure(figsize=(12, 8))\nplt.plot(net.dt*np.arange(tstop/net.dt), data['v'][:, 0])\nplt.xlabel('Time (ms)')\nplt.ylabel('Vm (mV)')\nplt.ylim([-66.0, -48.0])\nplt.title('Simple Network')\nplt.show()\n\nCompiling network 1...  OK",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html#if_cond_exp",
    "href": "notebooks/PyNN.html#if_cond_exp",
    "title": "PyNN and Brian examples",
    "section": "IF_cond_exp",
    "text": "IF_cond_exp\nA single IF neuron with exponential, conductance-based synapses, fed by two spike sources.\nThis is a reimplementation of the PyNN example:\nhttp://www.neuralensemble.org/trac/PyNN/wiki/Examples/IF_cond_exp\n\nclass IFCondExp (ann.Network):\n\n    def __init__(self):\n\n        # Input populations with predetermined spike times\n        spike_sourceE = self.create(ann.SpikeSourceArray(\n                spike_times= [float(i) for i in range(5, 105, 10)]\n            )\n        )\n        spike_sourceI = self.create(ann.SpikeSourceArray(\n                spike_times= [float(i) for i in range(155,255,10)]\n            )\n        )\n\n        # Population with one IF_cond_exp neuron\n        ifcell = self.create(1, ann.IF_cond_exp)\n        ifcell.set(\n            {   'i_offset' : 0.1,    'tau_refrac' : 3.0,\n                'v_thresh' : -51.0,  'tau_syn_E'  : 2.0,\n                'tau_syn_I': 5.0,    'v_reset'    : -70.0,\n                'e_rev_E'  : 0.,     'e_rev_I'    : -80.0 } )\n\n\n        # Projections\n        connE = self.connect(spike_sourceE, ifcell, 'exc')\n        connE.all_to_all(weights=0.006, delays=2.0)\n\n        connI = self.connect(spike_sourceI, ifcell, 'inh')\n        connI.all_to_all(weights=0.02,  delays=4.0)\n\n        # Monitor\n        self.n = self.monitor(spike_sourceE, ['spike'])\n        self.m = self.monitor(ifcell, ['spike', 'v'])\n\n\n# Parameters\ntstop = 200.0\n\n# Compile the network\nnet = IFCondExp(dt=0.1)\nnet.compile()\n\n# Simulate\nnet.simulate(tstop)\ndata = net.m.get()\n\n# Show the result\nplt.figure(figsize=(12, 8))\nplt.plot(net.dt * np.arange(tstop/net.dt), data['v'][:, 0])\nplt.xlabel('Time (ms)')\nplt.ylabel('Vm (mV)')\nplt.ylim([-66.0, -61.0])\nplt.title('IF_cond_exp')\nplt.show()\n\nCompiling network 2...  OK",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html#eif_cond_exp",
    "href": "notebooks/PyNN.html#eif_cond_exp",
    "title": "PyNN and Brian examples",
    "section": "EIF_cond_exp",
    "text": "EIF_cond_exp\nNetwork of EIF neurons with exponentially decreasing conductance-based synapses.\nThis is a reimplementation of the Brian example:\nhttp://brian.readthedocs.org/en/1.4.1/examples-misc_expIF_network.html\n\nEIF = ann.Neuron(\n    parameters = dict(\n        v_rest = -70.0,\n        cm = 0.2,\n        tau_m = 10.0,\n        tau_syn_E = 5.0,\n        tau_syn_I = 10.0,\n        e_rev_E = 0.0,\n        e_rev_I = -80.0,\n        delta_T = 3.0,\n        v_thresh = -55.0,\n        v_reset = -70.0,\n        v_spike = -20.0,\n    ),\n    equations = [\n        'dv/dt = (v_rest - v +  delta_T * exp( (v-v_thresh)/delta_T) )/tau_m + ( g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) )/cm',\n        'tau_syn_E * dg_exc/dt = - g_exc',\n        'tau_syn_I * dg_inh/dt = - g_inh',\n    ],\n    spike = \"v &gt; v_spike\",\n    reset = \"v = v_reset\",\n    refractory = 2.0\n\n)\n\n# Create the network\nnet = ann.Network(dt=0.1)\n\n# Poisson inputs\ni_exc = net.create(ann.PoissonPopulation(geometry=200, rates=\"if t &lt; 200.0 : 2000.0 else : 0.0\"))\ni_inh = net.create(ann.PoissonPopulation(geometry=200, rates=\"if t &lt; 100.0 : 2000.0 else : 0.0\"))\n\n# Main population\nP = net.create(geometry=4000, neuron=EIF)\n\n# Subpopulations\nPe = P[:3200]\nPi = P[3200:]\n\n# Projections\nwe = 1.5 / 1000.0 # excitatory synaptic weight\nwi = 2.5 * we # inhibitory synaptic weight\n\nCe = net.connect(Pe, P, 'exc')\nCe.fixed_probability(weights=we, probability=0.05)\n\nCi = net.connect(Pi, P, 'inh')\nCi.fixed_probability(weights=wi, probability=0.05)\n\nIe = net.connect(i_exc, P[:200], 'exc') # inputs to excitatory cells\nIe.one_to_one(weights=we)\n\nIi = net.connect(i_inh, P[3200:3400], 'exc')# inputs to inhibitory cells\nIi.one_to_one(weights=we)\n\n# Initialization of variables\nP.v = -70.0 + 10.0 * np.random.rand(P.size)\nP.g_exc = (np.random.randn(P.size) * 2.0 + 5.0) * we\nP.g_inh = (np.random.randn(P.size) * 2.0 + 5.0) * wi\n\n# Monitor\nm = net.monitor(P, 'spike')\n\n# Compile the Network\nnet.compile()\n\n# Simulate\nnet.simulate(500.0, measure_time=True)\n\n# Retrieve recordings\ndata = m.get()\nt, n = m.raster_plot(data['spike'])\n\nplt.figure(figsize=(12, 8))\nplt.plot(t, n, '.', markersize=0.2)\nplt.show()\n\nCompiling network 3...  OK \nSimulating 0.5 seconds of the network 3 took 0.32831788063049316 seconds.",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html#aeif_cond_exp",
    "href": "notebooks/PyNN.html#aeif_cond_exp",
    "title": "PyNN and Brian examples",
    "section": "AEIF_cond_exp",
    "text": "AEIF_cond_exp\nAdaptive exponential integrate-and-fire model.\nhttp://www.scholarpedia.org/article/Adaptive_exponential_integrate-and-fire_model\nModel introduced in:\n\nBrette R. and Gerstner W. (2005), Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity, J. Neurophysiol. 94: 3637 - 3642.\n\nThis is a reimplementation of the Brian example:\nhttps://brian.readthedocs.io/en/stable/examples-frompapers_Brette_Gerstner_2005.html\n\nAEIF = ann.Neuron(\n    parameters = dict(\n        v_rest = -70.6,\n        cm = 0.281, \n        tau_m = 9.3667, \n        tau_syn_E = 5.0,\n        tau_syn_I = 5.0, \n        e_rev_E = 0.0,\n        e_rev_I = -80.0,\n        tau_w = ann.Parameter(144.0), \n        a = ann.Parameter(4.0),\n        b = ann.Parameter(0.0805),\n        i_offset = 0.0,\n        delta_T = 2.0,\n        v_thresh = -50.4,\n        v_reset = ann.Parameter(-70.6),\n        v_spike = -40.0,\n    ), \n    equations = \"\"\"\n        I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset            \n        \n        tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6          \n        tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w           \n        \n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n    \"\"\",\n    spike = \"v &gt; v_spike\",\n    reset = \"\"\"\n        v = v_reset\n        w += b\n    \"\"\",\n    refractory = 0.1\n)\n\n\n# Create the network\nnet = ann.Network(dt=0.1)\n\n# Population of 3 neurons (regular, bursting, fast spiking)\npop = net.create(geometry=3, neuron=AEIF)\npop.tau_w = [144., 20., 144.]\npop.a = [4. ,4., 2000.0*pop.cm/144.0]\npop.b = [0.0805, 0.5, 0.0]\npop.v_reset = [-70.6, pop.v_thresh + 5.0, -70.6]\n\nm = net.monitor(pop, ['spike', 'v', 'w'])\n\n# Compile the network\nnet.compile()\n\n# Add current of 1 nA and simulate\nnet.simulate(20.0)\npop.i_offset = 1.0\nnet.simulate(100.0)\npop.i_offset = 0.0\nnet.simulate(20.0)\n\n# Retrieve the results\ndata = m.get()\n\n# Make spikes nicer\nfor i in range(3):\n    if len(data['spike'][i]) &gt; 0:\n        data['v'][:, i][data['spike'][i]] = 20.0\n\n# Plot the activity\nplt.figure(figsize=(15, 8))\nplt.subplot(2,3,1)\nplt.plot(net.dt*np.arange(140.0/net.dt), data['v'][:, 0])\nplt.title(\"Regular\")\nplt.ylabel('v')\nplt.subplot(2,3,2)\nplt.plot(net.dt*np.arange(140.0/net.dt), data['v'][:, 1])\nplt.title(\"Bursting\")\nplt.subplot(2,3,3)\nplt.plot(net.dt*np.arange(140.0/net.dt), data['v'][:, 2])\nplt.title('Fast spiking')\nplt.subplot(2,3,4)\nplt.plot(net.dt*np.arange(140.0/net.dt), data['w'][:, 0])\nplt.ylabel('w')\nplt.subplot(2,3,5)\nplt.plot(net.dt*np.arange(140.0/net.dt), data['w'][:, 1])\nplt.subplot(2,3,6)\nplt.plot(net.dt*np.arange(140.0/net.dt), data['w'][:, 2])\nplt.xlabel('Time (ms)')\nplt.show()\n\nCompiling network 4...  OK",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html#non-linear-synapses",
    "href": "notebooks/PyNN.html#non-linear-synapses",
    "title": "PyNN and Brian examples",
    "section": "Non-linear synapses",
    "text": "Non-linear synapses\nA single IF neuron with two non-linear NMDA synapses.\nThis is a reimplementation of the Brian example:\nhttp://brian.readthedocs.org/en/latest/examples-synapses_nonlinear_synapses.html\n\n# Neurons\nLinear = ann.Neuron(equations=\"dv/dt = 0.1\", spike=\"v&gt;1.0\", reset=\"v=0.0\")\nIntegrator = ann.Neuron(equations=\"dv/dt = 0.1*(g_exc -v)\", spike=\"v&gt;2.0\", reset=\"v=0.0\")\n\n# Non-linear synapse\nNMDA = ann.Synapse(\n    parameters = dict(tau = 10.0),\n    equations = [\n        'tau * dx/dt = -x',\n        'tau * dg/dt = -g +  x * (1 -g)',\n    ],\n    pre_spike = \"x += w\",\n    psp = \"g\"\n)\n\n# Network\nnet = ann.Network(dt=0.1)\n\n# Populations\ninput = net.create(geometry=2, neuron=Linear)\ninput.v = [0.0, 0.5]\npop = net.create(geometry=1, neuron=Integrator)\n\n# Projection\nproj = net.connect(pre=input, post=pop, target='exc', synapse=NMDA)\nproj.from_matrix(weights=[[1.0, 10.0]])\n\n# Monitors\nm = net.monitor(pop, 'v')\nw = net.monitor(proj, 'g')\n\n# Compile the network\nnet.compile()\n\n# Simulate for 100 ms\nnet.simulate(100.0)\n\n# Retrieve recordings\nv = m.get('v')[:, 0]\ns = w.get('g')[:, 0, :]\n\n# Plot the recordings\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2,1,1)\nplt.plot(s[:, 0])\nplt.plot(s[:, 1])\nplt.xlabel(\"Time (ms)\")\nplt.xlabel(\"Conductance\")\n\nplt.subplot(2,1,2)\nplt.plot(v)\nplt.xlabel(\"Time (ms)\")\nplt.xlabel(\"Membrane potential\")\nplt.show()\n\nWARNING: Monitor(): it is a bad idea to record synaptic variables of a projection at each time step! \nCompiling network 5...  OK",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html#stp",
    "href": "notebooks/PyNN.html#stp",
    "title": "PyNN and Brian examples",
    "section": "STP",
    "text": "STP\nNetwork (CUBA) with short-term synaptic plasticity for excitatory synapses (depressing at long timescales, facilitating at short timescales).\nAdapted from :\nhttps://brian.readthedocs.io/en/stable/examples-synapses_short_term_plasticity2.html\n\nLIF = ann.Neuron(\n    parameters = dict(\n        tau_m = 20.0,\n        tau_e = 5.0,\n        tau_i = 10.0,\n        E_rest = -49.0,\n        E_thresh = -50.0,\n        E_reset = -60.0,\n    ),\n    equations = [\n        'tau_m * dv/dt = E_rest -v + g_exc - g_inh',\n        'tau_e * dg_exc/dt = -g_exc',\n        'tau_i * dg_inh/dt = -g_inh',\n    ],\n    spike = \"v &gt; E_thresh\",\n    reset = \"v = E_reset\"\n)\n\nSTP = ann.Synapse(\n    parameters = dict(\n        tau_rec = 200.0,\n        tau_facil = 20.0,\n        U = 0.2,\n    ),\n    equations = [\n        ann.Variable('dx/dt = (1 - x)/tau_rec', init = 1.0, method='event-driven'),\n        ann.Variable('du/dt = (U - u)/tau_facil', init = 0.2, method='event-driven'),\n    ],\n    pre_spike = \"\"\"\n        g_target += w * u * x\n        x *= (1 - u)\n        u += U * (1 - u)\n    \"\"\"\n)\n\n# Network\nnet = ann.Network(dt=0.1)\n\n# Population\nP = net.create(geometry=4000, neuron=LIF)\nP.v = ann.Uniform(-60.0, -50.0)\nPe = P[:3200]\nPi = P[3200:]\n\n# Projections\ncon_e = net.connect(pre=Pe, post=P, target='exc', synapse = STP)\ncon_e.fixed_probability(weights=1.62, probability=0.02)\n\ncon_i = net.connect(pre=Pi, post=P, target='inh')\ncon_i.fixed_probability(weights=9.0, probability=0.02)\n\n# Monitor\nm = net.monitor(P, 'spike')\n\n# Compile the network\nnet.compile()\n\n# Simulate without plasticity\nnet.simulate(1000.0, measure_time=True)\n\ndata = m.get()\nt, n = m.raster_plot(data['spike'])\nrates = m.population_rate(data['spike'], 5.0)\nprint('Total number of spikes: ' + str(len(t)))\n\nplt.figure(figsize=(12, 8))\nplt.subplot(211)\nplt.plot(t, n, '.')\nplt.xlabel('Time (ms)')\nplt.ylabel('Neuron number')\nplt.subplot(212)\nplt.plot(np.arange(rates.size)*net.dt, rates)\nplt.show()\n\nCompiling network 6...  OK \nSimulating 1.0 seconds of the network 6 took 0.0765390396118164 seconds. \nTotal number of spikes: 14453",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/AdEx.html",
    "href": "notebooks/AdEx.html",
    "title": "Adaptive Exponential IF neuron",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis notebook explores how the AdEx neuron model can reproduce various spiking patterns observed in vivo.\nCode based on:\n\nNaud, R., Marcille, N., Clopath, C., and Gerstner, W. (2008). Firing patterns in the adaptive exponential integrate-and-fire model. Biol Cybern 99, 335. doi:10.1007/s00422-008-0264-7.\n\n\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nThe AdEx neuron is defined by the following equations:\n\n    C \\, \\frac{dv}{dt} = -g_L \\ (v - E_L) + g_L \\, \\Delta_T \\, \\exp(\\frac{v - v_T}{\\Delta_T}) + I - w\n\n\n    \\tau_w \\, \\frac{dw}{dt} = a \\, (v - E_L) - w\n\nif v &gt; v_\\text{spike}:\n\nv = v_R\nw = w + b\n\n\nAdEx = ann.Neuron(\n    parameters=dict(\n        C = ann.Parameter(200.),\n        gL = ann.Parameter(10.), # not g_L! g_ is reserved for spike transmission\n        E_L = ann.Parameter(-70.),\n        v_T = ann.Parameter(-50.),\n        delta_T = ann.Parameter(2.0),\n        a = ann.Parameter(2.0),\n        tau_w = ann.Parameter(30.),\n        b = ann.Parameter(0.),\n        v_r = ann.Parameter(-58.),\n        I = ann.Parameter(500.),\n        v_spike = ann.Parameter(0.0), \n    ),\n    equations= [\n        ann.Variable('C * dv/dt = - gL * (v - E_L) +  gL * delta_T * exp((v-v_T)/delta_T) + I - w', init=-70.0),\n        ann.Variable('tau_w * dw/dt = a * (v - E_L) - w'),\n    ],\n    spike = \"v &gt;= v_spike\",\n    reset = \"\"\"\n        v = v_r\n        w += b\n    \"\"\",\n    refractory = 2.0\n)\n\nWe create a population of 8 AdEx neurons which will get different parameter values.\n\nnet = ann.Network(dt=0.1)\npop = net.create(8, AdEx)\nnet.compile()\n\nCompiling network 1...  OK \n\n\nWe add a monitor to track the membrane potential and the spike timings during the simulation.\n\nm = net.monitor(pop, ['v', 'spike'])\n\nAs in the paper, we provide different parameters to each neuron and simulate the network for 500 ms with a fixed input current, and remove that current for an additional 50 ms.\n\n# a) tonic spiking b) adaptation, c) initial burst, d) regular bursting, e) delayed accelerating, f) delayed regular bursting, g) transcient spiking, h) irregular spiking\npop.C =       [200, 200, 130, 200, 200, 200, 100, 100]\npop.gL =      [ 10,  12,  18,  10,  12,  12,  10,  12]\npop.E_L =     [-70, -70, -58, -58, -70, -70, -65, -60]\npop.v_T =     [-50, -50, -50, -50, -50, -50, -50, -50]\npop.delta_T = [  2,   2,   2,   2,   2,   2,   2,   2]\npop.a =       [  2,   2,   4,   2,-10., -6.,-10.,-11.]\npop.tau_w =   [ 30, 300, 150, 120, 300, 300,  90, 130]\npop.b =       [  0,  60, 120, 100,   0,   0,  30,  30]\npop.v_r =     [-58, -58, -50, -46, -58, -58, -47, -48]\npop.I =       [500, 500, 400, 210, 300, 110, 350, 160]\n\n# Reset neuron\npop.v = pop.E_L\npop.w = 0.0\n\n# Simulate\nnet.simulate(500.)\npop.I = 0.0\nnet.simulate(50.)\n\n# Recordings\ndata = m.get('v')\nspikes = m.get('spike')\nfor n, t in spikes.items(): # Normalize the spikes\n    data[[x - m.times()['v']['start'][0] for x in t], n] = 0.0\n\nWe can now visualize the simulations:\n\nimport matplotlib.pyplot as plt\n\ntitles = [\n    \"a) tonic spiking\", \n    \"b) adaptation\", \n    \"c) initial burst\", \n    \"d) regular bursting\", \n    \"e) delayed accelerating\", \n    \"f) delayed regular bursting\", \n    \"g) transcient spiking\", \n    \"h) irregular spiking\"\n]\n\nplt.figure(figsize=(12, 15))\nplt.ylim((-70., 0.))\nfor i in range(8):\n    plt.subplot(4, 2, i+1)\n    plt.title(titles[i])\n    plt.plot(data[:, i], lw=3)\n    \n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "AdEx"
    ]
  },
  {
    "objectID": "notebooks/v4/NeuralField.html",
    "href": "notebooks/v4/NeuralField.html",
    "title": "Neural Field",
    "section": "",
    "text": "#!pip install ANNarchy\nThis notebook demonstrates a simple rate-coded model using Neural Fields. It consists of two 2D populations inp and pop, with one-to-one connections between inp and pop, and Difference-of-Gaussians (DoG) lateral connections within pop.\nIt is is based on the following paper:"
  },
  {
    "objectID": "notebooks/v4/NeuralField.html#model-overview",
    "href": "notebooks/v4/NeuralField.html#model-overview",
    "title": "Neural Field",
    "section": "Model overview",
    "text": "Model overview\nEach population consists of N*N neurons, with N=20. The inp population is solely used to represent inputs for pop. The pop population implements a discretized neural field, with neurons following the ODE:\n\\tau \\frac{d r_i(t)}{dt} + r_i(t) = r^\\text{input}_i(t) + \\sum_{j=1}^{N} w_{j, i} \\cdot r_j(t) + \\eta(t)\nwhere r_i(t) is the neuron’s firing rate, \\tau a time constant and w_{j, i} the weight value (synaptic efficiency) of the synapse between the neurons j and i. \\eta(t) is an additive noise uniformly taken in [-0.5, 0.5]. f() is a semi-linear function, ensuring the firing rate is bounded between 0 and 1.\nEach neuron in pop takes inputs from the neuron of inp which has the same position, leading to a one_to_one connection pattern.\nThe lateral connections within pop follow a difference-of-Gaussians (dog) connection pattern, with the connection weights w_{i,j} depending on the normalized euclidian distance between the neurons in the N*N population:\nw_{j, i} = A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_+^2}) -  A^- \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_-^2})\nIf i and j have coordinates (x_i, y_i) and (x_j, y_j) in the N*N space, the distance between them is computed as:\nd(i, j)^2 = (\\frac{x_i - x_j}{N})^2 + (\\frac{y_i - y_j}{N})^2\nInputs are given to the network by changing the firing rate of inp neurons."
  },
  {
    "objectID": "notebooks/v4/NeuralField.html#importing-annarchy",
    "href": "notebooks/v4/NeuralField.html#importing-annarchy",
    "title": "Neural Field",
    "section": "Importing ANNarchy",
    "text": "Importing ANNarchy\nWe first start by importing the numpy and ANNarchy libraries:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\n\n\nThe setup() method allows to configure ANNarchy to run in different modes, such as chosing the parallel framework (omp or cuda), setting the simulation step dt, the numerical method method or the seed of the random number generators."
  },
  {
    "objectID": "notebooks/v4/NeuralField.html#defining-the-neuron",
    "href": "notebooks/v4/NeuralField.html#defining-the-neuron",
    "title": "Neural Field",
    "section": "Defining the neuron",
    "text": "Defining the neuron\n\nNeuralFieldNeuron = ann.Neuron(\n    parameters=\"\"\" \n        tau = 10.0 : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dr/dt + r = sum(exc) + sum(inh) + Uniform(-0.5, 0.5) : min=0.0, max=1.0\n    \"\"\"\n)\n\nThe NeuralField neuron is governed by an ODE and considers inputs from other neurons. It has a non-linear activation function, which is linear when the firing rate is between 0.0 and 1.0, and constant otherwise.\ntau is a population-wise parameter, whose value will be the same for all neuron of the population.\nr is the firing rate of he neuron, whose dynamics are governed by a first-order linear ODE, integrating the sums of excitatory and inhibitory inputs with noise.\nAs explained in the manual for rate-coded neurons, sum(exc) retrieves the weighted sum of pre-synaptic firing rates for the synapses having the connection type exc, here the one_to_one connections between inp and pop. sum(inh) does the same for inh type connections, here the lateral connections within pop.\nThe firing rate is restricted to the range [0, 1] by setting the min and max accordingly in the flags section (everything after the :). This means that after evaluating the ODE and getting a new value for r, its value will be clamped if it outside these values. One can define both min and max, only one, or none."
  },
  {
    "objectID": "notebooks/v4/NeuralField.html#creating-the-populations",
    "href": "notebooks/v4/NeuralField.html#creating-the-populations",
    "title": "Neural Field",
    "section": "Creating the populations",
    "text": "Creating the populations\nThe two populations have a geometry of (20, 20), therefore 400 neurons each. They are created simply by instantiating the Population class:\n\nN = 20\ninp = ann.InputArray(geometry = (N, N), name='Input')\npop = ann.Population(geometry = (N, N), neuron = NeuralFieldNeuron, name='Focus')\n\nThe populations can be assigned a unique name (here ‘Input’ and ‘Focus’) in order to be be able to retrieve them if the references inp and focus are lost. They are given a 2D geometry and associated to the corresponding Neuron instance."
  },
  {
    "objectID": "notebooks/v4/NeuralField.html#creating-the-projections",
    "href": "notebooks/v4/NeuralField.html#creating-the-projections",
    "title": "Neural Field",
    "section": "Creating the projections",
    "text": "Creating the projections\nThe first projection is a one-to-one projection from Input to Focus with the type ‘exc’. This connection pattern pattern is possible because the two populations have the same geometry. The weights are initialized to 1.0, and this value will not change with time (no learning), so it is not necessary to define a synapse type:\n\nff = ann.Projection(pre=inp, post=pop, target='exc')\nff.connect_one_to_one(weights=1.0, delays = 20.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x116db8830&gt;\n\n\nThe references to the pre- and post-synaptic population (or their names), as well as the target type, are passed to the constructor of Projection. The connector method connect_one_to_one() is immediately applied to the Projection, defining how many synapses will be created. The weights are initialized uniformly to 1.0.\nThe second projection is a difference of gaussians (DoG) for the lateral connections within pop. The connector method is already provided by ANNarchy, so there is nothing more to do than to call it with the right parameters:\n\nlat = ann.Projection(pre=pop, post=pop, target='inh')\nlat.connect_dog(amp_pos=0.2, sigma_pos=0.1, amp_neg=0.1, sigma_neg=0.7)\n\n&lt;ANNarchy.core.Projection.Projection at 0x116ce60d0&gt;\n\n\nWe set two monitors recording the firing rate of the two populations:\n\nm = ann.Monitor(inp, 'r')\nn = ann.Monitor(pop, 'r')"
  },
  {
    "objectID": "notebooks/v4/NeuralField.html#compiling-the-network-and-simulating",
    "href": "notebooks/v4/NeuralField.html#compiling-the-network-and-simulating",
    "title": "Neural Field",
    "section": "Compiling the network and simulating",
    "text": "Compiling the network and simulating\nOnce the populations and projections are created, the network is ready to be generated, compiled and simulated. Compilation is simply done by calling compile():\n\nann.compile()\n\nCompiling ...  OK \n\n\nThis generates optimized C++ code from the neurons’ definition and network structure, compiles it with gcc/clang and instantiates all objects, particularly the synapses. If some errors were made in the neuron definition, they will be signaled at this point.\nHint: The call to compile() is mandatory in any script. After it is called, populations and projections can not be added anymore.\nOnce the compilation is successful, the network can be simulated by calling simulate(). As no input has been fed into the network yet, calling simulate() now won’t lead to anything interesting. The next step is to clamp inputs into the input population’s baseline."
  },
  {
    "objectID": "notebooks/v4/NeuralField.html#setting-inputs",
    "href": "notebooks/v4/NeuralField.html#setting-inputs",
    "title": "Neural Field",
    "section": "Setting inputs",
    "text": "Setting inputs\n\nTight loop\nIn this example, we use a moving bubble of activity rotating along a circle in the 20*20 input space in 1 second (i.e. 1000 steps). The simplest way of setting such inputs is to access population attributes (namely inp.r) in a tight loop in Python:\n\ndef gaussian(angle):\n    \"Unnormalized Gaussian at the specified angle.\"\n    # 20*20 matrices with XY coordinates\n    xx, yy = np.meshgrid(np.linspace(0, N-1, N), np.linspace(0, N-1, N))\n\n    # Compute the center of the bubble\n    cx = N / 2. * ( 1.0 + 0.5 * np.cos(2.0 * np.pi * angle ) )\n    cy = N / 2. * ( 1.0 + 0.5 * np.sin(2.0 * np.pi * angle ) )\n\n    # Gaussian\n    bubble = np.exp(-((xx-cx)**2 + (yy-cy)**2)/8.0)\n    return bubble\n\n# Main loop\nT = 1000\nangle = 0.0\nfor t in range(T):\n\n    # Update the angle\n    angle += 1.0/T\n    \n    # Clamp the firing rate of inp with the Gaussian and some noise\n    inp.r = gaussian(angle) + np.random.uniform(-0.5, 0.5, (N, N))\n    \n    # Simulate for one step = 1 ms\n    ann.step()  \n\nangle represents the angle made by the bubble with respect to the center of the input population. At each iteration of the simulation (i.e. every millisecond of simulation), the bubble is slightly rotated (angle is incremented) so as to make a complete revolution in 1 seconds (1000 steps).\nA Gaussian function (in the form of the Numpy array returned by the gaussian() methpd) is then clamped into the activity of inp. Some uniform noise is then added.\nLast, a single simulation step is performed using step(). step() is equivalent to simulate(dt), although a little bit faster as it does not check anything.\nLet’s plot the firing rates of the two populations at the end of the simulation:\n\nplt.figure(figsize=(15, 10))\nplt.subplot(121)\nplt.imshow(inp.r, interpolation='nearest', cmap=plt.cm.gray)\nplt.subplot(122)\nplt.imshow(pop.r, interpolation='nearest', cmap=plt.cm.gray)\nplt.show()\n\n\n\n\n\n\n\n\nWe see a noisy bubble of activity in inp and a clean one in pop, demonstrating the noise-filtering capacities of neural fields.\nLet’s retrieve the data recorded by the monitors, and use Matplotlib animations to show how the firing rates changed during the simulation (other methods are possible):\n\ninp_data = m.get('r')\npop_data = n.get('r')\n\n\n%matplotlib inline\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nfig = plt.figure(figsize=(10,5))\nax1 = plt.subplot(1,2,1)   \nax2 = plt.subplot(1,2,2)\n\nim1 = ax1.imshow(inp.r, interpolation='nearest', cmap=plt.cm.gray)\nim2 = ax2.imshow(pop.r, interpolation='nearest', cmap=plt.cm.gray)\n\ndef drawframe(n):\n    im1.set_data(inp_data[n].reshape((N, N)))\n    im2.set_data(pop_data[n].reshape((N, N)))    \n    return (im1, im2)\n\nanim = animation.FuncAnimation(fig, drawframe, frames=T, interval=2, blit=True)\nplt.close()\nHTML(anim.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nEverything works as expected. However, the simulation is actually quite slow (even if you do not notice it here), as Python is bad at tight loops like this one. For longer simulations, the overhead of python might become too damaging.\n\n\nTimedArray\nA much more efficient variant is to precompute the input rates and store them in an array that will be iteratively read by the TimedArrayobject. This is a new population that unfortunately cannot be added to the network anymore, as we have already compiled it. Let’s clear the network and redefine everything, replacing the InputArray with a TimedArray of the same shape:\n\nann.clear()\n\nNeuralFieldNeuron = ann.Neuron(\n    parameters=\"tau = 10.0 : population\",\n    equations=\"tau * dr/dt + r = sum(exc) + sum(inh) + Uniform(-0.5, 0.5) : min=0.0, max=1.0\")\n\nN = 20\ninp = ann.TimedArray(geometry = (N, N), name='Input')\npop = ann.Population(geometry = (N, N), neuron = NeuralFieldNeuron, name='Focus')\n\nff = ann.Projection(pre=inp, post=pop, target='exc')\nff.connect_one_to_one(weights=1.0, delays = 20.0)\n\nlat = ann.Projection(pre=pop, post=pop, target='inh')\nlat.connect_dog(amp_pos=0.2, sigma_pos=0.1, amp_neg=0.1, sigma_neg=0.7)\n\nm = ann.Monitor(inp, 'r')\nn = ann.Monitor(pop, 'r')\n\nann.compile()\n\nWe now create a numpy array of shape (T, N, N) where T is the duration of the simulation (1000 steps) and N the dimension of the neural field. We make sure to add noise to the inputs.\nWe then set the rates parameter of the TimedArrayto that array by using update(). At each step of the simulation, the timed array will “read” a new input activity in that array and store it as r. We simply need to call simulate()for the whole duration of the simulation, instead of using a slow tight loop in Python.\n\ndef gaussian(angle):\n    \"Unnormalized Gaussian at the specified angle.\"\n    # 20*20 matrices with XY coordinates\n    xx, yy = np.meshgrid(np.linspace(0, N-1, N), np.linspace(0, N-1, N))\n\n    # Compute the center of the bubble\n    cx = N / 2. * ( 1.0 + 0.5 * np.cos(2.0 * np.pi * angle ) )\n    cy = N / 2. * ( 1.0 + 0.5 * np.sin(2.0 * np.pi * angle ) )\n\n    # Gaussian\n    bubble = np.exp(-((xx-cx)**2 + (yy-cy)**2)/8.0)\n    return bubble\n\n# Precompute the angles for 1000 steps = 1 cycle\nT = 1000\nangles = np.linspace(0, 1, T)\n\n# Accumulate the 1000 inputs with time as the first dimension\ninput_rates = np.array([gaussian(angle) + np.random.uniform(-0.5, 0.5, (N, N)) for angle in angles])\n\n# Set it as the rates argument of the input population\ninp.update(input_rates)\n\n# Simulate for 1 second\nann.simulate(T)\n\n# Retrieve the recordings\ninp_data = m.get('r')\npop_data = n.get('r')\n\n\n%matplotlib inline\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nfig = plt.figure(figsize=(10,5))\nax1 = plt.subplot(1,2,1)   \nax2 = plt.subplot(1,2,2)\n\nim1 = ax1.imshow(inp.r, interpolation='nearest', cmap=plt.cm.gray)\nim2 = ax2.imshow(pop.r, interpolation='nearest', cmap=plt.cm.gray)\n\ndef drawframe(n):\n    im1.set_data(inp_data[n].reshape((N, N)))\n    im2.set_data(pop_data[n].reshape((N, N)))    \n    return (im1, im2)\n\nanim = animation.FuncAnimation(fig, drawframe, frames=T, interval=2, blit=True)\nplt.close()\nHTML(anim.to_html5_video())\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "notebooks/v4/COBA.html",
    "href": "notebooks/v4/COBA.html",
    "title": "COBA and CUBA networks",
    "section": "",
    "text": "#!pip install ANNarchy\nThis notebook reproduces the benchmarks used in:\nThey are based on the balanced network proposed by:\nEach network is composed of 4000 neurons (3200 excitatory and 800 inhibitory), reciprocally connected with a probability of 0.02 (sparse connection).\nThe CUBA network uses a current-based integrate-and-fire neuron model:\n\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) - g_\\text{inh} (t)\nwhile the COBA model uses conductance-based IF neurons:\n\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) * (E_\\text{exc} - v(t)) + g_\\text{inh} (t) * (E_\\text{inh} - v(t)) + I(t)\nApart from the neuron model and synaptic weights, both networks are equal, so we’ll focus on the COBA network here.\nThe discretization step has to be set to 0.1 ms:\nimport numpy as np\nimport ANNarchy as ann\n\nann.setup(dt=0.1) \n\nANNarchy 4.8 (4.8.2) on darwin (posix)."
  },
  {
    "objectID": "notebooks/v4/COBA.html#neuron-definition",
    "href": "notebooks/v4/COBA.html#neuron-definition",
    "title": "COBA and CUBA networks",
    "section": "Neuron definition",
    "text": "Neuron definition\n\nCOBA = ann.Neuron(\n    parameters=\"\"\"\n        El = -60.0          : population\n        Vr = -60.0          : population\n        Erev_exc = 0.0      : population\n        Erev_inh = -80.0    : population\n        Vt = -50.0          : population\n        tau = 20.0          : population\n        tau_exc = 5.0       : population\n        tau_inh = 10.0      : population\n        I = 20.0            : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I\n\n        tau_exc * dg_exc/dt = - g_exc\n        tau_inh * dg_inh/dt = - g_inh\n    \"\"\",\n    spike = \"v &gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\n\nCUBA = ann.Neuron(\n    parameters=\"\"\"\n        El = -49.0      : population\n        Vr = -60.0      : population\n        Vt = -50.0      : population\n        tau_m = 20.0    : population\n        tau_exc = 5.0   : population\n        tau_inh = 10.0  : population\n    \"\"\",\n    equations=\"\"\"\n        tau_m * dv/dt = (El - v) + g_exc - g_inh \n\n        tau_exc * dg_exc/dt = - g_exc \n        tau_inh * dg_inh/dt = - g_inh \n    \"\"\",\n    spike = \"v &gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\nThe neurons define exponentially-decreasing conductance g_exc and g_inh for the excitatory and inhibitory conductances/currents, respectively. They also define a refractory period of 5 ms."
  },
  {
    "objectID": "notebooks/v4/COBA.html#population",
    "href": "notebooks/v4/COBA.html#population",
    "title": "COBA and CUBA networks",
    "section": "Population",
    "text": "Population\n\n# COBA network\nP_COBA = ann.Population(geometry=4000, neuron=COBA)\nP_COBA_E = P_COBA[:3200] ; P_COBA_I = P_COBA[3200:]\n\n# CUBA network\nP_CUBA = ann.Population(geometry=4000, neuron=CUBA)\nP_CUBA_E = P_CUBA[:3200] ; P_CUBA_I = P_CUBA[3200:]\n\nFor both networks, we create a population of COBA neurons, and assign the 3200 first ones to an excitatory sub-population and the 800 last ones to an inhibitory sub-population.\nIt would have been equivalent to declare two separate populations as:\nP_COBA_E = ann.Population(geometry=3200, neuron=COBA)\nP_COBA_I = ann.Population(geometry= 800, neuron=COBA)\nbut splitting a global population allows to apply methods to all neurons, for example when recording all spikes with a single monitor, or when initializing populations parameters uniformly.\nWe now initialize the variables of both populations:\n\n# COBA\nP_COBA.v = ann.Normal(-55.0, 5.0)\nP_COBA.g_exc = ann.Normal(4.0, 1.5)\nP_COBA.g_inh = ann.Normal(20.0, 12.0)\n\n# CUBA\nP_CUBA.v = ann.Uniform(-60.0, -50.0)"
  },
  {
    "objectID": "notebooks/v4/COBA.html#connections",
    "href": "notebooks/v4/COBA.html#connections",
    "title": "COBA and CUBA networks",
    "section": "Connections",
    "text": "Connections\nThe neurons are randomly connected with a probability of 0.02. Excitatory neurons project on all other neurons with the target “exc” and a weight of 0.6 (COBA) or , while the inhibitory neurons have the target “inh” and a weight of 6.7.\n\n# COBA\nwe_COBA = 0.6\nwi_COBA = 6.7\nCe_COBA = ann.Projection(pre=P_COBA_E, post=P_COBA, target='exc')\nCe_COBA.connect_fixed_probability(weights=we_COBA, probability=0.02)\n\nCi_COBA = ann.Projection(pre=P_COBA_I, post=P_COBA, target='inh')\nCi_COBA.connect_fixed_probability(weights=wi_COBA, probability=0.02)\n\n# CUBA\nwe_CUBA = 0.27 * 60.0 / 10.0 # 0.7 * (Vmean - E_rev_exc) / gL (mV)\nwi_CUBA = 4.5 * 20.0 / 10.0 # 4.5 * (Vmean - E_rev_inh) / gL (mV)\nCe_CUBA = ann.Projection(pre=P_CUBA_E, post=P_CUBA, target='exc')\nCe_CUBA.connect_fixed_probability(weights=we_CUBA, probability=0.02)\n\nCi_CUBA = ann.Projection(pre=P_CUBA_I, post=P_CUBA, target='inh')\nCi_CUBA.connect_fixed_probability(weights=wi_CUBA, probability=0.02)\n\n&lt;ANNarchy.core.Projection.Projection at 0x1338f2d50&gt;\n\n\n\nann.compile()\n\nCompiling ...  OK"
  },
  {
    "objectID": "notebooks/v4/COBA.html#simulation",
    "href": "notebooks/v4/COBA.html#simulation",
    "title": "COBA and CUBA networks",
    "section": "Simulation",
    "text": "Simulation\nWe first define a monitor to record the spikes emitted in the two populations:\n\nm_COBA = ann.Monitor(P_COBA, ['spike'])\nm_CUBA = ann.Monitor(P_CUBA, ['spike'])\n\nWe can then simulate for 1 second:\n\nann.simulate(1000.)\n\nWe retrieve the recorded spikes from the monitor:\n\ndata_COBA = m_COBA.get('spike')\ndata_CUBA = m_CUBA.get('spike')\n\nand compute a raster plot from the data:\n\nt_COBA, n_COBA = m_COBA.raster_plot(data_COBA)\nt_CUBA, n_CUBA = m_CUBA.raster_plot(data_CUBA)\n\nt and n are lists representing for each spike emitted during the simulation the time at which it was emitted and the index the neuron which fired. The length of this list represents the total number of spikes in the popultion, so we can compute the population mean firing rate:\n\nprint('Mean firing rate in the COBA population: ' + str(len(t_COBA) / 4000.) + 'Hz')\nprint('Mean firing rate in the CUBA population: ' + str(len(t_CUBA) / 4000.) + 'Hz')\n\nMean firing rate in the COBA population: 22.0885Hz\nMean firing rate in the CUBA population: 5.7385Hz\n\n\nFinally, we can show the raster plot with matplotlib:\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.title(\"COBA\")\nplt.plot(t_COBA, n_COBA, '.', markersize=0.5)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.subplot(122)\nplt.title(\"CUBA\")\nplt.plot(t_CUBA, n_CUBA, '.', markersize=0.5)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\n\n\n\n\n\nMore detailed information about the activity of the population is provided by the inter-spike interval and the coefficient of variation, for both of which values we offer methods provided by the Monitor class.\n\nisi_COBA = m_COBA.inter_spike_interval(data_COBA)\nisi_CUBA = m_COBA.inter_spike_interval(data_CUBA)\n\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.title(\"COBA\")\nplt.hist(isi_COBA)\nplt.xlabel('ISI (ms)')\nplt.ylabel('n in bin')\nplt.subplot(122)\nplt.title(\"CUBA\")\nplt.hist(isi_CUBA)\nplt.xlabel('ISI (ms)')\nplt.ylabel('n in bin')\nplt.show()\n\n\n\n\n\n\n\n\n\ncov_COBA = m_COBA.coefficient_of_variation(data_COBA)\ncov_CUBA = m_COBA.coefficient_of_variation(data_CUBA)\n\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.title(\"COBA\")\nplt.hist(cov_COBA)\nplt.xlabel('ISI CV')\nplt.ylabel('n in bin')\nplt.subplot(122)\nplt.title(\"CUBA\")\nplt.hist(cov_CUBA)\nplt.xlabel('ISI CV')\nplt.ylabel('n in bin')\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/Image.html",
    "href": "notebooks/v4/Image.html",
    "title": "Convolutions and pooling",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis simple example in examples/image demonstrates how to load images directly into the firing rates of a population and apply basic linear filters on it.\nIt relies on the ANNarchy extensions image and convolution which must be explicitly imported:\n\nimport numpy as np\nimport ANNarchy as ann\n\nfrom ANNarchy.extensions.image import ImagePopulation\nfrom ANNarchy.extensions.convolution import Convolution, Pooling\n\nann.clear()\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\n\n\nANNarchy.extensions.image depends on the Python bindings of OpenCV, they must be installed before running the script.\nWe first create an ImagePopulation that will load images:\n\nimage = ImagePopulation(geometry=(480, 640, 3))\n\nIts geometry specifies the size of the images that can be loaded, here 640x480 RGB images. Note the geometry must be of the form (height, width, channels), where channels is 1 for grayscale images and 3 for color images.\nThe next step is to reduce the size of the image, what can be done by using the Pooling class of the convolution extension.\nWe define a dummy artificial neuron, whose firing rate r will simply be the sum of excitatory connections /ensured to be positive, but this should always be the case). We then create a smaller population pooled with this neuron type, and connect it to the ImagePopulation using mean-pooling:\n\n# Simple ANN\nLinearNeuron = ann.Neuron(equations=\"r=sum(exc): min=0.0\")\n\n# Subsampling population\npooled = ann.Population(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Mean-pooling projection\npool_proj = Pooling(pre=image, post=pooled, target='exc', operation='mean')\npool_proj.connect_pooling()\n\n&lt;ANNarchy.extensions.convolution.Pooling.Pooling at 0x117fe1010&gt;\n\n\nThe pooled population reduces the size of the image by a factor ten (defined by the size of the population) by averaging the pixels values over 10x10 regions (operation is set to 'mean', but one could use 'max' or 'min'). The connect_pooling() connector creates the “fake” connection pattern (as no weights are involved).\nLet’s apply now a 3x3 box filter on each channel of the pooled population:\n\n# Smoothing population\nsmoothed = ann.Population(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Box filter projection\nbox_filter = np.ones((3, 3, 1))/9.\nsmooth_proj = Convolution(pre=pooled, post=smoothed, target='exc')\nsmooth_proj.connect_filter(weights=box_filter)\n\n&lt;ANNarchy.extensions.convolution.Convolve.Convolution at 0x13016d2b0&gt;\n\n\nTo perform a convolution operation on the population (or more precisely a cross-correlation), we call the connect_filter() connector method of the Convolution projection. It requires to define a kernel (weights) that will be convolved over the input population. Here we use a simple box filter, but any filter can be used.\nAs the pooled population has three dimensions and we want to smooth the activities per color channel, we need to define a (3, 3, 1) kernel. If we wanted to smooth also over the color channels, we could have used a (3, 3) filter: the resulting population would have the shape (48, 64).\nWe now apply a bank of three filters, each selective to a particular color (red/green/blue). This filters do not have a spatial extent (1x1 convolution), but sum over the third dimension (the color channels):\n\n# Convolution population    \nfiltered = ann.Population(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Red/Green/Blue filter bank\nfilter_bank = np.array([ \n    [[ [2.0, -1.0, -1.0] ]] , # Red filter \n    [[ [-1.0, 2.0, -1.0] ]] , # Blue filter\n    [[ [-1.0, -1.0, 2.0] ]]   # Green filter\n])\nfilter_proj = Convolution(pre=smoothed, post=filtered, target='exc')\nfilter_proj.connect_filters(weights=filter_bank)\n\n&lt;ANNarchy.extensions.convolution.Convolve.Convolution at 0x1300f1810&gt;\n\n\nEach of the three filter has the shape (1, 1, 3). The result of each convolution would then be (48, 64), but as there are three filters, the output population is (48, 64, 3). The last dimension does not correspond to the number of color channels, but to the number of filters in the bank: if you add a filter, the population will have to be (48, 64, 4).\nBanks of filters require to use connect_filters() instead of connect_filter().\n\nann.compile()\n\nCompiling ...  OK \n\n\nAfter compilation, we can load an image into the input population:\n\nimage.set_image('test.jpg')\n\nTo see the result, we need to simulate for four time steps (4 milliseconds, as dt=1.0).\n\nStep 1: The image population loads the image.\nStep 2: The pooled population subsamples the image.\nStep 3: The smoothed population filters the pooled image.\nStep 4: The bank of filters are applied by filtered.\n\n\nann.simulate(4.0)\n\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(15, 20))\n\nplt.subplot(532)\nplt.imshow(image.r)\nplt.title('Original')\n\nplt.subplot(534)\nplt.imshow(image.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('image R')\nplt.subplot(535)\nplt.imshow(image.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('image G')\nplt.subplot(536)\nplt.imshow(image.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('image B')\n\nplt.subplot(537)\nplt.imshow(pooled.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('pooled R')\nplt.subplot(538)\nplt.imshow(pooled.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('pooled G')\nplt.subplot(539)\nplt.imshow(pooled.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('pooled B')\n\nplt.subplot(5, 3, 10)\nplt.imshow(smoothed.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('smoothed R')\nplt.subplot(5, 3, 11)\nplt.imshow(smoothed.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('smoothed G')\nplt.subplot(5, 3, 12)\nplt.imshow(smoothed.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('smoothed B')\n\nplt.subplot(5, 3, 13)\nplt.imshow(filtered.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('filtered R')\nplt.subplot(5, 3, 14)\nplt.imshow(filtered.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('filtered G')\nplt.subplot(5, 3, 15)\nplt.imshow(filtered.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('filtered B')\n\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/Izhikevich.html",
    "href": "notebooks/v4/Izhikevich.html",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "",
    "text": "#!pip install ANNarchy\nThis script reproduces the simple pulse-coupled network proposed by Eugene Izhikevich in the article:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\nann.clear()\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\nThe original Matlab code is provided below:"
  },
  {
    "objectID": "notebooks/v4/Izhikevich.html#neuron-type",
    "href": "notebooks/v4/Izhikevich.html#neuron-type",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "Neuron type",
    "text": "Neuron type\nThe network is composed of parameterized quadratic integrate-and-fire neurons, known as Izhikevich neurons. They are simply defined by the following equations:\n\n    \\frac{dv(t)}{dt} = 0.04 \\, v^2(t) + 5 \\, v(t) + 140 - u(t) + I(t)\n\n\n    \\frac{du(t)}{dt} = a \\, (b \\, v(t) - u)\n\nwith v(t) representing the membrane potential and u(t) the recovery variable. The spiking mechanism is defined by a threshold on v(t), so that it emits a spike when it exceeds V_T = 30 mV. The membrane potential is reset to c and the recovery variable is incremented from d\n\n    \\text{if} \\; v(t) &gt; V_T : \\; \\begin{cases}\n        \\text{emit a spike} \\\\\n        v(t) \\leftarrow c \\\\\n        u(t) \\leftarrow u(t) + d \\\\\n    \\end{cases}\n\na, b, c, d are parameters allowing to reproduce many types of neural firing.\nI(t) is the input voltage to a neuron at each time step t. For the desired network, it is the sum of a random value taken from a normal distribution with mean 0.0 and variance 1.0 (multiplied by a scaling factor) and the net effect of incoming spikes (excitatory and inhibitory).\nImplementing such a neuron in ANNarchy is straightforward:\n\nIzhikevich = ann.Neuron(\n    parameters=\"\"\"\n        noise = 5.0 \n        a = 0.02\n        b = 0.2\n        c = -65.0\n        d = 2.0 \n        v_thresh = 30.0\n    \"\"\",\n    equations=\"\"\"\n        # Input current\n        I = g_exc - g_inh + noise * Normal(0.0, 1.0)\n\n        # Membrane potential\n        dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I \n\n        # Recovery variable\n        du/dt = a * (b*v - u) \n    \"\"\",\n    spike = \"\"\"\n        v &gt;= v_thresh\n    \"\"\",\n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\"\n)\n\nThe parameters a, b, c, d as well as the noise amplitude noise are declared in the parameters argument, as their value is constant during the simulation.\nThe equations for v and u are direct translations of their mathematical counterparts. Note the use of dx/dt for the time derivative and ^2 for the square function.\nThe input voltage I is defined as the sum of:\n\nthe total conductance of excitatory synapses g_exc,\nthe total conductance of inhibitory synapses -g_inh (in this example, we consider all weights to be positive, so we need to invert g_inh in order to model inhibitory synapses),\na random number taken from the normal distribution N(0,1) and multiplied by the noise scale noise.\n\nIn the pulse-coupled network, synapses are considered as instantaneous, i.e. a pre-synaptic spikes increases immediately the post-synaptic conductance proportionally to the weight of the synapse, but does not leave further trace. As this is the default behavior in ANNarchy, nothing has to be specified in the neuron’s equations.\nThe spike argument specifies the condition for when a spike should be emitted (here the membrane potential v should be greater than v_thresh). The reset argument specifies the changes to neural variables that should occur after a spike is emitted: here, the membrane potential is reset to the resting potential c and the membrane recovery variable u is increased from d."
  },
  {
    "objectID": "notebooks/v4/Izhikevich.html#defining-the-populations",
    "href": "notebooks/v4/Izhikevich.html#defining-the-populations",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "Defining the populations",
    "text": "Defining the populations\nWe start by defining a population of 1000 Izhikevich neurons and split it into 800 excitatory neurons and 200 inhibitory ones:\n\npop = ann.Population(geometry=1000, neuron=Izhikevich)\n\nExc = pop[:800]\nInh = pop[800:]\n\nExc and Inh are subsets of pop, which have the same properties as a population. We can then set parameters differently for each population:\n\nre = np.random.random(800)      ; ri = np.random.random(200)\nExc.noise = 5.0                 ; Inh.noise = 2.0\nExc.a = 0.02                    ; Inh.a = 0.02 + 0.08 * ri\nExc.b = 0.2                     ; Inh.b = 0.25 - 0.05 * ri\nExc.c = -65.0 + 15.0 * re**2    ; Inh.c = -65.0\nExc.d = 8.0 - 6.0 * re**2       ; Inh.d = 2.0\nExc.v = -65.0                   ; Inh.v = -65.0\nExc.u = Exc.v * Exc.b           ; Inh.u = Inh.v * Inh.b"
  },
  {
    "objectID": "notebooks/v4/Izhikevich.html#defining-the-projections",
    "href": "notebooks/v4/Izhikevich.html#defining-the-projections",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "Defining the projections",
    "text": "Defining the projections\nWe can now define the connections within the network:\n\nThe excitatory neurons are connected to all neurons with a weight randomly chosen in [0, 0.5]\nThe inhibitory neurons are connected to all neurons with a weight randomly chosen in [0, 1]\n\n\nexc_proj = ann.Projection(pre=Exc, post=pop, target='exc')\nexc_proj.connect_all_to_all(weights=ann.Uniform(0.0, 0.5))\n   \ninh_proj = ann.Projection(pre=Inh, post=pop, target='inh')\ninh_proj.connect_all_to_all(weights=ann.Uniform(0.0, 1.0))\n\n&lt;ANNarchy.core.Projection.Projection at 0x127c3b9b0&gt;\n\n\nThe network is now ready, we can compile:\n\nann.compile()\n\nCompiling ...  OK"
  },
  {
    "objectID": "notebooks/v4/Izhikevich.html#running-the-simulation",
    "href": "notebooks/v4/Izhikevich.html#running-the-simulation",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "Running the simulation",
    "text": "Running the simulation\nWe start by monitoring the spikes and membrane potential in the whole population:\n\nm = ann.Monitor(pop, ['spike', 'v'])\n\nWe run the simulation for 1000 milliseconds:\n\nann.simulate(1000.0, measure_time=True)\n\nSimulating 1.0 seconds of the network took 0.038668155670166016 seconds. \n\n\nWe retrieve the recordings, generate a raster plot and the population firing rate:\n\nspikes = m.get('spike')\nv = m.get('v')\nt, n = m.raster_plot(spikes)\nfr = m.histogram(spikes)\n\nWe plot:\n\nThe raster plot of population\nThe evolution of the membrane potential of a single excitatory neuron\nThe population firing rate\n\n\nplt.figure(figsize=(12, 12))\n\n# First plot: raster plot\nplt.subplot(311)\nplt.plot(t, n, 'b.')\nplt.title('Raster plot')\n\n# Second plot: membrane potential of a single excitatory cell\nplt.subplot(312)\nplt.plot(v[:, 15]) # for example\nplt.title('Membrane potential')\n\n# Third plot: number of spikes per step in the population.\nplt.subplot(313)\nplt.plot(fr)\nplt.title('Number of spikes')\nplt.xlabel('Time (ms)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can observe how the pusle-coupled network oscillates first at a low frequency, before emitting higher-frequency oscillations."
  },
  {
    "objectID": "notebooks/v4/BoldMonitoring2.html",
    "href": "notebooks/v4/BoldMonitoring2.html",
    "title": "Recording BOLD signals - Davis model",
    "section": "",
    "text": "#!pip install ANNarchy\nimport numpy as np\n\nimport ANNarchy as ann\nfrom ANNarchy.extensions.bold import *\n\nANNarchy 4.8 (4.8.3) on darwin (posix)."
  },
  {
    "objectID": "notebooks/v4/BoldMonitoring2.html#davis-model",
    "href": "notebooks/v4/BoldMonitoring2.html#davis-model",
    "title": "Recording BOLD signals - Davis model",
    "section": "Davis model",
    "text": "Davis model\nLet’s now demonstrate how to define a custom BOLD model. The default Ballon model is defined by the following code:\nballoon_RN = BoldModel(\n    parameters = \"\"\"\n        phi       = 1.0         ;   kappa     = 1/1.54\n        gamma     = 1/2.46      ;   E_0       = 0.34\n        tau       = 0.98        ;   alpha     = 0.33\n        V_0       = 0.02        ;   v_0       = 40.3\n        TE        = 40/1000.    ;   epsilon   = 1.43\n        r_0       = 25.         ;   second    = 1000.0\n    \"\"\",\n    equations = \"\"\"\n        # CBF input\n        I_CBF          = sum(I_CBF)       \n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  \n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        # Balloon model\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear BOLD equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))\n    \"\"\",\n    inputs=['I_CBF']\n)\nIt is very similar to the interface of a Neuron model, with parameters and equations defined in two multi-line strings. The input signal I_CBF has to be explicitly defined in the inputs argument to help the BOLD monitor create the mapping.\nTo demonstrate how to create a custom BOLD model, let’s suppose we want a model that computes both the BOLD signal of the Balloon model and the one of the Davis model:\n\nDavis, T. L., Kwong, K. K., Weisskoff, R. M., and Rosen, B. R. (1998). Calibrated functional MRI: mapping the dynamics of oxidative metabolism. Proceedings of the National Academy of Sciences 95, 1834–1839\n\nWithout going into too many details, the Davis model computes the BOLD signal directly using f_in and E, without introducing a differential equation for the BOLD signal. Its implementation using the BOLD model would be:\nDavisModel = BoldModel(\n    parameters = \"\"\"\n        second = 1000.0\n        \n        phi    = 1.0    # Friston et al. (2000)\n        kappa  = 1/1.54\n        gamma  = 1/2.46\n        E_0    = 0.34\n        \n        M      = 0.149   # Griffeth & Buxton (2011)\n        alpha  = 0.14\n        beta   = 0.91\n    \"\"\",\n    equations = \"\"\"\n        # CBF-driving input as in Friston et al. (2000)\n        I_CBF    = sum(I_CBF)                                             : init=0\n        ds/dt    = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  : init=0\n        df_in/dt = s  / second                                            : init=1, min=0.01\n    ​\n        # Using part of the Balloon model to calculate r (normalized CMRO2) as in Buxton et al. (2004)\n        E        = 1 - (1 - E_0)**(1 / f_in)                              : init=0.34\n        r        = f_in * E / E_0\n        \n        # Davis model\n        BOLD     = M * (1 - f_in**alpha * (r / f_in)**beta)               : init=0\n    \"\"\",\n    inputs=['I_CBF']\n)\nNote that we could simply define two BOLD monitors using different models, but let’s create a complex model that does both for the sake of demonstration.\nLet’s first redefine the populations of the previous section:\n\n# Two populations of 100 izhikevich neurons\npop0 = ann.Population(100, neuron=ann.Izhikevich)\npop1 = ann.Population(100, neuron=ann.Izhikevich)\n\n# Set noise to create some baseline activity\npop0.noise = 5.0; pop1.noise = 5.0\n\n# Compute mean firing rate in Hz on 100ms window\npop0.compute_firing_rate(window=100.0)\npop1.compute_firing_rate(window=100.0)\n\n# Create required monitors\nmon_pop0 = ann.Monitor(pop0, [\"r\"], start=False)\nmon_pop1 = ann.Monitor(pop1, [\"r\"], start=False)\n\nWe can now create a hybrid model computing both the Balloon RN model of Stephan et al. (2007) and the Davis model:\n\nballoon_Davis = BoldModel(\n    parameters = \"\"\"\n        phi       = 1.0         ;   kappa     = 1/1.54\n        gamma     = 1/2.46      ;   E_0       = 0.34\n        tau       = 0.98        ;   alpha     = 0.33\n        V_0       = 0.02        ;   v_0       = 40.3\n        TE        = 40/1000.    ;   epsilon   = 1.43\n        r_0       = 25.         ;   second    = 1000.0\n        M         = 0.062       ;   alpha2    = 0.14\n        beta      = 0.91\n    \"\"\",\n    equations = \"\"\"\n        # CBF input\n        I_CBF          = sum(I_CBF)       \n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  \n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        # Balloon model\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear BOLD equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))\n        \n        # Davis model\n        r = f_in * E / E_0                                                         : init=1, min=0.01\n        BOLD_Davis =  M * (1 - f_in**alpha2 * (r / f_in)**beta) \n    \"\"\",\n    inputs=['I_CBF']\n)\n\nWe now only need to pass that new object to the BOLD monitor, and specify that we want to record both BOLD and BOLD_Davis:\n\nm_bold = BoldMonitor(\n    \n    populations = [pop0, pop1],  \n    \n    bold_model = balloon_Davis,\n    \n    mapping={'I_CBF': 'r'},            \n    \n    normalize_input=2000, \n    \n    recorded_variables=[\"I_CBF\", \"BOLD\", \"BOLD_Davis\"]\n)\n\nann.compile()\n\nCompiling ...  OK \n\n\nWe run the same simulation protocol and compare the two BOLD signals. Note that the value of M has been modified to give a similar amplitude to both signals:\n\n# Ramp up time\nann.simulate(1000)\n\n# Start recording\nmon_pop0.start()\nmon_pop1.start()\nm_bold.start()\n\n# we manipulate the noise for the half of the neurons\nann.simulate(5000)      # 5s with low noise\npop0.noise = 7.5\nann.simulate(5000)      # 5s with higher noise (one population)\npop0.noise = 5\nann.simulate(10000)     # 10s with low noise\n\n# retrieve the recordings\nmean_fr1 = np.mean(mon_pop0.get(\"r\"), axis=1)\nmean_fr2 = np.mean(mon_pop1.get(\"r\"), axis=1)\n\nIf_data = m_bold.get(\"I_CBF\")\nbold_data = m_bold.get(\"BOLD\")\ndavis_data = m_bold.get(\"BOLD_Davis\")\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 5))\n\n# mean firing rate\nax1 = plt.subplot(121)\nax1.plot(mean_fr1, label=\"pop0\")\nax1.plot(mean_fr2, label=\"pop1\")\nplt.legend()\nax1.set_ylabel(\"Average firing rate [Hz]\")\n\n# BOLD input signal as percent\nax2 = plt.subplot(122)\nax2.plot(bold_data*100.0, label=\"Balloon_RN\")\nax2.plot(davis_data*100.0, label=\"Davis\")\nplt.legend()\nax2.set_ylabel(\"BOLD [%]\")\n\n# x-axis labels as seconds\nfor ax in [ax1, ax2]:\n    ax.set_xticks(np.arange(0,21,2)*1000)\n    ax.set_xticklabels(np.arange(0,21,2))\n    ax.set_xlabel(\"time [s]\")\n\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/MultipleNetworks.html",
    "href": "notebooks/v4/MultipleNetworks.html",
    "title": "Parallel simulations",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis example demonstrates the use of parallel_run() to simulate the same network multiple times in parallel.\nWe start by creating the Izhikevich pulse-coupled network defined in Izhikevich.ipynb.\n\nimport numpy as np\nimport ANNarchy as ann\nann.clear()\n\nANNarchy 4.8 (4.8.3) on darwin (posix).\n\n\n\n# Create the whole population\nP = ann.Population(geometry=1000, neuron=ann.Izhikevich)\n\n# Create the excitatory population\nExc = P[:800]\nre = np.random.random(800)\nExc.noise = 5.0\nExc.a = 0.02\nExc.b = 0.2\nExc.c = -65.0 + 15.0 * re**2\nExc.d = 8.0 - 6.0 * re**2\nExc.v = -65.0\nExc.u = Exc.v * Exc.b\n\n# Create the Inh population\nInh = P[800:]\nri = np.random.random(200)\nInh.noise = 2.0\nInh.a = 0.02 + 0.08 * ri\nInh.b = 0.25 - 0.05 * ri\nInh.c = -65.0\nInh.d = 2.0\nInh.v = -65.0\nInh.u = Inh.v * Inh.b\n\n# Create the projections\nproj_exc = ann.Projection(Exc, P, 'exc')\nproj_inh = ann.Projection(Inh, P, 'inh')\n\nproj_exc.connect_all_to_all(weights=ann.Uniform(0.0, 0.5))\nproj_inh.connect_all_to_all(weights=ann.Uniform(0.0, 1.0))\n\n# Create a spike monitor\nM = ann.Monitor(P, 'spike')\n\nann.compile()\n\nCompiling ...  OK \n\n\nWe define a simulation method that re-initializes the network, runs a simulation and returns a raster plot.\nThe simulation method must take an index as first argument and a Network instance as second one.\n\ndef run_network(idx, net):\n\n    # Retrieve subpopulations\n    P_local = net.get(P)\n    Exc = P_local[:800]\n    Inh = P_local[800:]\n    \n    # Randomize initialization\n    re = np.random.random(800)\n    Exc.c = -65.0 + 15.0 * re**2\n    Exc.d = 8.0 - 6.0 * re**2\n    ri = np.random.random(200)\n    Inh.noise = 2.0\n    Inh.a = 0.02 + 0.08 * ri\n    Inh.b = 0.25 - 0.05 * ri\n    Inh.u = Inh.v * Inh.b\n    \n    # Simulate\n    net.simulate(1000.)\n    \n    # Recordings\n    t, n = net.get(M).raster_plot()\n    return t, n\n\nparallel_run() uses the multiprocessing module to start parallel processes. On Linux, it should work directly, but there is an issue on OSX. Since Python 3.8, the ‘spawn’ method is the default way to start processes, but it does not work on MacOS. The following cell should fix the issue, but it should only be ran once.\n\nimport platform\nif platform.system() == \"Darwin\":\n    import multiprocessing as mp\n    mp.set_start_method('fork')\n\nWe can now call parallel_run() to simulate 8 identical but differently initialized networks. The first call runs the simulations sequentially, while the second is in parallel.\nWe finally plot the raster plots of the two first simulations.\n\n# Run four identical simulations sequentially\nvals = ann.parallel_run(method=run_network, number=8, measure_time=True, sequential=True)\n\n# Run four identical simulations in parallel\nvals = ann.parallel_run(method=run_network, number=8, measure_time=True)\n\n# Data analysis\nt1, n1 = vals[0]\nt2, n2 = vals[1]\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.plot(t1, n1, '.')\nplt.subplot(122)\nplt.plot(t2, n2, '.')\nplt.show()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[5], line 2\n      1 # Run four identical simulations sequentially\n----&gt; 2 vals = ann.parallel_run(method=run_network, number=8, measure_time=True, sequential=True)\n      4 # Run four identical simulations in parallel\n      5 vals = ann.parallel_run(method=run_network, number=8, measure_time=True)\n\nAttributeError: module 'ANNarchy' has no attribute 'parallel_run'"
  },
  {
    "objectID": "notebooks/v4/BayesianOptimization.html",
    "href": "notebooks/v4/BayesianOptimization.html",
    "title": "Hyperparameter optimization",
    "section": "",
    "text": "#!pip install ANNarchy\n\nMost of the work in computational neuroscience is to guess the values of parameters which are not constrained by the biology. The most basic approach is to simply try out different values, run the simulation, reason about why the results are not what you want, change some parameters, run again, etc. It is very easy to get lost in this process and it requires a great deal of intuition about how the model works.\nIf you are able to define an objective function for your model (a single number that tells how well your model performs), you can use search algorithms to find this hyperparameters automatically, at the cost of running your model multiple times.\nLet’s take the example of a rate-coded model depending on two hyperparameters a and b, where is the objective is to have a minimal activity after 1 s of simulation (dummy example):\nimport ANNarchy as ann\n\npop = ann.Population(...)\n...\nann.compile()\n\ndef run(a, b):\n    pop.a = a\n    pop.b = b\n    \n    ann.simulate(1000.)\n    \n    return (pop.r)**2\nGrid search would iterate over all possible values of the parameters to perform the search:\nmin_loss = 1000.\nfor a in np.linspace(0.0, 1.0, 100):\n    for b in np.linspace(0.0, 1.0, 100):\n        loss = run(a, b)\n        if loss &lt; min_loss:\n            min_loss = loss\n            a_best = a ; b_best = b\nIf you try 100 values for each parameters, you need 10000 simulations to find your parameters. The number of simulations explodes with the number of free parameters. Moreover, you cannot stop the search before the end, as you could miss the interesting region.\nRandom search samples blindly values for the hyperparameters:\nmin_loss = 1000.\nfor _ in range(1000):\n    a = np.random.uniform(0.0, 1.0)\n    b = np.random.uniform(0.0, 1.0)\n    loss = run(a, b)\n    if loss &lt; min_loss:\n        min_loss = loss\n        a_best = a ; b_best = b\nIf you are lucky, you may find a good solution quite early in the search, so you can stop it when the loss is below a desired threshold. The main drawback is that the search may spend a lot of time in uninteresting regions: it does not learn anything between two samples.\nAn often much more efficient search method is Bayesian optimization (also called sequential model-based optimization - SMBO). It is a form of random search that updates beliefs on the hyperparameters. In short, if some parameter values do not lead to good values of the objective function in early samples, they will not be used in later samples. The search becomes more and more focused on the interesting regions of the hyperparameter space.\nAs always with Python, there are many libraries for that, including:\n\nhyperopt https://github.com/hyperopt/hyperopt\noptuna https://github.com/pfnet/optuna\ntalos (for keras models) https://github.com/autonomio/talos\n\nThis notebook demonstrates how to use hyperopt to find some hyperparameters of the COBA models already included in the ANNarchy examples:\nhttps://annarchy.github.io/notebooks/COBA.html\nAdditionally, we will use the tensorboard extension to visualize the dependency between the parameters and the objective function.\n\nimport numpy as np\n\nimport ANNarchy as ann\nfrom ANNarchy.extensions.tensorboard import Logger\nann.clear()\nann.setup(dt=0.1)\n\n\nCOBA = ann.Neuron(\n    parameters=\"\"\"\n        El = -60.0          : population\n        Vr = -60.0          : population\n        Erev_exc = 0.0      : population\n        Erev_inh = -80.0    : population\n        Vt = -50.0          : population\n        tau = 20.0          : population\n        tau_exc = 5.0       : population\n        tau_inh = 10.0      : population\n        I = 20.0            : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I\n\n        tau_exc * dg_exc/dt = - g_exc\n        tau_inh * dg_inh/dt = - g_inh\n    \"\"\",\n    spike = \"v &gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\nP = ann.Population(geometry=4000, neuron=COBA)\nPe = P[:3200]\nPi = P[3200:]\nP.v = ann.Normal(-55.0, 5.0)\nP.g_exc = ann.Normal(4.0, 1.5)\nP.g_inh = ann.Normal(20.0, 12.0)\n\nCe = ann.Projection(pre=Pe, post=P, target='exc')\nCe.connect_fixed_probability(weights=0.6, probability=0.02)\nCi = ann.Projection(pre=Pi, post=P, target='inh')\nCi.connect_fixed_probability(weights=6.7, probability=0.02)\n\nann.compile()\n\nm = ann.Monitor(P, ['spike'])\n\nCompiling ...  OK \n\n\nWith the default parameters, the COBA network fires at around 20 Hz:\n\nann.simulate(1000.0)\ndata = m.get('spike')\nfr = m.mean_fr(data)\nprint(fr)\n\n21.389999999999997\n\n\nLet’s suppose we now want the network to fire at 30 Hz. Which parameters should we change to obtain that value?\nMany parameters might influence the firing rate of the network (if not all). Here, we make the assumption that the weight values for the excitatory connections (0.6) and inhibitory ones (6.7) are the most critical ones.\nLet’s start by importing hyperopt (after installing it with pip install hyperopt):\n\nfrom hyperopt import fmin, tpe, hp, STATUS_OK\n\nWe define a trial() method taking values for the two hyperparameters as inputs. It starts by resetting the network, sets the excitatory and inhibitory weights to the desired value, simulates for one second, computes the mean firing rate of the population, logs the parameters and finally returns the objective function: the squared error between the recorded firing rate and 30 Hz.\n\nlogger = Logger()\n\ndef trial(args):\n    \n    # Retrieve the parameters\n    w_exc = args[0]\n    w_inh = args[1]\n    \n    # Reset the network\n    ann.reset()\n    \n    # Set the hyperparameters\n    Ce.w = w_exc\n    Ci.w = w_inh\n    \n    # Simulate 1 second\n    ann.simulate(1000.0)\n\n    # Retrieve the spike recordings and the membrane potential\n    spikes = m.get('spike')\n\n    # Compute the population firing rate\n    fr = m.mean_fr(spikes)\n    \n    # Compute a quadratic loss around 30 Hz\n    loss = 0.001*(fr - 30.0)**2   \n    \n    # Log the parameters\n    logger.add_parameters({'w_exc': w_exc, 'w_inh': w_inh},\n                         {'loss': loss, 'firing_rate': fr})\n    \n    return {\n        'loss': loss,\n        'status': STATUS_OK,\n        # -- store other results like this\n        'fr': fr,\n        }\n\nLogging in runs/May29_14-03-35_Juliens-MBP\n\n\nWe can check that the default parameters indeed lead to a firing rate of 20 Hz:\n\ntrial([0.6, 6.7])\n\n{'loss': 0.07413210000000005, 'status': 'ok', 'fr': 21.389999999999997}\n\n\nWe can now use hyperopt to find the hyperparameters making the network fire at 30 Hz.\nThe fmin() function takes:\n\nfn: the objective function for a set of parameters.\nspace: the search space for the hyperparameters (the prior).\nalgo: which algorithm to use, either tpe.suggest or random.suggest\nmax_evals: number of samples (simulations) to make.\n\nHere, we will sample the excitatory weights between 0.1 and 1, the inhibitory ones between 1 and 10. Of course, the smaller the range, the better. Refer to the doc of hyperopt for other sampling priors.\n\nbest = fmin(\n    fn=trial,\n    space=[\n        hp.uniform('w_exc', 0.1, 1.0), \n        hp.uniform('w_inh', 1.0, 10.0)\n    ],\n    algo=tpe.suggest,\n    max_evals=100)\nprint(best)\n\n100%|██████████| 100/100 [00:14&lt;00:00,  6.86trial/s, best loss: 8.702499999999263e-07]\n{'w_exc': 0.8752658685269741, 'w_inh': 6.989948034837044}\n\n\nAfter 100 simulations, hyperopt returns a set of hyperparameter values that make the network fire at 30Hz. We can check that it is true with:\n\ntrial([best['w_exc'], best['w_inh']])\n\n{'loss': 8.702499999999263e-07, 'status': 'ok', 'fr': 30.0295}\n\n\nThere are plenty of options to hyperopt (check Trials or the parallel search using MongoDB), but this simple example should get you started.\nIf we start tensorboard in the default directory runs/, we can additionally visualize how the firing rate depends on w_exc and w_inh in the HPARAMS tab.\n\nlogger.close()\n%load_ext tensorboard\n%tensorboard --logdir runs\n\n\n\n\nScreen Capture_select-area_20200529161854.png"
  },
  {
    "objectID": "notebooks/v4/STDP1.html",
    "href": "notebooks/v4/STDP1.html",
    "title": "STDP - single synapse",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis notebook demonstrates the online implementation of the spike time-dependent plasticity (STDP) rule for a pair of neurons.\n\nimport numpy as np\nimport ANNarchy as ann\nimport matplotlib.pyplot as plt\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\n\n\nThe STDP learning rule maintains exponentially-decaying traces for the pre-synaptic and post-synaptic spikes.\n\\tau^+ \\, \\frac{d x(t)}{dt} = -x (t)\n\\tau^- \\, \\frac{d y(t)}{dt} = -x (t)\nLTP and LTD occur at spike times depending on the corresponding traces.\n\nWhen a pre-synaptic spike occurs, x(t) is incremented and LTD is applied proportionally to y(t).\nWhen a post-synaptic spike occurs, y(t) is incremented and LTP is applied proportionally to x(t).\n\n\nSTDP = ann.Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection ; tau_minus = 20.0 : projection\n        A_plus = 0.01 : projection   ; A_minus = 0.01 : projection\n        w_min = 0.0 : projection     ; w_max = 2.0 : projection\n    \"\"\",\n    equations = \"\"\"\n    \n        tau_plus * dx/dt = -x : event-driven # pre-synaptic trace\n    \n        tau_minus * dy/dt = -y : event-driven # post-synaptic trace\n    \n    \"\"\",\n    pre_spike=\"\"\"\n        \n        g_target += w\n        \n        x += A_plus * w_max\n        \n        w = clip(w - y, w_min , w_max) # LTD\n    \"\"\",\n    post_spike=\"\"\"\n        \n        y += A_minus * w_max\n        \n        w = clip(w + x, w_min , w_max) # LTP\n    \"\"\"\n)\n\nWe create two dummy populations with one neuron each, whose spike times we can control.\n\npre = ann.SpikeSourceArray([[0.]])\npost = ann.SpikeSourceArray([[50.]])\n\nWe connect the population using a STDP synapse.\n\nproj = ann.Projection(pre, post, 'exc', STDP)\nproj.connect_all_to_all(1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x1291c8ec0&gt;\n\n\n\nann.compile()\n\nCompiling ...  OK \n\n\nThe presynaptic neuron will fire at avrious times between 0 and 100 ms, while the postsynaptic neuron keeps firing at 50 ms.\n\npre_times = np.linspace(100.0, 0.0, 101)\n\n\nweight_changes = []\nfor t_pre in pre_times:\n    \n    # Reset the populations\n    pre.clear()\n    post.clear()\n    pre.spike_times = [[t_pre]]\n    post.spike_times = [[50.0]]\n    \n    # Reset the traces\n    proj.x = 0.0\n    proj.y = 0.0\n    \n    # Weight before the simulation\n    w_before = proj[0].w[0]\n    \n    # Simulate long enough\n    ann.simulate(105.0)\n    \n    # Record weight change\n    delta_w = proj[0].w[0] - w_before\n    weight_changes.append(delta_w)\n\nWe can now plot the classical STDP figure:\n\nplt.figure(figsize=(10, 8))\nplt.plot(50. - pre_times, weight_changes, \"*\")\nplt.plot([-50, 50], [0, 0], 'k')\nplt.plot([0, 0], [min(weight_changes), max(weight_changes)], 'k')\nplt.xlabel(\"t_post - t_pre\")\nplt.ylabel(\"delta_w\")\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/PyNN.html",
    "href": "notebooks/v4/PyNN.html",
    "title": "PyNN and Brian examples",
    "section": "",
    "text": "#!pip install ANNarchy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ANNarchy as ann\n\nann.setup(dt=0.1)\n\nANNarchy 4.8 (4.8.3) on darwin (posix)."
  },
  {
    "objectID": "notebooks/v4/PyNN.html#if_curr_alpha",
    "href": "notebooks/v4/PyNN.html#if_curr_alpha",
    "title": "PyNN and Brian examples",
    "section": "IF_curr_alpha",
    "text": "IF_curr_alpha\nSimple network with a Poisson spike source projecting to a pair of IF_curr_alpha neurons.\nThis is a reimplementation of the PyNN example:\nhttp://www.neuralensemble.org/trac/PyNN/wiki/Examples/simpleNetwork\n\nann.clear()\n\n# Parameters\ntstop = 1000.0\nrate = 100.0\n\n# Create the Poisson spikes\nnumber = int(2*tstop*rate/1000.0)\nnp.random.seed(26278342)\nspike_times = np.add.accumulate(np.random.exponential(1000.0/rate, size=number))\n\n# Input population\ninp  = ann.SpikeSourceArray(list(spike_times))\n\n# Output population\npop = ann.Population(2, ann.IF_curr_alpha)\npop.tau_refrac = 2.0,\npop.v_thresh = -50.0,\npop.tau_syn_E = 2.0,\npop.tau_syn_I = 2.0\n\n# Excitatory projection\nproj = ann.Projection(inp, pop, 'exc')\nproj.connect_all_to_all(weights=1.0)\n\n# Monitor\nm = ann.Monitor(pop, ['spike', 'v'])\n\n# Compile the network\nann.compile()\n\n# Simulate\nann.simulate(tstop)\ndata = m.get()\n\n# Plot the results\nplt.figure(figsize=(12, 8))\nplt.plot(ann.dt()*np.arange(tstop/ann.dt()), data['v'][:, 0])\nplt.xlabel('Time (ms)')\nplt.ylabel('Vm (mV)')\nplt.ylim([-66.0, -48.0])\nplt.title('Simple Network')\nplt.show()\n\nCompiling ...  OK"
  },
  {
    "objectID": "notebooks/v4/PyNN.html#if_cond_exp",
    "href": "notebooks/v4/PyNN.html#if_cond_exp",
    "title": "PyNN and Brian examples",
    "section": "IF_cond_exp",
    "text": "IF_cond_exp\nA single IF neuron with exponential, conductance-based synapses, fed by two spike sources.\nThis is a reimplementation of the PyNN example:\nhttp://www.neuralensemble.org/trac/PyNN/wiki/Examples/IF_cond_exp\n\nann.clear()\n\n# Parameters\ntstop = 200.0\n\n# Input populations with predetermined spike times\nspike_sourceE = ann.SpikeSourceArray(spike_times= [float(i) for i in range(5,105,10)] )\nspike_sourceI = ann.SpikeSourceArray(spike_times= [float(i) for i in range(155,255,10)])\n\n# Population with one IF_cond_exp neuron\nifcell = ann.Population(1, ann.IF_cond_exp)\nifcell.set(\n    {   'i_offset' : 0.1,    'tau_refrac' : 3.0,\n        'v_thresh' : -51.0,  'tau_syn_E'  : 2.0,\n        'tau_syn_I': 5.0,    'v_reset'    : -70.0,\n        'e_rev_E'  : 0.,     'e_rev_I'    : -80.0 } )\n\n\n# Projections\nconnE = ann.Projection(spike_sourceE, ifcell, 'exc')\nconnE.connect_all_to_all(weights=0.006, delays=2.0)\n\nconnI = ann.Projection(spike_sourceI, ifcell, 'inh')\nconnI.connect_all_to_all(weights=0.02,  delays=4.0)\n\n# Monitor\nm = ann.Monitor(ifcell, ['spike', 'v'])\n\n# Compile the network\nann.compile()\n\n# Simulate\nann.simulate(tstop)\ndata = m.get()\n\n# Show the result\nplt.figure(figsize=(12, 8))\nplt.plot(ann.dt()*np.arange(tstop/ann.dt()), data['v'][:, 0])\nplt.xlabel('Time (ms)')\nplt.ylabel('Vm (mV)')\nplt.ylim([-66.0, -61.0])\nplt.title('IF_cond_exp')\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/PyNN.html#eif_cond_exp",
    "href": "notebooks/v4/PyNN.html#eif_cond_exp",
    "title": "PyNN and Brian examples",
    "section": "EIF_cond_exp",
    "text": "EIF_cond_exp\nNetwork of EIF neurons with exponentially decreasing conductance-based synapses.\nThis is a reimplementation of the Brian example:\nhttp://brian.readthedocs.org/en/1.4.1/examples-misc_expIF_network.html\n\nEIF = ann.Neuron(\n    parameters = \"\"\"\n        v_rest = -70.0 : population\n        cm = 0.2 : population\n        tau_m = 10.0 : population\n        tau_syn_E = 5.0 : population\n        tau_syn_I = 10.0 : population\n        e_rev_E = 0.0 : population\n        e_rev_I = -80.0 : population\n        delta_T = 3.0 : population\n        v_thresh = -55.0 : population\n        v_reset = -70.0 : population\n        v_spike = -20.0 : population\n    \"\"\",\n    equations=\"\"\"\n        dv/dt = (v_rest - v +  delta_T * exp( (v-v_thresh)/delta_T) )/tau_m + ( g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) )/cm\n\n        tau_syn_E * dg_exc/dt = - g_exc\n        tau_syn_I * dg_inh/dt = - g_inh\n    \"\"\",\n    spike = \"\"\"\n        v &gt; v_spike\n    \"\"\",\n    reset = \"\"\"\n        v = v_reset\n    \"\"\",\n    refractory = 2.0\n\n)\n\n# Poisson inputs\ni_exc = ann.PoissonPopulation(geometry=200, rates=\"if t &lt; 200.0 : 2000.0 else : 0.0\")\ni_inh = ann.PoissonPopulation(geometry=200, rates=\"if t &lt; 100.0 : 2000.0 else : 0.0\")\n\n# Main population\nP = ann.Population(geometry=4000, neuron=EIF)\n\n# Subpopulations\nPe = P[:3200]\nPi = P[3200:]\n\n# Projections\nwe = 1.5 / 1000.0 # excitatory synaptic weight\nwi = 2.5 * we # inhibitory synaptic weight\n\nCe = ann.Projection(Pe, P, 'exc')\nCe.connect_fixed_probability(weights=we, probability=0.05)\n\nCi = ann.Projection(Pi, P, 'inh')\nCi.connect_fixed_probability(weights=wi, probability=0.05)\n\nIe = ann.Projection(i_exc, P[:200], 'exc') # inputs to excitatory cells\nIe.connect_one_to_one(weights=we)\n\nIi = ann.Projection(i_inh, P[3200:3400], 'exc')# inputs to inhibitory cells\nIi.connect_one_to_one(weights=we)\n\n# Initialization of variables\nP.v = -70.0 + 10.0 * np.random.rand(P.size)\nP.g_exc = (np.random.randn(P.size) * 2.0 + 5.0) * we\nP.g_inh = (np.random.randn(P.size) * 2.0 + 5.0) * wi\n\n# Monitor\nm = ann.Monitor(P, 'spike')\n\n# Compile the Network\nann.compile()\n\n# Simulate\nann.simulate(500.0, measure_time=True)\n\n# Retrieve recordings\ndata = m.get()\nt, n = m.raster_plot(data['spike'])\n\nplt.figure(figsize=(12, 8))\nplt.plot(t, n, '.', markersize=0.2)\nplt.show()\n\nCompiling ...  OK \nSimulating 0.5 seconds of the network took 0.19683599472045898 seconds."
  },
  {
    "objectID": "notebooks/v4/PyNN.html#aeif_cond_exp",
    "href": "notebooks/v4/PyNN.html#aeif_cond_exp",
    "title": "PyNN and Brian examples",
    "section": "AEIF_cond_exp",
    "text": "AEIF_cond_exp\nAdaptive exponential integrate-and-fire model.\nhttp://www.scholarpedia.org/article/Adaptive_exponential_integrate-and-fire_model\nModel introduced in:\n\nBrette R. and Gerstner W. (2005), Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity, J. Neurophysiol. 94: 3637 - 3642.\n\nThis is a reimplementation of the Brian example:\nhttps://brian.readthedocs.io/en/stable/examples-frompapers_Brette_Gerstner_2005.html\n\nann.clear()\n\n# Create a population with one AdEx neuron\npop = ann.Population(geometry=1, neuron=ann.EIF_cond_exp_isfa_ista)\n\n# Regular spiking (paper)\npop.tau_w, pop.a, pop.b, pop.v_reset = 144.0,  4.0, 0.0805, -70.6\n\n# Bursting\n#pop.tau_w, pop.a, pop.b, pop.v_reset = 20.0, 4.0, 0.5, pop.v_thresh + 5.0\n\n# Fast spiking\n#pop.tau_w, pop.a, pop.b, pop.v_reset = 144.0, 2000.0*pop.cm/144.0, 0.0, -70.6\n\n# Monitor\nm = ann.Monitor(pop, ['spike', 'v', 'w'])\n\n# Compile the network\nann.compile()\n\n# Add current of 1 nA and simulate\nann.simulate(20.0)\npop.i_offset = 1.0\nann.simulate(100.0)\npop.i_offset = 0.0\nann.simulate(20.0)\n\n# Retrieve the results\ndata = m.get()\nspikes = data['spike'][0]\nv = data['v'][:, 0]\nw = data['w'][:, 0]\nif len(spikes)&gt;0:\n    v[spikes] = 20.0\n\n# Plot the activity\nplt.figure(figsize=(12, 8))\nplt.subplot(2,1,1)\nplt.plot(ann.dt()*np.arange(140.0/ann.dt()), v)\nplt.ylabel('v')\nplt.title('Adaptive exponential integrate-and-fire')\nplt.subplot(2,1,2)\nplt.plot(ann.dt()*np.arange(140.0/ann.dt()), w)\nplt.xlabel('Time (ms)')\nplt.ylabel('w')\nplt.show()\n\nCompiling ...  OK"
  },
  {
    "objectID": "notebooks/v4/PyNN.html#non-linear-synapses",
    "href": "notebooks/v4/PyNN.html#non-linear-synapses",
    "title": "PyNN and Brian examples",
    "section": "Non-linear synapses",
    "text": "Non-linear synapses\nA single IF neuron with two non-linear NMDA synapses.\nThis is a reimplementation of the Brian example:\nhttp://brian.readthedocs.org/en/latest/examples-synapses_nonlinear_synapses.html\n\nann.clear()\n\n# Neurons\nLinear = ann.Neuron(equations=\"dv/dt = 0.1\", spike=\"v&gt;1.0\", reset=\"v=0.0\")\nIntegrator = ann.Neuron(equations=\"dv/dt = 0.1*(g_exc -v)\", spike=\"v&gt;2.0\", reset=\"v=0.0\")\n\n# Non-linear synapse\nNMDA = ann.Synapse(\n    parameters = \"\"\"\n        tau = 10.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dx/dt = -x\n        tau * dg/dt = -g +  x * (1 -g)\n    \"\"\",\n    pre_spike = \"x += w\",\n    psp = \"g\"\n)\n\n# Populations\ninput = ann.Population(geometry=2, neuron=Linear)\ninput.v = [0.0, 0.5]\npop = ann.Population(geometry=1, neuron=Integrator)\n\n# Projection\nproj = ann.Projection(\n    pre=input, post=pop, target='exc',\n    synapse=NMDA)\nproj.connect_from_matrix(weights=[[1.0, 10.0]])\n\n# Monitors\nm = ann.Monitor(pop, 'v')\nw = ann.Monitor(proj, 'g')\n\n# Compile the network\nann.compile()\n\n# Simulate for 100 ms\nann.simulate(100.0)\n\n# Retrieve recordings\nv = m.get('v')[:, 0]\ns = w.get('g')[:, 0, :]\n\n# Plot the recordings\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2,1,1)\nplt.plot(s[:, 0])\nplt.plot(s[:, 1])\nplt.xlabel(\"Time (ms)\")\nplt.xlabel(\"Conductance\")\n\nplt.subplot(2,1,2)\nplt.plot(v)\nplt.xlabel(\"Time (ms)\")\nplt.xlabel(\"Membrane potential\")\nplt.show()\n\nWARNING: Monitor(): it is a bad idea to record synaptic variables of a projection at each time step! \nCompiling ...  OK"
  },
  {
    "objectID": "notebooks/v4/PyNN.html#stp",
    "href": "notebooks/v4/PyNN.html#stp",
    "title": "PyNN and Brian examples",
    "section": "STP",
    "text": "STP\nNetwork (CUBA) with short-term synaptic plasticity for excitatory synapses (depressing at long timescales, facilitating at short timescales).\nAdapted from :\nhttps://brian.readthedocs.io/en/stable/examples-synapses_short_term_plasticity2.html\n\nann.clear()\n\nduration = 1000.0\n\nLIF = ann.Neuron(\n    parameters = \"\"\"\n        tau_m = 20.0 : population\n        tau_e = 5.0 : population\n        tau_i = 10.0 : population\n        E_rest = -49.0 : population\n        E_thresh = -50.0 : population\n        E_reset = -60.0 : population\n    \"\"\",\n    equations = \"\"\"\n        tau_m * dv/dt = E_rest -v + g_exc - g_inh\n        tau_e * dg_exc/dt = -g_exc\n        tau_i * dg_inh/dt = -g_inh\n    \"\"\",\n    spike = \"v &gt; E_thresh\",\n    reset = \"v = E_reset\"\n)\n\nSTP = ann.Synapse(\n    parameters = \"\"\"\n        tau_rec = 200.0 : projection\n        tau_facil = 20.0 : projection\n        U = 0.2 : projection\n    \"\"\",\n    equations = \"\"\"\n        dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n        du/dt = (U - u)/tau_facil : init = 0.2, event-driven\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w * u * x\n        x *= (1 - u)\n        u += U * (1 - u)\n    \"\"\"\n)\n\n# Population\nP = ann.Population(geometry=4000, neuron=LIF)\nP.v = ann.Uniform(-60.0, -50.0)\nPe = P[:3200]\nPi = P[3200:]\n\n# Projections\ncon_e = ann.Projection(pre=Pe, post=P, target='exc', synapse = STP)\ncon_e.connect_fixed_probability(weights=1.62, probability=0.02)\n\ncon_i = ann.Projection(pre=Pi, post=P, target='inh')\ncon_i.connect_fixed_probability(weights=9.0, probability=0.02)\n\n# Monitor\nm = ann.Monitor(P, 'spike')\n\n# Compile the network\nann.compile()\n\n# Simulate without plasticity\nann.simulate(duration, measure_time=True)\n\ndata = m.get()\nt, n = m.raster_plot(data['spike'])\nrates = m.population_rate(data['spike'], 5.0)\nprint('Total number of spikes: ' + str(len(t)))\n\nplt.figure(figsize=(12, 8))\nplt.subplot(211)\nplt.plot(t, n, '.')\nplt.xlabel('Time (ms)')\nplt.ylabel('Neuron number')\nplt.subplot(212)\nplt.plot(np.arange(rates.size)*ann.dt(), rates)\nplt.show()\n\nCompiling ...  OK \nSimulating 1.0 seconds of the network took 0.07381081581115723 seconds. \nTotal number of spikes: 15508"
  },
  {
    "objectID": "notebooks/v4/AdEx.html",
    "href": "notebooks/v4/AdEx.html",
    "title": "Adaptive Exponential IF neuron",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis notebook explores how the AdEx neuron model can reproduce various spiking patterns observed in vivo.\nCode based on:\n\nNaud, R., Marcille, N., Clopath, C., and Gerstner, W. (2008). Firing patterns in the adaptive exponential integrate-and-fire model. Biol Cybern 99, 335. doi:10.1007/s00422-008-0264-7.\n\n\nimport ANNarchy as ann\nann.clear()\nann.setup(dt=0.1)\n\nThe AdEx neuron is defined by the following equations:\n\n    C \\, \\frac{dv}{dt} = -g_L \\ (v - E_L) + g_L \\, \\Delta_T \\, \\exp(\\frac{v - v_T}{\\Delta_T}) + I - w\n\n\n    \\tau_w \\, \\frac{dw}{dt} = a \\, (v - E_L) - w\n\nif v &gt; v_\\text{spike}:\n\nv = v_R\nw = w + b\n\n\nAdEx = ann.Neuron(\n    parameters=\"\"\"\n        C = 200.\n        gL = 10. # not g_L! g_ is reserved for spike transmission\n        E_L = -70.\n        v_T = -50.\n        delta_T = 2.0\n        a = 2.0\n        tau_w = 30.\n        b = 0.\n        v_r = -58.\n        I = 500.\n        v_spike = 0.0 \n    \"\"\",\n    equations=\"\"\"\n        C * dv/dt = - gL * (v - E_L) +  gL * delta_T * exp((v-v_T)/delta_T) + I - w : init=-70.0     \n        tau_w * dw/dt = a * (v - E_L) - w  : init=0.0\n    \"\"\",\n    spike=\"\"\"\n        v &gt;= v_spike\n    \"\"\",\n    reset=\"\"\"\n        v = v_r\n        w += b\n    \"\"\",\n    refractory = 2.0\n)\n\nWe create a population of 8 AdEx neurons which will get different parameter values.\n\npop = ann.Population(8, AdEx)\n\n\nann.compile()\n\nWe add a monitor to track the membrane potential and the spike timings during the simulation.\n\nm = ann.Monitor(pop, ['v', 'spike'])\n\nAs in the paper, we provide different parameters to each neuron and simulate the network for 500 ms with a fixed input current, and remove that current for an additional 50 ms.\n\n# a) tonic spiking b) adaptation, c) initial burst, d) regular bursting, e) delayed accelerating, f) delayed regular bursting, g) transcient spiking, h) irregular spiking\npop.C =       [200, 200, 130, 200, 200, 200, 100, 100]\npop.gL =      [ 10,  12,  18,  10,  12,  12,  10,  12]\npop.E_L =     [-70, -70, -58, -58, -70, -70, -65, -60]\npop.v_T =     [-50, -50, -50, -50, -50, -50, -50, -50]\npop.delta_T = [  2,   2,   2,   2,   2,   2,   2,   2]\npop.a =       [  2,   2,   4,   2,-10., -6.,-10.,-11.]\npop.tau_w =   [ 30, 300, 150, 120, 300, 300,  90, 130]\npop.b =       [  0,  60, 120, 100,   0,   0,  30,  30]\npop.v_r =     [-58, -58, -50, -46, -58, -58, -47, -48]\npop.I =       [500, 500, 400, 210, 300, 110, 350, 160]\n\n# Reset neuron\npop.v = pop.E_L\npop.w = 0.0\n\n# Simulate\nann.simulate(500.)\npop.I = 0.0\nann.simulate(50.)\n\n# Recordings\ndata = m.get('v')\nspikes = m.get('spike')\nfor n, t in spikes.items(): # Normalize the spikes\n    data[[x - m.times()['v']['start'][0] for x in t], n] = 0.0\n\nWe can now visualize the simulations:\n\nimport matplotlib.pyplot as plt\n\ntitles = [\n    \"a) tonic spiking\", \n    \"b) adaptation\", \n    \"c) initial burst\", \n    \"d) regular bursting\", \n    \"e) delayed accelerating\", \n    \"f) delayed regular bursting\", \n    \"g) transcient spiking\", \n    \"h) irregular spiking\"\n]\n\nplt.figure(figsize=(12, 15))\nplt.ylim((-70., 0.))\nfor i in range(8):\n    plt.subplot(4, 2, i+1)\n    plt.title(titles[i])\n    plt.plot(data[:, i], lw=3)\n    \n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nann.NetworkManager()\n\n&lt;ANNarchy.intern.NetworkManager.NetworkManager object at 0x106cef6e0&gt;\nNumber of registered networks = 1\nNetwork 0 (MagicNetwork)\n  populations = [Population at 0x1383089b0, ]\n  projections = []\n  monitors = [Monitor at 0x138334d70, ]\n  extensions = []\n  cyInstance = &lt;module 'ANNarchyCore0' from '/Users/vitay/Research/ANNarchy/documentation/notebooks/v4/annarchy//ANNarchyCore0.dylib'&gt; at 0x136999d00"
  },
  {
    "objectID": "notebooks/v4/GapJunctions.html",
    "href": "notebooks/v4/GapJunctions.html",
    "title": "Gap Junctions",
    "section": "",
    "text": "#!pip install ANNarchy\n\nA simple network with gap junctions.\nThis is a reimplementation of the Brian example:\nhttp://brian2.readthedocs.org/en/2.0b3/examples/synapses.gapjunctions.html\n\nimport numpy as np\nimport ANNarchy as ann\n\nann.clear()\nann.setup(dt=0.1)\n\nneuron = ann.Neuron(\n    parameters = \"v0 = 1.05: population; tau = 10.0: population\",\n    equations = \"tau*dv/dt = v0 - v + g_gap\",\n    spike = \"v &gt;  1.\",\n    reset = \"v = 0.\"\n)\n\ngap_junction = ann.Synapse(\n    psp = \"w * (pre.v - post.v)\"\n)\n\npop = ann.Population(10, neuron)\npop.v = np.linspace(0., 1., 10)\n\nproj = ann.Projection(pop, pop, 'gap', gap_junction)\nproj.connect_all_to_all(0.02)\n\ntrace = ann.Monitor(pop[0] + pop[5], 'v')\n\nann.compile()\n\nann.simulate(500.)\n\ndata = trace.get('v')\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\nplt.plot(data[:, 0])\nplt.plot(data[:, 1])\nplt.xlabel('Time (ms)')\nplt.ylabel('v')\nplt.show()\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\nCompiling ...  OK"
  },
  {
    "objectID": "notebooks/v4/ANN2SNN.html",
    "href": "notebooks/v4/ANN2SNN.html",
    "title": "ANN-to-SNN conversion - MLP",
    "section": "",
    "text": "This notebook demonstrates how to transform a fully-connected neural network trained using tensorflow/keras into an SNN network usable in ANNarchy.\nThe methods are adapted from the original models used in:\n#!pip install ANNarchy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nprint(f\"Tensorflow {tf.__version__}\")\n\nTensorflow 2.16.2\nFirst we need to download and process the MNIST dataset provided by tensorflow.\n# Download data\n(X_train, t_train), (X_test, t_test) = tf.keras.datasets.mnist.load_data()\n\n# Normalize inputs\nX_train = X_train.reshape(X_train.shape[0], 784).astype('float32') / 255.\nX_test = X_test.reshape(X_test.shape[0], 784).astype('float32') / 255.\n\n# One-hot output vectors\nT_train = tf.keras.utils.to_categorical(t_train, 10)\nT_test = tf.keras.utils.to_categorical(t_test, 10)"
  },
  {
    "objectID": "notebooks/v4/ANN2SNN.html#training-an-ann-in-tensorflowkeras",
    "href": "notebooks/v4/ANN2SNN.html#training-an-ann-in-tensorflowkeras",
    "title": "ANN-to-SNN conversion - MLP",
    "section": "Training an ANN in tensorflow/keras",
    "text": "Training an ANN in tensorflow/keras\nThe tensorflow.keras network is build using the functional API.\nThe fully-connected network has two fully connected layers with ReLU, no bias, dropout at 0.5, and a softmax output layer with 10 neurons. We use the standard SGD optimizer and the categorical crossentropy loss for classification.\n\ndef create_mlp():\n    # Model\n    inputs = tf.keras.layers.Input(shape=(784,))\n    x= tf.keras.layers.Dense(128, use_bias=False, activation='relu')(inputs)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    x= tf.keras.layers.Dense(128, use_bias=False, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    x=tf.keras.layers.Dense(10, use_bias=False, activation='softmax')(x)\n\n    model= tf.keras.Model(inputs, x)\n\n    # Optimizer\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n\n    # Loss function\n    model.compile(\n        loss='categorical_crossentropy', # loss function\n        optimizer=optimizer, # learning rule\n        metrics=['accuracy'] # show accuracy\n    )\n    print(model.summary())\n\n    return model\n\nWe can now train the network and save the weights in the HDF5 format.\n\n# Create model\nmodel = create_mlp()\n\n# Train model\nhistory = model.fit(\n    X_train, T_train,       # training data\n    batch_size=128,          # batch size\n    epochs=20,              # Maximum number of epochs\n    validation_split=0.1,   # Percentage of training data used for validation\n)\n\nmodel.save(\"runs/mlp.keras\")\n\n# Test model\npredictions_keras = model.predict(X_test, verbose=0)\ntest_loss, test_accuracy = model.evaluate(X_test, T_test, verbose=0)\nprint(f\"Test accuracy: {test_accuracy}\")\n\nModel: \"functional\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)        │ (None, 784)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 128)            │       100,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 128)            │        16,384 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 10)             │         1,280 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 118,016 (461.00 KB)\n\n\n\n Trainable params: 118,016 (461.00 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nNone\n\nEpoch 1/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.4837 - loss: 1.5160 - val_accuracy: 0.9123 - val_loss: 0.3443\n\nEpoch 2/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8178 - loss: 0.5996 - val_accuracy: 0.9310 - val_loss: 0.2473\n\nEpoch 3/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8566 - loss: 0.4747 - val_accuracy: 0.9405 - val_loss: 0.2079\n\nEpoch 4/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8771 - loss: 0.4162 - val_accuracy: 0.9492 - val_loss: 0.1831\n\nEpoch 5/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8934 - loss: 0.3715 - val_accuracy: 0.9532 - val_loss: 0.1676\n\nEpoch 6/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8999 - loss: 0.3453 - val_accuracy: 0.9588 - val_loss: 0.1531\n\nEpoch 7/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9066 - loss: 0.3198 - val_accuracy: 0.9612 - val_loss: 0.1427\n\nEpoch 8/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9113 - loss: 0.3056 - val_accuracy: 0.9648 - val_loss: 0.1340\n\nEpoch 9/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9183 - loss: 0.2816 - val_accuracy: 0.9648 - val_loss: 0.1290\n\nEpoch 10/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9229 - loss: 0.2682 - val_accuracy: 0.9653 - val_loss: 0.1226\n\nEpoch 11/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9225 - loss: 0.2673 - val_accuracy: 0.9678 - val_loss: 0.1180\n\nEpoch 12/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9239 - loss: 0.2624 - val_accuracy: 0.9683 - val_loss: 0.1135\n\nEpoch 13/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9272 - loss: 0.2503 - val_accuracy: 0.9693 - val_loss: 0.1106\n\nEpoch 14/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9281 - loss: 0.2475 - val_accuracy: 0.9695 - val_loss: 0.1103\n\nEpoch 15/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9319 - loss: 0.2331 - val_accuracy: 0.9712 - val_loss: 0.1040\n\nEpoch 16/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9368 - loss: 0.2208 - val_accuracy: 0.9713 - val_loss: 0.1031\n\nEpoch 17/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9369 - loss: 0.2183 - val_accuracy: 0.9720 - val_loss: 0.1028\n\nEpoch 18/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9352 - loss: 0.2153 - val_accuracy: 0.9737 - val_loss: 0.1000\n\nEpoch 19/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9426 - loss: 0.2030 - val_accuracy: 0.9737 - val_loss: 0.0971\n\nEpoch 20/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9430 - loss: 0.2052 - val_accuracy: 0.9727 - val_loss: 0.0987\n\nTest accuracy: 0.9652000069618225\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/v4/ANN2SNN.html#initialize-the-ann-to-snn-converter",
    "href": "notebooks/v4/ANN2SNN.html#initialize-the-ann-to-snn-converter",
    "title": "ANN-to-SNN conversion - MLP",
    "section": "Initialize the ANN-to-SNN converter",
    "text": "Initialize the ANN-to-SNN converter\nWe first create an instance of the ANN-to-SNN conversion object. The function receives the input_encoding parameter, which is the type of input encoding we want to use.\nBy default, there are intrinsically bursting (IB), phase shift oscillation (PSO) and Poisson (poisson) available.\n\nfrom ANNarchy.extensions.ann_to_snn_conversion import ANNtoSNNConverter\n\nsnn_converter = ANNtoSNNConverter(\n    input_encoding='IB', \n    hidden_neuron='IaF',\n    read_out='spike_count',\n)\n\nANNarchy 4.8 (4.8.3) on darwin (posix).\n\n\nAfter that, we provide the TensorFlow model stored as a .keras file to the conversion tool. The print-out of the network structure of the imported network is suppressed when show_info=False is provided to load_keras_model.\n\nnet = snn_converter.load_keras_model(\"runs/mlp.keras\", show_info=True)\n\nWARNING: Dense representation is an experimental feature for spiking models, we greatly appreciate bug reports. \n* Input layer: input_layer, (784,)\n* InputLayer skipped.\n* Dense layer: dense, 128 \n    weights: (128, 784)\n    mean -0.0038075943011790514, std 0.05276760458946228\n    min -0.32009223103523254, max 0.24077153205871582\n* Dropout skipped.\n* Dense layer: dense_1, 128 \n    weights: (128, 128)\n    mean 0.0048642707988619804, std 0.10200534760951996\n    min -0.2624298334121704, max 0.4079423248767853\n* Dropout skipped.\n* Dense layer: dense_2, 10 \n    weights: (10, 128)\n    mean -0.0005833255127072334, std 0.21552757918834686\n    min -0.5742316246032715, max 0.4535660445690155\n\n\n\nWhen the network has been built successfully, we can perform a test using all MNIST training samples. Using duration_per_sample, the duration simulated for each image can be specified. Here, 200 ms seem to be enough.\n\npredictions_snn = snn_converter.predict(X_test, duration_per_sample=200)\n\n  0%|          | 0/10000 [00:00&lt;?, ?it/s]100%|██████████| 10000/10000 [00:54&lt;00:00, 182.00it/s]\n\n\nUsing the recorded predictions, we can now compute the accuracy using scikit-learn for all presented samples.\n\nfrom sklearn.metrics import classification_report, accuracy_score\n\nprint(classification_report(t_test, predictions_snn))\nprint(\"Test accuracy of the SNN:\", accuracy_score(t_test, predictions_snn))\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.99      0.98       980\n           1       0.98      0.98      0.98      1135\n           2       0.96      0.97      0.96      1032\n           3       0.94      0.97      0.95      1010\n           4       0.97      0.95      0.96       982\n           5       0.97      0.94      0.95       892\n           6       0.96      0.97      0.97       958\n           7       0.96      0.96      0.96      1028\n           8       0.96      0.94      0.95       974\n           9       0.96      0.95      0.95      1009\n\n    accuracy                           0.96     10000\n   macro avg       0.96      0.96      0.96     10000\nweighted avg       0.96      0.96      0.96     10000\n\nTest accuracy of the SNN: 0.9623\n\n\nFor comparison, here is the performance of the original ANN in keras:\n\nprint(classification_report(t_test, predictions_keras.argmax(axis=1)))\nprint(\"Test accuracy of the ANN:\", accuracy_score(t_test, predictions_keras.argmax(axis=1)))\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.98       980\n           1       0.98      0.99      0.98      1135\n           2       0.96      0.97      0.96      1032\n           3       0.94      0.97      0.96      1010\n           4       0.96      0.96      0.96       982\n           5       0.97      0.95      0.96       892\n           6       0.96      0.97      0.97       958\n           7       0.97      0.97      0.97      1028\n           8       0.97      0.94      0.96       974\n           9       0.97      0.94      0.96      1009\n\n    accuracy                           0.97     10000\n   macro avg       0.97      0.96      0.96     10000\nweighted avg       0.97      0.97      0.97     10000\n\nTest accuracy of the ANN: 0.9652"
  },
  {
    "objectID": "notebooks/v4/index.html",
    "href": "notebooks/v4/index.html",
    "title": "List of notebooks",
    "section": "",
    "text": "This section provides a list of the sample models provided in the examples/ directory of the source code.\nThe Jupyter notebooks can be downloaded from:\nhttps://github.com/ANNarchy/ANNarchy.github.io/tree/master/notebooks\n\n\n\nEcho-state networks: echo-state networks are the rate-coded version of reservoir computing (Jaeger, 2001).\nNeural Field: a simple rate-coded model without learning using neural fields.\nBCM learning rule: basic demonstration of the Intrator & Cooper BCM learning rule.\nBar learning: an implementation of the bar learning problem, illustrating synaptic plasticity in rate-coded networks.\nMiconi: Reward-modulated recurrent network based on Miconi (2017).\nStructural Plasticity: a dummy example demonstrating structural plasticity.\n\n\n\n\n\nAdEx: how the AdEx neuron model (adaptive exponential) can reproduce various spiking patterns in vivo (Naud et al. 2008).\nPyNN/Brian: a set of single neuron models reproducing various examples from PyNN and Brian.\nIzhikevich: an implementation of the simple pulse-coupled network described in (Izhikevich, 2003). It shows how to build a simple spiking network without synaptic plasticity.\nGap Junctions: an example using gap junctions.\nHodgkinHuxley: a single Hodgkin-Huxley neuron.\nCOBA and CUBA: an implementation of the balanced network described in (Vogels and Abbott, 2005). It shows how to build a simple spiking network using integrate-and-fire neurons and sparse connectivity.\nSTP: an example of short-term plasticity based on the model of Tsodyks, Uziel and Markram (2000).\nSTDP I and II: two simple examples using spike-timing dependent plasticity (STDP).\nRamp: an example of homeostatic STDP based on the model of Carlson, Richert, Dutt and Krichmar (2013).\n\n\n\n\n\nHybrid networks: a simple hybrid network with both rate-coded and spiking sub-parts.\nParallel simulations: shows how to call parallel_run to run several networks in parallel.\nBayesian optimization: a demo showing how to use hyperopt to search for hyperparameters of a model.\n\n\n\n\n\nImage and Convolution: shows how to use the ImagePopulation class of the image extension to clamp directly images and video streams into a rate-coded network. Also demonstrates the convolution extension.\nLogging with tensorboard: a simple basal ganglia model to show how to use the tensorboard extension.\nBOLD monitoring I and II: a showcase of the bold extension allowing to record BOLD signals fron a network.\nANN2SNN I and II: demonstrates the ANN-to-SNN conversion tool using the MNIST dataset for a MLP and a CNN."
  },
  {
    "objectID": "notebooks/v4/index.html#rate-coded-networks",
    "href": "notebooks/v4/index.html#rate-coded-networks",
    "title": "List of notebooks",
    "section": "",
    "text": "Echo-state networks: echo-state networks are the rate-coded version of reservoir computing (Jaeger, 2001).\nNeural Field: a simple rate-coded model without learning using neural fields.\nBCM learning rule: basic demonstration of the Intrator & Cooper BCM learning rule.\nBar learning: an implementation of the bar learning problem, illustrating synaptic plasticity in rate-coded networks.\nMiconi: Reward-modulated recurrent network based on Miconi (2017).\nStructural Plasticity: a dummy example demonstrating structural plasticity."
  },
  {
    "objectID": "notebooks/v4/index.html#spiking-networks",
    "href": "notebooks/v4/index.html#spiking-networks",
    "title": "List of notebooks",
    "section": "",
    "text": "AdEx: how the AdEx neuron model (adaptive exponential) can reproduce various spiking patterns in vivo (Naud et al. 2008).\nPyNN/Brian: a set of single neuron models reproducing various examples from PyNN and Brian.\nIzhikevich: an implementation of the simple pulse-coupled network described in (Izhikevich, 2003). It shows how to build a simple spiking network without synaptic plasticity.\nGap Junctions: an example using gap junctions.\nHodgkinHuxley: a single Hodgkin-Huxley neuron.\nCOBA and CUBA: an implementation of the balanced network described in (Vogels and Abbott, 2005). It shows how to build a simple spiking network using integrate-and-fire neurons and sparse connectivity.\nSTP: an example of short-term plasticity based on the model of Tsodyks, Uziel and Markram (2000).\nSTDP I and II: two simple examples using spike-timing dependent plasticity (STDP).\nRamp: an example of homeostatic STDP based on the model of Carlson, Richert, Dutt and Krichmar (2013)."
  },
  {
    "objectID": "notebooks/v4/index.html#advanced-features",
    "href": "notebooks/v4/index.html#advanced-features",
    "title": "List of notebooks",
    "section": "",
    "text": "Hybrid networks: a simple hybrid network with both rate-coded and spiking sub-parts.\nParallel simulations: shows how to call parallel_run to run several networks in parallel.\nBayesian optimization: a demo showing how to use hyperopt to search for hyperparameters of a model."
  },
  {
    "objectID": "notebooks/v4/index.html#extensions",
    "href": "notebooks/v4/index.html#extensions",
    "title": "List of notebooks",
    "section": "",
    "text": "Image and Convolution: shows how to use the ImagePopulation class of the image extension to clamp directly images and video streams into a rate-coded network. Also demonstrates the convolution extension.\nLogging with tensorboard: a simple basal ganglia model to show how to use the tensorboard extension.\nBOLD monitoring I and II: a showcase of the bold extension allowing to record BOLD signals fron a network.\nANN2SNN I and II: demonstrates the ANN-to-SNN conversion tool using the MNIST dataset for a MLP and a CNN."
  },
  {
    "objectID": "notebooks/v4/STDP2.html",
    "href": "notebooks/v4/STDP2.html",
    "title": "STDP - network",
    "section": "",
    "text": "#!pip install ANNarchy\n\nA simple model showing the STDP learning rule on inputs converginf to a single neuron. Model adapted from Song, Miller and Abbott (2000) and Song and Abbott (2001)\nCode adapted from the Brian example: http://brian.readthedocs.org/en/1.4.1/examples-plasticity_STDP1.html\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nANNarchy 4.8 (4.8.2) on darwin (posix).\n\n\nSome parameters:\n\nF = 15.0 # Poisson distribution at 15 Hz\nN = 1000 # 1000 Poisson inputs\ngmax = 0.01 # Maximum weight\nduration = 100000.0 # Simulation for 100 seconds\n\nIntegrate-and-fire neuron:\n\nIF = ann.Neuron(\n    parameters = \"\"\"\n        tau_m = 10.0\n        tau_e = 5.0 \n        vt = -54.0 \n        vr = -60.0 \n        El = -74.0 \n        Ee = 0.0 \n    \"\"\",\n    equations = \"\"\"\n        tau_m * dv/dt = El - v + g_exc * (Ee - vr) : init = -60.0\n        tau_e * dg_exc/dt = - g_exc\n    \"\"\",\n    spike = \"\"\"\n        v &gt; vt\n    \"\"\",\n    reset = \"\"\"\n        v = vr\n    \"\"\"\n)\n\nAn input population of Poisson neurons, and a single post-synaptic neuron.\n\n# Input population\nInput = ann.PoissonPopulation(name = 'Input', geometry=N, rates=F)\n\n# Output neuron\nOutput = ann.Population(name = 'Output', geometry=1, neuron=IF)\n\n# Projection learned using STDP\nproj = ann.Projection( \n    pre = Input, \n    post = Output, \n    target = 'exc',\n    synapse = ann.STDP(tau_plus=20.0, tau_minus=20.0, A_plus=0.01, A_minus=0.0105, w_max=0.01)\n)\nproj.connect_all_to_all(weights=ann.Uniform(0.0, gmax))\n\n\n# Compile the network\nann.compile()\n\nCompiling ...  OK \n\n\n\n# Start recording\nMi = ann.Monitor(Input, 'spike') \nMo = ann.Monitor(Output, 'spike')\n\n# Start the simulation\nann.simulate(duration, measure_time=True)\n\n# Retrieve the recordings\ninput_spikes = Mi.get('spike')\noutput_spikes = Mo.get('spike')\n\nSimulating 100.0 seconds of the network took 0.48438501358032227 seconds. \n\n\n\n# Compute the mean firing rates during the simulation\nprint('Mean firing rate in the input population: ' + str(Mi.mean_fr(input_spikes)) )\nprint('Mean firing rate of the output neuron: ' + str(Mo.mean_fr(output_spikes)) )\n\n# Compute the instantaneous firing rate of the output neuron\noutput_rate = Mo.smoothed_rate(output_spikes, 100.0)\n\n# Receptive field after simulation\nweights = proj.w[0]\n\nMean firing rate in the input population: 15.0327\nMean firing rate of the output neuron: 26.99\n\n\n\nplt.figure(figsize=(12, 10))\nplt.subplot(3,1,1)\nplt.title('Firing rate')\nplt.plot(output_rate[0, :])\nplt.subplot(3,1,2)\nplt.title('Weights')\nplt.plot(weights, '.')\nplt.subplot(3,1,3)\nplt.title('Weights histogram')\nplt.hist(weights, bins=20)\nplt.show()"
  },
  {
    "objectID": "notebooks/GapJunctions.html",
    "href": "notebooks/GapJunctions.html",
    "title": "Gap Junctions",
    "section": "",
    "text": "#!pip install ANNarchy\n\nA simple network with gap junctions.\nThis is a reimplementation of the Brian example:\nhttp://brian2.readthedocs.org/en/2.0b3/examples/synapses.gapjunctions.html\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ANNarchy as ann\n\nneuron = ann.Neuron(\n    parameters = dict(v0 = 1.05, tau = 10.0),\n    equations = \"tau * dv/dt = v0 - v + g_gap\",\n    spike = \"v &gt;  1.\",\n    reset = \"v = 0.\"\n)\n\ngap_junction = ann.Synapse(\n    psp = \"w * (pre.v - post.v)\"\n)\n\nnet = ann.Network(dt=0.1)\npop = net.create(10, neuron)\npop.v = np.linspace(0., 1., 10)\n\nproj = net.connect(pop, pop, 'gap', gap_junction)\nproj.all_to_all(0.02)\n\ntrace = net.monitor(pop[0] + pop[5], 'v')\n\nnet.compile()\n\nnet.simulate(500.)\n\ndata = trace.get('v')\n\nplt.figure(figsize=(10, 8))\nplt.plot(data[:, 0])\nplt.plot(data[:, 1])\nplt.xlabel('Time (ms)')\nplt.ylabel('v')\nplt.show()\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\nCompiling network 1...  OK",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Gap junctions"
    ]
  },
  {
    "objectID": "notebooks/ANN2SNN.html",
    "href": "notebooks/ANN2SNN.html",
    "title": "ANN-to-SNN conversion - MLP",
    "section": "",
    "text": "This notebook demonstrates how to transform a fully-connected neural network trained using tensorflow/keras into an SNN network usable in ANNarchy.\nThe methods are adapted from the original models used in:\n#!pip install ANNarchy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nprint(f\"Tensorflow {tf.__version__}\")\n\nTensorflow 2.16.2\nFirst we need to download and process the MNIST dataset provided by tensorflow.\n# Download data\n(X_train, t_train), (X_test, t_test) = tf.keras.datasets.mnist.load_data()\n\n# Normalize inputs\nX_train = X_train.reshape(X_train.shape[0], 784).astype('float32') / 255.\nX_test = X_test.reshape(X_test.shape[0], 784).astype('float32') / 255.\n\n# One-hot output vectors\nT_train = tf.keras.utils.to_categorical(t_train, 10)\nT_test = tf.keras.utils.to_categorical(t_test, 10)",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "ANN to SNN I"
    ]
  },
  {
    "objectID": "notebooks/ANN2SNN.html#training-an-ann-in-tensorflowkeras",
    "href": "notebooks/ANN2SNN.html#training-an-ann-in-tensorflowkeras",
    "title": "ANN-to-SNN conversion - MLP",
    "section": "Training an ANN in tensorflow/keras",
    "text": "Training an ANN in tensorflow/keras\nThe tensorflow.keras network is build using the functional API.\nThe fully-connected network has two fully connected layers with ReLU, no bias, dropout at 0.5, and a softmax output layer with 10 neurons. We use the standard SGD optimizer and the categorical crossentropy loss for classification.\n\ndef create_mlp():\n    # Model\n    inputs = tf.keras.layers.Input(shape=(784,))\n    x= tf.keras.layers.Dense(128, use_bias=False, activation='relu')(inputs)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    x= tf.keras.layers.Dense(128, use_bias=False, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    x=tf.keras.layers.Dense(10, use_bias=False, activation='softmax')(x)\n\n    model= tf.keras.Model(inputs, x)\n\n    # Optimizer\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n\n    # Loss function\n    model.compile(\n        loss='categorical_crossentropy', # loss function\n        optimizer=optimizer, # learning rule\n        metrics=['accuracy'] # show accuracy\n    )\n    print(model.summary())\n\n    return model\n\nWe can now train the network and save the weights in the HDF5 format.\n\n# Create model\nmodel = create_mlp()\n\n# Train model\nhistory = model.fit(\n    X_train, T_train,       # training data\n    batch_size=128,          # batch size\n    epochs=20,              # Maximum number of epochs\n    validation_split=0.1,   # Percentage of training data used for validation\n)\n\nmodel.save(\"runs/mlp.keras\")\n\n# Test model\npredictions_keras = model.predict(X_test, verbose=0)\ntest_loss, test_accuracy = model.evaluate(X_test, T_test, verbose=0)\nprint(f\"Test accuracy: {test_accuracy}\")\n\nModel: \"functional\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)        │ (None, 784)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 128)            │       100,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 128)            │        16,384 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 10)             │         1,280 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 118,016 (461.00 KB)\n\n\n\n Trainable params: 118,016 (461.00 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nNone\n\nEpoch 1/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.4845 - loss: 1.5137 - val_accuracy: 0.9093 - val_loss: 0.3401\n\nEpoch 2/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8202 - loss: 0.5868 - val_accuracy: 0.9325 - val_loss: 0.2403\n\nEpoch 3/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8576 - loss: 0.4765 - val_accuracy: 0.9397 - val_loss: 0.2055\n\nEpoch 4/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8795 - loss: 0.4142 - val_accuracy: 0.9478 - val_loss: 0.1804\n\nEpoch 5/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8931 - loss: 0.3656 - val_accuracy: 0.9528 - val_loss: 0.1633\n\nEpoch 6/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9040 - loss: 0.3385 - val_accuracy: 0.9572 - val_loss: 0.1492\n\nEpoch 7/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9083 - loss: 0.3170 - val_accuracy: 0.9597 - val_loss: 0.1424\n\nEpoch 8/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9148 - loss: 0.3010 - val_accuracy: 0.9650 - val_loss: 0.1317\n\nEpoch 9/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9192 - loss: 0.2857 - val_accuracy: 0.9632 - val_loss: 0.1269\n\nEpoch 10/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9237 - loss: 0.2655 - val_accuracy: 0.9648 - val_loss: 0.1218\n\nEpoch 11/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9240 - loss: 0.2662 - val_accuracy: 0.9670 - val_loss: 0.1189\n\nEpoch 12/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9265 - loss: 0.2539 - val_accuracy: 0.9677 - val_loss: 0.1108\n\nEpoch 13/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9303 - loss: 0.2385 - val_accuracy: 0.9693 - val_loss: 0.1092\n\nEpoch 14/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9335 - loss: 0.2353 - val_accuracy: 0.9713 - val_loss: 0.1049\n\nEpoch 15/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9342 - loss: 0.2312 - val_accuracy: 0.9707 - val_loss: 0.1053\n\nEpoch 16/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9347 - loss: 0.2231 - val_accuracy: 0.9708 - val_loss: 0.1041\n\nEpoch 17/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9398 - loss: 0.2122 - val_accuracy: 0.9718 - val_loss: 0.0996\n\nEpoch 18/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9402 - loss: 0.2086 - val_accuracy: 0.9747 - val_loss: 0.0946\n\nEpoch 19/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9394 - loss: 0.2090 - val_accuracy: 0.9723 - val_loss: 0.0969\n\nEpoch 20/20\n\n422/422 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9406 - loss: 0.2005 - val_accuracy: 0.9728 - val_loss: 0.0944\n\nTest accuracy: 0.9666000008583069\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "ANN to SNN I"
    ]
  },
  {
    "objectID": "notebooks/ANN2SNN.html#initialize-the-ann-to-snn-converter",
    "href": "notebooks/ANN2SNN.html#initialize-the-ann-to-snn-converter",
    "title": "ANN-to-SNN conversion - MLP",
    "section": "Initialize the ANN-to-SNN converter",
    "text": "Initialize the ANN-to-SNN converter\nWe first create an instance of the ANN-to-SNN conversion object. The function receives the input_encoding parameter, which is the type of input encoding we want to use.\nBy default, there are intrinsically bursting (IB), phase shift oscillation (PSO) and Poisson (poisson) available.\n\nfrom ANNarchy.extensions.ann_to_snn_conversion import ANNtoSNNConverter\n\nsnn_converter = ANNtoSNNConverter(\n    input_encoding='IB', \n    hidden_neuron='IaF',\n    read_out='spike_count',\n)\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nAfter that, we provide the TensorFlow model stored as a .keras file to the conversion tool. The print-out of the network structure of the imported network is suppressed when show_info=False is provided to load_keras_model.\n\nnet = snn_converter.load_keras_model(\"runs/mlp.keras\", show_info=True)\n\nWARNING: Dense representation is an experimental feature for spiking models, we greatly appreciate bug reports. \n* Input layer: input_layer, (784,)\n* InputLayer skipped.\n* Dense layer: dense, 128 \n    weights: (128, 784)\n    mean -0.004177612718194723, std 0.05281704664230347\n    min -0.3429253101348877, max 0.22064846754074097\n* Dropout skipped.\n* Dense layer: dense_1, 128 \n    weights: (128, 128)\n    mean 0.005270183552056551, std 0.10235019028186798\n    min -0.28134748339653015, max 0.39932867884635925\n* Dropout skipped.\n* Dense layer: dense_2, 10 \n    weights: (10, 128)\n    mean 0.00408650329336524, std 0.21635150909423828\n    min -0.5984256267547607, max 0.46855056285858154\n\n\n\nWhen the network has been built successfully, we can perform a test using all MNIST training samples. Using duration_per_sample, the duration simulated for each image can be specified. Here, 200 ms seem to be enough.\n\npredictions_snn = snn_converter.predict(X_test, duration_per_sample=200)\n\n100%|██████████| 10000/10000 [00:56&lt;00:00, 178.57it/s]\n\n\nUsing the recorded predictions, we can now compute the accuracy using scikit-learn for all presented samples.\n\nfrom sklearn.metrics import classification_report, accuracy_score\n\nprint(classification_report(t_test, predictions_snn))\nprint(\"Test accuracy of the SNN:\", accuracy_score(t_test, predictions_snn))\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.99      0.98       980\n           1       0.98      0.98      0.98      1135\n           2       0.96      0.97      0.96      1032\n           3       0.96      0.95      0.96      1010\n           4       0.97      0.96      0.96       982\n           5       0.96      0.95      0.95       892\n           6       0.96      0.97      0.96       958\n           7       0.97      0.96      0.96      1028\n           8       0.94      0.95      0.95       974\n           9       0.97      0.94      0.96      1009\n\n    accuracy                           0.96     10000\n   macro avg       0.96      0.96      0.96     10000\nweighted avg       0.96      0.96      0.96     10000\n\nTest accuracy of the SNN: 0.9627\n\n\nFor comparison, here is the performance of the original ANN in keras:\n\nprint(classification_report(t_test, predictions_keras.argmax(axis=1)))\nprint(\"Test accuracy of the ANN:\", accuracy_score(t_test, predictions_keras.argmax(axis=1)))\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.99      0.98       980\n           1       0.98      0.99      0.98      1135\n           2       0.97      0.97      0.97      1032\n           3       0.97      0.96      0.97      1010\n           4       0.96      0.96      0.96       982\n           5       0.96      0.95      0.96       892\n           6       0.95      0.97      0.96       958\n           7       0.96      0.97      0.96      1028\n           8       0.96      0.96      0.96       974\n           9       0.98      0.94      0.96      1009\n\n    accuracy                           0.97     10000\n   macro avg       0.97      0.97      0.97     10000\nweighted avg       0.97      0.97      0.97     10000\n\nTest accuracy of the ANN: 0.9666",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "ANN to SNN I"
    ]
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "List of notebooks",
    "section": "",
    "text": "This section provides a list of the sample models provided in the examples/ directory of the source code.\nThe Jupyter notebooks can be downloaded from:\nhttps://github.com/ANNarchy/ANNarchy.github.io/tree/master/notebooks\n\n\n\nEcho-state networks: echo-state networks are the rate-coded version of reservoir computing (Jaeger, 2001).\nNeural Field: a simple rate-coded model without learning using neural fields.\nBCM learning rule: basic demonstration of the Intrator & Cooper BCM learning rule.\nBar learning: an implementation of the bar learning problem, illustrating synaptic plasticity in rate-coded networks.\nMiconi: Reward-modulated recurrent network based on Miconi (2017).\nStructural Plasticity: a dummy example demonstrating structural plasticity.\n\n\n\n\n\nAdEx: how the AdEx neuron model (adaptive exponential) can reproduce various spiking patterns in vivo (Naud et al. 2008).\nPyNN/Brian: a set of single neuron models reproducing various examples from PyNN and Brian.\nIzhikevich: an implementation of the simple pulse-coupled network described in (Izhikevich, 2003). It shows how to build a simple spiking network without synaptic plasticity.\nGap Junctions: an example using gap junctions.\nHodgkinHuxley: a single Hodgkin-Huxley neuron.\nCOBA and CUBA: an implementation of the balanced network described in (Vogels and Abbott, 2005). It shows how to build a simple spiking network using integrate-and-fire neurons and sparse connectivity.\nSTP: an example of short-term plasticity based on the model of Tsodyks, Uziel and Markram (2000).\nSTDP I and II: two simple examples using spike-timing dependent plasticity (STDP).\nRamp: an example of homeostatic STDP based on the model of Carlson, Richert, Dutt and Krichmar (2013).\n\n\n\n\n\nHybrid networks: a simple hybrid network with both rate-coded and spiking sub-parts.\nParallel simulations: shows how to call parallel_run to run several networks in parallel.\nBayesian optimization: a demo showing how to use hyperopt to search for hyperparameters of a model.\n\n\n\n\n\nImage and Convolution: shows how to use the ImagePopulation class of the image extension to clamp directly images and video streams into a rate-coded network. Also demonstrates the convolution extension.\nLogging with tensorboard: a simple basal ganglia model to show how to use the tensorboard extension.\nBOLD monitoring I and II: a showcase of the bold extension allowing to record BOLD signals fron a network.\nANN2SNN I and II: demonstrates the ANN-to-SNN conversion tool using the MNIST dataset for a MLP and a CNN.",
    "crumbs": [
      "Notebooks",
      "List of notebooks"
    ]
  },
  {
    "objectID": "notebooks/index.html#rate-coded-networks",
    "href": "notebooks/index.html#rate-coded-networks",
    "title": "List of notebooks",
    "section": "",
    "text": "Echo-state networks: echo-state networks are the rate-coded version of reservoir computing (Jaeger, 2001).\nNeural Field: a simple rate-coded model without learning using neural fields.\nBCM learning rule: basic demonstration of the Intrator & Cooper BCM learning rule.\nBar learning: an implementation of the bar learning problem, illustrating synaptic plasticity in rate-coded networks.\nMiconi: Reward-modulated recurrent network based on Miconi (2017).\nStructural Plasticity: a dummy example demonstrating structural plasticity.",
    "crumbs": [
      "Notebooks",
      "List of notebooks"
    ]
  },
  {
    "objectID": "notebooks/index.html#spiking-networks",
    "href": "notebooks/index.html#spiking-networks",
    "title": "List of notebooks",
    "section": "",
    "text": "AdEx: how the AdEx neuron model (adaptive exponential) can reproduce various spiking patterns in vivo (Naud et al. 2008).\nPyNN/Brian: a set of single neuron models reproducing various examples from PyNN and Brian.\nIzhikevich: an implementation of the simple pulse-coupled network described in (Izhikevich, 2003). It shows how to build a simple spiking network without synaptic plasticity.\nGap Junctions: an example using gap junctions.\nHodgkinHuxley: a single Hodgkin-Huxley neuron.\nCOBA and CUBA: an implementation of the balanced network described in (Vogels and Abbott, 2005). It shows how to build a simple spiking network using integrate-and-fire neurons and sparse connectivity.\nSTP: an example of short-term plasticity based on the model of Tsodyks, Uziel and Markram (2000).\nSTDP I and II: two simple examples using spike-timing dependent plasticity (STDP).\nRamp: an example of homeostatic STDP based on the model of Carlson, Richert, Dutt and Krichmar (2013).",
    "crumbs": [
      "Notebooks",
      "List of notebooks"
    ]
  },
  {
    "objectID": "notebooks/index.html#advanced-features",
    "href": "notebooks/index.html#advanced-features",
    "title": "List of notebooks",
    "section": "",
    "text": "Hybrid networks: a simple hybrid network with both rate-coded and spiking sub-parts.\nParallel simulations: shows how to call parallel_run to run several networks in parallel.\nBayesian optimization: a demo showing how to use hyperopt to search for hyperparameters of a model.",
    "crumbs": [
      "Notebooks",
      "List of notebooks"
    ]
  },
  {
    "objectID": "notebooks/index.html#extensions",
    "href": "notebooks/index.html#extensions",
    "title": "List of notebooks",
    "section": "",
    "text": "Image and Convolution: shows how to use the ImagePopulation class of the image extension to clamp directly images and video streams into a rate-coded network. Also demonstrates the convolution extension.\nLogging with tensorboard: a simple basal ganglia model to show how to use the tensorboard extension.\nBOLD monitoring I and II: a showcase of the bold extension allowing to record BOLD signals fron a network.\nANN2SNN I and II: demonstrates the ANN-to-SNN conversion tool using the MNIST dataset for a MLP and a CNN.",
    "crumbs": [
      "Notebooks",
      "List of notebooks"
    ]
  },
  {
    "objectID": "notebooks/STDP2.html",
    "href": "notebooks/STDP2.html",
    "title": "STDP - network",
    "section": "",
    "text": "#!pip install ANNarchy\n\nA simple model showing the STDP learning rule on inputs converginf to a single neuron. Model adapted from Song, Miller and Abbott (2000) and Song and Abbott (2001)\nCode adapted from the Brian example: http://brian.readthedocs.org/en/1.4.1/examples-plasticity_STDP1.html\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nANNarchy 5.0 (5.0.0) on darwin (posix).\n\n\nSome parameters:\n\nF = 15.0 # Poisson distribution at 15 Hz\nN = 1000 # 1000 Poisson inputs\ngmax = 0.01 # Maximum weight\nduration = 100000.0 # Simulation for 100 seconds\n\nIntegrate-and-fire neuron:\n\nIF = ann.Neuron(\n    parameters = dict(\n        tau_m = 10.0,\n        tau_e = 5.0 ,\n        vt = -54.0 ,\n        vr = -60.0 ,\n        El = -74.0 ,\n        Ee = 0.0 ,\n    ),\n    equations = [\n        ann.Variable('tau_m * dv/dt = El - v + g_exc * (Ee - vr)', init = -60.0),\n        ann.Variable('tau_e * dg_exc/dt = - g_exc'),\n    ],\n    spike = \"v &gt; vt\",\n    reset = \"v = vr\",\n)\n\nAn input population of Poisson neurons, and a single post-synaptic neuron.\n\n# Network\nnet = ann.Network()\n\n# Input population\nInput = net.create(ann.PoissonPopulation(geometry=N, rates=F, name = 'Input'))\n\n# Output neuron\nOutput = net.create(geometry=1, neuron=IF, name = 'Output')\n\n# Projection learned using STDP\nproj = net.connect( \n    pre = Input, \n    post = Output, \n    target = 'exc',\n    synapse = ann.STDP(tau_plus=20.0, tau_minus=20.0, A_plus=0.01, A_minus=0.0105, w_max=0.01)\n)\nproj.all_to_all(weights=ann.Uniform(0.0, gmax))\n\n\n# Compile the network\nnet.compile()\n\nCompiling network 1...  OK \n\n\n\n# Start recording\nMi = net.monitor(Input, 'spike') \nMo = net.monitor(Output, 'spike')\n\n# Start the simulation\nnet.simulate(duration, measure_time=True)\n\n# Retrieve the recordings\ninput_spikes = Mi.get('spike')\noutput_spikes = Mo.get('spike')\n\nSimulating 100.0 seconds of the network 1 took 0.8422238826751709 seconds. \n\n\n\n# Compute the mean firing rates during the simulation\nprint('Mean firing rate in the input population: ' + str(Mi.mean_fr(input_spikes)) )\nprint('Mean firing rate of the output neuron: ' + str(Mo.mean_fr(output_spikes)) )\n\n# Compute the instantaneous firing rate of the output neuron\noutput_rate = Mo.smoothed_rate(output_spikes, 100.0)\n\n# Receptive field after simulation\nweights = proj.w[0]\n\nMean firing rate in the input population: 14.991800000000001\nMean firing rate of the output neuron: 26.110000000000003\n\n\n\nplt.figure(figsize=(12, 12))\nplt.subplot(3,1,1)\nplt.title('Firing rate')\nplt.plot(output_rate[0, :])\nplt.subplot(3,1,2)\nplt.title('Weights')\nplt.plot(weights, '.')\nplt.subplot(3,1,3)\nplt.title('Weights histogram')\nplt.hist(weights, bins=20)\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "STDP II"
    ]
  },
  {
    "objectID": "manual/Reporting.html",
    "href": "manual/Reporting.html",
    "title": "Reporting",
    "section": "",
    "text": "ANNarchy includes an utility allowing to automatically generate a report based on the current structure of the network:\nann.report(filename=\"model_description.tex\")\nann.report(filename=\"model_description.qmd\")\nIf the filename ends with .tex, the LaTeX report will be generated based on the specifications provided in:\n\nNordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8):e1000456.\n\nIf the filename ends with .qmd, the report will be generated in Markdown, so it can later be exported to pdf or html using pandoc.\nreport() accepts several arguments:\n\nfilename: name of the file where the report will be written (default: \"./report.tex\")\nstandalone: tells if the generated TeX file should be directly compilable or only includable. Ignored in Markdown.\ngather_subprojections: if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False).\ntitle: title of the document (Markdown only)\nauthor: author of the document (Markdown only)\ndate: date of the document (Markdown only)\nnet_id: id of the network to be used for reporting (default: 0, everything that was declared)\n\n\n\nreport() produces a .tex file (by default report.tex in the current directory, but this can be changed by passing the filename argument) which can be directly compiled with pdflatex or integrated into a larger file:\npdflatex model_description.tex\nThis report consists of different tables describing several aspects of the model:\n\nSummary: A summary of the network, with a list of populations, neuron and synapse models, topologies, etc. This section may have to be adapted, as for example, ANNarchy does not make a distinction between synapse and plasticity models.\nPopulations: A list of populations, with their respective neural models and geometries.\nProjections: A list of projections, with the pre- and post-synaptic populations, the target, the synapse model if any, and a description of the connection pattern.\nNeuron models: For each neuron model, a description of its dynamics with equations parsed using SymPy and translated to the LaTeX mathematical language.\nSynapse models: For each synapse model, a description of its dynamics if any.\nParameters: The initial value (before the call to compile()) of the parameters of each population and projection (if any).\nInput: Inputs set to the network (has to be filled manually).\nMeasurements: Measurements done in the network (has to be filled manually).\n\n\n\n\nThe generated Mardown file is globally similar to the LaTeX one, with additional information that make it more useful for debugging (locality of attributes, type...). The Markown file is readable by design, but it can be translated to many markup languages (html, epub, latex, pdf...) using pandoc.\nTo obtain a pdf from the Markdown file (supposing you have a LaTeX distribution available), just type:\npandoc model_description.qmd -sN -V geometry:margin=1in -o model_description.pdf\nThe -V argument tells LaTex to use the full page instead of the default booklet format.\nTo obtain a html file, use:\npandoc model_description.qmd -sSN --mathjax -o model_description.html\nYou can omit the -S option if you only want to include the code into a webpage, otherwise it is a standalone file. --mathjax is needed to display mathematical equations using the javascript library MathJax.\nBy default, the html file has no styling, and tables can be very ugly. With a simple css file like this one, the html page looks nicer (feel free to edit):\npandoc model_description.qmd -sSN --mathjax --css=simple.css -o model_description.html\nIf you upload your model to a github-like service (bitbucket, gitlab, gogs...), it could be a good idea to generate the README.qmd directly with report(). Do not forget to set a title+author+date then.\n\n\n\nThe report is generated based entirely on the Python script. For it to make sense, the user has to provide the necessary information while defining the network:\n\nPopulations must be assigned a unique name. If no name is given, generic names such as pop0 or pop1 will be used. If two populations have the same name, the connectivity will be unreadable:\npop1 = net.create(geometry=(100, 100), neuron=ann.Izhikevich, name=\"Excitatory\")\npop2 = net.create(geometry=(20, 20), neuron=ann.Izhikevich, name=\"Inhibitory\")\nUser-defined neuron and synapse models should be assigned a name and description. The name should be relatively short and generic (e.g. “Izhikevich”, “BCM learning rule”), while the description should be more specific. They can contain LaTeX code, but remember to double the \\ which is the escape symbol in Python strings:\nLIF = ann.Neuron(\n    parameters = dict(\n        tau = 10.0\n    ),\n    equations = [\n        \"tau * dv/dt + v = g_exc\",\n    ],\n    spike = \"v &gt; 30.0\",\n    reset = \"v = 0.0\"\n    name = \"LIF\",\n    description = \"Leaky Integrate-and-Fire spiking neuron with time constant $\\\\tau$.\" \n)\n\nOja = ann.Synapse(\n    parameters = dict(\n        eta = 10.0,\n        tau = 10.0,\n    ),\n    equations = [\n        ann.Variable('tau * dalpha/dt + alpha = pos(post.r - 1.0)', locality='semiglobal'),\n        ann.Variable('eta * dw/dt = pre.r * post.r - alpha * post.r^2 * w', min=0.0),\n    ], \n    name=\"Oja learning rule\",\n    description= \"\"\"Oja learning rule ensuring regularization of the synaptic weights.\"\"\"\n) \nChoose simple parameter and variable names for the description of equations. If a parameter/variable name uses only one character, it will be treated as a mathematical variable in the equations (ex: v becomes v), otherwise the plain text representation will be used (ugly). If the name corresponds to a greek letter (alpha, tau, etc.), it will be represented by the corresponding greek letter (\\alpha, \\tau). If the name is composed of two terms separated by an underscore (tau_exc), a subscript will be used (\\tau_\\text{exc}). If more than one underscore is used, the text representation is used instead (LaTeX does not allow multiple subscripts).\n\n\n\n\nLet’s take the homeostatic STDP ramp example provided in examples/homeostatic_stdp/Ramp.py and add names/descriptions to the objects:\nimport numpy as np\nimport ANNarchy as ann\n\n# Izhikevich RS neuron\nRSNeuron = ann.Neuron(\n    parameters = dict(\n        a = 0.02,\n        b = 0.2,\n        c = -65.,\n        d = 8.,\n        tau_ampa = 5.,\n        tau_nmda = 150.,\n        vrev = 0.0,\n    ),\n    equations = [\n        # Inputs\n        'I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)',\n\n        # Midpoint scheme      \n        ann.Variable('dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I', init=-65., method=\"midpoint\"),\n        ann.Variable('du/dt = a * (b*v - u)', init=-13., method=\"midpoint\"),\n\n        # Conductances\n        ann.Variable('tau_ampa * dg_ampa/dt = -g_ampa', method=\"exponential\"),\n        ann.Variable('tau_nmda * dg_nmda/dt = -g_nmda', method=\"exponential\"),\n     ] , \n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\", \n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\",\n    name = \"Regular-spiking Izhikevich\",\n    description = \"Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\"\n)\n\n# Network\nnet = ann.Network()\n\n# Input population\ninp = net.create(ann.PoissonPopulation(100, rates=np.linspace(0.2, 20., 100), name=\"Poisson input\"))\n\n# RS neuron without homeostatic mechanism\npop1 = net.create(1, RSNeuron, name=\"RS neuron without homeostasis\")\npop1.compute_firing_rate(5000.)\n\n# RS neuron with homeostatic mechanism\npop2 = net.create(1, RSNeuron, name=\"RS neuron with homeostasis\")\npop2.compute_firing_rate(5000.)\n\n# Nearest Neighbour STDP\nnearest_neighbour_stdp = ann.Synapse(\n    parameters = dict(\n        tau_plus = 20.,\n        tau_minus = 60.,\n        A_plus = 0.0002,\n        A_minus = 0.000066,\n        w_max = 0.03,\n    ),\n    equations = [\n        # Traces\n        ann.Variable('tau_plus  * dltp/dt = -ltp', method='exponential'),\n        ann.Variable('tau_minus * dltd/dt = -ltd', method='exponential'),\n\n        # Nearest-neighbour\n        ann.Variable('w += if t_post &gt;= t_pre: ltp else: - ltd', min=0.0, max=w_max),\n    ],\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\",\n    name = \"Nearest-neighbour STDP\",\n    description = \"Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\"\n)\n\n# STDP with homeostatic regulation\nhomeo_stdp = ann.Synapse(\n    parameters = dict(\n        # STDP\n        tau_plus = 20.,\n        tau_minus = 60.,\n        A_plus = 0.0002,\n        A_minus = 0.000066,\n        w_min = 0.0,\n        w_max = 0.03,\n\n        # Homeostatic regulation\n        alpha = 0.1,\n        beta = 1.0,\n        gamma = 50.,\n        Rtarget = 35.,\n        T = 5000.,\n    ),\n    equations = [\n        # Traces\n        ann.Variable('tau_plus  * dltp/dt = -ltp', method='exponential'),\n        ann.Variable('tau_minus * dltd/dt = -ltd', method='exponential'),\n\n        # Homeostatic values\n        ann.Variable('R = post.r', locality='semiglobal'),\n        ann.Variable('K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma))', locality='semiglobal'),\n        \n        # Nearest-neighbour\n        ann.Variable('stdp = if t_post &gt;= t_pre: ltp else: - ltd'),\n        ann.Variable('w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K', min=w_min, max=w_max),\n    ],\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\" ,\n    name = \"Nearest-neighbour STDP with homeostasis\",\n    description = \"Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \"\n)\n\n# Projection without homeostatic mechanism\nproj1 = net.connect(inp, pop1, ['ampa', 'nmda'], synapse=nearest_neighbour_stdp)\nproj1.all_to_all(Uniform(0.01, 0.03))\n\n# Projection with homeostatic mechanism\nproj2 = net.connect(inp, pop2, ['ampa', 'nmda'], synapse=homeo_stdp)\nproj2.all_to_all(weights=Uniform(0.01, 0.03))\n\n\n# Record\nm1 = net.monitor(pop1, 'r')\nm2 = net.monitor(pop2, 'r')\n\nann.report(\n    network=net,\n    filename='ramp.qmd', \n    title=\"Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\", \n    author=\"Carlson, Richert, Dutt and Krichmar\",\n    date=\"Neural Networks (IJCNN) 2013\"\n)\nThis generates the following Markdown file:\n---\ntitle: Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\nauthor: Carlson, Richert, Dutt and Krichmar\ndate: Neural Networks (IJCNN) 2013\n---\n\n# Structure of the network\n\n* ANNarchy 5.0.0 using the default backend.\n* Numerical step size: 1.0 ms.\n\n## Populations\n\n| **Population**                | **Size** | **Neuron type**            | \n| ----------------------------- | -------- | -------------------------- | \n| Poisson input                 | 100      | Poisson                    | \n| RS neuron without homeostasis | 1        | Regular-spiking Izhikevich | \n| RS neuron with homeostasis    | 1        | Regular-spiking Izhikevich | \n\n\n## Projections\n\n| **Source**    | **Destination**               | **Target**  | **Synapse type**                        | **Pattern**                                               | \n| ------------- | ----------------------------- | ----------- | --------------------------------------- | --------------------------------------------------------- | \n| Poisson input | RS neuron without homeostasis | ampa / nmda | Nearest-neighbour STDP                  | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | \n| Poisson input | RS neuron with homeostasis    | ampa / nmda | Nearest-neighbour STDP with homeostasis | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | \n\n\n## Monitors\n\n| **Object**                    | **Variables** | **Period** | \n| ----------------------------- | ------------- | ---------- | \n| RS neuron without homeostasis | r             | 1.0        | \n| RS neuron with homeostasis    | r             | 1.0        | \n\n\n# Neuron models\n\n## Regular-spiking Izhikevich\n\nRegular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\n\n**Parameters:**\n\n| **Name**             | **Default value** | **Locality**   | **Type** | \n| -------------------- | ----------------- | -------------- | -------- | \n| $a$                  | 0.02              | per population | double   | \n| $b$                  | 0.2               | per population | double   | \n| $c$                  | -65.0             | per population | double   | \n| $d$                  | 8.0               | per population | double   | \n| $\\tau_{\\text{ampa}}$ | 5.0               | per population | double   | \n| $\\tau_{\\text{nmda}}$ | 150.0             | per population | double   | \n| ${\\text{vrev}}$      | 0.0               | per population | double   | \n\n**Equations:**\n\n* Variable $I$ : per neuron, initial value: 0.0\n\n$$\n{I}(t) = {g_{\\text{ampa}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) + {g_{\\text{nmda}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) \\cdot \\operatorname{nmda}{\\left ({v}(t),-80.0,60.0 \\right )}\n$$\n\n* Variable $v$ : per neuron, initial value: -65.0, midpoint numerical method\n\n$$\n\\frac{d{v}(t)}{dt} = {I}(t) - {u}(t) + {v}(t) \\cdot \\left(0.04 \\cdot {v}(t) + 5.0\\right) + 140.0\n$$\n\n* Variable $u$ : per neuron, initial value: -13.0, midpoint numerical method\n\n$$\n\\frac{d{u}(t)}{dt} = a \\cdot \\left(b \\cdot {v}(t) - {u}(t)\\right)\n$$\n\n* Variable $g_{\\text{ampa}}$ : per neuron, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{g_{\\text{ampa}}}(t)}{dt} \\cdot \\tau_{\\text{ampa}} = - {g_{\\text{ampa}}}(t)\n$$\n\n* Variable $g_{\\text{nmda}}$ : per neuron, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{g_{\\text{nmda}}}(t)}{dt} \\cdot \\tau_{\\text{nmda}} = - {g_{\\text{nmda}}}(t)\n$$\n\n**Spike emission:**\n\nif ${v}(t) \\geq 30.0$ :\n\n* Emit a spike a time $t$.\n* ${v}(t) = c$\n* ${u}(t) \\mathrel{+}= d$\n\n\n**Functions**\n\n$${\\text{nmda}}(v, t, s) = \\frac{\\left(- t + v\\right)^{2}}{s^{2} \\cdot \\left(1.0 + \\frac{1}{s^{2}} \\cdot \\left(- t + v\\right)^{2}\\right)}$$\n\n\n## Poisson\n\nSpiking neuron with spikes emitted according to a Poisson distribution.\n\n**Parameters:**\n\n| **Name**         | **Default value** | **Locality** | **Type** | \n| ---------------- | ----------------- | ------------ | -------- | \n| ${\\text{rates}}$ | 10.0              | per neuron   | double   | \n\n**Equations:**\n\n* Variable $p$ : per neuron, initial value: 0.0\n\n$$\n{p}(t) = \\frac{1000.0}{\\Delta t} \\cdot \\mathcal{U}{\\left (0.0,1.0 \\right )}\n$$\n\n**Spike emission:**\n\nif ${p}(t) &lt; {\\text{rates}}$ :\n\n* Emit a spike a time $t$.\n\n# Synapse models\n\n## Nearest-neighbour STDP\n\nNearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\n\n**Parameters:**\n\n| **Name**              | **Default value** | **Locality**   | **Type** | \n| --------------------- | ----------------- | -------------- | -------- | \n| $\\tau_{\\text{plus}}$  | 20.0              | per projection | double   | \n| $\\tau_{\\text{minus}}$ | 60.0              | per projection | double   | \n| $A_{\\text{plus}}$     | 0.0002            | per projection | double   | \n| $A_{\\text{minus}}$    | 6.6e-05           | per projection | double   | \n| $w_{\\text{max}}$      | 0.03              | per projection | double   | \n\n**Equations:**\n\n* Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n$$\n\n* Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n$$\n\n* Variable $w$ : per synapse, initial value: 0.0, minimum: 0.0, maximum: w_max\n\n$$\n{w}(t) \\mathrel{+}= \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n$$\n\n**Pre-synaptic event at $t_\\text{pre} + d$:**\n$$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$\n$${{\\text{ltp}}}(t) = A_{\\text{plus}}$$\n\n**Post-synaptic event at $t_\\text{post}$:**\n$${{\\text{ltd}}}(t) = A_{\\text{minus}}$$\n\n## Nearest-neighbour STDP with homeostasis\n\nNearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \n\n**Parameters:**\n\n| **Name**              | **Default value** | **Locality**   | **Type** | \n| --------------------- | ----------------- | -------------- | -------- | \n| $\\tau_{\\text{plus}}$  | 20.0              | per projection | double   | \n| $\\tau_{\\text{minus}}$ | 60.0              | per projection | double   | \n| $A_{\\text{plus}}$     | 0.0002            | per projection | double   | \n| $A_{\\text{minus}}$    | 6.6e-05           | per projection | double   | \n| $w_{\\text{min}}$      | 0.0               | per projection | double   | \n| $w_{\\text{max}}$      | 0.03              | per projection | double   | \n| $\\alpha$              | 0.1               | per projection | double   | \n| $\\beta$               | 1.0               | per projection | double   | \n| $\\gamma$              | 50.0              | per projection | double   | \n| ${\\text{Rtarget}}$    | 35.0              | per projection | double   | \n| $T$                   | 5000.0            | per projection | double   | \n\n**Equations:**\n\n* Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n$$\n\n* Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n$$\n\n* Variable $R$ : per post-synaptic neuron, initial value: 0.0\n\n$$\n{R}(t) = {r}^{\\text{post}}(t)\n$$\n\n* Variable $K$ : per post-synaptic neuron, initial value: 0.0\n\n$$\n{K}(t) = \\frac{{R}(t)}{T \\cdot \\left(\\gamma \\cdot \\left|{f}\\right|{\\left (- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1.0 \\right )} + 1.0\\right)}\n$$\n\n* Variable ${\\text{stdp}}$ : per synapse, initial value: 0.0\n\n$$\n{{\\text{stdp}}}(t) = \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n$$\n\n* Variable $w$ : per synapse, initial value: 0.0, minimum: w_min, maximum: w_max\n\n$$\n{w}(t) \\mathrel{+}= {K}(t) \\cdot \\left(\\alpha \\cdot {w}(t) \\cdot \\left(- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1\\right) + \\beta \\cdot {{\\text{stdp}}}(t)\\right)\n$$\n\n**Pre-synaptic event at $t_\\text{pre} + d$:**\n$$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$\n$${{\\text{ltp}}}(t) = A_{\\text{plus}}$$\n\n**Post-synaptic event at $t_\\text{post}$:**\n$${{\\text{ltd}}}(t) = A_{\\text{minus}}$$\n\n# Parameters\n\n## Population parameters\n\n| **Population**                | **Neuron type**            | **Name**             | **Value**     | \n| ----------------------------- | -------------------------- | -------------------- | ------------- | \n| Poisson input                 | Poisson                    | ${\\text{rates}}$     | $[0.2, 20.0]$ | \n| RS neuron without homeostasis | Regular-spiking Izhikevich | $a$                  | 0.02          | \n|                               |                            | $b$                  | 0.2           | \n|                               |                            | $c$                  | -65.0         | \n|                               |                            | $d$                  | 8.0           | \n|                               |                            | $\\tau_{\\text{ampa}}$ | 5.0           | \n|                               |                            | $\\tau_{\\text{nmda}}$ | 150.0         | \n|                               |                            | ${\\text{vrev}}$      | 0.0           | \n| RS neuron with homeostasis    | Regular-spiking Izhikevich | $a$                  | 0.02          | \n|                               |                            | $b$                  | 0.2           | \n|                               |                            | $c$                  | -65.0         | \n|                               |                            | $d$                  | 8.0           | \n|                               |                            | $\\tau_{\\text{ampa}}$ | 5.0           | \n|                               |                            | $\\tau_{\\text{nmda}}$ | 150.0         | \n|                               |                            | ${\\text{vrev}}$      | 0.0           | \n\n\n\n## Projection parameters\n\n| **Projection**                                                                     | **Synapse type**                        | **Name**              | **Value** | \n| ---------------------------------------------------------------------------------- | --------------------------------------- | --------------------- | --------- | \n| Poisson input  $\\rightarrow$ RS neuron without homeostasis with target ampa / nmda | Nearest-neighbour STDP                  | $\\tau_{\\text{plus}}$  | 20.0      | \n|                                                                                    |                                         | $\\tau_{\\text{minus}}$ | 60.0      | \n|                                                                                    |                                         | $A_{\\text{plus}}$     | 0.0002    | \n|                                                                                    |                                         | $A_{\\text{minus}}$    | 6.6e-05   | \n|                                                                                    |                                         | $w_{\\text{max}}$      | 0.03      | \n| Poisson input  $\\rightarrow$ RS neuron with homeostasis with target ampa / nmda    | Nearest-neighbour STDP with homeostasis | $\\tau_{\\text{plus}}$  | 20.0      | \n|                                                                                    |                                         | $\\tau_{\\text{minus}}$ | 60.0      | \n|                                                                                    |                                         | $A_{\\text{plus}}$     | 0.0002    | \n|                                                                                    |                                         | $A_{\\text{minus}}$    | 6.6e-05   | \n|                                                                                    |                                         | $w_{\\text{min}}$      | 0.0       | \n|                                                                                    |                                         | $w_{\\text{max}}$      | 0.03      | \n|                                                                                    |                                         | $\\alpha$              | 0.1       | \n|                                                                                    |                                         | $\\beta$               | 1.0       | \n|                                                                                    |                                         | $\\gamma$              | 50.0      | \n|                                                                                    |                                         | ${\\text{Rtarget}}$    | 35.0      | \n|                                                                                    |                                         | $T$                   | 5000.0    | \n\n\n\n\n\n\nANNarchy 5.0.0 using the default backend.\nNumerical step size: 1.0 ms.\n\n\n\n\n\n\n\n\n\n\n\nPopulation\nSize\nNeuron type\n\n\n\n\nPoisson input\n100\nPoisson\n\n\nRS neuron without homeostasis\n1\nRegular-spiking Izhikevich\n\n\nRS neuron with homeostasis\n1\nRegular-spiking Izhikevich\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nDestination\nTarget\nSynapse type\nPattern\n\n\n\n\nPoisson input\nRS neuron without homeostasis\nampa / nmda\nNearest-neighbour STDP\nAll-to-All, weights \\mathcal{U}(0.01, 0.03), delays 0.0\n\n\nPoisson input\nRS neuron with homeostasis\nampa / nmda\nNearest-neighbour STDP with homeostasis\nAll-to-All, weights \\mathcal{U}(0.01, 0.03), delays 0.0\n\n\n\n\n\n\n\n\nObject\nVariables\nPeriod\n\n\n\n\nRS neuron without homeostasis\nr\n1.0\n\n\nRS neuron with homeostasis\nr\n1.0\n\n\n\n\n\n\n\n\n\nRegular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\nParameters:\n\n\n\nName\nDefault value\nLocality\nType\n\n\n\n\na\n0.02\nper population\ndouble\n\n\nb\n0.2\nper population\ndouble\n\n\nc\n-65.0\nper population\ndouble\n\n\nd\n8.0\nper population\ndouble\n\n\n\\tau_{\\text{ampa}}\n5.0\nper population\ndouble\n\n\n\\tau_{\\text{nmda}}\n150.0\nper population\ndouble\n\n\n{\\text{vrev}}\n0.0\nper population\ndouble\n\n\n\nEquations:\n\nVariable I : per neuron, initial value: 0.0\n\n\n{I}(t) = {g_{\\text{ampa}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) + {g_{\\text{nmda}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) \\cdot \\operatorname{nmda}{\\left ({v}(t),-80.0,60.0 \\right )}\n\n\nVariable v : per neuron, initial value: -65.0, midpoint numerical method\n\n\n\\frac{d{v}(t)}{dt} = {I}(t) - {u}(t) + {v}(t) \\cdot \\left(0.04 \\cdot {v}(t) + 5.0\\right) + 140.0\n\n\nVariable u : per neuron, initial value: -13.0, midpoint numerical method\n\n\n\\frac{d{u}(t)}{dt} = a \\cdot \\left(b \\cdot {v}(t) - {u}(t)\\right)\n\n\nVariable g_{\\text{ampa}} : per neuron, initial value: 0.0, exponential numerical method\n\n\n\\frac{d{g_{\\text{ampa}}}(t)}{dt} \\cdot \\tau_{\\text{ampa}} = - {g_{\\text{ampa}}}(t)\n\n\nVariable g_{\\text{nmda}} : per neuron, initial value: 0.0, exponential numerical method\n\n\n\\frac{d{g_{\\text{nmda}}}(t)}{dt} \\cdot \\tau_{\\text{nmda}} = - {g_{\\text{nmda}}}(t)\n\nSpike emission:\nif {v}(t) \\geq 30.0 :\n\nEmit a spike a time t.\n{v}(t) = c\n{u}(t) \\mathrel{+}= d\n\nFunctions\n{\\text{nmda}}(v, t, s) = \\frac{\\left(- t + v\\right)^{2}}{s^{2} \\cdot \\left(1.0 + \\frac{1}{s^{2}} \\cdot \\left(- t + v\\right)^{2}\\right)}\n\n\n\nSpiking neuron with spikes emitted according to a Poisson distribution.\nParameters:\n\n\n\nName\nDefault value\nLocality\nType\n\n\n\n\n{\\text{rates}}\n10.0\nper neuron\ndouble\n\n\n\nEquations:\n\nVariable p : per neuron, initial value: 0.0\n\n\n{p}(t) = \\frac{1000.0}{\\Delta t} \\cdot \\mathcal{U}{\\left (0.0,1.0 \\right )}\n\nSpike emission:\nif {p}(t) &lt; {\\text{rates}} :\n\nEmit a spike a time t.\n\n\n\n\n\n\n\nNearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\nParameters:\n\n\n\n\n\n\n\n\n\nName\nDefault value\nLocality\nType\n\n\n\n\n\\tau_{\\text{plus}}\n20.0\nper projection\ndouble\n\n\n\\tau_{\\text{minus}}\n60.0\nper projection\ndouble\n\n\nA_{\\text{plus}}\n0.0002\nper projection\ndouble\n\n\nA_{\\text{minus}}\n6.6e-05\nper projection\ndouble\n\n\nw_{\\text{max}}\n0.03\nper projection\ndouble\n\n\n\nEquations:\n\nVariable {\\text{ltp}} : per synapse, initial value: 0.0, exponential numerical method\n\n\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n\n\nVariable {\\text{ltd}} : per synapse, initial value: 0.0, exponential numerical method\n\n\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n\n\nVariable w : per synapse, initial value: 0.0, minimum: 0.0, maximum: w_max\n\n\n{w}(t) \\mathrel{+}= \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n\nPre-synaptic event at t_\\text{pre} + d: g_{\\text{target}(t)} \\mathrel{+}= {w}(t) {{\\text{ltp}}}(t) = A_{\\text{plus}}\nPost-synaptic event at t_\\text{post}: {{\\text{ltd}}}(t) = A_{\\text{minus}}\n\n\n\nNearest-neighbour STDP synaptic plasticity with an additional homeostatic term.\nParameters:\n\n\n\n\n\n\n\n\n\nName\nDefault value\nLocality\nType\n\n\n\n\n\\tau_{\\text{plus}}\n20.0\nper projection\ndouble\n\n\n\\tau_{\\text{minus}}\n60.0\nper projection\ndouble\n\n\nA_{\\text{plus}}\n0.0002\nper projection\ndouble\n\n\nA_{\\text{minus}}\n6.6e-05\nper projection\ndouble\n\n\nw_{\\text{min}}\n0.0\nper projection\ndouble\n\n\nw_{\\text{max}}\n0.03\nper projection\ndouble\n\n\n\\alpha\n0.1\nper projection\ndouble\n\n\n\\beta\n1.0\nper projection\ndouble\n\n\n\\gamma\n50.0\nper projection\ndouble\n\n\n{\\text{Rtarget}}\n35.0\nper projection\ndouble\n\n\nT\n5000.0\nper projection\ndouble\n\n\n\nEquations:\n\nVariable {\\text{ltp}} : per synapse, initial value: 0.0, exponential numerical method\n\n\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n\n\nVariable {\\text{ltd}} : per synapse, initial value: 0.0, exponential numerical method\n\n\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n\n\nVariable R : per post-synaptic neuron, initial value: 0.0\n\n\n{R}(t) = {r}^{\\text{post}}(t)\n\n\nVariable K : per post-synaptic neuron, initial value: 0.0\n\n\n{K}(t) = \\frac{{R}(t)}{T \\cdot \\left(\\gamma \\cdot \\left|{f}\\right|{\\left (- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1.0 \\right )} + 1.0\\right)}\n\n\nVariable {\\text{stdp}} : per synapse, initial value: 0.0\n\n\n{{\\text{stdp}}}(t) = \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n\n\nVariable w : per synapse, initial value: 0.0, minimum: w_min, maximum: w_max\n\n\n{w}(t) \\mathrel{+}= {K}(t) \\cdot \\left(\\alpha \\cdot {w}(t) \\cdot \\left(- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1\\right) + \\beta \\cdot {{\\text{stdp}}}(t)\\right)\n\nPre-synaptic event at t_\\text{pre} + d: g_{\\text{target}(t)} \\mathrel{+}= {w}(t) {{\\text{ltp}}}(t) = A_{\\text{plus}}\nPost-synaptic event at t_\\text{post}: {{\\text{ltd}}}(t) = A_{\\text{minus}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation\nNeuron type\nName\nValue\n\n\n\n\nPoisson input\nPoisson\n{\\text{rates}}\n[0.2, 20.0]\n\n\nRS neuron without homeostasis\nRegular-spiking Izhikevich\na\n0.02\n\n\n\n\nb\n0.2\n\n\n\n\nc\n-65.0\n\n\n\n\nd\n8.0\n\n\n\n\n\\tau_{\\text{ampa}}\n5.0\n\n\n\n\n\\tau_{\\text{nmda}}\n150.0\n\n\n\n\n{\\text{vrev}}\n0.0\n\n\nRS neuron with homeostasis\nRegular-spiking Izhikevich\na\n0.02\n\n\n\n\nb\n0.2\n\n\n\n\nc\n-65.0\n\n\n\n\nd\n8.0\n\n\n\n\n\\tau_{\\text{ampa}}\n5.0\n\n\n\n\n\\tau_{\\text{nmda}}\n150.0\n\n\n\n\n{\\text{vrev}}\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjection\nSynapse type\nName\nValue\n\n\n\n\nPoisson input \\rightarrow RS neuron without homeostasis with target ampa / nmda\nNearest-neighbour STDP\n\\tau_{\\text{plus}}\n20.0\n\n\n\n\n\\tau_{\\text{minus}}\n60.0\n\n\n\n\nA_{\\text{plus}}\n0.0002\n\n\n\n\nA_{\\text{minus}}\n6.6e-05\n\n\n\n\nw_{\\text{max}}\n0.03\n\n\nPoisson input \\rightarrow RS neuron with homeostasis with target ampa / nmda\nNearest-neighbour STDP with homeostasis\n\\tau_{\\text{plus}}\n20.0\n\n\n\n\n\\tau_{\\text{minus}}\n60.0\n\n\n\n\nA_{\\text{plus}}\n0.0002\n\n\n\n\nA_{\\text{minus}}\n6.6e-05\n\n\n\n\nw_{\\text{min}}\n0.0\n\n\n\n\nw_{\\text{max}}\n0.03\n\n\n\n\n\\alpha\n0.1\n\n\n\n\n\\beta\n1.0\n\n\n\n\n\\gamma\n50.0\n\n\n\n\n{\\text{Rtarget}}\n35.0\n\n\n\n\nT\n5000.0",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Reporting"
    ]
  },
  {
    "objectID": "manual/Reporting.html#content-of-the-tex-file",
    "href": "manual/Reporting.html#content-of-the-tex-file",
    "title": "Reporting",
    "section": "",
    "text": "report() produces a .tex file (by default report.tex in the current directory, but this can be changed by passing the filename argument) which can be directly compiled with pdflatex or integrated into a larger file:\npdflatex model_description.tex\nThis report consists of different tables describing several aspects of the model:\n\nSummary: A summary of the network, with a list of populations, neuron and synapse models, topologies, etc. This section may have to be adapted, as for example, ANNarchy does not make a distinction between synapse and plasticity models.\nPopulations: A list of populations, with their respective neural models and geometries.\nProjections: A list of projections, with the pre- and post-synaptic populations, the target, the synapse model if any, and a description of the connection pattern.\nNeuron models: For each neuron model, a description of its dynamics with equations parsed using SymPy and translated to the LaTeX mathematical language.\nSynapse models: For each synapse model, a description of its dynamics if any.\nParameters: The initial value (before the call to compile()) of the parameters of each population and projection (if any).\nInput: Inputs set to the network (has to be filled manually).\nMeasurements: Measurements done in the network (has to be filled manually).",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Reporting"
    ]
  },
  {
    "objectID": "manual/Reporting.html#content-of-the-markdown-file",
    "href": "manual/Reporting.html#content-of-the-markdown-file",
    "title": "Reporting",
    "section": "",
    "text": "The generated Mardown file is globally similar to the LaTeX one, with additional information that make it more useful for debugging (locality of attributes, type...). The Markown file is readable by design, but it can be translated to many markup languages (html, epub, latex, pdf...) using pandoc.\nTo obtain a pdf from the Markdown file (supposing you have a LaTeX distribution available), just type:\npandoc model_description.qmd -sN -V geometry:margin=1in -o model_description.pdf\nThe -V argument tells LaTex to use the full page instead of the default booklet format.\nTo obtain a html file, use:\npandoc model_description.qmd -sSN --mathjax -o model_description.html\nYou can omit the -S option if you only want to include the code into a webpage, otherwise it is a standalone file. --mathjax is needed to display mathematical equations using the javascript library MathJax.\nBy default, the html file has no styling, and tables can be very ugly. With a simple css file like this one, the html page looks nicer (feel free to edit):\npandoc model_description.qmd -sSN --mathjax --css=simple.css -o model_description.html\nIf you upload your model to a github-like service (bitbucket, gitlab, gogs...), it could be a good idea to generate the README.qmd directly with report(). Do not forget to set a title+author+date then.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Reporting"
    ]
  },
  {
    "objectID": "manual/Reporting.html#documenting-the-network",
    "href": "manual/Reporting.html#documenting-the-network",
    "title": "Reporting",
    "section": "",
    "text": "The report is generated based entirely on the Python script. For it to make sense, the user has to provide the necessary information while defining the network:\n\nPopulations must be assigned a unique name. If no name is given, generic names such as pop0 or pop1 will be used. If two populations have the same name, the connectivity will be unreadable:\npop1 = net.create(geometry=(100, 100), neuron=ann.Izhikevich, name=\"Excitatory\")\npop2 = net.create(geometry=(20, 20), neuron=ann.Izhikevich, name=\"Inhibitory\")\nUser-defined neuron and synapse models should be assigned a name and description. The name should be relatively short and generic (e.g. “Izhikevich”, “BCM learning rule”), while the description should be more specific. They can contain LaTeX code, but remember to double the \\ which is the escape symbol in Python strings:\nLIF = ann.Neuron(\n    parameters = dict(\n        tau = 10.0\n    ),\n    equations = [\n        \"tau * dv/dt + v = g_exc\",\n    ],\n    spike = \"v &gt; 30.0\",\n    reset = \"v = 0.0\"\n    name = \"LIF\",\n    description = \"Leaky Integrate-and-Fire spiking neuron with time constant $\\\\tau$.\" \n)\n\nOja = ann.Synapse(\n    parameters = dict(\n        eta = 10.0,\n        tau = 10.0,\n    ),\n    equations = [\n        ann.Variable('tau * dalpha/dt + alpha = pos(post.r - 1.0)', locality='semiglobal'),\n        ann.Variable('eta * dw/dt = pre.r * post.r - alpha * post.r^2 * w', min=0.0),\n    ], \n    name=\"Oja learning rule\",\n    description= \"\"\"Oja learning rule ensuring regularization of the synaptic weights.\"\"\"\n) \nChoose simple parameter and variable names for the description of equations. If a parameter/variable name uses only one character, it will be treated as a mathematical variable in the equations (ex: v becomes v), otherwise the plain text representation will be used (ugly). If the name corresponds to a greek letter (alpha, tau, etc.), it will be represented by the corresponding greek letter (\\alpha, \\tau). If the name is composed of two terms separated by an underscore (tau_exc), a subscript will be used (\\tau_\\text{exc}). If more than one underscore is used, the text representation is used instead (LaTeX does not allow multiple subscripts).",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Reporting"
    ]
  },
  {
    "objectID": "manual/Reporting.html#example",
    "href": "manual/Reporting.html#example",
    "title": "Reporting",
    "section": "",
    "text": "Let’s take the homeostatic STDP ramp example provided in examples/homeostatic_stdp/Ramp.py and add names/descriptions to the objects:\nimport numpy as np\nimport ANNarchy as ann\n\n# Izhikevich RS neuron\nRSNeuron = ann.Neuron(\n    parameters = dict(\n        a = 0.02,\n        b = 0.2,\n        c = -65.,\n        d = 8.,\n        tau_ampa = 5.,\n        tau_nmda = 150.,\n        vrev = 0.0,\n    ),\n    equations = [\n        # Inputs\n        'I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)',\n\n        # Midpoint scheme      \n        ann.Variable('dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I', init=-65., method=\"midpoint\"),\n        ann.Variable('du/dt = a * (b*v - u)', init=-13., method=\"midpoint\"),\n\n        # Conductances\n        ann.Variable('tau_ampa * dg_ampa/dt = -g_ampa', method=\"exponential\"),\n        ann.Variable('tau_nmda * dg_nmda/dt = -g_nmda', method=\"exponential\"),\n     ] , \n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\", \n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\",\n    name = \"Regular-spiking Izhikevich\",\n    description = \"Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\"\n)\n\n# Network\nnet = ann.Network()\n\n# Input population\ninp = net.create(ann.PoissonPopulation(100, rates=np.linspace(0.2, 20., 100), name=\"Poisson input\"))\n\n# RS neuron without homeostatic mechanism\npop1 = net.create(1, RSNeuron, name=\"RS neuron without homeostasis\")\npop1.compute_firing_rate(5000.)\n\n# RS neuron with homeostatic mechanism\npop2 = net.create(1, RSNeuron, name=\"RS neuron with homeostasis\")\npop2.compute_firing_rate(5000.)\n\n# Nearest Neighbour STDP\nnearest_neighbour_stdp = ann.Synapse(\n    parameters = dict(\n        tau_plus = 20.,\n        tau_minus = 60.,\n        A_plus = 0.0002,\n        A_minus = 0.000066,\n        w_max = 0.03,\n    ),\n    equations = [\n        # Traces\n        ann.Variable('tau_plus  * dltp/dt = -ltp', method='exponential'),\n        ann.Variable('tau_minus * dltd/dt = -ltd', method='exponential'),\n\n        # Nearest-neighbour\n        ann.Variable('w += if t_post &gt;= t_pre: ltp else: - ltd', min=0.0, max=w_max),\n    ],\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\",\n    name = \"Nearest-neighbour STDP\",\n    description = \"Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\"\n)\n\n# STDP with homeostatic regulation\nhomeo_stdp = ann.Synapse(\n    parameters = dict(\n        # STDP\n        tau_plus = 20.,\n        tau_minus = 60.,\n        A_plus = 0.0002,\n        A_minus = 0.000066,\n        w_min = 0.0,\n        w_max = 0.03,\n\n        # Homeostatic regulation\n        alpha = 0.1,\n        beta = 1.0,\n        gamma = 50.,\n        Rtarget = 35.,\n        T = 5000.,\n    ),\n    equations = [\n        # Traces\n        ann.Variable('tau_plus  * dltp/dt = -ltp', method='exponential'),\n        ann.Variable('tau_minus * dltd/dt = -ltd', method='exponential'),\n\n        # Homeostatic values\n        ann.Variable('R = post.r', locality='semiglobal'),\n        ann.Variable('K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma))', locality='semiglobal'),\n        \n        # Nearest-neighbour\n        ann.Variable('stdp = if t_post &gt;= t_pre: ltp else: - ltd'),\n        ann.Variable('w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K', min=w_min, max=w_max),\n    ],\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\" ,\n    name = \"Nearest-neighbour STDP with homeostasis\",\n    description = \"Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \"\n)\n\n# Projection without homeostatic mechanism\nproj1 = net.connect(inp, pop1, ['ampa', 'nmda'], synapse=nearest_neighbour_stdp)\nproj1.all_to_all(Uniform(0.01, 0.03))\n\n# Projection with homeostatic mechanism\nproj2 = net.connect(inp, pop2, ['ampa', 'nmda'], synapse=homeo_stdp)\nproj2.all_to_all(weights=Uniform(0.01, 0.03))\n\n\n# Record\nm1 = net.monitor(pop1, 'r')\nm2 = net.monitor(pop2, 'r')\n\nann.report(\n    network=net,\n    filename='ramp.qmd', \n    title=\"Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\", \n    author=\"Carlson, Richert, Dutt and Krichmar\",\n    date=\"Neural Networks (IJCNN) 2013\"\n)\nThis generates the following Markdown file:\n---\ntitle: Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\nauthor: Carlson, Richert, Dutt and Krichmar\ndate: Neural Networks (IJCNN) 2013\n---\n\n# Structure of the network\n\n* ANNarchy 5.0.0 using the default backend.\n* Numerical step size: 1.0 ms.\n\n## Populations\n\n| **Population**                | **Size** | **Neuron type**            | \n| ----------------------------- | -------- | -------------------------- | \n| Poisson input                 | 100      | Poisson                    | \n| RS neuron without homeostasis | 1        | Regular-spiking Izhikevich | \n| RS neuron with homeostasis    | 1        | Regular-spiking Izhikevich | \n\n\n## Projections\n\n| **Source**    | **Destination**               | **Target**  | **Synapse type**                        | **Pattern**                                               | \n| ------------- | ----------------------------- | ----------- | --------------------------------------- | --------------------------------------------------------- | \n| Poisson input | RS neuron without homeostasis | ampa / nmda | Nearest-neighbour STDP                  | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | \n| Poisson input | RS neuron with homeostasis    | ampa / nmda | Nearest-neighbour STDP with homeostasis | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | \n\n\n## Monitors\n\n| **Object**                    | **Variables** | **Period** | \n| ----------------------------- | ------------- | ---------- | \n| RS neuron without homeostasis | r             | 1.0        | \n| RS neuron with homeostasis    | r             | 1.0        | \n\n\n# Neuron models\n\n## Regular-spiking Izhikevich\n\nRegular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\n\n**Parameters:**\n\n| **Name**             | **Default value** | **Locality**   | **Type** | \n| -------------------- | ----------------- | -------------- | -------- | \n| $a$                  | 0.02              | per population | double   | \n| $b$                  | 0.2               | per population | double   | \n| $c$                  | -65.0             | per population | double   | \n| $d$                  | 8.0               | per population | double   | \n| $\\tau_{\\text{ampa}}$ | 5.0               | per population | double   | \n| $\\tau_{\\text{nmda}}$ | 150.0             | per population | double   | \n| ${\\text{vrev}}$      | 0.0               | per population | double   | \n\n**Equations:**\n\n* Variable $I$ : per neuron, initial value: 0.0\n\n$$\n{I}(t) = {g_{\\text{ampa}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) + {g_{\\text{nmda}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) \\cdot \\operatorname{nmda}{\\left ({v}(t),-80.0,60.0 \\right )}\n$$\n\n* Variable $v$ : per neuron, initial value: -65.0, midpoint numerical method\n\n$$\n\\frac{d{v}(t)}{dt} = {I}(t) - {u}(t) + {v}(t) \\cdot \\left(0.04 \\cdot {v}(t) + 5.0\\right) + 140.0\n$$\n\n* Variable $u$ : per neuron, initial value: -13.0, midpoint numerical method\n\n$$\n\\frac{d{u}(t)}{dt} = a \\cdot \\left(b \\cdot {v}(t) - {u}(t)\\right)\n$$\n\n* Variable $g_{\\text{ampa}}$ : per neuron, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{g_{\\text{ampa}}}(t)}{dt} \\cdot \\tau_{\\text{ampa}} = - {g_{\\text{ampa}}}(t)\n$$\n\n* Variable $g_{\\text{nmda}}$ : per neuron, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{g_{\\text{nmda}}}(t)}{dt} \\cdot \\tau_{\\text{nmda}} = - {g_{\\text{nmda}}}(t)\n$$\n\n**Spike emission:**\n\nif ${v}(t) \\geq 30.0$ :\n\n* Emit a spike a time $t$.\n* ${v}(t) = c$\n* ${u}(t) \\mathrel{+}= d$\n\n\n**Functions**\n\n$${\\text{nmda}}(v, t, s) = \\frac{\\left(- t + v\\right)^{2}}{s^{2} \\cdot \\left(1.0 + \\frac{1}{s^{2}} \\cdot \\left(- t + v\\right)^{2}\\right)}$$\n\n\n## Poisson\n\nSpiking neuron with spikes emitted according to a Poisson distribution.\n\n**Parameters:**\n\n| **Name**         | **Default value** | **Locality** | **Type** | \n| ---------------- | ----------------- | ------------ | -------- | \n| ${\\text{rates}}$ | 10.0              | per neuron   | double   | \n\n**Equations:**\n\n* Variable $p$ : per neuron, initial value: 0.0\n\n$$\n{p}(t) = \\frac{1000.0}{\\Delta t} \\cdot \\mathcal{U}{\\left (0.0,1.0 \\right )}\n$$\n\n**Spike emission:**\n\nif ${p}(t) &lt; {\\text{rates}}$ :\n\n* Emit a spike a time $t$.\n\n# Synapse models\n\n## Nearest-neighbour STDP\n\nNearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\n\n**Parameters:**\n\n| **Name**              | **Default value** | **Locality**   | **Type** | \n| --------------------- | ----------------- | -------------- | -------- | \n| $\\tau_{\\text{plus}}$  | 20.0              | per projection | double   | \n| $\\tau_{\\text{minus}}$ | 60.0              | per projection | double   | \n| $A_{\\text{plus}}$     | 0.0002            | per projection | double   | \n| $A_{\\text{minus}}$    | 6.6e-05           | per projection | double   | \n| $w_{\\text{max}}$      | 0.03              | per projection | double   | \n\n**Equations:**\n\n* Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n$$\n\n* Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n$$\n\n* Variable $w$ : per synapse, initial value: 0.0, minimum: 0.0, maximum: w_max\n\n$$\n{w}(t) \\mathrel{+}= \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n$$\n\n**Pre-synaptic event at $t_\\text{pre} + d$:**\n$$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$\n$${{\\text{ltp}}}(t) = A_{\\text{plus}}$$\n\n**Post-synaptic event at $t_\\text{post}$:**\n$${{\\text{ltd}}}(t) = A_{\\text{minus}}$$\n\n## Nearest-neighbour STDP with homeostasis\n\nNearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \n\n**Parameters:**\n\n| **Name**              | **Default value** | **Locality**   | **Type** | \n| --------------------- | ----------------- | -------------- | -------- | \n| $\\tau_{\\text{plus}}$  | 20.0              | per projection | double   | \n| $\\tau_{\\text{minus}}$ | 60.0              | per projection | double   | \n| $A_{\\text{plus}}$     | 0.0002            | per projection | double   | \n| $A_{\\text{minus}}$    | 6.6e-05           | per projection | double   | \n| $w_{\\text{min}}$      | 0.0               | per projection | double   | \n| $w_{\\text{max}}$      | 0.03              | per projection | double   | \n| $\\alpha$              | 0.1               | per projection | double   | \n| $\\beta$               | 1.0               | per projection | double   | \n| $\\gamma$              | 50.0              | per projection | double   | \n| ${\\text{Rtarget}}$    | 35.0              | per projection | double   | \n| $T$                   | 5000.0            | per projection | double   | \n\n**Equations:**\n\n* Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n$$\n\n* Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n$$\n\n* Variable $R$ : per post-synaptic neuron, initial value: 0.0\n\n$$\n{R}(t) = {r}^{\\text{post}}(t)\n$$\n\n* Variable $K$ : per post-synaptic neuron, initial value: 0.0\n\n$$\n{K}(t) = \\frac{{R}(t)}{T \\cdot \\left(\\gamma \\cdot \\left|{f}\\right|{\\left (- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1.0 \\right )} + 1.0\\right)}\n$$\n\n* Variable ${\\text{stdp}}$ : per synapse, initial value: 0.0\n\n$$\n{{\\text{stdp}}}(t) = \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n$$\n\n* Variable $w$ : per synapse, initial value: 0.0, minimum: w_min, maximum: w_max\n\n$$\n{w}(t) \\mathrel{+}= {K}(t) \\cdot \\left(\\alpha \\cdot {w}(t) \\cdot \\left(- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1\\right) + \\beta \\cdot {{\\text{stdp}}}(t)\\right)\n$$\n\n**Pre-synaptic event at $t_\\text{pre} + d$:**\n$$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$\n$${{\\text{ltp}}}(t) = A_{\\text{plus}}$$\n\n**Post-synaptic event at $t_\\text{post}$:**\n$${{\\text{ltd}}}(t) = A_{\\text{minus}}$$\n\n# Parameters\n\n## Population parameters\n\n| **Population**                | **Neuron type**            | **Name**             | **Value**     | \n| ----------------------------- | -------------------------- | -------------------- | ------------- | \n| Poisson input                 | Poisson                    | ${\\text{rates}}$     | $[0.2, 20.0]$ | \n| RS neuron without homeostasis | Regular-spiking Izhikevich | $a$                  | 0.02          | \n|                               |                            | $b$                  | 0.2           | \n|                               |                            | $c$                  | -65.0         | \n|                               |                            | $d$                  | 8.0           | \n|                               |                            | $\\tau_{\\text{ampa}}$ | 5.0           | \n|                               |                            | $\\tau_{\\text{nmda}}$ | 150.0         | \n|                               |                            | ${\\text{vrev}}$      | 0.0           | \n| RS neuron with homeostasis    | Regular-spiking Izhikevich | $a$                  | 0.02          | \n|                               |                            | $b$                  | 0.2           | \n|                               |                            | $c$                  | -65.0         | \n|                               |                            | $d$                  | 8.0           | \n|                               |                            | $\\tau_{\\text{ampa}}$ | 5.0           | \n|                               |                            | $\\tau_{\\text{nmda}}$ | 150.0         | \n|                               |                            | ${\\text{vrev}}$      | 0.0           | \n\n\n\n## Projection parameters\n\n| **Projection**                                                                     | **Synapse type**                        | **Name**              | **Value** | \n| ---------------------------------------------------------------------------------- | --------------------------------------- | --------------------- | --------- | \n| Poisson input  $\\rightarrow$ RS neuron without homeostasis with target ampa / nmda | Nearest-neighbour STDP                  | $\\tau_{\\text{plus}}$  | 20.0      | \n|                                                                                    |                                         | $\\tau_{\\text{minus}}$ | 60.0      | \n|                                                                                    |                                         | $A_{\\text{plus}}$     | 0.0002    | \n|                                                                                    |                                         | $A_{\\text{minus}}$    | 6.6e-05   | \n|                                                                                    |                                         | $w_{\\text{max}}$      | 0.03      | \n| Poisson input  $\\rightarrow$ RS neuron with homeostasis with target ampa / nmda    | Nearest-neighbour STDP with homeostasis | $\\tau_{\\text{plus}}$  | 20.0      | \n|                                                                                    |                                         | $\\tau_{\\text{minus}}$ | 60.0      | \n|                                                                                    |                                         | $A_{\\text{plus}}$     | 0.0002    | \n|                                                                                    |                                         | $A_{\\text{minus}}$    | 6.6e-05   | \n|                                                                                    |                                         | $w_{\\text{min}}$      | 0.0       | \n|                                                                                    |                                         | $w_{\\text{max}}$      | 0.03      | \n|                                                                                    |                                         | $\\alpha$              | 0.1       | \n|                                                                                    |                                         | $\\beta$               | 1.0       | \n|                                                                                    |                                         | $\\gamma$              | 50.0      | \n|                                                                                    |                                         | ${\\text{Rtarget}}$    | 35.0      | \n|                                                                                    |                                         | $T$                   | 5000.0    |",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Reporting"
    ]
  },
  {
    "objectID": "manual/Reporting.html#rendered-version",
    "href": "manual/Reporting.html#rendered-version",
    "title": "Reporting",
    "section": "",
    "text": "ANNarchy 5.0.0 using the default backend.\nNumerical step size: 1.0 ms.\n\n\n\n\n\n\n\n\n\n\n\nPopulation\nSize\nNeuron type\n\n\n\n\nPoisson input\n100\nPoisson\n\n\nRS neuron without homeostasis\n1\nRegular-spiking Izhikevich\n\n\nRS neuron with homeostasis\n1\nRegular-spiking Izhikevich\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nDestination\nTarget\nSynapse type\nPattern\n\n\n\n\nPoisson input\nRS neuron without homeostasis\nampa / nmda\nNearest-neighbour STDP\nAll-to-All, weights \\mathcal{U}(0.01, 0.03), delays 0.0\n\n\nPoisson input\nRS neuron with homeostasis\nampa / nmda\nNearest-neighbour STDP with homeostasis\nAll-to-All, weights \\mathcal{U}(0.01, 0.03), delays 0.0\n\n\n\n\n\n\n\n\nObject\nVariables\nPeriod\n\n\n\n\nRS neuron without homeostasis\nr\n1.0\n\n\nRS neuron with homeostasis\nr\n1.0\n\n\n\n\n\n\n\n\n\nRegular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\nParameters:\n\n\n\nName\nDefault value\nLocality\nType\n\n\n\n\na\n0.02\nper population\ndouble\n\n\nb\n0.2\nper population\ndouble\n\n\nc\n-65.0\nper population\ndouble\n\n\nd\n8.0\nper population\ndouble\n\n\n\\tau_{\\text{ampa}}\n5.0\nper population\ndouble\n\n\n\\tau_{\\text{nmda}}\n150.0\nper population\ndouble\n\n\n{\\text{vrev}}\n0.0\nper population\ndouble\n\n\n\nEquations:\n\nVariable I : per neuron, initial value: 0.0\n\n\n{I}(t) = {g_{\\text{ampa}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) + {g_{\\text{nmda}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) \\cdot \\operatorname{nmda}{\\left ({v}(t),-80.0,60.0 \\right )}\n\n\nVariable v : per neuron, initial value: -65.0, midpoint numerical method\n\n\n\\frac{d{v}(t)}{dt} = {I}(t) - {u}(t) + {v}(t) \\cdot \\left(0.04 \\cdot {v}(t) + 5.0\\right) + 140.0\n\n\nVariable u : per neuron, initial value: -13.0, midpoint numerical method\n\n\n\\frac{d{u}(t)}{dt} = a \\cdot \\left(b \\cdot {v}(t) - {u}(t)\\right)\n\n\nVariable g_{\\text{ampa}} : per neuron, initial value: 0.0, exponential numerical method\n\n\n\\frac{d{g_{\\text{ampa}}}(t)}{dt} \\cdot \\tau_{\\text{ampa}} = - {g_{\\text{ampa}}}(t)\n\n\nVariable g_{\\text{nmda}} : per neuron, initial value: 0.0, exponential numerical method\n\n\n\\frac{d{g_{\\text{nmda}}}(t)}{dt} \\cdot \\tau_{\\text{nmda}} = - {g_{\\text{nmda}}}(t)\n\nSpike emission:\nif {v}(t) \\geq 30.0 :\n\nEmit a spike a time t.\n{v}(t) = c\n{u}(t) \\mathrel{+}= d\n\nFunctions\n{\\text{nmda}}(v, t, s) = \\frac{\\left(- t + v\\right)^{2}}{s^{2} \\cdot \\left(1.0 + \\frac{1}{s^{2}} \\cdot \\left(- t + v\\right)^{2}\\right)}\n\n\n\nSpiking neuron with spikes emitted according to a Poisson distribution.\nParameters:\n\n\n\nName\nDefault value\nLocality\nType\n\n\n\n\n{\\text{rates}}\n10.0\nper neuron\ndouble\n\n\n\nEquations:\n\nVariable p : per neuron, initial value: 0.0\n\n\n{p}(t) = \\frac{1000.0}{\\Delta t} \\cdot \\mathcal{U}{\\left (0.0,1.0 \\right )}\n\nSpike emission:\nif {p}(t) &lt; {\\text{rates}} :\n\nEmit a spike a time t.\n\n\n\n\n\n\n\nNearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\nParameters:\n\n\n\n\n\n\n\n\n\nName\nDefault value\nLocality\nType\n\n\n\n\n\\tau_{\\text{plus}}\n20.0\nper projection\ndouble\n\n\n\\tau_{\\text{minus}}\n60.0\nper projection\ndouble\n\n\nA_{\\text{plus}}\n0.0002\nper projection\ndouble\n\n\nA_{\\text{minus}}\n6.6e-05\nper projection\ndouble\n\n\nw_{\\text{max}}\n0.03\nper projection\ndouble\n\n\n\nEquations:\n\nVariable {\\text{ltp}} : per synapse, initial value: 0.0, exponential numerical method\n\n\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n\n\nVariable {\\text{ltd}} : per synapse, initial value: 0.0, exponential numerical method\n\n\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n\n\nVariable w : per synapse, initial value: 0.0, minimum: 0.0, maximum: w_max\n\n\n{w}(t) \\mathrel{+}= \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n\nPre-synaptic event at t_\\text{pre} + d: g_{\\text{target}(t)} \\mathrel{+}= {w}(t) {{\\text{ltp}}}(t) = A_{\\text{plus}}\nPost-synaptic event at t_\\text{post}: {{\\text{ltd}}}(t) = A_{\\text{minus}}\n\n\n\nNearest-neighbour STDP synaptic plasticity with an additional homeostatic term.\nParameters:\n\n\n\n\n\n\n\n\n\nName\nDefault value\nLocality\nType\n\n\n\n\n\\tau_{\\text{plus}}\n20.0\nper projection\ndouble\n\n\n\\tau_{\\text{minus}}\n60.0\nper projection\ndouble\n\n\nA_{\\text{plus}}\n0.0002\nper projection\ndouble\n\n\nA_{\\text{minus}}\n6.6e-05\nper projection\ndouble\n\n\nw_{\\text{min}}\n0.0\nper projection\ndouble\n\n\nw_{\\text{max}}\n0.03\nper projection\ndouble\n\n\n\\alpha\n0.1\nper projection\ndouble\n\n\n\\beta\n1.0\nper projection\ndouble\n\n\n\\gamma\n50.0\nper projection\ndouble\n\n\n{\\text{Rtarget}}\n35.0\nper projection\ndouble\n\n\nT\n5000.0\nper projection\ndouble\n\n\n\nEquations:\n\nVariable {\\text{ltp}} : per synapse, initial value: 0.0, exponential numerical method\n\n\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n\n\nVariable {\\text{ltd}} : per synapse, initial value: 0.0, exponential numerical method\n\n\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n\n\nVariable R : per post-synaptic neuron, initial value: 0.0\n\n\n{R}(t) = {r}^{\\text{post}}(t)\n\n\nVariable K : per post-synaptic neuron, initial value: 0.0\n\n\n{K}(t) = \\frac{{R}(t)}{T \\cdot \\left(\\gamma \\cdot \\left|{f}\\right|{\\left (- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1.0 \\right )} + 1.0\\right)}\n\n\nVariable {\\text{stdp}} : per synapse, initial value: 0.0\n\n\n{{\\text{stdp}}}(t) = \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n\n\nVariable w : per synapse, initial value: 0.0, minimum: w_min, maximum: w_max\n\n\n{w}(t) \\mathrel{+}= {K}(t) \\cdot \\left(\\alpha \\cdot {w}(t) \\cdot \\left(- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1\\right) + \\beta \\cdot {{\\text{stdp}}}(t)\\right)\n\nPre-synaptic event at t_\\text{pre} + d: g_{\\text{target}(t)} \\mathrel{+}= {w}(t) {{\\text{ltp}}}(t) = A_{\\text{plus}}\nPost-synaptic event at t_\\text{post}: {{\\text{ltd}}}(t) = A_{\\text{minus}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation\nNeuron type\nName\nValue\n\n\n\n\nPoisson input\nPoisson\n{\\text{rates}}\n[0.2, 20.0]\n\n\nRS neuron without homeostasis\nRegular-spiking Izhikevich\na\n0.02\n\n\n\n\nb\n0.2\n\n\n\n\nc\n-65.0\n\n\n\n\nd\n8.0\n\n\n\n\n\\tau_{\\text{ampa}}\n5.0\n\n\n\n\n\\tau_{\\text{nmda}}\n150.0\n\n\n\n\n{\\text{vrev}}\n0.0\n\n\nRS neuron with homeostasis\nRegular-spiking Izhikevich\na\n0.02\n\n\n\n\nb\n0.2\n\n\n\n\nc\n-65.0\n\n\n\n\nd\n8.0\n\n\n\n\n\\tau_{\\text{ampa}}\n5.0\n\n\n\n\n\\tau_{\\text{nmda}}\n150.0\n\n\n\n\n{\\text{vrev}}\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjection\nSynapse type\nName\nValue\n\n\n\n\nPoisson input \\rightarrow RS neuron without homeostasis with target ampa / nmda\nNearest-neighbour STDP\n\\tau_{\\text{plus}}\n20.0\n\n\n\n\n\\tau_{\\text{minus}}\n60.0\n\n\n\n\nA_{\\text{plus}}\n0.0002\n\n\n\n\nA_{\\text{minus}}\n6.6e-05\n\n\n\n\nw_{\\text{max}}\n0.03\n\n\nPoisson input \\rightarrow RS neuron with homeostasis with target ampa / nmda\nNearest-neighbour STDP with homeostasis\n\\tau_{\\text{plus}}\n20.0\n\n\n\n\n\\tau_{\\text{minus}}\n60.0\n\n\n\n\nA_{\\text{plus}}\n0.0002\n\n\n\n\nA_{\\text{minus}}\n6.6e-05\n\n\n\n\nw_{\\text{min}}\n0.0\n\n\n\n\nw_{\\text{max}}\n0.03\n\n\n\n\n\\alpha\n0.1\n\n\n\n\n\\beta\n1.0\n\n\n\n\n\\gamma\n50.0\n\n\n\n\n{\\text{Rtarget}}\n35.0\n\n\n\n\nT\n5000.0",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Reporting"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html",
    "href": "manual/RateSynapse.html",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "As for neurons, you can define the synaptic behavior using a Synapse object. Although the description is local to a synapse, the same ODE will be applied to all synapses of a given Projection from one population to another. The same vocabulary as for neurons is accessible (constants, functions, conditional statements), except that the synapse must distinguish pre-synaptic and post-synaptic parameters/variables.\nLike r for a rate-coded neuron, one variable is special for a rate-coded synapse:\n\nw represents the synaptic efficiency (or the weight of the connection). If an ODE is defined for this variable, this will implement a learning rule. If none is provided, the synapse is non-plastic.\n\nThe ODEs for synaptic variables follow the same syntax as for neurons. As for neurons, the following variables are already defined:\n\nt: time in milliseconds elapsed since the creation of the network.\ndt: the discretization step is 1.0ms by default.\n\n\n\nLearning is possible by modifying the variable w of a single synapse during the simulation.\nFor example, the Oja learning rule (see the example Bar learning):\n\\tau \\, \\frac{d w(t)}{dt} = r_\\text{pre} \\, r_\\text{post} - \\alpha \\, r_\\text{post}^2 \\, w(t)\ncould be implemented this way:\nOja = ann.Synapse(\n    parameters = dict(\n        tau = 5000,\n        alpha = 8.0\n    ),\n    equations = [\n        'tau * dw / dt = pre.r * post.r - alpha * post.r^2 * w',\n    ]\n)\nNote that it is equivalent to define the increment directly if you want to apply the explicit Euler method:\nequations = [\n    'w += dt / tau * ( pre.r * post.r - alpha * post.r^2 * w)',\n]\nThe same vocabulary as for rate-coded neurons applies. Custom functions can also be defined:\nOja = ann.Synapse(\n    parameters = dict(\n        tau = 5000,\n        alpha = 8.0\n    ),\n    equations = [\n        'tau * dw / dt = product(pre.r,  post.r) - alpha * post.r^2 * w',\n    ],\n    functions = \"\"\"\n        product(x,y) = x * y\n    \"\"\",\n)\n\n\n\nA synapse needs to access neural variables both at the pre- and post-synaptic levels. For the pre-synaptic neuron, biologically realistic synapses should only need its firing rate, but in some cases it may be useful to access other variables as well.\nIn order to use neural variables in a synaptic variable, you have to prefix them with pre. or post.. For example:\npre.r, post.baseline, post.mp...\nANNarchy will check before the compilation that the pre- or post-synaptic neuron types indeed define such variables.\n\n\n\n\n\n\nNote\n\n\n\nIf the projection uses delays, all pre-synaptic variables used in the synapse model will be delayed.\n\n\n\n\n\nThere are 3 levels of locality for a synaptic parameter or variable:\n\nlocal: there is one value per synapse in the projection (default for variables).\nsemiglobal: there is one value per post-synaptic neuron in the projection.\nglobal: there is only one value for the whole projection (default for parameters).\n\nThe following BCM learning rule makes use of the three levels of locality:\nBCM = ann.Synapse(\n    parameters = dict(\n        eta = 0.01, # global\n        tau = 100., # global\n    ),\n    equations = [\n        ann.Variable('tau * dtheta/dt + theta = post.r^2', locality='semiglobal'),\n        ann.Variable('dw/dt = eta * post.r * (post.r - theta) * pre.r', min=0.0), # local\n    ]\n)\neta and tau are global parameters to the projection: all synapses will use the same value. theta defines one value per post-synaptic neuron: it tracks the average of the post-synaptic firing rate. There is therefore no need to update one value per synapse, so we can use the value semiglobal. Naturally, w is local to each synapse, so no locality should be passed.\n\n\n\nSome learning rules require global information about the pre- or post-synaptic population, which is not local to the synapse, such as the mean or maximal activity in the pre-synaptic population. This information can be accessed at the synapse-level. The special functions:\n\nmin(v) for the minimum: \\min_i v_i,\nmax(v) for the maximum: \\max_i v_i,\nmean(v) for the mean: \\frac{1}{N} \\sum_i v_i,\nnorm1(v) for the L1-norm: \\frac{1}{N} \\sum_i |v_i|,\nnorm2(v) for the L2-norm: \\frac{1}{N} \\sum_i v_i^2\n\nare available for any pre- or post-synaptic variable.\nFor example, some covariance-based learning rules depend on the mean firing in the pre- and post-synaptic populations:\n\\tau \\frac{d w(t)}{dt} = (r_\\text{pre} - \\hat{r}_\\text{pre} )  * (r_\\text{post} - \\hat{r}_\\text{post} )\nUsing the global operations, such a learning rule is trivial to implement:\nCovariance = ann.Synapse(\n    parameters = dict(\n        tau = 5000.0\n    ),\n    equations = [\n        'tau * dw/dt = (pre.r - mean(pre.r) ) * (post.r - mean(post.r) )'\n    ]\n)\n\n\n\n\n\n\nWarning\n\n\n\n\nSuch global operations can become expensive to compute if the populations are too big.\nThe global operations are performed over the whole population, not only the synapses which actually reach the post-synaptic neuron.\nThey can only be applied to a single variable, not a combination or function of them.\n\n\n\n\n\n\nThe argument psp of a Synapse object represents the post-synaptic potential evoked by the pre-synaptic neuron. This value is actually summed by the post-synaptic neuron over all other synapses of the same projection in sum(target). If not defined, it will simply represent the product between the pre-synaptic firing rate (pre.r) and the weight value (w).\n\n    g_i = \\sum_{j} \\text{psp}(i) = \\sum_{i} w_{ij} \\, r^\\text{pre}_i\n\nThe post-synaptic potential of a single synapse is therefore by default:\ndefault_synapse = ann.Synapse(\n    psp = \"w * pre.r\"\n)\nwhere pre.r is the pre-synaptic firing rate, but you may want to override this behaviour in certain cases.\nFor example, you may want to model a non-linear synapse with a logarithmic term:\n\ng_i = \\sum_j \\log \\left( \\frac {( w_{ij} \\, r^\\text{pre}_j) + 1 } { ( w_{ij}/, r^\\text{pre}_j ) - 1 } \\right)\n\nIn this case, you can just modify the psp argument of the synapse:\nnonlinear_synapse = ann.Synapse( \n    psp = \"log( (pre.r * w + 1 ) / (pre.r * w - 1) )\"\n)\nNo further modification has to be done in the post-synaptic neuron, this value will be summed over all pre-synaptic neurons automatically when using sum(target).\n\n\n\nIt is possible to define a different operation than a sum performed on the connected synapses, using the operation argument of the synapse:\nMaxPooling = ann.Synapse(\n    psp = \"w * pre.r\",\n    operation = \"max\"\n)\nIn this case, sum(target) will represent the maximum value of w * pre.r over all incoming synapses, not their sum. It can be useful when defining pooling operations in a convolutional network, for example.\nThe available operations are:\n\n\"sum\": (default): sum of all incoming psps.\n\"max\": maximum of all incoming psps.\n\"min\": minimum of all incoming psps.\n\"mean\": mean of all incoming psps.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese operations are only possible for rate-coded synapses.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html#synaptic-plasticity",
    "href": "manual/RateSynapse.html#synaptic-plasticity",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "Learning is possible by modifying the variable w of a single synapse during the simulation.\nFor example, the Oja learning rule (see the example Bar learning):\n\\tau \\, \\frac{d w(t)}{dt} = r_\\text{pre} \\, r_\\text{post} - \\alpha \\, r_\\text{post}^2 \\, w(t)\ncould be implemented this way:\nOja = ann.Synapse(\n    parameters = dict(\n        tau = 5000,\n        alpha = 8.0\n    ),\n    equations = [\n        'tau * dw / dt = pre.r * post.r - alpha * post.r^2 * w',\n    ]\n)\nNote that it is equivalent to define the increment directly if you want to apply the explicit Euler method:\nequations = [\n    'w += dt / tau * ( pre.r * post.r - alpha * post.r^2 * w)',\n]\nThe same vocabulary as for rate-coded neurons applies. Custom functions can also be defined:\nOja = ann.Synapse(\n    parameters = dict(\n        tau = 5000,\n        alpha = 8.0\n    ),\n    equations = [\n        'tau * dw / dt = product(pre.r,  post.r) - alpha * post.r^2 * w',\n    ],\n    functions = \"\"\"\n        product(x,y) = x * y\n    \"\"\",\n)",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html#neuron-specific-variables",
    "href": "manual/RateSynapse.html#neuron-specific-variables",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "A synapse needs to access neural variables both at the pre- and post-synaptic levels. For the pre-synaptic neuron, biologically realistic synapses should only need its firing rate, but in some cases it may be useful to access other variables as well.\nIn order to use neural variables in a synaptic variable, you have to prefix them with pre. or post.. For example:\npre.r, post.baseline, post.mp...\nANNarchy will check before the compilation that the pre- or post-synaptic neuron types indeed define such variables.\n\n\n\n\n\n\nNote\n\n\n\nIf the projection uses delays, all pre-synaptic variables used in the synapse model will be delayed.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html#locality",
    "href": "manual/RateSynapse.html#locality",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "There are 3 levels of locality for a synaptic parameter or variable:\n\nlocal: there is one value per synapse in the projection (default for variables).\nsemiglobal: there is one value per post-synaptic neuron in the projection.\nglobal: there is only one value for the whole projection (default for parameters).\n\nThe following BCM learning rule makes use of the three levels of locality:\nBCM = ann.Synapse(\n    parameters = dict(\n        eta = 0.01, # global\n        tau = 100., # global\n    ),\n    equations = [\n        ann.Variable('tau * dtheta/dt + theta = post.r^2', locality='semiglobal'),\n        ann.Variable('dw/dt = eta * post.r * (post.r - theta) * pre.r', min=0.0), # local\n    ]\n)\neta and tau are global parameters to the projection: all synapses will use the same value. theta defines one value per post-synaptic neuron: it tracks the average of the post-synaptic firing rate. There is therefore no need to update one value per synapse, so we can use the value semiglobal. Naturally, w is local to each synapse, so no locality should be passed.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html#global-operations",
    "href": "manual/RateSynapse.html#global-operations",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "Some learning rules require global information about the pre- or post-synaptic population, which is not local to the synapse, such as the mean or maximal activity in the pre-synaptic population. This information can be accessed at the synapse-level. The special functions:\n\nmin(v) for the minimum: \\min_i v_i,\nmax(v) for the maximum: \\max_i v_i,\nmean(v) for the mean: \\frac{1}{N} \\sum_i v_i,\nnorm1(v) for the L1-norm: \\frac{1}{N} \\sum_i |v_i|,\nnorm2(v) for the L2-norm: \\frac{1}{N} \\sum_i v_i^2\n\nare available for any pre- or post-synaptic variable.\nFor example, some covariance-based learning rules depend on the mean firing in the pre- and post-synaptic populations:\n\\tau \\frac{d w(t)}{dt} = (r_\\text{pre} - \\hat{r}_\\text{pre} )  * (r_\\text{post} - \\hat{r}_\\text{post} )\nUsing the global operations, such a learning rule is trivial to implement:\nCovariance = ann.Synapse(\n    parameters = dict(\n        tau = 5000.0\n    ),\n    equations = [\n        'tau * dw/dt = (pre.r - mean(pre.r) ) * (post.r - mean(post.r) )'\n    ]\n)\n\n\n\n\n\n\nWarning\n\n\n\n\nSuch global operations can become expensive to compute if the populations are too big.\nThe global operations are performed over the whole population, not only the synapses which actually reach the post-synaptic neuron.\nThey can only be applied to a single variable, not a combination or function of them.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html#defining-the-post-synaptic-potential-psp",
    "href": "manual/RateSynapse.html#defining-the-post-synaptic-potential-psp",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "The argument psp of a Synapse object represents the post-synaptic potential evoked by the pre-synaptic neuron. This value is actually summed by the post-synaptic neuron over all other synapses of the same projection in sum(target). If not defined, it will simply represent the product between the pre-synaptic firing rate (pre.r) and the weight value (w).\n\n    g_i = \\sum_{j} \\text{psp}(i) = \\sum_{i} w_{ij} \\, r^\\text{pre}_i\n\nThe post-synaptic potential of a single synapse is therefore by default:\ndefault_synapse = ann.Synapse(\n    psp = \"w * pre.r\"\n)\nwhere pre.r is the pre-synaptic firing rate, but you may want to override this behaviour in certain cases.\nFor example, you may want to model a non-linear synapse with a logarithmic term:\n\ng_i = \\sum_j \\log \\left( \\frac {( w_{ij} \\, r^\\text{pre}_j) + 1 } { ( w_{ij}/, r^\\text{pre}_j ) - 1 } \\right)\n\nIn this case, you can just modify the psp argument of the synapse:\nnonlinear_synapse = ann.Synapse( \n    psp = \"log( (pre.r * w + 1 ) / (pre.r * w - 1) )\"\n)\nNo further modification has to be done in the post-synaptic neuron, this value will be summed over all pre-synaptic neurons automatically when using sum(target).",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html#defining-the-post-synaptic-operation",
    "href": "manual/RateSynapse.html#defining-the-post-synaptic-operation",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "It is possible to define a different operation than a sum performed on the connected synapses, using the operation argument of the synapse:\nMaxPooling = ann.Synapse(\n    psp = \"w * pre.r\",\n    operation = \"max\"\n)\nIn this case, sum(target) will represent the maximum value of w * pre.r over all incoming synapses, not their sum. It can be useful when defining pooling operations in a convolutional network, for example.\nThe available operations are:\n\n\"sum\": (default): sum of all incoming psps.\n\"max\": maximum of all incoming psps.\n\"min\": minimum of all incoming psps.\n\"mean\": mean of all incoming psps.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese operations are only possible for rate-coded synapses.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/Saving.html",
    "href": "manual/Saving.html",
    "title": "Saving and loading",
    "section": "",
    "text": "For the complete APIs, see IO in the library reference.\n\n\nThe global parameters of the network can be saved to and loaded from a JSON file using the functions save_parameters() and load_parameters():\nnet.save_parameters('network.json')\nnet.load_parameters('network.json')\nThe saved JSON file for a network of two populations of Izhikevich neurons connected with STDP will look like:\n{\n    \"populations\": {\n        \"pop0\": {\n            \"a\": 0.1,\n            \"b\": 0.2,\n            \"c\": -65.0,\n            \"d\": 8.0,\n            \"tau_ampa\": 5.0,\n            \"tau_nmda\": 150.0,\n            \"v_rev\": 0.0,\n            \"v_thresh\": 30.0\n        },\n        \"pop1\": {\n            \"a\": 0.02,\n            \"b\": 0.2,\n            \"c\": -65.0,\n            \"d\": 8.0,\n            \"tau_ampa\": 5.0,\n            \"tau_nmda\": 150.0,\n            \"v_rev\": 0.0,\n            \"v_thresh\": 30.0\n        }\n    },\n    \"projections\": {\n        \"proj0\": {\n            \"tau_plus\": 20.0,\n            \"tau_minus\": 60.0,\n            \"A_plus\": 0.0002,\n            \"A_minus\": 6.6e-05,\n            \"w_max\": 0.03\n        }\n    },\n    \"network\": {},\n}\nBy default, populations and projections have names like pop0 and proj1. For readability, we advise setting explicit (and unique) names in their constructor:\npop = net.create(100, Izhikevich, name=\"PFC_exc\")\nproj = net.create(pop, pop2, 'exc', STDP, name=\"PFC_exc_to_inh\")\nOnly global parameters can be saved (no array is allowed in the JSON file). By default, only global parameters will be loaded, except if the global_only argument to load_parameters() is set to False. In that case, even local parameters can be set by the JSON file, but they will all use the same values.\nIf you want to initialize other things than population/projection global parameters, you can define arbitrary values in the \"network\" dictionary:\n{\n    \"network\": {\n        \"pop1_r_min\": 0.1,\n        \"pop1_r_max\": 1.3,\n    },\n}\nload_parameters() will return the corresponding dictionary:\nparams = net.load_parameters('network.json')\nYou can then use them to initialize programmatically non-global parameters or variables:\npop1.r = ann.Uniform(params['pop1_r_min'], params['pop1_r_max'])\n\n\n\nThe state of all variables, including the synaptic weights, can be saved in a text file, compressed binary file or Matlab file using the save() method:\nnet.save('data.txt')\nnet.save('data.txt.gz')\nnet.save('data.mat')\nFilenames ending with .mat correspond to Matlab files (it requires the installation of Scipy), filenames ending with .gz are compressed using gzip (normally standard to all Python distributions, but may require installation), other extensions are normal text files using cPickle (standard).\nsave() also accepts the populations and projections boolean flags. If True (the default), the neural resp. synaptic variables will be saved. For example, if you only care about synaptic plasticity but not the neural variables, you can set populations to False, and only synaptic variables will be saved.\nnet.save('data.txt', populations=False)\nExcept for the Matlab format, you can also load the state of variables stored in these files once the network is compiled:\nnet.load('data.txt')\n\n\n\n\n\n\nWarning\n\n\n\nThe structure of the network must of course be the same as when the file was saved: number of populations, neurons and projections. The neuron and synapse types must define the same variables. If a variable was saved but does not exist anymore, it will be skipped. If the variable did not exist, its current value will be kept, what can lead to crashes.\n\n\nload() also accepts the populations and projections boolean flags (for example if you want to load only the synaptic weights but not to restore the neural variables).\n\n\n\nPopulation and Projection objects also have save() and load() methods, allowing to save the corresponding information individually:\npop1.save('pop1.npz')\nproj.save('proj.npz')\n\npop1.load('pop1.npz')\nproj.load('proj.npz')\nThe allowed file formats are:\n\n.npz: compressed Numpy binary format (np.savez_compressed), preferred.\n*.gz: gunzipped binary text file.\n*.mat: Matlab 7.2.\n*: binary text file.\n\nAs before, .mat can only be used for saving, not loading.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Saving and loading"
    ]
  },
  {
    "objectID": "manual/Saving.html#global-parameters",
    "href": "manual/Saving.html#global-parameters",
    "title": "Saving and loading",
    "section": "",
    "text": "The global parameters of the network can be saved to and loaded from a JSON file using the functions save_parameters() and load_parameters():\nnet.save_parameters('network.json')\nnet.load_parameters('network.json')\nThe saved JSON file for a network of two populations of Izhikevich neurons connected with STDP will look like:\n{\n    \"populations\": {\n        \"pop0\": {\n            \"a\": 0.1,\n            \"b\": 0.2,\n            \"c\": -65.0,\n            \"d\": 8.0,\n            \"tau_ampa\": 5.0,\n            \"tau_nmda\": 150.0,\n            \"v_rev\": 0.0,\n            \"v_thresh\": 30.0\n        },\n        \"pop1\": {\n            \"a\": 0.02,\n            \"b\": 0.2,\n            \"c\": -65.0,\n            \"d\": 8.0,\n            \"tau_ampa\": 5.0,\n            \"tau_nmda\": 150.0,\n            \"v_rev\": 0.0,\n            \"v_thresh\": 30.0\n        }\n    },\n    \"projections\": {\n        \"proj0\": {\n            \"tau_plus\": 20.0,\n            \"tau_minus\": 60.0,\n            \"A_plus\": 0.0002,\n            \"A_minus\": 6.6e-05,\n            \"w_max\": 0.03\n        }\n    },\n    \"network\": {},\n}\nBy default, populations and projections have names like pop0 and proj1. For readability, we advise setting explicit (and unique) names in their constructor:\npop = net.create(100, Izhikevich, name=\"PFC_exc\")\nproj = net.create(pop, pop2, 'exc', STDP, name=\"PFC_exc_to_inh\")\nOnly global parameters can be saved (no array is allowed in the JSON file). By default, only global parameters will be loaded, except if the global_only argument to load_parameters() is set to False. In that case, even local parameters can be set by the JSON file, but they will all use the same values.\nIf you want to initialize other things than population/projection global parameters, you can define arbitrary values in the \"network\" dictionary:\n{\n    \"network\": {\n        \"pop1_r_min\": 0.1,\n        \"pop1_r_max\": 1.3,\n    },\n}\nload_parameters() will return the corresponding dictionary:\nparams = net.load_parameters('network.json')\nYou can then use them to initialize programmatically non-global parameters or variables:\npop1.r = ann.Uniform(params['pop1_r_min'], params['pop1_r_max'])",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Saving and loading"
    ]
  },
  {
    "objectID": "manual/Saving.html#complete-state-of-the-network",
    "href": "manual/Saving.html#complete-state-of-the-network",
    "title": "Saving and loading",
    "section": "",
    "text": "The state of all variables, including the synaptic weights, can be saved in a text file, compressed binary file or Matlab file using the save() method:\nnet.save('data.txt')\nnet.save('data.txt.gz')\nnet.save('data.mat')\nFilenames ending with .mat correspond to Matlab files (it requires the installation of Scipy), filenames ending with .gz are compressed using gzip (normally standard to all Python distributions, but may require installation), other extensions are normal text files using cPickle (standard).\nsave() also accepts the populations and projections boolean flags. If True (the default), the neural resp. synaptic variables will be saved. For example, if you only care about synaptic plasticity but not the neural variables, you can set populations to False, and only synaptic variables will be saved.\nnet.save('data.txt', populations=False)\nExcept for the Matlab format, you can also load the state of variables stored in these files once the network is compiled:\nnet.load('data.txt')\n\n\n\n\n\n\nWarning\n\n\n\nThe structure of the network must of course be the same as when the file was saved: number of populations, neurons and projections. The neuron and synapse types must define the same variables. If a variable was saved but does not exist anymore, it will be skipped. If the variable did not exist, its current value will be kept, what can lead to crashes.\n\n\nload() also accepts the populations and projections boolean flags (for example if you want to load only the synaptic weights but not to restore the neural variables).",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Saving and loading"
    ]
  },
  {
    "objectID": "manual/Saving.html#populations-and-projections-individually",
    "href": "manual/Saving.html#populations-and-projections-individually",
    "title": "Saving and loading",
    "section": "",
    "text": "Population and Projection objects also have save() and load() methods, allowing to save the corresponding information individually:\npop1.save('pop1.npz')\nproj.save('proj.npz')\n\npop1.load('pop1.npz')\nproj.load('proj.npz')\nThe allowed file formats are:\n\n.npz: compressed Numpy binary format (np.savez_compressed), preferred.\n*.gz: gunzipped binary text file.\n*.mat: Matlab 7.2.\n*: binary text file.\n\nAs before, .mat can only be used for saving, not loading.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Saving and loading"
    ]
  },
  {
    "objectID": "manual/Inputs.html",
    "href": "manual/Inputs.html",
    "title": "Setting inputs",
    "section": "",
    "text": "The methods to set inputs to a network depends on whether it is rate-coded or spiking. Specific populations to set inputs are described in the API.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Setting inputs"
    ]
  },
  {
    "objectID": "manual/Inputs.html#inputs-to-a-rate-coded-network",
    "href": "manual/Inputs.html#inputs-to-a-rate-coded-network",
    "title": "Setting inputs",
    "section": "Inputs to a rate-coded network",
    "text": "Inputs to a rate-coded network\n\nStandard method\nThe simplest way to define an input population is to use a dummy neuron which simply holds a firing rate r as a local parameter, and connect it to another population:\nimport ANNarchy as ann\n\n# Create the network\nnet = ann.Network()\n\n# Input population\ninput_pop = net.create(10, Neuron(parameters=\"r=0.0\"))\n\n# Main population\npop = net.create(10, LeakyIntegrator)\n\n# Projection\nproj = net.connect(input_pop, pop, 'exc')\nproj.one_to_one(1.0)\n\n# Compile\nnet.compile()\n\n# Simulate for 100 ms\nnet.simulate(100.)\n\n# Change the input\ninput_pop.r = 1.0\n\n# Simulate\nnet.simulate(100.)\nThe only thing you need to do is to manipulate the numpy array r hold by input_pop, and it will influence the “real” population pop\nIt is important to define r as a local parameter of the neuron, not a variable in equations. A variable sees its value updated at each step, so the value you set would be immediately overwritten. Do not use a dictionary instead of the string, as the parameter would become global to the populations. This would be safer:\ninput_pop = net.create(10, Neuron(parameters=ann.Parameter(\"r=0.0\"))\n\n\nInput Array\nA totally equivalent way is to use an InputArray instance instead of the previous input population. The only difference is that the firing rate r of the input array can also be recorded, contrary to the previous case where r was a non-recordable parameter.\nThe specific population InputArray can be passed as the first and only argument to Network.create():\nimport ANNarchy as ann\n\n# Create the network\nnet = ann.Network()\n\n# Input population\ninput_pop = net.create(ann.InputArray())\n\n# Main population\npop = net.create(10, LeakyIntegrator)\n\n# Projection\nproj = net.connect(input_pop, pop, 'exc')\nproj.one_to_one(1.0)\n\n# Compile\nnet.compile()\n\n# Simulate for 100 ms\nnet.simulate(100.)\n\n# Change the input\ninput_pop.r = 1.0\n\n# Simulate\nnet.simulate(100.)\n\n\nTimed Arrays\nIf the inputs change frequently, it may be more efficient to store all these values in a TimedArray (doc in the API).\nLet’s suppose you have a population of 10 neurons which should be activated sequentially over time. You can store the inputs to these neurons in a Numpy array, where the first axis corresponds to time and the second (or more) to the geometry of the population:\ninputs = np.array(\n    [\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n    ]\n)\n\n# Network\nnet = ann.Network()\n\n# Input\ninp = net.create(ann.TimedArray(rates=inputs))\n\n# Population\npop = net.create(10, Neuron(equations=\"r=sum(exc)\"))\n\n# Projection\nproj = net.connect(inp, pop, 'exc')\nproj.one_to_one(1.0)\n\n# Compile and simulate\nnet.compile()\nnet.simulate(10.)\nWith this code, each neuron will be activated in sequence at each time step (dt=1.0 by default). If you simulate longer than 10 ms, the last input [0, 0, .., 1] will be kept forever.\nIf the rates array has two dimensions, the corresponding population will be 1D. You can pass a multidimensional array to obtain a 2D or 3D population.\nPresenting a input for only one time step is very short, especially if the population pop uses ODEs to integrate the inputs. You can provide a schedule parameter to the TimedArray to define how long (in ms) an input should be presented:\ninp = net.create(ann.TimedArray(rates=inputs, schedule=10.))\nHere each input will be kept constant for 10 ms, so the 10 inputs will need 100 ms of simulation to be presented. If you do not want a regular schedule, you can also provide a list of times where inputs should be set:\ninp = net.create(\n    ann.TimedArray(\n        rates=inputs, \n        schedule=[0., 10., 30., 60., 100., 150., 210., 280., 360., 450.]\n    )\n)\nThe length of the schedule list should be equal or smaller to the number of inputs defined in rates. If this length is smaller (e.g. 7), only the 7 first inputs will be used as inputs. If the length is bigger, it will lead to an error.\nA TimedArray can be reset to iterate again over the inputs:\ninp = net.create(ann.TimedArray(rates=inputs, schedule=10.))\n\n...\n\nnet.compile()\n\nnet.simulate(100.) # The ten inputs are shown with a schedule of 10 ms\n\ninp.reset()\n\nnet.simulate(100.) # The same ten inputs are presented again.\nThe times declared in schedule are therefore relative to the last call to reset() (or to t=0.0 at the beginning).\nIf you want to systematically iterate over the inputs without iterating over simulate() and reset(), you can provide the period argument to the TimedArray to define how often the inputs will be reset:\ninp = net.create(ann.TimedArray(rates=inputs, schedule=10.. period=100.))\n\n...\n\nnet.simulate(100000.)\nIf the period is smaller than the total durations of the inputs, the last inputs will be skipped.\nThe rates, schedule and period can be modified after compilation. The only constraint is that the size of the population (defined in the rates array) must stay the same.\n\n\nImages and Videos\nImages\nA simple utility to directly load an image into the firing rates r of a Population is provided by the ImagePopulation class. This class is not automatically imported with ANNarchy, you need to explicitly import it:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.image import ImagePopulation\n\ninp = net.create(ImagePopulation(geometry=(480, 640)))\ninp.set_image('image.jpg')\nUsing this class requires that you have the Python Image Library installed (pip install Pillow). Any image with a format supported by Pillow can be loaded, see the documentation.\nThe ImagePopulation must be initialized with a geometry corresponding to the desired size of the population. If it differs from the resolution of the image (set with set_image), the image will be first resized to match the geometry of the population.\n\n\n\n\n\n\nNote\n\n\n\nThe size of an image is defined as (height, width), so a 640x480 image should be loaded in a (480, 640) population.\n\n\nIf the geometry has only two dimensions (480, 640), each neuron will represent the luminance (or brightness) of the corresponding pixel.\nIf the geometry has three dimensions (480, 640, 3), the color channels will additionally be represented (RGB). Any other value than 3 for the third dimension will generate an error.\n\n\n\n\n\n\nNote\n\n\n\nThe firing rate r of a neuron is 1.0 when the corresponding pixel is white (value 255 as an unsigned integer on 8 bits).\n\n\nNote that the following code is functionally equivalent:\nimport ANNarchy as ann\nfrom PIL import Image\n\ninp = net.create(geometry=(480, 640), Neuron(parameters=\"r=0.0\"))\n\nimg = Image.open('image.jpg')\nimg = img.convert('L')\nimg = img.resize((480, 640)) /255.\n\ninp.r = np.array(img)\nAn example is provided in examples/image/Image.py.\nVideos\nThe VideoPopulation class allows to retrieve images from a Webcam, using the OpenCV computer vision library, version 4.0 or later. pkg-config opencv4 --cflags --libs should not return an error. vtk might have to be additionally installed.\nimport ANNarchy as ann\nfrom ANNarchy.extensions.image import VideoPopulation\n\ninp = net.create(VideoPopulation(geometry=(480, 640)))\n\nnet.compile()\n\ninp.start_camera(0)\n\nwhile(True):\n  inp.grab_image()\n  net.simulate(10.0)\nA geometry must be provided as for ImagePopulations. The camera must be explicitly started after compile() with inp.start_camera(0). 0 corresponds to the index of your camera, change it if you have multiple cameras.\nThe VideoPopulation can then acquire frames from the camera with inp.grab_image() and store the correponding image in its firing rate r (also scaled between 0.0 and 1.0). An example is provided in examples/image/Webcam.py.\n\n\n\n\n\n\nWarning\n\n\n\nVideoPopulation is not available with the CUDA backend.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Setting inputs"
    ]
  },
  {
    "objectID": "manual/Inputs.html#inputs-to-a-spiking-network",
    "href": "manual/Inputs.html#inputs-to-a-spiking-network",
    "title": "Setting inputs",
    "section": "Inputs to a spiking network",
    "text": "Inputs to a spiking network\n\nStandard method\nTo control the spiking patterns of a spiking population, the simplest way is to inject current into the corresponding membrane potentials. The built-in neuron types defined by ANNarchy have a i_offset variable that can be used for this purpose:\n\nnet = ann.Network(dt=0.1)\n\npop = net.create(100, ann.Izhikevich)\n\npop.i_offset= np.linspace(0.0, 30.0, 100)\n\nm = net.monitor(pop, 'spike')\n\nnet.compile(silent=True)\n\nnet.simulate(100.)\n\ndata = m.get('spike')\nt, n = m.raster_plot(data)\n\nplt.figure()\nplt.plot(t, n, '.')\nplt.ylim(0, 100)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCurrent injection\nIf you want the injected current to be time-varying, you can design a rate-coded population of the same size as the spiking population and create a CurrentInjection projection between them using Network.connect():\n\nnet = ann.Network()\n\ninp = net.create(30, ann.Neuron(equations=\"r = 2 + 2*sin(t/100) + Uniform(0, 1)\"))\npop = net.create(30, ann.Izhikevich)\n\nproj = net.connect(ann.CurrentInjection(inp, pop, 'exc'))\nproj.connect_current()\n\nm = net.monitor(pop, ['spike'])\n\nnet.compile(silent=True)\n\nnet.simulate(5000.)\n\ndata = m['spike']\nt, n = m.raster_plot(data)\n\nplt.figure()\nplt.plot(t, n, '.')\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\n\n\n\n\n\nThe current g_exc of a neuron in pop will be set at each time step to the firing rate r of the corresponding neuron in inp (i.e. with the same rank). inp can also be defined as a TimedArray.\nThe connector method should be connect_current(), which accepts no weight value and no delay.\n\n\nSpikeSourceArray\nIf you want to control precisely the spiking patterns used as inputs, you can provide a list of spike times to a SpikeSourceArray object:\n\nnet = ann.Network()\n\nspike_times = [\n  [  10 + i/10,\n     20 + i/10,\n     30 + i/10,\n     40 + i/10,\n     50 + i/10,\n     60 + i/10,\n     70 + i/10,\n     80 + i/10,\n     90 + i/10] for i in range(100)\n]\n\npop = net.create(ann.SpikeSourceArray(spike_times=spike_times))\n\nm = net.monitor(pop, 'spike')\n\nnet.compile(silent=True)\n\nnet.simulate(100.)\n\ndata = m.get('spike')\nt, n = m.raster_plot(data)\n\nplt.figure()\nplt.plot(t, n, '.')\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\n\n\n\n\n\nThe spike_times argument must be a list of lists containing the spike times in ms. Its length defines the number of neurons in the population. It is not possible to define a geometry. If one neuron should not spike at all, just provide an empty list. The different neurons can have a different number of spikes.\nIf you want to repeat the same stimulation, you can reset the SpikeSourceArray, what will set its internal time back to 0.0:\nnet.simulate(100.)\n\npop.reset()\n\nnet.simulate(100.)\nThe spikes times can be changed after compilation, bit it must have the same number of neurons:\npop.spike_times = new_spike_times_array\nAn example is provided in examples/pyNN/IF_curr_alpha.py.\n\n\n\n\n\n\nWarning\n\n\n\nSpikeSourceArray is not available with the CUDA backend.\n\n\n\n\nPoisson population\nThe PoissonPopulation class allows to create a population of identical spiking neurons, whose spiking patterns vary according to a Poisson distribution:\n\nnet = ann.Network(dt=0.1)\n\npop = net.create(ann.PoissonPopulation(100, rates=30.))\n\nm = net.monitor(pop, 'spike')\n\nnet.compile(silent=True)\n\nnet.simulate(100.)\n\ndata = m.get('spike')\nt, n = m.raster_plot(data)\n\nplt.figure()\nplt.plot(t, n, '.')\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\n\n\n\n\n\nIn this example, each of the 100 neurons fires randomly, with a mean firing rate of 30 Hz.\nIt is also possible to specify the mean firing rate individually for each neuron:\npop = net.create(ann.PoissonPopulation(100, rates=np.linspace(0.0, 200.0, 100)))\n\n\n\n\n\n\n\n\n\nThe rates attribute can be modified at any time during the simulation, as long as it has the same size as the population.\nAnother possibility is to define a rule for the evolution of the mean firing rate in the population:\npop = ann.create(\n    ann.PoissonPopulation(\n        geometry=100,\n        parameters = dict(\n            amp = 100.0,\n            frequency = 50.0\n        ),\n        rates=\"amp * (1.0 + sin(2*pi * frequency * t/1000.0) )/2.0\"\n    )\n)\n\n\n\n\n\n\n\n\n\nThe rule can only depend on the time t: the corresponding mean firing rate is the same for all neurons in the population.\nFinally, the rates argument can be replaced by a target, so it can be computed by another rate-coded population:\nrates = 10. * np.ones((2, 100))\nrates[0, :50] = 100.\nrates[1, 50:] = 100.\ninp = net.create(ann.TimedArray(rates=rates, schedule=50.))\n\npop = net.create(ann.PoissonPopulation(100, target=\"exc\"))\n\nproj = net.connect(inp, pop, 'exc')\nproj.one_to_one(1.0)\n\n\n\n\n\n\n\n\n\nIn the code above, we define a TimedArray for 100 neurons, so that half of the neurons fire at 100 Hz, while the others fire at 10 Hz. Every 50 ms, the two halves are swapped.\nWe just need to create a projection with the target \"exc\" between the TimedArray and the PoissonPopulation (with a one-to-one pattern and weights 1.0 to preserve scaling), and the Poisson population will reflect the firing rates defined by the TimedArray.\n\n\nHomogeneous correlated inputs\nHomogeneousCorrelatedSpikeTrains defines spiking neurons following a homogeneous distribution with correlated spike trains.\nThe method describing the generation of homogeneous correlated spike trains is described in:\n\nBrette, R. (2009). Generation of correlated spike trains. http://audition.ens.fr/brette/papers/Brette2008NC.html\n\nThe implementation is based on the one provided by Brian.\nTo generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation:\n\n    \\dfrac{dx}{dt} = \\dfrac{\\mu - x}{\\tau} + \\sigma \\, \\dfrac{\\xi }{\\sqrt{\\tau}}\n\nwhere \\xi is a random variable. Basically, x will randomly vary around $§ over time, with an amplitude determined by \\sigma and a speed determined by \\tau.\nThis doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process.\nTo avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates, the desired correlation strength corr and the time constant tau. See Brette’s paper for details.\nIn short, you should only define the parameters rates, corr and tau, and let the class compute mu and sigma for you. Changing rates, corr or tau after initialization automatically recomputes mu and sigma.\nExample:\n\nnet = ann.Network()\n\npop_poisson = net.create(ann.PoissonPopulation(100, rates=10.))\n\npop_corr    = net.create(ann.HomogeneousCorrelatedSpikeTrains(100, rates=10., corr=0.3, tau=10.))\n\nm = net.monitor(pop_poisson, 'spike')\nn = net.monitor(pop_corr, 'spike')\n\nnet.compile(silent=True)\n\nnet.simulate(2000.)\n\npop_poisson.rates=50.\npop_corr.rates=50.\n\nnet.simulate(2000.)",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Setting inputs"
    ]
  },
  {
    "objectID": "manual/RateNeuron.html",
    "href": "manual/RateNeuron.html",
    "title": "Rate-coded neurons",
    "section": "",
    "text": "Let’s consider first a simple rate-coded neuron of the leaky-integrator type, which simply integrates the weighted sum of its excitatory inputs:\n\n\\tau \\frac{d v(t)}{dt} = ( B - v(t) ) + \\sum_{i}^{\\text{exc}} w_i \\, r_{i}(t)\n\nr(t) = ( v(t) )^+\nwhere v(t) represents the membrane potential of the neuron, \\tau the time constant of the neuron, B its baseline firing rate, r(t) its instantaneous firing rate, i an index over all excitatory synapses of this neuron, w_i the efficiency of the synapse with the pre-synaptic neuron of firing rate r_{i}.\nIt can be implemented in the ANNarchy framework with:\nimport ANNarchy as ann\n\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.2\n    ),\n    equations = [\n        'tau * dv/dt + v = baseline + sum(exc)',\n        'r = pos(v)',\n    ]\n)\nThe only required variable is r, which represents the instantaneous firing rate and will be used to propagate activity in the network. All other parameters and variables are freely decided by the user.\n\n\n\nCustom functions can also be defined when creating the Neuron type and used inside the equations field:\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = dict(  \n        tau = 10.0,\n        baseline = -0.2\n    ),\n    equations = [\n        'tau * dv/dt + v = baseline + sum(exc)',\n        'r = pos(v)',\n    ],\n    functions = \"\"\"\n        sigmoid(x) = 1.0 / (1.0 + exp(-x))\n    \"\"\"\n)\nMake sure that the name of the function does not overlap with existing mathematical functions (cos, exp), existing variables (tau, r) or built-in functions (pos, t, dt).\n\n\n\nThe ODE can depend on other parameters of the neuron (e.g. r depends on v), but not on unknown names. ANNarchy already defines the following variables and parameters for a neuron:\n\nvariable t: time in milliseconds elapsed since the creation of the network.\nparameter dt: the discretization step, the default value is 1 ms.\n\n\n\n\nThe sum(target) term gives a direct access to the weighted sum of all inputs to the neuron having the corresponding target (see Projections to see how the target is defined). These inputs are organized in a data structure called Dendrite.\nIt is possible to modify how weighted sums are computed when creating a rate-coded synapse.\n\n\n\n\n\n\nNote\n\n\n\nThe connection type, e.g. exc or inh, needs to match with the names used as a target parameter when creating a Projection. If such a projection does not exist when the network is compiled, the weighted sum will be set to 0.0 for all neurons.\n\n\nUsing only sum() in the equations sums over all defined targets. For example, if two projections with targets \"exc\" and \"inh\" reach a neuron, sum() is equivalent to sum(exc) + sum(inh). Inhibitory weights must then be defined as negative.\n\n\n\nOne has the possibility to use global operations on the population inside the neuron definition, such as the maximal activity in the population. One only needs to use one of the following operations:\n\nmin(v) for the minimum: \\min_i v_i,\nmax(v) for the maximum: \\max_i v_i,\nmean(v) for the mean: \\frac{1}{N} \\sum_i v_i,\nnorm1(v) for the L1-norm: \\frac{1}{N} \\sum_i |v_i|,\nnorm2(v) for the L2-norm: \\frac{1}{N} \\sum_i v_i^2\n\nExample where neurons react to their inputs only where they exceed the mean over the population:\nWTANeuron = ann.Neuron(\n    parameters = dict(tau = 10.0),\n    equations = [\n        'input = sum(exc)',\n        'tau * dr/dt + r = pos(input - mean(input))',\n    ]\n)  \n\n\n\n\n\n\nNote\n\n\n\nThe global operations are computed using values at the previous time step (like weighted sums), not in the step currently evaluated. There is therefore implicitely a delay of dt, but it cannot be changed.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded neurons"
    ]
  },
  {
    "objectID": "manual/RateNeuron.html#defining-parameters-and-variables",
    "href": "manual/RateNeuron.html#defining-parameters-and-variables",
    "title": "Rate-coded neurons",
    "section": "",
    "text": "Let’s consider first a simple rate-coded neuron of the leaky-integrator type, which simply integrates the weighted sum of its excitatory inputs:\n\n\\tau \\frac{d v(t)}{dt} = ( B - v(t) ) + \\sum_{i}^{\\text{exc}} w_i \\, r_{i}(t)\n\nr(t) = ( v(t) )^+\nwhere v(t) represents the membrane potential of the neuron, \\tau the time constant of the neuron, B its baseline firing rate, r(t) its instantaneous firing rate, i an index over all excitatory synapses of this neuron, w_i the efficiency of the synapse with the pre-synaptic neuron of firing rate r_{i}.\nIt can be implemented in the ANNarchy framework with:\nimport ANNarchy as ann\n\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0,\n        baseline = -0.2\n    ),\n    equations = [\n        'tau * dv/dt + v = baseline + sum(exc)',\n        'r = pos(v)',\n    ]\n)\nThe only required variable is r, which represents the instantaneous firing rate and will be used to propagate activity in the network. All other parameters and variables are freely decided by the user.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded neurons"
    ]
  },
  {
    "objectID": "manual/RateNeuron.html#custom-functions",
    "href": "manual/RateNeuron.html#custom-functions",
    "title": "Rate-coded neurons",
    "section": "",
    "text": "Custom functions can also be defined when creating the Neuron type and used inside the equations field:\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = dict(  \n        tau = 10.0,\n        baseline = -0.2\n    ),\n    equations = [\n        'tau * dv/dt + v = baseline + sum(exc)',\n        'r = pos(v)',\n    ],\n    functions = \"\"\"\n        sigmoid(x) = 1.0 / (1.0 + exp(-x))\n    \"\"\"\n)\nMake sure that the name of the function does not overlap with existing mathematical functions (cos, exp), existing variables (tau, r) or built-in functions (pos, t, dt).",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded neurons"
    ]
  },
  {
    "objectID": "manual/RateNeuron.html#predefined-attributes",
    "href": "manual/RateNeuron.html#predefined-attributes",
    "title": "Rate-coded neurons",
    "section": "",
    "text": "The ODE can depend on other parameters of the neuron (e.g. r depends on v), but not on unknown names. ANNarchy already defines the following variables and parameters for a neuron:\n\nvariable t: time in milliseconds elapsed since the creation of the network.\nparameter dt: the discretization step, the default value is 1 ms.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded neurons"
    ]
  },
  {
    "objectID": "manual/RateNeuron.html#weighted-sum-of-inputs",
    "href": "manual/RateNeuron.html#weighted-sum-of-inputs",
    "title": "Rate-coded neurons",
    "section": "",
    "text": "The sum(target) term gives a direct access to the weighted sum of all inputs to the neuron having the corresponding target (see Projections to see how the target is defined). These inputs are organized in a data structure called Dendrite.\nIt is possible to modify how weighted sums are computed when creating a rate-coded synapse.\n\n\n\n\n\n\nNote\n\n\n\nThe connection type, e.g. exc or inh, needs to match with the names used as a target parameter when creating a Projection. If such a projection does not exist when the network is compiled, the weighted sum will be set to 0.0 for all neurons.\n\n\nUsing only sum() in the equations sums over all defined targets. For example, if two projections with targets \"exc\" and \"inh\" reach a neuron, sum() is equivalent to sum(exc) + sum(inh). Inhibitory weights must then be defined as negative.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded neurons"
    ]
  },
  {
    "objectID": "manual/RateNeuron.html#global-operations",
    "href": "manual/RateNeuron.html#global-operations",
    "title": "Rate-coded neurons",
    "section": "",
    "text": "One has the possibility to use global operations on the population inside the neuron definition, such as the maximal activity in the population. One only needs to use one of the following operations:\n\nmin(v) for the minimum: \\min_i v_i,\nmax(v) for the maximum: \\max_i v_i,\nmean(v) for the mean: \\frac{1}{N} \\sum_i v_i,\nnorm1(v) for the L1-norm: \\frac{1}{N} \\sum_i |v_i|,\nnorm2(v) for the L2-norm: \\frac{1}{N} \\sum_i v_i^2\n\nExample where neurons react to their inputs only where they exceed the mean over the population:\nWTANeuron = ann.Neuron(\n    parameters = dict(tau = 10.0),\n    equations = [\n        'input = sum(exc)',\n        'tau * dr/dt + r = pos(input - mean(input))',\n    ]\n)  \n\n\n\n\n\n\nNote\n\n\n\nThe global operations are computed using values at the previous time step (like weighted sums), not in the step currently evaluated. There is therefore implicitely a delay of dt, but it cannot be changed.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Rate-coded neurons"
    ]
  },
  {
    "objectID": "manual/StructuralPlasticity.html",
    "href": "manual/StructuralPlasticity.html",
    "title": "Structural plasticity",
    "section": "",
    "text": "ANNarchy supports the dynamic addition/suppression of synapses during the simulation (i.e. after compilation).\n\n\n\n\n\n\nWarning\n\n\n\nStructural plasticity is not available with the CUDA backend and will likely never be…\n\n\nThere are two possibilities to dynamically create or delete synapses:\n\nExternally, using methods at the dendrite level from Python.\nInternally, by defining conditions for creating/pruning in the synapse description.\n\n\n\nTwo methods of the Dendrite class are available for creating/deleting synapses:\n\ncreate_synapse()\nprune_synapse()\n\nBecause structural plasticity adds some complexity to the generated code, it has to be enabled before compilation by setting the structural_plasticity flag to True in the call to config():\nnet = ann.Network()\nnet.config(structural_plasticity=True)\nnet.compile()\nIf the flag is not set, the following methods will do nothing.\n\n\nLet’s suppose that we want to add regularly new synapses between strongly active but not yet connected neurons with a low probability. One could for example define a neuron type with an additional variable averaging the firing rate over a long period of time.\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = dict(  \n        tau = 10.0,\n        baseline = -0.2,\n        tau_mean = 100000.0,\n    ),\n    equations = [\n        'tau * dv/dt + v = baseline + sum(exc)',\n        'r = pos(v)',\n        'tau_mean * dmean_r/dt =  (r - mean_r)',\n    ]\n)\nTwo populations are created and connected using a sparse connectivity:\nnet = ann.Network()\nnet.config(structural_plasticity=True)\n\npop1 = net.create(1000, LeakyIntegratorNeuron)\npop2 = net.create(1000, LeakyIntegratorNeuron)\n\nproj = net.connect(pop1, pop2, 'exc', Oja)\nproj.fixed_probability(weights = 1.0, probability=0.1)\n\nnet.compile()\nAfter an initial period of simulation, one could add new synapses between strongly active pair of neurons:\n# For all post-synaptic neurons\nfor post in range(pop2.size):\n    # For all pre-synaptic neurons\n    for pre in range(pop1.size):\n        # If the neurons are not connected yet\n        if not pre in proj[post].ranks:\n            # If they are both sufficientely active\n            if pop1[pre].mean_r * pop2[post].mean_r &gt; 0.7:\n                # Add a synapse with weight 1.0 and the default delay\n                proj[post].create_synapse(rank=pre, w=1.0)   \ncreate_synapse() only allows to specify the value of the weight and the delay. Other synaptic variables will take the value they would have had before compile(). If another value is desired, it should be explicitely set afterwards.\n\n\n\nRemoving useless synapses (pruning) is also possible. Let’s consider a synapse type whose \"age\" is incremented as long as both pre- and post-synaptic neurons are inactive at the same time:\nAgingSynapse = ann.Synapse(\n    equations = ann.Variable(\"\"\"\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n              else :\n                age + 1 : init = 0, int\n    \"\"\", init=0, type=int)\n)\nOne could periodically track the too \"old\" synapses and remove them:\n# Threshold on the age:\nT = 100000\n# For all post-synaptic neurons receiving synapses\nfor post in proj.post_ranks:\n    # For all existing synapses\n    for pre in proj[post].ranks:\n        # If the synapse is too old\n        if proj[post][pre].age &gt; T :\n            # Remove it\n            proj[post].prune_synapse(rank=pre)\nThis form of structural plasticity is rather slow because:\n\nThe for loops are in Python, not C++. Implementing this structural plasticity in Cython should already help.\nThe memory allocated for the synapses of a projection may have to be displaced at another location. This can lead to massive transfer of data, slowing the simulation down.\n\nIt is of course the user’s responsability to balance synapse creation/destruction, otherwise projections could become either empty or fully connected on the long-term.\n\n\n\n\nConditions for creating or deleting synapses can also be specified in the synapse description, through the creating or pruning arguments. These arguments accept string descriptions of the boolean conditions at which a synapse should be created/deleted, using the same notation as other arguments.\n\n\nThe creation of a synapse can be described using a boolean expression inside the ann.Creating() object:\nCreatingSynapse = ann.Synapse(\n    creating = ann.Creating(\"pre.mean_r * post.mean_r &gt; 0.7\", proba = 0.5, w = 1.0),\n)\nThe condition can make use of any pre- or post-synaptic variable, but NOT synaptic variables, as they obviously do not exist yet. Global parameters of the projection can nevertheless be used.\nSeveral flags can be passed to the expression:\n\nproba specifies the probability according to which a synapse will be created, if the condition is met. The default is 1.0 (i.e. a synapse will be created whenever the condition is fulfilled).\nw specifies the value for the weight which will be created (default: 0.0).\nd specifies the delay (default: the same as all other synapses if the delay is constant in the projection, dt otherwise).\n\nNote that the new value for the delay can not exceed the maximal delay in the projection, nor be different from the others if they were all equal. Other synaptic variables will take the default value after creation.\nSynapse creation is not automatically enabled at the start of the simulation: the Projection method start_creating() must be called:\nproj.start_creating(period=100.0)\nThis method accepts a period parameter specifying how often the conditions for creating synapses will be checked (in ms). By default they would be checked at each time step (dt), what would be too costly.\nSimilarly, the stop_creating() method can be called to stop the creation conditions from being checked.\n\n\n\nSynaptic pruning also rely on a boolean expression and the ann.Pruning() object:\nPruningSynapse = ann.Synapse(\n    parameters = dict(T = ann.Parameter(10000, 'global', int),\n    equations = ann.Variable(\"\"\"\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n              else :\n                age + 1 : init = 0, int\n    \"\"\", init=0, type=int)\n    pruning = ann.Pruning(\"age &gt; T\", proba = 0.5),\n)\nA synapse type can combine creating and pruning arguments. The pruning argument can rely on synaptic variables (here age), as the synapse already exists. The proba flag can be passed to specify the probability at which the synapse will be deleted if the condition is met.\nPruning has to be started/stopped with the start_pruning() and stop_pruning() methods. start_pruning() accepts a period argument.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Structural plasticity"
    ]
  },
  {
    "objectID": "manual/StructuralPlasticity.html#dendrite-level",
    "href": "manual/StructuralPlasticity.html#dendrite-level",
    "title": "Structural plasticity",
    "section": "",
    "text": "Two methods of the Dendrite class are available for creating/deleting synapses:\n\ncreate_synapse()\nprune_synapse()\n\nBecause structural plasticity adds some complexity to the generated code, it has to be enabled before compilation by setting the structural_plasticity flag to True in the call to config():\nnet = ann.Network()\nnet.config(structural_plasticity=True)\nnet.compile()\nIf the flag is not set, the following methods will do nothing.\n\n\nLet’s suppose that we want to add regularly new synapses between strongly active but not yet connected neurons with a low probability. One could for example define a neuron type with an additional variable averaging the firing rate over a long period of time.\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = dict(  \n        tau = 10.0,\n        baseline = -0.2,\n        tau_mean = 100000.0,\n    ),\n    equations = [\n        'tau * dv/dt + v = baseline + sum(exc)',\n        'r = pos(v)',\n        'tau_mean * dmean_r/dt =  (r - mean_r)',\n    ]\n)\nTwo populations are created and connected using a sparse connectivity:\nnet = ann.Network()\nnet.config(structural_plasticity=True)\n\npop1 = net.create(1000, LeakyIntegratorNeuron)\npop2 = net.create(1000, LeakyIntegratorNeuron)\n\nproj = net.connect(pop1, pop2, 'exc', Oja)\nproj.fixed_probability(weights = 1.0, probability=0.1)\n\nnet.compile()\nAfter an initial period of simulation, one could add new synapses between strongly active pair of neurons:\n# For all post-synaptic neurons\nfor post in range(pop2.size):\n    # For all pre-synaptic neurons\n    for pre in range(pop1.size):\n        # If the neurons are not connected yet\n        if not pre in proj[post].ranks:\n            # If they are both sufficientely active\n            if pop1[pre].mean_r * pop2[post].mean_r &gt; 0.7:\n                # Add a synapse with weight 1.0 and the default delay\n                proj[post].create_synapse(rank=pre, w=1.0)   \ncreate_synapse() only allows to specify the value of the weight and the delay. Other synaptic variables will take the value they would have had before compile(). If another value is desired, it should be explicitely set afterwards.\n\n\n\nRemoving useless synapses (pruning) is also possible. Let’s consider a synapse type whose \"age\" is incremented as long as both pre- and post-synaptic neurons are inactive at the same time:\nAgingSynapse = ann.Synapse(\n    equations = ann.Variable(\"\"\"\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n              else :\n                age + 1 : init = 0, int\n    \"\"\", init=0, type=int)\n)\nOne could periodically track the too \"old\" synapses and remove them:\n# Threshold on the age:\nT = 100000\n# For all post-synaptic neurons receiving synapses\nfor post in proj.post_ranks:\n    # For all existing synapses\n    for pre in proj[post].ranks:\n        # If the synapse is too old\n        if proj[post][pre].age &gt; T :\n            # Remove it\n            proj[post].prune_synapse(rank=pre)\nThis form of structural plasticity is rather slow because:\n\nThe for loops are in Python, not C++. Implementing this structural plasticity in Cython should already help.\nThe memory allocated for the synapses of a projection may have to be displaced at another location. This can lead to massive transfer of data, slowing the simulation down.\n\nIt is of course the user’s responsability to balance synapse creation/destruction, otherwise projections could become either empty or fully connected on the long-term.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Structural plasticity"
    ]
  },
  {
    "objectID": "manual/StructuralPlasticity.html#synapse-level",
    "href": "manual/StructuralPlasticity.html#synapse-level",
    "title": "Structural plasticity",
    "section": "",
    "text": "Conditions for creating or deleting synapses can also be specified in the synapse description, through the creating or pruning arguments. These arguments accept string descriptions of the boolean conditions at which a synapse should be created/deleted, using the same notation as other arguments.\n\n\nThe creation of a synapse can be described using a boolean expression inside the ann.Creating() object:\nCreatingSynapse = ann.Synapse(\n    creating = ann.Creating(\"pre.mean_r * post.mean_r &gt; 0.7\", proba = 0.5, w = 1.0),\n)\nThe condition can make use of any pre- or post-synaptic variable, but NOT synaptic variables, as they obviously do not exist yet. Global parameters of the projection can nevertheless be used.\nSeveral flags can be passed to the expression:\n\nproba specifies the probability according to which a synapse will be created, if the condition is met. The default is 1.0 (i.e. a synapse will be created whenever the condition is fulfilled).\nw specifies the value for the weight which will be created (default: 0.0).\nd specifies the delay (default: the same as all other synapses if the delay is constant in the projection, dt otherwise).\n\nNote that the new value for the delay can not exceed the maximal delay in the projection, nor be different from the others if they were all equal. Other synaptic variables will take the default value after creation.\nSynapse creation is not automatically enabled at the start of the simulation: the Projection method start_creating() must be called:\nproj.start_creating(period=100.0)\nThis method accepts a period parameter specifying how often the conditions for creating synapses will be checked (in ms). By default they would be checked at each time step (dt), what would be too costly.\nSimilarly, the stop_creating() method can be called to stop the creation conditions from being checked.\n\n\n\nSynaptic pruning also rely on a boolean expression and the ann.Pruning() object:\nPruningSynapse = ann.Synapse(\n    parameters = dict(T = ann.Parameter(10000, 'global', int),\n    equations = ann.Variable(\"\"\"\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n              else :\n                age + 1 : init = 0, int\n    \"\"\", init=0, type=int)\n    pruning = ann.Pruning(\"age &gt; T\", proba = 0.5),\n)\nA synapse type can combine creating and pruning arguments. The pruning argument can rely on synaptic variables (here age), as the synapse already exists. The proba flag can be passed to specify the probability at which the synapse will be deleted if the condition is met.\nPruning has to be started/stopped with the start_pruning() and stop_pruning() methods. start_pruning() accepts a period argument.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Structural plasticity"
    ]
  },
  {
    "objectID": "manual/Hybrid.html",
    "href": "manual/Hybrid.html",
    "title": "Hybrid networks",
    "section": "",
    "text": "ANNarchy has the possibility to simulate either rate-coded or spiking networks. It is therefore possible to define hybrid networks mixing rate-coded and spiking populations.\nA typical application would be to define a rate-coded network to process visual inputs, which is used to feed a spiking network for action selection. A dummy example is provided in examples/hybrid.\n\n\nConverting a rate-coded population to a spiking network is straightforward. The PoissonPopulation (see its API) defines a population of spiking neurons emitting spikes following a Poisson distribution:\npop = net.create(ann.PoissonPopulation(1000, rates=50.))\nIn this case, the 1000 neurons emit spikes at a rate of 50 Hz (the rate of individual neurons can be later modified). It is possible to use a weighted sum of rate-coded synapses in order to determine the firing rate of each Poisson neuron. It requires to connect a rate-coded population to the PoissonPopulation with a given target:\npop1 = net.create(4, Neuron(parameters=\"r=0.0\"))\n\npop2 = net.create(ann.PoissonPopulation(1000, target='exc'))\n\nproj = net.connect(pop1, pop2, 'exc')\nproj.fixed_number_pre(weights=10.0, number=1)\nIn this example, each of the 4 pre-synaptic neurons “controls” the firing rate of one fourth (on average) of the post-synaptic ones. If target is used in the Poisson population, rates will be ignored.\nThe weights determine the scaling of the transmission: a presynaptic rate r of 1.0 generates a firing rate of w Hz in the post-synaptic neurons. Here setting pop1.r = 1.0 will make the post-synaptic neurons fire at 10 Hz.\n\n\n\nDecoding a spiking population is a harder process, because of the stochastic nature of spike trains. One can take advantage of the fact here that a rate-coded neuron usually represents an ensemble of spiking neurons, so the average firing rate in that ensemble can be more precisely decoded.\nIn order to do so, one needs to connect the spiking population to a rate-coded one with a many-to-one pattern using a DecodingProjection. A DecodingProjection heritates all methods of Projection (including the connection methods) but performs the necessary conversion from spike trains to a instantaneous rate. It can be passed as the first unnamed argument to Network.connect():\npop1 = net.create(ann.PoissonPopulation(1000, rates=50.0))\npop2 = net.create(1, Neuron(equations=\"r=sum(exc)\"))\n\nproj = net.connect(ann.DecodingProjection(pop1, pop2, 'exc', window=10.0))\nproj.all_to_all(weights=1.0)\nIn this example, the spiking population fires at 50 Hz. The single rate-coded neuron decoding that population will count how many spikes arrived in the last T milliseconds and divide it by the total number of synapses in order to estimate the population firing rate in pop1.\nThis would be accessed in sum(exc) (or whatever target is used in the projection). Because of its simple definition, it will therefore have its rate r at 50.0 (with some variance due to the stochastic nature of spike trains).\nThe window argument defines the duration in milliseconds of the sliding temporal window used to estimate the firing rate. By default, it is equal to dt, which means spikes are counted in a very narrow period of time, what could lead to very big variations of the decoded firing rate. If the window is too big, it would introduce a noticeable lag for the decoded firing rate if the input varies too quickly. window = 10.0 is usually a good compromise, but this depends on the input firing rate.\nThe weights of the projection define the scaling of the decoded firing rate. If one wants a firing rate of 100 Hz to be represented by r=1.0, the weights should be set to 0.01.\nNo Synapse model can be used in a DecodingProjection.\n\n\n\n\n\n\nWarning\n\n\n\nDecodingProjection is not implemented on CUDA yet.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Hybrid networks"
    ]
  },
  {
    "objectID": "manual/Hybrid.html#rate-coded-to-spike",
    "href": "manual/Hybrid.html#rate-coded-to-spike",
    "title": "Hybrid networks",
    "section": "",
    "text": "Converting a rate-coded population to a spiking network is straightforward. The PoissonPopulation (see its API) defines a population of spiking neurons emitting spikes following a Poisson distribution:\npop = net.create(ann.PoissonPopulation(1000, rates=50.))\nIn this case, the 1000 neurons emit spikes at a rate of 50 Hz (the rate of individual neurons can be later modified). It is possible to use a weighted sum of rate-coded synapses in order to determine the firing rate of each Poisson neuron. It requires to connect a rate-coded population to the PoissonPopulation with a given target:\npop1 = net.create(4, Neuron(parameters=\"r=0.0\"))\n\npop2 = net.create(ann.PoissonPopulation(1000, target='exc'))\n\nproj = net.connect(pop1, pop2, 'exc')\nproj.fixed_number_pre(weights=10.0, number=1)\nIn this example, each of the 4 pre-synaptic neurons “controls” the firing rate of one fourth (on average) of the post-synaptic ones. If target is used in the Poisson population, rates will be ignored.\nThe weights determine the scaling of the transmission: a presynaptic rate r of 1.0 generates a firing rate of w Hz in the post-synaptic neurons. Here setting pop1.r = 1.0 will make the post-synaptic neurons fire at 10 Hz.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Hybrid networks"
    ]
  },
  {
    "objectID": "manual/Hybrid.html#spike-to-rate-coded",
    "href": "manual/Hybrid.html#spike-to-rate-coded",
    "title": "Hybrid networks",
    "section": "",
    "text": "Decoding a spiking population is a harder process, because of the stochastic nature of spike trains. One can take advantage of the fact here that a rate-coded neuron usually represents an ensemble of spiking neurons, so the average firing rate in that ensemble can be more precisely decoded.\nIn order to do so, one needs to connect the spiking population to a rate-coded one with a many-to-one pattern using a DecodingProjection. A DecodingProjection heritates all methods of Projection (including the connection methods) but performs the necessary conversion from spike trains to a instantaneous rate. It can be passed as the first unnamed argument to Network.connect():\npop1 = net.create(ann.PoissonPopulation(1000, rates=50.0))\npop2 = net.create(1, Neuron(equations=\"r=sum(exc)\"))\n\nproj = net.connect(ann.DecodingProjection(pop1, pop2, 'exc', window=10.0))\nproj.all_to_all(weights=1.0)\nIn this example, the spiking population fires at 50 Hz. The single rate-coded neuron decoding that population will count how many spikes arrived in the last T milliseconds and divide it by the total number of synapses in order to estimate the population firing rate in pop1.\nThis would be accessed in sum(exc) (or whatever target is used in the projection). Because of its simple definition, it will therefore have its rate r at 50.0 (with some variance due to the stochastic nature of spike trains).\nThe window argument defines the duration in milliseconds of the sliding temporal window used to estimate the firing rate. By default, it is equal to dt, which means spikes are counted in a very narrow period of time, what could lead to very big variations of the decoded firing rate. If the window is too big, it would introduce a noticeable lag for the decoded firing rate if the input varies too quickly. window = 10.0 is usually a good compromise, but this depends on the input firing rate.\nThe weights of the projection define the scaling of the decoded firing rate. If one wants a firing rate of 100 Hz to be represented by r=1.0, the weights should be set to 0.01.\nNo Synapse model can be used in a DecodingProjection.\n\n\n\n\n\n\nWarning\n\n\n\nDecodingProjection is not implemented on CUDA yet.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Hybrid networks"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html",
    "href": "manual/ConvolutionalNetworks.html",
    "title": "Convolution and pooling",
    "section": "",
    "text": "Projections use by default a set of weights per post-synaptic neuron. Some networks, including convolutional networks, define a single operation (convolution or pooling) to be applied systematically on all pre-synaptic neurons. In such cases, it would be a waste of resources to allocate weights for each post-synaptic neuron.\nThe extension convolution (see its API) allows to implement such specific projections. It has to be imported explicitly at the beginning of the script:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.convolution import *\n\n\n\n\n\n\nWarning\n\n\n\nConvolutions are currently only implemented for rate-coded networks. The only possible backend is currently OpenMP, CUDA will be implemented later.\n\n\n\n\nThe simplest case of convolution is when the pre- and post-synaptic population have the same number of dimensions, for example:\npre = net.create(\n    geometry=(100, 100), \n    neuron=ann.Neuron(parameters=\"r = 0.0\")\n)\npost = net.create(\n    geometry=(100, 100), \n    neuron=ann.Neuron(equations=\"r = sum(exc)\")\n)\nContrary to normal projections, the geometry of the populations (number of dimensions and neurons in each dimension) has a great influence on the operation to be performed. In particular the number of dimensions will define how the convolution will be applied.\nIf for example the pre-synaptic population represents an 2D image, you may want to apply a vertical edge detector to it and get the result in the post-synaptic population. Such a filter can be defined by the following Numpy array:\nvertical_filter = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ]\n)\nWith 2 dimensions, the convolution operation (or more exactly, cross-correlation) with a 3*3 filter is defined for all neurons in the post-synaptic population by:\n\n\\text{post}[i, j] = \\sum_{c_i=-1}^1 \\sum_{c_j=-1}^1 \\text{filter}[c_i][c_j] \\cdot \\text{pre}[i + c_i, j + c_j]\n\nSuch a convolution is achieved by creating a Convolution object (inheriting from Projection, so it can be passed to Network.connect()) and using the connect_filter() method to create the connection pattern:\nproj = net.connect(Convolution(pre=pre, post=post, target='exc'))\nproj.connect_filter(weights=vertical_filter)\nEach neuron of the post-synaptic population will then receive in sum('exc') (or whatever target name is used) the convolution between the kernel and a sub-region of the pre-syanptic population. ANNarchy defines the convolution operation for populations having 1, 2, 3, or 4 dimensions.\nSeveral options can be passed to the convolve() method:\n\npadding defines the value of the pre-synaptic firing rates which will be used when the coordinates are out-of-bounds. By default zero-padding is used, but you can specify another value with this argument. You can also use the 'border' value to repeat the firing rate of the neurons on the border (for example, if the filter tries to reach a neuron of coordinates (-1, -1), the firing rate of the neuron (0, 0) will be used instead).\nsubsampling. In convolutional networks, the convolution operation is often coupled with a reduction in the number of neurons in each dimension. In the example above, the post-synaptic population could be defined with a geometry (50, 50). For each post-synaptic neuron, the coordinates of the center of the applied kernel would be automatically shifted from two pre-synaptic neurons compared to the previous one. However, if the number of neurons in one dimension of the pre-synaptic population is not exactly a multiple of the number of post-synaptic neurons in the same dimension, ANNarchy can not guess what the correct correspondance should be. In this case, you have to specify this mapping by providing to the subsampling argument a list of pre-synaptic coordinates defining the position of the center of the kernel for each post-synaptic neuron. The list is indexed by the rank of the post-synaptic neurons (use the rank_from_coordinates() method) and must have the same size as the population. Each element should be a list of coordinates in the pre-synaptic population’s geometry (with as many elements as dimensions). It is possible to provide a Numpy array instead of a list of lists.\n\nOne can access the coordinates in the pre-synaptic geometry of the center of the filter corresponding to a particular post-synaptic neuron by calling the center() method of Convolution with the rank or coordinates of the post neuron:\npre = net.create(geometry=(100, 100), neuron = ann.Neuron(parameters=\"r = 0.0\"))\npost = net.create(geometry=(50, 50), neuron = ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = net.connect(Convolution(pre=pre, post=post, target='exc'))\nproj.connect_filter(weights=vertical_filter)\n\npre_coordinates = proj.center(10, 10) # returns (20, 20)\nIn some cases, the post-synaptic population can have less dimensions than the pre-synaptic one. An example would be when the pre-synaptic population has three dimensions (e.g. (100, 100, 3)), the last representing the R, G and B components of an image. A 3D filter, with 3 components in the last dimension, would result in a (100, 100, 1) post-synaptic population (or any subsampling of it). ANNarchy accepts in this case the use of a 2D population (100, 100), but it will be checked that the number of elements in the last dimension of the filter equals the number of pre-synaptic neurons in the last dimension:\npre = net.create(geometry=(100, 100, 3), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = net.create(geometry=(100, 100), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nred_filter = np.array(\n    [\n        [\n            [2.0, -1.0, -1.0]\n        ]\n    ]\n)\n\nproj = net.connect(Convolution(pre=pre, post=post, target='exc'))\nproj.connect_filter(weights=red_filter)\n\n\n\nConvolution uses by default a regular cross-correlation, summing w * pre.r over the extent of the kernel. As for regular synapses, you can change this behavior when creating the projection:\n\nthe psp argument defines what will be summed. It is w*pre.r by default but can be changed to any combination of w and pre.r,such as w * log(1+pre.r):\nproj = net.connect(Convolution(pre=pre, post=post, target='exc', psp='w*log(1+pre.r)'))\nthe operation argument allows to change the summation operation. You can set it to ‘max’ (the maximum value of w*pre.r over the extent of the filter will be returned), ‘min’ (minimum) or ‘mean’ (same as ‘sum’, but normalized by the number of elements in the filter). The default is ‘sum’:\nproj = net.connect(Convolution(pre=pre, post=post, target='exc', operation='max'))\n\n\n\n\nIt is possible to define kernels with less dimensions than the pre-synaptic population. A 2D filter can for example be applied on each color component independently:\npre = net.create(geometry=(100, 100, 3), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = net.create(geometry=(50, 50, 3), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nvertical_filter = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ]\n)\n\nproj = net.connect(Convolution(pre=pre, post=post, target='exc'))\nproj.connect_filter(weights=vertical_filter, keep_last_dimension=True)\nThe important parameter in this case is keep_last_dimension which tells the code generator that the last dimension of the input should not be used for convolution. The important constraint is that the post-synaptic population must have the same number of neurons in the last dimension than the pre-synaptic one (no subsampling is possible by definition).\n\n\n\nConvolutional networks often use banks of filters to perform different operations (such as edge detection with various orientations). It is possible to specify this mode of functioning by using the connect_filters() method:\npre = net.create(geometry=(100, 100), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = net.create(geometry=(50, 50, 4), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nbank_filters = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ],\n    [\n        [-1.0, 0.0, 1.0],\n        [-1.0, 0.0, 1.0],\n        [-1.0, 0.0, 1.0]\n    ],\n    [\n        [-1.0, -1.0, -1.0],\n        [ 0.0,  0.0,  0.0],\n        [ 1.0,  1.0,  1.0]\n    ],\n    [\n        [ 1.0,  1.0,  1.0],\n        [ 0.0,  0.0,  0.0],\n        [-1.0, -1.0, -1.0]\n    ]\n)\n\nproj = net.connectConvolution(pre=pre, post=post, target='exc')\nproj.connect_filters(weights=bank_filters)\nHere the filter has three dimensions. The first one must correspond to each filter. The last dimension of the post-synaptic population must correspond to the total number of filters. It cannot be combined with keep_last_dimension.\n\n\n\n\n\n\nNote\n\n\n\nCurrent limitation: Each filter must have the same size, it is not possible yet to convolve over multiple scales.\n\n\n\n\n\nAnother form of atypical projection for a neural network is the pooling operation. In max-pooling, each post-synaptic neuron is associated to a region of the pre-synaptic population and responds like the maximum firing rate in this region. This is already possible by defining the operation argument of the synapse type, but it would use instantiated synapses, what would be a waste of memory.\nThe Pooling class allows to define such an operation without defining any synapse:\npre = net.create(geometry=(100, 100), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = net.create(geometry=(50, 50), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = net.connect(Pooling(pre=pre, post=post, target='exc', operation='max'))\nproj.pooling()\nThe pooling region of a post-synaptic region is automatically determined by comparing the dimensions of the two populations: here each post-synaptic neuron will cover an area of 2*2 neurons.\nIf the number of dimensions do not match, you have to specify the extent argument to pooling(). For example, you can pool completely over one dimension of the pre-synaptic population:\npre = net.connect(geometry=(100, 100, 10), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = net.connect(geometry=(50, 50), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = net.connect(Pooling(pre=pre, post=post, target='exc', operation='max'))\nproj.pooling(extent=(2, 2, 10))\n\n\n\nA different possibility to share weights is between two projections. If your network is composed of populations of the same size, and the projection patterns are identical, it could save some memory to “share” the weights of one projection with another, so they are created only once.\nTo this end, you can use the Copy class and pass it an existing projection:\npop1 = net.connect(geometry=(30, 30), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\npop2 = net.connect(geometry=(20, 20), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\npop3 = net.connect(geometry=(20, 20), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\n\nproj1 = net.connect(pop1, pop2, 'exc')\nproj1.gaussian(amp = 1.0, sigma=0.3, delays=2.0)\n\nproj2 = net.connect(Copy(pop1, pop3, 'exc'))\nproj2.copy(proj1)\nThis only works when the pre- and post-populations of each projection have the same geometry, but they can be different, of course. If the original projection is learnable, the copied projection will see the changes. However, it is not possible for the shared projection to learn on its own. Copy only accepts psp and operation as parameters, which can be different from the original projection.\nIt is only possible to copy regular projections, not other shared projections. The transmission delays will be identical between the two projections.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html#simple-convolutions",
    "href": "manual/ConvolutionalNetworks.html#simple-convolutions",
    "title": "Convolution and pooling",
    "section": "",
    "text": "The simplest case of convolution is when the pre- and post-synaptic population have the same number of dimensions, for example:\npre = net.create(\n    geometry=(100, 100), \n    neuron=ann.Neuron(parameters=\"r = 0.0\")\n)\npost = net.create(\n    geometry=(100, 100), \n    neuron=ann.Neuron(equations=\"r = sum(exc)\")\n)\nContrary to normal projections, the geometry of the populations (number of dimensions and neurons in each dimension) has a great influence on the operation to be performed. In particular the number of dimensions will define how the convolution will be applied.\nIf for example the pre-synaptic population represents an 2D image, you may want to apply a vertical edge detector to it and get the result in the post-synaptic population. Such a filter can be defined by the following Numpy array:\nvertical_filter = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ]\n)\nWith 2 dimensions, the convolution operation (or more exactly, cross-correlation) with a 3*3 filter is defined for all neurons in the post-synaptic population by:\n\n\\text{post}[i, j] = \\sum_{c_i=-1}^1 \\sum_{c_j=-1}^1 \\text{filter}[c_i][c_j] \\cdot \\text{pre}[i + c_i, j + c_j]\n\nSuch a convolution is achieved by creating a Convolution object (inheriting from Projection, so it can be passed to Network.connect()) and using the connect_filter() method to create the connection pattern:\nproj = net.connect(Convolution(pre=pre, post=post, target='exc'))\nproj.connect_filter(weights=vertical_filter)\nEach neuron of the post-synaptic population will then receive in sum('exc') (or whatever target name is used) the convolution between the kernel and a sub-region of the pre-syanptic population. ANNarchy defines the convolution operation for populations having 1, 2, 3, or 4 dimensions.\nSeveral options can be passed to the convolve() method:\n\npadding defines the value of the pre-synaptic firing rates which will be used when the coordinates are out-of-bounds. By default zero-padding is used, but you can specify another value with this argument. You can also use the 'border' value to repeat the firing rate of the neurons on the border (for example, if the filter tries to reach a neuron of coordinates (-1, -1), the firing rate of the neuron (0, 0) will be used instead).\nsubsampling. In convolutional networks, the convolution operation is often coupled with a reduction in the number of neurons in each dimension. In the example above, the post-synaptic population could be defined with a geometry (50, 50). For each post-synaptic neuron, the coordinates of the center of the applied kernel would be automatically shifted from two pre-synaptic neurons compared to the previous one. However, if the number of neurons in one dimension of the pre-synaptic population is not exactly a multiple of the number of post-synaptic neurons in the same dimension, ANNarchy can not guess what the correct correspondance should be. In this case, you have to specify this mapping by providing to the subsampling argument a list of pre-synaptic coordinates defining the position of the center of the kernel for each post-synaptic neuron. The list is indexed by the rank of the post-synaptic neurons (use the rank_from_coordinates() method) and must have the same size as the population. Each element should be a list of coordinates in the pre-synaptic population’s geometry (with as many elements as dimensions). It is possible to provide a Numpy array instead of a list of lists.\n\nOne can access the coordinates in the pre-synaptic geometry of the center of the filter corresponding to a particular post-synaptic neuron by calling the center() method of Convolution with the rank or coordinates of the post neuron:\npre = net.create(geometry=(100, 100), neuron = ann.Neuron(parameters=\"r = 0.0\"))\npost = net.create(geometry=(50, 50), neuron = ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = net.connect(Convolution(pre=pre, post=post, target='exc'))\nproj.connect_filter(weights=vertical_filter)\n\npre_coordinates = proj.center(10, 10) # returns (20, 20)\nIn some cases, the post-synaptic population can have less dimensions than the pre-synaptic one. An example would be when the pre-synaptic population has three dimensions (e.g. (100, 100, 3)), the last representing the R, G and B components of an image. A 3D filter, with 3 components in the last dimension, would result in a (100, 100, 1) post-synaptic population (or any subsampling of it). ANNarchy accepts in this case the use of a 2D population (100, 100), but it will be checked that the number of elements in the last dimension of the filter equals the number of pre-synaptic neurons in the last dimension:\npre = net.create(geometry=(100, 100, 3), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = net.create(geometry=(100, 100), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nred_filter = np.array(\n    [\n        [\n            [2.0, -1.0, -1.0]\n        ]\n    ]\n)\n\nproj = net.connect(Convolution(pre=pre, post=post, target='exc'))\nproj.connect_filter(weights=red_filter)",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html#non-linear-convolutions",
    "href": "manual/ConvolutionalNetworks.html#non-linear-convolutions",
    "title": "Convolution and pooling",
    "section": "",
    "text": "Convolution uses by default a regular cross-correlation, summing w * pre.r over the extent of the kernel. As for regular synapses, you can change this behavior when creating the projection:\n\nthe psp argument defines what will be summed. It is w*pre.r by default but can be changed to any combination of w and pre.r,such as w * log(1+pre.r):\nproj = net.connect(Convolution(pre=pre, post=post, target='exc', psp='w*log(1+pre.r)'))\nthe operation argument allows to change the summation operation. You can set it to ‘max’ (the maximum value of w*pre.r over the extent of the filter will be returned), ‘min’ (minimum) or ‘mean’ (same as ‘sum’, but normalized by the number of elements in the filter). The default is ‘sum’:\nproj = net.connect(Convolution(pre=pre, post=post, target='exc', operation='max'))",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html#layer-wise-convolutions",
    "href": "manual/ConvolutionalNetworks.html#layer-wise-convolutions",
    "title": "Convolution and pooling",
    "section": "",
    "text": "It is possible to define kernels with less dimensions than the pre-synaptic population. A 2D filter can for example be applied on each color component independently:\npre = net.create(geometry=(100, 100, 3), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = net.create(geometry=(50, 50, 3), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nvertical_filter = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ]\n)\n\nproj = net.connect(Convolution(pre=pre, post=post, target='exc'))\nproj.connect_filter(weights=vertical_filter, keep_last_dimension=True)\nThe important parameter in this case is keep_last_dimension which tells the code generator that the last dimension of the input should not be used for convolution. The important constraint is that the post-synaptic population must have the same number of neurons in the last dimension than the pre-synaptic one (no subsampling is possible by definition).",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html#bank-of-filters",
    "href": "manual/ConvolutionalNetworks.html#bank-of-filters",
    "title": "Convolution and pooling",
    "section": "",
    "text": "Convolutional networks often use banks of filters to perform different operations (such as edge detection with various orientations). It is possible to specify this mode of functioning by using the connect_filters() method:\npre = net.create(geometry=(100, 100), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = net.create(geometry=(50, 50, 4), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nbank_filters = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ],\n    [\n        [-1.0, 0.0, 1.0],\n        [-1.0, 0.0, 1.0],\n        [-1.0, 0.0, 1.0]\n    ],\n    [\n        [-1.0, -1.0, -1.0],\n        [ 0.0,  0.0,  0.0],\n        [ 1.0,  1.0,  1.0]\n    ],\n    [\n        [ 1.0,  1.0,  1.0],\n        [ 0.0,  0.0,  0.0],\n        [-1.0, -1.0, -1.0]\n    ]\n)\n\nproj = net.connectConvolution(pre=pre, post=post, target='exc')\nproj.connect_filters(weights=bank_filters)\nHere the filter has three dimensions. The first one must correspond to each filter. The last dimension of the post-synaptic population must correspond to the total number of filters. It cannot be combined with keep_last_dimension.\n\n\n\n\n\n\nNote\n\n\n\nCurrent limitation: Each filter must have the same size, it is not possible yet to convolve over multiple scales.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html#pooling",
    "href": "manual/ConvolutionalNetworks.html#pooling",
    "title": "Convolution and pooling",
    "section": "",
    "text": "Another form of atypical projection for a neural network is the pooling operation. In max-pooling, each post-synaptic neuron is associated to a region of the pre-synaptic population and responds like the maximum firing rate in this region. This is already possible by defining the operation argument of the synapse type, but it would use instantiated synapses, what would be a waste of memory.\nThe Pooling class allows to define such an operation without defining any synapse:\npre = net.create(geometry=(100, 100), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = net.create(geometry=(50, 50), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = net.connect(Pooling(pre=pre, post=post, target='exc', operation='max'))\nproj.pooling()\nThe pooling region of a post-synaptic region is automatically determined by comparing the dimensions of the two populations: here each post-synaptic neuron will cover an area of 2*2 neurons.\nIf the number of dimensions do not match, you have to specify the extent argument to pooling(). For example, you can pool completely over one dimension of the pre-synaptic population:\npre = net.connect(geometry=(100, 100, 10), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = net.connect(geometry=(50, 50), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = net.connect(Pooling(pre=pre, post=post, target='exc', operation='max'))\nproj.pooling(extent=(2, 2, 10))",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html#sharing-weights-with-another-projection",
    "href": "manual/ConvolutionalNetworks.html#sharing-weights-with-another-projection",
    "title": "Convolution and pooling",
    "section": "",
    "text": "A different possibility to share weights is between two projections. If your network is composed of populations of the same size, and the projection patterns are identical, it could save some memory to “share” the weights of one projection with another, so they are created only once.\nTo this end, you can use the Copy class and pass it an existing projection:\npop1 = net.connect(geometry=(30, 30), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\npop2 = net.connect(geometry=(20, 20), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\npop3 = net.connect(geometry=(20, 20), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\n\nproj1 = net.connect(pop1, pop2, 'exc')\nproj1.gaussian(amp = 1.0, sigma=0.3, delays=2.0)\n\nproj2 = net.connect(Copy(pop1, pop3, 'exc'))\nproj2.copy(proj1)\nThis only works when the pre- and post-populations of each projection have the same geometry, but they can be different, of course. If the original projection is learnable, the copied projection will see the changes. However, it is not possible for the shared projection to learn on its own. Copy only accepts psp and operation as parameters, which can be different from the original projection.\nIt is only possible to copy regular projections, not other shared projections. The transmission delays will be identical between the two projections.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/index.html",
    "href": "manual/index.html",
    "title": "Overview",
    "section": "",
    "text": "A neural network in ANNarchy is a collection of interconnected Populations. Each population comprises a set of similar artificial Neurons, whose mean-firing rate or spiking behavior is governed by one or many ordinary differential equations (ODE). These ODEs are dependent on the activity of other neurons through Synapses. The connection pattern between two populations is called a Projection.\nThe efficiency of the connections received by a neuron is stored in a connectivity matrix, depending on the type that was assigned to them: excitatory, inhibitory, modulatory… This typed organization of afferent connections also allows to easily apply them different learning rules (Hebbian, three-factor, Oja, BCM, STDP…).\n\n\n\n\n\nTo define a neural network and simulate its behavior, you need to define the following information:\n\nThe number of populations, their geometry (number of neurons, optionally the spatial structure - 1D/2D/3D).\nFor each population, the type of neuron composing it, with all the necessary equations.\nFor each projection between two populations, the connection pattern, the initial synaptic weights, and optionally the delays in synaptic transmission.\nFor plastic synapses, the ODEs describing the evolution of synaptic weights during the simulation (learning).\nThe interaction of the network with its environment (I/O relationships, rewarded tasks, fitting procedure…)\n\nANNarchy provides a convenient way to define this information in a single Python script or notebook. In this manual, we will focus on simple networks composed of a few interconnected populations, but more complex architectures are of course possible (see the examples in the provided Notebooks).\n\n\n\nIn a script file (e.g. MyNetwork.py) or a Jupyter notebook, you first need to import the ANNarchy package:\nimport ANNarchy as ann\nWe recommend using the ann alias to avoid polluting the global namespace. All the necessary objects and class definitions are then imported under the alias.\nThe second step is to define the neurons and synapses needed by your network. To keep things simple, we will define a simple neuron model, whose firing rate is determined by the leaky-integration of excitatory inputs:\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0, \n        baseline = -0.2\n    ),\n    equations = [\n        'tau * dv/dt  + v = baseline + sum(exc)',\n        'r = pos(v)'\n    ]\n)\nv is an internal variable integrating with the time constant tau the weighted sum of excitatory inputs sum(exc) to this neuron plus its baseline activity. r is the instantaneous firing rate of the neuron, defined as the positive part of mp. More details on the difference between parameters and variables, as well as details on the mathematical parser are to be found in the sections Parser and Rate-coded neurons.\nThe synapse type between the two populations will implement a simple Oja learning rule, which is a Hebbian learning rule with an additional regularization term:\nOja = ann.Synapse(\n    parameters = dict(\n        tau = 5000.0, \n        alpha = 8.0\n    ),\n    equations = [\n        'tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w',\n    ]\n)\nw represents the synaptic efficiency (or weight value). Its evolution over time depends on a time constant tau, the regularization parameter alpha, the pre-synaptic firing rate pre.r and the post-synaptic firing rate post.r. See Rate-coded synapses for more details.\nOnce these objects are defined, the network can be created. The main structure to create the model is an instance of the Network class, which can be simply created with:\nnet = ann.Network()\nUsing the network, the populations can be created (section Populations) by calling Network.create(). We create here two populations pop1 and pop2 containing 100 neurons each and using the LeakyIntegratorNeuron neural model:\npop1 = net.create(geometry=100, neuron=LeakyIntegratorNeuron)\n\npop2 = net.create(geometry=100, neuron=LeakyIntegratorNeuron)\nUsing Network.connect, we now define an excitatory projection between the neurons of pop1 and pop2, with a target exc and a all_to_all connection pattern (section Projections).\nproj = net.connect(pre=pop1, post=pop2, target='exc', synapse=Oja)\nThe synaptic weights are initialized randomly between 0.0 and 1.0:\nproj.all_to_all(weights = ann.Uniform(0.0, 1.0))\nNow that the structure of the network is defined, it can be analyzed to generate optimized C++ code in a subfolder and create the underlying data:\nnet.compile()\nThe network is now ready to be simulated for the desired duration:\nnet.simulate(1000.0) # simulate for 1 second\nIt remains to set inputs, record variables and analyze the results, but the structure of the network is already there. The rest of the manual is there to get into more details on these objects.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Overview"
    ]
  },
  {
    "objectID": "manual/index.html#structure-of-a-neural-network",
    "href": "manual/index.html#structure-of-a-neural-network",
    "title": "Overview",
    "section": "",
    "text": "A neural network in ANNarchy is a collection of interconnected Populations. Each population comprises a set of similar artificial Neurons, whose mean-firing rate or spiking behavior is governed by one or many ordinary differential equations (ODE). These ODEs are dependent on the activity of other neurons through Synapses. The connection pattern between two populations is called a Projection.\nThe efficiency of the connections received by a neuron is stored in a connectivity matrix, depending on the type that was assigned to them: excitatory, inhibitory, modulatory… This typed organization of afferent connections also allows to easily apply them different learning rules (Hebbian, three-factor, Oja, BCM, STDP…).\n\n\n\n\n\nTo define a neural network and simulate its behavior, you need to define the following information:\n\nThe number of populations, their geometry (number of neurons, optionally the spatial structure - 1D/2D/3D).\nFor each population, the type of neuron composing it, with all the necessary equations.\nFor each projection between two populations, the connection pattern, the initial synaptic weights, and optionally the delays in synaptic transmission.\nFor plastic synapses, the ODEs describing the evolution of synaptic weights during the simulation (learning).\nThe interaction of the network with its environment (I/O relationships, rewarded tasks, fitting procedure…)\n\nANNarchy provides a convenient way to define this information in a single Python script or notebook. In this manual, we will focus on simple networks composed of a few interconnected populations, but more complex architectures are of course possible (see the examples in the provided Notebooks).",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Overview"
    ]
  },
  {
    "objectID": "manual/index.html#definition-of-a-model",
    "href": "manual/index.html#definition-of-a-model",
    "title": "Overview",
    "section": "",
    "text": "In a script file (e.g. MyNetwork.py) or a Jupyter notebook, you first need to import the ANNarchy package:\nimport ANNarchy as ann\nWe recommend using the ann alias to avoid polluting the global namespace. All the necessary objects and class definitions are then imported under the alias.\nThe second step is to define the neurons and synapses needed by your network. To keep things simple, we will define a simple neuron model, whose firing rate is determined by the leaky-integration of excitatory inputs:\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = dict(\n        tau = 10.0, \n        baseline = -0.2\n    ),\n    equations = [\n        'tau * dv/dt  + v = baseline + sum(exc)',\n        'r = pos(v)'\n    ]\n)\nv is an internal variable integrating with the time constant tau the weighted sum of excitatory inputs sum(exc) to this neuron plus its baseline activity. r is the instantaneous firing rate of the neuron, defined as the positive part of mp. More details on the difference between parameters and variables, as well as details on the mathematical parser are to be found in the sections Parser and Rate-coded neurons.\nThe synapse type between the two populations will implement a simple Oja learning rule, which is a Hebbian learning rule with an additional regularization term:\nOja = ann.Synapse(\n    parameters = dict(\n        tau = 5000.0, \n        alpha = 8.0\n    ),\n    equations = [\n        'tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w',\n    ]\n)\nw represents the synaptic efficiency (or weight value). Its evolution over time depends on a time constant tau, the regularization parameter alpha, the pre-synaptic firing rate pre.r and the post-synaptic firing rate post.r. See Rate-coded synapses for more details.\nOnce these objects are defined, the network can be created. The main structure to create the model is an instance of the Network class, which can be simply created with:\nnet = ann.Network()\nUsing the network, the populations can be created (section Populations) by calling Network.create(). We create here two populations pop1 and pop2 containing 100 neurons each and using the LeakyIntegratorNeuron neural model:\npop1 = net.create(geometry=100, neuron=LeakyIntegratorNeuron)\n\npop2 = net.create(geometry=100, neuron=LeakyIntegratorNeuron)\nUsing Network.connect, we now define an excitatory projection between the neurons of pop1 and pop2, with a target exc and a all_to_all connection pattern (section Projections).\nproj = net.connect(pre=pop1, post=pop2, target='exc', synapse=Oja)\nThe synaptic weights are initialized randomly between 0.0 and 1.0:\nproj.all_to_all(weights = ann.Uniform(0.0, 1.0))\nNow that the structure of the network is defined, it can be analyzed to generate optimized C++ code in a subfolder and create the underlying data:\nnet.compile()\nThe network is now ready to be simulated for the desired duration:\nnet.simulate(1000.0) # simulate for 1 second\nIt remains to set inputs, record variables and analyze the results, but the structure of the network is already there. The rest of the manual is there to get into more details on these objects.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Overview"
    ]
  },
  {
    "objectID": "manual/NumericalMethods.html",
    "href": "manual/NumericalMethods.html",
    "title": "Numerical methods",
    "section": "",
    "text": "First-order ordinary differential equations (ODE) can be solved using different numerical methods. The method can be declared globally in the Network.config() method and used in all ODEs of the network:\nimport ANNarchy as ann\n\nnet = ann.Network()\n\nnet.config(method='exponential')\nor specified explicitely for each ODE by specifying the attribute:\nequations = [\n    ann.Variable('tau * dv/dt  + v =  A', init = 0.0, method='exponential'),\n]\nIf nothing is specified, the explicit Euler method is used.\nDifferent numerical methods are available:\n\nExplicit Euler 'explicit'\nImplicit Euler 'implicit'\nExponential Euler 'exponential'\nMidpoint 'midpoint'\nFourth-order Runge-Kutta 'rk4'\nEvent-driven 'event-driven'\n\nEach method has advantages/drawbacks in term of numerical error, stability and computational cost.\nTo describe these methods, we will take the example of a system of two linear first-order ODEs:\n\\frac{dx(t)}{dt} = f(x(t), y(t)) = a_x \\cdot x(t) + b_x \\cdot y(t) + c_x\n\\frac{dy(t)}{dt} = g(x(t), y(t)) = a_y \\cdot x(t) + b_y \\cdot y(t) + c_y\nThe objective of a numerical method is to approximate the value of x and y at time t+h based on its value at time t, where h is the discretization time step (noted dt in ANNarchy):\nx(t + h) = F(x(t), y(t))\ny(t + h) = G(x(t), y(t))\nAt each step of the simulation, the new values for the variables are computed using this update rule and will be used for the following step.\nThe derivative of each variable is usually approximated by:\n\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h}\n\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h}\nThe different numerical methods mostly differ in the time at which the functions f and g are evaluated.\n\n\nThe explicit (forward) Euler method computes the next value for the variables by estimating their derivative at time t:\n\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t), y(t))\n\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t), y(t))\nso the solution is straightforward to obtain:\nx(t+h) =  x(t) + h \\cdot  f(x(t), y(t))\ny(t+h) = y(t) + h \\cdot g(x(t), y(t))\n\n\n\nThe implicit (backward) Euler method computes the next value for the variables by estimating their derivative at time t + h:\n\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t+h), y(t+h))\n\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t+h), y(t+h))\nThis leads to a system of equations which must be solved in order to find the update rule. With the linear equations defined above, we need to solve:\n\\frac{x(t+h) - x(t)}{h} = a_x \\cdot x(t + h) + b_x \\cdot y(t + h) + c_x\n\\frac{y(t+h) - y(t)}{h} = a_y \\cdot x(t + h) + b_y \\cdot y(t + h) + c_y\nwhat gives something like:\nx(t+h) =  x(t) - h \\cdot \\frac{ \\left(a_{x} x(t) + b_{x} y(t) + c_{x} + h \\left(- a_{x} b_{y} x(t) + a_{y} b_{x} x(t) + b_{x} c_{y} - b_{y} c_{x}\\right)\\right)}{h^{2} \\left(- a_{x} b_{y} + a_{y} b_{x}\\right) + h \\left(a_{x} + b_{y}\\right) - 1}\ny(t+h) = y(t) -h \\cdot  \\frac{ a_{y} \\left(c_{x} h + x(t)\\right) + y(t) \\left(- a_{y} b_{x} h^{2} + \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)\\right) + \\left(a_{x} h - 1\\right) \\left(c_{y} h + y(t)\\right)}{a_{y} b_{x} h^{2} - \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)}\nANNarchy relies on Sympy to solve and simplify this system of equations and generate the update rule.\n\n\n\n\n\n\nNote\n\n\n\nThis method is obviously much more computationally expensive than the explicit Euler method, although more stable. The midpoint method is a better trade-off between complexity and stability than the implicit Euler method.\n\n\n\n\n\nThe exponential Euler method is particularly stable for single first-order linear equations, of the type:\n\\tau(t) \\cdot \\frac{dx(t)}{dt}  + x(t) =  A(t)\nThe update rule is then given by:\nx(t+h) = x(t) + (1 - \\exp(- \\frac{h}{\\tau(t)}) ) \\cdot (A(t) - x(t))\nThe difference with the explicit Euler method is the step size, which is an exponential function of the ratio \\frac{\\tau}{h}. The accurary of the exponential Euler method on linear first-order ODEs is close to perfect, compared to the other Euler methods. As it is an explicit method, systems of equations are solved very easily with the same rule.\nWhen the exponential method is used, ANNarchy first tries to reduce the ODE to its canonical form above (with the time constant being possibly dependent on time or inputs) and then generates the update rule accordingly.\nFor example, the description:\ntau * dv/dt = (E - v) + g_exc * (Ee - v) + g_inh * (v - Ei)\nwould be first transformed in:\n(1 + g_exc - g_inh) * dv/dt + v = (E + g_exc * Ee - g_inh * Ei) / (1 + g_exc - g_inh)\nbefore being transformed into an update rule, with \\tau(t) = 1 + g_\\text{exc} - g_\\text{inh}:\nv(t+h) = v(t) + (1 - \\exp(- \\frac{h}{1 + g_\\text{exc} - g_\\text{inh}}) ) \\cdot (\\frac{E + g_\\text{exc} \\cdot E_e - g_\\text{inh} \\cdot E_i}{1 + g_\\text{exc} - g_\\text{inh}} - v(t))\nThe exponential method can only be applied to first-order linear ODEs. Any other form of ODE will be rejected by the parser.\nImportant note: The step size 1 - \\exp(- \\frac{h}{\\tau(t)}) is computationally expensive because of the exponential function. If the time constant \\tau is a global parameter of the population or projection, ANNarchy can pre-compute the step size outside of the for loop over all neurons/synapses, which leads to huge increases in performance. The exponential method should therefore be reserved to first-order linear ODEs with the same time constant for all neurons/synapses:\nneuron = nn.Neuron(\n    parameters = dict(tau = 10.),\n    equations = ann.Variable('tau * dr/dt + r = sum(exc)', min=0.0, method='exponential'),\n)\n\n\n\nThe midpoint method is a Runge-Kutta method of order 2. It estimates the derivative in the middle of the interval t + \\frac{h}{2}.\nk_x = f(x(t), y(t))\nk_y = g(x(t), y(t))\nx(t+h) =  x(t) + h \\cdot  f(x(t) + k_x \\cdot \\frac{h}{2}, y(t) +  k_y \\cdot \\frac{h}{2})\ny(t+h) = y(t) + h \\cdot g(x(t) + k_x \\cdot \\frac{h}{2}, y(t) +  k_y \\cdot \\frac{h}{2})\n\n\n\nThe fourth-order Runge-Kutta method estimates the derivative at four different points and combines them:\n k_1 =  f(x(t)) \n k_2 =  f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_1) \n k_3 =  f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_2) \n k_4 =  f(x(t + h) + h \\cdot k_3) \n x(t+h) = x(t) + \\frac{h}{6} \\cdot (k_1 + 2 \\cdot k_2 + 2 \\cdot k_3 + k_4 ) \n\n\n\nEvent-driven integration is only available for spiking synapses with variables following linear first-order dynamics. Let’s consider the following STDP synapse (see Spiking Synapse for explanations):\nSTDP = ann.Synapse(\n    parameters = dict(\n        tau_pre = 10.0,\n        tau_post = 10.0,\n    ),\n    equations = [\n        ann.Variable('tau_pre * dApre/dt = - Apre', method='event-driven'),\n        ann.Variable('tau_post * dApost/dt = - Apost', method='event-driven'),\n    ],\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre\n        w = clip(w + Apost, 0.0 , 1.0)\n    \"\"\",\n    post_spike = \"\"\"\n        Apost += cApost\n        w = clip(w + Apre, 0.0 , 1.0)\n    \"\"\"\n)\nThe value of Apost and Apre is only needed when a pre- or post-synaptic spike occurs at the synapse, so there is no need to integrate the corresponding equations between two such events. First-order linear ODEs have the nice property that their analytical solution is easy to obtain. Let’s consider an equation of the form:\n\\tau  \\frac{dv}{dt} = E - v\nIf v has the value V_0 at time t, its value at time t + \\Delta t is given by:\nv(t + \\Delta t) = V_0 \\cdot \\exp(-\\frac{\\Delta t}{\\tau})\n\n\n\n\n\n\nNote\n\n\n\nIf the synapse defines a psp argument (synaptic transmission is continuous), or if another continuous variable depends on the value of an event-driven one, it is not possible to use event-driven integration.\n\n\n\n\n\n\nThe values of variables are stored in a single array in order to save some memory. Special care therefore has to be taken on whether the update of a variable depends on the value of another variable at the previous time step or in the same step.\n\n\nSystems of ODEs are integrated concurrently, which means that the following system:\ntau * dv/dt = I - v - u\ntau * du/dt = v - u\nwould be numerized using the explicit Euler method as:\nv[t+1] = v[t] + dt*(I - v[t] - u[t])/tau\nu[t+1] = u[t] + dt*(v[t] - u[t])/tau\nAs we use a single array, the generated code is similar to:\nnew_v = v + dt*(I - v - u)/tau\nnew_u = u + dt*(v - u)/tau\n\nv = new_v\nu = new_u\nThis way, we ensure that the interdependent ODEs use the correct value for the other variables.\n\n\n\nWhen assignments (=, +=...) are used in an equations field, the order of valuation is different:\n\nAssigments occurring before or after a system of ODEs are updated sequentially.\nSystems of ODEs are updated concurrently.\n\nLet’s consider the following dummy equations:\n# Process the inputs\nExc = some_function(sum(exc))\nInh = another_function(sum(inh))\nI = Exc - Inh\n\n# ODE for the membrane potential, with a recovery variable\ntau * dv/dt = I - v - u\ntau * du/dt = v - u\n\n# Firing rate is the positive part of v\nr = pos(v)\nHere, Exc and Inh represent the inputs to the neuron at the current time t. The new values should be immediately available for updating I, whose value should similarly be immediately used in the ODE of v. Similarly, the value of r should be the positive part of the value of v that was just calculated, not at the previous time step. Doing otherwise would introduce a lag in the neuron: changes in sum(exc) at t would be reflected in Exc at t+1, in I at t+2, in v at t+3 and finally in r at t+4. This is generally unwanted.\nThe generated code is therefore equivalent to:\n# Process the inputs\nExc = some_function(sum(exc))\nInh = another_function(sum(inh))\nI = Exc - Inh\n\n# ODE for the membrane potential, with a recovery variable\nnew_v = v + dt*(I - v - u)/tau\nnew_u = u + dt*(v - u)/tau\nv = new_v\nu = new_u\n\n# Firing rate is the positive part of v\nr = pos(v)\nOne can even define multiple groups of assignments and systems of ODEs: systems of ODEs separated by at least one assignment will be evaluated sequentially (but concurrently inside each system). For example, in:\ntau*du/dt = v - u\nI = g_exc - g_inh\ntau*dk/dt = v - k\ntau*dv/dt = I - v - u + k\nu and k are updated using the previous value of v, while v uses the new values of both I and u, but the previous one of k.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Numerical methods"
    ]
  },
  {
    "objectID": "manual/NumericalMethods.html#numerical-methods-1",
    "href": "manual/NumericalMethods.html#numerical-methods-1",
    "title": "Numerical methods",
    "section": "",
    "text": "First-order ordinary differential equations (ODE) can be solved using different numerical methods. The method can be declared globally in the Network.config() method and used in all ODEs of the network:\nimport ANNarchy as ann\n\nnet = ann.Network()\n\nnet.config(method='exponential')\nor specified explicitely for each ODE by specifying the attribute:\nequations = [\n    ann.Variable('tau * dv/dt  + v =  A', init = 0.0, method='exponential'),\n]\nIf nothing is specified, the explicit Euler method is used.\nDifferent numerical methods are available:\n\nExplicit Euler 'explicit'\nImplicit Euler 'implicit'\nExponential Euler 'exponential'\nMidpoint 'midpoint'\nFourth-order Runge-Kutta 'rk4'\nEvent-driven 'event-driven'\n\nEach method has advantages/drawbacks in term of numerical error, stability and computational cost.\nTo describe these methods, we will take the example of a system of two linear first-order ODEs:\n\\frac{dx(t)}{dt} = f(x(t), y(t)) = a_x \\cdot x(t) + b_x \\cdot y(t) + c_x\n\\frac{dy(t)}{dt} = g(x(t), y(t)) = a_y \\cdot x(t) + b_y \\cdot y(t) + c_y\nThe objective of a numerical method is to approximate the value of x and y at time t+h based on its value at time t, where h is the discretization time step (noted dt in ANNarchy):\nx(t + h) = F(x(t), y(t))\ny(t + h) = G(x(t), y(t))\nAt each step of the simulation, the new values for the variables are computed using this update rule and will be used for the following step.\nThe derivative of each variable is usually approximated by:\n\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h}\n\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h}\nThe different numerical methods mostly differ in the time at which the functions f and g are evaluated.\n\n\nThe explicit (forward) Euler method computes the next value for the variables by estimating their derivative at time t:\n\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t), y(t))\n\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t), y(t))\nso the solution is straightforward to obtain:\nx(t+h) =  x(t) + h \\cdot  f(x(t), y(t))\ny(t+h) = y(t) + h \\cdot g(x(t), y(t))\n\n\n\nThe implicit (backward) Euler method computes the next value for the variables by estimating their derivative at time t + h:\n\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t+h), y(t+h))\n\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t+h), y(t+h))\nThis leads to a system of equations which must be solved in order to find the update rule. With the linear equations defined above, we need to solve:\n\\frac{x(t+h) - x(t)}{h} = a_x \\cdot x(t + h) + b_x \\cdot y(t + h) + c_x\n\\frac{y(t+h) - y(t)}{h} = a_y \\cdot x(t + h) + b_y \\cdot y(t + h) + c_y\nwhat gives something like:\nx(t+h) =  x(t) - h \\cdot \\frac{ \\left(a_{x} x(t) + b_{x} y(t) + c_{x} + h \\left(- a_{x} b_{y} x(t) + a_{y} b_{x} x(t) + b_{x} c_{y} - b_{y} c_{x}\\right)\\right)}{h^{2} \\left(- a_{x} b_{y} + a_{y} b_{x}\\right) + h \\left(a_{x} + b_{y}\\right) - 1}\ny(t+h) = y(t) -h \\cdot  \\frac{ a_{y} \\left(c_{x} h + x(t)\\right) + y(t) \\left(- a_{y} b_{x} h^{2} + \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)\\right) + \\left(a_{x} h - 1\\right) \\left(c_{y} h + y(t)\\right)}{a_{y} b_{x} h^{2} - \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)}\nANNarchy relies on Sympy to solve and simplify this system of equations and generate the update rule.\n\n\n\n\n\n\nNote\n\n\n\nThis method is obviously much more computationally expensive than the explicit Euler method, although more stable. The midpoint method is a better trade-off between complexity and stability than the implicit Euler method.\n\n\n\n\n\nThe exponential Euler method is particularly stable for single first-order linear equations, of the type:\n\\tau(t) \\cdot \\frac{dx(t)}{dt}  + x(t) =  A(t)\nThe update rule is then given by:\nx(t+h) = x(t) + (1 - \\exp(- \\frac{h}{\\tau(t)}) ) \\cdot (A(t) - x(t))\nThe difference with the explicit Euler method is the step size, which is an exponential function of the ratio \\frac{\\tau}{h}. The accurary of the exponential Euler method on linear first-order ODEs is close to perfect, compared to the other Euler methods. As it is an explicit method, systems of equations are solved very easily with the same rule.\nWhen the exponential method is used, ANNarchy first tries to reduce the ODE to its canonical form above (with the time constant being possibly dependent on time or inputs) and then generates the update rule accordingly.\nFor example, the description:\ntau * dv/dt = (E - v) + g_exc * (Ee - v) + g_inh * (v - Ei)\nwould be first transformed in:\n(1 + g_exc - g_inh) * dv/dt + v = (E + g_exc * Ee - g_inh * Ei) / (1 + g_exc - g_inh)\nbefore being transformed into an update rule, with \\tau(t) = 1 + g_\\text{exc} - g_\\text{inh}:\nv(t+h) = v(t) + (1 - \\exp(- \\frac{h}{1 + g_\\text{exc} - g_\\text{inh}}) ) \\cdot (\\frac{E + g_\\text{exc} \\cdot E_e - g_\\text{inh} \\cdot E_i}{1 + g_\\text{exc} - g_\\text{inh}} - v(t))\nThe exponential method can only be applied to first-order linear ODEs. Any other form of ODE will be rejected by the parser.\nImportant note: The step size 1 - \\exp(- \\frac{h}{\\tau(t)}) is computationally expensive because of the exponential function. If the time constant \\tau is a global parameter of the population or projection, ANNarchy can pre-compute the step size outside of the for loop over all neurons/synapses, which leads to huge increases in performance. The exponential method should therefore be reserved to first-order linear ODEs with the same time constant for all neurons/synapses:\nneuron = nn.Neuron(\n    parameters = dict(tau = 10.),\n    equations = ann.Variable('tau * dr/dt + r = sum(exc)', min=0.0, method='exponential'),\n)\n\n\n\nThe midpoint method is a Runge-Kutta method of order 2. It estimates the derivative in the middle of the interval t + \\frac{h}{2}.\nk_x = f(x(t), y(t))\nk_y = g(x(t), y(t))\nx(t+h) =  x(t) + h \\cdot  f(x(t) + k_x \\cdot \\frac{h}{2}, y(t) +  k_y \\cdot \\frac{h}{2})\ny(t+h) = y(t) + h \\cdot g(x(t) + k_x \\cdot \\frac{h}{2}, y(t) +  k_y \\cdot \\frac{h}{2})\n\n\n\nThe fourth-order Runge-Kutta method estimates the derivative at four different points and combines them:\n k_1 =  f(x(t)) \n k_2 =  f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_1) \n k_3 =  f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_2) \n k_4 =  f(x(t + h) + h \\cdot k_3) \n x(t+h) = x(t) + \\frac{h}{6} \\cdot (k_1 + 2 \\cdot k_2 + 2 \\cdot k_3 + k_4 ) \n\n\n\nEvent-driven integration is only available for spiking synapses with variables following linear first-order dynamics. Let’s consider the following STDP synapse (see Spiking Synapse for explanations):\nSTDP = ann.Synapse(\n    parameters = dict(\n        tau_pre = 10.0,\n        tau_post = 10.0,\n    ),\n    equations = [\n        ann.Variable('tau_pre * dApre/dt = - Apre', method='event-driven'),\n        ann.Variable('tau_post * dApost/dt = - Apost', method='event-driven'),\n    ],\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre\n        w = clip(w + Apost, 0.0 , 1.0)\n    \"\"\",\n    post_spike = \"\"\"\n        Apost += cApost\n        w = clip(w + Apre, 0.0 , 1.0)\n    \"\"\"\n)\nThe value of Apost and Apre is only needed when a pre- or post-synaptic spike occurs at the synapse, so there is no need to integrate the corresponding equations between two such events. First-order linear ODEs have the nice property that their analytical solution is easy to obtain. Let’s consider an equation of the form:\n\\tau  \\frac{dv}{dt} = E - v\nIf v has the value V_0 at time t, its value at time t + \\Delta t is given by:\nv(t + \\Delta t) = V_0 \\cdot \\exp(-\\frac{\\Delta t}{\\tau})\n\n\n\n\n\n\nNote\n\n\n\nIf the synapse defines a psp argument (synaptic transmission is continuous), or if another continuous variable depends on the value of an event-driven one, it is not possible to use event-driven integration.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Numerical methods"
    ]
  },
  {
    "objectID": "manual/NumericalMethods.html#order-of-evaluation",
    "href": "manual/NumericalMethods.html#order-of-evaluation",
    "title": "Numerical methods",
    "section": "",
    "text": "The values of variables are stored in a single array in order to save some memory. Special care therefore has to be taken on whether the update of a variable depends on the value of another variable at the previous time step or in the same step.\n\n\nSystems of ODEs are integrated concurrently, which means that the following system:\ntau * dv/dt = I - v - u\ntau * du/dt = v - u\nwould be numerized using the explicit Euler method as:\nv[t+1] = v[t] + dt*(I - v[t] - u[t])/tau\nu[t+1] = u[t] + dt*(v[t] - u[t])/tau\nAs we use a single array, the generated code is similar to:\nnew_v = v + dt*(I - v - u)/tau\nnew_u = u + dt*(v - u)/tau\n\nv = new_v\nu = new_u\nThis way, we ensure that the interdependent ODEs use the correct value for the other variables.\n\n\n\nWhen assignments (=, +=...) are used in an equations field, the order of valuation is different:\n\nAssigments occurring before or after a system of ODEs are updated sequentially.\nSystems of ODEs are updated concurrently.\n\nLet’s consider the following dummy equations:\n# Process the inputs\nExc = some_function(sum(exc))\nInh = another_function(sum(inh))\nI = Exc - Inh\n\n# ODE for the membrane potential, with a recovery variable\ntau * dv/dt = I - v - u\ntau * du/dt = v - u\n\n# Firing rate is the positive part of v\nr = pos(v)\nHere, Exc and Inh represent the inputs to the neuron at the current time t. The new values should be immediately available for updating I, whose value should similarly be immediately used in the ODE of v. Similarly, the value of r should be the positive part of the value of v that was just calculated, not at the previous time step. Doing otherwise would introduce a lag in the neuron: changes in sum(exc) at t would be reflected in Exc at t+1, in I at t+2, in v at t+3 and finally in r at t+4. This is generally unwanted.\nThe generated code is therefore equivalent to:\n# Process the inputs\nExc = some_function(sum(exc))\nInh = another_function(sum(inh))\nI = Exc - Inh\n\n# ODE for the membrane potential, with a recovery variable\nnew_v = v + dt*(I - v - u)/tau\nnew_u = u + dt*(v - u)/tau\nv = new_v\nu = new_u\n\n# Firing rate is the positive part of v\nr = pos(v)\nOne can even define multiple groups of assignments and systems of ODEs: systems of ODEs separated by at least one assignment will be evaluated sequentially (but concurrently inside each system). For example, in:\ntau*du/dt = v - u\nI = g_exc - g_inh\ntau*dk/dt = v - k\ntau*dv/dt = I - v - u + k\nu and k are updated using the previous value of v, while v uses the new values of both I and u, but the previous one of k.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Numerical methods"
    ]
  },
  {
    "objectID": "manual/Connector.html",
    "href": "manual/Connector.html",
    "title": "Connectors",
    "section": "",
    "text": "There are basically four methods to instantiate projections:",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Connectors"
    ]
  },
  {
    "objectID": "manual/Connector.html#built-in-connectors",
    "href": "manual/Connector.html#built-in-connectors",
    "title": "Connectors",
    "section": "Built-in connectors",
    "text": "Built-in connectors\nConnectors are methods of Projection that allow to instantiate the underlying data structures like the connectivity matrix and other local attributes. However, this only happens after the network is compiled.\nConnector methods take the form:\nproj.xxx(weights, delays, *other_parameters)\nThe weights argument defines the initial value taken by the weight wof the projection. You can provide either:\n\na single value (weights=1.0) which will be the same for all synapses.\na random distribution instance (RandomDistribution) allowing to sample values for each synapse.\n\nIt is not possible to provide a list of weights, as the underling synapses are not created yet and might be random.\nThe delays argument controls the delays in synaptic transmission. By default it is 0 (or dt), meaning that transitting inormation through a synapse takes one simulation step. You canalso provide either:\n\na single value (delays=5.0), meaning that all synapses will have the same constant delay (called uniform delays).\na random distribution instance, providing a unique random delay for each synapse (called non-uniform delays).\n\n\nall_to_all (dense)\nIn a all-to-all pattern (also called dense or fully-connected), all neurons of the post-synaptic population form connections with all neurons of the pre-synaptic population.\nproj.all_to_all(weights=ann.Uniform(0.0, 0.5)) \nWhen the projection is between a projection and itself (recurrent), self-connections (a single neuron to itself) are avoided by default (the diagonal of the connectivity matrix is removed), but the parameter allow_self_connections can be set to True:\nproj.all_to_all(weights=1.0, delays=2.0, allow_self_connections=False) \nIf you provide a single initial weight value and the synapse has no plasticity (the weight w will never be modified), the projection will not store one value of w per synapse, but one per projection. This allows to save a lot of memory space for this kind of static projections. If you do not want this, you can disable it with:\nproj.all_to_all(weights=1.0, force_multiple_weights=True) \n\n\nfixed_probability (sparse)\nFor each post-synaptic neuron, there is a fixed probability that it forms a connection with a neuron of the pre-synaptic population. It is basically am all_to_all projection, except some synapses are not created, making the projection sparse:\nproj.fixed_probability(probability = 0.2, weights=1.0) \nallow_self_connections and force_multiple_weights can also be set.\n\n\none_to_one (diagonal)\nA neuron of the post-synaptic population forms a connection with only one neuron of the pre-synaptic population. The order depends on the ranks: neuron 0 is connected with neuron 0 and so on. It is advised that the pre- and post-populations have the same size/geometry, especially when using population views.\npop1 = net.create((20, 20), Neuron(parameters=\"r=0.0\"))\npop2 = net.create((10, 10), Neuron(equations=\"r=sum(exc)\"))\n\nproj = net.connect(pop1[5:15, 5:15], pop2, 'exc')\nproj.one_to_one(weights=1.0) \nSelf-connections cannot be created. force_multiple_weights can be set to True.\n\n\nfixed_number_pre\nEach neuron in the post-synaptic population receives connections from a fixed number of neurons of the pre-synaptic population chosen randomly. It may happen that two post-synaptic neurons are connected to the same pre-synaptic neuron and that some pre-synaptic neurons are connected to nothing:\nproj.fixed_number_pre(number = 20, weights=1.0) \n\n\nfixed_number_post\nEach neuron in the pre-synaptic population sends a connection to a fixed number of neurons of the post-synaptic population chosen randomly. It may happen that two pre-synaptic neurons are connected to the same post-synaptic neuron and that some post-synaptic neurons receive no connection at all:\nproj.fixed_number_post(number = 20, weights=1.0) \nThe following figure shows the fixed_number_pre (left) and fixed_number_post projections between two populations of 4 neurons, with number=2. In fixed_number_pre, each post-synaptic neuron receives exactly 2 connections, while in fixed_number_post, each pre-synaptic neuron send exactly two connections:\n\n\n\ngaussian\nA neuron of the post-synaptic population forms a connection with a limited region of the pre-synaptic population, centered around the neuron with the same normalized position. Weight values are initialized using a Gaussian function, with a maximal value amp for the neuron of same position and decreasing with distance (standard deviation sigma):\n\nw(x, y) = A \\, \\exp(-\\dfrac{1}{2}\\dfrac{(x-x_c)^2+(y-y_c)^2}{\\sigma^2})\n\nwhere (x, y) is the position of the pre-synaptic neuron (normalized to [0, 1]^d) and (x_c, y_c) is the position of the post-synaptic neuron (normalized to [0, 1]^d). A = amp, sigma = \\sigma.\nIn order to void creating useless synapses, the parameter limit can be set to restrict the creation of synapses to the cases where the value of the weight would be superior to limit*abs(amp). Default is 0.01 (1%).\nSelf-connections are avoided by default (parameter allow_self_connections).\nThe two populations must have the same number of dimensions, but the number of neurons can vary as the positions of each neuron are normalized in [0, 1]^d:\nproj.gaussian( amp=1.0, sigma=0.2, limit=0.001) \n\n\ndog\nThe same as gaussian, except weight values are computed using a Difference-of-Gaussians (DoG), usually positive in the center, negative a bit further away and small at long distances.\n\n\\begin{aligned}\nw(x, y) &= A^+ \\, \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_+^2}) \\\\\n    &-  A^- \\, \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_-^2}) \\\\\n\\end{aligned}\n\nWeights smaller than limit * abs(amp_pos - amp_neg) are not created and self-connections are avoided by default (parameter allow_self_connections):\nproj.dog(\n    amp_pos=1.0, sigma_pos=0.2, \n    amp_neg=0.3, sigma_neg=0.7, \n    limit=0.001\n) \nThe following figure shows the example of a neuron of coordinates (10, 10) in the post-synaptic population, which is connected through the gaussian (left) and dog (right) projections to a population of geometry 30*30. The X and Y axis denote the coordinates of the pre-synaptic neurons, while the Z axis is the weight value.\n\n\n\n\n\n\n\nImportant\n\n\n\nIf a single value is used for the weights argument of all_to_all, one_to_one, fixed_probability, fixed_number_pre and fixed_number_post, and the default synapse is used (no synaptic plasticity), ANNarchy will generate a single weight value for all the synapses of the projection, not one per synapse.\nThis allows to save a lot of memory and improve performance. However, if you wish to manually change the weights of some of the synapses after the creation, you need to force the creation of one value per synapse by setting force_multiple_weights=True in the call to the connector.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Connectors"
    ]
  },
  {
    "objectID": "manual/Connector.html#saved-connectivity",
    "href": "manual/Connector.html#saved-connectivity",
    "title": "Connectors",
    "section": "Saved connectivity",
    "text": "Saved connectivity\nIt is also possible to build a connection pattern using data saved during a precedent simulation. This is useful when:\n\npre-learning is done in another context;\na connector method for static synapses is particularly slow (e.g. DoG), but loading the result from a file is faster.\n\nThe connectivity of a projection can be saved (after compile()) using:\nproj.save_connectivity(filename='proj.npz')\nThe filename can used relative or absolute paths. The data is saved in a binary format:\n\nCompressed Numpy format when the filename ends with .npz.\nCompressed binary file format when the filename ends with .gz.\nBinary file format otherwise.\n\nIt can then be used to instantiate another projection:\nproj.from_file(filename='proj.npz')\nOnly the connectivity (which neurons are connected), the weights and delays are loaded. Other synaptic variables are left untouched. The pre- and post-synaptic population must have the same size during saving and loading.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Connectors"
    ]
  },
  {
    "objectID": "manual/Connector.html#from-connectivity-matrices",
    "href": "manual/Connector.html#from-connectivity-matrices",
    "title": "Connectors",
    "section": "From connectivity matrices",
    "text": "From connectivity matrices\nOne can also create connections using Numpy dense matrices or Scipy sparse matrices.\n\nfrom_matrix\nThis method accepts a Numpy array to define the weights of the projection (and optionally the delays). By default, the matrix should have the size (post.size, pre.size), so that the first index represents a post-synaptic neuron and the second the pre-synaptic neurons. If your matrix is defined in the reversed order, you can either transpose it or set the pre_post argument to True.\nThis method is useful for dense connectivity matrices (all-to-all). If you do not want to create some synapses, the weight value should be set to None.\nThe following code creates a synfire chain inside a population of 100 neurons:\nN = 100\nnet = ann.Network()\npop = net.create(N, Neuron(equations=\"r=sum(exc)\"))\nproj = net.connect(pop, pop, 'exc')\n\n# Initialize an empty connectivity matrix\nw = np.array([[None]*N]*N)\n\n# Connect each post-synaptic neuron to its predecessor\nfor i in range(N):\n    w[i, (i-1)%N] = 1.0\n\n# Create the connections\nproj.from_matrix(w)\nConnectivity matrices can not work with multi-dimensional coordinates, only ranks are used. Population views can be used in the projection, but the connection matrix must have the corresponding size:\nproj = net.connect(pop[10:20], pop[50:60], 'exc')\n\n# Create the connectivity matrix\nw = np.ones((10, 10))\n\n# Create the connections\nproj.from_matrix(w)\n\n\nfrom_sparse\nFor sparse connection matrices, the Numpy array format may have a huge memory overhead if most of its values are None. It is possible to use Scipy sparse matrices in that case. The previous synfire chain example becomes:\nfrom scipy.sparse import lil_matrix\n\nproj = net.connect(pop, pop, 'exc')\n\nw = lil_matrix((N, N))\nfor i in range(N):\n    w[i, (i+1)%N] = 1.0\n\nproj.from_sparse(w)\n\n\n\n\n\n\nNote\n\n\n\nContrary to from_matrix(), the first index of the sparse matrix represents the pre-synaptic neurons, not the post-synaptic ones. This is for compatibility with other neural simulators.\n\n\nfrom_sparse() accepts lil_matrix, csr_matrix and csc_matrix objects, although lil_matrix should be preferred for its simplicity of element access.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Connectors"
    ]
  },
  {
    "objectID": "manual/Connector.html#user-defined-patterns",
    "href": "manual/Connector.html#user-defined-patterns",
    "title": "Connectors",
    "section": "User-defined patterns",
    "text": "User-defined patterns\nThis section describes the creation of user-specific connection patterns in ANNarchy, if the available patterns are not enough. A connection pattern is simply implemented as a method returning a LILConnectivity (list-of-list) object containing all the necessary information to create the synapses.\nA connector method must take on the first position the pre-synaptic population (or a subset of it) and on the second one the post-synaptic population. Other arguments are free, but should be passed when creating the projection.\nprobabilistic_pattern(pre, post, &lt;other arguments&gt;)\nAs an example, we will recreate the fixed_probability connector method, building synapses with a given probability. For this new pattern we need a weight value (common for all synapses) and a probability value as additional arguments. We consider that no delay is introduced in the synaptic transmission.\ndef probabilistic_pattern(pre, post, weight, probability):\n\n    synapses = ann.LILConnectivity()\n\n    ... pattern code comes here ...\n\n    return synapses\n\nfixed_probability in Python\nThe connector method needs to return a LILConnectivity object storing the connectivity. For each post-synaptic neuron receiving synapses, a list of pre-synaptic ranks, weight values and delays must be added to the structure. If you use 2D or 3D populations you need to transform the coordinates into ranks with the rank_from_coordinates function.\nimport random\nimport ANNarchy as ann\n\ndef probabilistic_pattern(pre, post, weight, probability):\n    \n    # Create a LIL structure for the connectivity matrix\n    synapses = ann.LILConnectivity()\n    \n    # For all neurons in the post-synaptic population\n    for post_rank in range(post.size):\n        \n        # Decide which pre-synaptic neurons should form synapses\n        ranks = []\n        for pre_rank in range(pre.size):\n            if random.random() &lt; probability:\n                ranks.append(pre_rank)\n        \n        # Create weights and delays arrays of the same size\n        values = [weight for i in range(len(ranks)) ]\n        delays = [0 for i in range(len(ranks)) ]\n        \n        # Add this information to the LIL matrix\n        synapses.add(post_rank, ranks, values, delays)\n\n    return synapses\nThe first for - loop iterates over all post-synaptic neurons in the projection. The inner for loop decides for each of these neurons if a synapse with a pre-synaptic neuron should be created, based on the value probability provided as argument to the function.\nThe lists values and delays are then created with the same size as ranks (important!), and filled with the desired value. All this information is then fed into the LIL matrix using the add(post_rank, ranks, values, delays) method.\n\n\n\n\n\n\nNote\n\n\n\nBuilding such connectivity matrices in Python can be extremely slow, as Python is not made for tight nested loops. If the construction of your network lasts too long, you should definitely write this function in Cython.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe add() should be only called once per post-synaptic neuron! If not, ANNarchy will have to reorder its internal representations and this will be really slow.\n\n\nUsage of the pattern\nTo use the pattern within a projection you provide the pattern method to the from_function method of Projection:\nproj = net.connect(\n    pre = pop1, \n    post = pop2, \n    target = 'inh' \n)\n\nproj.from_function(\n    method=probabilistic_pattern, \n    weight=1.0, \n    probability=0.3\n)   \nmethod is the method you just wrote. Extra arguments (other than pre and post) should be passed with the same name.\n\n\nfixed_probability in Cython\nFor this example, we will create a Cython file CustomPatterns.pyx in the same directory as the script. Its content should be relatively similar to the Python version, except some type definitions:\n# distutils: language = c++\nimport random\nimport ANNarchy\ncimport ANNarchy.cython_ext.Connector as Connector\n\ndef probabilistic_pattern(pre, post, weight, probability):\n    # Typedefs\n    cdef Connector.LILConnectivity synapses\n    cdef int post_rank, pre_rank\n    cdef list ranks, values, delays\n\n    # Create a LILConnectivity structure for the connectivity matrix\n    synapses = Connector.LILConnectivity()\n    # For all neurons in the post-synaptic population\n    for post_rank in range(post.size):\n        # Decide which pre-synaptic neurons should form synapses\n        ranks = []\n        for pre_rank in range(pre.size):\n            if random.random() &lt; probability:\n                ranks.append(pre_rank)\n        # Create weights and delays arrays of the same size\n        values = [weight for i in range(len(ranks)) ]\n        delays = [0 for i in range(len(ranks)) ]\n        # Add this information to the LILConnectivity matrix\n        synapses.add(post_rank, ranks, values, delays)\n\n    return synapses\nThe only differences with the Python code are:\n\nThe module Connector where the LILConnectivity connection matrix class is defined should be cimported with:\n\ncimport ANNarchy.cython_ext.Connector as Connector\n\nData structures should be declared with cdef at the beginning of the method:\n\n# Typedefs\ncdef Connector.LILConnectivity synapses\ncdef int post_rank, pre_rank\ncdef list ranks, values, delays \nTo allow Cython to compile this file, we also need to provide with a kind of \"Makefile\" specifying that the code should be generated in C++, not C. This file should have the same name as the Cython file but end with .pyxbld, here : CustomPatterns.pyxbld.\nfrom distutils.extension import Extension\nimport ANNarchy\n\ndef make_ext(modname, pyxfilename):\n    return Extension(name=modname,\n                     sources=[pyxfilename],\n                     include_dirs = ANNarchy.include_path(),\n                     extra_compile_args=['-std=c++11'],\n                     language=\"c++\")\n\n\n\n\n\n\nNote\n\n\n\nThis .pyxbld is generic, you don’t need to modify anything, except its name.\n\n\nNow you can import the method probabilistic_pattern() into your Python code using the pyximport module of Cython and build the Projection normally:\nimport pyximport; pyximport.install()\nfrom CustomPatterns import probabilistic_pattern\n\nproj.from_function(method=probabilistic_pattern, weight=1.0, probability=0.3)\nWriting the connector in Cython can bring speedups up to 100x compared to Python if the projection has a lot of synapses.",
    "crumbs": [
      "Manual",
      "**Core objects**",
      "Connectors"
    ]
  },
  {
    "objectID": "License.html",
    "href": "License.html",
    "title": "License",
    "section": "",
    "text": "License\n                    GNU GENERAL PUBLIC LICENSE\n                       Version 2, June 1991\n\n Copyright (C) 1989, 1991 Free Software Foundation, Inc.,\n 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The licenses for most software are designed to take away your\nfreedom to share and change it.  By contrast, the GNU General Public\nLicense is intended to guarantee your freedom to share and change free\nsoftware--to make sure the software is free for all its users.  This\nGeneral Public License applies to most of the Free Software\nFoundation's software and to any other program whose authors commit to\nusing it.  (Some other Free Software Foundation software is covered by\nthe GNU Lesser General Public License instead.)  You can apply it to\nyour programs, too.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthis service if you wish), that you receive source code or can get it\nif you want it, that you can change the software or use pieces of it\nin new free programs; and that you know you can do these things.\n\n  To protect your rights, we need to make restrictions that forbid\nanyone to deny you these rights or to ask you to surrender the rights.\nThese restrictions translate to certain responsibilities for you if you\ndistribute copies of the software, or if you modify it.\n\n  For example, if you distribute copies of such a program, whether\ngratis or for a fee, you must give the recipients all the rights that\nyou have.  You must make sure that they, too, receive or can get the\nsource code.  And you must show them these terms so they know their\nrights.\n\n  We protect your rights with two steps: (1) copyright the software, and\n(2) offer you this license which gives you legal permission to copy,\ndistribute and/or modify the software.\n\n  Also, for each author's protection and ours, we want to make certain\nthat everyone understands that there is no warranty for this free\nsoftware.  If the software is modified by someone else and passed on, we\nwant its recipients to know that what they have is not the original, so\nthat any problems introduced by others will not reflect on the original\nauthors' reputations.\n\n  Finally, any free program is threatened constantly by software\npatents.  We wish to avoid the danger that redistributors of a free\nprogram will individually obtain patent licenses, in effect making the\nprogram proprietary.  To prevent this, we have made it clear that any\npatent must be licensed for everyone's free use or not licensed at all.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                    GNU GENERAL PUBLIC LICENSE\n   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n\n  0. This License applies to any program or other work which contains\na notice placed by the copyright holder saying it may be distributed\nunder the terms of this General Public License.  The \"Program\", below,\nrefers to any such program or work, and a \"work based on the Program\"\nmeans either the Program or any derivative work under copyright law:\nthat is to say, a work containing the Program or a portion of it,\neither verbatim or with modifications and/or translated into another\nlanguage.  (Hereinafter, translation is included without limitation in\nthe term \"modification\".)  Each licensee is addressed as \"you\".\n\nActivities other than copying, distribution and modification are not\ncovered by this License; they are outside its scope.  The act of\nrunning the Program is not restricted, and the output from the Program\nis covered only if its contents constitute a work based on the\nProgram (independent of having been made by running the Program).\nWhether that is true depends on what the Program does.\n\n  1. You may copy and distribute verbatim copies of the Program's\nsource code as you receive it, in any medium, provided that you\nconspicuously and appropriately publish on each copy an appropriate\ncopyright notice and disclaimer of warranty; keep intact all the\nnotices that refer to this License and to the absence of any warranty;\nand give any other recipients of the Program a copy of this License\nalong with the Program.\n\nYou may charge a fee for the physical act of transferring a copy, and\nyou may at your option offer warranty protection in exchange for a fee.\n\n  2. You may modify your copy or copies of the Program or any portion\nof it, thus forming a work based on the Program, and copy and\ndistribute such modifications or work under the terms of Section 1\nabove, provided that you also meet all of these conditions:\n\n    a) You must cause the modified files to carry prominent notices\n    stating that you changed the files and the date of any change.\n\n    b) You must cause any work that you distribute or publish, that in\n    whole or in part contains or is derived from the Program or any\n    part thereof, to be licensed as a whole at no charge to all third\n    parties under the terms of this License.\n\n    c) If the modified program normally reads commands interactively\n    when run, you must cause it, when started running for such\n    interactive use in the most ordinary way, to print or display an\n    announcement including an appropriate copyright notice and a\n    notice that there is no warranty (or else, saying that you provide\n    a warranty) and that users may redistribute the program under\n    these conditions, and telling the user how to view a copy of this\n    License.  (Exception: if the Program itself is interactive but\n    does not normally print such an announcement, your work based on\n    the Program is not required to print an announcement.)\n\nThese requirements apply to the modified work as a whole.  If\nidentifiable sections of that work are not derived from the Program,\nand can be reasonably considered independent and separate works in\nthemselves, then this License, and its terms, do not apply to those\nsections when you distribute them as separate works.  But when you\ndistribute the same sections as part of a whole which is a work based\non the Program, the distribution of the whole must be on the terms of\nthis License, whose permissions for other licensees extend to the\nentire whole, and thus to each and every part regardless of who wrote it.\n\nThus, it is not the intent of this section to claim rights or contest\nyour rights to work written entirely by you; rather, the intent is to\nexercise the right to control the distribution of derivative or\ncollective works based on the Program.\n\nIn addition, mere aggregation of another work not based on the Program\nwith the Program (or with a work based on the Program) on a volume of\na storage or distribution medium does not bring the other work under\nthe scope of this License.\n\n  3. You may copy and distribute the Program (or a work based on it,\nunder Section 2) in object code or executable form under the terms of\nSections 1 and 2 above provided that you also do one of the following:\n\n    a) Accompany it with the complete corresponding machine-readable\n    source code, which must be distributed under the terms of Sections\n    1 and 2 above on a medium customarily used for software interchange; or,\n\n    b) Accompany it with a written offer, valid for at least three\n    years, to give any third party, for a charge no more than your\n    cost of physically performing source distribution, a complete\n    machine-readable copy of the corresponding source code, to be\n    distributed under the terms of Sections 1 and 2 above on a medium\n    customarily used for software interchange; or,\n\n    c) Accompany it with the information you received as to the offer\n    to distribute corresponding source code.  (This alternative is\n    allowed only for noncommercial distribution and only if you\n    received the program in object code or executable form with such\n    an offer, in accord with Subsection b above.)\n\nThe source code for a work means the preferred form of the work for\nmaking modifications to it.  For an executable work, complete source\ncode means all the source code for all modules it contains, plus any\nassociated interface definition files, plus the scripts used to\ncontrol compilation and installation of the executable.  However, as a\nspecial exception, the source code distributed need not include\nanything that is normally distributed (in either source or binary\nform) with the major components (compiler, kernel, and so on) of the\noperating system on which the executable runs, unless that component\nitself accompanies the executable.\n\nIf distribution of executable or object code is made by offering\naccess to copy from a designated place, then offering equivalent\naccess to copy the source code from the same place counts as\ndistribution of the source code, even though third parties are not\ncompelled to copy the source along with the object code.\n\n  4. You may not copy, modify, sublicense, or distribute the Program\nexcept as expressly provided under this License.  Any attempt\notherwise to copy, modify, sublicense or distribute the Program is\nvoid, and will automatically terminate your rights under this License.\nHowever, parties who have received copies, or rights, from you under\nthis License will not have their licenses terminated so long as such\nparties remain in full compliance.\n\n  5. You are not required to accept this License, since you have not\nsigned it.  However, nothing else grants you permission to modify or\ndistribute the Program or its derivative works.  These actions are\nprohibited by law if you do not accept this License.  Therefore, by\nmodifying or distributing the Program (or any work based on the\nProgram), you indicate your acceptance of this License to do so, and\nall its terms and conditions for copying, distributing or modifying\nthe Program or works based on it.\n\n  6. Each time you redistribute the Program (or any work based on the\nProgram), the recipient automatically receives a license from the\noriginal licensor to copy, distribute or modify the Program subject to\nthese terms and conditions.  You may not impose any further\nrestrictions on the recipients' exercise of the rights granted herein.\nYou are not responsible for enforcing compliance by third parties to\nthis License.\n\n  7. If, as a consequence of a court judgment or allegation of patent\ninfringement or for any other reason (not limited to patent issues),\nconditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot\ndistribute so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you\nmay not distribute the Program at all.  For example, if a patent\nlicense would not permit royalty-free redistribution of the Program by\nall those who receive copies directly or indirectly through you, then\nthe only way you could satisfy both it and this License would be to\nrefrain entirely from distribution of the Program.\n\nIf any portion of this section is held invalid or unenforceable under\nany particular circumstance, the balance of the section is intended to\napply and the section as a whole is intended to apply in other\ncircumstances.\n\nIt is not the purpose of this section to induce you to infringe any\npatents or other property right claims or to contest validity of any\nsuch claims; this section has the sole purpose of protecting the\nintegrity of the free software distribution system, which is\nimplemented by public license practices.  Many people have made\ngenerous contributions to the wide range of software distributed\nthrough that system in reliance on consistent application of that\nsystem; it is up to the author/donor to decide if he or she is willing\nto distribute software through any other system and a licensee cannot\nimpose that choice.\n\nThis section is intended to make thoroughly clear what is believed to\nbe a consequence of the rest of this License.\n\n  8. If the distribution and/or use of the Program is restricted in\ncertain countries either by patents or by copyrighted interfaces, the\noriginal copyright holder who places the Program under this License\nmay add an explicit geographical distribution limitation excluding\nthose countries, so that distribution is permitted only in or among\ncountries not thus excluded.  In such case, this License incorporates\nthe limitation as if written in the body of this License.\n\n  9. The Free Software Foundation may publish revised and/or new versions\nof the General Public License from time to time.  Such new versions will\nbe similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\nEach version is given a distinguishing version number.  If the Program\nspecifies a version number of this License which applies to it and \"any\nlater version\", you have the option of following the terms and conditions\neither of that version or of any later version published by the Free\nSoftware Foundation.  If the Program does not specify a version number of\nthis License, you may choose any version ever published by the Free Software\nFoundation.\n\n  10. If you wish to incorporate parts of the Program into other free\nprograms whose distribution conditions are different, write to the author\nto ask for permission.  For software which is copyrighted by the Free\nSoftware Foundation, write to the Free Software Foundation; we sometimes\nmake exceptions for this.  Our decision will be guided by the two goals\nof preserving the free status of all derivatives of our free software and\nof promoting the sharing and reuse of software generally.\n\n                            NO WARRANTY\n\n  11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY\nFOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN\nOTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES\nPROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED\nOR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS\nTO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE\nPROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING,\nREPAIR OR CORRECTION.\n\n  12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR\nREDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES,\nINCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING\nOUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED\nTO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY\nYOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER\nPROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nconvey the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software; you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation; either version 2 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License along\n    with this program; if not, write to the Free Software Foundation, Inc.,\n    51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\nAlso add information on how to contact you by electronic and paper mail.\n\nIf the program is interactive, make it output a short notice like this\nwhen it starts in an interactive mode:\n\n    Gnomovision version 69, Copyright (C) year name of author\n    Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n\nThe hypothetical commands `show w' and `show c' should show the appropriate\nparts of the General Public License.  Of course, the commands you use may\nbe called something other than `show w' and `show c'; they could even be\nmouse-clicks or menu items--whatever suits your program.\n\nYou should also get your employer (if you work as a programmer) or your\nschool, if any, to sign a \"copyright disclaimer\" for the program, if\nnecessary.  Here is a sample; alter the names:\n\n  Yoyodyne, Inc., hereby disclaims all copyright interest in the program\n  `Gnomovision' (which makes passes at compilers) written by James Hacker.\n\n  &lt;signature of Ty Coon&gt;, 1 April 1989\n  Ty Coon, President of Vice\n\nThis General Public License does not permit incorporating your program into\nproprietary programs.  If your program is a subroutine library, you may\nconsider it more useful to permit linking proprietary applications with the\nlibrary.  If this is what you want to do, use the GNU Lesser General\nPublic License instead of this License.",
    "crumbs": [
      "ANNarchy",
      "License"
    ]
  },
  {
    "objectID": "tutorial/index.html",
    "href": "tutorial/index.html",
    "title": "ANNarchy - Artificial Neural Networks architect",
    "section": "",
    "text": "ANNarchy (Artificial Neural Networks architect) is a neuro-simulator for rate-coded or spiking neural networks in Python.\nThis tutorial for beginners is a condensed version of the manual and the associated Jupyter notebooks. Its intent is to quickly introduce users to ANNarchy so they can start modeling right away.\nThe slides of the tutorial are available here (or simply press 'f' in the frame below).",
    "crumbs": [
      "Tutorial"
    ]
  }
]