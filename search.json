[
  {
    "objectID": "reference/get_projection.html",
    "href": "reference/get_projection.html",
    "title": "get_projection",
    "section": "",
    "text": "core.Global.get_projection(name, net_id=0)\nReturns the projection with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the projection.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nANNarchy.core.Projection\nThe requested Projection object if existing, None otherwise.",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "get_projection"
    ]
  },
  {
    "objectID": "reference/get_projection.html#parameters",
    "href": "reference/get_projection.html#parameters",
    "title": "get_projection",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the projection.\nrequired",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "get_projection"
    ]
  },
  {
    "objectID": "reference/get_projection.html#returns",
    "href": "reference/get_projection.html#returns",
    "title": "get_projection",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nANNarchy.core.Projection\nThe requested Projection object if existing, None otherwise.",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "get_projection"
    ]
  },
  {
    "objectID": "reference/callbacks_enabled.html",
    "href": "reference/callbacks_enabled.html",
    "title": "callbacks_enabled",
    "section": "",
    "text": "callbacks_enabled\ncore.Simulate.callbacks_enabled(net_id=0)\nReturns True if callbacks are enabled for the network.",
    "crumbs": [
      "Reference",
      "**Callbacks**",
      "callbacks_enabled"
    ]
  },
  {
    "objectID": "reference/Pooling.html",
    "href": "reference/Pooling.html",
    "title": "Pooling",
    "section": "",
    "text": "extensions.convolution.Pooling.Pooling(self, pre, post, target, psp='pre.r', operation='max', name=None, copied=False)\nPerforms a pooling operation (e.g. max.pooling) on the pre-synaptic population.\nEach post-synaptic neuron covers a specific region (extent) of the pre-synaptic population, over which the result of the operation on firing rates will be assigned to sum(target).\nThe extent is automatically computed using the geometry of the populations, but can be specified in the `connect_pooling()`` methods.\nExample:\ninp = ann.Population(geometry=(100, 100), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npop = ann.Population(geometry=(50, 50), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = Pooling(inp, pop, 'exc', operation='max') # max-pooling\nproj.connect_pooling() # extent=(2, 2) is implicit\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\n\npre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\n\npost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\n\ntype of the connection\nrequired\n\n\noperation\n\npooling function to be applied (“max”, “min”, “mean”)\n'max'\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nconnect_pooling\n\n\n\nconnectivity_matrix\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nsave\nNot available.\n\n\nsave_connectivity\nNot available.\n\n\n\n\n\nextensions.convolution.Pooling.Pooling.connect_pooling(extent=None, delays=0.0)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nextent\ntuple\nextent of the pooling area expressed in the geometry of the pre-synaptic population (e.g (2, 2)). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None.\nNone\n\n\ndelays\nfloat\nsynaptic delay in ms\n0.0\n\n\n\n\n\n\n\nextensions.convolution.Pooling.Pooling.connectivity_matrix(fill=0.0)\nNot available.\n\n\n\nextensions.convolution.Pooling.Pooling.load(filename)\nNot available.\n\n\n\nextensions.convolution.Pooling.Pooling.receptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nextensions.convolution.Pooling.Pooling.save(filename)\nNot available.\n\n\n\nextensions.convolution.Pooling.Pooling.save_connectivity(filename)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Pooling"
    ]
  },
  {
    "objectID": "reference/Pooling.html#parameters",
    "href": "reference/Pooling.html#parameters",
    "title": "Pooling",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\n\npre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\n\npost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\n\ntype of the connection\nrequired\n\n\noperation\n\npooling function to be applied (“max”, “min”, “mean”)\n'max'",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Pooling"
    ]
  },
  {
    "objectID": "reference/Pooling.html#methods",
    "href": "reference/Pooling.html#methods",
    "title": "Pooling",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nconnect_pooling\n\n\n\nconnectivity_matrix\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nsave\nNot available.\n\n\nsave_connectivity\nNot available.\n\n\n\n\n\nextensions.convolution.Pooling.Pooling.connect_pooling(extent=None, delays=0.0)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nextent\ntuple\nextent of the pooling area expressed in the geometry of the pre-synaptic population (e.g (2, 2)). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None.\nNone\n\n\ndelays\nfloat\nsynaptic delay in ms\n0.0\n\n\n\n\n\n\n\nextensions.convolution.Pooling.Pooling.connectivity_matrix(fill=0.0)\nNot available.\n\n\n\nextensions.convolution.Pooling.Pooling.load(filename)\nNot available.\n\n\n\nextensions.convolution.Pooling.Pooling.receptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nextensions.convolution.Pooling.Pooling.save(filename)\nNot available.\n\n\n\nextensions.convolution.Pooling.Pooling.save_connectivity(filename)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Pooling"
    ]
  },
  {
    "objectID": "reference/get_current_step.html",
    "href": "reference/get_current_step.html",
    "title": "get_current_step",
    "section": "",
    "text": "get_current_step\ncore.Global.get_current_step(net_id=0)\nReturns the current simulation step.",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "get_current_step"
    ]
  },
  {
    "objectID": "reference/balloon_two_inputs.html",
    "href": "reference/balloon_two_inputs.html",
    "title": "balloon_two_inputs",
    "section": "",
    "text": "balloon_two_inputs\nextensions.bold.PredefinedModels.balloon_two_inputs(self)\nBOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007).",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_two_inputs"
    ]
  },
  {
    "objectID": "reference/Transpose.html",
    "href": "reference/Transpose.html",
    "title": "Transpose",
    "section": "",
    "text": "extensions.convolution.Transpose.Transpose(self, proj, target)\nTransposed projection reusing the weights of an already-defined rate-coded projection.\nEven though the original projection can be learnable, this one can not. The computed post-synaptic potential is the default case for rate-coded projections: “w * pre.r”\nThe proposed target can differ from the target of the forward projection.\nExample:\nproj_ff = ann.Projection( input, output, target=\"exc\" )\nproj_ff.connect_all_to_all(weights=Uniform(0,1)\n\nproj_fb = Transpose(proj_ff, target=\"inh\")\nproj_fb.connect()\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nproj\n\noriginal projection.\nrequired\n\n\ntarget\n\ntype of the connection (can differ from the original one).\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nconnectivity_matrix\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nsave\nNot available.\n\n\nsave_connectivity\nNot available.\n\n\n\n\n\nextensions.convolution.Transpose.Transpose.connectivity_matrix(fill=0.0)\nNot available.\n\n\n\nextensions.convolution.Transpose.Transpose.load(filename)\nNot available.\n\n\n\nextensions.convolution.Transpose.Transpose.receptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nextensions.convolution.Transpose.Transpose.save(filename)\nNot available.\n\n\n\nextensions.convolution.Transpose.Transpose.save_connectivity(filename)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Transpose"
    ]
  },
  {
    "objectID": "reference/Transpose.html#parameters",
    "href": "reference/Transpose.html#parameters",
    "title": "Transpose",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nproj\n\noriginal projection.\nrequired\n\n\ntarget\n\ntype of the connection (can differ from the original one).\nrequired",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Transpose"
    ]
  },
  {
    "objectID": "reference/Transpose.html#methods",
    "href": "reference/Transpose.html#methods",
    "title": "Transpose",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nconnectivity_matrix\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nsave\nNot available.\n\n\nsave_connectivity\nNot available.\n\n\n\n\n\nextensions.convolution.Transpose.Transpose.connectivity_matrix(fill=0.0)\nNot available.\n\n\n\nextensions.convolution.Transpose.Transpose.load(filename)\nNot available.\n\n\n\nextensions.convolution.Transpose.Transpose.receptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nextensions.convolution.Transpose.Transpose.save(filename)\nNot available.\n\n\n\nextensions.convolution.Transpose.Transpose.save_connectivity(filename)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Transpose"
    ]
  },
  {
    "objectID": "reference/TimedPoissonPopulation.html",
    "href": "reference/TimedPoissonPopulation.html",
    "title": "TimedPoissonPopulation",
    "section": "",
    "text": "TimedPoissonPopulation\ninputs.TimedArray.TimedPoissonPopulation(self, geometry, rates, schedule, period=-1.0, name=None, copied=False)\nPoisson population whose rate vary with the provided schedule.\nExample:\ninp = TimedPoissonPopulation(\n    geometry = 100,\n    rates = [10., 20., 100., 20., 5.],\n    schedule = [0., 100., 200., 500., 600.],\n)\nThis creates a population of 100 Poisson neurons whose rate will be:\n\n10 Hz during the first 100 ms.\n20 HZ during the next 100 ms.\n100 Hz during the next 300 ms.\n20 Hz during the next 100 ms.\n5 Hz until the end of the simulation.\n\nIf you want the TimedPoissonPopulation to “loop” over the schedule, you can specify a period:\ninp = TimedPoissonPopulation(\n    geometry = 100,\n    rates = [10., 20., 100., 20., 5.],\n    schedule = [0., 100., 200., 500., 600.],\n    period = 1000.,\n)\nHere the rate will become 10Hz again every 1 second of simulation. If the period is smaller than the schedule, the remaining rates will not be set.\nNote that you can use the reset() method to manually reinitialize the schedule, times becoming relative to that call:\nsimulate(1200.) # Should switch to 100 Hz due to the period of 1000.\ninp.reset()\nsimulate(1000.) # Starts at 10 Hz again.\nThe rates were here global to the population. If you want each neuron to have a different rate, rates must have additional dimensions corresponding to the geometry of the population.\ninp = TimedPoissonPopulation(\n    geometry = 100,\n    rates = [ \n        [10. + 0.05*i for i in range(100)], \n        [20. + 0.05*i for i in range(100)],\n    ],\n    schedule = [0., 100.],\n    period = 1000.,\n)",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "TimedPoissonPopulation"
    ]
  },
  {
    "objectID": "reference/Projection.html",
    "href": "reference/Projection.html",
    "title": "Projection",
    "section": "",
    "text": "core.Projection.Projection(self, pre, post, target, synapse=None, name=None, disable_omp=True, copied=False)\nStructure holding all synapses of the same type between two populations.\nBy default, the synapse only ensures linear synaptic transmission:\n\nFor rate-coded populations: psp = w * pre.r\nFor spiking populations: g_target += w\n\nTo modify this behavior, a Synapse object can be provided.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\nstr | ANNarchy.core.Population.Population\nPre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\nstr | ANNarchy.core.Population.Population\nPost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\nstr\nType of the connection.\nrequired\n\n\nsynapse\nANNarchy.core.Synapse.Synapse\nA Synapse instance.\nNone\n\n\nname\nstr\nUnique name of the projection (optional, it defaults to proj0, proj1, etc).\nNone\n\n\ndisable_omp\nbool\nEspecially for small- and mid-scale sparse spiking networks, the parallelization of spike propagation is not scalable and disabled by default. It can be enabled by setting this parameter to False.\nTrue\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndendrites\nIteratively returns the dendrites corresponding to this projection.\n\n\nnb_synapses\nTotal number of synapses in the projection.\n\n\nsize\nNumber of post-synaptic neurons receiving synapses.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nconnect_all_to_all\nall-to-all (fully-connected) connection pattern.\n\n\nconnect_dog\nDifference-Of-Gaussians connection pattern.\n\n\nconnect_fixed_number_post\nEach pre-synaptic neuron randomly sends a fixed number of connections to the post-synaptic neurons.\n\n\nconnect_fixed_number_pre\nConnection pattern where each post-synaptic neuron receives a fixed number of pre-synaptic neurons.\n\n\nconnect_fixed_probability\nProbabilistic sparse connection pattern.\n\n\nconnect_from_file\nBuilds the connectivity matrix using data saved using Projection.save_connectivity() (not save()!).\n\n\nconnect_from_matrix\nBuilds a connection pattern according to a dense connectivity matrix.\n\n\nconnect_from_matrix_market\nLoads a weight matrix encoded in the Matrix Market format. This connector is intended for benchmarking purposes.\n\n\nconnect_from_sparse\nBuilds a connectivity pattern using a Scipy sparse matrix for the weights and (optionally) delays.\n\n\nconnect_gaussian\nGaussian connection pattern.\n\n\nconnect_one_to_one\none-to-one connection pattern.\n\n\nconnect_with_func\nConnection pattern based on a user-defined function.\n\n\nconnectivity_matrix\nReturns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations.\n\n\ndendrite\nReturns the dendrite of a postsynaptic neuron according to its rank.\n\n\ndisable_learning\nDisables learning for all synapses of this projection.\n\n\nenable_learning\nEnables learning for all the synapses of this projection.\n\n\nget\nReturns a list of parameters/variables values for each dendrite in the projection.\n\n\nload\nLoads the saved state of the projection by Projection.save().\n\n\nnb_efferent_synapses\nNumber of efferent connections. Intended only for spiking models.\n\n\nnb_synapses_per_dendrite\nTotal number of synapses for each dendrite as a list.\n\n\nreceptive_fields\nGathers all receptive fields within this projection.\n\n\nreset\nResets all parameters and variables of the projection to their initial value (before the call to compile()).\n\n\nsave\nSaves all information about the projection (connectivity, current value of parameters and variables) into a file.\n\n\nsave_connectivity\nSaves the connectivity of the projection into a file.\n\n\nset\nSets the parameters/variables values for each dendrite in the projection.\n\n\nsize_in_bytes\nReturns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked.\n\n\nstart_creating\nStarts creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n\n\nstart_pruning\nStarts pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n\n\nstop_creating\nStops creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n\n\nstop_pruning\nStops pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n\n\nsynapse\nReturns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise.\n\n\nupdate_launch_config\nAllows the adjustment of the CUDA launch config (since 4.7.2).\n\n\n\n\n\ncore.Projection.Projection.connect_all_to_all(self, weights, delays=0.0, allow_self_connections=False, force_multiple_weights=False, storage_format=None, storage_order=None)\nall-to-all (fully-connected) connection pattern.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nfloat | ANNarchy.core.Random.RandomDistribution\nSynaptic values, either a single value or a random distribution object.\nrequired\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nSynaptic delays, either a single value or a random distribution object (default=dt).\n0.0\n\n\nallow_self_connections\nbool\nIf True, self-connections between a neuron and itself are allowed (default = False if the pre- and post-populations are identical, True otherwise).\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_dog(self, amp_pos, sigma_pos, amp_neg, sigma_neg, delays=0.0, limit=0.01, allow_self_connections=False, storage_format=None)\nDifference-Of-Gaussians connection pattern.\nEach neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Difference-Of-Gaussians profile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\namp_pos\nfloat\nAmplitude of the positive Gaussian function\nrequired\n\n\nsigma_pos\nfloat\nWidth of the positive Gaussian function\nrequired\n\n\namp_neg\nfloat\nAmplitude of the negative Gaussian function\nrequired\n\n\nsigma_neg\nfloat\nWidth of the negative Gaussian function\nrequired\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nSynaptic delay, either a single value or a random distribution object (default=dt).\n0.0\n\n\nlimit\nfloat\nProportion of amp below which synapses are not created (default: 0.01)\n0.01\n\n\nallow_self_connections\nbool\nAllows connections between a neuron and itself.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_fixed_number_post(self, number, weights=1.0, delays=0.0, allow_self_connections=False, force_multiple_weights=False, storage_format=None, storage_order=None)\nEach pre-synaptic neuron randomly sends a fixed number of connections to the post-synaptic neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnumber\nint\nNumber of synapses per pre-synaptic neuron.\nrequired\n\n\nweights\nfloat | ANNarchy.core.Random.RandomDistribution\nEither a single value for all synapses or a RandomDistribution object.\n1.0\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nEither a single value for all synapses or a RandomDistribution object (default = dt)\n0.0\n\n\nallow_self_connections\nbool\nDefines if self-connections are allowed (default=False)\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_fixed_number_pre(self, number, weights, delays=0.0, allow_self_connections=False, force_multiple_weights=False, storage_format=None, storage_order=None)\nConnection pattern where each post-synaptic neuron receives a fixed number of pre-synaptic neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnumber\nint\nNumber of synapses per postsynaptic neuron.\nrequired\n\n\nweights\nfloat | ANNarchy.core.Random.RandomDistribution\nEither a single value for all synapses or a RandomDistribution object.\nrequired\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nEither a single value for all synapses or a RandomDistribution object (default = dt)\n0.0\n\n\nallow_self_connections\nbool\nDefines if self-connections are allowed (default=False).\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_fixed_probability(self, probability, weights, delays=0.0, allow_self_connections=False, force_multiple_weights=False, storage_format=None, storage_order=None)\nProbabilistic sparse connection pattern.\nEach neuron in the postsynaptic population is connected to neurons of the presynaptic population with the given probability. Self-connections are avoided by default.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprobability\nfloat\nProbability that a synapse is created.\nrequired\n\n\nweights\nfloat | ANNarchy.core.Random.RandomDistribution\nEither a single value for all synapses or a RandomDistribution object.\nrequired\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nEither a single value for all synapses or a RandomDistribution object (default = dt)\n0.0\n\n\nallow_self_connections\nbool\nDefines if self-connections are allowed (default=False).\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_from_file(self, filename, pickle_encoding=None, storage_format=None, storage_order=None)\nBuilds the connectivity matrix using data saved using Projection.save_connectivity() (not save()!).\nAdmissible file formats are compressed Numpy files (.npz), gunzipped binary text files (.gz) or binary text files.\nNote: Only the ranks, weights and delays are loaded, not the other variables.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfile where the connections were saved.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_from_matrix(self, weights, delays=0.0, pre_post=False, storage_format=None, storage_order=None)\nBuilds a connection pattern according to a dense connectivity matrix.\nThe matrix must be N*M, where N is the number of neurons in the post-synaptic population and M in the pre-synaptic one. Lists of lists must have the same size.\nIf a synapse should not be created, the weight value should be None.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nnumpy.numpy.array\nNumpy array (or list of lists of equal size) representing the weights. If a value is None, the corresponding synapse will not be created.\nrequired\n\n\ndelays\n\nNumpy array representing the delays. Must represent the same synapses as the weights argument. If omitted, the delays are considered 0.\n0.0\n\n\npre_post\n\nStates which index is first. By default, the first dimension is related to the post-synaptic population. If pre_post is True, the first dimension is the pre-synaptic population.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_from_matrix_market(self, filename, storage_format=None, storage_order=None)\nLoads a weight matrix encoded in the Matrix Market format. This connector is intended for benchmarking purposes.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nFilename of the Matrix Market (.mtx) file.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_from_sparse(self, weights, delays=0.0, storage_format=None, storage_order=None)\nBuilds a connectivity pattern using a Scipy sparse matrix for the weights and (optionally) delays.\nWarning: a sparse matrix has pre-synaptic ranks as first dimension.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nscipy.scipy.sparse.scipy.sparse.lil_matrix\na sparse lil_matrix object created from scipy.\nrequired\n\n\ndelays\nint | float\nthe value of the constant delay (default: dt). Variable delays are not allowed.\n0.0\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_gaussian(self, amp, sigma, delays=0.0, limit=0.01, allow_self_connections=False, storage_format=None)\nGaussian connection pattern.\nEach neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Gaussian profile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\namp\nfloat\nAmplitude of the Gaussian function\nrequired\n\n\nsigma\nfloat\nWidth of the Gaussian function\nrequired\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nSynaptic delay, either a single value or a random distribution object (default=dt).\n0.0\n\n\nlimit\nfloat\nProportion of amp below which synapses are not created\n0.01\n\n\nallow_self_connections\nbool\nAllows connections between a neuron and itself.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_one_to_one(self, weights=1.0, delays=0.0, force_multiple_weights=False, storage_format=None, storage_order=None)\none-to-one connection pattern.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nfloat | ANNarchy.core.Random.RandomDistribution\nInitial synaptic values, either a single value (float) or a random distribution object.\n1.0\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nSynaptic delays, either a single value or a random distribution object (default=dt).\n0.0\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_with_func(self, method, storage_format=None, storage_order=None, **args)\nConnection pattern based on a user-defined function.\nThe two first arguments of the function must be the pre and post populations. Additional arguments can be passed at creation time.\nThe function must return a ann.LILConnectivity object.\nExample:\ndef probabilistic_pattern(pre, post, weight, probability):\n    # Create a LIL structure for the connectivity matrix\n    synapses = ann.LILConnectivity()\n    # For all neurons in the post-synaptic population\n    for post_rank in xrange(post.size):\n        # Decide which pre-synaptic neurons should form synapses\n        ranks = []\n        for pre_rank in xrange(pre.size):\n            if random.random() &lt; probability:\n                ranks.append(pre_rank)\n        # Create weights and delays arrays of the same size\n        values = [weight for i in xrange(len(ranks)) ]\n        delays = [0 for i in xrange(len(ranks)) ]\n        # Add this information to the LIL matrix\n        synapses.add(post_rank, ranks, values, delays)\n\n    return synapses\n\nproj = ann.Projection(pop1, pop2, target = 'inh')\nproj.connect_with_func(\n    method=probabilistic_pattern, \n    weight=1.0, \n    probability=0.3\n) \n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmethod\n\nMethod to call. The method must return a LILConnectivity object.\nrequired\n\n\nargs\n\nList of additional arguments needed by the function.\n{}\n\n\n\n\n\n\n\ncore.Projection.Projection.connectivity_matrix(fill=0.0)\nReturns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations.\nThe first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones.\nIf PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfill\nfloat\nvalue to put in the matrix when there is no connection (default: 0.0).\n0.0\n\n\n\n\n\n\n\ncore.Projection.Projection.dendrite(post)\nReturns the dendrite of a postsynaptic neuron according to its rank.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npost\n\ncan be either the rank or the coordinates of the post-synaptic neuron.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.disable_learning(update=None)\nDisables learning for all synapses of this projection.\nThe effect depends on the rate-coded or spiking nature of the projection:\n\nRate-coded: the updating of all synaptic variables is disabled (including the weights w). This is equivalent to proj.update = False.\nSpiking: the updating of the weights w is disabled, but all other variables are updated. This is equivalent to proj.plasticity = False.\n\nThis method is useful when performing some tests on a trained network without messing with the learned weights.\n\n\n\ncore.Projection.Projection.enable_learning(period=None, offset=None)\nEnables learning for all the synapses of this projection.\nFor example, providing the following parameters at time 10 ms:\nenable_learning(period=10., offset=5.)\nwould call the updating methods at times 15, 25, 35, etc…\nThe default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of dt.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nperiod\n\ndetermines how often the synaptic variables will be updated.\nNone\n\n\noffset\n\ndetermines the offset at which the synaptic variables will be updated relative to the current time.\nNone\n\n\n\n\n\n\n\ncore.Projection.Projection.get(name)\nReturns a list of parameters/variables values for each dendrite in the projection.\nThe list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nthe name of the parameter or variable\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.load(filename, pickle_encoding=None)\nLoads the saved state of the projection by Projection.save().\nWarning: Matlab data can not be loaded.\nExample:\nproj.load('proj1.npz')\nproj.load('proj1.txt')\nproj.load('proj1.txt.gz')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nthe file name with relative or absolute path.\nrequired\n\n\npickle_encoding\nstr\nWhat encoding to use when reading Python 2 strings. Only useful when loading Python 2 generated pickled files in Python 3, which includes npy/npz files containing object arrays. Values other than latin1, ASCII, and bytes are not allowed, as they can corrupt numerical data.\nNone\n\n\n\n\n\n\n\ncore.Projection.Projection.nb_efferent_synapses()\nNumber of efferent connections. Intended only for spiking models.\n\n\n\ncore.Projection.Projection.nb_synapses_per_dendrite()\nTotal number of synapses for each dendrite as a list.\n\n\n\ncore.Projection.Projection.receptive_fields(variable='w', in_post_geometry=True)\nGathers all receptive fields within this projection.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr\nName of the variable.\n'w'\n\n\nin_post_geometry\nbool\nIf False, the data will be plotted as square grid.\nTrue\n\n\n\n\n\n\n\ncore.Projection.Projection.reset(attributes=-1, synapses=False)\nResets all parameters and variables of the projection to their initial value (before the call to compile()).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattributes\n\nlist of attributes (parameter or variable) which should be reinitialized. Default: all attributes (-1).\n-1\n\n\nsynapses\n\ndefines if the weights and delays should also be recreated. Default: False\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.save(filename)\nSaves all information about the projection (connectivity, current value of parameters and variables) into a file.\n\nIf the file name is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the file name ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nIf the file name is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nOtherwise, the data will be pickled into a simple binary text file using pickle.\n\nWarning: the ‘.mat’ data will not be loadable by ANNarchy, it is only for external analysis purpose.\nExample:\nproj.save('proj1.npz')\nproj.save('proj1.txt')\nproj.save('proj1.txt.gz')\nproj.save('proj1.mat')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfile name, may contain relative or absolute path.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.save_connectivity(filename)\nSaves the connectivity of the projection into a file.\nOnly the connectivity matrix, the weights and delays are saved, not the other synaptic variables.\nThe generated data can be used to create a projection in another network:\nproj.connect_from_file(filename)\n\nIf the file name is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the file name ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nIf the file name is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nOtherwise, the data will be pickled into a simple binary text file using pickle.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\n\nfile name, may contain relative or absolute path.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.set(value)\nSets the parameters/variables values for each dendrite in the projection.\nFor parameters, you can provide:\n\na single value, which will be the same for all dendrites.\na list or 1D numpy array of the same length as the number of actual dendrites (self.size).\n\nFor variables, you can provide:\n\na single value, which will be the same for all synapses of all dendrites.\na list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value.\n\nWarning: it is not possible to set different values to each synapse using this method. One should iterate over the dendrites:\nfor dendrite in proj.dendrites:\n    dendrite.w = np.ones(dendrite.size)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\n\na dictionary with the name of the parameter/variable as key.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.size_in_bytes()\nReturns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked.\n\n\n\ncore.Projection.Projection.start_creating(period=None)\nStarts creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nperiod\nfloat\nhow often creating should be evaluated (default: dt, i.e. each step)\nNone\n\n\n\n\n\n\n\ncore.Projection.Projection.start_pruning(period=None)\nStarts pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nperiod\nfloat\nhow often pruning should be evaluated (default: dt, i.e. each step)\nNone\n\n\n\n\n\n\n\ncore.Projection.Projection.stop_creating()\nStops creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\ncore.Projection.Projection.stop_pruning()\nStops pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\ncore.Projection.Projection.synapse(pre, post)\nReturns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\n\nrank of the pre-synaptic neuron.\nrequired\n\n\npost\n\nrank of the post-synaptic neuron.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.update_launch_config(nb_blocks=-1, threads_per_block=32)\nAllows the adjustment of the CUDA launch config (since 4.7.2).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnb_blocks\nint\nnumber of CUDA blocks which can be 65535 at maximum. If set to -1, the number of launched blocks is computed by ANNarchy.\n-1\n\n\nthreads_per_block\nint\nnumber of CUDA threads for one block which can be maximally 1024.\n32",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Projection"
    ]
  },
  {
    "objectID": "reference/Projection.html#parameters",
    "href": "reference/Projection.html#parameters",
    "title": "Projection",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\nstr | ANNarchy.core.Population.Population\nPre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\nstr | ANNarchy.core.Population.Population\nPost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\nstr\nType of the connection.\nrequired\n\n\nsynapse\nANNarchy.core.Synapse.Synapse\nA Synapse instance.\nNone\n\n\nname\nstr\nUnique name of the projection (optional, it defaults to proj0, proj1, etc).\nNone\n\n\ndisable_omp\nbool\nEspecially for small- and mid-scale sparse spiking networks, the parallelization of spike propagation is not scalable and disabled by default. It can be enabled by setting this parameter to False.\nTrue",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Projection"
    ]
  },
  {
    "objectID": "reference/Projection.html#attributes",
    "href": "reference/Projection.html#attributes",
    "title": "Projection",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndendrites\nIteratively returns the dendrites corresponding to this projection.\n\n\nnb_synapses\nTotal number of synapses in the projection.\n\n\nsize\nNumber of post-synaptic neurons receiving synapses.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Projection"
    ]
  },
  {
    "objectID": "reference/Projection.html#methods",
    "href": "reference/Projection.html#methods",
    "title": "Projection",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nconnect_all_to_all\nall-to-all (fully-connected) connection pattern.\n\n\nconnect_dog\nDifference-Of-Gaussians connection pattern.\n\n\nconnect_fixed_number_post\nEach pre-synaptic neuron randomly sends a fixed number of connections to the post-synaptic neurons.\n\n\nconnect_fixed_number_pre\nConnection pattern where each post-synaptic neuron receives a fixed number of pre-synaptic neurons.\n\n\nconnect_fixed_probability\nProbabilistic sparse connection pattern.\n\n\nconnect_from_file\nBuilds the connectivity matrix using data saved using Projection.save_connectivity() (not save()!).\n\n\nconnect_from_matrix\nBuilds a connection pattern according to a dense connectivity matrix.\n\n\nconnect_from_matrix_market\nLoads a weight matrix encoded in the Matrix Market format. This connector is intended for benchmarking purposes.\n\n\nconnect_from_sparse\nBuilds a connectivity pattern using a Scipy sparse matrix for the weights and (optionally) delays.\n\n\nconnect_gaussian\nGaussian connection pattern.\n\n\nconnect_one_to_one\none-to-one connection pattern.\n\n\nconnect_with_func\nConnection pattern based on a user-defined function.\n\n\nconnectivity_matrix\nReturns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations.\n\n\ndendrite\nReturns the dendrite of a postsynaptic neuron according to its rank.\n\n\ndisable_learning\nDisables learning for all synapses of this projection.\n\n\nenable_learning\nEnables learning for all the synapses of this projection.\n\n\nget\nReturns a list of parameters/variables values for each dendrite in the projection.\n\n\nload\nLoads the saved state of the projection by Projection.save().\n\n\nnb_efferent_synapses\nNumber of efferent connections. Intended only for spiking models.\n\n\nnb_synapses_per_dendrite\nTotal number of synapses for each dendrite as a list.\n\n\nreceptive_fields\nGathers all receptive fields within this projection.\n\n\nreset\nResets all parameters and variables of the projection to their initial value (before the call to compile()).\n\n\nsave\nSaves all information about the projection (connectivity, current value of parameters and variables) into a file.\n\n\nsave_connectivity\nSaves the connectivity of the projection into a file.\n\n\nset\nSets the parameters/variables values for each dendrite in the projection.\n\n\nsize_in_bytes\nReturns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked.\n\n\nstart_creating\nStarts creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n\n\nstart_pruning\nStarts pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n\n\nstop_creating\nStops creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n\n\nstop_pruning\nStops pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n\n\nsynapse\nReturns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise.\n\n\nupdate_launch_config\nAllows the adjustment of the CUDA launch config (since 4.7.2).\n\n\n\n\n\ncore.Projection.Projection.connect_all_to_all(self, weights, delays=0.0, allow_self_connections=False, force_multiple_weights=False, storage_format=None, storage_order=None)\nall-to-all (fully-connected) connection pattern.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nfloat | ANNarchy.core.Random.RandomDistribution\nSynaptic values, either a single value or a random distribution object.\nrequired\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nSynaptic delays, either a single value or a random distribution object (default=dt).\n0.0\n\n\nallow_self_connections\nbool\nIf True, self-connections between a neuron and itself are allowed (default = False if the pre- and post-populations are identical, True otherwise).\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_dog(self, amp_pos, sigma_pos, amp_neg, sigma_neg, delays=0.0, limit=0.01, allow_self_connections=False, storage_format=None)\nDifference-Of-Gaussians connection pattern.\nEach neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Difference-Of-Gaussians profile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\namp_pos\nfloat\nAmplitude of the positive Gaussian function\nrequired\n\n\nsigma_pos\nfloat\nWidth of the positive Gaussian function\nrequired\n\n\namp_neg\nfloat\nAmplitude of the negative Gaussian function\nrequired\n\n\nsigma_neg\nfloat\nWidth of the negative Gaussian function\nrequired\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nSynaptic delay, either a single value or a random distribution object (default=dt).\n0.0\n\n\nlimit\nfloat\nProportion of amp below which synapses are not created (default: 0.01)\n0.01\n\n\nallow_self_connections\nbool\nAllows connections between a neuron and itself.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_fixed_number_post(self, number, weights=1.0, delays=0.0, allow_self_connections=False, force_multiple_weights=False, storage_format=None, storage_order=None)\nEach pre-synaptic neuron randomly sends a fixed number of connections to the post-synaptic neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnumber\nint\nNumber of synapses per pre-synaptic neuron.\nrequired\n\n\nweights\nfloat | ANNarchy.core.Random.RandomDistribution\nEither a single value for all synapses or a RandomDistribution object.\n1.0\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nEither a single value for all synapses or a RandomDistribution object (default = dt)\n0.0\n\n\nallow_self_connections\nbool\nDefines if self-connections are allowed (default=False)\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_fixed_number_pre(self, number, weights, delays=0.0, allow_self_connections=False, force_multiple_weights=False, storage_format=None, storage_order=None)\nConnection pattern where each post-synaptic neuron receives a fixed number of pre-synaptic neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnumber\nint\nNumber of synapses per postsynaptic neuron.\nrequired\n\n\nweights\nfloat | ANNarchy.core.Random.RandomDistribution\nEither a single value for all synapses or a RandomDistribution object.\nrequired\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nEither a single value for all synapses or a RandomDistribution object (default = dt)\n0.0\n\n\nallow_self_connections\nbool\nDefines if self-connections are allowed (default=False).\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_fixed_probability(self, probability, weights, delays=0.0, allow_self_connections=False, force_multiple_weights=False, storage_format=None, storage_order=None)\nProbabilistic sparse connection pattern.\nEach neuron in the postsynaptic population is connected to neurons of the presynaptic population with the given probability. Self-connections are avoided by default.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprobability\nfloat\nProbability that a synapse is created.\nrequired\n\n\nweights\nfloat | ANNarchy.core.Random.RandomDistribution\nEither a single value for all synapses or a RandomDistribution object.\nrequired\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nEither a single value for all synapses or a RandomDistribution object (default = dt)\n0.0\n\n\nallow_self_connections\nbool\nDefines if self-connections are allowed (default=False).\nFalse\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_from_file(self, filename, pickle_encoding=None, storage_format=None, storage_order=None)\nBuilds the connectivity matrix using data saved using Projection.save_connectivity() (not save()!).\nAdmissible file formats are compressed Numpy files (.npz), gunzipped binary text files (.gz) or binary text files.\nNote: Only the ranks, weights and delays are loaded, not the other variables.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfile where the connections were saved.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_from_matrix(self, weights, delays=0.0, pre_post=False, storage_format=None, storage_order=None)\nBuilds a connection pattern according to a dense connectivity matrix.\nThe matrix must be N*M, where N is the number of neurons in the post-synaptic population and M in the pre-synaptic one. Lists of lists must have the same size.\nIf a synapse should not be created, the weight value should be None.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nnumpy.numpy.array\nNumpy array (or list of lists of equal size) representing the weights. If a value is None, the corresponding synapse will not be created.\nrequired\n\n\ndelays\n\nNumpy array representing the delays. Must represent the same synapses as the weights argument. If omitted, the delays are considered 0.\n0.0\n\n\npre_post\n\nStates which index is first. By default, the first dimension is related to the post-synaptic population. If pre_post is True, the first dimension is the pre-synaptic population.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_from_matrix_market(self, filename, storage_format=None, storage_order=None)\nLoads a weight matrix encoded in the Matrix Market format. This connector is intended for benchmarking purposes.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nFilename of the Matrix Market (.mtx) file.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_from_sparse(self, weights, delays=0.0, storage_format=None, storage_order=None)\nBuilds a connectivity pattern using a Scipy sparse matrix for the weights and (optionally) delays.\nWarning: a sparse matrix has pre-synaptic ranks as first dimension.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nscipy.scipy.sparse.scipy.sparse.lil_matrix\na sparse lil_matrix object created from scipy.\nrequired\n\n\ndelays\nint | float\nthe value of the constant delay (default: dt). Variable delays are not allowed.\n0.0\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_gaussian(self, amp, sigma, delays=0.0, limit=0.01, allow_self_connections=False, storage_format=None)\nGaussian connection pattern.\nEach neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Gaussian profile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\namp\nfloat\nAmplitude of the Gaussian function\nrequired\n\n\nsigma\nfloat\nWidth of the Gaussian function\nrequired\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nSynaptic delay, either a single value or a random distribution object (default=dt).\n0.0\n\n\nlimit\nfloat\nProportion of amp below which synapses are not created\n0.01\n\n\nallow_self_connections\nbool\nAllows connections between a neuron and itself.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_one_to_one(self, weights=1.0, delays=0.0, force_multiple_weights=False, storage_format=None, storage_order=None)\none-to-one connection pattern.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\nfloat | ANNarchy.core.Random.RandomDistribution\nInitial synaptic values, either a single value (float) or a random distribution object.\n1.0\n\n\ndelays\nfloat | ANNarchy.core.Random.RandomDistribution\nSynaptic delays, either a single value or a random distribution object (default=dt).\n0.0\n\n\nforce_multiple_weights\nbool\nIf a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used.\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.connect_with_func(self, method, storage_format=None, storage_order=None, **args)\nConnection pattern based on a user-defined function.\nThe two first arguments of the function must be the pre and post populations. Additional arguments can be passed at creation time.\nThe function must return a ann.LILConnectivity object.\nExample:\ndef probabilistic_pattern(pre, post, weight, probability):\n    # Create a LIL structure for the connectivity matrix\n    synapses = ann.LILConnectivity()\n    # For all neurons in the post-synaptic population\n    for post_rank in xrange(post.size):\n        # Decide which pre-synaptic neurons should form synapses\n        ranks = []\n        for pre_rank in xrange(pre.size):\n            if random.random() &lt; probability:\n                ranks.append(pre_rank)\n        # Create weights and delays arrays of the same size\n        values = [weight for i in xrange(len(ranks)) ]\n        delays = [0 for i in xrange(len(ranks)) ]\n        # Add this information to the LIL matrix\n        synapses.add(post_rank, ranks, values, delays)\n\n    return synapses\n\nproj = ann.Projection(pop1, pop2, target = 'inh')\nproj.connect_with_func(\n    method=probabilistic_pattern, \n    weight=1.0, \n    probability=0.3\n) \n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmethod\n\nMethod to call. The method must return a LILConnectivity object.\nrequired\n\n\nargs\n\nList of additional arguments needed by the function.\n{}\n\n\n\n\n\n\n\ncore.Projection.Projection.connectivity_matrix(fill=0.0)\nReturns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations.\nThe first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones.\nIf PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfill\nfloat\nvalue to put in the matrix when there is no connection (default: 0.0).\n0.0\n\n\n\n\n\n\n\ncore.Projection.Projection.dendrite(post)\nReturns the dendrite of a postsynaptic neuron according to its rank.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npost\n\ncan be either the rank or the coordinates of the post-synaptic neuron.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.disable_learning(update=None)\nDisables learning for all synapses of this projection.\nThe effect depends on the rate-coded or spiking nature of the projection:\n\nRate-coded: the updating of all synaptic variables is disabled (including the weights w). This is equivalent to proj.update = False.\nSpiking: the updating of the weights w is disabled, but all other variables are updated. This is equivalent to proj.plasticity = False.\n\nThis method is useful when performing some tests on a trained network without messing with the learned weights.\n\n\n\ncore.Projection.Projection.enable_learning(period=None, offset=None)\nEnables learning for all the synapses of this projection.\nFor example, providing the following parameters at time 10 ms:\nenable_learning(period=10., offset=5.)\nwould call the updating methods at times 15, 25, 35, etc…\nThe default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of dt.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nperiod\n\ndetermines how often the synaptic variables will be updated.\nNone\n\n\noffset\n\ndetermines the offset at which the synaptic variables will be updated relative to the current time.\nNone\n\n\n\n\n\n\n\ncore.Projection.Projection.get(name)\nReturns a list of parameters/variables values for each dendrite in the projection.\nThe list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nthe name of the parameter or variable\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.load(filename, pickle_encoding=None)\nLoads the saved state of the projection by Projection.save().\nWarning: Matlab data can not be loaded.\nExample:\nproj.load('proj1.npz')\nproj.load('proj1.txt')\nproj.load('proj1.txt.gz')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nthe file name with relative or absolute path.\nrequired\n\n\npickle_encoding\nstr\nWhat encoding to use when reading Python 2 strings. Only useful when loading Python 2 generated pickled files in Python 3, which includes npy/npz files containing object arrays. Values other than latin1, ASCII, and bytes are not allowed, as they can corrupt numerical data.\nNone\n\n\n\n\n\n\n\ncore.Projection.Projection.nb_efferent_synapses()\nNumber of efferent connections. Intended only for spiking models.\n\n\n\ncore.Projection.Projection.nb_synapses_per_dendrite()\nTotal number of synapses for each dendrite as a list.\n\n\n\ncore.Projection.Projection.receptive_fields(variable='w', in_post_geometry=True)\nGathers all receptive fields within this projection.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr\nName of the variable.\n'w'\n\n\nin_post_geometry\nbool\nIf False, the data will be plotted as square grid.\nTrue\n\n\n\n\n\n\n\ncore.Projection.Projection.reset(attributes=-1, synapses=False)\nResets all parameters and variables of the projection to their initial value (before the call to compile()).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattributes\n\nlist of attributes (parameter or variable) which should be reinitialized. Default: all attributes (-1).\n-1\n\n\nsynapses\n\ndefines if the weights and delays should also be recreated. Default: False\nFalse\n\n\n\n\n\n\n\ncore.Projection.Projection.save(filename)\nSaves all information about the projection (connectivity, current value of parameters and variables) into a file.\n\nIf the file name is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the file name ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nIf the file name is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nOtherwise, the data will be pickled into a simple binary text file using pickle.\n\nWarning: the ‘.mat’ data will not be loadable by ANNarchy, it is only for external analysis purpose.\nExample:\nproj.save('proj1.npz')\nproj.save('proj1.txt')\nproj.save('proj1.txt.gz')\nproj.save('proj1.mat')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfile name, may contain relative or absolute path.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.save_connectivity(filename)\nSaves the connectivity of the projection into a file.\nOnly the connectivity matrix, the weights and delays are saved, not the other synaptic variables.\nThe generated data can be used to create a projection in another network:\nproj.connect_from_file(filename)\n\nIf the file name is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the file name ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nIf the file name is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nOtherwise, the data will be pickled into a simple binary text file using pickle.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\n\nfile name, may contain relative or absolute path.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.set(value)\nSets the parameters/variables values for each dendrite in the projection.\nFor parameters, you can provide:\n\na single value, which will be the same for all dendrites.\na list or 1D numpy array of the same length as the number of actual dendrites (self.size).\n\nFor variables, you can provide:\n\na single value, which will be the same for all synapses of all dendrites.\na list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value.\n\nWarning: it is not possible to set different values to each synapse using this method. One should iterate over the dendrites:\nfor dendrite in proj.dendrites:\n    dendrite.w = np.ones(dendrite.size)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\n\na dictionary with the name of the parameter/variable as key.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.size_in_bytes()\nReturns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked.\n\n\n\ncore.Projection.Projection.start_creating(period=None)\nStarts creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nperiod\nfloat\nhow often creating should be evaluated (default: dt, i.e. each step)\nNone\n\n\n\n\n\n\n\ncore.Projection.Projection.start_pruning(period=None)\nStarts pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nperiod\nfloat\nhow often pruning should be evaluated (default: dt, i.e. each step)\nNone\n\n\n\n\n\n\n\ncore.Projection.Projection.stop_creating()\nStops creating the synapses in the projection if the synapse defines a ‘creating’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\ncore.Projection.Projection.stop_pruning()\nStops pruning the synapses in the projection if the synapse defines a ‘pruning’ argument.\n‘structural_plasticity’ must be set to True in setup().\n\n\n\ncore.Projection.Projection.synapse(pre, post)\nReturns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\n\nrank of the pre-synaptic neuron.\nrequired\n\n\npost\n\nrank of the post-synaptic neuron.\nrequired\n\n\n\n\n\n\n\ncore.Projection.Projection.update_launch_config(nb_blocks=-1, threads_per_block=32)\nAllows the adjustment of the CUDA launch config (since 4.7.2).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnb_blocks\nint\nnumber of CUDA blocks which can be 65535 at maximum. If set to -1, the number of launched blocks is computed by ANNarchy.\n-1\n\n\nthreads_per_block\nint\nnumber of CUDA threads for one block which can be maximally 1024.\n32",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Projection"
    ]
  },
  {
    "objectID": "reference/dt.html",
    "href": "reference/dt.html",
    "title": "dt",
    "section": "",
    "text": "dt\ncore.Global.dt()\nReturns the simulation step size dt used in the simulation.",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "dt"
    ]
  },
  {
    "objectID": "reference/LeakyIntegrator.html",
    "href": "reference/LeakyIntegrator.html",
    "title": "LeakyIntegrator",
    "section": "",
    "text": "models.Neurons.LeakyIntegrator(self, tau=10.0, B=0.0, T=0.0, sum='sum(exc) - sum(inh)', noise=None)\nLeaky-integrator rate-coded neuron, optionally noisy.\nThis simple rate-coded neuron defines an internal variable v(t) which integrates the inputs I(t) with a time constant \\tau and a baseline B. An additive noise N(t) can be optionally defined:\n\\tau \\cdot \\frac{dv(t)}{dt} + v(t) = I(t) + B + N(t)\nThe transfer function is the positive (or rectified linear ReLU) function with a threshold T:\nr(t) = (v(t) - T)^+\nBy default, the input I(t) to this neuron is “sum(exc) - sum(inh)”, but this can be changed by setting the sum argument:\nneuron = ann.LeakyIntegrator(sum=\"sum('exc')\")\nBy default, there is no additive noise, but the noise argument can be passed with a specific distribution:\nneuron = ann.LeakyIntegrator(noise=\"Normal(0.0, 1.0)\")\nParameters:\n\ntau = 10.0 : Time constant in ms of the neuron.\nB = 0.0 : Baseline value for v.\nT = 0.0 : Threshold for the positive transfer function.\n\nVariables:\n\nv : internal variable (init = 0.0):\ntau * dv/dt + v = sum(exc) - sum(inh) + B + N\nr : firing rate (init = 0.0):\nr = pos(v - T)\n\nThe ODE is solved using the exponential Euler method.\nEquivalent code:\nLeakyIntegrator = Neuron(\n    parameters='''\n        tau = 10.0 : population\n        B = 0.0\n        T = 0.0 : population\n    ''', \n    equations='''\n        tau * dv/dt + v = sum(exc) - sum(inh) + B : exponential\n        r = pos(v - T)\n    '''\n)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntau\nfloat\nTime constant.\n10.0\n\n\nB\nfloat\nBaseline.\n0.0\n\n\nT\nfloat\nThreshold.\n0.0\n\n\nsum\nstr\nInput sums.\n'sum(exc) - sum(inh)'",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "LeakyIntegrator"
    ]
  },
  {
    "objectID": "reference/LeakyIntegrator.html#parameters",
    "href": "reference/LeakyIntegrator.html#parameters",
    "title": "LeakyIntegrator",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntau\nfloat\nTime constant.\n10.0\n\n\nB\nfloat\nBaseline.\n0.0\n\n\nT\nfloat\nThreshold.\n0.0\n\n\nsum\nstr\nInput sums.\n'sum(exc) - sum(inh)'",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "LeakyIntegrator"
    ]
  },
  {
    "objectID": "reference/EIF_cond_alpha_isfa_ista.html",
    "href": "reference/EIF_cond_alpha_isfa_ista.html",
    "title": "EIF_cond_alpha_isfa_ista",
    "section": "",
    "text": "EIF_cond_alpha_isfa_ista\nmodels.Neurons.EIF_cond_alpha_isfa_ista(self, v_rest=-70.6, cm=0.281, tau_m=9.3667, tau_refrac=0.1, tau_syn_E=5.0, tau_syn_I=5.0, e_rev_E=0.0, e_rev_I=-80.0, tau_w=144.0, a=4.0, b=0.0805, i_offset=0.0, delta_T=2.0, v_thresh=-50.4, v_reset=-70.6, v_spike=-40.0)\nExponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.).\nDefinition according to:\n\nBrette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642\n\nParameters:\n\nv_rest = -70.6 : Resting membrane potential (mV)\ncm = 0.281 : Capacity of the membrane (nF)\ntau_m = 9.3667 : Membrane time constant (ms)\ntau_refrac = 0.1 : Duration of refractory period (ms)\ntau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms)\ntau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms)\ne_rev_E = 0.0 : Reversal potential for excitatory input (mV)\ne_rev_I = -80.0 : Reversal potential for inhibitory input (mv)\ntau_w = 144.0 : Time constant of the adaptation variable (ms)\na = 4.0 : Scaling of the adaptation variable\nb = 0.0805 : Increment on the adaptation variable after a spike\ni_offset = 0.0 : Offset current (nA)\ndelta_T = 2.0 : Speed of the exponential (mV)\nv_thresh = -50.4 : Spike threshold for the exponential (mV)\nv_reset = -70.6 : Reset potential after a spike (mV)\nv_spike = -40.0 : Spike threshold (mV)\n\nVariables:\n\nI : input current (nA):\nI = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset\nv : membrane potential in mV (init=-70.6):\ntau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w)\nw : adaptation variable (init=0.0):\ntau_w * dw/dt = a * (v - v_rest) / 1000.0 - w\ng_exc : excitatory current (init = 0.0):\ntau_syn_E * dg_exc/dt = - g_exc\ng_inh : inhibitory current (init = 0.0):\ntau_syn_I * dg_inh/dt = - g_inh\nalpha_exc : alpha function of excitatory current (init = 0.0):\ntau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc\nalpha_inh: alpha function of inhibitory current (init = 0.0):\ntau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh\n\nSpike emission:\nv &gt; v_spike\nReset:\nv = v_reset\nu += b\nThe ODEs are solved using the explicit Euler method.\nEquivalent code:\n\nEIF_cond_alpha_isfa_ista = Neuron(\n    parameters = \"\"\"\n        v_rest = -70.6\n        cm = 0.281 \n        tau_m = 9.3667 \n        tau_syn_E = 5.0 \n        tau_syn_I = 5.0 \n        e_rev_E = 0.0 \n        e_rev_I = -80.0\n        tau_w = 144.0 \n        a = 4.0\n        b = 0.0805\n        i_offset = 0.0\n        delta_T = 2.0 \n        v_thresh = -50.4\n        v_reset = -70.6\n        v_spike = -40.0 \n    \"\"\", \n    equations = \"\"\"\n        gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)\n        gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)                \n        I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset\n        tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6\n        tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w \n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n        tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc  : exponential\n        tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh  : exponential\n    \"\"\",\n    spike = \"v &gt; v_spike\",\n    reset = \"\"\"\n        v = v_reset\n        w += b\n    \"\"\",\n    refractory = 0.1\n)",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "EIF_cond_alpha_isfa_ista"
    ]
  },
  {
    "objectID": "reference/Uniform.html",
    "href": "reference/Uniform.html",
    "title": "Uniform",
    "section": "",
    "text": "core.Random.Uniform(self, min, max)\nUniform distribution between min and max.\nThe returned values are floats in the range [min, max].\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmin\nfloat\nminimum value.\nrequired\n\n\nmax\nfloat\nmaximum value.\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.Uniform.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Uniform"
    ]
  },
  {
    "objectID": "reference/Uniform.html#parameters",
    "href": "reference/Uniform.html#parameters",
    "title": "Uniform",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmin\nfloat\nminimum value.\nrequired\n\n\nmax\nfloat\nmaximum value.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Uniform"
    ]
  },
  {
    "objectID": "reference/Uniform.html#methods",
    "href": "reference/Uniform.html#methods",
    "title": "Uniform",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.Uniform.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Uniform"
    ]
  },
  {
    "objectID": "reference/TimedArray.html",
    "href": "reference/TimedArray.html",
    "title": "TimedArray",
    "section": "",
    "text": "inputs.TimedArray.TimedArray(self, rates=None, geometry=None, schedule=0.0, period=-1.0, name=None, copied=False)\nData structure holding sequential inputs for a rate-coded network.\nThe input values are stored in the (recordable) attribute r, without any further processing. You will need to connect this population to another one using the connect_one_to_one() method.\nBy default, the firing rate of this population will iterate over the different values step by step:\ninputs = np.array(\n    [\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n    ]\n)\n\ninp = ann.TimedArray(rates=inputs)\n\npop = ann.Population(10, ...)\n\nproj = ann.Projection(inp, pop, 'exc')\nproj.connect_one_to_one(1.0)\n\nann.compile()\n\nann.simulate(10.)\nThis creates a population of 10 neurons whose activity will change during the first 10*dt milliseconds of the simulation. After that delay, the last input will be kept (i.e. 1 for the last neuron).\nIf you want the TimedArray to “loop” over the different input vectors, you can specify a period for the inputs:\ninp = ann.TimedArray(rates=inputs, period=10.)\nIf the period is smaller than the length of the rates, the last inputs will not be set.\nIf you do not want the inputs to be set at every step, but every 10 ms for example, youcan use the schedule argument:\ninp = ann.TimedArray(rates=inputs, schedule=10.)\nThe input [1, 0, 0,…] will stay for 10 ms, then[0, 1, 0, …] for the next 10 ms, etc…\nIf you need a less regular schedule, you can specify it as a list of times:\ninp = ann.TimedArray(rates=inputs, schedule=[10., 20., 50., 60., 100., 110.])\nThe first input is set at t = 10 ms (r = 0.0 in the first 10 ms), the second at t = 20 ms, the third at t = 50 ms, etc.\nIf you specify less times than in the array of rates, the last ones will be ignored.\nScheduling can be combined with periodic cycling. Note that you can use the reset() method to manually reinitialize the TimedArray, times becoming relative to that call:\nann.simulate(100.) # ten inputs are shown with a schedule of 10 ms\ninp.reset()\nann.simulate(100.) # the same ten inputs are presented again.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrates\nnumpy.numpy.ndarray\narray of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population.\nNone\n\n\ngeometry\nint | tuple\ndesired dimensions of the population. This argument will be considered if rates is None.\nNone\n\n\nschedule\nfloat\neither a single value or a list of time points where inputs should be set. Default: every timestep.\n0.0\n\n\nperiod\nfloat\ntime when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.).\n-1.0\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nupdate\nSet a new list of inputs. The first axis corresponds to time, the others to the desired dimensions of the population. Note, the\n\n\n\n\n\ninputs.TimedArray.TimedArray.update(rates, schedule=0.0, period=-1)\nSet a new list of inputs. The first axis corresponds to time, the others to the desired dimensions of the population. Note, the geometry is set during construction phase of the object.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrates\n\narray of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population.\nrequired\n\n\nschedule\n\neither a single value or a list of time points where inputs should be set. Default: every timestep.\n0.0\n\n\nperiod\n\ntime when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.).\n-1",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "TimedArray"
    ]
  },
  {
    "objectID": "reference/TimedArray.html#parameters",
    "href": "reference/TimedArray.html#parameters",
    "title": "TimedArray",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nrates\nnumpy.numpy.ndarray\narray of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population.\nNone\n\n\ngeometry\nint | tuple\ndesired dimensions of the population. This argument will be considered if rates is None.\nNone\n\n\nschedule\nfloat\neither a single value or a list of time points where inputs should be set. Default: every timestep.\n0.0\n\n\nperiod\nfloat\ntime when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.).\n-1.0",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "TimedArray"
    ]
  },
  {
    "objectID": "reference/TimedArray.html#methods",
    "href": "reference/TimedArray.html#methods",
    "title": "TimedArray",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nupdate\nSet a new list of inputs. The first axis corresponds to time, the others to the desired dimensions of the population. Note, the\n\n\n\n\n\ninputs.TimedArray.TimedArray.update(rates, schedule=0.0, period=-1)\nSet a new list of inputs. The first axis corresponds to time, the others to the desired dimensions of the population. Note, the geometry is set during construction phase of the object.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrates\n\narray of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population.\nrequired\n\n\nschedule\n\neither a single value or a list of time points where inputs should be set. Default: every timestep.\n0.0\n\n\nperiod\n\ntime when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.).\n-1",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "TimedArray"
    ]
  },
  {
    "objectID": "reference/VideoPopulation.html",
    "href": "reference/VideoPopulation.html",
    "title": "VideoPopulation",
    "section": "",
    "text": "extensions.image.ImagePopulation.VideoPopulation(self, geometry, opencv_version='4', name=None, copied=False)\nRate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel).\nThis extension requires the C++ library OpenCV &gt;= 4.0 (apt-get/yum install opencv). pkg-config opencv4 --cflags --libs should not return an error. vtk might additionally have to be installed.\nThe extensions has to be explicitly imported:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.image import VideoPopulation\n\npop = VideoPopulation(geometry=(480, 640))\n\ncompile()\n\npop.start_camera(0)\n\nwhile(True):\n    pop.grab_image()\n    simulate(10.0)\nAbout the geometry:\n\nIf the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image).\nIf the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color).\n\nIf the third dimension is 3, each will correspond to the RGB values of the pixels.\nWarning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple\npopulation geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized.\nrequired\n\n\nopencv_version\nstr\nOpenCV version (default=4).\n'4'\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ngrab_image\nGrabs one image from the camera and feeds it into the population.\n\n\nrelease\nReleases the camera:\n\n\nstart_camera\nStarts the webcam with the corresponding device (default = 0).\n\n\n\n\n\nextensions.image.ImagePopulation.VideoPopulation.grab_image()\nGrabs one image from the camera and feeds it into the population.\nThe camera must be first started with:\npop.start_camera(0)\n\n\n\nextensions.image.ImagePopulation.VideoPopulation.release()\nReleases the camera:\npop.release()\n\n\n\nextensions.image.ImagePopulation.VideoPopulation.start_camera(camera_port=0)\nStarts the webcam with the corresponding device (default = 0).\nOn linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc.",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "VideoPopulation"
    ]
  },
  {
    "objectID": "reference/VideoPopulation.html#parameters",
    "href": "reference/VideoPopulation.html#parameters",
    "title": "VideoPopulation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple\npopulation geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized.\nrequired\n\n\nopencv_version\nstr\nOpenCV version (default=4).\n'4'\n\n\nname\nstr\nunique name of the population (optional).\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "VideoPopulation"
    ]
  },
  {
    "objectID": "reference/VideoPopulation.html#methods",
    "href": "reference/VideoPopulation.html#methods",
    "title": "VideoPopulation",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ngrab_image\nGrabs one image from the camera and feeds it into the population.\n\n\nrelease\nReleases the camera:\n\n\nstart_camera\nStarts the webcam with the corresponding device (default = 0).\n\n\n\n\n\nextensions.image.ImagePopulation.VideoPopulation.grab_image()\nGrabs one image from the camera and feeds it into the population.\nThe camera must be first started with:\npop.start_camera(0)\n\n\n\nextensions.image.ImagePopulation.VideoPopulation.release()\nReleases the camera:\npop.release()\n\n\n\nextensions.image.ImagePopulation.VideoPopulation.start_camera(camera_port=0)\nStarts the webcam with the corresponding device (default = 0).\nOn linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc.",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "VideoPopulation"
    ]
  },
  {
    "objectID": "reference/add_function.html",
    "href": "reference/add_function.html",
    "title": "add_function",
    "section": "",
    "text": "core.Global.add_function(function)\nDefines a global function which can be used by all neurons and synapses.\nThe function must have only one return value and use only the passed arguments.\nExamples of valid functions:\nann.add_function('logistic(x) = 1 / (1 + exp(-x))')\n\nann.add_function('''\n    piecewise(x, a, b) = if x &lt; a:\n                            a\n                         else: \n                            if x &gt; b :\n                                b\n                            else:\n                                x\n''')\nPlease refer to the manual to know the allowed mathematical functions.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunction\nstr\n(multi)string representing the function.\nrequired",
    "crumbs": [
      "Reference",
      "**Functions and Constants**",
      "add_function"
    ]
  },
  {
    "objectID": "reference/add_function.html#parameters",
    "href": "reference/add_function.html#parameters",
    "title": "add_function",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfunction\nstr\n(multi)string representing the function.\nrequired",
    "crumbs": [
      "Reference",
      "**Functions and Constants**",
      "add_function"
    ]
  },
  {
    "objectID": "reference/parallel_run.html",
    "href": "reference/parallel_run.html",
    "title": "parallel_run",
    "section": "",
    "text": "core.Network.parallel_run(method, networks=None, number=0, max_processes=-1, measure_time=False, sequential=False, same_seed=False, annarchy_json='', visible_cores=[], **args)\nAllows to run multiple networks in parallel using multiprocessing.\nIf the networks argument is provided as a list of Network objects, the given method will be executed for each of these networks.\nIf number is given instead, the same number of networks will be created and the method is applied.\nIf number is used, the created networks are not returned, you should return what you need to analyse.\nExample:\npop1 = ann.PoissonPopulation(100, rates=10.0)\npop2 = ann.Population(100, ann.Izhikevich)\nproj = ann.Projection(pop1, pop2, 'exc')\nproj.connect_fixed_probability(weights=5.0, probability=0.2)\nm = ann.Monitor(pop2, 'spike')\n\nann.compile()\n\ndef simulation(idx, net):\n    net.get(pop1).rates = 10. * idx\n    net.simulate(1000.)\n    return net.get(m).raster_plot()\n\nresults = ann.parallel_run(method=simulation, number = 3)\n\nt1, n1 = results[0]\nt2, n2 = results[1]\nt3, n3 = results[2]\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmethod\n\na Python method which will be executed for each network. This function must accept an integer as first argument (id of the simulation) and a Network object as second argument.\nrequired\n\n\nnetworks\nlist\na list of networks to simulate in parallel.\nNone\n\n\nnumber\nint\nthe number of identical networks to run in parallel.\n0\n\n\nmax_processes\nint\nmaximal number of processes to start concurrently (default: the available number of cores on the machine).\n-1\n\n\nmeasure_time\nbool\nif the total simulation time should be printed out.\nFalse\n\n\nsequential\nbool\nif True, runs the simulations sequentially instead of in parallel (default: False).\nFalse\n\n\nsame_seed\nbool\nif True, all networks will use the same seed. If not, the seed will be randomly initialized with time(0) for each network (default). It has no influence when the networks argument is set (the seed has to be set individually for each network using net.set_seed()), only when number is used.\nFalse\n\n\nannarchy_json\nstr\npath to a different configuration file if needed (default ““).\n''\n\n\nvisible_cores\nlist\na list of CPU core ids to simulate on (must have max_processes entries and max_processes must be != -1)\n[]\n\n\nargs\n\nother named arguments you want to pass to the simulation method.\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\na list of the values returned by each call to method.",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "parallel_run"
    ]
  },
  {
    "objectID": "reference/parallel_run.html#parameters",
    "href": "reference/parallel_run.html#parameters",
    "title": "parallel_run",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmethod\n\na Python method which will be executed for each network. This function must accept an integer as first argument (id of the simulation) and a Network object as second argument.\nrequired\n\n\nnetworks\nlist\na list of networks to simulate in parallel.\nNone\n\n\nnumber\nint\nthe number of identical networks to run in parallel.\n0\n\n\nmax_processes\nint\nmaximal number of processes to start concurrently (default: the available number of cores on the machine).\n-1\n\n\nmeasure_time\nbool\nif the total simulation time should be printed out.\nFalse\n\n\nsequential\nbool\nif True, runs the simulations sequentially instead of in parallel (default: False).\nFalse\n\n\nsame_seed\nbool\nif True, all networks will use the same seed. If not, the seed will be randomly initialized with time(0) for each network (default). It has no influence when the networks argument is set (the seed has to be set individually for each network using net.set_seed()), only when number is used.\nFalse\n\n\nannarchy_json\nstr\npath to a different configuration file if needed (default ““).\n''\n\n\nvisible_cores\nlist\na list of CPU core ids to simulate on (must have max_processes entries and max_processes must be != -1)\n[]\n\n\nargs\n\nother named arguments you want to pass to the simulation method.\n{}",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "parallel_run"
    ]
  },
  {
    "objectID": "reference/parallel_run.html#returns",
    "href": "reference/parallel_run.html#returns",
    "title": "parallel_run",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nlist\na list of the values returned by each call to method.",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "parallel_run"
    ]
  },
  {
    "objectID": "reference/Gamma.html",
    "href": "reference/Gamma.html",
    "title": "Gamma",
    "section": "",
    "text": "core.Random.Gamma(self, alpha, beta=1.0, seed=-1, min=None, max=None)\nGamma distribution.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nalpha\nfloat\nShape of the gamma distribution.\nrequired\n\n\nbeta\nfloat\nScale of the gamma distribution.\n1.0\n\n\nmin\nfloat\nMinimum value returned (default: unlimited).\nNone\n\n\nmax\nfloat\nMaximum value returned (default: unlimited).\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.Gamma.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Gamma"
    ]
  },
  {
    "objectID": "reference/Gamma.html#parameters",
    "href": "reference/Gamma.html#parameters",
    "title": "Gamma",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nalpha\nfloat\nShape of the gamma distribution.\nrequired\n\n\nbeta\nfloat\nScale of the gamma distribution.\n1.0\n\n\nmin\nfloat\nMinimum value returned (default: unlimited).\nNone\n\n\nmax\nfloat\nMaximum value returned (default: unlimited).\nNone",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Gamma"
    ]
  },
  {
    "objectID": "reference/Gamma.html#methods",
    "href": "reference/Gamma.html#methods",
    "title": "Gamma",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.Gamma.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Gamma"
    ]
  },
  {
    "objectID": "reference/Izhikevich.html",
    "href": "reference/Izhikevich.html",
    "title": "Izhikevich",
    "section": "",
    "text": "models.Neurons.Izhikevich(self, a=0.02, b=0.2, c=-65.0, d=8.0, v_thresh=30.0, i_offset=0.0, noise=0.0, tau_refrac=0.0, conductance='g_exc - g_inh')\nIzhikevich quadratic spiking neuron.\n\nIzhikevich, E.M. (2003). Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks, 14:6. http://dx.doi.org/10.1109/TNN.2003.820440\n\nThe neural equations are:\n\\frac{dv}{dt} = 0.04 * v^2 + 5.0 * v + 140.0 - u + I\n\\frac{du}{dt} = a * (b * v - u)\nBy default, the conductance is “g_exc - g_inh”, but this can be changed by setting the conductance argument:\nneuron = ann.Izhikevich(conductance='g_ampa * (1 + g_nmda) - g_gaba')\nThe synapses are instantaneous, i.e the corresponding conductance is increased from the synaptic efficiency w at the time step when a spike is received.\nParameters:\n\na = 0.02 : Speed of the recovery variable\nb = 0.2: Scaling of the recovery variable\nc = -65.0 : Reset potential.\nd = 8.0 : Increment of the recovery variable after a spike.\nv_thresh = 30.0 : Spike threshold (mV).\ni_offset = 0.0 : external current (nA).\nnoise = 0.0 : Amplitude of the normal additive noise.\ntau_refrac = 0.0 : Duration of refractory period (ms).\n\nVariables:\n\nI : input current (user-defined conductance/current + external current + normal noise):\nI = conductance + i_offset + noise * Normal(0.0, 1.0)\nv : membrane potential in mV (init = c):\ndv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I\nu : recovery variable (init= b * c):\ndu/dt = a * (b * v - u)\n\nSpike emission:\nv &gt; v_thresh\nReset:\nv = c\nu += d \nThe ODEs are solved using the explicit Euler method.\nEquivalent code:\n\n    Izhikevich = ann.Neuron(\n        parameters = \"\"\"\n            noise = 0.0\n            a = 0.02\n            b = 0.2\n            c = -65.0\n            d = 8.0\n            v_thresh = 30.0\n            i_offset = 0.0\n        \"\"\", \n        equations = \"\"\"\n            I = g_exc - g_inh + noise * Normal(0.0, 1.0) + i_offset\n            dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init = -65.0\n            du/dt = a * (b*v - u) : init= -13.0\n        \"\"\",\n        spike = \"v &gt; v_thresh\",\n        reset = \"v = c; u += d\",\n        refractory = 0.0\n    )\nThe default parameters are for a regular spiking (RS) neuron derived from the above mentioned article.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nfloat\nSpeed of the recovery variable\n0.02\n\n\nb\nfloat\nScaling of the recovery variable\n0.2\n\n\nc\nfloat\nReset potential.\n-65.0\n\n\nd\nfloat\nIncrement of the recovery variable after a spike.\n8.0\n\n\nv_thresh\nfloat\nSpike threshold (mV).\n30.0\n\n\ni_offset\nfloat\nexternal current (nA).\n0.0\n\n\nnoise\nfloat\nAmplitude of the normal additive noise.\n0.0\n\n\ntau_refrac\nfloat\nDuration of refractory period (ms).\n0.0\n\n\nconductance\nstr\nConductances used as inputs.\n'g_exc - g_inh'",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "reference/Izhikevich.html#parameters",
    "href": "reference/Izhikevich.html#parameters",
    "title": "Izhikevich",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\na\nfloat\nSpeed of the recovery variable\n0.02\n\n\nb\nfloat\nScaling of the recovery variable\n0.2\n\n\nc\nfloat\nReset potential.\n-65.0\n\n\nd\nfloat\nIncrement of the recovery variable after a spike.\n8.0\n\n\nv_thresh\nfloat\nSpike threshold (mV).\n30.0\n\n\ni_offset\nfloat\nexternal current (nA).\n0.0\n\n\nnoise\nfloat\nAmplitude of the normal additive noise.\n0.0\n\n\ntau_refrac\nfloat\nDuration of refractory period (ms).\n0.0\n\n\nconductance\nstr\nConductances used as inputs.\n'g_exc - g_inh'",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "reference/Binomial.html",
    "href": "reference/Binomial.html",
    "title": "Binomial",
    "section": "",
    "text": "core.Random.Binomial(self, n, p)\nBinomial distribution.\nParameters: n trials and p probability of success where n an integer &gt;= 0 and p is in the interval [0,1].\nThe returned values are the number of successes over the n trials.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of trials.\nrequired\n\n\np\nfloat\nProbability of success.\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.Binomial.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Binomial"
    ]
  },
  {
    "objectID": "reference/Binomial.html#parameters",
    "href": "reference/Binomial.html#parameters",
    "title": "Binomial",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of trials.\nrequired\n\n\np\nfloat\nProbability of success.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Binomial"
    ]
  },
  {
    "objectID": "reference/Binomial.html#methods",
    "href": "reference/Binomial.html#methods",
    "title": "Binomial",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.Binomial.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Binomial"
    ]
  },
  {
    "objectID": "reference/enable_callbacks.html",
    "href": "reference/enable_callbacks.html",
    "title": "enable_callbacks",
    "section": "",
    "text": "enable_callbacks\ncore.Simulate.enable_callbacks(net_id=0)\nEnables all declared callbacks for the network.",
    "crumbs": [
      "Reference",
      "**Callbacks**",
      "enable_callbacks"
    ]
  },
  {
    "objectID": "reference/IF_curr_alpha.html",
    "href": "reference/IF_curr_alpha.html",
    "title": "IF_curr_alpha",
    "section": "",
    "text": "IF_curr_alpha\nmodels.Neurons.IF_curr_alpha(self, v_rest=-65.0, cm=1.0, tau_m=20.0, tau_refrac=0.0, tau_syn_E=5.0, tau_syn_I=5.0, v_thresh=-50.0, v_reset=-65.0, i_offset=0.0)\nLeaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents.\nSeparate synaptic currents for excitatory and inhibitory synapses.\nThe alpha currents are calculated through a system of two linears ODEs. After a spike is received at t_spike, it peaks at t_spike + tau_syn_X, with a maximum equal to the synaptic efficiency.\nParameters:\n\nv_rest = -65.0 : Resting membrane potential (mV)\ncm = 1.0 : Capacity of the membrane (nF)\ntau_m = 20.0 : Membrane time constant (ms)\ntau_refrac = 0.0 : Duration of refractory period (ms)\ntau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms)\ntau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms)\ni_offset = 0.0 : Offset current (nA)\nv_reset = -65.0 : Reset potential after a spike (mV)\nv_thresh = -50.0 : Spike threshold (mV)\n\nVariables:\n\nv : membrane potential in mV (init=-65.0):\ncm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset\ng_exc : excitatory current (init = 0.0):\ntau_syn_E * dg_exc/dt = - g_exc\nalpha_exc : alpha function of excitatory current (init = 0.0):\ntau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc\ng_inh : inhibitory current (init = 0.0):\ntau_syn_I * dg_inh/dt = - g_inh\nalpha_inh : alpha function of inhibitory current (init = 0.0):\ntau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh\n\nSpike emission:\nv &gt; v_thresh\nReset:\nv = v_reset\nThe ODEs are solved using the exponential Euler method.\nEquivalent code:\n\nIF_curr_alpha = Neuron(\n    parameters = \"\"\"\n        v_rest = -65.0\n        cm  = 1.0\n        tau_m  = 20.0\n        tau_syn_E = 5.0\n        tau_syn_I = 5.0\n        v_thresh = -50.0\n        v_reset = -65.0\n        i_offset = 0.0\n    \"\"\", \n    equations = \"\"\"\n        gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)\n        gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)  \n        cm * dv/dt = cm/tau_m*(v_rest -v)   + alpha_exc - alpha_inh + i_offset : exponential, init=-65.0\n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n        tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc  : exponential\n        tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh  : exponential\n    \"\"\",\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = v_reset\",\n    refractory = 0.0\n)",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_curr_alpha"
    ]
  },
  {
    "objectID": "reference/set_seed.html",
    "href": "reference/set_seed.html",
    "title": "set_seed",
    "section": "",
    "text": "core.Global.set_seed(seed, use_seed_seq=True, net_id=0)\nSets the seed of the random number generators, both in numpy.random and in the C++ library when it is created.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nseed\nint\ninteger value used to seed the C++ and Numpy RNG\nrequired\n\n\nuse_seed_seq\nbool\nfor openMP and parallel RNGs, we use either the STL SeedSequence (True, default) or a specialized implementation proposed by Melissa O’Neil (False, see _optimization_flags for more details).\nTrue",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "set_seed"
    ]
  },
  {
    "objectID": "reference/set_seed.html#parameters",
    "href": "reference/set_seed.html#parameters",
    "title": "set_seed",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nseed\nint\ninteger value used to seed the C++ and Numpy RNG\nrequired\n\n\nuse_seed_seq\nbool\nfor openMP and parallel RNGs, we use either the STL SeedSequence (True, default) or a specialized implementation proposed by Melissa O’Neil (False, see _optimization_flags for more details).\nTrue",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "set_seed"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Reference",
    "section": "",
    "text": "Basic objects that should be used to create a network.\n\n\n\nPopulation\nStructure for a population of homogeneous neurons.\n\n\nProjection\nStructure holding all synapses of the same type between two populations.\n\n\nNeuron\nBase class to define a neuron.\n\n\nSynapse\nBase class to define a synapse.\n\n\nMonitor\nMonitoring class allowing to record easily parameters or variables from Population, PopulationView, Dendrite or Projection objects.\n\n\nDendrite\nA Dendrite is a sub-group of a Projection, gathering the synapses between the pre-synaptic population and a single post-synaptic neuron.\n\n\nNetwork\nA network gathers already defined populations, projections and monitors in order to run them independently.\n\n\n\n\n\n\nConfiguration and compilation of the network.\n\n\n\nsetup\nThe setup function is used to configure ANNarchy simulation environment. It takes various optional arguments:\n\n\ncompile\nThis method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation.\n\n\nclear\nClears all variables (erasing already defined populations, projections, monitors and constants), as if you had just imported ANNarchy.\n\n\nreset\nReinitialises the network to its state before the call to compile. The network time will be set to 0ms.\n\n\nset_seed\nSets the seed of the random number generators, both in numpy.random and in the C++ library when it is created.\n\n\nget_population\nReturns the population with the given name.\n\n\nget_projection\nReturns the projection with the given name.\n\n\npopulations\nReturns a list of all declared populations.\n\n\nprojections\nReturns a list of all declared populations. By default, the method returns all connections which were defined.\n\n\n\n\n\n\nMethods to control the simulation.\n\n\n\nsimulate\nSimulates the network for the given duration in milliseconds.\n\n\nsimulate_until\nRuns the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped.\n\n\nstep\nPerforms a single simulation step (duration = dt).\n\n\nparallel_run\nAllows to run multiple networks in parallel using multiprocessing.\n\n\nenable_learning\nEnables learning for all projections. Optionally period and offset can be changed for all projections.\n\n\ndisable_learning\nDisables learning for all projections.\n\n\nget_time\nReturns the current time in ms.\n\n\nset_time\nSets the current time in ms.\n\n\nget_current_step\nReturns the current simulation step.\n\n\nset_current_step\nSets the current simulation step (integer).\n\n\ndt\nReturns the simulation step size dt used in the simulation.\n\n\n\n\n\n\nDefault neuron models that can be used directly. The naming follows the PyNN convention.\n\n\n\nLeakyIntegrator\nLeaky-integrator rate-coded neuron, optionally noisy.\n\n\nIzhikevich\nIzhikevich quadratic spiking neuron.\n\n\nIF_curr_exp\nLeaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current.\n\n\nIF_cond_exp\nLeaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance.\n\n\nIF_curr_alpha\nLeaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents.\n\n\nIF_cond_alpha\nLeaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance.\n\n\nHH_cond_exp\nSingle-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub.\n\n\nEIF_cond_alpha_isfa_ista\nExponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.).\n\n\nEIF_cond_exp_isfa_ista\nExponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.).\n\n\n\n\n\n\nDefault synapse models that can be used directly.\n\n\n\nSTP\nSynapse exhibiting short-term facilitation and depression.\n\n\nSTDP\nSpike-timing dependent plasticity, online version.\n\n\nHebb\nRate-coded synapse with Hebbian plasticity.\n\n\nOja\nRate-coded synapse with regularized Hebbian plasticity (Oja).\n\n\nIBCM\nRate-coded synapse with Intrator & Cooper (1992) plasticity.\n\n\n\n\n\n\nInput populations that can be used to stimulate the networks.\n\n\n\nInputArray\nPopulation holding static inputs for a rate-coded network.\n\n\nTimedArray\nData structure holding sequential inputs for a rate-coded network.\n\n\nPoissonPopulation\nPopulation of spiking neurons following a Poisson distribution.\n\n\nTimedPoissonPopulation\nPoisson population whose rate vary with the provided schedule.\n\n\nSpikeSourceArray\nSpike source generating spikes at the times given in the spike_times array.\n\n\nHomogeneousCorrelatedSpikeTrains\nPopulation of spiking neurons following a homogeneous distribution with correlated spike trains.\n\n\nCurrentInjection\nInject current from a rate-coded population into a spiking population.\n\n\nDecodingProjection\nDecoding projection to transform spike trains into firing rates.\n\n\nImagePopulation\nRate-coded Population allowing to represent images (png, jpg…) as the firing rate of a population (each neuron represents one pixel).\n\n\nVideoPopulation\nRate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel).\n\n\n\n\n\n\nSaving and loading methods for the network.\n\n\n\nsave\nSave the current network state (parameters and variables) to a file.\n\n\nload\nLoads a saved state of the network.\n\n\nsave_parameters\nSaves the global parameters of a network (flag population for neurons, projection for synapses) to a JSON file.\n\n\nload_parameters\nLoads the global parameters of a network (flag population for neurons, projection for synapses) from a JSON file.\n\n\n\n\n\n\nVarious additional utilities.\n\n\n\nreport\nGenerates a report describing the network.\n\n\n\n\n\n\nRandom distributions that can be used to generate numpy arrays.\n\n\n\nUniform\nUniform distribution between min and max.\n\n\nDiscreteUniform\nDiscrete uniform distribution between min and max.\n\n\nNormal\nNormal distribution.\n\n\nLogNormal\nLog-normal distribution.\n\n\nExponential\nExponential distribution, according to the density function:\n\n\nGamma\nGamma distribution.\n\n\nBinomial\nBinomial distribution.\n\n\n\n\n\n\nFunctions and constants declared at the global level.\n\n\n\nadd_function\nDefines a global function which can be used by all neurons and synapses.\n\n\nfunctions\nAllows to access a global function declared with add_function and use it from Python using arrays after compilation.\n\n\nConstant\nConstant parameter that can be used by all neurons and synapses.\n\n\nlist_constants\nReturns a list of all constants declared with Constant(name, value).\n\n\nget_constant\nReturns the Constant object with the given name, None otherwise.\n\n\n\n\n\n\nPlotting methods from Monitorthat can be used without the Monitor object, i.e. offline.\n\n\n\nraster_plot\nReturns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.\n\n\nhistogram\nReturns a histogram for the recorded spikes in the population.\n\n\ninter_spike_interval\nComputes the inter-spike interval (ISI) for the recorded spike events of a population.\n\n\ncoefficient_of_variation\nComputes the coefficient of variation of the inter-spike intervals for the recorded spike events of a population.\n\n\npopulation_rate\nTakes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons.\n\n\nsmoothed_rate\nComputes the smoothed firing rate of the recorded spiking neurons.\n\n\n\n\n\n\nDecorator used to interrupt the simulation at the desired time.\n@every(period=100., offset=-10.)\ndef step_input(n):\n    pop.I = float(n) / 100.\n\nsimulate(10000.)\n\n\n\nevery\nDecorator to declare a callback method that will be called periodically during the simulation.\n\n\ncallbacks_enabled\nReturns True if callbacks are enabled for the network.\n\n\ndisable_callbacks\nDisables all callbacks for the network.\n\n\nenable_callbacks\nEnables all declared callbacks for the network.\n\n\nclear_all_callbacks\nClears the list of declared callbacks for the network.\n\n\n\n\n\n\nExtension for convolution and pooling projections. The extension has to be explicitly imported:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.convolution import *\n\n\n\nConvolution\nPerforms a convolution of a weight kernel on the pre-synaptic population.\n\n\nPooling\nPerforms a pooling operation (e.g. max.pooling) on the pre-synaptic population.\n\n\nTranspose\nTransposed projection reusing the weights of an already-defined rate-coded projection.\n\n\nCopy\nCreates a virtual projection reusing the weights and delays of an already-defined projection.\n\n\n\n\n\n\nExtension for monitoring BOLD signals in a population.\nimport ANNarchy as ann\nfrom ANNarchy.extensions.bold import *\n\n\n\nBoldMonitor\nMonitors the BOLD signal for several populations using a computational model.\n\n\nBoldModel\nBase class to define a BOLD model to be used in a BOLD monitor.\n\n\nballoon_RN\nA balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_RL\nA balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_CN\nA balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_CL\nA balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_maith2021\nThe balloon model as used in Maith et al. (2021).\n\n\nballoon_two_inputs\nBOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007).\n\n\n\n\n\n\nLogging utilities based on tensorboard are provided in the module ANNarchy.extensions.tensorboard, which must be explicitly imported:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.tensorboard import Logger\nThe main object is the Logger class.\n\n\n\nLogger\nLogger class to use tensorboard to visualize ANNarchy simulations. Requires the tensorboardX package (pip install tensorboardX).\n\n\n\n\n\n\nModule allowing to convert an ANN trained with keras into a spiking neural network:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.ann_to_snn_conversion import ANNtoSNNConverter\n\n\n\nANNtoSNNConverter\nConverts a pre-trained Keras model into a spiking neural network.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#core-components",
    "href": "reference/index.html#core-components",
    "title": "Reference",
    "section": "",
    "text": "Basic objects that should be used to create a network.\n\n\n\nPopulation\nStructure for a population of homogeneous neurons.\n\n\nProjection\nStructure holding all synapses of the same type between two populations.\n\n\nNeuron\nBase class to define a neuron.\n\n\nSynapse\nBase class to define a synapse.\n\n\nMonitor\nMonitoring class allowing to record easily parameters or variables from Population, PopulationView, Dendrite or Projection objects.\n\n\nDendrite\nA Dendrite is a sub-group of a Projection, gathering the synapses between the pre-synaptic population and a single post-synaptic neuron.\n\n\nNetwork\nA network gathers already defined populations, projections and monitors in order to run them independently.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#configuration",
    "href": "reference/index.html#configuration",
    "title": "Reference",
    "section": "",
    "text": "Configuration and compilation of the network.\n\n\n\nsetup\nThe setup function is used to configure ANNarchy simulation environment. It takes various optional arguments:\n\n\ncompile\nThis method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation.\n\n\nclear\nClears all variables (erasing already defined populations, projections, monitors and constants), as if you had just imported ANNarchy.\n\n\nreset\nReinitialises the network to its state before the call to compile. The network time will be set to 0ms.\n\n\nset_seed\nSets the seed of the random number generators, both in numpy.random and in the C++ library when it is created.\n\n\nget_population\nReturns the population with the given name.\n\n\nget_projection\nReturns the projection with the given name.\n\n\npopulations\nReturns a list of all declared populations.\n\n\nprojections\nReturns a list of all declared populations. By default, the method returns all connections which were defined.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#simulation",
    "href": "reference/index.html#simulation",
    "title": "Reference",
    "section": "",
    "text": "Methods to control the simulation.\n\n\n\nsimulate\nSimulates the network for the given duration in milliseconds.\n\n\nsimulate_until\nRuns the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped.\n\n\nstep\nPerforms a single simulation step (duration = dt).\n\n\nparallel_run\nAllows to run multiple networks in parallel using multiprocessing.\n\n\nenable_learning\nEnables learning for all projections. Optionally period and offset can be changed for all projections.\n\n\ndisable_learning\nDisables learning for all projections.\n\n\nget_time\nReturns the current time in ms.\n\n\nset_time\nSets the current time in ms.\n\n\nget_current_step\nReturns the current simulation step.\n\n\nset_current_step\nSets the current simulation step (integer).\n\n\ndt\nReturns the simulation step size dt used in the simulation.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#neuron-models",
    "href": "reference/index.html#neuron-models",
    "title": "Reference",
    "section": "",
    "text": "Default neuron models that can be used directly. The naming follows the PyNN convention.\n\n\n\nLeakyIntegrator\nLeaky-integrator rate-coded neuron, optionally noisy.\n\n\nIzhikevich\nIzhikevich quadratic spiking neuron.\n\n\nIF_curr_exp\nLeaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current.\n\n\nIF_cond_exp\nLeaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance.\n\n\nIF_curr_alpha\nLeaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents.\n\n\nIF_cond_alpha\nLeaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance.\n\n\nHH_cond_exp\nSingle-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub.\n\n\nEIF_cond_alpha_isfa_ista\nExponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.).\n\n\nEIF_cond_exp_isfa_ista\nExponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.).",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#synapse-models",
    "href": "reference/index.html#synapse-models",
    "title": "Reference",
    "section": "",
    "text": "Default synapse models that can be used directly.\n\n\n\nSTP\nSynapse exhibiting short-term facilitation and depression.\n\n\nSTDP\nSpike-timing dependent plasticity, online version.\n\n\nHebb\nRate-coded synapse with Hebbian plasticity.\n\n\nOja\nRate-coded synapse with regularized Hebbian plasticity (Oja).\n\n\nIBCM\nRate-coded synapse with Intrator & Cooper (1992) plasticity.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#inputs",
    "href": "reference/index.html#inputs",
    "title": "Reference",
    "section": "",
    "text": "Input populations that can be used to stimulate the networks.\n\n\n\nInputArray\nPopulation holding static inputs for a rate-coded network.\n\n\nTimedArray\nData structure holding sequential inputs for a rate-coded network.\n\n\nPoissonPopulation\nPopulation of spiking neurons following a Poisson distribution.\n\n\nTimedPoissonPopulation\nPoisson population whose rate vary with the provided schedule.\n\n\nSpikeSourceArray\nSpike source generating spikes at the times given in the spike_times array.\n\n\nHomogeneousCorrelatedSpikeTrains\nPopulation of spiking neurons following a homogeneous distribution with correlated spike trains.\n\n\nCurrentInjection\nInject current from a rate-coded population into a spiking population.\n\n\nDecodingProjection\nDecoding projection to transform spike trains into firing rates.\n\n\nImagePopulation\nRate-coded Population allowing to represent images (png, jpg…) as the firing rate of a population (each neuron represents one pixel).\n\n\nVideoPopulation\nRate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel).",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#io",
    "href": "reference/index.html#io",
    "title": "Reference",
    "section": "",
    "text": "Saving and loading methods for the network.\n\n\n\nsave\nSave the current network state (parameters and variables) to a file.\n\n\nload\nLoads a saved state of the network.\n\n\nsave_parameters\nSaves the global parameters of a network (flag population for neurons, projection for synapses) to a JSON file.\n\n\nload_parameters\nLoads the global parameters of a network (flag population for neurons, projection for synapses) from a JSON file.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#utilities",
    "href": "reference/index.html#utilities",
    "title": "Reference",
    "section": "",
    "text": "Various additional utilities.\n\n\n\nreport\nGenerates a report describing the network.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#random-distributions",
    "href": "reference/index.html#random-distributions",
    "title": "Reference",
    "section": "",
    "text": "Random distributions that can be used to generate numpy arrays.\n\n\n\nUniform\nUniform distribution between min and max.\n\n\nDiscreteUniform\nDiscrete uniform distribution between min and max.\n\n\nNormal\nNormal distribution.\n\n\nLogNormal\nLog-normal distribution.\n\n\nExponential\nExponential distribution, according to the density function:\n\n\nGamma\nGamma distribution.\n\n\nBinomial\nBinomial distribution.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#functions-and-constants",
    "href": "reference/index.html#functions-and-constants",
    "title": "Reference",
    "section": "",
    "text": "Functions and constants declared at the global level.\n\n\n\nadd_function\nDefines a global function which can be used by all neurons and synapses.\n\n\nfunctions\nAllows to access a global function declared with add_function and use it from Python using arrays after compilation.\n\n\nConstant\nConstant parameter that can be used by all neurons and synapses.\n\n\nlist_constants\nReturns a list of all constants declared with Constant(name, value).\n\n\nget_constant\nReturns the Constant object with the given name, None otherwise.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#plotting",
    "href": "reference/index.html#plotting",
    "title": "Reference",
    "section": "",
    "text": "Plotting methods from Monitorthat can be used without the Monitor object, i.e. offline.\n\n\n\nraster_plot\nReturns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.\n\n\nhistogram\nReturns a histogram for the recorded spikes in the population.\n\n\ninter_spike_interval\nComputes the inter-spike interval (ISI) for the recorded spike events of a population.\n\n\ncoefficient_of_variation\nComputes the coefficient of variation of the inter-spike intervals for the recorded spike events of a population.\n\n\npopulation_rate\nTakes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons.\n\n\nsmoothed_rate\nComputes the smoothed firing rate of the recorded spiking neurons.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#callbacks",
    "href": "reference/index.html#callbacks",
    "title": "Reference",
    "section": "",
    "text": "Decorator used to interrupt the simulation at the desired time.\n@every(period=100., offset=-10.)\ndef step_input(n):\n    pop.I = float(n) / 100.\n\nsimulate(10000.)\n\n\n\nevery\nDecorator to declare a callback method that will be called periodically during the simulation.\n\n\ncallbacks_enabled\nReturns True if callbacks are enabled for the network.\n\n\ndisable_callbacks\nDisables all callbacks for the network.\n\n\nenable_callbacks\nEnables all declared callbacks for the network.\n\n\nclear_all_callbacks\nClears the list of declared callbacks for the network.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#convolution",
    "href": "reference/index.html#convolution",
    "title": "Reference",
    "section": "",
    "text": "Extension for convolution and pooling projections. The extension has to be explicitly imported:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.convolution import *\n\n\n\nConvolution\nPerforms a convolution of a weight kernel on the pre-synaptic population.\n\n\nPooling\nPerforms a pooling operation (e.g. max.pooling) on the pre-synaptic population.\n\n\nTranspose\nTransposed projection reusing the weights of an already-defined rate-coded projection.\n\n\nCopy\nCreates a virtual projection reusing the weights and delays of an already-defined projection.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#bold-monitoring",
    "href": "reference/index.html#bold-monitoring",
    "title": "Reference",
    "section": "",
    "text": "Extension for monitoring BOLD signals in a population.\nimport ANNarchy as ann\nfrom ANNarchy.extensions.bold import *\n\n\n\nBoldMonitor\nMonitors the BOLD signal for several populations using a computational model.\n\n\nBoldModel\nBase class to define a BOLD model to be used in a BOLD monitor.\n\n\nballoon_RN\nA balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_RL\nA balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_CN\nA balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_CL\nA balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007).\n\n\nballoon_maith2021\nThe balloon model as used in Maith et al. (2021).\n\n\nballoon_two_inputs\nBOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007).",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#tensorboard-logging",
    "href": "reference/index.html#tensorboard-logging",
    "title": "Reference",
    "section": "",
    "text": "Logging utilities based on tensorboard are provided in the module ANNarchy.extensions.tensorboard, which must be explicitly imported:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.tensorboard import Logger\nThe main object is the Logger class.\n\n\n\nLogger\nLogger class to use tensorboard to visualize ANNarchy simulations. Requires the tensorboardX package (pip install tensorboardX).",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/index.html#ann-to-snn-conversion",
    "href": "reference/index.html#ann-to-snn-conversion",
    "title": "Reference",
    "section": "",
    "text": "Module allowing to convert an ANN trained with keras into a spiking neural network:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.ann_to_snn_conversion import ANNtoSNNConverter\n\n\n\nANNtoSNNConverter\nConverts a pre-trained Keras model into a spiking neural network.",
    "crumbs": [
      "Reference",
      "**Reference**"
    ]
  },
  {
    "objectID": "reference/HomogeneousCorrelatedSpikeTrains.html",
    "href": "reference/HomogeneousCorrelatedSpikeTrains.html",
    "title": "HomogeneousCorrelatedSpikeTrains",
    "section": "",
    "text": "inputs.SpikeTrains.HomogeneousCorrelatedSpikeTrains(self, geometry, rates, corr, tau, schedule=None, period=-1.0, name=None, refractory=None, copied=False)\nPopulation of spiking neurons following a homogeneous distribution with correlated spike trains.\nThe method describing the generation of homogeneous correlated spike trains is described in:\n\nBrette, R. (2009). Generation of correlated spike trains. Neural Computation 21(1). http://romainbrette.fr/WordPress3/wp-content/uploads/2014/06/Brette2008NC.pdf\n\nThe implementation is based on the one provided by Brian http://briansimulator.org.\nTo generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation:\n\\frac{dx}{dt} = \\frac{\\mu - x}{\\tau} + \\sigma \\, \\frac{\\xi}{\\sqrt{\\tau}}\nwhere \\xi is a random sample from the standard normal distribution. In short, x will randomly vary around \\mu over time, with an amplitude determined by \\sigma and a speed determined by \\tau.\nThis doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process.\nTo avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates, the desired correlation strength corr and the time constant tau. See Brette’s paper for details.\nIn short, you should only define the parameters rates, corr and tau, and let the class compute mu and sigma for you. Changing rates, corr or tau after initialization automatically recomputes mu and sigma.\nExample:\nimport ANNarchy as ann\nann.setup(dt=0.1)\n\npop_corr = ann.HomogeneousCorrelatedSpikeTrains(200, rates=10., corr=0.3, tau=10.)\n\nann.compile()\n\nann.simulate(1000.)\n\npop_corr.rates=30.\n\nann.simulate(1000.)\nAlternatively, a schedule can be provided to change automatically the value of rates and corr (but not tau) at the required times (as in TimedArray or TimedPoissonPopulation):\npop_corr = ann.HomogeneousCorrelatedSpikeTrains(\n    geometry=200, \n    rates= [10., 30.], \n    corr=[0.3, 0.5], \n    tau=10.,\n    schedule=[0., 1000.]\n)\n\nann.compile()\n\nann.simulate(2000.)\nEven when using a schedule, corr accepts a single constant value. The first value of schedule must be 0. period specifies when the schedule “loops” back to its initial value.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\nint | tuple[int]\npopulation geometry as tuple.\nrequired\n\n\nrates\nfloat | list[float]\nrate in Hz of the population (must be a positive float or a list)\nrequired\n\n\ncorr\nfloat | list[float]\ntotal correlation strength (float in [0, 1], or a list)\nrequired\n\n\ntau\nfloat\ncorrelation time constant in ms.\nrequired\n\n\nschedule\nlist[float]\nlist of times where new values of ratesand corrwill be used to computre mu and sigma.\nNone\n\n\nperiod\nfloat\ntime when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.)\n-1.0\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\nrefractory\nfloat\nrefractory period in ms (careful: may break the correlation)\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "HomogeneousCorrelatedSpikeTrains"
    ]
  },
  {
    "objectID": "reference/HomogeneousCorrelatedSpikeTrains.html#parameters",
    "href": "reference/HomogeneousCorrelatedSpikeTrains.html#parameters",
    "title": "HomogeneousCorrelatedSpikeTrains",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngeometry\nint | tuple[int]\npopulation geometry as tuple.\nrequired\n\n\nrates\nfloat | list[float]\nrate in Hz of the population (must be a positive float or a list)\nrequired\n\n\ncorr\nfloat | list[float]\ntotal correlation strength (float in [0, 1], or a list)\nrequired\n\n\ntau\nfloat\ncorrelation time constant in ms.\nrequired\n\n\nschedule\nlist[float]\nlist of times where new values of ratesand corrwill be used to computre mu and sigma.\nNone\n\n\nperiod\nfloat\ntime when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.)\n-1.0\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\nrefractory\nfloat\nrefractory period in ms (careful: may break the correlation)\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "HomogeneousCorrelatedSpikeTrains"
    ]
  },
  {
    "objectID": "reference/LogNormal.html",
    "href": "reference/LogNormal.html",
    "title": "LogNormal",
    "section": "",
    "text": "core.Random.LogNormal(self, mu, sigma, min=None, max=None)\nLog-normal distribution.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmu\nfloat\nMean of the distribution.\nrequired\n\n\nsigma\nfloat\nStandard deviation of the distribution.\nrequired\n\n\nmin\nfloat\nMinimum value (default: unlimited).\nNone\n\n\nmax\nfloat\nMaximum value (default: unlimited).\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.LogNormal.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "LogNormal"
    ]
  },
  {
    "objectID": "reference/LogNormal.html#parameters",
    "href": "reference/LogNormal.html#parameters",
    "title": "LogNormal",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmu\nfloat\nMean of the distribution.\nrequired\n\n\nsigma\nfloat\nStandard deviation of the distribution.\nrequired\n\n\nmin\nfloat\nMinimum value (default: unlimited).\nNone\n\n\nmax\nfloat\nMaximum value (default: unlimited).\nNone",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "LogNormal"
    ]
  },
  {
    "objectID": "reference/LogNormal.html#methods",
    "href": "reference/LogNormal.html#methods",
    "title": "LogNormal",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.LogNormal.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "LogNormal"
    ]
  },
  {
    "objectID": "reference/Dendrite.html",
    "href": "reference/Dendrite.html",
    "title": "Dendrite",
    "section": "",
    "text": "core.Dendrite.Dendrite(self, proj, post_rank, idx)\nA Dendrite is a sub-group of a Projection, gathering the synapses between the pre-synaptic population and a single post-synaptic neuron.\nIt can not be created directly, only through a call to Projection.dendrite(rank):\ndendrite = proj.dendrite(6)\n\n\n\n\n\nName\nDescription\n\n\n\n\npre_ranks\nList of ranks of pre-synaptic neurons.\n\n\nsize\nNumber of synapses.\n\n\nsynapses\nIteratively returns the synapses corresponding to this dendrite.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncreate_synapse\nCreates a synapse for this dendrite with the given pre-synaptic neuron.\n\n\ncreate_synapses\nCreates a synapse for this dendrite with the given pre-synaptic neurons.\n\n\nget\nReturns the value of a variable/parameter.\n\n\nprune_synapse\nRemoves the synapse with the given pre-synaptic neuron from the dendrite.\n\n\nprune_synapses\nRemoves the synapses which belong to the provided pre-synaptic neurons from the dendrite.\n\n\nreceptive_field\nReturns the given variable as a receptive field.\n\n\nset\nSets the value of a parameter/variable of all synapses.\n\n\nsynapse\nReturns the synapse coming from the corresponding presynaptic neuron.\n\n\n\n\n\ncore.Dendrite.Dendrite.create_synapse(rank, w=0.0, delay=0)\nCreates a synapse for this dendrite with the given pre-synaptic neuron.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the pre-synaptic neuron\nrequired\n\n\nw\nfloat\nsynaptic weight.\n0.0\n\n\ndelay\nfloat\nsynaptic delay.\n0\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.create_synapses(ranks, weights=None, delays=None)\nCreates a synapse for this dendrite with the given pre-synaptic neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nranks\nlist[int]\nlist of ranks of the pre-synaptic neurons.\nrequired\n\n\nweights\nlist[float]\nlist of synaptic weights (default: 0.0).\nNone\n\n\ndelays\nlist[float]\nlist of synaptic delays (default = dt).\nNone\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.get(name)\nReturns the value of a variable/parameter.\nExample:\ndendrite.get('w')\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the parameter/variable.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\na single value.\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.prune_synapse(rank)\nRemoves the synapse with the given pre-synaptic neuron from the dendrite.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the pre-synaptic neuron\nrequired\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.prune_synapses(ranks)\nRemoves the synapses which belong to the provided pre-synaptic neurons from the dendrite.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nranks\nlist[int]\nlist of ranks of the pre-synaptic neurons.\nrequired\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.receptive_field(variable='w', fill=0.0)\nReturns the given variable as a receptive field.\nA Numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value fill).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr\nname of the variable (default = ‘w’)\n'w'\n\n\nfill\nfloat\nvalue to use when a synapse does not exist (default: 0.0).\n0.0\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.array\nan array.\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.set(value)\nSets the value of a parameter/variable of all synapses.\nExample:\ndendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } )\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\ndict\na dictionary containing the parameter/variable names as keys.\nrequired\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.synapse(pos)\nReturns the synapse coming from the corresponding presynaptic neuron.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npos\nint | tuple[int]\ncan be either the rank or the coordinates of the presynaptic neuron\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nANNarchy.core.Dendrite.IndividualSynapse\nIndividualSynapse wrapper instance.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Dendrite"
    ]
  },
  {
    "objectID": "reference/Dendrite.html#attributes",
    "href": "reference/Dendrite.html#attributes",
    "title": "Dendrite",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npre_ranks\nList of ranks of pre-synaptic neurons.\n\n\nsize\nNumber of synapses.\n\n\nsynapses\nIteratively returns the synapses corresponding to this dendrite.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Dendrite"
    ]
  },
  {
    "objectID": "reference/Dendrite.html#methods",
    "href": "reference/Dendrite.html#methods",
    "title": "Dendrite",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncreate_synapse\nCreates a synapse for this dendrite with the given pre-synaptic neuron.\n\n\ncreate_synapses\nCreates a synapse for this dendrite with the given pre-synaptic neurons.\n\n\nget\nReturns the value of a variable/parameter.\n\n\nprune_synapse\nRemoves the synapse with the given pre-synaptic neuron from the dendrite.\n\n\nprune_synapses\nRemoves the synapses which belong to the provided pre-synaptic neurons from the dendrite.\n\n\nreceptive_field\nReturns the given variable as a receptive field.\n\n\nset\nSets the value of a parameter/variable of all synapses.\n\n\nsynapse\nReturns the synapse coming from the corresponding presynaptic neuron.\n\n\n\n\n\ncore.Dendrite.Dendrite.create_synapse(rank, w=0.0, delay=0)\nCreates a synapse for this dendrite with the given pre-synaptic neuron.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the pre-synaptic neuron\nrequired\n\n\nw\nfloat\nsynaptic weight.\n0.0\n\n\ndelay\nfloat\nsynaptic delay.\n0\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.create_synapses(ranks, weights=None, delays=None)\nCreates a synapse for this dendrite with the given pre-synaptic neurons.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nranks\nlist[int]\nlist of ranks of the pre-synaptic neurons.\nrequired\n\n\nweights\nlist[float]\nlist of synaptic weights (default: 0.0).\nNone\n\n\ndelays\nlist[float]\nlist of synaptic delays (default = dt).\nNone\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.get(name)\nReturns the value of a variable/parameter.\nExample:\ndendrite.get('w')\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the parameter/variable.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\na single value.\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.prune_synapse(rank)\nRemoves the synapse with the given pre-synaptic neuron from the dendrite.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nrank of the pre-synaptic neuron\nrequired\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.prune_synapses(ranks)\nRemoves the synapses which belong to the provided pre-synaptic neurons from the dendrite.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nranks\nlist[int]\nlist of ranks of the pre-synaptic neurons.\nrequired\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.receptive_field(variable='w', fill=0.0)\nReturns the given variable as a receptive field.\nA Numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value fill).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr\nname of the variable (default = ‘w’)\n'w'\n\n\nfill\nfloat\nvalue to use when a synapse does not exist (default: 0.0).\n0.0\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.array\nan array.\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.set(value)\nSets the value of a parameter/variable of all synapses.\nExample:\ndendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } )\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\ndict\na dictionary containing the parameter/variable names as keys.\nrequired\n\n\n\n\n\n\n\ncore.Dendrite.Dendrite.synapse(pos)\nReturns the synapse coming from the corresponding presynaptic neuron.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npos\nint | tuple[int]\ncan be either the rank or the coordinates of the presynaptic neuron\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nANNarchy.core.Dendrite.IndividualSynapse\nIndividualSynapse wrapper instance.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Dendrite"
    ]
  },
  {
    "objectID": "reference/InputArray.html",
    "href": "reference/InputArray.html",
    "title": "InputArray",
    "section": "",
    "text": "inputs.InputArray.InputArray(self, geometry=None, name=None, copied=False)\nPopulation holding static inputs for a rate-coded network.\nThe input values are stored in the recordable attribute r, without any further processing.\ninp = ann.InputArray(geometry=10)\ninp.r = np.linspace(1, 10, 10)\n\npop = ann.Population(100, ...)\n\nproj = ann.Projection(inp, pop, 'exc')\nproj.connect_all_to_all(1.0)\nNote that this population is functionally equivalent to:\ninp = ann.Population(geometry, ann.Neuron(parameters=\"r=0.0\"))\nbut r is recordable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\nint | tuple\nshape of the population, either an integer or a tuple.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "InputArray"
    ]
  },
  {
    "objectID": "reference/InputArray.html#parameters",
    "href": "reference/InputArray.html#parameters",
    "title": "InputArray",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngeometry\nint | tuple\nshape of the population, either an integer or a tuple.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "InputArray"
    ]
  },
  {
    "objectID": "reference/Constant.html",
    "href": "reference/Constant.html",
    "title": "Constant",
    "section": "",
    "text": "core.Constant.Constant(self, name, value, net_id=0)\nConstant parameter that can be used by all neurons and synapses.\nThe class Constant derives from float, so any legal operation on floats (addition, multiplication) can be used.\nIf a Neuron/Synapse defines a parameter with the same name, the constant parameters will not be visible.\nExample:\n\ntau = ann.Constant('tau', 20)\nfactor = ann.Constant('factor', 0.1)\nreal_tau = ann.Constant('real_tau', tau*factor)\n\nneuron = ann.Neuron(\n    equations='''\n        real_tau*dr/dt + r =1.0\n    '''\n)\nThe value of the constant can be changed anytime with the set() method. Assignments will have no effect (e.g. tau = 10.0 only creates a new float).\nThe value of constants defined as combination of other constants (real_tau) is not updated if the value of these constants changes (changing tau with tau.set(10.0) will not modify the value of real_tau).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nname of the constant (unique), which can be used in equations.\nrequired\n\n\nvalue\n\nthe value of the constant, which must be a float, or a combination of Constants.\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nset\nChanges the value of the constant.\n\n\n\n\n\ncore.Constant.Constant.set(value)\nChanges the value of the constant.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nfloat\nValue.\nrequired",
    "crumbs": [
      "Reference",
      "**Functions and Constants**",
      "Constant"
    ]
  },
  {
    "objectID": "reference/Constant.html#parameters",
    "href": "reference/Constant.html#parameters",
    "title": "Constant",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\n\nname of the constant (unique), which can be used in equations.\nrequired\n\n\nvalue\n\nthe value of the constant, which must be a float, or a combination of Constants.\nrequired",
    "crumbs": [
      "Reference",
      "**Functions and Constants**",
      "Constant"
    ]
  },
  {
    "objectID": "reference/Constant.html#methods",
    "href": "reference/Constant.html#methods",
    "title": "Constant",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nset\nChanges the value of the constant.\n\n\n\n\n\ncore.Constant.Constant.set(value)\nChanges the value of the constant.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nfloat\nValue.\nrequired",
    "crumbs": [
      "Reference",
      "**Functions and Constants**",
      "Constant"
    ]
  },
  {
    "objectID": "reference/get_time.html",
    "href": "reference/get_time.html",
    "title": "get_time",
    "section": "",
    "text": "get_time\ncore.Global.get_time(net_id=0)\nReturns the current time in ms.",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "get_time"
    ]
  },
  {
    "objectID": "reference/Oja.html",
    "href": "reference/Oja.html",
    "title": "Oja",
    "section": "",
    "text": "Oja\nmodels.Synapses.Oja(self, eta=0.01, alpha=1.0)\nRate-coded synapse with regularized Hebbian plasticity (Oja).\nParameters (global):\n\neta = 0.01 : learning rate.\nalpha = 1.0 : regularization constant.\n\nLearning rule:\n\nw : weight:\n\ndw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w )\nEquivalent code:\nOja = ann.Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        alpha = 1.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) : min=0.0\n    \"\"\"\n)",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "Oja"
    ]
  },
  {
    "objectID": "reference/load.html",
    "href": "reference/load.html",
    "title": "load",
    "section": "",
    "text": "core.IO.load(filename, populations=True, projections=True, pickle_encoding=None, net_id=0)\nLoads a saved state of the network.\nWarning: Matlab data can not be loaded.\nExample:\nann.load('results/network.npz')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nthe filename with relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be loaded (by default True)\nTrue\n\n\nprojections\nbool\nif True, projection data will be loaded (by default True)\nTrue\n\n\npickle_encoding\nstr\noptional parameter provided to the pickle.load() method. If set to None the default is used.\nNone",
    "crumbs": [
      "Reference",
      "**IO**",
      "load"
    ]
  },
  {
    "objectID": "reference/load.html#parameters",
    "href": "reference/load.html#parameters",
    "title": "load",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nthe filename with relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be loaded (by default True)\nTrue\n\n\nprojections\nbool\nif True, projection data will be loaded (by default True)\nTrue\n\n\npickle_encoding\nstr\noptional parameter provided to the pickle.load() method. If set to None the default is used.\nNone",
    "crumbs": [
      "Reference",
      "**IO**",
      "load"
    ]
  },
  {
    "objectID": "reference/Logger.html",
    "href": "reference/Logger.html",
    "title": "Logger",
    "section": "",
    "text": "extensions.tensorboard.Logger.Logger(self, logdir='runs/', experiment=None)\nLogger class to use tensorboard to visualize ANNarchy simulations. Requires the tensorboardX package (pip install tensorboardX).\nThe Logger class is a thin wrapper around tensorboardX.SummaryWriter, which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io/. Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch.\nThe extension has to be imported explictly:\nfrom ANNarchy.extensions.tensorboard import Logger\nThe Logger class has to be closed properly at the end of the script, so it is advised to use a context:\nwith Logger() as logger:\n    logger.add_scalar(\"Accuracy\", acc, trial)\nYou can also make sure to close it:\nlogger = Logger()\nlogger.add_scalar(\"Accuracy\", acc, trial)\nlogger.close()\nBy default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine. You can control these two elements by passing arguments to Logger():\nwith Logger(logdir=\"/tmp/annarchy\", experiment=\"trial1\"): # logs in /tmp/annarchy/trial1\nThe add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc.\nA tag should be given to each plot. In the example above, the figure with the accuracy will be labelled “Accuracy” in tensorboard. You can also group plots together with tags such as “Global performance/Accuracy”, “Global performance/Error rate”, “Neural activity/Population 1”, etc.\nAfter (or while) logging data within your simulation, run tensorboard in the terminal by specifying the log directory:\ntensorboard --logdir runs\nTensorboardX enqueues the data in memory before writing to disk. You can force flushing with:\nlogger.flush()\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlogdir\nstr\npath (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is “runs/”\n'runs/'\n\n\nexperiment\nstr\nname of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_figure\nLogs a Matplotlib figure.\n\n\nadd_histogram\nLogs an histogram.\n\n\nadd_image\nLogs an image.\n\n\nadd_images\nLogs a set of images (e.g. receptive fields).\n\n\nadd_parameters\nLogs parameters of a simulation.\n\n\nadd_scalar\nLogs a single scalar value, e.g. a success rate at various stages of learning.\n\n\nadd_scalars\nLogs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities.\n\n\nclose\nCloses the logger.\n\n\nflush\nForces the logged data to be flushed to disk.\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_figure(tag, figure, step=None, close=True)\nLogs a Matplotlib figure.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        fig = plt.figure()\n        plt.plot(pop.r)\n        logger.add_figure(\"Activity\", fig, trial)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the image in tensorboard.\nrequired\n\n\nfigure\nlist | numpy.numpy.array\na list or 1D numpy array of values.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\nclose\nbool\nwhether the logger will close the figure when done (default: True).\nTrue\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_histogram(tag, hist, step=None)\nLogs an histogram.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        weights= proj.w.flatten()\n        logger.add_histogram(\"Weight distribution\", weights, trial)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nname\nname of the figure in tensorboard.\nrequired\n\n\nhist\nlist | numpy.numpy.array\na list or 1D numpy array of values.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_image(tag, img, step=None, equalize=False)\nLogs an image.\nThe image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values.\nExample::\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        img = pop.r.reshape((10, 10))\n        logger.add_image(\"Population / Firing rate\", img, trial, equalize=True)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nimg\nnumpy.numpy.array\narray for the image.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\nequalize\nbool\nrescales the pixels between 0 and 1 using the min and max values of the array.\nFalse\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_images(tag, img, step=None, equalize=False, equalize_per_image=False)\nLogs a set of images (e.g. receptive fields).\nThe numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons\n        logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nimg\nnumpy.numpy.array\narray for the images.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\nequalize\nbool\nrescales the pixels between 0 and 1 using the min and max values of the array.\nFalse\n\n\nequalize_per_image\nbool\nwhether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False.\nFalse\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_parameters(params, metrics)\nLogs parameters of a simulation.\nThis should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc.\nExample:\nwith Logger() as logger:\n    # ...\n    logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy})\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparams\ndict\ndictionary of parameters.\nrequired\n\n\nmetrics\ndict\ndictionary of metrics.\nrequired\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_scalar(tag, value, step=None)\nLogs a single scalar value, e.g. a success rate at various stages of learning.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        accuracy = ...\n        logger.add_scalar(\"Accuracy\", accuracy, trial)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nvalue\nfloat\nvalue.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_scalars(tag, value, step=None)\nLogs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        act1 = pop.r[0]\n        act2 = pop.r[1]\n        logger.add_scalars(\n            \"Accuracy\", \n            {'First neuron': act1, 'Second neuron': act2}, \n            trial)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nvalue\ndict\ndictionary of values.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.close()\nCloses the logger.\n\n\n\nextensions.tensorboard.Logger.Logger.flush()\nForces the logged data to be flushed to disk.",
    "crumbs": [
      "Reference",
      "**Tensorboard logging**",
      "Logger"
    ]
  },
  {
    "objectID": "reference/Logger.html#parameters",
    "href": "reference/Logger.html#parameters",
    "title": "Logger",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nlogdir\nstr\npath (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is “runs/”\n'runs/'\n\n\nexperiment\nstr\nname of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended.\nNone",
    "crumbs": [
      "Reference",
      "**Tensorboard logging**",
      "Logger"
    ]
  },
  {
    "objectID": "reference/Logger.html#methods",
    "href": "reference/Logger.html#methods",
    "title": "Logger",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nadd_figure\nLogs a Matplotlib figure.\n\n\nadd_histogram\nLogs an histogram.\n\n\nadd_image\nLogs an image.\n\n\nadd_images\nLogs a set of images (e.g. receptive fields).\n\n\nadd_parameters\nLogs parameters of a simulation.\n\n\nadd_scalar\nLogs a single scalar value, e.g. a success rate at various stages of learning.\n\n\nadd_scalars\nLogs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities.\n\n\nclose\nCloses the logger.\n\n\nflush\nForces the logged data to be flushed to disk.\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_figure(tag, figure, step=None, close=True)\nLogs a Matplotlib figure.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        fig = plt.figure()\n        plt.plot(pop.r)\n        logger.add_figure(\"Activity\", fig, trial)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the image in tensorboard.\nrequired\n\n\nfigure\nlist | numpy.numpy.array\na list or 1D numpy array of values.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\nclose\nbool\nwhether the logger will close the figure when done (default: True).\nTrue\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_histogram(tag, hist, step=None)\nLogs an histogram.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        weights= proj.w.flatten()\n        logger.add_histogram(\"Weight distribution\", weights, trial)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nname\nname of the figure in tensorboard.\nrequired\n\n\nhist\nlist | numpy.numpy.array\na list or 1D numpy array of values.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_image(tag, img, step=None, equalize=False)\nLogs an image.\nThe image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values.\nExample::\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        img = pop.r.reshape((10, 10))\n        logger.add_image(\"Population / Firing rate\", img, trial, equalize=True)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nimg\nnumpy.numpy.array\narray for the image.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\nequalize\nbool\nrescales the pixels between 0 and 1 using the min and max values of the array.\nFalse\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_images(tag, img, step=None, equalize=False, equalize_per_image=False)\nLogs a set of images (e.g. receptive fields).\nThe numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons\n        logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nimg\nnumpy.numpy.array\narray for the images.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\nequalize\nbool\nrescales the pixels between 0 and 1 using the min and max values of the array.\nFalse\n\n\nequalize_per_image\nbool\nwhether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False.\nFalse\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_parameters(params, metrics)\nLogs parameters of a simulation.\nThis should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc.\nExample:\nwith Logger() as logger:\n    # ...\n    logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy})\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparams\ndict\ndictionary of parameters.\nrequired\n\n\nmetrics\ndict\ndictionary of metrics.\nrequired\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_scalar(tag, value, step=None)\nLogs a single scalar value, e.g. a success rate at various stages of learning.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        accuracy = ...\n        logger.add_scalar(\"Accuracy\", accuracy, trial)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nvalue\nfloat\nvalue.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.add_scalars(tag, value, step=None)\nLogs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities.\nExample:\nwith Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        act1 = pop.r[0]\n        act2 = pop.r[1]\n        logger.add_scalars(\n            \"Accuracy\", \n            {'First neuron': act1, 'Second neuron': act2}, \n            trial)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntag\nstr\nname of the figure in tensorboard.\nrequired\n\n\nvalue\ndict\ndictionary of values.\nrequired\n\n\nstep\nint\ntime index.\nNone\n\n\n\n\n\n\n\nextensions.tensorboard.Logger.Logger.close()\nCloses the logger.\n\n\n\nextensions.tensorboard.Logger.Logger.flush()\nForces the logged data to be flushed to disk.",
    "crumbs": [
      "Reference",
      "**Tensorboard logging**",
      "Logger"
    ]
  },
  {
    "objectID": "reference/DiscreteUniform.html",
    "href": "reference/DiscreteUniform.html",
    "title": "DiscreteUniform",
    "section": "",
    "text": "core.Random.DiscreteUniform(self, min, max)\nDiscrete uniform distribution between min and max.\nThe returned values are integers in the range [min, max].\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmin\nint\nminimum value.\nrequired\n\n\nmax\nint\nmaximum value.\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.DiscreteUniform.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "DiscreteUniform"
    ]
  },
  {
    "objectID": "reference/DiscreteUniform.html#parameters",
    "href": "reference/DiscreteUniform.html#parameters",
    "title": "DiscreteUniform",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmin\nint\nminimum value.\nrequired\n\n\nmax\nint\nmaximum value.\nrequired",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "DiscreteUniform"
    ]
  },
  {
    "objectID": "reference/DiscreteUniform.html#methods",
    "href": "reference/DiscreteUniform.html#methods",
    "title": "DiscreteUniform",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.DiscreteUniform.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "DiscreteUniform"
    ]
  },
  {
    "objectID": "reference/list_constants.html",
    "href": "reference/list_constants.html",
    "title": "list_constants",
    "section": "",
    "text": "list_constants\ncore.Global.list_constants(net_id=0)\nReturns a list of all constants declared with Constant(name, value).",
    "crumbs": [
      "Reference",
      "**Functions and Constants**",
      "list_constants"
    ]
  },
  {
    "objectID": "reference/ImagePopulation.html",
    "href": "reference/ImagePopulation.html",
    "title": "ImagePopulation",
    "section": "",
    "text": "extensions.image.ImagePopulation.ImagePopulation(self, geometry, name=None, copied=False)\nRate-coded Population allowing to represent images (png, jpg…) as the firing rate of a population (each neuron represents one pixel).\nThis extension requires the Python Image Library (pip install Pillow).\nThe extensions has to be explicitly imported:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.image import ImagePopulation\n\npop = ImagePopulation(geometry=(480, 640))\npop.set_image('image.jpg')\nAbout the geometry:\n\nIf the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image).\nIf the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color).\n\nIf the third dimension is 3, each will correspond to the RGB values of the pixels.\nWarning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple\npopulation geometry as tuple. It must correspond to the image size and be fixed through the whole simulation.\nrequired\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nset_image\nSets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population.\n\n\n\n\n\nextensions.image.ImagePopulation.ImagePopulation.set_image(image_name)\nSets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population.\nIf the image has a different size from the population, it will be resized.",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "ImagePopulation"
    ]
  },
  {
    "objectID": "reference/ImagePopulation.html#parameters",
    "href": "reference/ImagePopulation.html#parameters",
    "title": "ImagePopulation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple\npopulation geometry as tuple. It must correspond to the image size and be fixed through the whole simulation.\nrequired\n\n\nname\nstr\nunique name of the population (optional).\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "ImagePopulation"
    ]
  },
  {
    "objectID": "reference/ImagePopulation.html#methods",
    "href": "reference/ImagePopulation.html#methods",
    "title": "ImagePopulation",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nset_image\nSets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population.\n\n\n\n\n\nextensions.image.ImagePopulation.ImagePopulation.set_image(image_name)\nSets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population.\nIf the image has a different size from the population, it will be resized.",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "ImagePopulation"
    ]
  },
  {
    "objectID": "reference/IF_cond_alpha.html",
    "href": "reference/IF_cond_alpha.html",
    "title": "IF_cond_alpha",
    "section": "",
    "text": "IF_cond_alpha\nmodels.Neurons.IF_cond_alpha(self, v_rest=-65.0, cm=1.0, tau_m=20.0, tau_refrac=0.0, tau_syn_E=5.0, tau_syn_I=5.0, e_rev_E=0.0, e_rev_I=-70.0, v_thresh=-50.0, v_reset=-65.0, i_offset=0.0)\nLeaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance.\nParameters:\n\nv_rest = -65.0 : Resting membrane potential (mV)\ncm = 1.0 : Capacity of the membrane (nF)\ntau_m = 20.0 : Membrane time constant (ms)\ntau_refrac = 0.0 : Duration of refractory period (ms)\ntau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms)\ntau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms)\ne_rev_E = 0.0 : Reversal potential for excitatory input (mV)\ne_rev_I = -70.0 : Reversal potential for inhibitory input (mv)\ni_offset = 0.0 : Offset current (nA)\nv_reset = -65.0 : Reset potential after a spike (mV)\nv_thresh = -50.0 : Spike threshold (mV)\n\nVariables:\n\nv : membrane potential in mV (init=-65.0):\ncm * dv/dt = cm/tau_m(v_rest -v) + alpha_exc  (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset\ng_exc : excitatory conductance (init = 0.0):\ntau_syn_E * dg_exc/dt = - g_exc\nalpha_exc : alpha function of excitatory conductance (init = 0.0):\ntau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc\ng_inh : inhibitory conductance (init = 0.0):\ntau_syn_I * dg_inh/dt = - g_inh\nalpha_inh : alpha function of inhibitory current (init = 0.0):\ntau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh\n\nSpike emission:\nv &gt; v_thresh\nReset:\nv = v_reset\nThe ODEs are solved using the exponential Euler method.\nEquivalent code:\n\nIF_cond_alpha = Neuron(\n    parameters = \"\"\"\n        v_rest = -65.0\n        cm  = 1.0\n        tau_m  = 20.0\n        tau_syn_E = 5.0\n        tau_syn_I = 5.0\n        e_rev_E = 0.0\n        e_rev_I = -70.0\n        v_thresh = -50.0\n        v_reset = -65.0\n        i_offset = 0.0\n    \"\"\", \n    equations = \"\"\"\n        gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)\n        gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)\n        cm * dv/dt = cm/tau_m*(v_rest -v)   + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0\n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n        tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc  : exponential\n        tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh  : exponential\n    \"\"\",\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = v_reset\",\n    refractory = 0.0\n)",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_cond_alpha"
    ]
  },
  {
    "objectID": "reference/balloon_CN.html",
    "href": "reference/balloon_CN.html",
    "title": "balloon_CN",
    "section": "",
    "text": "extensions.bold.PredefinedModels.balloon_CN(self, phi=1.0, kappa=1 / 1.54, gamma=1 / 2.46, E_0=0.34, tau=0.98, alpha=0.33, V_0=0.02, v_0=40.3, TE=40 / 1000.0, epsilon=1.43)\nA balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\nEquivalent code:\nballoon_CN = BoldModel(\n    parameters = '''\n        second    = 1000.0\n        phi       = 1.0\n        kappa     = 1/1.54\n        gamma     = 1/2.46\n        E_0       = 0.34\n        tau       = 0.98\n        alpha     = 0.33\n        V_0       = 0.02\n        v_0       = 40.3\n        TE        = 40/1000.\n        epsilon   = 1.43\n    ''',\n    equations = '''\n        # Single input\n        I_CBF          = sum(I_CBF)                                                : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Classic coefficients\n        k_1            = (1 - V_0) * 4.3 * v_0 * E_0 * TE\n        k_2            = 2 * E_0\n        k_3            = 1 - epsilon\n\n        # Non-linear equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))  : init=0\n    ''',\n    inputs=\"I_CBF\",\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_CN"
    ]
  },
  {
    "objectID": "reference/balloon_CN.html#parameters",
    "href": "reference/balloon_CN.html#parameters",
    "title": "balloon_CN",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_CN"
    ]
  },
  {
    "objectID": "reference/SpikeSourceArray.html",
    "href": "reference/SpikeSourceArray.html",
    "title": "SpikeSourceArray",
    "section": "",
    "text": "inputs.SpikeSourceArray.SpikeSourceArray(self, spike_times, name=None, copied=False)\nSpike source generating spikes at the times given in the spike_times array.\nDepending on the initial array provided, the population will have one or several neurons, but the geometry can only be one-dimensional.\nYou can later modify the spike_times attribute of the population, but it must have the same number of neurons as the initial one.\nThe spike times are by default relative to the start of a simulation (ANNarchy.get_time() is 0.0). If you call the reset() method of a SpikeSourceArray, this will set the spike times relative to the current time. You can then repeat a stimulation many times.\n# 2 neurons firing at 100Hz with a 1 ms delay\ntimes = [\n    [ 10, 20, 30, 40],\n    [ 11, 21, 31, 41]\n]\ninp = ann.SpikeSourceArray(spike_times=times)\n\nann.compile()\n\n# Spikes at 10/11, 20/21, etc\nann.simulate(50)\n\n# Reset the internal time of the SpikeSourceArray\ninp.reset()\n\n# Spikes at 60/61, 70/71, etc\nann.simulate(50)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspike_times\nlist[float]\na list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt.\nrequired\n\n\nname\nstr\noptional name for the population.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "SpikeSourceArray"
    ]
  },
  {
    "objectID": "reference/SpikeSourceArray.html#parameters",
    "href": "reference/SpikeSourceArray.html#parameters",
    "title": "SpikeSourceArray",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nspike_times\nlist[float]\na list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt.\nrequired\n\n\nname\nstr\noptional name for the population.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "SpikeSourceArray"
    ]
  },
  {
    "objectID": "reference/get_population.html",
    "href": "reference/get_population.html",
    "title": "get_population",
    "section": "",
    "text": "core.Global.get_population(name, net_id=0)\nReturns the population with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the population.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nANNarchy.core.Population\nThe requested Population object if existing, None otherwise.",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "get_population"
    ]
  },
  {
    "objectID": "reference/get_population.html#parameters",
    "href": "reference/get_population.html#parameters",
    "title": "get_population",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the population.\nrequired",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "get_population"
    ]
  },
  {
    "objectID": "reference/get_population.html#returns",
    "href": "reference/get_population.html#returns",
    "title": "get_population",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nANNarchy.core.Population\nThe requested Population object if existing, None otherwise.",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "get_population"
    ]
  },
  {
    "objectID": "reference/CurrentInjection.html",
    "href": "reference/CurrentInjection.html",
    "title": "CurrentInjection",
    "section": "",
    "text": "inputs.CurrentInjection.CurrentInjection(self, pre, post, target, name=None, copied=False)\nInject current from a rate-coded population into a spiking population.\nThe pre-synaptic population must be be rate-coded, the post-synaptic one must be spiking, both must have the same size and no plasticity is allowed.\nFor each post-synaptic neuron, the current g_target will be set at each time step to the firing rate r of the pre-synaptic neuron with the same rank.\nThe projection must be connected with connect_current(), which takes no parameter and does not accept delays. It is equivalent to connect_one_to_one(weights=1).\nExample:\ninp = ann.Population(100, ann.Neuron(equations=\"r = sin(t)\"))\n\npop = ann.Population(100, Izhikevich)\n\nproj = ann.CurrentInjection(inp, pop, 'exc')\nproj.connect_current()\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\nANNarchy.core.Population.Population\npre-synaptic population.\nrequired\n\n\npost\nANNarchy.core.Population.Population\npost-synaptic population.\nrequired\n\n\ntarget\nstr\ntype of the connection.\nrequired\n\n\nname\nstr\noptional name.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "CurrentInjection"
    ]
  },
  {
    "objectID": "reference/CurrentInjection.html#parameters",
    "href": "reference/CurrentInjection.html#parameters",
    "title": "CurrentInjection",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\nANNarchy.core.Population.Population\npre-synaptic population.\nrequired\n\n\npost\nANNarchy.core.Population.Population\npost-synaptic population.\nrequired\n\n\ntarget\nstr\ntype of the connection.\nrequired\n\n\nname\nstr\noptional name.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "CurrentInjection"
    ]
  },
  {
    "objectID": "reference/Hebb.html",
    "href": "reference/Hebb.html",
    "title": "Hebb",
    "section": "",
    "text": "Hebb\nmodels.Synapses.Hebb(self, eta=0.01)\nRate-coded synapse with Hebbian plasticity.\nParameters (global):\n\neta = 0.01 : learning rate.\n\nLearning rule:\n\nw : weight.\n\ndw/dt = eta * pre.r * post.r\nEquivalent code:\nHebb = ann.Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n    \"\"\",\n    equations = \"\"\"\n        dw/dt = eta * pre.r * post.r : min=0.0\n    \"\"\"\n)",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "Hebb"
    ]
  },
  {
    "objectID": "reference/inter_spike_interval.html",
    "href": "reference/inter_spike_interval.html",
    "title": "inter_spike_interval",
    "section": "",
    "text": "core.Monitor.inter_spike_interval(spikes, ranks=None, per_neuron=False)\nComputes the inter-spike interval (ISI) for the recorded spike events of a population.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike').\nrequired\n\n\nranks\nlist\nlist of ranks.\nNone\n\n\nper_neuron\nbool\nif True, the ISI will be computed per neuron, not globally.\nFalse",
    "crumbs": [
      "Reference",
      "**Plotting**",
      "inter_spike_interval"
    ]
  },
  {
    "objectID": "reference/inter_spike_interval.html#parameters",
    "href": "reference/inter_spike_interval.html#parameters",
    "title": "inter_spike_interval",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike').\nrequired\n\n\nranks\nlist\nlist of ranks.\nNone\n\n\nper_neuron\nbool\nif True, the ISI will be computed per neuron, not globally.\nFalse",
    "crumbs": [
      "Reference",
      "**Plotting**",
      "inter_spike_interval"
    ]
  },
  {
    "objectID": "reference/IBCM.html",
    "href": "reference/IBCM.html",
    "title": "IBCM",
    "section": "",
    "text": "IBCM\nmodels.Synapses.IBCM(self, eta=0.01, tau=2000.0)\nRate-coded synapse with Intrator & Cooper (1992) plasticity.\nParameters (global):\n\neta = 0.01 : learning rate.\ntau = 2000.0 : time constant of the post-synaptic threshold.\n\nLearning rule:\n\ntheta : post-synaptic threshold:\n\ntau * dtheta/dt + theta = post.r^2\n\nw : weight:\n\ndw/dt = eta * post.r * (post.r - theta) * pre.r \nEquivalent code:\nIBCM = ann.Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 2000.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit\n    \"\"\"\n)",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "IBCM"
    ]
  },
  {
    "objectID": "reference/PoissonPopulation.html",
    "href": "reference/PoissonPopulation.html",
    "title": "PoissonPopulation",
    "section": "",
    "text": "inputs.PoissonPopulation.PoissonPopulation(self, geometry, name=None, rates=None, target=None, parameters=None, refractory=None, copied=False)\nPopulation of spiking neurons following a Poisson distribution.\nCase 1: Input population\nEach neuron of the population will randomly emit spikes, with a mean firing rate defined by the rates argument.\nThe mean firing rate in Hz can be a fixed value for all neurons:\npop = ann.PoissonPopulation(geometry=100, rates=100.0)\nbut it can be modified later as a normal parameter:\npop.rates = np.linspace(10, 150, 100)\nIt is also possible to define a temporal equation for the rates, by passing a string to the argument:\npop = ann.PoissonPopulation(\n    geometry=100, \n    rates=\"100.0 * (1.0 + sin(2*pi*t/1000.0) )/2.0\"\n)\nThe syntax of this equation follows the same structure as neural variables.\nIt is also possible to add parameters to the population which can be used in the equation of rates:\npop = ann.PoissonPopulation(\n    geometry=100,\n    parameters = '''\n        amp = 100.0\n        frequency = 1.0\n    ''',\n    rates=\"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\"\n)\nNote: The preceding definition is fully equivalent to the definition of this neuron:\npoisson = ann.Neuron(\n    parameters = '''\n        amp = 100.0\n        frequency = 1.0\n    ''',\n    equations = '''\n        rates = amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\n        p = Uniform(0.0, 1.0) * 1000.0 / dt\n    ''',\n    spike = '''\n        p &lt; rates\n    '''\n)\nThe refractory period can also be set, so that a neuron can not emit two spikes too close from each other.\nCase 2: Hybrid population\nIf the rates argument is not set, the population can be used as an interface from a rate-coded population.\nThe target argument specifies which incoming projections will be summed to determine the instantaneous firing rate of each neuron.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\nint | tuple[int]\npopulation geometry as tuple.\nrequired\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\nrates\nfloat | str\nmean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string).\nNone\n\n\ntarget\nstr\nthe mean firing rate will be the weighted sum of inputs having this target name (e.g. “exc”).\nNone\n\n\nparameters\nstr\nadditional parameters which can be used in the rates equation.\nNone\n\n\nrefractory\nfloat\nrefractory period in ms.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "PoissonPopulation"
    ]
  },
  {
    "objectID": "reference/PoissonPopulation.html#parameters",
    "href": "reference/PoissonPopulation.html#parameters",
    "title": "PoissonPopulation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngeometry\nint | tuple[int]\npopulation geometry as tuple.\nrequired\n\n\nname\nstr\nunique name of the population (optional).\nNone\n\n\nrates\nfloat | str\nmean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string).\nNone\n\n\ntarget\nstr\nthe mean firing rate will be the weighted sum of inputs having this target name (e.g. “exc”).\nNone\n\n\nparameters\nstr\nadditional parameters which can be used in the rates equation.\nNone\n\n\nrefractory\nfloat\nrefractory period in ms.\nNone",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "PoissonPopulation"
    ]
  },
  {
    "objectID": "reference/coefficient_of_variation.html",
    "href": "reference/coefficient_of_variation.html",
    "title": "coefficient_of_variation",
    "section": "",
    "text": "core.Monitor.coefficient_of_variation(spikes, ranks=None, per_neuron=False)\nComputes the coefficient of variation of the inter-spike intervals for the recorded spike events of a population.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike').\nrequired\n\n\nranks\nlist\nlist of ranks.\nNone\n\n\nper_neuron\nbool\nif True, the ISI will be computed per neuron, not globally.\nFalse",
    "crumbs": [
      "Reference",
      "**Plotting**",
      "coefficient_of_variation"
    ]
  },
  {
    "objectID": "reference/coefficient_of_variation.html#parameters",
    "href": "reference/coefficient_of_variation.html#parameters",
    "title": "coefficient_of_variation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike').\nrequired\n\n\nranks\nlist\nlist of ranks.\nNone\n\n\nper_neuron\nbool\nif True, the ISI will be computed per neuron, not globally.\nFalse",
    "crumbs": [
      "Reference",
      "**Plotting**",
      "coefficient_of_variation"
    ]
  },
  {
    "objectID": "reference/step.html",
    "href": "reference/step.html",
    "title": "step",
    "section": "",
    "text": "step\ncore.Simulate.step(net_id=0)\nPerforms a single simulation step (duration = dt).",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "step"
    ]
  },
  {
    "objectID": "reference/Population.html",
    "href": "reference/Population.html",
    "title": "Population",
    "section": "",
    "text": "core.Population.Population(self, geometry, neuron, name=None, stop_condition=None, storage_order='post_to_pre', copied=False)\nStructure for a population of homogeneous neurons.\nExample:\npop = ann.Population(100, neuron=ann.Izhikevich, name=\"Excitatory population\")\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple | int\npopulation geometry as tuple. If an integer is given, it is the size of the population.\nrequired\n\n\nneuron\nANNarchy.core.Neuron.Neuron\nNeuroninstance. It can be user-defined or a built-in model.\nrequired\n\n\nname\nstr\nunique name of the population (optional, it defaults to pop0, pop1, etc).\nNone\n\n\nstop_condition\nstr\na single condition on a neural variable which can stop the simulation whenever it is true.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nneurons\nReturns iteratively each neuron in the population.\n\n\nrefractory\nRefractory period (in ms).\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nclear\nClears all spiking events previously emitted (history of spikes, delayed spikes).\n\n\ncompute_firing_rate\nTells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable r.\n\n\ncoordinates_from_rank\nReturns the coordinates of a neuron based on its rank.\n\n\ndisable\nTemporarily disables computations in this population (including the projections leading to it).\n\n\nenable\n(Re)-enables computations in this population, after they were disabled by the disable() method.\n\n\nget\nReturns the value of neural variables and parameters.\n\n\nload\nLoad the saved state of the population by Population.save().\n\n\nneuron\nReturns an IndividualNeuron object wrapping the neuron with the provided rank or coordinates.\n\n\nnormalized_coordinates_from_rank\nReturns normalized coordinates of a neuron based on its rank.\n\n\nrank_from_coordinates\nReturns the rank of a neuron based on coordinates.\n\n\nreset\nResets all parameters and variables of the population to the value they had before the call to compile().\n\n\nsave\nSaves all information about the population (structure, current value of parameters and variables) into a file.\n\n\nset\nSets the value of neural variables and parameters.\n\n\nsize_in_bytes\nReturns the size of allocated memory on the C++ side. Please note that this does not contain monitored data and works only if compile() has been invoked.\n\n\nsum\nReturns the array of weighted sums corresponding to the target:\n\n\n\n\n\ncore.Population.Population.clear()\nClears all spiking events previously emitted (history of spikes, delayed spikes).\nCan be useful if you do not want to totally reset a population (i.e. all variables), only to clear the spiking history between two trials.\nNote: does nothing for rate-coded networks.\n\n\n\ncore.Population.Population.compute_firing_rate(window)\nTells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable r.\nThis method has an effect on spiking neurons only.\nIf this method is not called, r will always be 0.0. r can of course be accessed and recorded as any other variable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwindow\nfloat\nwindow in ms over which the spikes will be counted.\nrequired\n\n\n\n\n\n\n\ncore.Population.Population.coordinates_from_rank(rank)\nReturns the coordinates of a neuron based on its rank.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nRank of the neuron.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple\nCoordinates.\n\n\n\n\n\n\n\ncore.Population.Population.disable()\nTemporarily disables computations in this population (including the projections leading to it).\nYou can re-enable it with the enable() method.\n\n\n\ncore.Population.Population.enable()\n(Re)-enables computations in this population, after they were disabled by the disable() method.\nThe status of the population is accessible through the enabled flag.\n\n\n\ncore.Population.Population.get(name)\nReturns the value of neural variables and parameters.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nattribute name as a string.\nrequired\n\n\n\n\n\n\n\ncore.Population.Population.load(filename, pickle_encoding=None)\nLoad the saved state of the population by Population.save().\nWarning: Matlab data can not be loaded.\nExample:\npop.load('pop1.npz')\npop.load('pop1.txt')\npop.load('pop1.txt.gz')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\n\nthe filename with relative or absolute path.\nrequired\n\n\n\n\n\n\n\ncore.Population.Population.neuron(*coord)\nReturns an IndividualNeuron object wrapping the neuron with the provided rank or coordinates.\n\n\n\n\n\nType\nDescription\n\n\n\n\nANNarchy.core.Neuron.IndividualNeuron\nIndividualNeuron instance.\n\n\n\n\n\n\n\ncore.Population.Population.normalized_coordinates_from_rank(rank, norm=1.0)\nReturns normalized coordinates of a neuron based on its rank. The geometry of the population is mapped to the hypercube [0, 1]^d\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nRank of the neuron\nrequired\n\n\nnorm\nfloat\nNorm of the cube (default = 1.0)\n1.0\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple\nCoordinates.\n\n\n\n\n\n\n\ncore.Population.Population.rank_from_coordinates(coord)\nReturns the rank of a neuron based on coordinates.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncoord\ntuple\nCoordinate tuple, can be multidimensional.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nint\nRank.\n\n\n\n\n\n\n\ncore.Population.Population.reset(attributes=None)\nResets all parameters and variables of the population to the value they had before the call to compile().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattributes\nlist\nlist of attributes (parameter or variable) which should be reinitialized. Default: all attributes.\nNone\n\n\n\n\n\n\n\ncore.Population.Population.save(filename)\nSaves all information about the population (structure, current value of parameters and variables) into a file.\n\nIf the file name is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the file name ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nIf the file name is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nOtherwise, the data will be pickled into a simple binary text file using pickle.\n\nWarning: The ‘.mat’ data will not be loadable by ANNarchy, it is only for external analysis purpose.\nExample:\npop.save('pop1.npz')\npop.save('pop1.txt')\npop.save('pop1.txt.gz')\npop.save('pop1.mat')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nFilename, may contain relative or absolute path.\nrequired\n\n\n\n\n\n\n\ncore.Population.Population.set(values)\nSets the value of neural variables and parameters.\nExample:\npop.set({'tau': 20.0, 'r': np.random.rand((8,8)) } )\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\ndict\ndictionary of attributes to be updated.\nrequired\n\n\n\n\n\n\n\ncore.Population.Population.size_in_bytes()\nReturns the size of allocated memory on the C++ side. Please note that this does not contain monitored data and works only if compile() has been invoked.\n\n\n\n\n\nType\nDescription\n\n\n\n\nint\nSize.\n\n\n\n\n\n\n\ncore.Population.Population.sum(target)\nReturns the array of weighted sums corresponding to the target:\nexcitatory = pop.sum('exc')\nFor spiking networks, this is equivalent to accessing the conductances directly:\nexcitatory = pop.g_exc\nIf no incoming projection has the given target, the method returns zeros.\nNote: it is not possible to distinguish the original population when the same target is used.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntarget\nstr\nthe desired projection target.\nrequired",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Population"
    ]
  },
  {
    "objectID": "reference/Population.html#parameters",
    "href": "reference/Population.html#parameters",
    "title": "Population",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ngeometry\ntuple | int\npopulation geometry as tuple. If an integer is given, it is the size of the population.\nrequired\n\n\nneuron\nANNarchy.core.Neuron.Neuron\nNeuroninstance. It can be user-defined or a built-in model.\nrequired\n\n\nname\nstr\nunique name of the population (optional, it defaults to pop0, pop1, etc).\nNone\n\n\nstop_condition\nstr\na single condition on a neural variable which can stop the simulation whenever it is true.\nNone",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Population"
    ]
  },
  {
    "objectID": "reference/Population.html#attributes",
    "href": "reference/Population.html#attributes",
    "title": "Population",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nneurons\nReturns iteratively each neuron in the population.\n\n\nrefractory\nRefractory period (in ms).",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Population"
    ]
  },
  {
    "objectID": "reference/Population.html#methods",
    "href": "reference/Population.html#methods",
    "title": "Population",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nclear\nClears all spiking events previously emitted (history of spikes, delayed spikes).\n\n\ncompute_firing_rate\nTells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable r.\n\n\ncoordinates_from_rank\nReturns the coordinates of a neuron based on its rank.\n\n\ndisable\nTemporarily disables computations in this population (including the projections leading to it).\n\n\nenable\n(Re)-enables computations in this population, after they were disabled by the disable() method.\n\n\nget\nReturns the value of neural variables and parameters.\n\n\nload\nLoad the saved state of the population by Population.save().\n\n\nneuron\nReturns an IndividualNeuron object wrapping the neuron with the provided rank or coordinates.\n\n\nnormalized_coordinates_from_rank\nReturns normalized coordinates of a neuron based on its rank.\n\n\nrank_from_coordinates\nReturns the rank of a neuron based on coordinates.\n\n\nreset\nResets all parameters and variables of the population to the value they had before the call to compile().\n\n\nsave\nSaves all information about the population (structure, current value of parameters and variables) into a file.\n\n\nset\nSets the value of neural variables and parameters.\n\n\nsize_in_bytes\nReturns the size of allocated memory on the C++ side. Please note that this does not contain monitored data and works only if compile() has been invoked.\n\n\nsum\nReturns the array of weighted sums corresponding to the target:\n\n\n\n\n\ncore.Population.Population.clear()\nClears all spiking events previously emitted (history of spikes, delayed spikes).\nCan be useful if you do not want to totally reset a population (i.e. all variables), only to clear the spiking history between two trials.\nNote: does nothing for rate-coded networks.\n\n\n\ncore.Population.Population.compute_firing_rate(window)\nTells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable r.\nThis method has an effect on spiking neurons only.\nIf this method is not called, r will always be 0.0. r can of course be accessed and recorded as any other variable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwindow\nfloat\nwindow in ms over which the spikes will be counted.\nrequired\n\n\n\n\n\n\n\ncore.Population.Population.coordinates_from_rank(rank)\nReturns the coordinates of a neuron based on its rank.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nRank of the neuron.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple\nCoordinates.\n\n\n\n\n\n\n\ncore.Population.Population.disable()\nTemporarily disables computations in this population (including the projections leading to it).\nYou can re-enable it with the enable() method.\n\n\n\ncore.Population.Population.enable()\n(Re)-enables computations in this population, after they were disabled by the disable() method.\nThe status of the population is accessible through the enabled flag.\n\n\n\ncore.Population.Population.get(name)\nReturns the value of neural variables and parameters.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nattribute name as a string.\nrequired\n\n\n\n\n\n\n\ncore.Population.Population.load(filename, pickle_encoding=None)\nLoad the saved state of the population by Population.save().\nWarning: Matlab data can not be loaded.\nExample:\npop.load('pop1.npz')\npop.load('pop1.txt')\npop.load('pop1.txt.gz')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\n\nthe filename with relative or absolute path.\nrequired\n\n\n\n\n\n\n\ncore.Population.Population.neuron(*coord)\nReturns an IndividualNeuron object wrapping the neuron with the provided rank or coordinates.\n\n\n\n\n\nType\nDescription\n\n\n\n\nANNarchy.core.Neuron.IndividualNeuron\nIndividualNeuron instance.\n\n\n\n\n\n\n\ncore.Population.Population.normalized_coordinates_from_rank(rank, norm=1.0)\nReturns normalized coordinates of a neuron based on its rank. The geometry of the population is mapped to the hypercube [0, 1]^d\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrank\nint\nRank of the neuron\nrequired\n\n\nnorm\nfloat\nNorm of the cube (default = 1.0)\n1.0\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple\nCoordinates.\n\n\n\n\n\n\n\ncore.Population.Population.rank_from_coordinates(coord)\nReturns the rank of a neuron based on coordinates.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncoord\ntuple\nCoordinate tuple, can be multidimensional.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nint\nRank.\n\n\n\n\n\n\n\ncore.Population.Population.reset(attributes=None)\nResets all parameters and variables of the population to the value they had before the call to compile().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattributes\nlist\nlist of attributes (parameter or variable) which should be reinitialized. Default: all attributes.\nNone\n\n\n\n\n\n\n\ncore.Population.Population.save(filename)\nSaves all information about the population (structure, current value of parameters and variables) into a file.\n\nIf the file name is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the file name ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nIf the file name is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nOtherwise, the data will be pickled into a simple binary text file using pickle.\n\nWarning: The ‘.mat’ data will not be loadable by ANNarchy, it is only for external analysis purpose.\nExample:\npop.save('pop1.npz')\npop.save('pop1.txt')\npop.save('pop1.txt.gz')\npop.save('pop1.mat')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nFilename, may contain relative or absolute path.\nrequired\n\n\n\n\n\n\n\ncore.Population.Population.set(values)\nSets the value of neural variables and parameters.\nExample:\npop.set({'tau': 20.0, 'r': np.random.rand((8,8)) } )\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\ndict\ndictionary of attributes to be updated.\nrequired\n\n\n\n\n\n\n\ncore.Population.Population.size_in_bytes()\nReturns the size of allocated memory on the C++ side. Please note that this does not contain monitored data and works only if compile() has been invoked.\n\n\n\n\n\nType\nDescription\n\n\n\n\nint\nSize.\n\n\n\n\n\n\n\ncore.Population.Population.sum(target)\nReturns the array of weighted sums corresponding to the target:\nexcitatory = pop.sum('exc')\nFor spiking networks, this is equivalent to accessing the conductances directly:\nexcitatory = pop.g_exc\nIf no incoming projection has the given target, the method returns zeros.\nNote: it is not possible to distinguish the original population when the same target is used.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntarget\nstr\nthe desired projection target.\nrequired",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Population"
    ]
  },
  {
    "objectID": "notebooks/STDP2.html",
    "href": "notebooks/STDP2.html",
    "title": "STDP - network",
    "section": "",
    "text": "#!pip install ANNarchy\n\nA simple model showing the STDP learning rule on inputs converginf to a single neuron. Model adapted from Song, Miller and Abbott (2000) and Song and Abbott (2001)\nCode adapted from the Brian example: http://brian.readthedocs.org/en/1.4.1/examples-plasticity_STDP1.html\n\nimport numpy as np\nimport ANNarchy as ann\nimport matplotlib.pyplot as plt\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nSome parameters:\n\nF = 15.0 # Poisson distribution at 15 Hz\nN = 1000 # 1000 Poisson inputs\ngmax = 0.01 # Maximum weight\nduration = 100000.0 # Simulation for 100 seconds\n\nIntegrate-and-fire neuron:\n\nIF = ann.Neuron(\n    parameters = \"\"\"\n        tau_m = 10.0\n        tau_e = 5.0 \n        vt = -54.0 \n        vr = -60.0 \n        El = -74.0 \n        Ee = 0.0 \n    \"\"\",\n    equations = \"\"\"\n        tau_m * dv/dt = El - v + g_exc * (Ee - vr) : init = -60.0\n        tau_e * dg_exc/dt = - g_exc\n    \"\"\",\n    spike = \"\"\"\n        v &gt; vt\n    \"\"\",\n    reset = \"\"\"\n        v = vr\n    \"\"\"\n)\n\nAn input population of Poisson neurons, and a single post-synaptic neuron.\n\n# Input population\nInput = ann.PoissonPopulation(name = 'Input', geometry=N, rates=F)\n\n# Output neuron\nOutput = ann.Population(name = 'Output', geometry=1, neuron=IF)\n\n# Projection learned using STDP\nproj = ann.Projection( \n    pre = Input, \n    post = Output, \n    target = 'exc',\n    synapse = ann.STDP(tau_plus=20.0, tau_minus=20.0, A_plus=0.01, A_minus=0.0105, w_max=0.01)\n)\nproj.connect_all_to_all(weights=ann.Uniform(0.0, gmax))\n\n\n# Compile the network\nann.compile()\n\n\n# Start recording\nMi = ann.Monitor(Input, 'spike') \nMo = ann.Monitor(Output, 'spike')\n\n# Start the simulation\nann.simulate(duration, measure_time=True)\n\n# Retrieve the recordings\ninput_spikes = Mi.get('spike')\noutput_spikes = Mo.get('spike')\n\nSimulating 100.0 seconds of the network took 0.5005269050598145 seconds. \n\n\n\n# Compute the mean firing rates during the simulation\nprint('Mean firing rate in the input population: ' + str(Mi.mean_fr(input_spikes)) )\nprint('Mean firing rate of the output neuron: ' + str(Mo.mean_fr(output_spikes)) )\n\n# Compute the instantaneous firing rate of the output neuron\noutput_rate = Mo.smoothed_rate(output_spikes, 100.0)\n\n# Receptive field after simulation\nweights = proj.w[0]\n\nplt.figure(figsize=(12, 10))\nplt.subplot(3,1,1)\nplt.title('Firing rate')\nplt.plot(output_rate[0, :])\nplt.subplot(3,1,2)\nplt.title('Weights')\nplt.plot(weights, '.')\nplt.subplot(3,1,3)\nplt.title('Weights histogram')\nplt.hist(weights, bins=20)\nplt.show()\n\nMean firing rate in the input population: 15.03328\nMean firing rate of the output neuron: 27.380000000000003",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "STDP II"
    ]
  },
  {
    "objectID": "notebooks/GapJunctions.html",
    "href": "notebooks/GapJunctions.html",
    "title": "Gap Junctions",
    "section": "",
    "text": "#!pip install ANNarchy\n\nA simple network with gap junctions.\nThis is a reimplementation of the Brian example:\nhttp://brian2.readthedocs.org/en/2.0b3/examples/synapses.gapjunctions.html\n\nimport numpy as np\nimport ANNarchy as ann\n\nann.clear()\nann.setup(dt=0.1)\n\nneuron = ann.Neuron(\n    parameters = \"v0 = 1.05: population; tau = 10.0: population\",\n    equations = \"tau*dv/dt = v0 - v + g_gap\",\n    spike = \"v &gt;  1.\",\n    reset = \"v = 0.\"\n)\n\ngap_junction = ann.Synapse(\n    psp = \"w * (pre.v - post.v)\"\n)\n\npop = ann.Population(10, neuron)\npop.v = np.linspace(0., 1., 10)\n\nproj = ann.Projection(pop, pop, 'gap', gap_junction)\nproj.connect_all_to_all(0.02)\n\ntrace = ann.Monitor(pop[0] + pop[5], 'v')\n\nann.compile()\n\nann.simulate(500.)\n\ndata = trace.get('v')\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\nplt.plot(data[:, 0])\nplt.plot(data[:, 1])\nplt.xlabel('Time (ms)')\nplt.ylabel('v')\nplt.show()\n\nCompiling ...  OK",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Gap junctions"
    ]
  },
  {
    "objectID": "notebooks/STDP1.html",
    "href": "notebooks/STDP1.html",
    "title": "STDP - single synapse",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis notebook demonstrates the online implementation of the spike time-dependent plasticity (STDP) rule for a pair of neurons.\n\nimport numpy as np\nimport ANNarchy as ann\nimport matplotlib.pyplot as plt\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nThe STDP learning rule maintains exponentially-decaying traces for the pre-synaptic and post-synaptic spikes.\n\\tau^+ \\, \\frac{d x(t)}{dt} = -x (t)\n\\tau^- \\, \\frac{d y(t)}{dt} = -x (t)\nLTP and LTD occur at spike times depending on the corresponding traces.\n\nWhen a pre-synaptic spike occurs, x(t) is incremented and LTD is applied proportionally to y(t).\nWhen a post-synaptic spike occurs, y(t) is incremented and LTP is applied proportionally to x(t).\n\n\nSTDP = ann.Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection ; tau_minus = 20.0 : projection\n        A_plus = 0.01 : projection   ; A_minus = 0.01 : projection\n        w_min = 0.0 : projection     ; w_max = 2.0 : projection\n    \"\"\",\n    equations = \"\"\"\n    \n        tau_plus * dx/dt = -x : event-driven # pre-synaptic trace\n    \n        tau_minus * dy/dt = -y : event-driven # post-synaptic trace\n    \n    \"\"\",\n    pre_spike=\"\"\"\n        \n        g_target += w\n        \n        x += A_plus * w_max\n        \n        w = clip(w - y, w_min , w_max) # LTD\n    \"\"\",\n    post_spike=\"\"\"\n        \n        y += A_minus * w_max\n        \n        w = clip(w + x, w_min , w_max) # LTP\n    \"\"\"\n)\n\nWe create two dummy populations with one neuron each, whose spike times we can control.\n\npre = ann.SpikeSourceArray([[0.]])\npost = ann.SpikeSourceArray([[50.]])\n\nWe connect the population using a STDP synapse.\n\nproj = ann.Projection(pre, post, 'exc', STDP)\nproj.connect_all_to_all(1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x11c94fdd0&gt;\n\n\n\nann.compile()\n\nCompiling ...  OK \n\n\nThe presynaptic neuron will fire at avrious times between 0 and 100 ms, while the postsynaptic neuron keeps firing at 50 ms.\n\npre_times = np.linspace(100.0, 0.0, 101)\n\n\nweight_changes = []\nfor t_pre in pre_times:\n    \n    # Reset the populations\n    pre.clear()\n    post.clear()\n    pre.spike_times = [[t_pre]]\n    post.spike_times = [[50.0]]\n    \n    # Reset the traces\n    proj.x = 0.0\n    proj.y = 0.0\n    \n    # Weight before the simulation\n    w_before = proj[0].w[0]\n    \n    # Simulate long enough\n    ann.simulate(105.0)\n    \n    # Record weight change\n    delta_w = proj[0].w[0] - w_before\n    weight_changes.append(delta_w)\n\nWe can now plot the classical STDP figure:\n\nplt.figure(figsize=(10, 8))\nplt.plot(50. - pre_times, weight_changes, \"*\")\nplt.plot([-50, 50], [0, 0], 'k')\nplt.plot([0, 0], [min(weight_changes), max(weight_changes)], 'k')\nplt.xlabel(\"t_post - t_pre\")\nplt.ylabel(\"delta_w\")\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "STDP I"
    ]
  },
  {
    "objectID": "notebooks/BCM.html",
    "href": "notebooks/BCM.html",
    "title": "BCM learning rule",
    "section": "",
    "text": "The goal of this notebook is to investigate the Intrator & Cooper BCM learning rule for rate-coded networks.\n\\Delta w = \\eta \\, r^\\text{pre} \\, r^\\text{post}  \\,  (r^\\text{post} - \\mathbb{E}[(r^\\text{post})^2])\n\nIntrator, N., & Cooper, L. N. (1992). Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. Neural Networks, 5(1), 3–17. https://doi.org/10.1016/S0893-6080(05)80003-6\n\n\n#!pip install ANNarchy\n\nWe first import ANNarchy:\n\nimport numpy as np\nimport ANNarchy as ann\n\nann.clear()\nann.setup(dt=1.0)\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nWe will keep a minimal experimental setup, with two input neurons connected to a single output neuron. Note how the input neurons are defined by setting r as a parameter that can be set externally.\n\n# Input\ninput_neuron = ann.Neuron(\n    parameters = \"\"\"\n        r = 0.0\n    \"\"\"\n)\npre = ann.Population(2, input_neuron)\n\n# Output\nneuron = ann.Neuron(\n    equations = \"\"\"\n        r = sum(exc)\n    \"\"\"\n)\npost = ann.Population(1, neuron)\n\nWe can now define a synapse model implementing the Intrator and Cooper version of the BCM learning rule.\nThe synapse has two parameters: The learning rate eta and the time constant tau of the moving average theta. Both are defined as projection parameters, as we only need one value for the whole projection. If you omit this flag, there will be one value per synapse, which would be a waste of RAM.\nThe moving average theta tracks the square of the post-synaptic firing rate post.r. It has the flag postsynaptic, as we need to compute only one variable per post-synaptic neuron (it does not really matter in our example as have only one output neuron…). It uses the exponential numerical method, as it is a first-order linear ODE that can be solved exactly. However, the default explicit Euler method would work just as well here.\nThe weight change dw/dt follows the BCM learning rule. min=0.0 ensures that the weight w stays positive throughout learning. The explicit Euler method is the default and could be omitted.\nThe psp argument w * pre.r (what is summed by the post-synaptic neuron over its incoming connections) is also the default value and could be omitted.\n\nIBCM = ann.Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = (post.r)^2 : postsynaptic, exponential\n\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit\n    \"\"\",\n    psp = \"w * pre.r\"\n)\n\nWe can now create a projection between the two populations using the synapse type. The connection method is all-to-all, initialozing the two weights to 1.\n\nproj = ann.Projection(pre, post, 'exc', IBCM)\nproj.connect_all_to_all(1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x127db15d0&gt;\n\n\nWe can now compile the network and record the post-synaptic firing rate as well as the evolution of the weights and thresholds during learning.\n\nann.compile()\n\nm = ann.Monitor(post, 'r')\nn = ann.Monitor(proj, ['w', 'theta'])\n\nWARNING: Monitor(): it is a bad idea to record synaptic variables of a projection at each time step! \n\n\nThe simulation protocol is kept simple, as it consists of setting constant firing rates for the two input neurons and simulating for one second.\n\npre.r = np.array([1.0, 0.1])\nann.simulate(1000.)\n\nWe can now retrieve the recordings and plot the evolution of the various variables.\n\nr = m.get('r')\nw = n.get('w')\ntheta = n.get('theta')\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(10, 5))\nplt.subplot(211)\nplt.plot(r[:, 0], label='r')\nplt.plot(theta[:, 0], label='theta')\nplt.legend()\nplt.subplot(212)\nplt.plot(w[:, 0, 0], label=\"$w_1$\")\nplt.plot(w[:, 0, 1], label=\"$w_2$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNotice how the first weight increases when r is higher than theta (LTP), but decreases afterwards (LTD). Unintuitively, the input neuron with the highest activity sees its weight decreased at the end of the stimulation."
  },
  {
    "objectID": "notebooks/SynapticTransmission.html",
    "href": "notebooks/SynapticTransmission.html",
    "title": "Synaptic transmission",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis notebook simply demonstrates the three main type of synaptic transmission for spiking neurons:\n\nInstantaneous\nExponentially-decreasing\nAlpha-shaped\n\n\nimport numpy as anp\nimport ANNarchy as ann\nann.clear()\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nWe use here a simple LIF neuron receving three types of projections (a, b, c). The conductance g_a uses instantaneous transmission, as it is reset to 0 after each step. g_b decreases exponentially with time following a first order ODE. g_c is integrated twice in alpha_c, leading to the alpha shape.\nAll methods use the exponential numerical method, as they are first order linear ODEs and can be solved exactly.\n\nLIF = ann.Neuron(\n    parameters=\"\"\"\n        tau = 20.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        tau_b = 10.0\n        tau_c = 10.0\n    \"\"\",\n    equations=\"\"\"\n        # Membrane potential\n        tau * dv/dt = (E_L - v) + g_a + g_b + alpha_c : init=-70.\n        \n        # Exponentially decreasing\n        tau_b * dg_b/dt = -g_b : exponential\n        \n        # Alpha-shaped\n        tau_c * dg_c/dt = -g_c : exponential\n        tau_c * dalpha_c/dt = exp((tau_c - dt/2.0)/tau_c) * g_c - alpha_c  : exponential\n    \"\"\",\n    spike=\"v &gt;= v_T\",\n    reset=\"v = v_r\",\n    refractory = 2.0\n)\n\nThe LIF neuron will receive a single spike at t = 10 ms, using the SpikeSourceArray specific population.\n\ninp = ann.SpikeSourceArray([10.])\npop = ann.Population(1, LIF)\n\nWe implement three different projections between the same neurons, to highlight the three possible transmission mechanisms.\n\nproj = ann.Projection(inp, pop, 'a')\nproj.connect_all_to_all(weights=1.0)\n\nproj = ann.Projection(inp, pop, 'b')\nproj.connect_all_to_all(weights=1.0)\n\nproj = ann.Projection(inp, pop, 'c')\nproj.connect_all_to_all(weights=1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x13a687c90&gt;\n\n\n\nann.compile()\n\nCompiling ...  OK \n\n\nWe monitor the three conductances:\n\nm = ann.Monitor(pop, ['g_a', 'g_b', 'alpha_c'])\n\n\ninp.clear()\nann.simulate(100.)\n\n\ndata = m.get()\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(10, 10))\nplt.subplot(311)\nplt.plot(data['g_a'][:, 0])\nplt.ylabel(\"Instantaneous\")\nplt.subplot(312)\nplt.plot(data['g_b'][:, 0])\nplt.ylabel(\"Exponential\")\nplt.subplot(313)\nplt.plot(data['alpha_c'][:, 0])\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"Alpha\")\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Synaptic transmission"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html",
    "href": "notebooks/NeuralField.html",
    "title": "Neural Field",
    "section": "",
    "text": "#!pip install ANNarchy\nThis notebook demonstrates a simple rate-coded model using Neural Fields. It consists of two 2D populations inp and pop, with one-to-one connections between inp and pop, and Difference-of-Gaussians (DoG) lateral connections within pop.\nIt is is based on the following paper:",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#model-overview",
    "href": "notebooks/NeuralField.html#model-overview",
    "title": "Neural Field",
    "section": "Model overview",
    "text": "Model overview\nEach population consists of N*N neurons, with N=20. The inp population is solely used to represent inputs for pop. The pop population implements a discretized neural field, with neurons following the ODE:\n\\tau \\frac{d r_i(t)}{dt} + r_i(t) = r^\\text{input}_i(t) + \\sum_{j=1}^{N} w_{j, i} \\cdot r_j(t) + \\eta(t)\nwhere r_i(t) is the neuron’s firing rate, \\tau a time constant and w_{j, i} the weight value (synaptic efficiency) of the synapse between the neurons j and i. \\eta(t) is an additive noise uniformly taken in [-0.5, 0.5]. f() is a semi-linear function, ensuring the firing rate is bounded between 0 and 1.\nEach neuron in pop takes inputs from the neuron of inp which has the same position, leading to a one_to_one connection pattern.\nThe lateral connections within pop follow a difference-of-Gaussians (dog) connection pattern, with the connection weights w_{i,j} depending on the normalized euclidian distance between the neurons in the N*N population:\nw_{j, i} = A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_+^2}) -  A^- \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_-^2})\nIf i and j have coordinates (x_i, y_i) and (x_j, y_j) in the N*N space, the distance between them is computed as:\nd(i, j)^2 = (\\frac{x_i - x_j}{N})^2 + (\\frac{y_i - y_j}{N})^2\nInputs are given to the network by changing the firing rate of inp neurons.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#importing-annarchy",
    "href": "notebooks/NeuralField.html#importing-annarchy",
    "title": "Neural Field",
    "section": "Importing ANNarchy",
    "text": "Importing ANNarchy\nWe first start by importing the numpy and ANNarchy libraries:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nThe setup() method allows to configure ANNarchy to run in different modes, such as chosing the parallel framework (omp or cuda), setting the simulation step dt, the numerical method method or the seed of the random number generators.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#defining-the-neuron",
    "href": "notebooks/NeuralField.html#defining-the-neuron",
    "title": "Neural Field",
    "section": "Defining the neuron",
    "text": "Defining the neuron\n\nNeuralFieldNeuron = ann.Neuron(\n    parameters=\"\"\" \n        tau = 10.0 : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dr/dt + r = sum(exc) + sum(inh) + Uniform(-0.5, 0.5) : min=0.0, max=1.0\n    \"\"\"\n)\n\nThe NeuralField neuron is governed by an ODE and considers inputs from other neurons. It has a non-linear activation function, which is linear when the firing rate is between 0.0 and 1.0, and constant otherwise.\ntau is a population-wise parameter, whose value will be the same for all neuron of the population.\nr is the firing rate of he neuron, whose dynamics are governed by a first-order linear ODE, integrating the sums of excitatory and inhibitory inputs with noise.\nAs explained in the manual for rate-coded neurons, sum(exc) retrieves the weighted sum of pre-synaptic firing rates for the synapses having the connection type exc, here the one_to_one connections between inp and pop. sum(inh) does the same for inh type connections, here the lateral connections within pop.\nThe firing rate is restricted to the range [0, 1] by setting the min and max accordingly in the flags section (everything after the :). This means that after evaluating the ODE and getting a new value for r, its value will be clamped if it outside these values. One can define both min and max, only one, or none.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#creating-the-populations",
    "href": "notebooks/NeuralField.html#creating-the-populations",
    "title": "Neural Field",
    "section": "Creating the populations",
    "text": "Creating the populations\nThe two populations have a geometry of (20, 20), therefore 400 neurons each. They are created simply by instantiating the Population class:\n\nN = 20\ninp = ann.InputArray(geometry = (N, N), name='Input')\npop = ann.Population(geometry = (N, N), neuron = NeuralFieldNeuron, name='Focus')\n\nThe populations can be assigned a unique name (here ‘Input’ and ‘Focus’) in order to be be able to retrieve them if the references inp and focus are lost. They are given a 2D geometry and associated to the corresponding Neuron instance.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#creating-the-projections",
    "href": "notebooks/NeuralField.html#creating-the-projections",
    "title": "Neural Field",
    "section": "Creating the projections",
    "text": "Creating the projections\nThe first projection is a one-to-one projection from Input to Focus with the type ‘exc’. This connection pattern pattern is possible because the two populations have the same geometry. The weights are initialized to 1.0, and this value will not change with time (no learning), so it is not necessary to define a synapse type:\n\nff = ann.Projection(pre=inp, post=pop, target='exc')\nff.connect_one_to_one(weights=1.0, delays = 20.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x14057edd0&gt;\n\n\nThe references to the pre- and post-synaptic population (or their names), as well as the target type, are passed to the constructor of Projection. The connector method connect_one_to_one() is immediately applied to the Projection, defining how many synapses will be created. The weights are initialized uniformly to 1.0.\nThe second projection is a difference of gaussians (DoG) for the lateral connections within pop. The connector method is already provided by ANNarchy, so there is nothing more to do than to call it with the right parameters:\n\nlat = ann.Projection(pre=pop, post=pop, target='inh')\nlat.connect_dog(amp_pos=0.2, sigma_pos=0.1, amp_neg=0.1, sigma_neg=0.7)\n\n&lt;ANNarchy.core.Projection.Projection at 0x141413050&gt;\n\n\nWe set two monitors recording the firing rate of the two populations:\n\nm = ann.Monitor(inp, 'r')\nn = ann.Monitor(pop, 'r')",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#compiling-the-network-and-simulating",
    "href": "notebooks/NeuralField.html#compiling-the-network-and-simulating",
    "title": "Neural Field",
    "section": "Compiling the network and simulating",
    "text": "Compiling the network and simulating\nOnce the populations and projections are created, the network is ready to be generated, compiled and simulated. Compilation is simply done by calling compile():\n\nann.compile()\n\nCompiling ...  OK \n\n\nThis generates optimized C++ code from the neurons’ definition and network structure, compiles it with gcc/clang and instantiates all objects, particularly the synapses. If some errors were made in the neuron definition, they will be signaled at this point.\nHint: The call to compile() is mandatory in any script. After it is called, populations and projections can not be added anymore.\nOnce the compilation is successful, the network can be simulated by calling simulate(). As no input has been fed into the network yet, calling simulate() now won’t lead to anything interesting. The next step is to clamp inputs into the input population’s baseline.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/NeuralField.html#setting-inputs",
    "href": "notebooks/NeuralField.html#setting-inputs",
    "title": "Neural Field",
    "section": "Setting inputs",
    "text": "Setting inputs\n\nTight loop\nIn this example, we use a moving bubble of activity rotating along a circle in the 20*20 input space in 1 second (i.e. 1000 steps). The simplest way of setting such inputs is to access population attributes (namely inp.r) in a tight loop in Python:\n\ndef gaussian(angle):\n    \"Unnormalized Gaussian at the specified angle.\"\n    # 20*20 matrices with XY coordinates\n    xx, yy = np.meshgrid(np.linspace(0, N-1, N), np.linspace(0, N-1, N))\n\n    # Compute the center of the bubble\n    cx = N / 2. * ( 1.0 + 0.5 * np.cos(2.0 * np.pi * angle ) )\n    cy = N / 2. * ( 1.0 + 0.5 * np.sin(2.0 * np.pi * angle ) )\n\n    # Gaussian\n    bubble = np.exp(-((xx-cx)**2 + (yy-cy)**2)/8.0)\n    return bubble\n\n# Main loop\nT = 1000\nangle = 0.0\nfor t in range(T):\n\n    # Update the angle\n    angle += 1.0/T\n    \n    # Clamp the firing rate of inp with the Gaussian and some noise\n    inp.r = gaussian(angle) + np.random.uniform(-0.5, 0.5, (N, N))\n    \n    # Simulate for one step = 1 ms\n    ann.step()  \n\nangle represents the angle made by the bubble with respect to the center of the input population. At each iteration of the simulation (i.e. every millisecond of simulation), the bubble is slightly rotated (angle is incremented) so as to make a complete revolution in 1 seconds (1000 steps).\nA Gaussian function (in the form of the Numpy array returned by the gaussian() methpd) is then clamped into the activity of inp. Some uniform noise is then added.\nLast, a single simulation step is performed using step(). step() is equivalent to simulate(dt), although a little bit faster as it does not check anything.\nLet’s plot the firing rates of the two populations at the end of the simulation:\n\nplt.figure(figsize=(15, 10))\nplt.subplot(121)\nplt.imshow(inp.r, interpolation='nearest', cmap=plt.cm.gray)\nplt.subplot(122)\nplt.imshow(pop.r, interpolation='nearest', cmap=plt.cm.gray)\nplt.show()\n\n\n\n\n\n\n\n\nWe see a noisy bubble of activity in inp and a clean one in pop, demonstrating the noise-filtering capacities of neural fields.\nLet’s retrieve the data recorded by the monitors, and use Matplotlib animations to show how the firing rates changed during the simulation (other methods are possible):\n\ninp_data = m.get('r')\npop_data = n.get('r')\n\n\n%matplotlib inline\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nfig = plt.figure(figsize=(10,5))\nax1 = plt.subplot(1,2,1)   \nax2 = plt.subplot(1,2,2)\n\nim1 = ax1.imshow(inp.r, interpolation='nearest', cmap=plt.cm.gray)\nim2 = ax2.imshow(pop.r, interpolation='nearest', cmap=plt.cm.gray)\n\ndef drawframe(n):\n    im1.set_data(inp_data[n].reshape((N, N)))\n    im2.set_data(pop_data[n].reshape((N, N)))    \n    return (im1, im2)\n\nanim = animation.FuncAnimation(fig, drawframe, frames=T, interval=2, blit=True)\nplt.close()\nHTML(anim.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nEverything works as expected. However, the simulation is actually quite slow (even if you do not notice it here), as Python is bad at tight loops like this one. For longer simulations, the overhead of python might become too damaging.\n\n\nTimedArray\nA much more efficient variant is to precompute the input rates and store them in an array that will be iteratively read by the TimedArrayobject. This is a new population that unfortunately cannot be added to the network anymore, as we have already compiled it. Let’s clear the network and redefine everything, replacing the InputArray with a TimedArray of the same shape:\n\nann.clear()\n\nNeuralFieldNeuron = ann.Neuron(\n    parameters=\"tau = 10.0 : population\",\n    equations=\"tau * dr/dt + r = sum(exc) + sum(inh) + Uniform(-0.5, 0.5) : min=0.0, max=1.0\")\n\nN = 20\ninp = ann.TimedArray(geometry = (N, N), name='Input')\npop = ann.Population(geometry = (N, N), neuron = NeuralFieldNeuron, name='Focus')\n\nff = ann.Projection(pre=inp, post=pop, target='exc')\nff.connect_one_to_one(weights=1.0, delays = 20.0)\n\nlat = ann.Projection(pre=pop, post=pop, target='inh')\nlat.connect_dog(amp_pos=0.2, sigma_pos=0.1, amp_neg=0.1, sigma_neg=0.7)\n\nm = ann.Monitor(inp, 'r')\nn = ann.Monitor(pop, 'r')\n\nann.compile()\n\nCompiling ...  OK \n\n\nWe now create a numpy array of shape (T, N, N) where T is the duration of the simulation (1000 steps) and N the dimension of the neural field. We make sure to add noise to the inputs.\nWe then set the rates parameter of the TimedArrayto that array by using update(). At each step of the simulation, the timed array will “read” a new input activity in that array and store it as r. We simply need to call simulate()for the whole duration of the simulation, instead of using a slow tight loop in Python.\n\ndef gaussian(angle):\n    \"Unnormalized Gaussian at the specified angle.\"\n    # 20*20 matrices with XY coordinates\n    xx, yy = np.meshgrid(np.linspace(0, N-1, N), np.linspace(0, N-1, N))\n\n    # Compute the center of the bubble\n    cx = N / 2. * ( 1.0 + 0.5 * np.cos(2.0 * np.pi * angle ) )\n    cy = N / 2. * ( 1.0 + 0.5 * np.sin(2.0 * np.pi * angle ) )\n\n    # Gaussian\n    bubble = np.exp(-((xx-cx)**2 + (yy-cy)**2)/8.0)\n    return bubble\n\n# Precompute the angles for 1000 steps = 1 cycle\nT = 1000\nangles = np.linspace(0, 1, T)\n\n# Accumulate the 1000 inputs with time as the first dimension\ninput_rates = np.array([gaussian(angle) + np.random.uniform(-0.5, 0.5, (N, N)) for angle in angles])\n\n# Set it as the rates argument of the input population\ninp.update(input_rates)\n\n# Simulate for 1 second\nann.simulate(T)\n\n# Retrieve the recordings\ninp_data = m.get('r')\npop_data = n.get('r')\n\n\n%matplotlib inline\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nfig = plt.figure(figsize=(10,5))\nax1 = plt.subplot(1,2,1)   \nax2 = plt.subplot(1,2,2)\n\nim1 = ax1.imshow(inp.r, interpolation='nearest', cmap=plt.cm.gray)\nim2 = ax2.imshow(pop.r, interpolation='nearest', cmap=plt.cm.gray)\n\ndef drawframe(n):\n    im1.set_data(inp_data[n].reshape((N, N)))\n    im2.set_data(pop_data[n].reshape((N, N)))    \n    return (im1, im2)\n\nanim = animation.FuncAnimation(fig, drawframe, frames=T, interval=2, blit=True)\nplt.close()\nHTML(anim.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Neural field"
    ]
  },
  {
    "objectID": "notebooks/MultipleNetworks.html",
    "href": "notebooks/MultipleNetworks.html",
    "title": "Parallel simulations",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis example demonstrates the use of parallel_run() to simulate the same network multiple times in parallel.\nWe start by creating the Izhikevich pulse-coupled network defined in Izhikevich.ipynb.\n\nimport numpy as np\nimport ANNarchy as ann\nann.clear()\n\n# Create the whole population\nP = ann.Population(geometry=1000, neuron=ann.Izhikevich)\n\n# Create the excitatory population\nExc = P[:800]\nre = np.random.random(800)\nExc.noise = 5.0\nExc.a = 0.02\nExc.b = 0.2\nExc.c = -65.0 + 15.0 * re**2\nExc.d = 8.0 - 6.0 * re**2\nExc.v = -65.0\nExc.u = Exc.v * Exc.b\n\n# Create the Inh population\nInh = P[800:]\nri = np.random.random(200)\nInh.noise = 2.0\nInh.a = 0.02 + 0.08 * ri\nInh.b = 0.25 - 0.05 * ri\nInh.c = -65.0\nInh.d = 2.0\nInh.v = -65.0\nInh.u = Inh.v * Inh.b\n\n# Create the projections\nproj_exc = ann.Projection(Exc, P, 'exc')\nproj_inh = ann.Projection(Inh, P, 'inh')\n\nproj_exc.connect_all_to_all(weights=ann.Uniform(0.0, 0.5))\nproj_inh.connect_all_to_all(weights=ann.Uniform(0.0, 1.0))\n\n# Create a spike monitor\nM = ann.Monitor(P, 'spike')\n\nann.compile()\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nWe define a simulation method that re-initializes the network, runs a simulation and returns a raster plot.\nThe simulation method must take an index as first argument and a Network instance as second one.\n\ndef run_network(idx, net):\n    # Retrieve subpopulations\n    P_local = net.get(P)\n    Exc = P_local[:800]\n    Inh = P_local[800:]\n    # Randomize initialization\n    re = np.random.random(800)\n    Exc.c = -65.0 + 15.0 * re**2\n    Exc.d = 8.0 - 6.0 * re**2\n    ri = np.random.random(200)\n    Inh.noise = 2.0\n    Inh.a = 0.02 + 0.08 * ri\n    Inh.b = 0.25 - 0.05 * ri\n    Inh.u = Inh.v * Inh.b\n    # Simulate\n    net.simulate(1000.)\n    # Recordings\n    t, n = net.get(M).raster_plot()\n    return t, n\n\nparallel_run() uses the multiprocessing module to start parallel processes. On Linux, it should work directly, but there is an issue on OSX. Since Python 3.8, the ‘spawn’ method is the default way to start processes, but it does not work on MacOS. The following cell should fix the issue, but it should only be ran once.\n\nimport platform\nif platform.system() == \"Darwin\":\n    import multiprocessing as mp\n    mp.set_start_method('fork')\n\nWe can now call parallel_run() to simulate 8 identical but differently initialized networks. The first call runs the simulations sequentially, while the second is in parallel.\nWe finally plot the raster plots of the two first simulations.\n\n# Run four identical simulations sequentially\nvals = ann.parallel_run(method=run_network, number=8, measure_time=True, sequential=True)\n\n# Run four identical simulations in parallel\nvals = ann.parallel_run(method=run_network, number=8, measure_time=True)\n\n# Data analysis\nt1, n1 = vals[0]\nt2, n2 = vals[1]\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.plot(t1, n1, '.')\nplt.subplot(122)\nplt.plot(t2, n2, '.')\nplt.show()\n\nRunning 8 networks sequentially took: 1.3988769054412842 \nRunning 8 networks in parallel took: 0.3649940490722656",
    "crumbs": [
      "Notebooks",
      "**Advanced features**",
      "Parallel run"
    ]
  },
  {
    "objectID": "notebooks/BasalGanglia.html",
    "href": "notebooks/BasalGanglia.html",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "The tensorboard extension allows to log various information (scalars, images, etc) during training for visualization using tensorboard.\n\n#!pip install ANNarchy\n\nThe extension has to be explicitly imported after ANNarchy:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\nfrom ANNarchy.extensions.tensorboard import Logger\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nAs it is just for demonstration purposes, we will be an extremely simplified model of the basal ganglia learning to solve through reinforcement learning a stimulus-response task with 4 stimuli and 2 responses (left and right). The two first stimuli should be responded with left, the two others with right.\n\nstimuli = [\n    ([1, 0, 0, 0], 0), # A : left\n    ([0, 1, 0, 0], 0), # B : left\n    ([0, 0, 1, 0], 1), # C : right\n    ([0, 0, 0, 1], 1), # D : right\n]\n\nWe keep here the model as simple as possible. It is inspired from the rate-coded model described here:\nVitay J, Hamker FH. 2010. A computational model of Basal Ganglia and its role in memory retrieval in rewarded visual memory tasks. Frontiers in computational neuroscience 4. doi:10.3389/fncom.2010.00013\nThe input population is composed of 4 static neurons to represent the inputs:\n\ncortex = ann.Population(4, ann.Neuron(parameters=\"r=0.0\"))\n\nThe cortex projects on the striatum, which is composed of 10 neurons integrating excitatory and inhibitory inputs:\n\nmsn = ann.Neuron(\n    parameters=\"tau = 10.0 : population; noise = 0.1 : population\", \n    equations=\"\"\"\n        tau*dv/dt + v = sum(exc) - sum(inh) + noise * Uniform(-1, 1)\n        r = clip(v, 0.0, 1.0)\n        \"\"\")\nstriatum = ann.Population(10, msn)\n\nThe striatum projects inhibitorily on GPi, whose neurons are tonically active (high baseline). Normally, GPi would project on the thalamus and back to the cortex, but here we read the output of the network directly in GPi: if the first neuron (corresponding to the left action) is less active than the second neuron, the selected action is left.\n\ngp_neuron = ann.Neuron(\n    parameters=\"tau = 10.0 : population; B = 1.0\",\n    equations=\"tau*dv/dt + v = B - sum(inh); r= pos(v)\")\ngpi = ann.Population(2, gp_neuron)\n\nLearning occurs at the cortico-striatal synapses, using a reward-modulated Hebbian learning rule, with Oja regularization:\n\ncorticostriatal = ann.Synapse(\n    parameters=\"\"\"\n        eta = 0.1 : projection\n        alpha = 0.5 : projection\n        dopamine = 0.0 : projection\"\"\",\n    equations=\"w += eta*(dopamine * pre.r * post.r - alpha*w*post.r*post.r) : min=0.0\"\n)\ncx_str = ann.Projection(cortex, striatum, \"exc\", corticostriatal)\ncx_str.connect_all_to_all(weights=ann.Uniform(0.0, 0.5))\n\n&lt;ANNarchy.core.Projection.Projection at 0x11f76a250&gt;\n\n\nSome lateral competition between the striatal neurons:\n\nstr_str = ann.Projection(striatum, striatum, \"inh\")\nstr_str.connect_all_to_all(weights=0.6)\n\n&lt;ANNarchy.core.Projection.Projection at 0x11f874c50&gt;\n\n\nOne half of the striatal population is connected to the left GPi neuron, the other half to the right neuron:\n\nstr_gpi1 = ann.Projection(striatum[:int(striatum.size/2)], gpi[0], 'inh').connect_all_to_all(1.0)\nstr_gpi2 = ann.Projection(striatum[int(striatum.size/2):], gpi[1], 'inh').connect_all_to_all(1.0)\n\nWe add a monitor on GPi and compile:\n\nm = ann.Monitor(gpi, 'r')\n\nann.compile()\n\nCompiling ...  OK \n\n\nEach trial is very simple: we get a stimulus x from the stimuli array and a correct response t, reset the network for 40 ms, set the input and simulate for 50 ms, observe the activity in GPi to decide what the answer of the network is, provide reward accordingly to the corticostriatal projection and let learn for 10 ms.\nHere the “dopamine” signal is directly the reward (+1 for success, -1 for failure), not the reward prediction error, but it is just for demonstration.\n\ndef training_trial(x, t):\n    \n    # Delay period\n    cortex.r = 0.0\n    cx_str.dopamine = 0.0\n    ann.simulate(40.0)\n    \n    # Set inputs\n    cortex.r = np.array(x)\n    ann.simulate(50.0)\n    \n    # Read output\n    output = gpi.r\n    answer = np.argmin(output)\n    \n    # Provide reward\n    reward = 1.0 if answer == t else -1.0\n    cx_str.dopamine = reward\n    ann.simulate(10.0)\n    \n    # Get recordings\n    data = m.get('r')\n    \n    return reward, data\n\nThe whole training procedure will simply iterate over the four stimuli for 100 trials:\nfor trial in range(100):\n    # Get a stimulus\n    x, t = stimuli[trial%len(stimuli)]\n    # Perform a trial\n    reward, data = training_trial(x, t)\nWe use the Logger class of the tensorboard extension to keep track of various data:\nwith Logger() as logger:\n    for trial in range(100):\n        # Get a stimulus\n        x, t = stimuli[trial%len(stimuli)]\n        # Perform a trial\n        reward, data = training_trial(x, t)\n        # Log data...\nNote that it would be equivalent to manually close the Logger after training:\nlogger = Logger()\nfor trial in range(100):\n    # Get a stimulus\n    x, t = stimuli[trial%len(stimuli)]\n    # Perform a trial\n    reward, data = training_trial(x, t)\n        # Log data...\nlogger.close()\nWe log here different quantities, just to demonstrate the different methods of the Logger class:\n\nThe reward received after each trial:\n\nlogger.add_scalar(\"Reward\", reward, trial)\nThe tag “Reward” will be the name of the plot in tensorboard. reward is the value that will be displayed, while trial is the index of the current trial (x-axis).\n\nThe activity of the two GPi cells at the end of the trial, in separate plots depending on the stimulus:\n\nif trial%len(stimuli) == 0:\n    label = \"GPi activity/A\"\nelif trial%len(stimuli) == 1:\n    label = \"GPi activity/B\"\nelif trial%len(stimuli) == 2:\n    label = \"GPi activity/C\"\nelif trial%len(stimuli) == 3:\n    label = \"GPi activity/D\"\nlogger.add_scalars(label, {\"Left neuron\": gpi.r[0], \"Right neuron\": gpi.r[1]}, trial)\nThe four plots will be grouped under the label “GPi activity”, with a title A, B, C or D. Note that add_scalars() requires a dictionary of values that will plot together.\n\nThe activity in the striatum as a 2*5 image:\n\nlogger.add_image(\"Activity/Striatum\", striatum.r.reshape((2, 5)), trial)\nThe activity should be reshaped to the correct dimensions. Note that activity in the striatum is bounded between 0 and 1, so there is no need for equalization.\n\nAn histogram of the preference for the stimuli A and B of striatal cells:\n\nw = np.array(cx_str.w)\nlogger.add_histogram(\"Cortico-striatal weights/Left - AB/CD\", np.mean(w[:5, :2] - w[:5, 2:], axis=1), trial)\nlogger.add_histogram(\"Cortico-striatal weights/Right - AB/CD\", np.mean(w[5:, :2] - w[5:, 2:], axis=1), trial)\nWe make here two plots, one for the first 5 striatal cells, the other for the rest. We plot the difference between the mean weights of each cell for the stimuli A and B, and the mean weights for the stimuli C and D. If learning goes well, the first five striatal cells should have stronger weights for A and B than for C and D, as they project to the left GPi cell.\n\nA matplotlib figure showing the time course of the two GPi cells (as recorded by the monitor):\n\nfig = plt.figure(figsize=(10, 8))\nplt.plot(data[:, 0], label=\"left\")\nplt.plot(data[:, 1], label=\"right\")\nplt.legend()\nlogger.add_figure(\"Activity/GPi\", fig, trial)\nNote that the figure will be automatically closed by the logger, no need to call show(). Logging figures is extremely slow, use that feature wisely.\nBy default, the logs are saved in the subfolder runs/, but this can be changed when creating the Logger:\nwith Logger(\"/tmp/experiment\") as logger:\nEach run of the network will be saved in this folder. You may want to delete the folder before each run, in order to only visualize the last run:\n\n%rm -rf runs\n\n\nwith Logger() as logger:\n    \n    for trial in range(100):\n        \n        # Get a stimulus\n        x, t = stimuli[trial%len(stimuli)]\n    \n        # Perform a trial\n        reward, data = training_trial(x, t)\n        \n        # Log received rewards\n        logger.add_scalar(\"Reward\", reward, trial)\n\n        # Log outputs depending on the task\n        if trial%len(stimuli) == 0:\n            label = \"GPi activity/A\"\n        elif trial%len(stimuli) == 1:\n            label = \"GPi activity/B\"\n        elif trial%len(stimuli) == 2:\n            label = \"GPi activity/C\"\n        elif trial%len(stimuli) == 3:\n            label = \"GPi activity/D\"\n        logger.add_scalars(label, {\"Left neuron\": gpi.r[0], \"Right neuron\": gpi.r[1]}, trial)\n\n        # Log striatal activity as a 2*5 image\n        logger.add_image(\"Activity/Striatum\", striatum.r.reshape((2, 5)), trial)\n\n        # Log histogram of cortico-striatal weights\n        w = np.array(cx_str.w)\n        logger.add_histogram(\"Cortico-striatal weights/Left - AB/CD\", np.mean(w[:5, :2] - w[:5, 2:], axis=1), trial)\n        logger.add_histogram(\"Cortico-striatal weights/Right - AB/CD\", np.mean(w[5:, :2] - w[5:, 2:], axis=1), trial)\n\n        # Log matplotlib figure of GPi activity\n        fig = plt.figure(figsize=(10, 8))\n        plt.plot(data[:, 0], label=\"left\")\n        plt.plot(data[:, 1], label=\"right\")\n        plt.legend()\n        logger.add_figure(\"Activity/GPi\", fig, trial)\n\nLogging in runs/Apr18_11-59-07_Juliens-MBP\n\n\nYou can now visualize the logged information by running tensorboard in a separate terminal and opening the corresponding page:\ntensorboard --logdir runs\nor directly in the notebook if you have the tensorboard extension installed:\n\n%load_ext tensorboard\n%tensorboard --logdir runs --samples_per_plugin images=100\n\nLaunching TensorBoard...\n\n\nYou should see a tensorboard page with four tabs Scalars, Images, Distributions and Histograms:\n\n\n\ntensorboard.png\n\n\nThe Reward plot shows that the network successfully learns to solve the task, as it consistently gets rewards of +1 (note that this may vary from run to run, depending on weight initialization):\n\n\n\nreward.png\n\n\nThe GPi activity tab shows that the two GPi cells quickly learn to be inhibited for the right stimuli.\n\n\n\ngpi.png\n\n\nIn the Images tab, the plot for the striatum allows to visualize activity at the end of each rtial, showing that only one cell in the correct subpopulation is active:\n\n\n\nstriatum.png\n\n\nThe matplotlib figure for the GPi activity shows what happens during a trial, especially at the end of the reset period:\n\n\n\ntimecourse.png\n\n\nIn the histograms tab, we can see that the left striatal population has acquired a preference (stronger weights) for the stimuli A and B, as the values are positive. The right population has negative values, so the neurons have stronger weights to the stimuli C and D. Note that some neurons in the right population still have stronger weights from A and B, but they are probably inhibited by the left population, so they do not impair performance.\n\n\n\nweights-left.png\n\n\n\n\n\nweights-right.png",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "Tensorboard"
    ]
  },
  {
    "objectID": "notebooks/ANN2SNN.html",
    "href": "notebooks/ANN2SNN.html",
    "title": "ANN-to-SNN conversion",
    "section": "",
    "text": "This notebook demonstrates how to transform a neural network trained using tensorflow/keras into an SNN network usable in ANNarchy.\nThe models are adapted from the original models used in:\n#!pip install ANNarchy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nFirst we need to download and process the MNIST dataset provided by tensorflow.\n# Download data\n(X_train, t_train), (X_test, t_test) = tf.keras.datasets.mnist.load_data()\n\n# Normalize inputs\nX_train = X_train.reshape(X_train.shape[0], 784).astype('float32') / 255.\nX_test = X_test.reshape(X_test.shape[0], 784).astype('float32') / 255.\n\n# One-hot output vectors\nT_train = tf.keras.utils.to_categorical(t_train, 10)\nT_test = tf.keras.utils.to_categorical(t_test, 10)",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "ANN to SNN"
    ]
  },
  {
    "objectID": "notebooks/ANN2SNN.html#training-an-ann-in-tensorflowkeras",
    "href": "notebooks/ANN2SNN.html#training-an-ann-in-tensorflowkeras",
    "title": "ANN-to-SNN conversion",
    "section": "Training an ANN in tensorflow/keras",
    "text": "Training an ANN in tensorflow/keras\nThe tensorflow networks are build using the functional API.\nThe fully-connected network has two fully connected layers with ReLU, no bias, dropout at 0.5, and a softmax output layer with 10 neurons. We use the standard SGD optimizer and the categorical crossentropy loss for classification.\n\ndef create_mlp():\n    # Model\n    inputs = tf.keras.layers.Input(shape=(784,))\n    x= tf.keras.layers.Dense(128, use_bias=False, activation='relu')(inputs)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    x= tf.keras.layers.Dense(128, use_bias=False, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    x=tf.keras.layers.Dense(10, use_bias=False, activation='softmax')(x)\n\n    model= tf.keras.Model(inputs, x)\n\n    # Optimizer\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n\n    # Loss function\n    model.compile(\n        loss='categorical_crossentropy', # loss function\n        optimizer=optimizer, # learning rule\n        metrics=['accuracy'] # show accuracy\n    )\n    print(model.summary())\n\n    return model\n\nWe can now train the network and save the weights in the HDF5 format.\n\n# Create model\nmodel = create_mlp()\n\n# Train model\nhistory = model.fit(\n    X_train, T_train,       # training data\n    batch_size=128,          # batch size\n    epochs=20,              # Maximum number of epochs\n    validation_split=0.1,   # Percentage of training data used for validation\n)\nmodel.save(\"runs/mlp.h5\")\n\n# Test model\npredictions_keras = model.predict(X_test, verbose=0)\ntest_loss, test_accuracy = model.evaluate(X_test, T_test, verbose=0)\nprint(f\"Test accuracy: {test_accuracy}\")\n\nplt.figure()\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n2024-04-18 11:51:43.012838: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n2024-04-18 11:51:43.012879: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n2024-04-18 11:51:43.012885: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n2024-04-18 11:51:43.012962: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2024-04-18 11:51:43.013007: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)\nWARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n2024-04-18 11:51:43.898726: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n2024-04-18 11:51:43.916026: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node SGD/AssignVariableOp.\n/Users/vitay/.virtualenvs/ANNarchy/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 784)]             0         \n                                                                 \n dense (Dense)               (None, 128)               100352    \n                                                                 \n dropout (Dropout)           (None, 128)               0         \n                                                                 \n dense_1 (Dense)             (None, 128)               16384     \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense_2 (Dense)             (None, 10)                1280      \n                                                                 \n=================================================================\nTotal params: 118016 (461.00 KB)\nTrainable params: 118016 (461.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nEpoch 1/20\n422/422 [==============================] - 7s 11ms/step - loss: 1.0195 - accuracy: 0.6695 - val_loss: 0.3294 - val_accuracy: 0.9118\nEpoch 2/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.4740 - accuracy: 0.8596 - val_loss: 0.2327 - val_accuracy: 0.9330\nEpoch 3/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.3772 - accuracy: 0.8910 - val_loss: 0.1968 - val_accuracy: 0.9430\nEpoch 4/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.3273 - accuracy: 0.9053 - val_loss: 0.1740 - val_accuracy: 0.9490\nEpoch 5/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.2935 - accuracy: 0.9153 - val_loss: 0.1564 - val_accuracy: 0.9545\nEpoch 6/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.2692 - accuracy: 0.9230 - val_loss: 0.1460 - val_accuracy: 0.9583\nEpoch 7/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.2512 - accuracy: 0.9260 - val_loss: 0.1330 - val_accuracy: 0.9605\nEpoch 8/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.2352 - accuracy: 0.9317 - val_loss: 0.1270 - val_accuracy: 0.9628\nEpoch 9/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.2211 - accuracy: 0.9356 - val_loss: 0.1204 - val_accuracy: 0.9650\nEpoch 10/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.2122 - accuracy: 0.9376 - val_loss: 0.1107 - val_accuracy: 0.9682\nEpoch 11/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.2041 - accuracy: 0.9413 - val_loss: 0.1108 - val_accuracy: 0.9693\nEpoch 12/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.1921 - accuracy: 0.9435 - val_loss: 0.1046 - val_accuracy: 0.9688\nEpoch 13/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.1839 - accuracy: 0.9468 - val_loss: 0.0992 - val_accuracy: 0.9700\nEpoch 14/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.1802 - accuracy: 0.9468 - val_loss: 0.0981 - val_accuracy: 0.9708\nEpoch 15/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.1738 - accuracy: 0.9494 - val_loss: 0.0968 - val_accuracy: 0.9718\nEpoch 16/20\n422/422 [==============================] - 4s 10ms/step - loss: 0.1674 - accuracy: 0.9511 - val_loss: 0.0928 - val_accuracy: 0.9730\nEpoch 17/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.1605 - accuracy: 0.9540 - val_loss: 0.0945 - val_accuracy: 0.9730\nEpoch 18/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.1549 - accuracy: 0.9549 - val_loss: 0.0927 - val_accuracy: 0.9733\nEpoch 19/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.1523 - accuracy: 0.9550 - val_loss: 0.0884 - val_accuracy: 0.9725\nEpoch 20/20\n422/422 [==============================] - 4s 9ms/step - loss: 0.1472 - accuracy: 0.9565 - val_loss: 0.0868 - val_accuracy: 0.9743\nTest accuracy: 0.9696000218391418",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "ANN to SNN"
    ]
  },
  {
    "objectID": "notebooks/ANN2SNN.html#initialize-the-ann-to-snn-converter",
    "href": "notebooks/ANN2SNN.html#initialize-the-ann-to-snn-converter",
    "title": "ANN-to-SNN conversion",
    "section": "Initialize the ANN-to-SNN converter",
    "text": "Initialize the ANN-to-SNN converter\nWe first create an instance of the ANN-to-SNN conversion object. The function receives the input_encoding parameter, which is the type of input encoding we want to use.\nBy default, there are intrinsically bursting (IB), phase shift oscillation (PSO) and Poisson (poisson) available.\n\nfrom ANNarchy.extensions.ann_to_snn_conversion import ANNtoSNNConverter\n\nsnn_converter = ANNtoSNNConverter(\n    input_encoding='IB',\n    hidden_neuron='IaF', \n    read_out='spike_count',\n)\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nAfter that, we provide the TensorFlow model stored as h5py file to the conversion tool. The print-out of the network structure of the imported network is suppressed when show_info=False is provided to init_from_keras_model.\n\nnet = snn_converter.init_from_keras_model(\"runs/mlp.h5\")\n\nWARNING: Dense representation is an experimental feature for spiking models, we greatly appreciate bug reports. \nCompiling network 1...  OK \nParameters\n----------------------\n* input encoding: IB\n* hidden neuron: IaF neuron\n* read-out method: spike_count\n\nLayers\n----------------------\n* name=dense, dense layer, geometry=128\n* name=dense_1, dense layer, geometry=128\n* name=dense_2, dense layer, geometry=10\n\nProjections\n----------------------\n* input_1 (784,) -&gt; dense (128,)\n    weight matrix size (128, 784)\n    mean -0.002207014709711075, std 0.05199548974633217\n    min -0.35995912551879883, max 0.20337390899658203\n* dense (128,) -&gt; dense_1 (128,)\n    weight matrix size (128, 128)\n    mean 0.005490789655596018, std 0.10046137124300003\n    min -0.36605578660964966, max 0.47181716561317444\n* dense_1 (128,) -&gt; dense_2 (10,)\n    weight matrix size (10, 128)\n    mean 0.002933102659881115, std 0.20365405082702637\n    min -0.5556147694587708, max 0.5943390130996704\n\n\n\nWhen the network has been built successfully, we can perform a test using all MNIST training samples. Using duration_per_sample, the number of steps simulated for each image can be specified.\n\npredictions_snn = snn_converter.predict(X_test, duration_per_sample=100)\n\n9900/10000\n\n\nDepending on the selected read-out method, it can happen that multiple neurons/classes are selected as a winner for an example. For example, if duration_per_sample is too low, several output neurons might output the same number of spikes.\nIn the following cell, we force the predictions to keep only one of the winning neurons by using np.random.choice.\n\npredictions_snn = [ [np.random.choice(p)] for p in predictions_snn ]\n\nUsing the recorded predictions, we can now compute the accuracy using scikit-learn for all presented samples.\n\nfrom sklearn.metrics import classification_report, accuracy_score\n\nprint(classification_report(t_test, predictions_snn))\nprint(\"Test accuracy of the SNN:\", accuracy_score(t_test, predictions_snn))\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.99      0.97       980\n           1       0.99      0.99      0.99      1135\n           2       0.96      0.96      0.96      1032\n           3       0.91      0.97      0.94      1010\n           4       0.96      0.94      0.95       982\n           5       0.96      0.93      0.95       892\n           6       0.95      0.97      0.96       958\n           7       0.98      0.95      0.96      1028\n           8       0.97      0.92      0.94       974\n           9       0.93      0.95      0.94      1009\n\n    accuracy                           0.96     10000\n   macro avg       0.96      0.96      0.96     10000\nweighted avg       0.96      0.96      0.96     10000\n\nTest accuracy of the SNN: 0.9568\n\n\nFor comparison, here is the performance of the original SNN:\n\nprint(classification_report(t_test, predictions_keras.argmax(axis=1)))\nprint(\"Test accuracy of the ANN:\", accuracy_score(t_test, predictions_keras.argmax(axis=1)))\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.99      0.98       980\n           1       0.98      0.99      0.98      1135\n           2       0.96      0.97      0.97      1032\n           3       0.94      0.98      0.96      1010\n           4       0.97      0.97      0.97       982\n           5       0.97      0.96      0.97       892\n           6       0.97      0.97      0.97       958\n           7       0.98      0.96      0.97      1028\n           8       0.98      0.95      0.96       974\n           9       0.97      0.96      0.96      1009\n\n    accuracy                           0.97     10000\n   macro avg       0.97      0.97      0.97     10000\nweighted avg       0.97      0.97      0.97     10000\n\nTest accuracy of the ANN: 0.9696\n\n\nA comparison of the predictions made by the ANN and the SNN on each class may reveal different behavior:\n\nprint(classification_report(predictions_keras.argmax(axis=1), predictions_snn))\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1001\n           1       1.00      0.99      0.99      1142\n           2       0.98      0.97      0.97      1037\n           3       0.95      0.98      0.96      1049\n           4       0.97      0.96      0.97       983\n           5       0.97      0.96      0.97       879\n           6       0.97      0.98      0.97       957\n           7       0.99      0.97      0.98      1007\n           8       0.98      0.95      0.97       948\n           9       0.95      0.98      0.96       997\n\n    accuracy                           0.97     10000\n   macro avg       0.97      0.97      0.97     10000\nweighted avg       0.97      0.97      0.97     10000",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "ANN to SNN"
    ]
  },
  {
    "objectID": "notebooks/BarLearning.html",
    "href": "notebooks/BarLearning.html",
    "title": "Bar Learning problem",
    "section": "",
    "text": "The bar learning problem describes the process of learning receptive fields on an artificial input pattern. Images consisting of independent bars are used. Those images are generated as following: an 8*8 image can filled randomly by eight horizontal or vertical bars, with a probability of 1/8 for each.\nThese input images are fed into a neural population, whose neurons should learn to extract the independent components of the input distribution, namely single horizontal or vertical bars.\n#!pip install ANNarchy",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Bar Learning"
    ]
  },
  {
    "objectID": "notebooks/BarLearning.html#model-overview",
    "href": "notebooks/BarLearning.html#model-overview",
    "title": "Bar Learning problem",
    "section": "Model overview",
    "text": "Model overview\nThe model consists of two populations inp and pop. The size of inp should be chosen to fit the input image size (here 8*8). The number of neurons in the pop population should be higher than the total number of independent bars (16, we choose here 32 neurons). The pop population gets excitory connections from inp through an all-to-all connection pattern. The same pattern is used for the inhibitory connections within pop.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Bar Learning"
    ]
  },
  {
    "objectID": "notebooks/BarLearning.html#defining-the-neurons-and-populations",
    "href": "notebooks/BarLearning.html#defining-the-neurons-and-populations",
    "title": "Bar Learning problem",
    "section": "Defining the neurons and populations",
    "text": "Defining the neurons and populations\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\nann.clear()\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nInput population:\nThe input pattern will be clamped into this population by the main loop for every trial, so we just need an InputArray to store the values:\n\nN = 8\ninp = ann.InputArray(geometry=(N, N))\n\nLearning population:\nThe neuron type composing this population sums up all the excitory inputs gain from inp and the lateral inhibition within pop.\n\\tau \\frac {dr_{j}}{dt} + r_{j} = \\sum_{i} w_{ij} \\cdot r_{i}^{\\text{inp}}  - \\sum_{k, k \\ne j} w_{kj} * r_{k}\ncould be implemented as the following:\n\nLeakyNeuron = ann.Neuron(\n    parameters=\"\"\" \n        tau = 10.0 : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dr/dt + r = sum(exc) - sum(inh) : min=0.0\n    \"\"\"\n)\n\nThe firing rate is restricted to positive values with the min=0.0 flag. The population of 32 neurons is created in the following way:\n\npop = ann.Population(geometry=(N, int(N/2)), neuron=LeakyNeuron)\n\nWe define the population with a (8, 4) geometry for visualization only, its 2D structure does not influence computations at all. We could also use geometry=32 and reshape the array afterwards.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Bar Learning"
    ]
  },
  {
    "objectID": "notebooks/BarLearning.html#defining-the-synapse-and-projections",
    "href": "notebooks/BarLearning.html#defining-the-synapse-and-projections",
    "title": "Bar Learning problem",
    "section": "Defining the synapse and projections",
    "text": "Defining the synapse and projections\nBoth feedforward (inp \\rightarrow pop) and lateral (pop \\rightarrow pop) projections are learned using the Oja learning rule (a regularized Hebbian learning rule ensuring the sum of all weights coming to a neuron is constant). Only some parameters will differ between the projections.\n\\tau \\frac{dw_{ij}}{dt} = r_{i} \\cdot r_{j} - \\alpha \\cdot r_{j}^{2} \\cdot w_{ij}\nwhere \\alpha is a parameter defining the strength of the regularization, r_i is the pre-synaptic firing rate and r_j the post-synaptic one. The implementation of this synapse type is straightforward:\n\nOja = ann.Synapse(\n    parameters=\"\"\" \n        tau = 2000.0 : postsynaptic\n        alpha = 8.0 : postsynaptic\n        min_w = 0.0 : postsynaptic\n    \"\"\",\n    equations=\"\"\"\n        tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=min_w\n    \"\"\"\n)  \n\nFor this network we need to create two projections, one excitory between the populations inp and pop and one inhibitory within the pop population itself:\n\nff = ann.Projection(\n    pre=inp, \n    post=pop, \n    target='exc', \n    synapse = Oja    \n)\nff.connect_all_to_all(\n    weights = ann.Uniform(0.0, 0.5)\n)\n                     \nlat = ann.Projection(\n    pre=pop, \n    post=pop, \n    target='inh', \n    synapse = Oja\n)\nlat.connect_all_to_all(\n    weights = ann.Uniform(0.0, 1.0)\n)\n\n&lt;ANNarchy.core.Projection.Projection at 0x103c25e50&gt;\n\n\nThe two projections are all-to-all and use the Oja synapse type. They only differ by the parameter alpha (lower in pop):\n\nlat.alpha = 0.3\n\nWe can now compile the network:\n\nann.compile()",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Bar Learning"
    ]
  },
  {
    "objectID": "notebooks/BarLearning.html#setting-inputs",
    "href": "notebooks/BarLearning.html#setting-inputs",
    "title": "Bar Learning problem",
    "section": "Setting inputs",
    "text": "Setting inputs\nOnce the network is defined, one has to specify how inputs are fed into the inp population. A simple solution is to define a method that sets the firing rate of inp according to the specified probabilities every time it is called, and runs the simulation for 50 ms:\n\ndef trial():\n\n    # Reset the firing rate for all neurons\n    inp.r = 0.0\n\n    # Clamp horizontal bars randomly\n    hbars = np.random.binomial(n=1, p=1./N, size=N) == 1\n    for i, exists in enumerate(hbars): inp[i, :].r = 1.0 if exists else inp[i, :].r\n\n    # Clamp vertical bars randomly\n    vbars = np.random.binomial(n=1, p=1./N, size=N) == 1\n    for j, exists in enumerate(vbars): inp[:, j].r = 1.0 if exists else inp[:, j].r\n\n    # Simulate for 50ms\n    ann.simulate(50.)\n    \n    # Return firing rates and receptive fields for visualization\n    return inp.r, pop.r, ff.receptive_fields()\n\nOne can use here a single value or a Numpy array (e.g. np.zeros(inp.geometry))) to reset activity in inp, it does not matter.\nFor the random bars, we use the binomial distribution to decide for the existence of a vertical or horizontal bar with a probability of 1/8.\ninp[i, :] and inp[:, j] are PopulationViews, i.e. groups of neurons defined by the sub-indices (here the rows and columns of inp). Their attributes, such as r, can be accessed and modified as if it were a regular population.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Bar Learning"
    ]
  },
  {
    "objectID": "notebooks/BarLearning.html#running-the-simulation",
    "href": "notebooks/BarLearning.html#running-the-simulation",
    "title": "Bar Learning problem",
    "section": "Running the simulation",
    "text": "Running the simulation\nLet’s have a look at the activities and receptive fields after one trial:\n\ninput_array, activity_array, weights = trial()\n\n\nfrom matplotlib.gridspec import GridSpec\n\nfig = plt.figure(layout=\"constrained\", figsize=(8, 8))\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0]) \nax2 = fig.add_subplot(gs[0, 1]) \nax3 = fig.add_subplot(gs[1, :]) \n\nim1 = ax1.imshow(input_array.T, interpolation='nearest', cmap=plt.cm.gray)\nax1.set_title('Input')\nim2 = ax2.imshow(activity_array.T, interpolation='nearest', cmap=plt.cm.gray)\nax2.set_title('Feature')\nim3 = ax3.imshow(weights.T, interpolation='nearest', cmap=plt.cm.gray, vmin=0.0, vmax=0.5)\nax3.set_title('Receptive fields')\nfig.colorbar(im3)\n\n\n\n\n\n\n\n\nOne or a few bars are present in inp, a few neurons react in pop, but the receptive fields are all random.\nLet’s now define a for loop where the trial() method is called repetitively 10000 times:\n\ninputs = []; features = []; rfs = []\n\nT = 10000\nfor t in range(T):\n    \n    # Single trial\n    input_r, feature_r, weights = trial()\n\n    # Record every 10 trials\n    if t % 10 == 0:\n        inputs.append(input_r)\n        features.append(feature_r)\n        rfs.append(weights)\n\nWe can now visualize the activities and receptive fields after learning:\n\nfig = plt.figure(layout=\"constrained\", figsize=(8, 8))\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0]) \nax2 = fig.add_subplot(gs[0, 1]) \nax3 = fig.add_subplot(gs[1, :]) \n\nim1 = ax1.imshow(inputs[-1].T, interpolation='nearest', cmap=plt.cm.gray)\nax1.set_title('Input')\nim2 = ax2.imshow(features[-1].T, interpolation='nearest', cmap=plt.cm.gray)\nax2.set_title('Feature')\nim3 = ax3.imshow(rfs[-1].T, interpolation='nearest', cmap=plt.cm.gray, vmin=0.0, vmax=0.3)\nax3.set_title('Receptive fields')\nfig.colorbar(im3)\n\n\n\n\n\n\n\n\nAfter 10000 trials, most neurons have developed a receptive field specific for a single horizontal or vertical bar, although these were always presented together.\nLet’s now have a look at how these receptive fields develop over the course of training.\n\n%matplotlib inline\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nfig = plt.figure(layout=\"constrained\", figsize=(8, 8))\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0]) \nax2 = fig.add_subplot(gs[0, 1]) \nax3 = fig.add_subplot(gs[1, :]) \n\nim1 = ax1.imshow(inputs[-1].T, interpolation='nearest', cmap=plt.cm.gray)\nax1.set_title('Input')\nim2 = ax2.imshow(features[-1].T, interpolation='nearest', cmap=plt.cm.gray)\nax2.set_title('Feature')\nim3 = ax3.imshow(rfs[-1].T, interpolation='nearest', cmap=plt.cm.gray, vmin=0.0, vmax=0.3)\nax3.set_title('Receptive fields')\ncb = fig.colorbar(im3)\n\ndef drawframe(n):\n    im1.set_data(inputs[n].T)\n    im2.set_data(features[n].T) \n    im3.set_data(rfs[n].T) \n    return (im1, im2, im3)\n\nanim = animation.FuncAnimation(fig, drawframe, frames=int(T/10), interval=20, blit=True)\nplt.close()\nHTML(anim.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nMost neurons become selective for a single bar. However, due to the lateral inhibition, two neurons selective for the same bar will compete with each other. The “losing” neuron will have to modify its receptive field to let the winner have it. This explains the instability of some cells during learning.",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Bar Learning"
    ]
  },
  {
    "objectID": "notebooks/AdEx.html",
    "href": "notebooks/AdEx.html",
    "title": "Adaptive Exponential IF neuron",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis notebook explores how the AdEx neuron model can reproduce various spiking patterns observed in vivo.\nCode based on:\n\nNaud, R., Marcille, N., Clopath, C., and Gerstner, W. (2008). Firing patterns in the adaptive exponential integrate-and-fire model. Biol Cybern 99, 335. doi:10.1007/s00422-008-0264-7.\n\n\nimport  ANNarchy as ann\nann.clear()\nann.setup(dt=0.1)\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nThe AdEx neuron is defined by the following equations:\n\n    C \\, \\frac{dv}{dt} = -g_L \\ (v - E_L) + g_L \\, \\Delta_T \\, \\exp(\\frac{v - v_T}{\\Delta_T}) + I - w\n\n\n    \\tau_w \\, \\frac{dw}{dt} = a \\, (v - E_L) - w\n\nif v &gt; v_\\text{spike}:\n\nv = v_R\nw = w + b\n\n\nAdEx = ann.Neuron(\n    parameters=\"\"\"\n        C = 200.\n        gL = 10. # not g_L! g_ is reserved for spike transmission\n        E_L = -70.\n        v_T = -50.\n        delta_T = 2.0\n        a = 2.0\n        tau_w = 30.\n        b = 0.\n        v_r = -58.\n        I = 500.\n        v_spike = 0.0 \n    \"\"\",\n    equations=\"\"\"\n        C * dv/dt = - gL * (v - E_L) +  gL * delta_T * exp((v-v_T)/delta_T) + I - w : init=-70.0     \n        tau_w * dw/dt = a * (v - E_L) - w  : init=0.0\n    \"\"\",\n    spike=\"\"\"\n        v &gt;= v_spike\n    \"\"\",\n    reset=\"\"\"\n        v = v_r\n        w += b\n    \"\"\",\n    refractory = 2.0\n)\n\nWe create a population of 8 AdEx neurons which will get different parameter values.\n\npop = ann.Population(8, AdEx)\n\n\nann.compile()\n\nCompiling ...  OK \n\n\nWe add a monitor to track the membrane potential and the spike timings during the simulation.\n\nm = ann.Monitor(pop, ['v', 'spike'])\n\nAs in the paper, we provide different parameters to each neuron and simulate the network for 500 ms with a fixed input current, and remove that current for an additional 50 ms.\n\n# a) tonic spiking b) adaptation, c) initial burst, d) regular bursting, e) delayed accelerating, f) delayed regular bursting, g) transcient spiking, h) irregular spiking\npop.C =       [200, 200, 130, 200, 200, 200, 100, 100]\npop.gL =      [ 10,  12,  18,  10,  12,  12,  10,  12]\npop.E_L =     [-70, -70, -58, -58, -70, -70, -65, -60]\npop.v_T =     [-50, -50, -50, -50, -50, -50, -50, -50]\npop.delta_T = [  2,   2,   2,   2,   2,   2,   2,   2]\npop.a =       [  2,   2,   4,   2,-10., -6.,-10.,-11.]\npop.tau_w =   [ 30, 300, 150, 120, 300, 300,  90, 130]\npop.b =       [  0,  60, 120, 100,   0,   0,  30,  30]\npop.v_r =     [-58, -58, -50, -46, -58, -58, -47, -48]\npop.I =       [500, 500, 400, 210, 300, 110, 350, 160]\n\n# Reset neuron\npop.v = pop.E_L\npop.w = 0.0\n\n# Simulate\nann.simulate(500.)\npop.I = 0.0\nann.simulate(50.)\n\n# Recordings\ndata = m.get('v')\nspikes = m.get('spike')\nfor n, t in spikes.items(): # Normalize the spikes\n    data[[x - m.times()['v']['start'][0] for x in t], n] = 0.0\n\nWe can now visualize the simulations:\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\ntitles = [\n    \"a) tonic spiking\", \n    \"b) adaptation\", \n    \"c) initial burst\", \n    \"d) regular bursting\", \n    \"e) delayed accelerating\", \n    \"f) delayed regular bursting\", \n    \"g) transcient spiking\", \n    \"h) irregular spiking\"\n]\n\nplt.figure(figsize=(20, 20))\nplt.ylim((-70., 0.))\nfor i in range(8):\n    plt.subplot(4, 2, i+1)\n    plt.title(titles[i])\n    plt.plot(data[:, i], lw=3)\n    \n\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n/var/folders/6w/6msx49ws7k13cc0bbys0tt4m0000gn/T/ipykernel_98871/1457585605.py:21: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(4, 2, i+1)",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "AdEx"
    ]
  },
  {
    "objectID": "notebooks/Izhikevich.html",
    "href": "notebooks/Izhikevich.html",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "",
    "text": "#!pip install ANNarchy\nThis script reproduces the simple pulse-coupled network proposed by Eugene Izhikevich in the article:\nThe original Matlab code is provided below:",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "notebooks/Izhikevich.html#neuron-type",
    "href": "notebooks/Izhikevich.html#neuron-type",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "Neuron type",
    "text": "Neuron type\nThe network is composed of parameterized quadratic integrate-and-fire neurons, known as Izhikevich neurons. They are simply defined by the following equations:\n\n    \\frac{dv}{dt} = 0.04 \\, v^2 + 5 \\, v + 140 - u + I\n\n\n    \\frac{du}{dt} = a \\, (b \\, v - u)\n\nThe spiking mechanism is defined by:\nif v &gt; 30.0:\n    emit_spike()\n    v = c\n    u = u + d\nv is the membrane potential, u is the membrane recovery variable and a, b, c, d are parameters allowing to reproduce many types of neural firing.\nI is the input voltage to a neuron at each time t. For the desired network, it is the sum of a random value taken from a normal distribution with mean 0.0 and variance 1.0 (multiplied by a scaling factor) and the net effect of incoming spikes (excitatory and inhibitory).\nImplementing such a neuron in ANNarchy is straightforward:\nIzhikevich = Neuron(\n    parameters=\"\"\"\n        noise = 5.0\n        a = 0.02\n        b = 0.2\n        c = -65.0\n        d = 2.0 \n        v_thresh = 30.0\n    \"\"\",\n    equations=\"\"\"\n        I = g_exc - g_inh + noise * Normal(0.0, 1.0)\n        dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I \n        du/dt = a * (b*v - u) \n    \"\"\",\n    spike = \"\"\"\n        v &gt;= v_thresh\n    \"\"\",\n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\"\n)\nThe parameters a, b, c, d as well as the noise amplitude noise are declared in the parameters argument, as their value is constant during the simulation. noise is declared as the same throughout the population with the population flag.\nThe equations for v and u are direct translations of their mathematical counterparts. Note the use of dx/dt for the time derivative and ^2 for the square function.\nThe input voltage I is defined as the sum of:\n\nthe total conductance of excitatory synapses g_exc,\nthe total conductance of inhibitory synapses -g_inh (in this example, we consider all weights to be positive, so we need to invert g_inh in order to model inhibitory synapses),\na random number taken from the normal distribution N(0,1) and multiplied by the noise scale noise.\n\nIn the pulse-coupled network, synapses are considered as instantaneous, i.e. a pre-synaptic spikes increases immediately the post-synaptic conductance proportionally to the weight of the synapse, but does not leave further trace. As this is the default behavior in ANNarchy, nothing has to be specified in the neuron’s equations.\nThe spike argument specifies the condition for when a spike should be emitted (here the membrane potential v should be greater than v_thresh). The reset argument specifies the changes to neural variables that should occur after a spike is emitted: here, the membrane potential is reset to the resting potential c and the membrane recovery variable u is increased from d.\nThe Izhikevich neuron is already defined in ANNarchy, so we will use it directly.",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "notebooks/Izhikevich.html#defining-the-populations",
    "href": "notebooks/Izhikevich.html#defining-the-populations",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "Defining the populations",
    "text": "Defining the populations\nWe start by defining a population of 1000 Izhikevich neurons and split it into 800 excitatory neurons and 200 inhibitory ones:\n\nimport numpy as np\n\nimport ANNarchy as ann\nann.clear()\n\npop = ann.Population(geometry=1000, neuron=ann.Izhikevich)\n\nExc = pop[:800]\nInh = pop[800:]\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nExc and Inh are subsets of pop, which have the same properties as a population. We can then set parameters differently for each population:\n\nre = np.random.random(800)      ; ri = np.random.random(200)\nExc.noise = 5.0                 ; Inh.noise = 2.0\nExc.a = 0.02                    ; Inh.a = 0.02 + 0.08 * ri\nExc.b = 0.2                     ; Inh.b = 0.25 - 0.05 * ri\nExc.c = -65.0 + 15.0 * re**2    ; Inh.c = -65.0\nExc.d = 8.0 - 6.0 * re**2       ; Inh.d = 2.0\nExc.v = -65.0                   ; Inh.v = -65.0\nExc.u = Exc.v * Exc.b           ; Inh.u = Inh.v * Inh.b",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "notebooks/Izhikevich.html#defining-the-projections",
    "href": "notebooks/Izhikevich.html#defining-the-projections",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "Defining the projections",
    "text": "Defining the projections\nWe can now define the connections within the network:\n\nThe excitatory neurons are connected to all neurons with a weight randomly chosen in [0, 0.5]\nThe inhibitory neurons are connected to all neurons with a weight randomly chosen in [0, 1]\n\n\nexc_proj = ann.Projection(pre=Exc, post=pop, target='exc')\nexc_proj.connect_all_to_all(weights=ann.Uniform(0.0, 0.5))\n   \ninh_proj = ann.Projection(pre=Inh, post=pop, target='inh')\ninh_proj.connect_all_to_all(weights=ann.Uniform(0.0, 1.0))\n\n&lt;ANNarchy.core.Projection.Projection at 0x116d56110&gt;\n\n\nThe network is now ready, we can compile:\n\nann.compile()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "notebooks/Izhikevich.html#running-the-simulation",
    "href": "notebooks/Izhikevich.html#running-the-simulation",
    "title": "Izhikevich’s pulse-coupled network",
    "section": "Running the simulation",
    "text": "Running the simulation\nWe start by monitoring the spikes and membrane potential in the whole population:\n\nM = ann.Monitor(pop, ['spike', 'v'])\n\nWe run the simulation for 1000 milliseconds:\n\nann.simulate(1000.0, measure_time=True)\n\nSimulating 1.0 seconds of the network took 0.04808664321899414 seconds. \n\n\nWe retrieve the recordings, generate a raster plot and the population firing rate:\n\nspikes = M.get('spike')\nv = M.get('v')\nt, n = M.raster_plot(spikes)\nfr = M.histogram(spikes)\n\nWe plot:\n\nThe raster plot of population\nThe evolution of the membrane potential of a single excitatory neuron\nThe population firing rate\n\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(12, 12))\n\n# First plot: raster plot\nplt.subplot(311)\nplt.plot(t, n, 'b.')\nplt.title('Raster plot')\n\n# Second plot: membrane potential of a single excitatory cell\nplt.subplot(312)\nplt.plot(v[:, 15]) # for example\nplt.title('Membrane potential')\n\n# Third plot: number of spikes per step in the population.\nplt.subplot(313)\nplt.plot(fr)\nplt.title('Number of spikes')\nplt.xlabel('Time (ms)')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Izhikevich"
    ]
  },
  {
    "objectID": "notebooks/Ramp.html",
    "href": "notebooks/Ramp.html",
    "title": "Homeostatic STDP",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis example is a reimplementation of the mechanism described in:\n\nCarlson, K.D.; Richert, M.; Dutt, N.; Krichmar, J.L., “Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks,” in Neural Networks (IJCNN), The 2013 International Joint Conference on , vol., no., pp.1-8, 4-9 Aug. 2013. doi: 10.1109/IJCNN.2013.6706961\n\nIt is based on the corresponding Carlsim tutorial:\nhttp://www.socsci.uci.edu/~jkrichma/CARLsim/doc/tut3_plasticity.html\nThis notebook focuses on the simple “Ramp” experiment, but the principle is similar for the self-organizing receptive fields (SORF) in the next notebook.\n\nimport numpy as np\n\nimport ANNarchy as ann\nann.clear()\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nThe network uses regular-spiking Izhikevich neurons (see the Izhikevich notebook), but using exponentially-decaying conductances and NMDA synapses:\n\nRSNeuron = ann.Neuron(\n    parameters = \"\"\"\n        a = 0.02 : population\n        b = 0.2 : population\n        c = -65. : population\n        d = 8. : population\n        tau_ampa = 5. : population\n        tau_nmda = 150. : population\n        vrev = 0.0 : population\n    \"\"\" ,\n    equations=\"\"\"\n        # Inputs\n        I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)  \n        # Membrane potential and recovery variable are solved using the midpoint method for stability     \n        dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint\n        du/dt = a * (b*v - u) : init=-13., midpoint\n        # AMPA and NMDA conductances\n        tau_ampa * dg_ampa/dt = -g_ampa : exponential\n        tau_nmda * dg_nmda/dt = -g_nmda : exponential\n    \"\"\" , \n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\", \n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\"\n)\n\nThe main particularity about NMDA synaptic models is that a single synaptic connection influences two conductances:\n\nThe AMPA conductance, which primarily drives the post-synaptic neuron:\n\n\n    I_\\text{AMPA} = g_\\text{AMPA} \\times (V_\\text{rev} - V)\n\n\nThe NMDA conductance, which is non-linearly dependent on the membrane potential:\n\n\n    I_\\text{NMDA} = g_\\text{NMDA} \\times \\frac{(\\frac{V - V_\\text{NMDA}}{\\sigma})^2}{1 + (\\frac{V - V_\\text{NMDA}}{\\sigma})^2} \\times (V_\\text{rev} - V)\n\nIn short, the NMDA conductance only increases if the post-synaptic neuron is already depolarized.\nThe nmda function is defined in the functions argument for readability. The parameters V_\\text{NMDA} =-80 \\text{mV} and \\sigma = 60 \\text{mV} are here hardcoded in the equation, but they could be defined as global parameters.\nThe AMPA and NMDA conductances are exponentially decreasing with different time constants:\n\n    \\tau_\\text{AMPA} \\frac{dg_\\text{AMPA}(t)}{dt} + g_\\text{AMPA}(t) = 0\n \n    \\tau_\\text{NMDA} \\frac{dg_\\text{NMDA}(t)}{dt} + g_\\text{NMDA}(t) = 0\n\nAnother thing to notice in this neuron model is that the differential equations for the membrane potential and recovery variable are solved concurrently using the midpoint numerical method for stability: the semi-implicit method initially proposed by Izhikevich would fail.\nThe input of the network is a population of 100 Poisson neurons, whose firing rate vary linearly from 0.2 to 20 Hz:\n\n# Input population\ninp = ann.PoissonPopulation(100, rates=np.linspace(0.2, 20., 100))\n\nWe will consider two RS neurons, one learning inputs from the Poisson population using the regular STDP, the other learning using the proposed homeostatic STDP:\n\n# RS neuron without homeostatic mechanism\npop1 = ann.Population(1, RSNeuron)\n\n# RS neuron with homeostatic mechanism\npop2 = ann.Population(1, RSNeuron)\n\nThe regular STDP used in the article is a nearest-neighbour variant, which integrates LTP and LTD traces triggered after each pre- or post-synaptic spikes, respectively.\nContrary to the STDP synapse provided by ANNarchy, weight changes occur at each each time step:\n\nIn a post-pre interval, weight changes follow the LTP trace,\nIn a pre-post interval, weight changes follow the LTD trace.\n\nThe weights are clipped between 0 and w_\\text{max}.\n\nnearest_neighbour_stdp = ann.Synapse(\n    parameters=\"\"\"\n        tau_plus = 20. : projection\n        tau_minus = 60. : projection\n        A_plus = 0.0002 : projection\n        A_minus = 0.000066 : projection\n        w_max = 0.03 : projection\n    \"\"\",\n    equations = \"\"\"\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n        # Nearest-neighbour\n        w += if t_post &gt;= t_pre: ltp else: - ltd : min=0.0, max=w_max\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\"\n)\n\nThe homeostatic STDP rule proposed by Carlson et al. is more complex. It has a regular STDP part (the nearest-neighbour variant above) and a homeostatic regularization part, ensuring that the post-synaptic firing rate R does not exceed a target firing rate R_\\text{target} = 35 Hz.\nThe firing rate of a spiking neuron can be automatically computed by ANNarchy (see later). It is then accessible as the variable r of the neuron (as if it were a regular rate-coded neuron).\nThe homeostatic STDP rule is defined by:\n\n    \\Delta w = K \\, (\\alpha  \\, (1 - \\frac{R}{R_\\text{target}}) \\, w + \\beta \\, \\text{stdp} )\n\nwhere stdp is the regular STDP weight change, and K is a firing rate-dependent learning rate:\n\n    K =  \\frac{R}{ T \\, (1 + |1 - \\gamma \\, \\frac{R}{R_\\text{target}}|})\n\nwith T being the window over which the mean firing rate is computed (5 seconds) and \\alpha, \\beta, \\gamma are parameters.\n\nhomeo_stdp = ann.Synapse(\n    parameters=\"\"\"\n        # STDP\n        tau_plus = 20. : projection\n        tau_minus = 60. : projection\n        A_plus = 0.0002 : projection\n        A_minus = 0.000066 : projection\n        w_min = 0.0 : projection\n        w_max = 0.03 : projection\n\n        # Homeostatic regulation\n        alpha = 0.1 : projection\n        beta = 1.0 : projection\n        gamma = 50. : projection\n        Rtarget = 35. : projection\n        T = 5000. : projection\n    \"\"\",\n    equations = \"\"\"\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n        # Homeostatic values\n        R = post.r : postsynaptic\n        K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic\n        # Nearest-neighbour\n        stdp = if t_post &gt;= t_pre: ltp else: - ltd \n        w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\" \n)\n\nThis rule necessitates that the post-synaptic neurons compute their average firing rate over a 5 seconds window. This has to be explicitely enabled, as it would be computationally too expensive to allow it by default:\n\npop1.compute_firing_rate(5000.)\npop2.compute_firing_rate(5000.)\n\nWe can now fully connect the input population to the two neurons with random weights:\n\n# Projection without homeostatic mechanism\nproj1 = ann.Projection(inp, pop1, ['ampa', 'nmda'], synapse=nearest_neighbour_stdp)\nproj1.connect_all_to_all(ann.Uniform(0.01, 0.03))\n\n# Projection with homeostatic mechanism\nproj2 = ann.Projection(inp, pop2, ['ampa', 'nmda'], synapse=homeo_stdp)\nproj2.connect_all_to_all(weights=ann.Uniform(0.01, 0.03))\n\n&lt;ANNarchy.core.Projection.Projection at 0x142fa6850&gt;\n\n\nNote that the same weights will target both AMPA and NMDA conductances in the post-synaptic neurons. By default, the argument target of Projection should be a string, but you can also pass a list of strings to reach several conductances with the same weights.\nWe can now compile and simulate for 1000 seconds while recording the relevat information:\n\nann.compile()\n\nCompiling ...  OK \n\n\n\n# Record\nm1 = ann.Monitor(pop1, 'r')\nm2 = ann.Monitor(pop2, 'r')\nm3 = ann.Monitor(proj1[0], 'w', period=1000.)\nm4 = ann.Monitor(proj2[0], 'w', period=1000.)\n\n# Simulate\nT = 1000 # 1000s\nann.simulate(T*1000., True)\n\n# Get the data\ndata1 = m1.get('r')\ndata2 = m2.get('r')\ndata3 = m3.get('w')\ndata4 = m4.get('w')\n\nprint('Mean Firing Rate without homeostasis:', np.mean(data1[:, 0]))\nprint('Mean Firing Rate with homeostasis:', np.mean(data2[:, 0]))\n\nSimulating 1000.0 seconds of the network took 1.0793671607971191 seconds. \nMean Firing Rate without homeostasis: 55.573825200000016\nMean Firing Rate with homeostasis: 35.24460820000001\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\n\nplt.subplot(311)\nplt.plot(np.linspace(0, T, len(data1[:, 0])), data1[:, 0], 'r-', label=\"Without homeostasis\")\nplt.plot(np.linspace(0, T, len(data2[:, 0])), data2[:, 0], 'b-', label=\"With homeostasis\")\nplt.xlabel('Time (s)')\nplt.ylabel('Firing rate (Hz)')\n\nplt.subplot(312)\nplt.plot(data3[-1, :], 'r-')\nplt.plot(data4[-1, :], 'bx')\naxes = plt.gca()\naxes.set_ylim([0., 0.035])\nplt.xlabel('# neuron')\nplt.ylabel('Weights after 1000s')\n\nplt.subplot(313)\nplt.imshow(np.array(data4, dtype='float').T, aspect='auto', cmap='hot')\nplt.xlabel('Time (s)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\n\n\n\n\n\nWe see that without homeostasis, the post-synaptic neuron reaches quickly a firing of 55 Hz, with all weights saturating at their maximum value 0.03. This is true even for inputs as low as 0.2Hz.\nMeanwhile, with homeostasis, the post-synaptic neuron gets a firing rate of 35 Hz (its desired value), and the weights from the input population are proportional to the underlying activity.",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Homeostatic STDP - Ramp"
    ]
  },
  {
    "objectID": "notebooks/BoldMonitoring1.html",
    "href": "notebooks/BoldMonitoring1.html",
    "title": "BOLD monitoring - Balloon model",
    "section": "",
    "text": "#!pip install ANNarchy\nThis notebook demonstrates the usage of the BOLD monitoring extension. It has to be explicitly imported:\nimport numpy as np\n\nimport ANNarchy as ann\nfrom ANNarchy.extensions.bold import *\n\nANNarchy 4.8 (4.8.0) on darwin (posix).",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "BOLD monitor I"
    ]
  },
  {
    "objectID": "notebooks/BoldMonitoring1.html#background",
    "href": "notebooks/BoldMonitoring1.html#background",
    "title": "BOLD monitoring - Balloon model",
    "section": "Background",
    "text": "Background\nANNarchy pre-implements some model variants of the BOLD models presented in Stephan et al. (2007) which are variations of the Balloon Model originally designed by Buxton et al. (1998). The four balloon variants model pre-implemented in ANNarchy follow the naming scheme of Stephan et al. (2007). The model name starts with balloon_ followed by either C (i. e. classical coefficient) or R (i. e. revised coefficient) and then followed by either N which means non-linear BOLD equation or L which means linear bold equation.\nWe only provide here the equations without much explanations, for more details please refer to the literature:\n\nBuxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855–864. doi:10.1002/mrm.1910390602\n\n\nFriston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466–477\n\n\nBuxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220–S233. doi:10.1016/j.neuroimage.2004.07.013\n\n\nStephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387–401. doi:10.1016/j.neuroimage.2007.07.040\n\n\nMaith et al. (2021). A computational model-based analysis of basal ganglia pathway changes in Parkinson’s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278– 2295. doi:10.1111/ejn.14868\n\n\nMaith et al. (2022). BOLD Monitoring in the Neural Simulator ANNarchy. Frontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966.",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "BOLD monitor I"
    ]
  },
  {
    "objectID": "notebooks/BoldMonitoring1.html#single-input-balloon-model",
    "href": "notebooks/BoldMonitoring1.html#single-input-balloon-model",
    "title": "BOLD monitoring - Balloon model",
    "section": "Single input Balloon model",
    "text": "Single input Balloon model\nThis script shows a simple example for the application of one of the default models (the balloon_RN model) on a simple network. The balloon_RN model is described by the following equations:\n\n    \\tau \\cdot \\frac{ds}{dt} = \\phi \\cdot I_\\text{CBF} - \\kappa \\cdot s - \\gamma \\cdot (f_{in} - 1)\n\n\n    \\frac{df_{in}}{dt} = s\n\n\n    E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} }\n\n\n    \\tau \\cdot \\frac{dv}{dt} = \\frac{1}{\\tau_0} \\cdot (f_{in} - f_{out})\n\n\n    f_{out} = v^{\\frac{1}{\\alpha}}\n\n\n    \\tau \\cdot \\frac{dq}{dt} = \\frac{1}{\\tau_0} \\cdot ( f_{in} \\cdot \\frac{E}{E_0} - \\frac{q}{v} \\cdot f_{out} )\n\nwith revised coefficients and non-linear bold equation:\nk_1            = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\nk_2            = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\nk_3            = 1 - \\epsilon\n\n    BOLD = v_0 \\cdot ( k_1 \\cdot (1-q) + k_2 \\cdot (1 - \\dfrac{q}{v}) + k_3 \\cdot (1 - v) )\n\nThere are two important variables in that model: BOLD which is the output of the model and I_CBF which is the input signal, reflecting the mean firing rate of the input populations.\nAs the BOLD model reflects the change of activity levels in the recorded area, we will implement the following experiment. We record from two populations with 100 Izhikevich neurons each. After a short period of time we raise the activity level of one population for a fixed time window. For simplicity, we do not use connections between the neurons but simply increase the noise term in the Izhikevich model. This should lead to a noticeable BOLD signal.\n\nPopulations\nWe first create two populations of Izhikevich neurons:\n\nann.clear()\n\npop0 = ann.Population(100, neuron=ann.Izhikevich)\npop1 = ann.Population(100, neuron=ann.Izhikevich)\n\nAs we will not have any connections between the neurons, we need to increase the noise to create some baseline activity:\n\n# Set noise to create some baseline activity\npop0.noise = 5.0; pop1.noise = 5.0\n\nThe mean firing rate of the neurons will be used as an input to the BOLD model. This need to be activated explicitly as the computation of this value is quite expensive. In our example, the mean firing rate should be computed across a time window of 100 ms. The resulting value will be stored in the r attribute of the populations and can be easily recorded.\n\n# Compute mean firing rate in Hz on 100ms window\npop0.compute_firing_rate(window=100.0)\npop1.compute_firing_rate(window=100.0)\n\n# Record the mean firing rate\nmon_pop0 = ann.Monitor(pop0, [\"r\"], start=False)\nmon_pop1 = ann.Monitor(pop1, [\"r\"], start=False)\n\n\n\nBOLD Monitor definition\nThe BOLD monitor expects a list of populations which we want to record (in our case pop0 and pop1). A BOLD model should be specified, here we take balloon_RN which is the default. We then specify the mapping from the population variable that should influence the BOLD signal, in our case the mean firing rate r, to the input variable of the BOLD model I_CBF.\nThe mean firing rate is normalized over a time window of 2000 ms and we record both input and output signals:\n\nm_bold = BoldMonitor(\n    \n    populations = [pop0, pop1], # recorded populations\n    \n    bold_model = balloon_RN(), # BOLD model to use (default is balloon_RN)\n    \n    mapping = {'I_CBF': 'r'}, # mapping from pop.r to I_CBF\n    \n    normalize_input = 2000,  # time window to compute baseline.\n    \n    recorded_variables = [\"I_CBF\", \"BOLD\"]  # variables to be recorded\n)\n\nNow we can compile and initialize the network:\n\nann.compile()\n\nCompiling ...  OK \n\n\n\n\nSimulation\nWe first simulate 1 second biological time to ensure that the network reaches a stable firing rate:\n\nann.simulate(1000)\n\nWe then enable the recording of all monitors:\n\nmon_pop0.start()\nmon_pop1.start()\nm_bold.start()\n\nWe simulate for 5 seconds with lower noise and we increase the noise in pop0 for 5 seconds before decreasing it again:\n\n# We manipulate the noise for the half of the neurons\nann.simulate(5000)      # 5s with low noise\npop0.noise = 7.5\nann.simulate(5000)      # 5s with higher noise (one population)\npop0.noise = 5\nann.simulate(10000)     # 10s with low noise\n\n# Retrieve the recordings\nmean_fr1 = np.mean(mon_pop0.get(\"r\"), axis=1)\nmean_fr2 = np.mean(mon_pop1.get(\"r\"), axis=1)\n\ninput_data = m_bold.get(\"I_CBF\")\nbold_data = m_bold.get(\"BOLD\")\n\n\n\nEvaluation\nWe can now plot:\n\nthe mean firing rate in the input populations.\nthe recorded activity I which serves as an input to the BOLD model.\nthe resulting BOLD signal.\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20,6))\ngrid = plt.GridSpec(1, 3, left=0.05, right=0.95)\n\n# mean firing rate\nax1 = plt.subplot(grid[0, 0])\n\nax1.plot(mean_fr1, label=\"pop0\")\nax1.plot(mean_fr2, label=\"pop1\")\nplt.legend()\nax1.set_ylabel(\"Average mean firing rate [Hz]\", fontweight=\"bold\", fontsize=18)\n\n# BOLD input signal\nax2 = plt.subplot(grid[0, 1])\n\nax2.plot(input_data)\nax2.set_ylabel(\"BOLD input I_CBF\", fontweight=\"bold\", fontsize=18)\n\n# BOLD output signal\nax3 = plt.subplot(grid[0, 2])\n\nax3.plot(bold_data*100.0)\nax3.set_ylabel(\"BOLD [%]\", fontweight=\"bold\", fontsize=18)\n\n# x-axis labels as seconds\nfor ax in [ax1, ax2, ax3]:\n    ax.set_xticks(np.arange(0,21,2)*1000)\n    ax.set_xticklabels(np.arange(0,21,2))\n    ax.set_xlabel(\"time [s]\", fontweight=\"bold\", fontsize=18)\n\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "BOLD monitor I"
    ]
  },
  {
    "objectID": "License.html",
    "href": "License.html",
    "title": "License",
    "section": "",
    "text": "License\n                    GNU GENERAL PUBLIC LICENSE\n                       Version 2, June 1991\n\n Copyright (C) 1989, 1991 Free Software Foundation, Inc.,\n 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The licenses for most software are designed to take away your\nfreedom to share and change it.  By contrast, the GNU General Public\nLicense is intended to guarantee your freedom to share and change free\nsoftware--to make sure the software is free for all its users.  This\nGeneral Public License applies to most of the Free Software\nFoundation's software and to any other program whose authors commit to\nusing it.  (Some other Free Software Foundation software is covered by\nthe GNU Lesser General Public License instead.)  You can apply it to\nyour programs, too.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthis service if you wish), that you receive source code or can get it\nif you want it, that you can change the software or use pieces of it\nin new free programs; and that you know you can do these things.\n\n  To protect your rights, we need to make restrictions that forbid\nanyone to deny you these rights or to ask you to surrender the rights.\nThese restrictions translate to certain responsibilities for you if you\ndistribute copies of the software, or if you modify it.\n\n  For example, if you distribute copies of such a program, whether\ngratis or for a fee, you must give the recipients all the rights that\nyou have.  You must make sure that they, too, receive or can get the\nsource code.  And you must show them these terms so they know their\nrights.\n\n  We protect your rights with two steps: (1) copyright the software, and\n(2) offer you this license which gives you legal permission to copy,\ndistribute and/or modify the software.\n\n  Also, for each author's protection and ours, we want to make certain\nthat everyone understands that there is no warranty for this free\nsoftware.  If the software is modified by someone else and passed on, we\nwant its recipients to know that what they have is not the original, so\nthat any problems introduced by others will not reflect on the original\nauthors' reputations.\n\n  Finally, any free program is threatened constantly by software\npatents.  We wish to avoid the danger that redistributors of a free\nprogram will individually obtain patent licenses, in effect making the\nprogram proprietary.  To prevent this, we have made it clear that any\npatent must be licensed for everyone's free use or not licensed at all.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                    GNU GENERAL PUBLIC LICENSE\n   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n\n  0. This License applies to any program or other work which contains\na notice placed by the copyright holder saying it may be distributed\nunder the terms of this General Public License.  The \"Program\", below,\nrefers to any such program or work, and a \"work based on the Program\"\nmeans either the Program or any derivative work under copyright law:\nthat is to say, a work containing the Program or a portion of it,\neither verbatim or with modifications and/or translated into another\nlanguage.  (Hereinafter, translation is included without limitation in\nthe term \"modification\".)  Each licensee is addressed as \"you\".\n\nActivities other than copying, distribution and modification are not\ncovered by this License; they are outside its scope.  The act of\nrunning the Program is not restricted, and the output from the Program\nis covered only if its contents constitute a work based on the\nProgram (independent of having been made by running the Program).\nWhether that is true depends on what the Program does.\n\n  1. You may copy and distribute verbatim copies of the Program's\nsource code as you receive it, in any medium, provided that you\nconspicuously and appropriately publish on each copy an appropriate\ncopyright notice and disclaimer of warranty; keep intact all the\nnotices that refer to this License and to the absence of any warranty;\nand give any other recipients of the Program a copy of this License\nalong with the Program.\n\nYou may charge a fee for the physical act of transferring a copy, and\nyou may at your option offer warranty protection in exchange for a fee.\n\n  2. You may modify your copy or copies of the Program or any portion\nof it, thus forming a work based on the Program, and copy and\ndistribute such modifications or work under the terms of Section 1\nabove, provided that you also meet all of these conditions:\n\n    a) You must cause the modified files to carry prominent notices\n    stating that you changed the files and the date of any change.\n\n    b) You must cause any work that you distribute or publish, that in\n    whole or in part contains or is derived from the Program or any\n    part thereof, to be licensed as a whole at no charge to all third\n    parties under the terms of this License.\n\n    c) If the modified program normally reads commands interactively\n    when run, you must cause it, when started running for such\n    interactive use in the most ordinary way, to print or display an\n    announcement including an appropriate copyright notice and a\n    notice that there is no warranty (or else, saying that you provide\n    a warranty) and that users may redistribute the program under\n    these conditions, and telling the user how to view a copy of this\n    License.  (Exception: if the Program itself is interactive but\n    does not normally print such an announcement, your work based on\n    the Program is not required to print an announcement.)\n\nThese requirements apply to the modified work as a whole.  If\nidentifiable sections of that work are not derived from the Program,\nand can be reasonably considered independent and separate works in\nthemselves, then this License, and its terms, do not apply to those\nsections when you distribute them as separate works.  But when you\ndistribute the same sections as part of a whole which is a work based\non the Program, the distribution of the whole must be on the terms of\nthis License, whose permissions for other licensees extend to the\nentire whole, and thus to each and every part regardless of who wrote it.\n\nThus, it is not the intent of this section to claim rights or contest\nyour rights to work written entirely by you; rather, the intent is to\nexercise the right to control the distribution of derivative or\ncollective works based on the Program.\n\nIn addition, mere aggregation of another work not based on the Program\nwith the Program (or with a work based on the Program) on a volume of\na storage or distribution medium does not bring the other work under\nthe scope of this License.\n\n  3. You may copy and distribute the Program (or a work based on it,\nunder Section 2) in object code or executable form under the terms of\nSections 1 and 2 above provided that you also do one of the following:\n\n    a) Accompany it with the complete corresponding machine-readable\n    source code, which must be distributed under the terms of Sections\n    1 and 2 above on a medium customarily used for software interchange; or,\n\n    b) Accompany it with a written offer, valid for at least three\n    years, to give any third party, for a charge no more than your\n    cost of physically performing source distribution, a complete\n    machine-readable copy of the corresponding source code, to be\n    distributed under the terms of Sections 1 and 2 above on a medium\n    customarily used for software interchange; or,\n\n    c) Accompany it with the information you received as to the offer\n    to distribute corresponding source code.  (This alternative is\n    allowed only for noncommercial distribution and only if you\n    received the program in object code or executable form with such\n    an offer, in accord with Subsection b above.)\n\nThe source code for a work means the preferred form of the work for\nmaking modifications to it.  For an executable work, complete source\ncode means all the source code for all modules it contains, plus any\nassociated interface definition files, plus the scripts used to\ncontrol compilation and installation of the executable.  However, as a\nspecial exception, the source code distributed need not include\nanything that is normally distributed (in either source or binary\nform) with the major components (compiler, kernel, and so on) of the\noperating system on which the executable runs, unless that component\nitself accompanies the executable.\n\nIf distribution of executable or object code is made by offering\naccess to copy from a designated place, then offering equivalent\naccess to copy the source code from the same place counts as\ndistribution of the source code, even though third parties are not\ncompelled to copy the source along with the object code.\n\n  4. You may not copy, modify, sublicense, or distribute the Program\nexcept as expressly provided under this License.  Any attempt\notherwise to copy, modify, sublicense or distribute the Program is\nvoid, and will automatically terminate your rights under this License.\nHowever, parties who have received copies, or rights, from you under\nthis License will not have their licenses terminated so long as such\nparties remain in full compliance.\n\n  5. You are not required to accept this License, since you have not\nsigned it.  However, nothing else grants you permission to modify or\ndistribute the Program or its derivative works.  These actions are\nprohibited by law if you do not accept this License.  Therefore, by\nmodifying or distributing the Program (or any work based on the\nProgram), you indicate your acceptance of this License to do so, and\nall its terms and conditions for copying, distributing or modifying\nthe Program or works based on it.\n\n  6. Each time you redistribute the Program (or any work based on the\nProgram), the recipient automatically receives a license from the\noriginal licensor to copy, distribute or modify the Program subject to\nthese terms and conditions.  You may not impose any further\nrestrictions on the recipients' exercise of the rights granted herein.\nYou are not responsible for enforcing compliance by third parties to\nthis License.\n\n  7. If, as a consequence of a court judgment or allegation of patent\ninfringement or for any other reason (not limited to patent issues),\nconditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot\ndistribute so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you\nmay not distribute the Program at all.  For example, if a patent\nlicense would not permit royalty-free redistribution of the Program by\nall those who receive copies directly or indirectly through you, then\nthe only way you could satisfy both it and this License would be to\nrefrain entirely from distribution of the Program.\n\nIf any portion of this section is held invalid or unenforceable under\nany particular circumstance, the balance of the section is intended to\napply and the section as a whole is intended to apply in other\ncircumstances.\n\nIt is not the purpose of this section to induce you to infringe any\npatents or other property right claims or to contest validity of any\nsuch claims; this section has the sole purpose of protecting the\nintegrity of the free software distribution system, which is\nimplemented by public license practices.  Many people have made\ngenerous contributions to the wide range of software distributed\nthrough that system in reliance on consistent application of that\nsystem; it is up to the author/donor to decide if he or she is willing\nto distribute software through any other system and a licensee cannot\nimpose that choice.\n\nThis section is intended to make thoroughly clear what is believed to\nbe a consequence of the rest of this License.\n\n  8. If the distribution and/or use of the Program is restricted in\ncertain countries either by patents or by copyrighted interfaces, the\noriginal copyright holder who places the Program under this License\nmay add an explicit geographical distribution limitation excluding\nthose countries, so that distribution is permitted only in or among\ncountries not thus excluded.  In such case, this License incorporates\nthe limitation as if written in the body of this License.\n\n  9. The Free Software Foundation may publish revised and/or new versions\nof the General Public License from time to time.  Such new versions will\nbe similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\nEach version is given a distinguishing version number.  If the Program\nspecifies a version number of this License which applies to it and \"any\nlater version\", you have the option of following the terms and conditions\neither of that version or of any later version published by the Free\nSoftware Foundation.  If the Program does not specify a version number of\nthis License, you may choose any version ever published by the Free Software\nFoundation.\n\n  10. If you wish to incorporate parts of the Program into other free\nprograms whose distribution conditions are different, write to the author\nto ask for permission.  For software which is copyrighted by the Free\nSoftware Foundation, write to the Free Software Foundation; we sometimes\nmake exceptions for this.  Our decision will be guided by the two goals\nof preserving the free status of all derivatives of our free software and\nof promoting the sharing and reuse of software generally.\n\n                            NO WARRANTY\n\n  11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY\nFOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN\nOTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES\nPROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED\nOR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS\nTO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE\nPROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING,\nREPAIR OR CORRECTION.\n\n  12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR\nREDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES,\nINCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING\nOUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED\nTO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY\nYOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER\nPROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nconvey the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software; you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation; either version 2 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License along\n    with this program; if not, write to the Free Software Foundation, Inc.,\n    51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\nAlso add information on how to contact you by electronic and paper mail.\n\nIf the program is interactive, make it output a short notice like this\nwhen it starts in an interactive mode:\n\n    Gnomovision version 69, Copyright (C) year name of author\n    Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n\nThe hypothetical commands `show w' and `show c' should show the appropriate\nparts of the General Public License.  Of course, the commands you use may\nbe called something other than `show w' and `show c'; they could even be\nmouse-clicks or menu items--whatever suits your program.\n\nYou should also get your employer (if you work as a programmer) or your\nschool, if any, to sign a \"copyright disclaimer\" for the program, if\nnecessary.  Here is a sample; alter the names:\n\n  Yoyodyne, Inc., hereby disclaims all copyright interest in the program\n  `Gnomovision' (which makes passes at compilers) written by James Hacker.\n\n  &lt;signature of Ty Coon&gt;, 1 April 1989\n  Ty Coon, President of Vice\n\nThis General Public License does not permit incorporating your program into\nproprietary programs.  If your program is a subroutine library, you may\nconsider it more useful to permit linking proprietary applications with the\nlibrary.  If this is what you want to do, use the GNU Lesser General\nPublic License instead of this License.",
    "crumbs": [
      "ANNarchy",
      "License"
    ]
  },
  {
    "objectID": "manual/SpikeSynapse.html",
    "href": "manual/SpikeSynapse.html",
    "title": "Spiking synapses",
    "section": "",
    "text": "Synapses in spiking networks differ from rate-coded synapses in that they are event-driven, i.e. the most important changes occur whenever a pre- or post-synaptic spike is emitted. For this reason, additional arguments have to be passed to the Synapse object.\n\n\nIn the simplest case, a pre-synaptic spike increases a target conductance value in the post-synaptic neuron. The rule defining how this conductance is modified has to be placed in the pre_spike argument of a Synapse object.\nThe default spiking synapse in ANNarchy is equivalent to:\nDefaultSynapse = ann.Synapse(\n    parameters = \"w=0.0\",\n    equations = \"\",\n    pre_spike = \"\"\"\n        g_target += w\n    \"\"\"     \n) \nThe only thing it does is to increase the conductance g_target of the post-synaptic neuron (for example g_exc if the target is exc) every time a pre-synaptic spike arrives at the synapse, proportionally to the synaptic efficiency w of the synapse. Note that w is implicitely defined in all synapses, you will never need to define it explicitely.\nYou can override this default behavior by providing a new Synapse object when building a Projection. For example, you may want to implement a \"fatigue\" mechanism for the synapse, transciently reducing the synaptic efficiency when the pre-synaptic neuron fires too strongly.\nOne solution would be to decrease a synaptic variable everytime a pre-synaptic spike is received and increase the post-synaptic conductance proportionally to this value. When no spike is received, this trace variable should slowly return to its maximal value.\nFatigueSynapse = ann.Synapse(\n    parameters = \"\"\"\n        tau = 1000 : postsynaptic # Time constant of the trace is 1 second\n        dec = 0.05 : postsynaptic # Decrement of the trace\n    \"\"\",\n    equations = \"\"\"\n        tau * dtrace/dt + trace = 1.0 : min = 0.0\n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w * trace\n        trace -= dec\n    \"\"\"     \n) \nEach time a pre-synaptic spike occurs, the post-synaptic conductance is increased from w*trace. As the baseline of trace is 1.0 (as defined in equations), this means that a \"fresh\" synapse will use the full synaptic efficiency. However, after each pre-synaptic spike, trace is decreased from dec = 0.05, meaning that the \"real\" synaptic efficiency can go down to 0.0 (the minimal value of trace) if the pre-synaptic neuron fires too often.\nIt is important here to restrict trace to positive values with the flags min=0.0, as it could otherwise transform an excitatory synapse into an inhibitory one.\n\nIt is obligatory to use the keyword g_target for the post-synaptic conductance. This value relates to the corresponding value in post-synaptic neuron: The target will be replaced with the projection’s target (for example exc or inh). So if you use this synapse in a projection with target = 'exc', the value of g_exc in post-synaptic neuron will be automatically replaced.\n\n\n\n\nIn spiking networks, there are usually two methods to implement event-driven synaptic plasticity (see the entry on STDP at Scholarpedia):\n\nby using the difference in spike times between the pre- and post-synaptic neurons;\nby using online implementations.\n\n\n\nA Synapse has access to two specific variables:\n\nt_pre corresponding to the time of the last pre-synaptic spike in milliseconds.\nt_post corresponding to the time of the last post-synaptic spike in milliseconds.\n\nThese times are relative to the creation of the network, so they only make sense when compared to each other or to t.\nSpike-timing dependent plasticity can for example be implemented the following way:\nSTDP = ann.Synapse(\n    parameters = \"\"\"\n        tau_pre = 10.0 : projection\n        tau_post = 10.0 : projection\n        cApre = 0.01 : projection\n        cApost = 0.0105 : projection\n        wmax = 0.01 : projection\n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w\n        w = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \n    \"\"\",                  \n    post_spike = \"\"\"\n        w = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax)\n    \"\"\"      \n) \npre_spike\nEvery time a pre-synaptic spike arrives at the synapse (pre_spike), the post-synaptic conductance is increased from the current value of the synaptic efficiency.\ng_target += w\nWhen a synapse object is defined, this behavior should be explicitely declared.\nThe value w is then decreased using a decreasing exponential function of the time elapsed since the last post-synaptic spike:\nw = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \nThe clip() global function is there to ensure that w is bounded between 0.0 and wmax. As t &gt;= t_post, the exponential part is smaller than 1.0. The pre_spike argument therefore ensures that the synapse is depressed is a pre-synaptic spike occurs shortly after a post-synaptic one. \"Shortly\" is quantified by the time constant tau_post, usually in the range of 10 ms.\npost_spike\nEvery time a post-synaptic spike is emitted (post_spike), the value w is increased proportionally to the time elapsed since the last pre-synaptic spike:\nw = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax)\nThis term defines the potentiation of a synapse when a pre-synaptic spike is followed immediately by a post-synaptic one: the inferred causality between the two events should be reinforced.\n\n\n\n\n\n\nWarning\n\n\n\nOnly the last pre- and post-synaptic spikes are accessible, not the whole history. Only nearest-neighbor spike-interactions are possible using ANNarchy, not temporal all-to-all interactions where the whole spike history is used for learning (see the entry on STDP at Scholarpedia).\nSome networks may not work properly when using this simulation mode. For example, whenever the pre-synaptic neurons fires twice in a very short interval and causes a post-synaptic spike, the corresponding weight should be reinforced twice. With the proposed STDP rule, it would be reinforced only once.\nIt is therefore generally advised to use online versions of STDP.\n\n\n\n\n\nThe online version of STDP requires two synaptic traces, which are increased whenever a pre- resp. post-synaptic spike is perceived, and decay with their own dynamics in between. Using the same vocabulary as Brian, such an implementation would be:\nSTDP_online = ann.Synapse(\n    parameters = \"\"\"\n        tau_pre = 10.0 : projection\n        tau_post = 10.0 : projection\n        cApre = 0.01 : projection\n        cApost = 0.0105 : projection\n        wmax = 0.01 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_pre * dApre/dt = - Apre : event-driven\n        tau_post * dApost/dt = - Apost : event-driven\n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre * wmax\n        w = clip(w - Apost, 0.0 , wmax)\n    \"\"\",                  \n    post_spike = \"\"\"\n        Apost += cApost * wmax\n        w = clip(w + Apre, 0.0 , wmax)\n    \"\"\"      \n) \nThe variables Apre and Apost are exponentially decreasing traces of pre- and post-synaptic spikes, as shown by the leaky integration in equations. When a pre-synaptic spike is emitted, Apre is incremented, the conductance level of the post-synaptic neuron g_target too, and the synaptic efficiency is decreased proportionally to Apost (this means that if a post-synaptic spike was emitted shortly before, LTD will strongly be applied, while if it was longer ago, no major change will be observed). When a post-synaptic spike is observed, Apost increases and the synaptic efficiency is increased proportionally to Apre.\nThe effect of this online version is globally the same as the spike timing dependent version, except that the history of pre- and post-synaptic spikes is fully contained in the variables Apre and Apost.\nThe event-driven keyword allows event-driven integration of the variables Apre and Apost. This means the equations are not updated at each time step, but only when a pre- or post-synaptic spike occurs at the synapse. This is only possible because the two variables follow linear first-order ODEs. The event-driven integration method allows to spare a lot of computations if the number of spikes is not too high in the network.\n\n\n\nThree types of updates are potentially executed at every time step:\n\nPre-synaptic events, defined by pre_spike and triggered after each pre-synaptic spike, after a delay of at least dt.\nSynaptic variables defined by equations.\nPost-synaptic events, defined by post_spike and triggered after each post-synaptic spike, without delay.\n\nThese updates are conducted in that order at each time step. First, all spikes emitted in the previous step (or earlier if there are delays) are propagated to the corresponding synapses and influence variables there (especially conductance increases), then all synaptic variables are updated according to their ODE (after the neurons' equations are updated), then all neurons which have emitted a spike in the current step modify their synapses.\nA potential problem arises when a pre-synaptic and a post-synaptic spike are emitted at the same time. STDP-like plasticity rules are usually not defined when the spike time difference is 0, as the two spikes can not be correlated in that case (the pre-spike can not possibly be the cause of the post-spike).\nBy default, both event-driven updates (pre_spike leading to LTD, post_spike leading to LTP) will be conducted when the spikes are emitted at the same time. This can be problematic for some plastic models, for example the simple_stdp example provided in the source code.\nTo avoid this problem, the flag unless_post can be specified in pre_spike to indicate that the corresponding variable should be updated after each pre-synaptic spike, unless the post-synaptic neuron also fired at the previous time step. Without even-driven integration, the online STDP learning rule would become:\nSTDP_online = ann.Synapse(\n    parameters = \"\"\"\n        tau_pre = 10.0 : projection\n        tau_post = 10.0 : projection\n        cApre = 0.01 : projection\n        cApost = 0.0105 : projection\n        wmax = 0.01 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_pre * dApre/dt = - Apre \n        tau_post * dApost/dt = - Apost \n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre : unless_post\n        w = clip(w - Apost, 0.0 , wmax) : unless_post\n    \"\"\",                  \n    post_spike = \"\"\"\n        Apost += cApost\n        w = clip(w + Apre, 0.0 , wmax)\n    \"\"\"      \n) \n\n\n\n\nIn some cases, synaptic transmission cannot be described in an event-driven framework. Synapses using the NMDA neurotransmitter are for example often modeled as non-linear synapses. Non-linear synapses can require the post-synaptic conductance to be a sum of synapse-specific variables, as for rate-coded neurons, and not simply incremented when a pre-synaptic spike occurs. NMDA synapses can be represented by two variables x(t) and g(t) following first-order ODEs:\n\\begin{aligned}\n\\begin{aligned}\n\\tau \\cdot \\frac{dx(t)}{dt} &= - x(t) \\\\\n\\tau \\cdot \\frac{dg(t)}{dt} &= - g(t) +  x(t) \\cdot (1 - g(t))\n\\end{aligned}\n\\end{aligned}\nWhen a pre-synaptic spike occurs, x(t) is incremented by the weight w(t). However, it does not influence directly the post-synaptic neuron, as the output of a synapse is the signal g(t). The post-synaptic conductance is defined at each time t as the sum over all synapses of the same type of their variable g(t):\ng_\\text{exc}(t) = \\sum_{i=1}^{N_\\text{exc}} g_i (t)\nSuch a synapse could be implemented the following way:\nNMDA = ann.Synapse(\n    parameters = \"\"\"\n    tau = 10.0 : projection\n    \"\"\",\n    equations = \"\"\"\n    tau * dx/dt = -x\n    tau * dg/dt = -g +  x * (1 -g)\n    \"\"\", \n    pre_spike = \"x += w\",\n    psp = \"g\"\n)\nThe synapse defines a psp argument which means that the output of this synapse is non-linear and the post-synaptic conductance should be summed over this value (g in this case). It is not possible to use the event-driven integration scheme for such non-linear synapses.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Spiking synapses"
    ]
  },
  {
    "objectID": "manual/SpikeSynapse.html#increase-of-conductance-after-a-pre-synaptic-spike",
    "href": "manual/SpikeSynapse.html#increase-of-conductance-after-a-pre-synaptic-spike",
    "title": "Spiking synapses",
    "section": "",
    "text": "In the simplest case, a pre-synaptic spike increases a target conductance value in the post-synaptic neuron. The rule defining how this conductance is modified has to be placed in the pre_spike argument of a Synapse object.\nThe default spiking synapse in ANNarchy is equivalent to:\nDefaultSynapse = ann.Synapse(\n    parameters = \"w=0.0\",\n    equations = \"\",\n    pre_spike = \"\"\"\n        g_target += w\n    \"\"\"     \n) \nThe only thing it does is to increase the conductance g_target of the post-synaptic neuron (for example g_exc if the target is exc) every time a pre-synaptic spike arrives at the synapse, proportionally to the synaptic efficiency w of the synapse. Note that w is implicitely defined in all synapses, you will never need to define it explicitely.\nYou can override this default behavior by providing a new Synapse object when building a Projection. For example, you may want to implement a \"fatigue\" mechanism for the synapse, transciently reducing the synaptic efficiency when the pre-synaptic neuron fires too strongly.\nOne solution would be to decrease a synaptic variable everytime a pre-synaptic spike is received and increase the post-synaptic conductance proportionally to this value. When no spike is received, this trace variable should slowly return to its maximal value.\nFatigueSynapse = ann.Synapse(\n    parameters = \"\"\"\n        tau = 1000 : postsynaptic # Time constant of the trace is 1 second\n        dec = 0.05 : postsynaptic # Decrement of the trace\n    \"\"\",\n    equations = \"\"\"\n        tau * dtrace/dt + trace = 1.0 : min = 0.0\n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w * trace\n        trace -= dec\n    \"\"\"     \n) \nEach time a pre-synaptic spike occurs, the post-synaptic conductance is increased from w*trace. As the baseline of trace is 1.0 (as defined in equations), this means that a \"fresh\" synapse will use the full synaptic efficiency. However, after each pre-synaptic spike, trace is decreased from dec = 0.05, meaning that the \"real\" synaptic efficiency can go down to 0.0 (the minimal value of trace) if the pre-synaptic neuron fires too often.\nIt is important here to restrict trace to positive values with the flags min=0.0, as it could otherwise transform an excitatory synapse into an inhibitory one.\n\nIt is obligatory to use the keyword g_target for the post-synaptic conductance. This value relates to the corresponding value in post-synaptic neuron: The target will be replaced with the projection’s target (for example exc or inh). So if you use this synapse in a projection with target = 'exc', the value of g_exc in post-synaptic neuron will be automatically replaced.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Spiking synapses"
    ]
  },
  {
    "objectID": "manual/SpikeSynapse.html#synaptic-plasticity",
    "href": "manual/SpikeSynapse.html#synaptic-plasticity",
    "title": "Spiking synapses",
    "section": "",
    "text": "In spiking networks, there are usually two methods to implement event-driven synaptic plasticity (see the entry on STDP at Scholarpedia):\n\nby using the difference in spike times between the pre- and post-synaptic neurons;\nby using online implementations.\n\n\n\nA Synapse has access to two specific variables:\n\nt_pre corresponding to the time of the last pre-synaptic spike in milliseconds.\nt_post corresponding to the time of the last post-synaptic spike in milliseconds.\n\nThese times are relative to the creation of the network, so they only make sense when compared to each other or to t.\nSpike-timing dependent plasticity can for example be implemented the following way:\nSTDP = ann.Synapse(\n    parameters = \"\"\"\n        tau_pre = 10.0 : projection\n        tau_post = 10.0 : projection\n        cApre = 0.01 : projection\n        cApost = 0.0105 : projection\n        wmax = 0.01 : projection\n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w\n        w = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \n    \"\"\",                  \n    post_spike = \"\"\"\n        w = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax)\n    \"\"\"      \n) \npre_spike\nEvery time a pre-synaptic spike arrives at the synapse (pre_spike), the post-synaptic conductance is increased from the current value of the synaptic efficiency.\ng_target += w\nWhen a synapse object is defined, this behavior should be explicitely declared.\nThe value w is then decreased using a decreasing exponential function of the time elapsed since the last post-synaptic spike:\nw = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \nThe clip() global function is there to ensure that w is bounded between 0.0 and wmax. As t &gt;= t_post, the exponential part is smaller than 1.0. The pre_spike argument therefore ensures that the synapse is depressed is a pre-synaptic spike occurs shortly after a post-synaptic one. \"Shortly\" is quantified by the time constant tau_post, usually in the range of 10 ms.\npost_spike\nEvery time a post-synaptic spike is emitted (post_spike), the value w is increased proportionally to the time elapsed since the last pre-synaptic spike:\nw = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax)\nThis term defines the potentiation of a synapse when a pre-synaptic spike is followed immediately by a post-synaptic one: the inferred causality between the two events should be reinforced.\n\n\n\n\n\n\nWarning\n\n\n\nOnly the last pre- and post-synaptic spikes are accessible, not the whole history. Only nearest-neighbor spike-interactions are possible using ANNarchy, not temporal all-to-all interactions where the whole spike history is used for learning (see the entry on STDP at Scholarpedia).\nSome networks may not work properly when using this simulation mode. For example, whenever the pre-synaptic neurons fires twice in a very short interval and causes a post-synaptic spike, the corresponding weight should be reinforced twice. With the proposed STDP rule, it would be reinforced only once.\nIt is therefore generally advised to use online versions of STDP.\n\n\n\n\n\nThe online version of STDP requires two synaptic traces, which are increased whenever a pre- resp. post-synaptic spike is perceived, and decay with their own dynamics in between. Using the same vocabulary as Brian, such an implementation would be:\nSTDP_online = ann.Synapse(\n    parameters = \"\"\"\n        tau_pre = 10.0 : projection\n        tau_post = 10.0 : projection\n        cApre = 0.01 : projection\n        cApost = 0.0105 : projection\n        wmax = 0.01 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_pre * dApre/dt = - Apre : event-driven\n        tau_post * dApost/dt = - Apost : event-driven\n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre * wmax\n        w = clip(w - Apost, 0.0 , wmax)\n    \"\"\",                  \n    post_spike = \"\"\"\n        Apost += cApost * wmax\n        w = clip(w + Apre, 0.0 , wmax)\n    \"\"\"      \n) \nThe variables Apre and Apost are exponentially decreasing traces of pre- and post-synaptic spikes, as shown by the leaky integration in equations. When a pre-synaptic spike is emitted, Apre is incremented, the conductance level of the post-synaptic neuron g_target too, and the synaptic efficiency is decreased proportionally to Apost (this means that if a post-synaptic spike was emitted shortly before, LTD will strongly be applied, while if it was longer ago, no major change will be observed). When a post-synaptic spike is observed, Apost increases and the synaptic efficiency is increased proportionally to Apre.\nThe effect of this online version is globally the same as the spike timing dependent version, except that the history of pre- and post-synaptic spikes is fully contained in the variables Apre and Apost.\nThe event-driven keyword allows event-driven integration of the variables Apre and Apost. This means the equations are not updated at each time step, but only when a pre- or post-synaptic spike occurs at the synapse. This is only possible because the two variables follow linear first-order ODEs. The event-driven integration method allows to spare a lot of computations if the number of spikes is not too high in the network.\n\n\n\nThree types of updates are potentially executed at every time step:\n\nPre-synaptic events, defined by pre_spike and triggered after each pre-synaptic spike, after a delay of at least dt.\nSynaptic variables defined by equations.\nPost-synaptic events, defined by post_spike and triggered after each post-synaptic spike, without delay.\n\nThese updates are conducted in that order at each time step. First, all spikes emitted in the previous step (or earlier if there are delays) are propagated to the corresponding synapses and influence variables there (especially conductance increases), then all synaptic variables are updated according to their ODE (after the neurons' equations are updated), then all neurons which have emitted a spike in the current step modify their synapses.\nA potential problem arises when a pre-synaptic and a post-synaptic spike are emitted at the same time. STDP-like plasticity rules are usually not defined when the spike time difference is 0, as the two spikes can not be correlated in that case (the pre-spike can not possibly be the cause of the post-spike).\nBy default, both event-driven updates (pre_spike leading to LTD, post_spike leading to LTP) will be conducted when the spikes are emitted at the same time. This can be problematic for some plastic models, for example the simple_stdp example provided in the source code.\nTo avoid this problem, the flag unless_post can be specified in pre_spike to indicate that the corresponding variable should be updated after each pre-synaptic spike, unless the post-synaptic neuron also fired at the previous time step. Without even-driven integration, the online STDP learning rule would become:\nSTDP_online = ann.Synapse(\n    parameters = \"\"\"\n        tau_pre = 10.0 : projection\n        tau_post = 10.0 : projection\n        cApre = 0.01 : projection\n        cApost = 0.0105 : projection\n        wmax = 0.01 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_pre * dApre/dt = - Apre \n        tau_post * dApost/dt = - Apost \n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre : unless_post\n        w = clip(w - Apost, 0.0 , wmax) : unless_post\n    \"\"\",                  \n    post_spike = \"\"\"\n        Apost += cApost\n        w = clip(w + Apre, 0.0 , wmax)\n    \"\"\"      \n)",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Spiking synapses"
    ]
  },
  {
    "objectID": "manual/SpikeSynapse.html#continuous-synaptic-transmission",
    "href": "manual/SpikeSynapse.html#continuous-synaptic-transmission",
    "title": "Spiking synapses",
    "section": "",
    "text": "In some cases, synaptic transmission cannot be described in an event-driven framework. Synapses using the NMDA neurotransmitter are for example often modeled as non-linear synapses. Non-linear synapses can require the post-synaptic conductance to be a sum of synapse-specific variables, as for rate-coded neurons, and not simply incremented when a pre-synaptic spike occurs. NMDA synapses can be represented by two variables x(t) and g(t) following first-order ODEs:\n\\begin{aligned}\n\\begin{aligned}\n\\tau \\cdot \\frac{dx(t)}{dt} &= - x(t) \\\\\n\\tau \\cdot \\frac{dg(t)}{dt} &= - g(t) +  x(t) \\cdot (1 - g(t))\n\\end{aligned}\n\\end{aligned}\nWhen a pre-synaptic spike occurs, x(t) is incremented by the weight w(t). However, it does not influence directly the post-synaptic neuron, as the output of a synapse is the signal g(t). The post-synaptic conductance is defined at each time t as the sum over all synapses of the same type of their variable g(t):\ng_\\text{exc}(t) = \\sum_{i=1}^{N_\\text{exc}} g_i (t)\nSuch a synapse could be implemented the following way:\nNMDA = ann.Synapse(\n    parameters = \"\"\"\n    tau = 10.0 : projection\n    \"\"\",\n    equations = \"\"\"\n    tau * dx/dt = -x\n    tau * dg/dt = -g +  x * (1 -g)\n    \"\"\", \n    pre_spike = \"x += w\",\n    psp = \"g\"\n)\nThe synapse defines a psp argument which means that the output of this synapse is non-linear and the post-synaptic conductance should be summed over this value (g in this case). It is not possible to use the event-driven integration scheme for such non-linear synapses.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Spiking synapses"
    ]
  },
  {
    "objectID": "manual/Structure.html",
    "href": "manual/Structure.html",
    "title": "Structure of an ANNarchy model",
    "section": "",
    "text": "A neural network in ANNarchy is a collection of interconnected Populations. Each population comprises a set of similar artificial Neurons, whose mean-firing rate or spiking behavior is governed by one or many ordinary differential equations (ODE). These ODEs are dependent on the activity of other neurons through Synapses. The connection pattern between two populations is called a Projection.\nThe efficiency of the connections received by a neuron is stored in a connectivity matrix, depending on the type that was assigned to them: excitatory, inhibitory, modulatory... This typed organization of afferent connections also allows to easily apply them different learning rules (Hebbian, three-factor, Oja, BCM, STDP…).\n\n\n\nStructure of a neural network\n\n\nTo define a neural network and simulate its behavior, you need to define the following information:\n\nThe number of populations, their geometry (number of neurons, optionally the spatial structure - 1D/2D/3D).\nFor each population, the type of neuron composing it, with all the necessary ODEs.\nFor each projection between two populations, the connection pattern (all-to-all, one-to-one, distance-dependent…), the initial synaptic weights, and optionally the delays in synaptic transmission.\nFor plastic synapses, the ODEs describing the evolution of synaptic weights during the simulation (learning).\nThe interaction of the network with its environment (I/O relationships, rewarded tasks, fitting procedure…)\n\nANNarchy provides a convenient way to define this information in a single Python script. In this manual, we will focus on simple networks composed of a few interconnected populations, but more complex architectures are of course possible (see the examples in the provided Notebooks).\n\n\n\nIn a script file (e.g. MyNetwork.py) or a Jupyter notebook, you first need to import the ANNarchy package:\nimport ANNarchy as ann\nWe recommend using the ann alias to avoid polluting the global namespace.\nAll the necessary objects and class definitions are then imported under the alias. The next step is to define the neurons and synapses needed by your network. To keep things simple, we will define a simple neuron model, whose firing rate is determined by the leaky-integration of excitatory inputs:\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = \"\"\"\n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt  + v = baseline + sum(exc)\n        r = pos(v)\n    \"\"\"\n)\nv is an internal variable integrating with the time constant tau the weighted sum of excitatory inputs sum(exc) to this neuron plus its baseline activity. r is the instantaneous firing rate of the neuron, defined as the positive part of mp. More details on the difference between parameters and variables, as well as details on the mathematical parser are to be found in the sections Parser and Rate-coded neurons.\nThe synapse type between the two populations will implement a simple Oja learning rule, which is a Hebbian learning rule with an additional regularization term:\nOja = ann.Synapse(\n    parameters=\"\"\"\n        tau = 5000.0\n        alpha = 8.0\n    \"\"\",\n    equations = \"\"\"\n        tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w\n    \"\"\"\n)\nw represents the synaptic efficiency (or weight value). Its evolution over time depends on a time constant tau, the regularization parameter alpha, the pre-synaptic firing rate pre.r and the post-synaptic firing rate post.r. See Rate-coded synapses for more details.\nOnce these objects are defined, the populations can be created (section Populations). We create here two populations pop1 and pop2 containing 100 neurons each and using the LeakyIntegratorNeuron neural model:\npop1 = ann.Population(name='pop1', geometry=100, neuron=LeakyIntegratorNeuron)\npop2 = ann.Population(name='pop2', geometry=100, neuron=LeakyIntegratorNeuron)\nWe additionally define an excitatory projection between the neurons of pop1 and pop2, with a target exc and a all_to_all connection pattern (section Projections). The synaptic weights are initialized randomly between 0.0 and 1.0:\nproj = ann.Projection(pre=pop1, post=pop2, target='exc', synapse=Oja)\nproj.connect_all_to_all(weights = Uniform(0.0, 1.0))\nNow that the structure of the network is defined, it can be analyzed to generate optimized C++ code in the annarchy/ subfolder and create the objects:\ncompile()\nThe network is now ready to be simulated for the desired duration:\nsimulate(1000.0) # simulate for 1 second\nIt remains to set inputs, record variables and analyze the results, but the structure of the network is already there.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Structure of an ANNarchy model"
    ]
  },
  {
    "objectID": "manual/Structure.html#definition-of-a-neural-network",
    "href": "manual/Structure.html#definition-of-a-neural-network",
    "title": "Structure of an ANNarchy model",
    "section": "",
    "text": "A neural network in ANNarchy is a collection of interconnected Populations. Each population comprises a set of similar artificial Neurons, whose mean-firing rate or spiking behavior is governed by one or many ordinary differential equations (ODE). These ODEs are dependent on the activity of other neurons through Synapses. The connection pattern between two populations is called a Projection.\nThe efficiency of the connections received by a neuron is stored in a connectivity matrix, depending on the type that was assigned to them: excitatory, inhibitory, modulatory... This typed organization of afferent connections also allows to easily apply them different learning rules (Hebbian, three-factor, Oja, BCM, STDP…).\n\n\n\nStructure of a neural network\n\n\nTo define a neural network and simulate its behavior, you need to define the following information:\n\nThe number of populations, their geometry (number of neurons, optionally the spatial structure - 1D/2D/3D).\nFor each population, the type of neuron composing it, with all the necessary ODEs.\nFor each projection between two populations, the connection pattern (all-to-all, one-to-one, distance-dependent…), the initial synaptic weights, and optionally the delays in synaptic transmission.\nFor plastic synapses, the ODEs describing the evolution of synaptic weights during the simulation (learning).\nThe interaction of the network with its environment (I/O relationships, rewarded tasks, fitting procedure…)\n\nANNarchy provides a convenient way to define this information in a single Python script. In this manual, we will focus on simple networks composed of a few interconnected populations, but more complex architectures are of course possible (see the examples in the provided Notebooks).",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Structure of an ANNarchy model"
    ]
  },
  {
    "objectID": "manual/Structure.html#basic-structure-of-a-script",
    "href": "manual/Structure.html#basic-structure-of-a-script",
    "title": "Structure of an ANNarchy model",
    "section": "",
    "text": "In a script file (e.g. MyNetwork.py) or a Jupyter notebook, you first need to import the ANNarchy package:\nimport ANNarchy as ann\nWe recommend using the ann alias to avoid polluting the global namespace.\nAll the necessary objects and class definitions are then imported under the alias. The next step is to define the neurons and synapses needed by your network. To keep things simple, we will define a simple neuron model, whose firing rate is determined by the leaky-integration of excitatory inputs:\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = \"\"\"\n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt  + v = baseline + sum(exc)\n        r = pos(v)\n    \"\"\"\n)\nv is an internal variable integrating with the time constant tau the weighted sum of excitatory inputs sum(exc) to this neuron plus its baseline activity. r is the instantaneous firing rate of the neuron, defined as the positive part of mp. More details on the difference between parameters and variables, as well as details on the mathematical parser are to be found in the sections Parser and Rate-coded neurons.\nThe synapse type between the two populations will implement a simple Oja learning rule, which is a Hebbian learning rule with an additional regularization term:\nOja = ann.Synapse(\n    parameters=\"\"\"\n        tau = 5000.0\n        alpha = 8.0\n    \"\"\",\n    equations = \"\"\"\n        tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w\n    \"\"\"\n)\nw represents the synaptic efficiency (or weight value). Its evolution over time depends on a time constant tau, the regularization parameter alpha, the pre-synaptic firing rate pre.r and the post-synaptic firing rate post.r. See Rate-coded synapses for more details.\nOnce these objects are defined, the populations can be created (section Populations). We create here two populations pop1 and pop2 containing 100 neurons each and using the LeakyIntegratorNeuron neural model:\npop1 = ann.Population(name='pop1', geometry=100, neuron=LeakyIntegratorNeuron)\npop2 = ann.Population(name='pop2', geometry=100, neuron=LeakyIntegratorNeuron)\nWe additionally define an excitatory projection between the neurons of pop1 and pop2, with a target exc and a all_to_all connection pattern (section Projections). The synaptic weights are initialized randomly between 0.0 and 1.0:\nproj = ann.Projection(pre=pop1, post=pop2, target='exc', synapse=Oja)\nproj.connect_all_to_all(weights = Uniform(0.0, 1.0))\nNow that the structure of the network is defined, it can be analyzed to generate optimized C++ code in the annarchy/ subfolder and create the objects:\ncompile()\nThe network is now ready to be simulated for the desired duration:\nsimulate(1000.0) # simulate for 1 second\nIt remains to set inputs, record variables and analyze the results, but the structure of the network is already there.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Structure of an ANNarchy model"
    ]
  },
  {
    "objectID": "manual/RateNeuron.html",
    "href": "manual/RateNeuron.html",
    "title": "Rate-coded neurons",
    "section": "",
    "text": "Let’s consider first a simple rate-coded neuron of the leaky-integrator type, which simply integrates the weighted sum of its excitatory inputs:\n\n\\tau \\frac{d v(t)}{dt} = ( B - v(t) ) + \\sum_{i}^{\\text{exc}} w_i \\, r_{i}(t)\n\nr(t) = ( v(t) )^+\nwhere v(t) represents the membrane potential of the neuron, \\tau the time constant of the neuron, B its baseline firing rate, r(t) its instantaneous firing rate, i an index over all excitatory synapses of this neuron, w_i the efficiency of the synapse with the pre-synaptic neuron of firing rate r_{i}.\nIt can be implemented in the ANNarchy framework with:\nimport ANNarchy as ann\n\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt + v = baseline + sum(exc)\n        r = pos(v)\n    \"\"\"\n)\nThe only required variable is r, which represents the instantaneous firing rate and will be used to propagate activity in the network. All other parameters and variables are freely decided by the user.\n\n\n\nCustom functions can also be defined when creating the Neuron type and used inside the equations field:\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt + v = baseline + sum(exc)\n        r = sigmoid(v)\n    \"\"\",\n    functions = \"\"\"\n        sigmoid(x) = 1.0 / (1.0 + exp(-x))\n    \"\"\"\n)\nMake sure that the name of the function does not overlap with existing mathematical functions (cos, exp), existing variables (tau, r) or built-in functions (pos, t, dt).\n\n\n\nThe ODE can depend on other parameters of the neuron (e.g. r depends on v), but not on unknown names. ANNarchy already defines the following variables and parameters for a neuron:\n\nvariable t: time in milliseconds elapsed since the creation of the network.\nparameter dt: the discretization step, the default value is 1 ms.\n\n\n\n\nThe sum(target) term gives a direct access to the weighted sum of all inputs to the neuron having the corresponding target (see Projections to see how the target is defined). These inputs are organized in a data structure called Dendrite.\nIt is possible to modify how weighted sums are computed when creating a rate-coded synapse.\n\n\n\n\n\n\nNote\n\n\n\nThe connection type, e.g. exc or inh, needs to match with the names used as a target parameter when creating a Projection. If such a projection does not exist when the network is compiled, the weighted sum will be set to 0.0 for all neurons.\n\n\nUsing only sum() in the equations sums over all defined targets. For example, if two projections with targets \"exc\" and \"inh\" reach a neuron, sum() is equivalent to sum(exc) + sum(inh). Inhibitory weights must then be defined as negative.\n\n\n\nOne has the possibility to use global operations on the population inside the neuron definition, such as the maximal activity in the population. One only needs to use one of the following operations:\n\nmin(v) for the minimum: \\min_i v_i,\nmax(v) for the maximum: \\max_i v_i,\nmean(v) for the mean: \\frac{1}{N} \\sum_i v_i,\nnorm1(v) for the L1-norm: \\frac{1}{N} \\sum_i |v_i|,\nnorm2(v) for the L2-norm: \\frac{1}{N} \\sum_i v_i^2\n\nExample where neurons react to their inputs only where they exceed the mean over the population:\nWTANeuron = ann.Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n    \"\"\",\n    equations = \"\"\"\n        input = sum(exc)\n        tau * dr/dt + r = pos(input - mean(input))\n    \"\"\"\n)  \n\n\n\n\n\n\nNote\n\n\n\nThe global operations are computed using values at the previous time step (like weighted sums), not in the step currently evaluated. There is therefore implicitely a delay of dt, but it cannot be changed.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded neurons"
    ]
  },
  {
    "objectID": "manual/RateNeuron.html#defining-parameters-and-variables",
    "href": "manual/RateNeuron.html#defining-parameters-and-variables",
    "title": "Rate-coded neurons",
    "section": "",
    "text": "Let’s consider first a simple rate-coded neuron of the leaky-integrator type, which simply integrates the weighted sum of its excitatory inputs:\n\n\\tau \\frac{d v(t)}{dt} = ( B - v(t) ) + \\sum_{i}^{\\text{exc}} w_i \\, r_{i}(t)\n\nr(t) = ( v(t) )^+\nwhere v(t) represents the membrane potential of the neuron, \\tau the time constant of the neuron, B its baseline firing rate, r(t) its instantaneous firing rate, i an index over all excitatory synapses of this neuron, w_i the efficiency of the synapse with the pre-synaptic neuron of firing rate r_{i}.\nIt can be implemented in the ANNarchy framework with:\nimport ANNarchy as ann\n\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt + v = baseline + sum(exc)\n        r = pos(v)\n    \"\"\"\n)\nThe only required variable is r, which represents the instantaneous firing rate and will be used to propagate activity in the network. All other parameters and variables are freely decided by the user.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded neurons"
    ]
  },
  {
    "objectID": "manual/RateNeuron.html#custom-functions",
    "href": "manual/RateNeuron.html#custom-functions",
    "title": "Rate-coded neurons",
    "section": "",
    "text": "Custom functions can also be defined when creating the Neuron type and used inside the equations field:\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt + v = baseline + sum(exc)\n        r = sigmoid(v)\n    \"\"\",\n    functions = \"\"\"\n        sigmoid(x) = 1.0 / (1.0 + exp(-x))\n    \"\"\"\n)\nMake sure that the name of the function does not overlap with existing mathematical functions (cos, exp), existing variables (tau, r) or built-in functions (pos, t, dt).",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded neurons"
    ]
  },
  {
    "objectID": "manual/RateNeuron.html#predefined-attributes",
    "href": "manual/RateNeuron.html#predefined-attributes",
    "title": "Rate-coded neurons",
    "section": "",
    "text": "The ODE can depend on other parameters of the neuron (e.g. r depends on v), but not on unknown names. ANNarchy already defines the following variables and parameters for a neuron:\n\nvariable t: time in milliseconds elapsed since the creation of the network.\nparameter dt: the discretization step, the default value is 1 ms.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded neurons"
    ]
  },
  {
    "objectID": "manual/RateNeuron.html#weighted-sum-of-inputs",
    "href": "manual/RateNeuron.html#weighted-sum-of-inputs",
    "title": "Rate-coded neurons",
    "section": "",
    "text": "The sum(target) term gives a direct access to the weighted sum of all inputs to the neuron having the corresponding target (see Projections to see how the target is defined). These inputs are organized in a data structure called Dendrite.\nIt is possible to modify how weighted sums are computed when creating a rate-coded synapse.\n\n\n\n\n\n\nNote\n\n\n\nThe connection type, e.g. exc or inh, needs to match with the names used as a target parameter when creating a Projection. If such a projection does not exist when the network is compiled, the weighted sum will be set to 0.0 for all neurons.\n\n\nUsing only sum() in the equations sums over all defined targets. For example, if two projections with targets \"exc\" and \"inh\" reach a neuron, sum() is equivalent to sum(exc) + sum(inh). Inhibitory weights must then be defined as negative.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded neurons"
    ]
  },
  {
    "objectID": "manual/RateNeuron.html#global-operations",
    "href": "manual/RateNeuron.html#global-operations",
    "title": "Rate-coded neurons",
    "section": "",
    "text": "One has the possibility to use global operations on the population inside the neuron definition, such as the maximal activity in the population. One only needs to use one of the following operations:\n\nmin(v) for the minimum: \\min_i v_i,\nmax(v) for the maximum: \\max_i v_i,\nmean(v) for the mean: \\frac{1}{N} \\sum_i v_i,\nnorm1(v) for the L1-norm: \\frac{1}{N} \\sum_i |v_i|,\nnorm2(v) for the L2-norm: \\frac{1}{N} \\sum_i v_i^2\n\nExample where neurons react to their inputs only where they exceed the mean over the population:\nWTANeuron = ann.Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n    \"\"\",\n    equations = \"\"\"\n        input = sum(exc)\n        tau * dr/dt + r = pos(input - mean(input))\n    \"\"\"\n)  \n\n\n\n\n\n\nNote\n\n\n\nThe global operations are computed using values at the previous time step (like weighted sums), not in the step currently evaluated. There is therefore implicitely a delay of dt, but it cannot be changed.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded neurons"
    ]
  },
  {
    "objectID": "manual/StructuralPlasticity.html",
    "href": "manual/StructuralPlasticity.html",
    "title": "Structural plasticity",
    "section": "",
    "text": "ANNarchy supports the dynamic addition/suppression of synapses during the simulation (i.e. after compilation).\n\n\n\n\n\n\nWarning\n\n\n\nStructural plasticity is not available with the CUDA backend and will likely never be…\n\n\nBecause structural plasticity adds some complexity to the generated code, it has to be enabled before compilation by setting the structural_plasticity flag to True in the call to setup():\nann.setup(structural_plasticity=True)\nIf the flag is not set, the following methods will do nothing.\nThere are two possibilities to dynamically create or delete synapses:\n\nExternally, using methods at the dendrite level from Python.\nInternally, by defining conditions for creating/pruning in the synapse description.\n\n\n\nTwo methods of the Dendrite class are available for creating/deleting synapses:\n\ncreate_synapse()\nprune_synapse()\n\n\n\nLet’s suppose that we want to add regularly new synapses between strongly active but not yet connected neurons with a low probability. One could for example define a neuron type with an additional variable averaging the firing rate over a long period of time.\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n        baseline = -0.2\n        tau_mean = 100000.0\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt + v = baseline + sum(exc)\n        r = pos(v)\n        tau_mean * dmean_r/dt =  (r - mean_r) : init = 0.0\n    \"\"\"\n)\nTwo populations are created and connected using a sparse connectivity:\npop1 = ann.Population(1000, LeakyIntegratorNeuron)\npop2 = ann.Population(1000, LeakyIntegratorNeuron)\nproj = ann.Projection(pop1, pop2, 'exc', Oja)\nproj.connect_fixed_probability(weights = 1.0, probability=0.1)\nAfter an initial period of simulation, one could add new synapses between strongly active pair of neurons:\n# For all post-synaptic neurons\nfor post in xrange(pop2.size):\n    # For all pre-synaptic neurons\n    for pre in xrange(pop1.size):\n        # If the neurons are not connected yet\n        if not pre in proj[post].ranks:\n            # If they are both sufficientely active\n            if pop1[pre].mean_r * pop2[post].mean_r &gt; 0.7:\n                # Add a synapse with weight 1.0 and the default delay\n                proj[post].create_synapse(pre, 1.0)   \ncreate_synapse only allows to specify the value of the weight and the delay. Other syanptic variables will take the value they would have had before compile(). If another value is desired, it should be explicitely set afterwards.\n\n\n\nRemoving useless synapses (pruning) is also possible. Let’s consider a synapse type whose \"age\" is incremented as long as both pre- and post-synaptic neurons are inactive at the same time:\nAgingSynapse = ann.Synapse(\n    equations=\"\"\"\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n              else :\n                age + 1 : init = 0, int\n    \"\"\"\n)\nOne could periodically track the too \"old\" synapses and remove them:\n# Threshold on the age:\nT = 100000\n# For all post-synaptic neurons receiving synapses\nfor post in proj.post_ranks:\n    # For all existing synapses\n    for pre in proj[post].ranks:\n        # If the synapse is too old\n        if proj[post][pre].age &gt; T :\n            # Remove it\n            proj[post].prune_synapse(pre)\nThis form of structural plasticity is rather slow because:\n\nThe for loops are in Python, not C++. Implementing this structural plasticity in Cython should already help.\nThe memory allocated for the synapses of a projection may have to be displaced at another location. This can lead to massive transfer of data, slowing the simulation down.\n\nIt is of course the user’s responsability to balance synapse creation/destruction, otherwise projections could become either empty or fully connected on the long-term.\n\n\n\n\nConditions for creating or deleting synapses can also be specified in the synapse description, through the creating or pruning arguments. These arguments accept string descriptions of the boolean conditions at which a synapse should be created/deleted, using the same notation as other arguments.\n\n\nThe creation of a synapse must be described by a boolean expression:\nCreatingSynapse = ann.Synapse(\n    parameters = \" ... \",\n    equations = \" ... \",\n    creating = \"pre.mean_r * post.mean_r &gt; 0.7 : proba = 0.5, w = 1.0\"\n)\nThe condition can make use of any pre- or post-synaptic variable, but NOT synaptic variables, as they obviously do not exist yet. Global parameters (defined with the postsynaptic or projection flags) can nevertheless be used.\nSeveral flags can be passed to the expression:\n\nproba specifies the probability according to which a synapse will be created, if the condition is met. The default is 1.0 (i.e. a synapse will be created whenever the condition is fulfilled).\nw specifies the value for the weight which will be created (default: 0.0).\nd specifies the delay (default: the same as all other synapses if the delay is constant in the projection, dt otherwise).\n\nNote that the new value for the delay can not exceed the maximal delay in the projection, nor be different from the others if they were all equal. Other synaptic variables will take the default value after creation.\nSynapse creation is not automatically enabled at the start of the simulation: the Projectiom method start_creating() must be called:\nproj.start_creating(period=100.0)\nThis method accepts a period parameter specifying how often the conditions for creating synapses will be checked (in ms). By default they would be checked at each time step (dt), what would be too costly.\nSimilarly, the stop_creating() method can be called to stop the creation conditions from being checked.\n\n\n\nSynaptic pruning also rely on a boolean expression:\nPruningSynapse = ann.Synapse(\n    parameters = \" T = 100000 : int, projection \",\n    equations = \"\"\"\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n              else :\n                age + 1 : init = 0, int\"\"\",\n    pruning = \"age &gt; T : proba = 0.5\"\n)\n\nA synapse type can combine creating and pruning arguments.\nThe pruning argument can rely on synaptic variables (here age), as the synapse already exist.\nOnly the proba flag can be passed to specify the probability at which the synapse will be deleted if the condition is met.\nPruning has to be started/stopped with the start_pruning() and stop_pruning() methods. start_pruning() accepts a period argument.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Structural plasticity"
    ]
  },
  {
    "objectID": "manual/StructuralPlasticity.html#dendrite-level",
    "href": "manual/StructuralPlasticity.html#dendrite-level",
    "title": "Structural plasticity",
    "section": "",
    "text": "Two methods of the Dendrite class are available for creating/deleting synapses:\n\ncreate_synapse()\nprune_synapse()\n\n\n\nLet’s suppose that we want to add regularly new synapses between strongly active but not yet connected neurons with a low probability. One could for example define a neuron type with an additional variable averaging the firing rate over a long period of time.\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n        baseline = -0.2\n        tau_mean = 100000.0\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt + v = baseline + sum(exc)\n        r = pos(v)\n        tau_mean * dmean_r/dt =  (r - mean_r) : init = 0.0\n    \"\"\"\n)\nTwo populations are created and connected using a sparse connectivity:\npop1 = ann.Population(1000, LeakyIntegratorNeuron)\npop2 = ann.Population(1000, LeakyIntegratorNeuron)\nproj = ann.Projection(pop1, pop2, 'exc', Oja)\nproj.connect_fixed_probability(weights = 1.0, probability=0.1)\nAfter an initial period of simulation, one could add new synapses between strongly active pair of neurons:\n# For all post-synaptic neurons\nfor post in xrange(pop2.size):\n    # For all pre-synaptic neurons\n    for pre in xrange(pop1.size):\n        # If the neurons are not connected yet\n        if not pre in proj[post].ranks:\n            # If they are both sufficientely active\n            if pop1[pre].mean_r * pop2[post].mean_r &gt; 0.7:\n                # Add a synapse with weight 1.0 and the default delay\n                proj[post].create_synapse(pre, 1.0)   \ncreate_synapse only allows to specify the value of the weight and the delay. Other syanptic variables will take the value they would have had before compile(). If another value is desired, it should be explicitely set afterwards.\n\n\n\nRemoving useless synapses (pruning) is also possible. Let’s consider a synapse type whose \"age\" is incremented as long as both pre- and post-synaptic neurons are inactive at the same time:\nAgingSynapse = ann.Synapse(\n    equations=\"\"\"\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n              else :\n                age + 1 : init = 0, int\n    \"\"\"\n)\nOne could periodically track the too \"old\" synapses and remove them:\n# Threshold on the age:\nT = 100000\n# For all post-synaptic neurons receiving synapses\nfor post in proj.post_ranks:\n    # For all existing synapses\n    for pre in proj[post].ranks:\n        # If the synapse is too old\n        if proj[post][pre].age &gt; T :\n            # Remove it\n            proj[post].prune_synapse(pre)\nThis form of structural plasticity is rather slow because:\n\nThe for loops are in Python, not C++. Implementing this structural plasticity in Cython should already help.\nThe memory allocated for the synapses of a projection may have to be displaced at another location. This can lead to massive transfer of data, slowing the simulation down.\n\nIt is of course the user’s responsability to balance synapse creation/destruction, otherwise projections could become either empty or fully connected on the long-term.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Structural plasticity"
    ]
  },
  {
    "objectID": "manual/StructuralPlasticity.html#synapse-level",
    "href": "manual/StructuralPlasticity.html#synapse-level",
    "title": "Structural plasticity",
    "section": "",
    "text": "Conditions for creating or deleting synapses can also be specified in the synapse description, through the creating or pruning arguments. These arguments accept string descriptions of the boolean conditions at which a synapse should be created/deleted, using the same notation as other arguments.\n\n\nThe creation of a synapse must be described by a boolean expression:\nCreatingSynapse = ann.Synapse(\n    parameters = \" ... \",\n    equations = \" ... \",\n    creating = \"pre.mean_r * post.mean_r &gt; 0.7 : proba = 0.5, w = 1.0\"\n)\nThe condition can make use of any pre- or post-synaptic variable, but NOT synaptic variables, as they obviously do not exist yet. Global parameters (defined with the postsynaptic or projection flags) can nevertheless be used.\nSeveral flags can be passed to the expression:\n\nproba specifies the probability according to which a synapse will be created, if the condition is met. The default is 1.0 (i.e. a synapse will be created whenever the condition is fulfilled).\nw specifies the value for the weight which will be created (default: 0.0).\nd specifies the delay (default: the same as all other synapses if the delay is constant in the projection, dt otherwise).\n\nNote that the new value for the delay can not exceed the maximal delay in the projection, nor be different from the others if they were all equal. Other synaptic variables will take the default value after creation.\nSynapse creation is not automatically enabled at the start of the simulation: the Projectiom method start_creating() must be called:\nproj.start_creating(period=100.0)\nThis method accepts a period parameter specifying how often the conditions for creating synapses will be checked (in ms). By default they would be checked at each time step (dt), what would be too costly.\nSimilarly, the stop_creating() method can be called to stop the creation conditions from being checked.\n\n\n\nSynaptic pruning also rely on a boolean expression:\nPruningSynapse = ann.Synapse(\n    parameters = \" T = 100000 : int, projection \",\n    equations = \"\"\"\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n              else :\n                age + 1 : init = 0, int\"\"\",\n    pruning = \"age &gt; T : proba = 0.5\"\n)\n\nA synapse type can combine creating and pruning arguments.\nThe pruning argument can rely on synaptic variables (here age), as the synapse already exist.\nOnly the proba flag can be passed to specify the probability at which the synapse will be deleted if the condition is met.\nPruning has to be started/stopped with the start_pruning() and stop_pruning() methods. start_pruning() accepts a period argument.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Structural plasticity"
    ]
  },
  {
    "objectID": "manual/Saving.html",
    "href": "manual/Saving.html",
    "title": "Saving and loading a network",
    "section": "",
    "text": "For the complete APIs, see IO in the library reference.\n\n\nThe global parameters of the network (flagged with population or projection in the Neuron/Synapse definitions) can be saved to and loaded from a JSON file using the functions save_parameters() and load_parameters():\nann.save_parameters('network.json')\nann.load_parameters('network.json')\nThe saved JSON file for a network of two populations of Izhikevich neurons connected with STDP will look like:\n{\n    \"populations\": {\n        \"pop0\": {\n            \"a\": 0.1,\n            \"b\": 0.2,\n            \"c\": -65.0,\n            \"d\": 8.0,\n            \"tau_ampa\": 5.0,\n            \"tau_nmda\": 150.0,\n            \"v_rev\": 0.0,\n            \"v_thresh\": 30.0\n        },\n        \"pop1\": {\n            \"a\": 0.02,\n            \"b\": 0.2,\n            \"c\": -65.0,\n            \"d\": 8.0,\n            \"tau_ampa\": 5.0,\n            \"tau_nmda\": 150.0,\n            \"v_rev\": 0.0,\n            \"v_thresh\": 30.0\n        }\n    },\n    \"projections\": {\n        \"proj0\": {\n            \"tau_plus\": 20.0,\n            \"tau_minus\": 60.0,\n            \"A_plus\": 0.0002,\n            \"A_minus\": 6.6e-05,\n            \"w_max\": 0.03\n        }\n    },\n    \"network\": {},\n}\nBy default, populations and projections have names like pop0 and proj1. For readability, we advise setting explicit (and unique) names in their constructor:\npop = ann.Population(100, Izhikevich, name=\"PFC_exc\")\nproj = ann.Projection(pop, pop2, 'exc', STDP, name=\"PFC_exc_to_inh\")\nOnly global parameters can be saved (no array is allowed in the JSON file). By default, only global parameters will be loaded, except if the global_only argument to load_parameters() is set to False. In that case, even local parameters can be set by the JSON file, but they will all use the same values.\nIf you want to initialize other things than population/projection global parameters, you can define arbitrary values in the \"network\" dictionary:\n{\n    \"network\": {\n        \"pop1_r_min\": 0.1,\n        \"pop1_r_max\": 1.3,\n    },\n}\nload_parameters() will return the corresponding dictionary:\nparams = ann.load_parameters('network.json')\nYou can then use them to initialize programmatically non-global parameters or variables:\npop1.r = ann.Uniform(params['pop1_r_min'], params['pop1_r_max'])\n\n\n\nThe state of all variables, including the synaptic weights, can be saved in a text file, compressed binary file or Matlab file using the save() method:\nann.save('data.txt')\nann.save('data.txt.gz')\nann.save('data.mat')\nFilenames ending with .mat correspond to Matlab files (it requires the installation of Scipy), filenames ending with .gz are compressed using gzip (normally standard to all Python distributions, but may require installation), other extensions are normal text files using cPickle (standard).\nsave() also accepts the populations and projections boolean flags. If True (the default), the neural resp. synaptic variables will be saved. For example, if you only care about synaptic plasticity but not the neural variables, you can set populations to False, and only synaptic variables will be saved.\nann.save('data.txt', populations=False)\nExcept for the Matlab format, you can also load the state of variables stored in these files once the network is compiled:\nann.load('data.txt')\n\n\n\n\n\n\nWarning\n\n\n\nThe structure of the network must of course be the same as when the file was saved: number of populations, neurons and projections. The neuron and synapse types must define the same variables. If a variable was saved but does not exist anymore, it will be skipped. If the variable did not exist, its current value will be kept, what can lead to crashes.\n\n\nload() also accepts the populations and projections boolean flags (for example if you want to load only the synaptic weights but not to restore the neural variables).\n\n\n\nPopulation and Projection objects also have save() and load() methods, allowing to save the corresponding information individually:\npop1.save('pop1.npz')\nproj.save('proj.npz')\n\npop1.load('pop1.npz')\nproj.load('proj.npz')\nThe allowed file formats are:\n\n.npz: compressed Numpy binary format (np.savez_compressed), preferred.\n*.gz: gunzipped binary text file.\n*.mat: Matlab 7.2.\n*: binary text file.\n\nAs before, .mat can only be used for saving, not loading.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Saving and loading a network"
    ]
  },
  {
    "objectID": "manual/Saving.html#global-parameters",
    "href": "manual/Saving.html#global-parameters",
    "title": "Saving and loading a network",
    "section": "",
    "text": "The global parameters of the network (flagged with population or projection in the Neuron/Synapse definitions) can be saved to and loaded from a JSON file using the functions save_parameters() and load_parameters():\nann.save_parameters('network.json')\nann.load_parameters('network.json')\nThe saved JSON file for a network of two populations of Izhikevich neurons connected with STDP will look like:\n{\n    \"populations\": {\n        \"pop0\": {\n            \"a\": 0.1,\n            \"b\": 0.2,\n            \"c\": -65.0,\n            \"d\": 8.0,\n            \"tau_ampa\": 5.0,\n            \"tau_nmda\": 150.0,\n            \"v_rev\": 0.0,\n            \"v_thresh\": 30.0\n        },\n        \"pop1\": {\n            \"a\": 0.02,\n            \"b\": 0.2,\n            \"c\": -65.0,\n            \"d\": 8.0,\n            \"tau_ampa\": 5.0,\n            \"tau_nmda\": 150.0,\n            \"v_rev\": 0.0,\n            \"v_thresh\": 30.0\n        }\n    },\n    \"projections\": {\n        \"proj0\": {\n            \"tau_plus\": 20.0,\n            \"tau_minus\": 60.0,\n            \"A_plus\": 0.0002,\n            \"A_minus\": 6.6e-05,\n            \"w_max\": 0.03\n        }\n    },\n    \"network\": {},\n}\nBy default, populations and projections have names like pop0 and proj1. For readability, we advise setting explicit (and unique) names in their constructor:\npop = ann.Population(100, Izhikevich, name=\"PFC_exc\")\nproj = ann.Projection(pop, pop2, 'exc', STDP, name=\"PFC_exc_to_inh\")\nOnly global parameters can be saved (no array is allowed in the JSON file). By default, only global parameters will be loaded, except if the global_only argument to load_parameters() is set to False. In that case, even local parameters can be set by the JSON file, but they will all use the same values.\nIf you want to initialize other things than population/projection global parameters, you can define arbitrary values in the \"network\" dictionary:\n{\n    \"network\": {\n        \"pop1_r_min\": 0.1,\n        \"pop1_r_max\": 1.3,\n    },\n}\nload_parameters() will return the corresponding dictionary:\nparams = ann.load_parameters('network.json')\nYou can then use them to initialize programmatically non-global parameters or variables:\npop1.r = ann.Uniform(params['pop1_r_min'], params['pop1_r_max'])",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Saving and loading a network"
    ]
  },
  {
    "objectID": "manual/Saving.html#complete-state-of-the-network",
    "href": "manual/Saving.html#complete-state-of-the-network",
    "title": "Saving and loading a network",
    "section": "",
    "text": "The state of all variables, including the synaptic weights, can be saved in a text file, compressed binary file or Matlab file using the save() method:\nann.save('data.txt')\nann.save('data.txt.gz')\nann.save('data.mat')\nFilenames ending with .mat correspond to Matlab files (it requires the installation of Scipy), filenames ending with .gz are compressed using gzip (normally standard to all Python distributions, but may require installation), other extensions are normal text files using cPickle (standard).\nsave() also accepts the populations and projections boolean flags. If True (the default), the neural resp. synaptic variables will be saved. For example, if you only care about synaptic plasticity but not the neural variables, you can set populations to False, and only synaptic variables will be saved.\nann.save('data.txt', populations=False)\nExcept for the Matlab format, you can also load the state of variables stored in these files once the network is compiled:\nann.load('data.txt')\n\n\n\n\n\n\nWarning\n\n\n\nThe structure of the network must of course be the same as when the file was saved: number of populations, neurons and projections. The neuron and synapse types must define the same variables. If a variable was saved but does not exist anymore, it will be skipped. If the variable did not exist, its current value will be kept, what can lead to crashes.\n\n\nload() also accepts the populations and projections boolean flags (for example if you want to load only the synaptic weights but not to restore the neural variables).",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Saving and loading a network"
    ]
  },
  {
    "objectID": "manual/Saving.html#populations-and-projections-individually",
    "href": "manual/Saving.html#populations-and-projections-individually",
    "title": "Saving and loading a network",
    "section": "",
    "text": "Population and Projection objects also have save() and load() methods, allowing to save the corresponding information individually:\npop1.save('pop1.npz')\nproj.save('proj.npz')\n\npop1.load('pop1.npz')\nproj.load('proj.npz')\nThe allowed file formats are:\n\n.npz: compressed Numpy binary format (np.savez_compressed), preferred.\n*.gz: gunzipped binary text file.\n*.mat: Matlab 7.2.\n*: binary text file.\n\nAs before, .mat can only be used for saving, not loading.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Saving and loading a network"
    ]
  },
  {
    "objectID": "manual/Notebooks.html",
    "href": "manual/Notebooks.html",
    "title": "Jupyter / IPython Notebooks",
    "section": "",
    "text": "It is possible to use ANNarchy in Jupyter / IPython notebooks. Several examples are provided in the Notebooks section of the documentation.\nThere are nevertheless two things to take into account when re-running cells:",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Jupyter / IPython Notebooks"
    ]
  },
  {
    "objectID": "manual/Notebooks.html#annarchy-uses-global-variables-to-store-the-populations-and-projections",
    "href": "manual/Notebooks.html#annarchy-uses-global-variables-to-store-the-populations-and-projections",
    "title": "Jupyter / IPython Notebooks",
    "section": "ANNarchy uses global variables to store the populations and projections",
    "text": "ANNarchy uses global variables to store the populations and projections\nInternally, there is a network manager that stores which population and projection has been declared. It is empty when starting a script, but it can know anything about the Jupyter cells. Here are the main consequences:\n\nIf you re-run the line import ANNarchy as ann, Python will not clear the stored populations and projections (by design, as ANNarchy is already cached)\nIf you re-run a cell creating a population or projection, it will create a new population, not replace the previous one.\nIf you create a new population / projection after a call to compile() in the current kernel, this will lead to an error, as the network is already compiled and new objects cannot (yet) be added on the fly.\n\nThe solution to these problems is to call the clear() command right after importing ANNarchy. This deletes all created objects and puts ANNarchy in a “clean” state:\n\nimport ANNarchy as ann\nann.clear()\n\nANNarchy 4.8 (4.8.0b) on darwin (posix).\n\n\nWhen you change something to the parameters of your network, you can re-run this cell, as well as all cells defining your network. You will then be able to call compile() again.\nThis is equivalent to a reset of your network. However, if the structure of your network changes (new populations, changed equations), this will have no effect because of the second issue.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Jupyter / IPython Notebooks"
    ]
  },
  {
    "objectID": "manual/Notebooks.html#python-cannot-dynamically-reload-modules",
    "href": "manual/Notebooks.html#python-cannot-dynamically-reload-modules",
    "title": "Jupyter / IPython Notebooks",
    "section": "Python cannot dynamically reload modules",
    "text": "Python cannot dynamically reload modules\nIf you change something in the definition of the neurons or synapses (anything that usually provoke a recompilation), this will have NO effect in the notebook, even if the network is successfully recompiled.\nPython is unable to reload libraries dynamically at runtime (https://www.python.org/dev/peps/pep-0489/). The C++ core actually running the simulation is imported as a dynamic library, so the result of the recompilation will not be reloaded.\nThe only solution is to restart the kernel after all structural changes to the network.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Jupyter / IPython Notebooks"
    ]
  },
  {
    "objectID": "manual/Simulation.html",
    "href": "manual/Simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Once all the relevant information has been defined, one needs to actually compile the network, by calling the ANNarchy.compile() method:\nann.compile()\nThe optimized C++ code will be generated in the annarchy/ subfolder relative to your script, compiled, the underlying objects created and made available to the Python interface.\nYou can specify the following arguments to compile():\n\ndirectory: relative path to the directory where files will be generated and compiled (default: annarchy/)\npopulations and projections: to compile only a subpart of the network, see Network.\ncompiler: to select which C++ compiler will be used. By default g++ on Linux and clang++ on OS X are used, you can change it here. Note that only these two compilers are supported for now, and that they must be in your $PATH.\ncompiler_flags: to select which flags are passed to the compiler. By default it is -march=native -O3, but you can fine-tune it here.\n\n\n\n\nAfter the network is correctly compiled, the simulation can be run for the specified duration (in milliseconds) through the ANNarchy.simulate() method:\nann.simulate(1000.0) # Simulate for 1 second\nThe provided duration should be a multiple of dt. If not, the number of simulation steps performed will be approximated.\nIn some cases, you may want to perform only one step of the simulation, instead of specifing the duration. The ANNarchy.step() can then be used.\nann.step() # Simulate for 1 step\n\n\n\nIn some cases, it is desired to stop the simulation whenever a criterion is fulfilled (for example, a neural integrator exceeds a certain threshold), not after a fixed amount of time.\nThere is the possibility to define a stop_condition at the Population level:\npop1 = ann.Population( ... , stop_condition = \"r &gt; 1.0\")\nWhen calling the simulate_until() method instead of simulate():\nt = ann.simulate_until(max_duration=1000.0, populations=pop1)\nthe simulation will be stopped whenever the stop_condition of pop1 is met, i.e. when the firing rate of any neuron of pop1 is above 1.0. If the condition is never met, the simulation will last maximally max_duration. The methods returns the effective duration of the simulation (to compute reaction times, for example).\nThe stop_condition can use any logical operation on the parameters and variables of the neuron associated to the population:\npop1 = ann.Population( ... , stop_condition = \"(r &gt; 1.0) and (mp &lt; 2.0)\")\nBy default, the simulation stops when at least one neuron in the population fulfills the criterion. If you want to stop the simulation when all neurons fulfill the condition, you can use the flag all after the condition:\npop1 = ann.Population( ... , stop_condition = \"r &gt; 1.0 : all\")\nThe flag any is the default behavior and can be omitted.\nThe stop criterion can depend on several populations, by providing a list of populations to the populations argument instead of a single population:\nt = ann.simulate_until(max_duration=1000.0, populations=[pop1, pop2])\nThe simulation will then stop when the criterion is met in both populations at the same time. If you want that the simulation stops when at least one population meets its criterion, you can specify the operator argument:\nt = simulate_until(max_duration=1000.0, populations=[pop1, pop2], operator='or')\nThe default value of operator is a 'and' function between the populations’ criteria.\n\n\n\n\n\n\nWarning\n\n\n\nGlobal operations (min, max, mean) are not possible inside the stop_condition. If you need them, store them in a variable in the equations argument of the neuron and use it as the condition:\nequations = \"\"\"\n    r = ...\n    max_r = max(r)\n\"\"\"\n\n\n\n\n\nIn most cases, your simulation will be decomposed into a series of fixed-duration trials, where you basically set inputs at the beginning of the trial, run the simulation for a fixed duration, and possibly read out results at the end:\n# Iterate over 100 trials\nresult = []\nfor trial in range(100):\n    # Set inputs to the network\n    pop.I = ann.Uniform(0.0, 1.0)\n    # Simulate for 1 second\n    ann.simulate(1000.)\n    # Save the output\n    result.append(pop.r)\nFor convenience, we provide the decorator every, which allows to register a python method and call it automatically during the simulation with a fixed period:\nresult = []\n\n@ann.every(period=1000.)\ndef set inputs(n):\n    # Set inputs to the network\n    pop.I = ann.Uniform(0.0, 1.0)\n    # Save the output of the previous step\n    if n &gt; 0:\n        result.append(pop.r)\n\nann.simulate(100 * 1000.)\nIn this example, set_inputs() will be executed just before the steps corresponding to times t = 0., 1000., 2000., and so on until t = 100000.\nThe method can have any name, but must accept only one argument, the integer n which will be incremented at each call of the method (i.e. it will take the values 0, 1, 2 until 99). This can for example be used to access data in a numpy array:\nimages = np.random.random((100, 640, 480))\n\n@ann.every(period=1000.)\ndef set inputs(n):\n    # Set inputs to the network\n    pop.I = images[n, :, :]\n\nann.simulate(100 * 1000.)\nOne can define several methods that will be called in the order of their definition:\n@ann.every(period=1000.)\ndef set inputs(n):\n    pop.I = 1.0\n\n@ann.every(period=1000.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this example, set_inputs() will be called first, followed by reset_inputs, so pop.I will finally be 0.0. The decorator every accepts an argument offset defining a delay within the period to call the method:\n@ann.every(period=1000.)\ndef set inputs(n):\n    pop.I = 1.0\n\n@ann.every(period=1000., offset=500.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this case, set_inputs() will be called at times 0, 1000, 2000... while reset_inputs() will be called at times 500, 1500, 2500..., allowing to structure a trial more effectively. The offset can be set negative, in which case it will be relative to the end of the trial:\n@every(period=1000., offset=-100.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this example, the method will be called at times 900, 1900, 2900 and so on. The offset value can not be longer than the period, by definition. If you try to do so, a modulo operation will anyway be applied (i.e. an offset of 1500 with a period of 1000 becomes 500).\nFinally, the wait argument allows to delay the first call to the method from a fixed interval:\n@every(period=1000., wait=5000.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this case, the method will be called at times 5000, 6000 and so on.\nBetween two calls to simulate(), the callbacks can be disabled or re-enabled using the following methods:\n@every(period=1000.)\ndef reset inputs(n):\n    pop.I = 0.0\n\n# Simulate with callbacks\nann.simulate(10000.)\n\n# Disable callbacks\nann.disable_callbacks()\n\n# Simulate without callbacks\nann.simulate(10000.)\n\n# Re-enable callbacks\nann.enable_callbacks()\n\n# Simulate with callbacks\nann.simulate(10000.)\nNote that the period is always relative to the time when simulate() is called, so if no offset is defined, the callbacks will be called before the first step of a simulation, no matter how long the previous simulation lasted. In the current state, it is not possible yet to enable/disable callbacks selectively, it is all or none.\nCallbacks can only be used with simulate(), not with step() or simulate_until().",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Simulation"
    ]
  },
  {
    "objectID": "manual/Simulation.html#compiling-the-network",
    "href": "manual/Simulation.html#compiling-the-network",
    "title": "Simulation",
    "section": "",
    "text": "Once all the relevant information has been defined, one needs to actually compile the network, by calling the ANNarchy.compile() method:\nann.compile()\nThe optimized C++ code will be generated in the annarchy/ subfolder relative to your script, compiled, the underlying objects created and made available to the Python interface.\nYou can specify the following arguments to compile():\n\ndirectory: relative path to the directory where files will be generated and compiled (default: annarchy/)\npopulations and projections: to compile only a subpart of the network, see Network.\ncompiler: to select which C++ compiler will be used. By default g++ on Linux and clang++ on OS X are used, you can change it here. Note that only these two compilers are supported for now, and that they must be in your $PATH.\ncompiler_flags: to select which flags are passed to the compiler. By default it is -march=native -O3, but you can fine-tune it here.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Simulation"
    ]
  },
  {
    "objectID": "manual/Simulation.html#simulating-the-network",
    "href": "manual/Simulation.html#simulating-the-network",
    "title": "Simulation",
    "section": "",
    "text": "After the network is correctly compiled, the simulation can be run for the specified duration (in milliseconds) through the ANNarchy.simulate() method:\nann.simulate(1000.0) # Simulate for 1 second\nThe provided duration should be a multiple of dt. If not, the number of simulation steps performed will be approximated.\nIn some cases, you may want to perform only one step of the simulation, instead of specifing the duration. The ANNarchy.step() can then be used.\nann.step() # Simulate for 1 step",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Simulation"
    ]
  },
  {
    "objectID": "manual/Simulation.html#early-stopping",
    "href": "manual/Simulation.html#early-stopping",
    "title": "Simulation",
    "section": "",
    "text": "In some cases, it is desired to stop the simulation whenever a criterion is fulfilled (for example, a neural integrator exceeds a certain threshold), not after a fixed amount of time.\nThere is the possibility to define a stop_condition at the Population level:\npop1 = ann.Population( ... , stop_condition = \"r &gt; 1.0\")\nWhen calling the simulate_until() method instead of simulate():\nt = ann.simulate_until(max_duration=1000.0, populations=pop1)\nthe simulation will be stopped whenever the stop_condition of pop1 is met, i.e. when the firing rate of any neuron of pop1 is above 1.0. If the condition is never met, the simulation will last maximally max_duration. The methods returns the effective duration of the simulation (to compute reaction times, for example).\nThe stop_condition can use any logical operation on the parameters and variables of the neuron associated to the population:\npop1 = ann.Population( ... , stop_condition = \"(r &gt; 1.0) and (mp &lt; 2.0)\")\nBy default, the simulation stops when at least one neuron in the population fulfills the criterion. If you want to stop the simulation when all neurons fulfill the condition, you can use the flag all after the condition:\npop1 = ann.Population( ... , stop_condition = \"r &gt; 1.0 : all\")\nThe flag any is the default behavior and can be omitted.\nThe stop criterion can depend on several populations, by providing a list of populations to the populations argument instead of a single population:\nt = ann.simulate_until(max_duration=1000.0, populations=[pop1, pop2])\nThe simulation will then stop when the criterion is met in both populations at the same time. If you want that the simulation stops when at least one population meets its criterion, you can specify the operator argument:\nt = simulate_until(max_duration=1000.0, populations=[pop1, pop2], operator='or')\nThe default value of operator is a 'and' function between the populations’ criteria.\n\n\n\n\n\n\nWarning\n\n\n\nGlobal operations (min, max, mean) are not possible inside the stop_condition. If you need them, store them in a variable in the equations argument of the neuron and use it as the condition:\nequations = \"\"\"\n    r = ...\n    max_r = max(r)\n\"\"\"",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Simulation"
    ]
  },
  {
    "objectID": "manual/Simulation.html#setting-inputs-periodically",
    "href": "manual/Simulation.html#setting-inputs-periodically",
    "title": "Simulation",
    "section": "",
    "text": "In most cases, your simulation will be decomposed into a series of fixed-duration trials, where you basically set inputs at the beginning of the trial, run the simulation for a fixed duration, and possibly read out results at the end:\n# Iterate over 100 trials\nresult = []\nfor trial in range(100):\n    # Set inputs to the network\n    pop.I = ann.Uniform(0.0, 1.0)\n    # Simulate for 1 second\n    ann.simulate(1000.)\n    # Save the output\n    result.append(pop.r)\nFor convenience, we provide the decorator every, which allows to register a python method and call it automatically during the simulation with a fixed period:\nresult = []\n\n@ann.every(period=1000.)\ndef set inputs(n):\n    # Set inputs to the network\n    pop.I = ann.Uniform(0.0, 1.0)\n    # Save the output of the previous step\n    if n &gt; 0:\n        result.append(pop.r)\n\nann.simulate(100 * 1000.)\nIn this example, set_inputs() will be executed just before the steps corresponding to times t = 0., 1000., 2000., and so on until t = 100000.\nThe method can have any name, but must accept only one argument, the integer n which will be incremented at each call of the method (i.e. it will take the values 0, 1, 2 until 99). This can for example be used to access data in a numpy array:\nimages = np.random.random((100, 640, 480))\n\n@ann.every(period=1000.)\ndef set inputs(n):\n    # Set inputs to the network\n    pop.I = images[n, :, :]\n\nann.simulate(100 * 1000.)\nOne can define several methods that will be called in the order of their definition:\n@ann.every(period=1000.)\ndef set inputs(n):\n    pop.I = 1.0\n\n@ann.every(period=1000.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this example, set_inputs() will be called first, followed by reset_inputs, so pop.I will finally be 0.0. The decorator every accepts an argument offset defining a delay within the period to call the method:\n@ann.every(period=1000.)\ndef set inputs(n):\n    pop.I = 1.0\n\n@ann.every(period=1000., offset=500.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this case, set_inputs() will be called at times 0, 1000, 2000... while reset_inputs() will be called at times 500, 1500, 2500..., allowing to structure a trial more effectively. The offset can be set negative, in which case it will be relative to the end of the trial:\n@every(period=1000., offset=-100.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this example, the method will be called at times 900, 1900, 2900 and so on. The offset value can not be longer than the period, by definition. If you try to do so, a modulo operation will anyway be applied (i.e. an offset of 1500 with a period of 1000 becomes 500).\nFinally, the wait argument allows to delay the first call to the method from a fixed interval:\n@every(period=1000., wait=5000.)\ndef reset inputs(n):\n    pop.I = 0.0\nIn this case, the method will be called at times 5000, 6000 and so on.\nBetween two calls to simulate(), the callbacks can be disabled or re-enabled using the following methods:\n@every(period=1000.)\ndef reset inputs(n):\n    pop.I = 0.0\n\n# Simulate with callbacks\nann.simulate(10000.)\n\n# Disable callbacks\nann.disable_callbacks()\n\n# Simulate without callbacks\nann.simulate(10000.)\n\n# Re-enable callbacks\nann.enable_callbacks()\n\n# Simulate with callbacks\nann.simulate(10000.)\nNote that the period is always relative to the time when simulate() is called, so if no offset is defined, the callbacks will be called before the first step of a simulation, no matter how long the previous simulation lasted. In the current state, it is not possible yet to enable/disable callbacks selectively, it is all or none.\nCallbacks can only be used with simulate(), not with step() or simulate_until().",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Simulation"
    ]
  },
  {
    "objectID": "manual/Populations.html",
    "href": "manual/Populations.html",
    "title": "Populations",
    "section": "",
    "text": "Once the Neuron objects have been defined, the populations can be created. Let’s suppose we have defined the following rate-coded neuron:\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters = \"\"\"\n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt  + v = baseline + sum(exc)\n        r = pos(v)\n    \"\"\"\n)\n\n\nPopulations of neurons are created using the Population class:\npop1 = ann.Population(geometry=100, neuron=LeakyIntegratorNeuron)\npop2 = ann.Population(geometry=(8, 8), neuron=LeakyIntegratorNeuron, name=\"pop2\")\nThe rate-coded or spiking nature of the Neuron instance is irrelevant when creating the Population object.\nIt takes different parameters:\n\ngeometry defines the number of neurons in the population, as well as its spatial structure (1D/2D/3D or more). For example, a two-dimensional population with 15*10 neurons takes the argument (15, 10), while a one-dimensional array of 100 neurons would take (100,) or simply 100.\nneuron indicates the neuron type to use for this population (which must have been defined before). It requires a Neuron instance.\nname is an unique string for each population in the network. If name is omitted, an internal name such as pop0 will be given (the number is incremented every time a new population is defined). Although this argument is optional, it is strongly recommended to give an understandable name to each population: if you somehow “lose” the reference to the Population object in some portion of your code, you can always retrieve it using the get_population(name) method.\n\nAfter creation, each population has several attributes defined (corresponding to the parameters and variables of the Neuron type) and is assigned a fixed size (pop.size corresponding to the total number of neurons, here 100 for pop1 and 64 for pop2) and geometry (pop1.geometry, here (100, ) and (8, 8)).\n\n\n\nEach neuron in the population has therefore a set of coordinates (expressed relative to pop1.geometry) and a rank (from 0 to pop1.size -1). The reason is that spatial coordinates are useful for visualization, or when defining a distance-dependent connection pattern, but that ANNarchy internally uses flat arrays for performance reasons.\nThe coordinates use the matrix notation for multi-dimensional arrays, which is also used by Numpy (for a 2D matrix, the first index represents the row, the second the column). You can therefore use safely the reshape() method of Numpy to switch between coordinates-based and rank-based representations of an array.\nTo convert the rank of a neuron to its coordinates (and vice-versa), you can use the ravel_multi_index and unravel_index methods of Numpy, but they can be quite slow. The Population class provides two more efficient methods to do this conversion:\n\ncoordinates_from_rank returns a tuple representing the coordinates of neuron based on its rank (between 0 and size -1, otherwise an error is thrown).\nrank_from_coordinates returns the rank corresponding to the coordinates.\n\nFor example, with pop2 having a geometry (8, 8):\n&gt;&gt;&gt; pop2.coordinates_from_rank(15)\n(1, 7)\n&gt;&gt;&gt; pop2.rank_from_coordinates((4, 6))\n38\n\n\n\nThe value of the parameters and variables of all neurons in a population can be accessed and modified through population attributes.\nWith the previously defined populations, you can list all their parameters and variables with:\n&gt;&gt;&gt; pop2.attributes\n['tau', 'baseline', 'v', 'r']\n&gt;&gt;&gt; pop2.parameters\n['tau', 'baseline']\n&gt;&gt;&gt; pop2.variables\n['r', 'v']\nReading their value is straightforward:\n&gt;&gt;&gt; pop2.tau\n10.0\n&gt;&gt;&gt; pop2.r\narray([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\nPopulation-wise parameters/variables have a single value for the population, while neuron-specific ones return a NumPy array with the same geometry has the population.\nSetting their value is also simple:\n&gt;&gt;&gt; pop2.tau = 20.0\n&gt;&gt;&gt; pop2.tau\n20.0\n&gt;&gt;&gt; pop2.r = 1.0\n&gt;&gt;&gt; pop2.r\narray([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])\n&gt;&gt;&gt; pop2.v = 0.5 * np.ones(pop2.geometry)\narray([[ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5]])\n&gt;&gt;&gt; pop2.r = Uniform(0.0, 1.0)\narray([[ 0.97931939,  0.64865327,  0.29740417,  0.49352664,  0.36511704,\n         0.59879869,  0.10835491,  0.38481751],\n       [ 0.07664157,  0.77532887,  0.04773084,  0.75395453,  0.56072342,\n         0.54139054,  0.28553319,  0.96159595],\n       [ 0.01811468,  0.30214921,  0.45321071,  0.56728733,  0.24577655,\n         0.32798484,  0.84929103,  0.63025331],\n       [ 0.34168482,  0.07411291,  0.6510492 ,  0.89025337,  0.31192464,\n         0.59834719,  0.77102494,  0.88537967],\n       [ 0.41813573,  0.47395247,  0.46603402,  0.45863676,  0.76628989,\n         0.42256749,  0.18527079,  0.8322103 ],\n       [ 0.70616692,  0.73210377,  0.05255718,  0.01939817,  0.24659769,\n         0.50349528,  0.79201573,  0.19159611],\n       [ 0.21246111,  0.93570727,  0.68544108,  0.61158741,  0.17954022,\n         0.90084004,  0.41286698,  0.45550662],\n       [ 0.14720568,  0.51426136,  0.36225438,  0.06096426,  0.77209455,\n         0.07348683,  0.43178591,  0.32451531]])\nFor population-wide attributes, you can only specify a single value (float, int or bool depending on the type of the parameter/variable). For neuron-specific attributes, you can provide either:\n\na single value which will be applied to all neurons of the population.\na list or a one-dimensional Numpy array of the same length as the number of neurons in the population. This information is provided by pop1.size.\na Numpy array of the same shape as the geometry of the population. This information is provided by pop1.geometry.\na random number generator object (Uniform, Normal...).\n\n\n\n\n\n\n\nNote\n\n\n\nIf you do not want to use the attributes of Python (for example when doing a loop over unknown attributes), you can also use the get(name) and set(values) methods of Population:\npop1.get('tau')\npop1.set({'v': 1.0, 'r': Uniform(0.0, 1.0)})\n\n\n\n\n\nThere exists a purely semantic access to individual neurons of a population. The IndividualNeuron class wraps population data for a specific neuron. It can be accessed through the Population.neuron() method using either the rank of the neuron (from 0 to pop1.size - 1) or its coordinates in the population’s geometry:\n&gt;&gt;&gt; print pop2.neuron(2, 2)\nNeuron of the population pop2 with rank 18 (coordinates (2, 2)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  v = 0.0\n  r = 0.0\nThe individual neurons can be manipulated individually:\n&gt;&gt;&gt; my_neuron = pop2.neuron(2, 2)\n&gt;&gt;&gt; my_neuron.r = 1.0\n&gt;&gt;&gt; print my_neuron\nNeuron of the population pop2 with rank 18 (coordinates (2, 2)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  v = 0.0\n  r = 1.0\n:::callout-warning\nIndividualNeuron is only a wrapper for ease of use, the real data is stored in arrays for the whole population, so accessing individual neurons is much slower and should be reserved to specific cases (i.e. only from time to time and for a limited set of neurons).\n\n\n\nIndividual neurons can be grouped into PopulationView objects, which hold references to different neurons of the same population. One can create population views by “adding” several neurons together:\n&gt;&gt;&gt; popview = pop2.neuron(2,2) + pop2.neuron(3,3) + pop2.neuron(4,4)\n&gt;&gt;&gt; popview\nPopulationView of pop2\n  Ranks: [18, 27, 36]\n* Neuron of the population pop2 with rank 18 (coordinates (2, 2)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  v = 0.0\n  r = 0.0\n\n* Neuron of the population pop2 with rank 27 (coordinates (3, 3)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  v = 0.0\n  r = 0.0\n\n* Neuron of the population pop2 with rank 36 (coordinates (4, 4)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  v = 0.0\n  r = 0.0\n&gt;&gt;&gt; popview.r = 1.0\n&gt;&gt;&gt; pop2.r\narray([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\nOne can also use the slice operators to create PopulationViews:\n&gt;&gt;&gt; popview = pop2[3, :]\n&gt;&gt;&gt; popview.r = 1.0\n&gt;&gt;&gt; pop2.r \narray([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\nor:\n&gt;&gt;&gt; popview = pop2[2:5, 4] \n&gt;&gt;&gt; popview.r = 1.0\n&gt;&gt;&gt; pop1.r\narray([[ 0., 0., 0., 0., 0., 0., 0., 0.], \n       [ 0., 0., 0., 0., 0., 0., 0., 0.], \n       [ 0., 0., 0., 0., 1., 0., 0., 0.], \n       [ 0., 0., 0., 0., 1., 0., 0., 0.], \n       [ 0., 0., 0., 0., 1., 0., 0., 0.], \n       [ 0., 0., 0., 0., 0., 0., 0., 0.], \n       [ 0., 0., 0., 0., 0., 0., 0., 0.], \n       [ 0., 0., 0., 0., 0., 0., 0., 0.]])\nPopulationView objects can be used to create projections.\n\n\n\n\n\n\nWarning\n\n\n\nContrary to the equivalent in PyNN, PopulationViews in ANNarchy can only group neurons from the same population.\n\n\n\n\n\nIf you have defined a function inside a Neuron definition:\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n        slope = 1.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt + v = baseline + sum(exc)\n        r = sigmoid(v, slope)\n    \"\"\",\n    functions = \"\"\"\n        sigmoid(x, k) = 1.0 / (1.0 + exp(-x*k))\n    \"\"\"\n)\nyou can use this function in Python as if it were a method of the corresponding object:\npop = ann.Population(1000, LeakyIntegratorNeuron)\n\nx = np.linspace(-1., 1., 100)\nk = np.ones(100)\nr = pop.sigmoid(x, k)\nYou can pass either a list or a 1D Numpy array to each argument (not a single value, nor a multidimensional array!).\nThe size of the arrays passed for each argument is arbitrary (it must not match the population’s size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Populations.html#creating-populations",
    "href": "manual/Populations.html#creating-populations",
    "title": "Populations",
    "section": "",
    "text": "Populations of neurons are created using the Population class:\npop1 = ann.Population(geometry=100, neuron=LeakyIntegratorNeuron)\npop2 = ann.Population(geometry=(8, 8), neuron=LeakyIntegratorNeuron, name=\"pop2\")\nThe rate-coded or spiking nature of the Neuron instance is irrelevant when creating the Population object.\nIt takes different parameters:\n\ngeometry defines the number of neurons in the population, as well as its spatial structure (1D/2D/3D or more). For example, a two-dimensional population with 15*10 neurons takes the argument (15, 10), while a one-dimensional array of 100 neurons would take (100,) or simply 100.\nneuron indicates the neuron type to use for this population (which must have been defined before). It requires a Neuron instance.\nname is an unique string for each population in the network. If name is omitted, an internal name such as pop0 will be given (the number is incremented every time a new population is defined). Although this argument is optional, it is strongly recommended to give an understandable name to each population: if you somehow “lose” the reference to the Population object in some portion of your code, you can always retrieve it using the get_population(name) method.\n\nAfter creation, each population has several attributes defined (corresponding to the parameters and variables of the Neuron type) and is assigned a fixed size (pop.size corresponding to the total number of neurons, here 100 for pop1 and 64 for pop2) and geometry (pop1.geometry, here (100, ) and (8, 8)).",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Populations.html#geometry-and-ranks",
    "href": "manual/Populations.html#geometry-and-ranks",
    "title": "Populations",
    "section": "",
    "text": "Each neuron in the population has therefore a set of coordinates (expressed relative to pop1.geometry) and a rank (from 0 to pop1.size -1). The reason is that spatial coordinates are useful for visualization, or when defining a distance-dependent connection pattern, but that ANNarchy internally uses flat arrays for performance reasons.\nThe coordinates use the matrix notation for multi-dimensional arrays, which is also used by Numpy (for a 2D matrix, the first index represents the row, the second the column). You can therefore use safely the reshape() method of Numpy to switch between coordinates-based and rank-based representations of an array.\nTo convert the rank of a neuron to its coordinates (and vice-versa), you can use the ravel_multi_index and unravel_index methods of Numpy, but they can be quite slow. The Population class provides two more efficient methods to do this conversion:\n\ncoordinates_from_rank returns a tuple representing the coordinates of neuron based on its rank (between 0 and size -1, otherwise an error is thrown).\nrank_from_coordinates returns the rank corresponding to the coordinates.\n\nFor example, with pop2 having a geometry (8, 8):\n&gt;&gt;&gt; pop2.coordinates_from_rank(15)\n(1, 7)\n&gt;&gt;&gt; pop2.rank_from_coordinates((4, 6))\n38",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Populations.html#population-attributes",
    "href": "manual/Populations.html#population-attributes",
    "title": "Populations",
    "section": "",
    "text": "The value of the parameters and variables of all neurons in a population can be accessed and modified through population attributes.\nWith the previously defined populations, you can list all their parameters and variables with:\n&gt;&gt;&gt; pop2.attributes\n['tau', 'baseline', 'v', 'r']\n&gt;&gt;&gt; pop2.parameters\n['tau', 'baseline']\n&gt;&gt;&gt; pop2.variables\n['r', 'v']\nReading their value is straightforward:\n&gt;&gt;&gt; pop2.tau\n10.0\n&gt;&gt;&gt; pop2.r\narray([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\nPopulation-wise parameters/variables have a single value for the population, while neuron-specific ones return a NumPy array with the same geometry has the population.\nSetting their value is also simple:\n&gt;&gt;&gt; pop2.tau = 20.0\n&gt;&gt;&gt; pop2.tau\n20.0\n&gt;&gt;&gt; pop2.r = 1.0\n&gt;&gt;&gt; pop2.r\narray([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])\n&gt;&gt;&gt; pop2.v = 0.5 * np.ones(pop2.geometry)\narray([[ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5]])\n&gt;&gt;&gt; pop2.r = Uniform(0.0, 1.0)\narray([[ 0.97931939,  0.64865327,  0.29740417,  0.49352664,  0.36511704,\n         0.59879869,  0.10835491,  0.38481751],\n       [ 0.07664157,  0.77532887,  0.04773084,  0.75395453,  0.56072342,\n         0.54139054,  0.28553319,  0.96159595],\n       [ 0.01811468,  0.30214921,  0.45321071,  0.56728733,  0.24577655,\n         0.32798484,  0.84929103,  0.63025331],\n       [ 0.34168482,  0.07411291,  0.6510492 ,  0.89025337,  0.31192464,\n         0.59834719,  0.77102494,  0.88537967],\n       [ 0.41813573,  0.47395247,  0.46603402,  0.45863676,  0.76628989,\n         0.42256749,  0.18527079,  0.8322103 ],\n       [ 0.70616692,  0.73210377,  0.05255718,  0.01939817,  0.24659769,\n         0.50349528,  0.79201573,  0.19159611],\n       [ 0.21246111,  0.93570727,  0.68544108,  0.61158741,  0.17954022,\n         0.90084004,  0.41286698,  0.45550662],\n       [ 0.14720568,  0.51426136,  0.36225438,  0.06096426,  0.77209455,\n         0.07348683,  0.43178591,  0.32451531]])\nFor population-wide attributes, you can only specify a single value (float, int or bool depending on the type of the parameter/variable). For neuron-specific attributes, you can provide either:\n\na single value which will be applied to all neurons of the population.\na list or a one-dimensional Numpy array of the same length as the number of neurons in the population. This information is provided by pop1.size.\na Numpy array of the same shape as the geometry of the population. This information is provided by pop1.geometry.\na random number generator object (Uniform, Normal...).\n\n\n\n\n\n\n\nNote\n\n\n\nIf you do not want to use the attributes of Python (for example when doing a loop over unknown attributes), you can also use the get(name) and set(values) methods of Population:\npop1.get('tau')\npop1.set({'v': 1.0, 'r': Uniform(0.0, 1.0)})",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Populations.html#accessing-individual-neurons",
    "href": "manual/Populations.html#accessing-individual-neurons",
    "title": "Populations",
    "section": "",
    "text": "There exists a purely semantic access to individual neurons of a population. The IndividualNeuron class wraps population data for a specific neuron. It can be accessed through the Population.neuron() method using either the rank of the neuron (from 0 to pop1.size - 1) or its coordinates in the population’s geometry:\n&gt;&gt;&gt; print pop2.neuron(2, 2)\nNeuron of the population pop2 with rank 18 (coordinates (2, 2)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  v = 0.0\n  r = 0.0\nThe individual neurons can be manipulated individually:\n&gt;&gt;&gt; my_neuron = pop2.neuron(2, 2)\n&gt;&gt;&gt; my_neuron.r = 1.0\n&gt;&gt;&gt; print my_neuron\nNeuron of the population pop2 with rank 18 (coordinates (2, 2)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  v = 0.0\n  r = 1.0\n:::callout-warning\nIndividualNeuron is only a wrapper for ease of use, the real data is stored in arrays for the whole population, so accessing individual neurons is much slower and should be reserved to specific cases (i.e. only from time to time and for a limited set of neurons).",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Populations.html#accessing-groups-of-neurons",
    "href": "manual/Populations.html#accessing-groups-of-neurons",
    "title": "Populations",
    "section": "",
    "text": "Individual neurons can be grouped into PopulationView objects, which hold references to different neurons of the same population. One can create population views by “adding” several neurons together:\n&gt;&gt;&gt; popview = pop2.neuron(2,2) + pop2.neuron(3,3) + pop2.neuron(4,4)\n&gt;&gt;&gt; popview\nPopulationView of pop2\n  Ranks: [18, 27, 36]\n* Neuron of the population pop2 with rank 18 (coordinates (2, 2)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  v = 0.0\n  r = 0.0\n\n* Neuron of the population pop2 with rank 27 (coordinates (3, 3)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  v = 0.0\n  r = 0.0\n\n* Neuron of the population pop2 with rank 36 (coordinates (4, 4)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  v = 0.0\n  r = 0.0\n&gt;&gt;&gt; popview.r = 1.0\n&gt;&gt;&gt; pop2.r\narray([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\nOne can also use the slice operators to create PopulationViews:\n&gt;&gt;&gt; popview = pop2[3, :]\n&gt;&gt;&gt; popview.r = 1.0\n&gt;&gt;&gt; pop2.r \narray([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\nor:\n&gt;&gt;&gt; popview = pop2[2:5, 4] \n&gt;&gt;&gt; popview.r = 1.0\n&gt;&gt;&gt; pop1.r\narray([[ 0., 0., 0., 0., 0., 0., 0., 0.], \n       [ 0., 0., 0., 0., 0., 0., 0., 0.], \n       [ 0., 0., 0., 0., 1., 0., 0., 0.], \n       [ 0., 0., 0., 0., 1., 0., 0., 0.], \n       [ 0., 0., 0., 0., 1., 0., 0., 0.], \n       [ 0., 0., 0., 0., 0., 0., 0., 0.], \n       [ 0., 0., 0., 0., 0., 0., 0., 0.], \n       [ 0., 0., 0., 0., 0., 0., 0., 0.]])\nPopulationView objects can be used to create projections.\n\n\n\n\n\n\nWarning\n\n\n\nContrary to the equivalent in PyNN, PopulationViews in ANNarchy can only group neurons from the same population.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Populations.html#functions",
    "href": "manual/Populations.html#functions",
    "title": "Populations",
    "section": "",
    "text": "If you have defined a function inside a Neuron definition:\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n        slope = 1.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt + v = baseline + sum(exc)\n        r = sigmoid(v, slope)\n    \"\"\",\n    functions = \"\"\"\n        sigmoid(x, k) = 1.0 / (1.0 + exp(-x*k))\n    \"\"\"\n)\nyou can use this function in Python as if it were a method of the corresponding object:\npop = ann.Population(1000, LeakyIntegratorNeuron)\n\nx = np.linspace(-1., 1., 100)\nk = np.ones(100)\nr = pop.sigmoid(x, k)\nYou can pass either a list or a 1D Numpy array to each argument (not a single value, nor a multidimensional array!).\nThe size of the arrays passed for each argument is arbitrary (it must not match the population’s size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Populations"
    ]
  },
  {
    "objectID": "manual/Configuration.html",
    "href": "manual/Configuration.html",
    "title": "Configuration",
    "section": "",
    "text": "The setup() function can be used at the beginning of a script to configure the numerical behavior of ANNarchy.\n\n\nAn important value for the simulation is the discretization step dt. Its default value is 1 ms, which is usually fine for rate-coded networks, but may be too high for spiking networks, as the equations are stiffer. Taken too high, it can lead to high numerical errors. Too low, and the simulation will take an unnecessary amount of time.\nTo set the discretization step, just pass the desired value to setup() at the beginning of the script, or at any rate before the call to compile():\nann.setup(dt=0.1)\nChanging its value after calling compile() will not have any effect.\n\n\n\nBy default, the random number generators are seeded with time(NULL), so each simulation will be different. If you want to have deterministic simulations, you simply need to provide a fixed seed to setup():\nann.setup(seed=62756)\nNote that this also sets the seed of Numpy, so you can also reproduce random initialization values produced by np.random.\n\n\n\n\n\n\nNote\n\n\n\nUsing the same seed with the OpenMP and CUDA backends will not lead to the same sequences of numbers!\n\n\n\n\n\nWhen calling compile() for the first time, a subfolder annarchy/ will be created in the current directory, where the generated code will be compiled. The first compilation may last a couple of seconds, but further modifications to the script are much faster. If no modification to the network has been made except for parameter values, it will not be recompiled, sparing us this overhead.\nANNarchy tracks the changes in the script and re-generates the corresponding code. In some cases (a new version of ANNarchy has been installed, bugs), it may be necessary to perform a fresh compilation of the network. You can either delete the annarchy/ subfolder and restart the script:\nrm -rf annarchy/\npython MyNetwork.py\nor pass the --clean flag to Python:\npython MyNetwork.py --clean \n\n\n\nANNarchy requires a C++ compiler. On GNU/Linux, the default choice is g++, while on MacOS it is clang++. You can change the compiler (and its flags) to use either during the call to compile() in your script:\ncompile(compiler=\"clang++\", compiler_flags=\"-march=native -O3\")\nor globally by modifying the configuration file located at ~/.config/ANNarchy/annarchy.json:\n{\n    \"openmp\": {\n        \"compiler\": \"clang++\",\n        \"flags\": \"-march=native -O3\"\n    }\n}\nBe careful with the flags: for example, the optimization level -O3 does not obligatorily produce faster code. But this is the case for most models, therefore it is the default in the ANNarchy 4.7.x releases.\nEven more caution is required when using the -ffast-math flag. It can increase the performance, in particular in combination with SIMD. However, the application of -ffast-math enables a set of optimizations which might violate IEEE 754 compliance (which might be okay in many cases, but it is important that the user verifies the result). For more details, see the g++ documentation: https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html\n\n\n\n\n\n\nNote\n\n\n\nIn rare cases, it may occur that the CPU architecture is not detectable for the used g++ compiler (e.g. Intel’s Tigerlake and g++ &lt;= 9.4). This will result in a compiler error which can be fixed by removing the ‘-march=native’ flag. To get access to AVX-512 SIMD instructions, you need to add -mavx512f instead, as well as -ftree-vectorize if -O3 is not already used.\n\n\n\n\n\nThe default paradigm for an ANNarchy simulation is through openMP, which distributes automatically the computations over the available CPU cores.\nBy default, OpenMP would use all the available cores for your simulation, even if it is not optimal: small networks in particular tend to run faster with a small amount of cores. For this reason, the OMP_NUM_THREADS environment variable has no effect in ANNarchy. You can control the number of cores by passing the -j flag to the Python command:\npython NeuralField.py -j2\nIt is the responsability of the user to find out which number of cores is optimal for his network, by comparing simulation times. When this optimal number is found, it can be hard-coded in the script by setting the num_threads argument to ANNarchy.setup():\nimport ANNarchy as ann\nann.setup(num_threads=2)\n\n\n\nTo run your network on GPUs, you need to declare to ANNarchy that you want to use CUDA. One way to do so is to pass the --gpu flag to the command line:\npython NeuralField.py --gpu\nYou can also set the paradigm argument of ANNarchy.setup() to make it permanent:\nimport ANNarchy as ann\nann.setup(paradigm=\"cuda\")\nIf there are multiple GPUs on your machine, you can select the ID of the device by specifying it to the --gpu flag on the command line:\npython NeuralField.py --gpu=2\nYou can also pass the cuda_config dictionary argument to compile():\nann.compile(cuda_config={'device': 2})\nThe default GPU is defined in the configuration file ~/.config/ANNarchy/annarchy.json (0 unless you modify it).\n{\n    \"cuda\": {\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\n\nAs the current implementation is a development version, some of the features provided by ANNarchy are not supported yet with CUDA:\n\nweight sharing (convolutions),\nnon-uniform synaptic delays,\nstructural plasticity,\nspiking neurons: a) with mean firing rate and b) continous integration of inputs,\nSpikeSourceArray.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Configuration"
    ]
  },
  {
    "objectID": "manual/Configuration.html#setting-the-discretization-step",
    "href": "manual/Configuration.html#setting-the-discretization-step",
    "title": "Configuration",
    "section": "",
    "text": "An important value for the simulation is the discretization step dt. Its default value is 1 ms, which is usually fine for rate-coded networks, but may be too high for spiking networks, as the equations are stiffer. Taken too high, it can lead to high numerical errors. Too low, and the simulation will take an unnecessary amount of time.\nTo set the discretization step, just pass the desired value to setup() at the beginning of the script, or at any rate before the call to compile():\nann.setup(dt=0.1)\nChanging its value after calling compile() will not have any effect.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Configuration"
    ]
  },
  {
    "objectID": "manual/Configuration.html#setting-the-seed-of-the-random-number-generators",
    "href": "manual/Configuration.html#setting-the-seed-of-the-random-number-generators",
    "title": "Configuration",
    "section": "",
    "text": "By default, the random number generators are seeded with time(NULL), so each simulation will be different. If you want to have deterministic simulations, you simply need to provide a fixed seed to setup():\nann.setup(seed=62756)\nNote that this also sets the seed of Numpy, so you can also reproduce random initialization values produced by np.random.\n\n\n\n\n\n\nNote\n\n\n\nUsing the same seed with the OpenMP and CUDA backends will not lead to the same sequences of numbers!",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Configuration"
    ]
  },
  {
    "objectID": "manual/Configuration.html#cleaning-the-compilation-directory",
    "href": "manual/Configuration.html#cleaning-the-compilation-directory",
    "title": "Configuration",
    "section": "",
    "text": "When calling compile() for the first time, a subfolder annarchy/ will be created in the current directory, where the generated code will be compiled. The first compilation may last a couple of seconds, but further modifications to the script are much faster. If no modification to the network has been made except for parameter values, it will not be recompiled, sparing us this overhead.\nANNarchy tracks the changes in the script and re-generates the corresponding code. In some cases (a new version of ANNarchy has been installed, bugs), it may be necessary to perform a fresh compilation of the network. You can either delete the annarchy/ subfolder and restart the script:\nrm -rf annarchy/\npython MyNetwork.py\nor pass the --clean flag to Python:\npython MyNetwork.py --clean",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Configuration"
    ]
  },
  {
    "objectID": "manual/Configuration.html#selecting-the-compiler",
    "href": "manual/Configuration.html#selecting-the-compiler",
    "title": "Configuration",
    "section": "",
    "text": "ANNarchy requires a C++ compiler. On GNU/Linux, the default choice is g++, while on MacOS it is clang++. You can change the compiler (and its flags) to use either during the call to compile() in your script:\ncompile(compiler=\"clang++\", compiler_flags=\"-march=native -O3\")\nor globally by modifying the configuration file located at ~/.config/ANNarchy/annarchy.json:\n{\n    \"openmp\": {\n        \"compiler\": \"clang++\",\n        \"flags\": \"-march=native -O3\"\n    }\n}\nBe careful with the flags: for example, the optimization level -O3 does not obligatorily produce faster code. But this is the case for most models, therefore it is the default in the ANNarchy 4.7.x releases.\nEven more caution is required when using the -ffast-math flag. It can increase the performance, in particular in combination with SIMD. However, the application of -ffast-math enables a set of optimizations which might violate IEEE 754 compliance (which might be okay in many cases, but it is important that the user verifies the result). For more details, see the g++ documentation: https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html\n\n\n\n\n\n\nNote\n\n\n\nIn rare cases, it may occur that the CPU architecture is not detectable for the used g++ compiler (e.g. Intel’s Tigerlake and g++ &lt;= 9.4). This will result in a compiler error which can be fixed by removing the ‘-march=native’ flag. To get access to AVX-512 SIMD instructions, you need to add -mavx512f instead, as well as -ftree-vectorize if -O3 is not already used.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Configuration"
    ]
  },
  {
    "objectID": "manual/Configuration.html#parallel-computing-with-openmp",
    "href": "manual/Configuration.html#parallel-computing-with-openmp",
    "title": "Configuration",
    "section": "",
    "text": "The default paradigm for an ANNarchy simulation is through openMP, which distributes automatically the computations over the available CPU cores.\nBy default, OpenMP would use all the available cores for your simulation, even if it is not optimal: small networks in particular tend to run faster with a small amount of cores. For this reason, the OMP_NUM_THREADS environment variable has no effect in ANNarchy. You can control the number of cores by passing the -j flag to the Python command:\npython NeuralField.py -j2\nIt is the responsability of the user to find out which number of cores is optimal for his network, by comparing simulation times. When this optimal number is found, it can be hard-coded in the script by setting the num_threads argument to ANNarchy.setup():\nimport ANNarchy as ann\nann.setup(num_threads=2)",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Configuration"
    ]
  },
  {
    "objectID": "manual/Configuration.html#parallel-computing-with-cuda",
    "href": "manual/Configuration.html#parallel-computing-with-cuda",
    "title": "Configuration",
    "section": "",
    "text": "To run your network on GPUs, you need to declare to ANNarchy that you want to use CUDA. One way to do so is to pass the --gpu flag to the command line:\npython NeuralField.py --gpu\nYou can also set the paradigm argument of ANNarchy.setup() to make it permanent:\nimport ANNarchy as ann\nann.setup(paradigm=\"cuda\")\nIf there are multiple GPUs on your machine, you can select the ID of the device by specifying it to the --gpu flag on the command line:\npython NeuralField.py --gpu=2\nYou can also pass the cuda_config dictionary argument to compile():\nann.compile(cuda_config={'device': 2})\nThe default GPU is defined in the configuration file ~/.config/ANNarchy/annarchy.json (0 unless you modify it).\n{\n    \"cuda\": {\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\n\nAs the current implementation is a development version, some of the features provided by ANNarchy are not supported yet with CUDA:\n\nweight sharing (convolutions),\nnon-uniform synaptic delays,\nstructural plasticity,\nspiking neurons: a) with mean firing rate and b) continous integration of inputs,\nSpikeSourceArray.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Configuration"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html",
    "href": "manual/RateSynapse.html",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "As for neurons, you can define the synaptic behavior using a Synapse object. Although the description is local to a synapse, the same ODE will be applied to all synapses of a given Projection from one population to another. The same vocabulary as for neurons is accessible (constants, functions, conditional statements), except that the synapse must distinguish pre-synaptic and post-synaptic parameters/variables.\nLike r for a rate-coded neuron, one variable is special for a rate-coded synapse:\n\nw represents the synaptic efficiency (or the weight of the connection). If an ODE is defined for this variable, this will implement a learning rule. If none is provided, the synapse is non-plastic.\n\nThe ODEs for synaptic variables follow the same syntax as for neurons. As for neurons, the following variables are already defined:\n\nt: time in milliseconds elapsed since the creation of the network.\ndt: the discretization step is 1.0ms by default.\n\n\n\nLearning is possible by modifying the variable w of a single synapse during the simulation.\nFor example, the Oja learning rule (see the example Bar learning):\n\\tau \\, \\frac{d w(t)}{dt} = r_\\text{pre} \\, r_\\text{post} - \\alpha \\, r_\\text{post}^2 \\, w(t)\ncould be implemented this way:\nOja = ann.Synapse(\n    parameters=\"\"\"\n        tau = 5000\n        alpha = 8.0\n    \"\"\",\n    equations=\"\"\"\n        tau * dw / dt = pre.r * post.r - alpha * post.r^2 * w\n    \"\"\"\n)\nNote that it is equivalent to define the increment directly if you want to apply the explicit Euler method:\nequations=\"\"\"\n    w += dt / tau * ( pre.r * post.r - alpha * post.r^2 * w)\n\"\"\"\nThe same vocabulary as for rate-coded neurons applies. Custom functions can also be defined:\nOja = ann.Synapse(\n    parameters=\"\"\"\n        tau = 5000\n        alpha = 8.0\n    \"\"\",\n    equations=\"\"\"\n        tau * dw / dt = product(pre.r,  post.r) - alpha * post.r^2 * w\n    \"\"\",\n    functions=\"\"\"\n        product(x,y) = x * y\n    \"\"\",\n)\n\n\n\nA synapse needs to access neural variables both at the pre- and post-synaptic levels. For the pre-synaptic neuron, biologically realistic synapses should only need its firing rate, but in some cases it may be useful to access other variables as well.\nIn order to use neural variables in a synaptic variable, you have to prefix them with pre. or post.. For example:\npre.r, post.baseline, post.mp...\nANNarchy will check before the compilation that the pre- or post-synaptic neuron types indeed define such variables.\n\n\n\n\n\n\nNote\n\n\n\nIf the projection uses delays, all pre-synaptic variables used in the synapse model will be delayed.\n\n\n\n\n\nThere are 3 levels of locality for a synaptic parameter or variable:\n\nsynaptic: there is one value per synapse in the projection (default).\npostsynaptic: there is one value per post-synaptic neuron in the projection.\nprojection: there is only one value for the whole projection.\n\nThe following BCM learning rule makes use of the three levels of locality:\nBCM = ann.Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100. : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0\n    \"\"\"\n)\neta and tau are global parameters to the projection: all synapses will use the same value. theta defines one value per post-synaptic neuron: it tracks the average of the post-synaptic firing rate. There is therefore no need to update one value per synapse, so we can use the flag \"postsynaptic\". Naturally, w is local to each synapse, so no locality flag should be passed.\n\n\n\nSome learning rules require global information about the pre- or post-synaptic population, which is not local to the synapse, such as the mean or maximal activity in the pre-synaptic population. This information can be accessed at the synapse-level. The special functions:\n\nmin(v) for the minimum: \\min_i v_i,\nmax(v) for the maximum: \\max_i v_i,\nmean(v) for the mean: \\frac{1}{N} \\sum_i v_i,\nnorm1(v) for the L1-norm: \\frac{1}{N} \\sum_i |v_i|,\nnorm2(v) for the L2-norm: \\frac{1}{N} \\sum_i v_i^2\n\nare available for any pre- or post-synaptic variable.\nFor example, some covariance-based learning rules depend on the mean firing in the pre- and post-synaptic populations:\n\\tau \\frac{d w(t)}{dt} = (r_\\text{pre} - \\hat{r}_\\text{pre} )  * (r_\\text{post} - \\hat{r}_\\text{post} )\nUsing the global operations, such a learning rule is trivial to implement:\nCovariance = ann.Synapse(\n    parameters=\"\"\"\n        tau = 5000.0\n    \"\"\",\n    equations=\"\"\"\n        tau * dw/dt = (pre.r - mean(pre.r) ) * (post.r - mean(post.r) )\n    \"\"\"\n)\n\n\n\n\n\n\nWarning\n\n\n\n\nSuch global operations can become expensive to compute if the populations are too big.\nThe global operations are performed over the whole population, not only the synapses which actually reach the post-synaptic neuron.\nThey can only be applied to a single variable, not a combination or function of them.\n\n\n\n\n\n\nThe argument psp of a Synapse object represents the post-synaptic potential evoked by the pre-synaptic neuron. This value is actually summed by the post-synaptic neuron over all other synapses of the same projection in sum(target). If not defined, it will simply represent the product between the pre-synaptic firing rate (pre.r) and the weight value (w).\nThe post-synaptic potential of a single synapse is by default:\npsp = w * pre.r\nwhere pre.r is the pre-synaptic firing rate, but you may want to override this behaviour in certain cases.\nFor example, you may want to model a non-linear synapse with a logarithmic term:\n\nr_{i} = \\sum_j \\log \\left( \\frac {( r_{j} * w_{ij} ) + 1 } { ( r_{j} * w_{ij} ) - 1 } \\right)\n\nIn this case, you can just modify the psp argument of the synapse:\nNonLinearSynapse = ann.Synapse( \n    psp = \"\"\"\n        log( (pre.r * w + 1 ) / (pre.r * w - 1) )\n    \"\"\"\n)\nNo further modification has to be done in the post-synaptic neuron, this value will be summed over all pre-synaptic neurons automatically when using sum(target).\n\n\n\nBy default, a post-synaptic neuron calling sum(target) will compute the sum over all incoming synapses of their defined psp:\n\\text{sum(exc)} = \\sum_{i \\in \\text{exc}} \\text{psp}(i) = \\sum_{i \\in \\text{exc}} w_i * \\text{pre}.r_i\nIt is possible to define a different operation performed on the connected synapses, using the operation argument of the synapse:\nMaxPooling = ann.Synapse(\n    psp = \"w * pre.r\",\n    operation = \"max\"\n)\nIn this case, sum(target) will represent the maximum value of w * pre.r over all incoming synapses, not their sum. It can be useful when defining pooling operations in a convolutional network, for example.\nThe available operations are:\n\n\"sum\": (default): sum of all incoming psps.\n\"max\": maximum of all incoming psps.\n\"min\": minimum of all incoming psps.\n\"mean\": mean of all incoming psps.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese operations are only possible for rate-coded synapses.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html#synaptic-plasticity",
    "href": "manual/RateSynapse.html#synaptic-plasticity",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "Learning is possible by modifying the variable w of a single synapse during the simulation.\nFor example, the Oja learning rule (see the example Bar learning):\n\\tau \\, \\frac{d w(t)}{dt} = r_\\text{pre} \\, r_\\text{post} - \\alpha \\, r_\\text{post}^2 \\, w(t)\ncould be implemented this way:\nOja = ann.Synapse(\n    parameters=\"\"\"\n        tau = 5000\n        alpha = 8.0\n    \"\"\",\n    equations=\"\"\"\n        tau * dw / dt = pre.r * post.r - alpha * post.r^2 * w\n    \"\"\"\n)\nNote that it is equivalent to define the increment directly if you want to apply the explicit Euler method:\nequations=\"\"\"\n    w += dt / tau * ( pre.r * post.r - alpha * post.r^2 * w)\n\"\"\"\nThe same vocabulary as for rate-coded neurons applies. Custom functions can also be defined:\nOja = ann.Synapse(\n    parameters=\"\"\"\n        tau = 5000\n        alpha = 8.0\n    \"\"\",\n    equations=\"\"\"\n        tau * dw / dt = product(pre.r,  post.r) - alpha * post.r^2 * w\n    \"\"\",\n    functions=\"\"\"\n        product(x,y) = x * y\n    \"\"\",\n)",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html#neuron-specific-variables",
    "href": "manual/RateSynapse.html#neuron-specific-variables",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "A synapse needs to access neural variables both at the pre- and post-synaptic levels. For the pre-synaptic neuron, biologically realistic synapses should only need its firing rate, but in some cases it may be useful to access other variables as well.\nIn order to use neural variables in a synaptic variable, you have to prefix them with pre. or post.. For example:\npre.r, post.baseline, post.mp...\nANNarchy will check before the compilation that the pre- or post-synaptic neuron types indeed define such variables.\n\n\n\n\n\n\nNote\n\n\n\nIf the projection uses delays, all pre-synaptic variables used in the synapse model will be delayed.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html#locality",
    "href": "manual/RateSynapse.html#locality",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "There are 3 levels of locality for a synaptic parameter or variable:\n\nsynaptic: there is one value per synapse in the projection (default).\npostsynaptic: there is one value per post-synaptic neuron in the projection.\nprojection: there is only one value for the whole projection.\n\nThe following BCM learning rule makes use of the three levels of locality:\nBCM = ann.Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100. : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0\n    \"\"\"\n)\neta and tau are global parameters to the projection: all synapses will use the same value. theta defines one value per post-synaptic neuron: it tracks the average of the post-synaptic firing rate. There is therefore no need to update one value per synapse, so we can use the flag \"postsynaptic\". Naturally, w is local to each synapse, so no locality flag should be passed.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html#global-operations",
    "href": "manual/RateSynapse.html#global-operations",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "Some learning rules require global information about the pre- or post-synaptic population, which is not local to the synapse, such as the mean or maximal activity in the pre-synaptic population. This information can be accessed at the synapse-level. The special functions:\n\nmin(v) for the minimum: \\min_i v_i,\nmax(v) for the maximum: \\max_i v_i,\nmean(v) for the mean: \\frac{1}{N} \\sum_i v_i,\nnorm1(v) for the L1-norm: \\frac{1}{N} \\sum_i |v_i|,\nnorm2(v) for the L2-norm: \\frac{1}{N} \\sum_i v_i^2\n\nare available for any pre- or post-synaptic variable.\nFor example, some covariance-based learning rules depend on the mean firing in the pre- and post-synaptic populations:\n\\tau \\frac{d w(t)}{dt} = (r_\\text{pre} - \\hat{r}_\\text{pre} )  * (r_\\text{post} - \\hat{r}_\\text{post} )\nUsing the global operations, such a learning rule is trivial to implement:\nCovariance = ann.Synapse(\n    parameters=\"\"\"\n        tau = 5000.0\n    \"\"\",\n    equations=\"\"\"\n        tau * dw/dt = (pre.r - mean(pre.r) ) * (post.r - mean(post.r) )\n    \"\"\"\n)\n\n\n\n\n\n\nWarning\n\n\n\n\nSuch global operations can become expensive to compute if the populations are too big.\nThe global operations are performed over the whole population, not only the synapses which actually reach the post-synaptic neuron.\nThey can only be applied to a single variable, not a combination or function of them.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html#defining-the-post-synaptic-potential-psp",
    "href": "manual/RateSynapse.html#defining-the-post-synaptic-potential-psp",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "The argument psp of a Synapse object represents the post-synaptic potential evoked by the pre-synaptic neuron. This value is actually summed by the post-synaptic neuron over all other synapses of the same projection in sum(target). If not defined, it will simply represent the product between the pre-synaptic firing rate (pre.r) and the weight value (w).\nThe post-synaptic potential of a single synapse is by default:\npsp = w * pre.r\nwhere pre.r is the pre-synaptic firing rate, but you may want to override this behaviour in certain cases.\nFor example, you may want to model a non-linear synapse with a logarithmic term:\n\nr_{i} = \\sum_j \\log \\left( \\frac {( r_{j} * w_{ij} ) + 1 } { ( r_{j} * w_{ij} ) - 1 } \\right)\n\nIn this case, you can just modify the psp argument of the synapse:\nNonLinearSynapse = ann.Synapse( \n    psp = \"\"\"\n        log( (pre.r * w + 1 ) / (pre.r * w - 1) )\n    \"\"\"\n)\nNo further modification has to be done in the post-synaptic neuron, this value will be summed over all pre-synaptic neurons automatically when using sum(target).",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/RateSynapse.html#defining-the-post-synaptic-operation",
    "href": "manual/RateSynapse.html#defining-the-post-synaptic-operation",
    "title": "Rate-coded synapses",
    "section": "",
    "text": "By default, a post-synaptic neuron calling sum(target) will compute the sum over all incoming synapses of their defined psp:\n\\text{sum(exc)} = \\sum_{i \\in \\text{exc}} \\text{psp}(i) = \\sum_{i \\in \\text{exc}} w_i * \\text{pre}.r_i\nIt is possible to define a different operation performed on the connected synapses, using the operation argument of the synapse:\nMaxPooling = ann.Synapse(\n    psp = \"w * pre.r\",\n    operation = \"max\"\n)\nIn this case, sum(target) will represent the maximum value of w * pre.r over all incoming synapses, not their sum. It can be useful when defining pooling operations in a convolutional network, for example.\nThe available operations are:\n\n\"sum\": (default): sum of all incoming psps.\n\"max\": maximum of all incoming psps.\n\"min\": minimum of all incoming psps.\n\"mean\": mean of all incoming psps.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese operations are only possible for rate-coded synapses.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Rate-coded synapses"
    ]
  },
  {
    "objectID": "manual/Recording.html",
    "href": "manual/Recording.html",
    "title": "Recording with Monitors",
    "section": "",
    "text": "Between two calls to simulate(), all neural and synaptic variables can be accessed through the generated attributes. The evolution of neural or synaptic variables during a simulation phase can be selectively recorded using Monitor objects.\nThe Monitor object can be created at any time (before or after compile()) to record any variable of a Population, PopulationView, Dendrite or Projection.\n\n\n\n\n\n\nImportant\n\n\n\nThe value of each variable is stored for every simulation step in the RAM. For huge networks and long simulations, this can very rapidly fill up the available memory and lead to cache defaults, thereby degrading strongly the performance. It is the user’s responsability to record only the needed variables and to regularly save the values in a file.\n\n\n\n\nThe Monitor object takes four arguments:\n\nobj: the object to monitor. It can be a population, a population view (a slice of a population or an individual neuron), a dendrite (the synapses of a projection which reach a single post-synaptic neuron) or a projection.\nvariables: a (list of) variable name(s) which should be recorded. They should be variables of the neuron/synapse model of the corresponding object. Although it generally makes no sense, you can also record parameters of an object. By definition a parameter is constant throughout a simulation, but it maybe useful when tracking externally-set inputs, for example. You can know which attributes are recordable by checking the attributes attribute of the object (pop.attributes or proj.attributes).\nperiod: the period in ms at which recordings should be made. By default, recording is done after each simulation step (dt), but this may be overkill in long simulations.\nstart: boolean value stating if the recordings should start immediately after the creation of the monitor (default), or if it should be started later.\n\nSome examples:\nm = ann.Monitor(pop, 'r') # record r in all neurons of pop\nm = ann.Monitor(pop, ['r', 'v']) # record r and v of all neurons\nm = ann.Monitor(pop[:100], 'r', period=10.0) # record r in the first 100 neurons of pop, every 10 ms\nm = ann.Monitor(pop, 'r', start=False) # record r in all neurons, but do not start recording\nSpiking networks additionally allow to record the spike events in a population (see later). You also can record conductances (e.g. g_exc) and weighted sums of inputs in rate-coded networks (sum(exc)) the same way:\nm = ann.Monitor(pop, ['spike', 'g_exc', 'g_inh'])\nm = ann.Monitor(pop, ['r', 'sum(exc)', 'sum(inh)'])\n\n\nIf start is set to False, recordings can be started later by calling the start() method:\nm = ann.Monitor(pop, 'r', start=False)\nann.simulate(100.)\nm.start()\nann.simulate(100.)\nIn this case, only the last 100 ms of the simulation will be recorded. Otherwise, recording would start immediately after the creation of the object.\n\n\n\nIf you are interested in recording only specific periods of the simulation, you can ause and resume recordings:\nm = ann.Monitor(pop, 'r')\nann.simulate(100.)\nm.pause()\nann.simulate(1000.)\nm.resume()\nann.simulate(100.)\nIn this example, only the first and last 100 ms of the simulation are recorded.\n\n\n\nThe recorded values are obtained through the get() method. If no argument is passed, a dictionary is returned with one element per recorded variable. If the name of a variable is passed (for example get('r')), the recorded values for this variable are directly returned:\nm = ann.Monitor(pop, ['r', 'v'])\nann.simulate(100.)\ndata = m.get()\nann.simulate(100.)\nr = m.get('r')\nv = m.get('v')\nIn the example above, data is a dictionary with two keys 'r' and 'v', while r and v are directly the recorded arrays.\nThe recorded values are Numpy arrays with two dimensions, the first one representing time, the second one representing the ranks of the recorded neurons.\nFor example, the time course of the firing rate of the neuron of rank 15 is accessed through:\ndata['r'][:, 15]\nThe firing rates of the whole population after 50 ms of simulation are accessed with:\ndata['r'][50, :]\n\n\n\n\n\n\nNote\n\n\n\nOnce you call get(), the internal data is erased, so calling it immediately afterwards will return an empty recording data. You need to simulate again in order to retrieve new values.\n\n\nRepresentation of time\nThe time indices are in simulation steps (integers), not in real time (ms). If dt is different from 1.0, this indices must be multiplied by dt() in order to plot real times:\nann.setup(dt=0.1)\n# ...\nm = ann.Monitor(pop, 'r')\nann.simulate(100.)\nr = m.get('r')\nplt.plot(dt()*np.arange(100), r[:, 15])\nIf recordings used the pause() and resume() methods, get() returns only one array with all recordings concatenated. You can access the steps at which the recording started or paused with the times() method:\nm = ann.Monitor(pop, 'r')\nann.simulate(100.)\nm.pause()\nann.simulate(1000.)\nm.resume()\nann.simulate(100.)\nr = m.get('r') # A (200, N) Numpy array\nprint(m.times()) # {'start': [0, 1100], 'stop': [100, 1200]}\n\n\n\nAny variable defined in the neuron type can be recorded. An exception for spiking neurons is the spike variable itself, which is never explicitely defined in the neuron type but can be recorded:\nm = ann.Monitor(pop, ['v', 'spike'])\nUnlike other variables, the binary value of spike is not recorded at each time step, which would lead to very sparse matrices, but only the times (in steps, not milliseconds) at which spikes actually occur.\nAs each neuron fires differently (so each neuron will have recorded spikes of different lengths), get() in this case does not return a Numpy array, but a dictionary associating to each recorded neuron a list of spike times:\nm = ann.Monitor(pop, ['v', 'spike'])\nann.simulate(100.0)\ndata = m.get('spike')\nprint(data[0]) # [23, 76, 98]\nIn the example above, the neuron of rank 0 has spiked 3 times (at t = 23, 76 and 98 ms if dt = 1.0) during the first 100 ms of the simulation.\nRaster plots\nIn order to easily display raster plots, the method raster_plot() is provided to transform this data into an easily plottable format:\nspike_times, ranks = m.raster_plot(data)\nplt.plot(spike_times, ranks, '.')\nraster_plot() returns two Numpy arrays, whose length is the total number of spikes emitted during the simulation. The first array contains the spike times (ín ms) while the second contains the ranks of the neurons who fired. They can be directly used t produce the raster plot with Matplotlib.\nAn example of the use of raster_plot() can be seen in the Izhikevich pulse network section.\nInter-spike interval (ISI) and coefficient of variation (ISI CV)\nIn addition to a raster plot, the distribution of inter-spike intervals could be considered for evaluation. The inter-spike interval (short ISI) is defined as the time in milliseconds between two consecutive spike events. The method inter_spike_interval() transforms the recorded spike events into a list of ISI across all neurons (default) or for indivdual neurons (add per_neuron = True to argument list) which could be fed into a histogram method provided by matplotlib:\npop_isi = m.inter_spike_interval(data)\nplt.hist(pop_isi)\nThe coefficient of variation is a measure often reported together with the inter-spike interval. These values can be easily obtained using another function of the Monitor object:\npop_isi_cv = m.coefficient_of_variation(data)\nplt.hist(pop_isi_cv)\nAn example of the use of inter_spike_interval() and coefficient_of_variation() can be seen in the COBA network section.\nMean firing rate\nThe mean firing rate in the population can be easily calculated using the length of the arrays returned by raster_plot:\nN = 1000 # number of neurons\nduration = 500. # duration of the simulation\ndata = m.get('spike')\nspike_times, ranks = m.raster_plot(data)\nprint('Mean firing rate:', len(spike_times)/float(N)/duration*1000., 'Hz.')\nFor convenience, this value is returned by the mean_fr() method, which has access to the number of recorded neurons and the duration of the recordings:\nprint('Mean firing rate:', m.mean_fr(data), 'Hz.')\nFiring rates\nAnother useful method is smoothed_rate(). It allows to display the instantaneous firing rate of each neuron based on the spike recordings:\nrates = m.smoothed_rate(data)\nplt.imshow(rates, aspect='auto')\nFor each neuron, it returns an array with the instantaneous firing rate during the whole simulation. The instantaneous firing rate is computed by inverting the inter-spike interval (ISI) between two consecutive spikes, and assigning it to all simulation steps between the two spikes.\nAs this value can be quite fluctuating, a smooth argument in milliseconds can be passed to smoothed_rate() to apply a low-pass filter on the firing rates:\nrates = m.smoothed_rate(data, smooth=200.0)\nplt.imshow(rates, aspect='auto')\nA smoothed firing rate for the whole population is also accessible through population_rate():\nfr = m.population_rate(data, smooth=200.0)\nHistogram\nhistogram() allows to count the spikes emitted in the whole population during successive bins of the recording duration:\nhisto = m.histogram(data, bins=1.0)\nplt.plot(histo)\nbins represents the size of each bin, here 1 ms. By default, the bin size is dt.\nNote : the methods to analyse the spike patterns are also available outside the monitors. For example if you save the spike recordings into a file using numpy:\nspikes = m.get('spike')\n\nnp.save('spikes.npy', spikes)\nyou can analyze them in a separate file like this:\n# Load the data\nspikes = np.load('spikes.npy').item()\n\n# Compute the raster plot\nt, n = ann.raster_plot(spikes)\n\n# Compute the population firing rate\nfr = ann.histogram(spikes, bins=1.)\n\n# Smoothed firing rate\nsr = ann.smoothed_rate(spikes, smooth=10.0)\n\n# Population firing rate\npr = ann.population_rate(spikes, smooth=10.0)\n\n# Global firing rate\nmfr = ann.mean_fr(spikes)\n\n\n\n\nRecording of synaptic variables such as weights w during learning is also possible using the monitor object. However, it can very easily lead to important memory consumption. Let’s suppose we have a network composed of two populations of 1000 neurons each, fully connected: each neuron of the second population receives 1000 synapses. This makes a total of 1 million synapses for the projection and, supposing the weights w use the double floating precision, requires 4 MB of memory. If you record w during a simulation of 1 second (1000 steps, with dt=1.0), the total added memory consumption would already be around 4GB.\nTo avoid fast memory fills, you should either 1) record the projection variables infrequently (by setting the period argument of the Monitor), or 2) selectively record particular dendrites. The corresponding dendrite should be simply passed to the monitor:\ndendrite = proj.dendrite(12) # or simply proj[12]\nm = ann.Monitor(dendrite, 'w')\nann.simulate(1000.0)\ndata = m.get('w')\nThe Monitor object has the same start(), pause(), resume() and get() methods as for populations. get() returns also 2D Numpy arrays, the first index being time, the second being the index of the synapse. To know to which pre-synaptic neuron it corresponds, use the pre_ranks attribute of the dendrite:\ndendrite.pre_ranks # [0, 3, 12, ..]\nTo record a complete projection, simply pass it to the Monitor:\nm = ann.Monitor(proj, 'w', period=1000.)\nann.simulate(10000.0)\ndata = m.get('w')\nOne last time, do not record all weights of a projection at each time step!\n\n\n\n\n\n\nWarning\n\n\n\nRecording synaptic variables with CUDA is not available.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Recording with Monitors"
    ]
  },
  {
    "objectID": "manual/Recording.html#neural-variables",
    "href": "manual/Recording.html#neural-variables",
    "title": "Recording with Monitors",
    "section": "",
    "text": "The Monitor object takes four arguments:\n\nobj: the object to monitor. It can be a population, a population view (a slice of a population or an individual neuron), a dendrite (the synapses of a projection which reach a single post-synaptic neuron) or a projection.\nvariables: a (list of) variable name(s) which should be recorded. They should be variables of the neuron/synapse model of the corresponding object. Although it generally makes no sense, you can also record parameters of an object. By definition a parameter is constant throughout a simulation, but it maybe useful when tracking externally-set inputs, for example. You can know which attributes are recordable by checking the attributes attribute of the object (pop.attributes or proj.attributes).\nperiod: the period in ms at which recordings should be made. By default, recording is done after each simulation step (dt), but this may be overkill in long simulations.\nstart: boolean value stating if the recordings should start immediately after the creation of the monitor (default), or if it should be started later.\n\nSome examples:\nm = ann.Monitor(pop, 'r') # record r in all neurons of pop\nm = ann.Monitor(pop, ['r', 'v']) # record r and v of all neurons\nm = ann.Monitor(pop[:100], 'r', period=10.0) # record r in the first 100 neurons of pop, every 10 ms\nm = ann.Monitor(pop, 'r', start=False) # record r in all neurons, but do not start recording\nSpiking networks additionally allow to record the spike events in a population (see later). You also can record conductances (e.g. g_exc) and weighted sums of inputs in rate-coded networks (sum(exc)) the same way:\nm = ann.Monitor(pop, ['spike', 'g_exc', 'g_inh'])\nm = ann.Monitor(pop, ['r', 'sum(exc)', 'sum(inh)'])\n\n\nIf start is set to False, recordings can be started later by calling the start() method:\nm = ann.Monitor(pop, 'r', start=False)\nann.simulate(100.)\nm.start()\nann.simulate(100.)\nIn this case, only the last 100 ms of the simulation will be recorded. Otherwise, recording would start immediately after the creation of the object.\n\n\n\nIf you are interested in recording only specific periods of the simulation, you can ause and resume recordings:\nm = ann.Monitor(pop, 'r')\nann.simulate(100.)\nm.pause()\nann.simulate(1000.)\nm.resume()\nann.simulate(100.)\nIn this example, only the first and last 100 ms of the simulation are recorded.\n\n\n\nThe recorded values are obtained through the get() method. If no argument is passed, a dictionary is returned with one element per recorded variable. If the name of a variable is passed (for example get('r')), the recorded values for this variable are directly returned:\nm = ann.Monitor(pop, ['r', 'v'])\nann.simulate(100.)\ndata = m.get()\nann.simulate(100.)\nr = m.get('r')\nv = m.get('v')\nIn the example above, data is a dictionary with two keys 'r' and 'v', while r and v are directly the recorded arrays.\nThe recorded values are Numpy arrays with two dimensions, the first one representing time, the second one representing the ranks of the recorded neurons.\nFor example, the time course of the firing rate of the neuron of rank 15 is accessed through:\ndata['r'][:, 15]\nThe firing rates of the whole population after 50 ms of simulation are accessed with:\ndata['r'][50, :]\n\n\n\n\n\n\nNote\n\n\n\nOnce you call get(), the internal data is erased, so calling it immediately afterwards will return an empty recording data. You need to simulate again in order to retrieve new values.\n\n\nRepresentation of time\nThe time indices are in simulation steps (integers), not in real time (ms). If dt is different from 1.0, this indices must be multiplied by dt() in order to plot real times:\nann.setup(dt=0.1)\n# ...\nm = ann.Monitor(pop, 'r')\nann.simulate(100.)\nr = m.get('r')\nplt.plot(dt()*np.arange(100), r[:, 15])\nIf recordings used the pause() and resume() methods, get() returns only one array with all recordings concatenated. You can access the steps at which the recording started or paused with the times() method:\nm = ann.Monitor(pop, 'r')\nann.simulate(100.)\nm.pause()\nann.simulate(1000.)\nm.resume()\nann.simulate(100.)\nr = m.get('r') # A (200, N) Numpy array\nprint(m.times()) # {'start': [0, 1100], 'stop': [100, 1200]}\n\n\n\nAny variable defined in the neuron type can be recorded. An exception for spiking neurons is the spike variable itself, which is never explicitely defined in the neuron type but can be recorded:\nm = ann.Monitor(pop, ['v', 'spike'])\nUnlike other variables, the binary value of spike is not recorded at each time step, which would lead to very sparse matrices, but only the times (in steps, not milliseconds) at which spikes actually occur.\nAs each neuron fires differently (so each neuron will have recorded spikes of different lengths), get() in this case does not return a Numpy array, but a dictionary associating to each recorded neuron a list of spike times:\nm = ann.Monitor(pop, ['v', 'spike'])\nann.simulate(100.0)\ndata = m.get('spike')\nprint(data[0]) # [23, 76, 98]\nIn the example above, the neuron of rank 0 has spiked 3 times (at t = 23, 76 and 98 ms if dt = 1.0) during the first 100 ms of the simulation.\nRaster plots\nIn order to easily display raster plots, the method raster_plot() is provided to transform this data into an easily plottable format:\nspike_times, ranks = m.raster_plot(data)\nplt.plot(spike_times, ranks, '.')\nraster_plot() returns two Numpy arrays, whose length is the total number of spikes emitted during the simulation. The first array contains the spike times (ín ms) while the second contains the ranks of the neurons who fired. They can be directly used t produce the raster plot with Matplotlib.\nAn example of the use of raster_plot() can be seen in the Izhikevich pulse network section.\nInter-spike interval (ISI) and coefficient of variation (ISI CV)\nIn addition to a raster plot, the distribution of inter-spike intervals could be considered for evaluation. The inter-spike interval (short ISI) is defined as the time in milliseconds between two consecutive spike events. The method inter_spike_interval() transforms the recorded spike events into a list of ISI across all neurons (default) or for indivdual neurons (add per_neuron = True to argument list) which could be fed into a histogram method provided by matplotlib:\npop_isi = m.inter_spike_interval(data)\nplt.hist(pop_isi)\nThe coefficient of variation is a measure often reported together with the inter-spike interval. These values can be easily obtained using another function of the Monitor object:\npop_isi_cv = m.coefficient_of_variation(data)\nplt.hist(pop_isi_cv)\nAn example of the use of inter_spike_interval() and coefficient_of_variation() can be seen in the COBA network section.\nMean firing rate\nThe mean firing rate in the population can be easily calculated using the length of the arrays returned by raster_plot:\nN = 1000 # number of neurons\nduration = 500. # duration of the simulation\ndata = m.get('spike')\nspike_times, ranks = m.raster_plot(data)\nprint('Mean firing rate:', len(spike_times)/float(N)/duration*1000., 'Hz.')\nFor convenience, this value is returned by the mean_fr() method, which has access to the number of recorded neurons and the duration of the recordings:\nprint('Mean firing rate:', m.mean_fr(data), 'Hz.')\nFiring rates\nAnother useful method is smoothed_rate(). It allows to display the instantaneous firing rate of each neuron based on the spike recordings:\nrates = m.smoothed_rate(data)\nplt.imshow(rates, aspect='auto')\nFor each neuron, it returns an array with the instantaneous firing rate during the whole simulation. The instantaneous firing rate is computed by inverting the inter-spike interval (ISI) between two consecutive spikes, and assigning it to all simulation steps between the two spikes.\nAs this value can be quite fluctuating, a smooth argument in milliseconds can be passed to smoothed_rate() to apply a low-pass filter on the firing rates:\nrates = m.smoothed_rate(data, smooth=200.0)\nplt.imshow(rates, aspect='auto')\nA smoothed firing rate for the whole population is also accessible through population_rate():\nfr = m.population_rate(data, smooth=200.0)\nHistogram\nhistogram() allows to count the spikes emitted in the whole population during successive bins of the recording duration:\nhisto = m.histogram(data, bins=1.0)\nplt.plot(histo)\nbins represents the size of each bin, here 1 ms. By default, the bin size is dt.\nNote : the methods to analyse the spike patterns are also available outside the monitors. For example if you save the spike recordings into a file using numpy:\nspikes = m.get('spike')\n\nnp.save('spikes.npy', spikes)\nyou can analyze them in a separate file like this:\n# Load the data\nspikes = np.load('spikes.npy').item()\n\n# Compute the raster plot\nt, n = ann.raster_plot(spikes)\n\n# Compute the population firing rate\nfr = ann.histogram(spikes, bins=1.)\n\n# Smoothed firing rate\nsr = ann.smoothed_rate(spikes, smooth=10.0)\n\n# Population firing rate\npr = ann.population_rate(spikes, smooth=10.0)\n\n# Global firing rate\nmfr = ann.mean_fr(spikes)",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Recording with Monitors"
    ]
  },
  {
    "objectID": "manual/Recording.html#synaptic-variables",
    "href": "manual/Recording.html#synaptic-variables",
    "title": "Recording with Monitors",
    "section": "",
    "text": "Recording of synaptic variables such as weights w during learning is also possible using the monitor object. However, it can very easily lead to important memory consumption. Let’s suppose we have a network composed of two populations of 1000 neurons each, fully connected: each neuron of the second population receives 1000 synapses. This makes a total of 1 million synapses for the projection and, supposing the weights w use the double floating precision, requires 4 MB of memory. If you record w during a simulation of 1 second (1000 steps, with dt=1.0), the total added memory consumption would already be around 4GB.\nTo avoid fast memory fills, you should either 1) record the projection variables infrequently (by setting the period argument of the Monitor), or 2) selectively record particular dendrites. The corresponding dendrite should be simply passed to the monitor:\ndendrite = proj.dendrite(12) # or simply proj[12]\nm = ann.Monitor(dendrite, 'w')\nann.simulate(1000.0)\ndata = m.get('w')\nThe Monitor object has the same start(), pause(), resume() and get() methods as for populations. get() returns also 2D Numpy arrays, the first index being time, the second being the index of the synapse. To know to which pre-synaptic neuron it corresponds, use the pre_ranks attribute of the dendrite:\ndendrite.pre_ranks # [0, 3, 12, ..]\nTo record a complete projection, simply pass it to the Monitor:\nm = ann.Monitor(proj, 'w', period=1000.)\nann.simulate(10000.0)\ndata = m.get('w')\nOne last time, do not record all weights of a projection at each time step!\n\n\n\n\n\n\nWarning\n\n\n\nRecording synaptic variables with CUDA is not available.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Recording with Monitors"
    ]
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Oliver Maith, Helge Ülo Dinkelbach, Javier Baladron, Julien Vitay, and Fred H. Hamker (2022).\nBOLD monitoring in the neural simulator ANNarchy.\nFrontiers in Neuroinformatics 16:790966\ndoi:10.3389/fninf.2022.790966\nHelge Ülo Dinkelbach, Badr-Eddine Bouhlal, Julien Vitay, and Fred H. Hamker (2022).\nAuto-selection of an optimal sparse matrix format in the neuro-simulator ANNarchy.\nFrontiers in Neuroinformatics 16:877945\ndoi:10.3389/fninf.2022.877945\nHelge Ülo Dinkelbach, Julien Vitay, and Fred H. Hamker (2019).\nScalable simulation of rate-coded and spiking neural networks on shared memory systems.\n2019 Conference on Cognitive Computational Neuroscience, 13-16 September 2019, Berlin, Germany.\ndoi:10.32470/CCN.2019.1109-0\nJulien Vitay, Helge Ülo Dinkelbach, and Fred H. Hamker (2015).\nANNarchy: a code generation approach to neural simulations on parallel hardware.\nFrontiers in Neuroinformatics 9:19\ndoi:10.3389/fninf.2015.00019\nHelge Ü. Dinkelbach, Julien Vitay, and Fred H. Hamker (2012).\nComparison of GPU- and CPU-implementations of mean-firing rate neural networks on parallel hardware.\nNetwork: Computation in Neural Systems 23(4)\ndoi:10.3109/0954898X.2012.739292",
    "crumbs": [
      "ANNarchy",
      "Publications"
    ]
  },
  {
    "objectID": "tutorial/index.html",
    "href": "tutorial/index.html",
    "title": "ANNarchy - Artificial Neural Networks architect",
    "section": "",
    "text": "ANNarchy (Artificial Neural Networks architect) is a neuro-simulator for rate-coded or spiking neural networks in Python.\nThis tutorial for beginners is a condensed version of the manual and the associated Jupyter notebooks. Its intent is to quickly introduce users to ANNarchy so they can start modeling right away.\nThe slides of the tutorial are available here (or simply press 'f' in the frame below).",
    "crumbs": [
      "Tutorial"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ANNarchy (Artificial Neural Networks architect)",
    "section": "",
    "text": "ANNarchy (Artificial Neural Networks architect) is a Python neurosimulator designed for rate-coded and spiking neural networks, released under the GNU GPL v2 or later.\n\n\n\nResources\n\nSource code: github.com/ANNarchy/ANNarchy\nDocumentation: annarchy.github.io\nForum: Google Forum\nBug reports and feature requests: Github Issues\n\n\nDocumentation\n\nInstallation instructions.\nTutorial for a quick presentation of the simulator.\nManual with a full description of the functionalities.\nNotebooks showcasing many different networks.\nReference implementation with all classes and functions.\n\n\n\n\n\n\n\n\n\n\nCitation\n\n\nIf you use ANNarchy for your research, we would appreciate if you cite the following paper. See the full list of publications related to ANNarchy here\n\nVitay J, Dinkelbach HÜ and Hamker FH (2015). ANNarchy: a code generation approach to neural simulations on parallel hardware. Frontiers in Neuroinformatics 9:19. doi:10.3389/fninf.2015.00019\n\n\n@article{Vitay2015,\n  title = {{{ANNarchy}}: A Code Generation Approach to Neural Simulations on Parallel Hardware},\n  author = {Vitay, Julien and Dinkelbach, Helge {\\\"U}. and Hamker, Fred H.},\n  year = {2015},\n  journal = {Frontiers in Neuroinformatics},\n  volume = {9},\n  number = {19},\n  doi = {10.3389/fninf.2015.00019},\n  url = {https://www.frontiersin.org/articles/10.3389/fninf.2015.00019},\n  abstract = {Many modern neural simulators focus on the simulation of networks of spiking neurons on parallel hardware. Another important framework in computational neuroscience, rate-coded neural networks, is mostly difficult or impossible to implement using these simulators. We present here the ANNarchy (Artificial Neural Networks architect) neural simulator, which allows to easily define and simulate rate-coded and spiking networks, as well as combinations of both. The interface in Python has been designed to be close to the PyNN interface, while the definition of neuron and synapse models can be specified using an equation-oriented mathematical description similar to the Brian neural simulator. This information is used to generate C++ code that will efficiently perform the simulation on the chosen parallel hardware (multi-core system or graphical processing unit). Several numerical methods are available to transform ordinary differential equations into an efficient C++code. We compare the parallel performance of the simulator to existing solutions.}\n}",
    "crumbs": [
      "ANNarchy"
    ]
  },
  {
    "objectID": "manual/Inputs.html",
    "href": "manual/Inputs.html",
    "title": "Setting inputs",
    "section": "",
    "text": "The methods to set inputs to a network depends on whether it is rate-coded or spiking. Specific populations to set inputs are described in the API.\n\n\n\n\nThe simplest way to define an input population is to use a dummy neuron which simply holds a firing rate r as parameter, and connect it to another population:\nimport ANNarchy as ann\n\ninput_pop = ann.Population(10, Neuron(parameters=\"r=0.0\"))\n\npop = ann.Population (10, LeakyIntegrator)\n\nproj = ann.Projection(input_pop, pop, 'exc')\nproj.connect_one_to_one(1.0)\n\nann.compile()\n\nann.simulate(100.)\n\ninput_pop.r = 1.0\n\nann.simulate(100.)\nThe only thing you need to do is to manipulate the numpy array r holded by input_pop, and it will influence the “real” population pop\nIt is important to define r as a parameter of the neuron, not a variable in equations. A variable sees its value updated at each step, so the value you set would be immediately forgotten.\n\nUsing this method necessitates to interact with the input population in the Python script everytime you want to change the inputs. If the inputs change every time step, your simulation will alternate between Python and C++ executions and potentially become very slow.\n\n\n\n\nIf the inputs change frequently, it may be more efficient to store all these values in a TimedArray (doc in the API).\nLet’s suppose you have a population of 10 neurons which should be activated sequentially over time. You can store the inputs to these neurons in a Numpy array, where the first axis corresponds to time and the second (or more) to the geometry of the population:\ninputs = np.array(\n    [\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n    ]\n)\n\ninp = ann.TimedArray(rates=inputs)\n\npop = ann.Population(10, Neuron(equations=\"r=sum(exc)\"))\n\nproj = ann.Projection(inp, pop, 'exc')\nproj.connect_one_to_one(1.0)\n\nann.compile()\n\nann.simulate(10.)\nWith this code, each neuron will be activated in sequence at each time step (dt=1.0 by default). If you simulate longer than 10 ms, the last input [0, 0, .., 1] will be kept forever.\nIf the rates array has two dimensions, the corresponding population will be 1D. You can pass a multidimensional array to obtain a 2D or 3D population.\nPresenting a input for only one time step is very short, especially if the population pop uses ODEs to integrate the inputs. You can provide a schedule parameter to the TimedArray to define how long (in ms) an input should be presented:\ninp = ann.TimedArray(rates=inputs, schedule=10.)\nHere each input will be kept constant for 10 ms, so the 10 inputs will need 100 ms of simulation to be presented. If you do not want a regular schedule, you can also provide a list of times where inputs should be set:\ninp = ann.TimedArray(rates=inputs, \n    schedule=[0., 10., 30., 60., 100., 150., 210., 280., 360., 450.])\nThe length of the schedule list should be equal or smaller to the number of inputs defined in rates. If this length is smaller (e.g. 7), only the 7 first inputs will be used as inputs. If the length is bigger, it will lead to an error.\nA TimedArray can be reset to iterate again over the inputs:\ninp = ann.TimedArray(rates=inputs, schedule=10.)\n\n...\n\nann.compile()\n\nann.simulate(100.) # The ten inputs are shown with a schedule of 10 ms\n\ninp.reset()\n\nann.simulate(100.) # The same ten inputs are presented again.\nThe times declared in schedule are therefore relative to the last call to reset() (or to t=0.0 at the beginning).\nIf you want to systematically iterate over the inputs without iterating over simulate() and reset(), you can provide the period argument to the TimedArray to define how often the inputs will be reset:\ninp = ann.TimedArray(rates=inputs, schedule=10.. period=100.)\n\n...\n\nann.simulate(100000.)\nIf the period is smaller than the total durations of the inputs, the last inputs will be skipped.\nThe rates, schedule and period can be modified after compilation. The only constraint is that the size of the population (defined in the rates array) must stay the same.\n\n\n\nImages\nA simple utility to directly load an image into the firing rates r of a Population is provided by the ImagePopulation class. This class is not automatically imported with ANNarchy, you need to explicitly import it:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.image import ImagePopulation\n\ninp = ImagePopulation(geometry=(480, 640))\ninp.set_image('image.jpg')\nUsing this class requires that you have the Python Image Library installed (pip install Pillow). Any image with a format supported by Pillow can be loaded, see the documentation.\nThe ImagePopulation must be initialized with a geometry corresponding to the desired size of the population. If it differs from the resolution of the image (set with set_image), the image will be first resized to match the geometry of the population.\n\n\n\n\n\n\nNote\n\n\n\nThe size of an image is defined as (height, width), so a 640x480 image should be loaded in a (480, 640) population.\n\n\nIf the geometry has only two dimensions (480, 640), each neuron will represent the luminance (or brightness) of the corresponding pixel.\nIf the geometry has three dimensions (480, 640, 3), the color channels will additionally be represented (RGB). Any other value than 3 for the third dimension will generate an error.\n\n\n\n\n\n\nNote\n\n\n\nThe firing rate r of a neuron is 1.0 when the corresponding pixel is white (value 255 as an unsigned integer on 8 bits).\n\n\nNote that the following code is functionally equivalent:\nimport ANNarchy as ann\nfrom PIL import Image\n\ninp = ann.Population(geometry=(480, 640), Neuron(parameters=\"r=0.0\"))\n\nimg = Image.open('image.jpg')\nimg = img.convert('L')\nimg = img.resize((480, 640)) /255.\n\ninp.r = np.array(img)\nAn example is provided in examples/image/Image.py.\nVideos\nThe VideoPopulation class allows to retrieve images from a Webcam, using the OpenCV computer vision library, version 4.0 or later. pkg-config opencv4 --cflags --libs should not return an error. vtk might have to be additionally installed.\nimport ANNarchy as ann\nfrom ANNarchy.extensions.image import VideoPopulation\n\ninp = VideoPopulation(geometry=(480, 640))\n\nann.compile()\n\ninp.start_camera(0)\n\nwhile(True):\n  inp.grab_image()\n  ann.simulate(10.0)\nA geometry must be provided as for ImagePopulations. The camera must be explicitly started after compile() with inp.start_camera(0). 0 corresponds to the index of your camera, change it if you have multiple cameras.\nThe VideoPopulation can then acquire frames from the camera with inp.grab_image() and store the correponding image in its firing rate r (also scaled between 0.0 and 1.0). An example is provided in examples/image/Webcam.py.\n\n\n\n\n\n\nWarning\n\n\n\nVideoPopulation is not available with the CUDA backend.\n\n\n\n\n\n\n\n\nTo control the spiking patterns of a spiking population, the simplest way is to inject current into the corresponding membrane potentials. The built-in neuron types defined by ANNarchy have a i_offset variable that can be used for this purpose:\nimport numpy as np\nimport ANNarchy as ann\nann.setup(dt=0.1)\n\npop = ann.Population(100, Izhikevich)\n\npop.i_offset= np.linspace(0.0, 30.0, 100)\n\nm = ann.Monitor(pop, 'spike')\n\nann.compile()\n\nann.simulate(100.)\n\ndata = m.get('spike')\nt, n = m.raster_plot(data)\n\nimport matplotlib.pyplot as plt\nplt.plot(t, n, '.')\nplt.ylim(0, 100)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\nIf you want the injected current to be time-varying, you can design a rate-coded population of the same size as the spiking population and create a CurrentInjection projection between them:\ninp = ann.Population(100, Neuron(equations=\"r = sin(t)\"))\n\npop = ann.Population(100, Izhikevich)\n\nproj = ann.CurrentInjection(inp, pop, 'exc')\nproj.connect_current()\nThe current g_exc of a neuron in pop will be set at each time step to the firing rate r of the corresponding neuron in inp (i.e. with the same rank). inp can also be defined as a TimedArray.\nThe connector method should be connect_current(), which accepts no weight value and no delay.\n\n\n\n\nIf you want to control precisely the spiking patterns used as inputs, you can provide a list of spike times to a SpikeSourceArray object:\nimport ANNarchy as ann\nann.setup(dt=0.1)\n\nspike_times = [\n  [  10 + i/10,\n     20 + i/10,\n     30 + i/10,\n     40 + i/10,\n     50 + i/10,\n     60 + i/10,\n     70 + i/10,\n     80 + i/10,\n     90 + i/10] for i in range(100)\n]\n\npop = ann.SpikeSourceArray(spike_times=spike_times)\n\nm = ann.Monitor(pop, 'spike')\n\nann.compile()\n\nann.simulate(100.)\n\ndata = m.get('spike')\nt, n = m.raster_plot(data)\n\nimport matplotlib.pyplot as plt\nplt.plot(t, n, '.')\nplt.ylim(0, 100)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n\nThe spike_times argument must be a list of lists containing the spike times in ms. Its length defines the number of neurons in the population. It is not possible to define a geometry. If one neuron should not spike at all, just provide an empty list. The different neurons can have a different number of spikes.\nIf you want to repeat the same stimulation, you can reset the SpikeSourceArray, what will set its internal time back to 0.0:\nann.simulate(100.)\n\npop.reset()\n\nann.simulate(100.)\nThe spikes times can be changed after compilation, bit it must have the same number of neurons:\npop.spike_times = new_spike_times_array\nAn example is provided in examples/pyNN/IF_curr_alpha.py.\n\n\n\n\n\n\nWarning\n\n\n\nSpikeSourceArray is not available with the CUDA backend.\n\n\n\n\n\nThe PoissonPopulation class allows to create a population of identical spiking neurons, whose spiking patterns vary according to a Poisson distribution:\nimport ANNarchy as ann\nann.setup(dt=0.1)\n\npop = ann.PoissonPopulation(100, rates=30.)\n\nm = ann.Monitor(pop, 'spike')\n\nann.compile()\n\nann.simulate(100.)\n\ndata = m.get('spike')\nt, n = m.raster_plot(data)\n\nimport matplotlib.pyplot as plt\nplt.plot(t, n, '.')\nplt.ylim(0, 100)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\nIn this example, each of the 100 neurons fires randomly, with a mean firing rate of 30 Hz (next figure, top-left).\nIt is also possible to specify the mean firing rate individually for each neuron (next figure, top-right):\npop = PoissonPopulation(100, rates=np.linspace(0.0, 100.0, 100))\nThe rates attribute can be modified at any time during the simulation, as long as it has the same size as the population.\nAnother possibility is to define a rule for the evolution of the mean firing rate in the population (next figure, bottom-left):\npop = PoissonPopulation(\n        geometry=100,\n        parameters = \"\"\"\n            amp = 100.0\n            frequency = 50.0\n        \"\"\",\n        rates=\"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\"\n    )\nThe rule can only depend on the time t: the corresponding mean firing rate is the same for all neurons in the population.\nFinally, the rates argument can be replaced by a target, so it can be computed by another rate-coded population (next figure, bottom-right):\nrates = 10.*np.ones((2, 100))\nrates[0, :50] = 100.\nrates[1, 50:] = 100.\ninp = TimedArray(rates = rates, schedule=50.)\n\npop = PoissonPopulation(100, target=\"exc\")\n\nproj = Projection(inp, pop, 'exc')\nproj.connect_one_to_one(1.0)\nIn the code above, we define a TimedArray for 100 neurons, so that half of the neurons fire at 100 Hz, while the others fire at 10 Hz. Every 50 ms, the two halves are swapped.\nWe just need to create a projection with the target \"exc\" between the TimedArray and the PoissonPopulation (with a one-to-one pattern and weights 1.0 to preserve scaling), and the Poisson population will reflect the firing rates defined by the TimedArray.\n\n\n\n\nHomogeneousCorrelatedSpikeTrains defines spiking neurons following a homogeneous distribution with correlated spike trains.\nThe method describing the generation of homogeneous correlated spike trains is described in:\n\nBrette, R. (2009). Generation of correlated spike trains. http://audition.ens.fr/brette/papers/Brette2008NC.html\n\nThe implementation is based on the one provided by Brian.\nTo generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation:\n\\dfrac{dx}{dt} = \\dfrac{\\mu - x}{\\tau} + \\sigma \\, \\dfrac{\\xi }{\\sqrt{\\tau}}\nwhere \\xi is a random variable. Basically, x will randomly vary around $§ over time, with an amplitude determined by \\sigma and a speed determined by \\tau.\nThis doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process.\nTo avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates, the desired correlation strength corr and the time constant tau. See Brette’s paper for details.\nIn short, you should only define the parameters rates, corr and tau, and let the class compute mu and sigma for you. Changing rates, corr or tau after initialization automatically recomputes mu and sigma.\nExample:\nimport ANNarchy as ann\nann.setup(dt=0.1)\n\npop_poisson = ann.PoissonPopulation(200, rates=10.)\npop_corr    = ann.HomogeneousCorrelatedSpikeTrains(200, rates=10., corr=0.3, tau=10.)\n\nann.compile()\n\nann.simulate(1000.)\n\npop_poisson.rates=30.\npop_corr.rates=30.\n\nann.simulate(1000.)",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Setting inputs"
    ]
  },
  {
    "objectID": "manual/Inputs.html#inputs-to-a-rate-coded-network",
    "href": "manual/Inputs.html#inputs-to-a-rate-coded-network",
    "title": "Setting inputs",
    "section": "",
    "text": "The simplest way to define an input population is to use a dummy neuron which simply holds a firing rate r as parameter, and connect it to another population:\nimport ANNarchy as ann\n\ninput_pop = ann.Population(10, Neuron(parameters=\"r=0.0\"))\n\npop = ann.Population (10, LeakyIntegrator)\n\nproj = ann.Projection(input_pop, pop, 'exc')\nproj.connect_one_to_one(1.0)\n\nann.compile()\n\nann.simulate(100.)\n\ninput_pop.r = 1.0\n\nann.simulate(100.)\nThe only thing you need to do is to manipulate the numpy array r holded by input_pop, and it will influence the “real” population pop\nIt is important to define r as a parameter of the neuron, not a variable in equations. A variable sees its value updated at each step, so the value you set would be immediately forgotten.\n\nUsing this method necessitates to interact with the input population in the Python script everytime you want to change the inputs. If the inputs change every time step, your simulation will alternate between Python and C++ executions and potentially become very slow.\n\n\n\n\nIf the inputs change frequently, it may be more efficient to store all these values in a TimedArray (doc in the API).\nLet’s suppose you have a population of 10 neurons which should be activated sequentially over time. You can store the inputs to these neurons in a Numpy array, where the first axis corresponds to time and the second (or more) to the geometry of the population:\ninputs = np.array(\n    [\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n    ]\n)\n\ninp = ann.TimedArray(rates=inputs)\n\npop = ann.Population(10, Neuron(equations=\"r=sum(exc)\"))\n\nproj = ann.Projection(inp, pop, 'exc')\nproj.connect_one_to_one(1.0)\n\nann.compile()\n\nann.simulate(10.)\nWith this code, each neuron will be activated in sequence at each time step (dt=1.0 by default). If you simulate longer than 10 ms, the last input [0, 0, .., 1] will be kept forever.\nIf the rates array has two dimensions, the corresponding population will be 1D. You can pass a multidimensional array to obtain a 2D or 3D population.\nPresenting a input for only one time step is very short, especially if the population pop uses ODEs to integrate the inputs. You can provide a schedule parameter to the TimedArray to define how long (in ms) an input should be presented:\ninp = ann.TimedArray(rates=inputs, schedule=10.)\nHere each input will be kept constant for 10 ms, so the 10 inputs will need 100 ms of simulation to be presented. If you do not want a regular schedule, you can also provide a list of times where inputs should be set:\ninp = ann.TimedArray(rates=inputs, \n    schedule=[0., 10., 30., 60., 100., 150., 210., 280., 360., 450.])\nThe length of the schedule list should be equal or smaller to the number of inputs defined in rates. If this length is smaller (e.g. 7), only the 7 first inputs will be used as inputs. If the length is bigger, it will lead to an error.\nA TimedArray can be reset to iterate again over the inputs:\ninp = ann.TimedArray(rates=inputs, schedule=10.)\n\n...\n\nann.compile()\n\nann.simulate(100.) # The ten inputs are shown with a schedule of 10 ms\n\ninp.reset()\n\nann.simulate(100.) # The same ten inputs are presented again.\nThe times declared in schedule are therefore relative to the last call to reset() (or to t=0.0 at the beginning).\nIf you want to systematically iterate over the inputs without iterating over simulate() and reset(), you can provide the period argument to the TimedArray to define how often the inputs will be reset:\ninp = ann.TimedArray(rates=inputs, schedule=10.. period=100.)\n\n...\n\nann.simulate(100000.)\nIf the period is smaller than the total durations of the inputs, the last inputs will be skipped.\nThe rates, schedule and period can be modified after compilation. The only constraint is that the size of the population (defined in the rates array) must stay the same.\n\n\n\nImages\nA simple utility to directly load an image into the firing rates r of a Population is provided by the ImagePopulation class. This class is not automatically imported with ANNarchy, you need to explicitly import it:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.image import ImagePopulation\n\ninp = ImagePopulation(geometry=(480, 640))\ninp.set_image('image.jpg')\nUsing this class requires that you have the Python Image Library installed (pip install Pillow). Any image with a format supported by Pillow can be loaded, see the documentation.\nThe ImagePopulation must be initialized with a geometry corresponding to the desired size of the population. If it differs from the resolution of the image (set with set_image), the image will be first resized to match the geometry of the population.\n\n\n\n\n\n\nNote\n\n\n\nThe size of an image is defined as (height, width), so a 640x480 image should be loaded in a (480, 640) population.\n\n\nIf the geometry has only two dimensions (480, 640), each neuron will represent the luminance (or brightness) of the corresponding pixel.\nIf the geometry has three dimensions (480, 640, 3), the color channels will additionally be represented (RGB). Any other value than 3 for the third dimension will generate an error.\n\n\n\n\n\n\nNote\n\n\n\nThe firing rate r of a neuron is 1.0 when the corresponding pixel is white (value 255 as an unsigned integer on 8 bits).\n\n\nNote that the following code is functionally equivalent:\nimport ANNarchy as ann\nfrom PIL import Image\n\ninp = ann.Population(geometry=(480, 640), Neuron(parameters=\"r=0.0\"))\n\nimg = Image.open('image.jpg')\nimg = img.convert('L')\nimg = img.resize((480, 640)) /255.\n\ninp.r = np.array(img)\nAn example is provided in examples/image/Image.py.\nVideos\nThe VideoPopulation class allows to retrieve images from a Webcam, using the OpenCV computer vision library, version 4.0 or later. pkg-config opencv4 --cflags --libs should not return an error. vtk might have to be additionally installed.\nimport ANNarchy as ann\nfrom ANNarchy.extensions.image import VideoPopulation\n\ninp = VideoPopulation(geometry=(480, 640))\n\nann.compile()\n\ninp.start_camera(0)\n\nwhile(True):\n  inp.grab_image()\n  ann.simulate(10.0)\nA geometry must be provided as for ImagePopulations. The camera must be explicitly started after compile() with inp.start_camera(0). 0 corresponds to the index of your camera, change it if you have multiple cameras.\nThe VideoPopulation can then acquire frames from the camera with inp.grab_image() and store the correponding image in its firing rate r (also scaled between 0.0 and 1.0). An example is provided in examples/image/Webcam.py.\n\n\n\n\n\n\nWarning\n\n\n\nVideoPopulation is not available with the CUDA backend.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Setting inputs"
    ]
  },
  {
    "objectID": "manual/Inputs.html#inputs-to-a-spiking-network",
    "href": "manual/Inputs.html#inputs-to-a-spiking-network",
    "title": "Setting inputs",
    "section": "",
    "text": "To control the spiking patterns of a spiking population, the simplest way is to inject current into the corresponding membrane potentials. The built-in neuron types defined by ANNarchy have a i_offset variable that can be used for this purpose:\nimport numpy as np\nimport ANNarchy as ann\nann.setup(dt=0.1)\n\npop = ann.Population(100, Izhikevich)\n\npop.i_offset= np.linspace(0.0, 30.0, 100)\n\nm = ann.Monitor(pop, 'spike')\n\nann.compile()\n\nann.simulate(100.)\n\ndata = m.get('spike')\nt, n = m.raster_plot(data)\n\nimport matplotlib.pyplot as plt\nplt.plot(t, n, '.')\nplt.ylim(0, 100)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\nIf you want the injected current to be time-varying, you can design a rate-coded population of the same size as the spiking population and create a CurrentInjection projection between them:\ninp = ann.Population(100, Neuron(equations=\"r = sin(t)\"))\n\npop = ann.Population(100, Izhikevich)\n\nproj = ann.CurrentInjection(inp, pop, 'exc')\nproj.connect_current()\nThe current g_exc of a neuron in pop will be set at each time step to the firing rate r of the corresponding neuron in inp (i.e. with the same rank). inp can also be defined as a TimedArray.\nThe connector method should be connect_current(), which accepts no weight value and no delay.\n\n\n\n\nIf you want to control precisely the spiking patterns used as inputs, you can provide a list of spike times to a SpikeSourceArray object:\nimport ANNarchy as ann\nann.setup(dt=0.1)\n\nspike_times = [\n  [  10 + i/10,\n     20 + i/10,\n     30 + i/10,\n     40 + i/10,\n     50 + i/10,\n     60 + i/10,\n     70 + i/10,\n     80 + i/10,\n     90 + i/10] for i in range(100)\n]\n\npop = ann.SpikeSourceArray(spike_times=spike_times)\n\nm = ann.Monitor(pop, 'spike')\n\nann.compile()\n\nann.simulate(100.)\n\ndata = m.get('spike')\nt, n = m.raster_plot(data)\n\nimport matplotlib.pyplot as plt\nplt.plot(t, n, '.')\nplt.ylim(0, 100)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n\nThe spike_times argument must be a list of lists containing the spike times in ms. Its length defines the number of neurons in the population. It is not possible to define a geometry. If one neuron should not spike at all, just provide an empty list. The different neurons can have a different number of spikes.\nIf you want to repeat the same stimulation, you can reset the SpikeSourceArray, what will set its internal time back to 0.0:\nann.simulate(100.)\n\npop.reset()\n\nann.simulate(100.)\nThe spikes times can be changed after compilation, bit it must have the same number of neurons:\npop.spike_times = new_spike_times_array\nAn example is provided in examples/pyNN/IF_curr_alpha.py.\n\n\n\n\n\n\nWarning\n\n\n\nSpikeSourceArray is not available with the CUDA backend.\n\n\n\n\n\nThe PoissonPopulation class allows to create a population of identical spiking neurons, whose spiking patterns vary according to a Poisson distribution:\nimport ANNarchy as ann\nann.setup(dt=0.1)\n\npop = ann.PoissonPopulation(100, rates=30.)\n\nm = ann.Monitor(pop, 'spike')\n\nann.compile()\n\nann.simulate(100.)\n\ndata = m.get('spike')\nt, n = m.raster_plot(data)\n\nimport matplotlib.pyplot as plt\nplt.plot(t, n, '.')\nplt.ylim(0, 100)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\nIn this example, each of the 100 neurons fires randomly, with a mean firing rate of 30 Hz (next figure, top-left).\nIt is also possible to specify the mean firing rate individually for each neuron (next figure, top-right):\npop = PoissonPopulation(100, rates=np.linspace(0.0, 100.0, 100))\nThe rates attribute can be modified at any time during the simulation, as long as it has the same size as the population.\nAnother possibility is to define a rule for the evolution of the mean firing rate in the population (next figure, bottom-left):\npop = PoissonPopulation(\n        geometry=100,\n        parameters = \"\"\"\n            amp = 100.0\n            frequency = 50.0\n        \"\"\",\n        rates=\"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\"\n    )\nThe rule can only depend on the time t: the corresponding mean firing rate is the same for all neurons in the population.\nFinally, the rates argument can be replaced by a target, so it can be computed by another rate-coded population (next figure, bottom-right):\nrates = 10.*np.ones((2, 100))\nrates[0, :50] = 100.\nrates[1, 50:] = 100.\ninp = TimedArray(rates = rates, schedule=50.)\n\npop = PoissonPopulation(100, target=\"exc\")\n\nproj = Projection(inp, pop, 'exc')\nproj.connect_one_to_one(1.0)\nIn the code above, we define a TimedArray for 100 neurons, so that half of the neurons fire at 100 Hz, while the others fire at 10 Hz. Every 50 ms, the two halves are swapped.\nWe just need to create a projection with the target \"exc\" between the TimedArray and the PoissonPopulation (with a one-to-one pattern and weights 1.0 to preserve scaling), and the Poisson population will reflect the firing rates defined by the TimedArray.\n\n\n\n\nHomogeneousCorrelatedSpikeTrains defines spiking neurons following a homogeneous distribution with correlated spike trains.\nThe method describing the generation of homogeneous correlated spike trains is described in:\n\nBrette, R. (2009). Generation of correlated spike trains. http://audition.ens.fr/brette/papers/Brette2008NC.html\n\nThe implementation is based on the one provided by Brian.\nTo generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation:\n\\dfrac{dx}{dt} = \\dfrac{\\mu - x}{\\tau} + \\sigma \\, \\dfrac{\\xi }{\\sqrt{\\tau}}\nwhere \\xi is a random variable. Basically, x will randomly vary around $§ over time, with an amplitude determined by \\sigma and a speed determined by \\tau.\nThis doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process.\nTo avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates, the desired correlation strength corr and the time constant tau. See Brette’s paper for details.\nIn short, you should only define the parameters rates, corr and tau, and let the class compute mu and sigma for you. Changing rates, corr or tau after initialization automatically recomputes mu and sigma.\nExample:\nimport ANNarchy as ann\nann.setup(dt=0.1)\n\npop_poisson = ann.PoissonPopulation(200, rates=10.)\npop_corr    = ann.HomogeneousCorrelatedSpikeTrains(200, rates=10., corr=0.3, tau=10.)\n\nann.compile()\n\nann.simulate(1000.)\n\npop_poisson.rates=30.\npop_corr.rates=30.\n\nann.simulate(1000.)",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Setting inputs"
    ]
  },
  {
    "objectID": "manual/Projections.html",
    "href": "manual/Projections.html",
    "title": "Projections",
    "section": "",
    "text": "Once the populations are created, one can connect them by creating Projection instances:\nproj = ann.Projection(\n    pre = pop1,\n    post = pop2,\n    target = \"exc\",\n    synapse = BCM\n)\n\npre is either the name of the pre-synaptic population or the corresponding Population object.\npost is either the name of the post-synaptic population or the corresponding Population object.\ntarget is the type of the connection.\nsynapse is an optional argument requiring a Synapse instance.\n\nThe post-synaptic neuron type must use sum(exc) in the rate-coded case respectively g_exc in the spiking case, otherwise the projection will be useless.\nIf the synapse argument is omitted, the default synapse will be used:\n\nthe default rate-coded synapse defines psp = w * pre.r,\nthe default spiking synapse defines g_target += w.\n\n\n\n\nCreating the Projection objects only defines the information that two populations are connected. The synapses must be explicitely created by applying a connector method on the Projection object.\nTo this end, ANNarchy already provides a set of predefined connector methods, but the user has also the possibility to define his own (see the following page Connectivity).\nThe connection pattern should be applied right after the creation of the Projection:\nproj = Projection(\n    pre = pop1,\n    post = pop2,\n    target = \"exc\",\n    synapse = BCM\n)\nproj.connect_all_to_all( weights = 1.0 )\nThe connector method must be called before the network is compiled.\n\n\n\nLet’s suppose the BCM synapse is used to create the Projection proj (spiking synapses are accessed similarly):\nBCM = ann.Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100. : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0\n    \"\"\"\n)\n\n\nThe global parameters and variables of a projection (i.e. defined with the postsynaptic or projection flags) can be accessed directly through attributes. Attributes defined with projection have a single value for the whole population:\n&gt;&gt;&gt; proj.tau\n100\nAttributes defined with postsynaptic have one value per post-synaptic neuron, so the result is a vector:\n&gt;&gt;&gt; proj.theta\n[3.575, 15.987, ... , 4.620]\nPost-synaptic variables can be modified by passing:\n\na single value, which will be the same for all post-synaptic neurons.\na list of values, with the same size as the number of neurons receiving synapses (for some sparse connectivity patterns, it may not be the same as the size of the population, so no multidimensional array is accepted).\n\nAfter compilation (and therefore creation of the synapses), you can access how many post-synaptic neurons receive actual synapses with:\n&gt;&gt;&gt; proj.size\n4\nThe list of ranks of the post-synaptic neurons receiving synapses is obtained with:\n&gt;&gt;&gt; proj.post_ranks\n[0, 1, 2, 3]\n\n\n\nAt the projection level\nLocal attributes can also be accessed globally through attributes. It will return a list of lists containing the synapse-specific values.\nThe first index represents the post-synaptic neurons. It has the same length as proj.post_ranks. Beware that if some post-synaptic neurons do not receive any connection, this index will not correspond to the ranks of the post-synaptic population.\nThe second index addresses the pre-synaptic neurons. If the connection is sparse, it also is unrelated to the ranks of the pre-synaptic neurons in their populations.\n\n\n\n\n\n\nWarning\n\n\n\nModifying these lists of lists is error-prone, so this method should be avoided if possible.\n\n\nAt the post-synaptic level\nThe local parameters and variables of a projection (synapse-specific) should better be accessed through the Dendrite object, which gathers for a single post-synaptic neuron all synapses belonging to the projection.\nBeware: As projections are only instantiated after the call to compile(), local attributes of a Projection are only available then. Trying to access them before compilation will lead to an error!\nEach dendrite stores the parameters and variables of the corresponding synapses as attributes, as populations do for neurons. You can loop over all post-synaptic neurons receiving synapses with the dendrites iterator:\nfor dendrite in proj.dendrites:\n    print dendrite.pre_ranks\n    print dendrite.size\n    print dendrite.tau\n    print dendrite.alpha\n    print dendrite.w\ndendrite.pre_ranks returns a list of pre-synaptic neuron ranks. dendrite.size returns the number of synapses for the considered post-synaptic neuron. Global parameters/variables return a single value (dendrite.tau) and local ones return a list (dendrite.w).\nYou can even omit the .dendrites part of the iterator:\nfor dendrite in proj:\n    print(dendrite.pre_ranks)\n    print(dendrite.size)\n    print(dendrite.tau)\n    print(dendrite.alpha)\n    print(dendrite.w)\nYou can also access the dendrites individually, either by specifying the rank of the post-synaptic neuron:\ndendrite = proj.dendrite(13)\nprint(dendrite.w)\nor its coordinates:\ndendrite = proj.dendrite(5, 5)\nprint(dendrite.w)\nWhen using ranks, you can also directly address the projection as an array:\ndendrite = proj[13]\nprint(dendrite.w)\n\n\n\n\n\n\nWarning\n\n\n\nYou should make sure that the dendrite actually exists before accessing it through its rank, because it is otherwise a None object.\n\n\n\n\n\nIf you have defined a function inside a Synapse definition:\nBCM = ann.Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100. : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic\n        dw/dt = eta * BCMRule(pre.r, post.r, theta) : min=0.0\n    \"\"\",\n    functions = \"\"\"\n        BCMRule(pre, post, theta) = post * (post - theta) * pre\n    \"\"\"\n)\nyou can use this function in Python as if it were a method of the corresponding object:\nproj = ann.Projection(pop1, pop2, 'exc', BCM).connect_xxx()\n\npre = np.linspace(0., 1., 100)\npost = np.linspace(0., 1., 100)\ntheta = 0.01 * np.ones(100)\n\nweight_change = proj.BCMRule(pre, post, theta)\nYou can pass either a list or a 1D Numpy array to each argument (not a single value, nor a multidimensional array!).\nThe size of the arrays passed for each argument is arbitrary (it must not match the projection’s size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.\n\n\n\n\nProjections are usually understood as a connectivity pattern between two populations. Complex connectivity patterns have to specifically designed (see Connectivity).\nIn some cases, it can be much simpler to connect subsets of neurons directly, using built-in connector methods. To this end, the Projection object also accepts PopulationView objects for the pre and post arguments.\nLet’s suppose we want to connect the (8,8) populations pop1 and pop2 in a all-to-all manner, but only for the (4,4) neurons in the center of these populations. The first step is to create the PopulationView objects using the slice operator:\npop1_center = pop1[2:7, 2:7]\npop2_center = pop2[2:7, 2:7]\nThey can then be simply used to create a projection:\nproj = ann.Projection(\n    pre = pop1_center,\n    post = pop2_center,\n    target = \"exc\",\n    synapse = BCM)\nproj.connect_all_to_all( weights = 1.0 )\nEach neuron of pop2_center will receive synapses from all neurons of pop1_center, and only them. Neurons of pop2 which are not in pop2_center will not receive any synapse.\n\n\n\n\n\n\nWarning\n\n\n\nIf you define your own connector method and want to use PopulationViews, you will need to iterate over the ranks attribute of the PopulationView object.\n\n\n\n\n\nBy default, synaptic transmission is considered to be instantaneous (or more precisely, it takes one simulation step (dt) for a newly computed firing rate to be taken into account by post-synaptic neurons).\nIn order to take longer propagation times into account in the transmission of information between two populations, one has the possibility to define synaptic delays for a projection. All the built-in connector methods take an argument delays (default=dt), which can be a float (in milliseconds) or a random number generator.\nproj.connect_all_to_all( weights = 1.0, delays = 10.0)\nproj.connect_all_to_all( weights = 1.0, delays = Uniform(1.0, 10.0))\nIf the delay is not a multiple of the simulation time step (dt = 1.0 by default), it will be rounded to the closest multiple. The same is true for the values returned by a random number generator.\nNote: Per design, the minimal possible delay is equal to dt: values smaller than dt will be replaced by dt. Negative values do not make any sense and are ignored.\n\n\n\n\n\n\nWarning\n\n\n\nNon-uniform delays are not available on CUDA.\n\n\n\n\n\nSynaptic transmission, update and plasticity\nIt is possible to selectively control synaptic transmission and plasticity at the projection level. The boolean flags transmission, update and plasticity can be set for that purpose:\nproj.transmission = False\nproj.update = False\nproj.plasticity = False\n\nIf transmission is False, the projection is totally shut down: it does not transmit any information to the post-synaptic population (the corresponding weighted sums or conductances are constantly 0) and all synaptic variables are frozen to their current value (including the synaptic weights w).\nIf update is False, synaptic transmission occurs normally, but the synaptic variables are not updated. For spiking synapses, this includes traces when they are computed at each step, but not when they are integrated in an event-driven manner (flag event-driven). Beware: continous synaptic transmission as in NMDA synapses will not work in this mode, as internal variables are not updated.\nIf only plasticity is False, synaptic transmission and synaptic variable updates occur normally, but changes to the synaptic weight w are ignored.\n\nDisabling learning\nAlternatively, one can use the enable_learning() and disable_learning() methods of Projection. The effect of disable_learning() depends on the type of the projection:\n\nfor rate-coded projections, disable_learning() is equivalent to update=False: no synaptic variables is updated.\nfor spiking projections, it is equivalent to plasticity=False: only the weights are blocked.\n\nThe reason of this difference is to allow continuous synaptic transmission and computation of traces. Calling enable_learning() without arguments resumes the default learning behaviour.\nPeriodic learning\nenable_learning() also accepts two arguments period and offset. period defines the interval in ms between two evaluations of the synaptic variables. This can be useful when learning should only occur once at the end of a trial. It is recommended not to use ODEs in the equations in this case, as they are numerized according to a fixed time step. offset defines the time inside the period at which the evaluation should occur. By default, it is 0, so the variable updates will occur at the next step, then after period ms, and so on. Setting it to -1 will shift the update at the end of the period.\nNote that spiking synapses using online evaluation will not be affected by these parameters, as they are event-driven.\n\n\n\nFor spiking neurons, it may be desirable that a single synapses activates different currents (or conductances) in the post-synaptic neuron. One example are AMPA/NMDA synapses, where a single spike generates a “classical” AMPA current, plus a voltage-gated slower NMDA current. The following conductance-based Izhikevich is an example:\nRSNeuron = ann.Neuron(\n    parameters = \"\"\"\n        a = 0.02\n        b = 0.2\n        c = -65.\n        d = 8.\n        tau_ampa = 5.\n        tau_nmda = 150.\n        vrev = 0.0\n    \"\"\" ,\n    equations=\"\"\"\n        I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)\n        dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init=-65., midpoint\n        du/dt = a * (b*v - u) : init=-13.\n        tau_ampa * dg_ampa/dt = -g_ampa\n        tau_nmda * dg_nmda/dt = -g_nmda\n    \"\"\" ,\n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\",\n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\"\n)\nHowever, g_ampa and g_nmda collect by default spikes from different projections, so the weights will not be shared between the \"ampa\" projection and the \"nmda\" one. It is therefore possible to specify a list of targets when building a projection, meaning that a single pre-synaptic spike will increase both g_ampa and g_nmda from the same weight:\nproj = Projection(pop1, pop2, ['ampa', 'nmda'], STDP)\nAn example is provided in /notebooks/Ramp.ipynb.\n\n\n\n\n\n\nWarning\n\n\n\nMultiple targets are not available on CUDA yet.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#declaring-the-projections",
    "href": "manual/Projections.html#declaring-the-projections",
    "title": "Projections",
    "section": "",
    "text": "Once the populations are created, one can connect them by creating Projection instances:\nproj = ann.Projection(\n    pre = pop1,\n    post = pop2,\n    target = \"exc\",\n    synapse = BCM\n)\n\npre is either the name of the pre-synaptic population or the corresponding Population object.\npost is either the name of the post-synaptic population or the corresponding Population object.\ntarget is the type of the connection.\nsynapse is an optional argument requiring a Synapse instance.\n\nThe post-synaptic neuron type must use sum(exc) in the rate-coded case respectively g_exc in the spiking case, otherwise the projection will be useless.\nIf the synapse argument is omitted, the default synapse will be used:\n\nthe default rate-coded synapse defines psp = w * pre.r,\nthe default spiking synapse defines g_target += w.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#building-the-projections",
    "href": "manual/Projections.html#building-the-projections",
    "title": "Projections",
    "section": "",
    "text": "Creating the Projection objects only defines the information that two populations are connected. The synapses must be explicitely created by applying a connector method on the Projection object.\nTo this end, ANNarchy already provides a set of predefined connector methods, but the user has also the possibility to define his own (see the following page Connectivity).\nThe connection pattern should be applied right after the creation of the Projection:\nproj = Projection(\n    pre = pop1,\n    post = pop2,\n    target = \"exc\",\n    synapse = BCM\n)\nproj.connect_all_to_all( weights = 1.0 )\nThe connector method must be called before the network is compiled.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#projection-attributes",
    "href": "manual/Projections.html#projection-attributes",
    "title": "Projections",
    "section": "",
    "text": "Let’s suppose the BCM synapse is used to create the Projection proj (spiking synapses are accessed similarly):\nBCM = ann.Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100. : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0\n    \"\"\"\n)\n\n\nThe global parameters and variables of a projection (i.e. defined with the postsynaptic or projection flags) can be accessed directly through attributes. Attributes defined with projection have a single value for the whole population:\n&gt;&gt;&gt; proj.tau\n100\nAttributes defined with postsynaptic have one value per post-synaptic neuron, so the result is a vector:\n&gt;&gt;&gt; proj.theta\n[3.575, 15.987, ... , 4.620]\nPost-synaptic variables can be modified by passing:\n\na single value, which will be the same for all post-synaptic neurons.\na list of values, with the same size as the number of neurons receiving synapses (for some sparse connectivity patterns, it may not be the same as the size of the population, so no multidimensional array is accepted).\n\nAfter compilation (and therefore creation of the synapses), you can access how many post-synaptic neurons receive actual synapses with:\n&gt;&gt;&gt; proj.size\n4\nThe list of ranks of the post-synaptic neurons receiving synapses is obtained with:\n&gt;&gt;&gt; proj.post_ranks\n[0, 1, 2, 3]\n\n\n\nAt the projection level\nLocal attributes can also be accessed globally through attributes. It will return a list of lists containing the synapse-specific values.\nThe first index represents the post-synaptic neurons. It has the same length as proj.post_ranks. Beware that if some post-synaptic neurons do not receive any connection, this index will not correspond to the ranks of the post-synaptic population.\nThe second index addresses the pre-synaptic neurons. If the connection is sparse, it also is unrelated to the ranks of the pre-synaptic neurons in their populations.\n\n\n\n\n\n\nWarning\n\n\n\nModifying these lists of lists is error-prone, so this method should be avoided if possible.\n\n\nAt the post-synaptic level\nThe local parameters and variables of a projection (synapse-specific) should better be accessed through the Dendrite object, which gathers for a single post-synaptic neuron all synapses belonging to the projection.\nBeware: As projections are only instantiated after the call to compile(), local attributes of a Projection are only available then. Trying to access them before compilation will lead to an error!\nEach dendrite stores the parameters and variables of the corresponding synapses as attributes, as populations do for neurons. You can loop over all post-synaptic neurons receiving synapses with the dendrites iterator:\nfor dendrite in proj.dendrites:\n    print dendrite.pre_ranks\n    print dendrite.size\n    print dendrite.tau\n    print dendrite.alpha\n    print dendrite.w\ndendrite.pre_ranks returns a list of pre-synaptic neuron ranks. dendrite.size returns the number of synapses for the considered post-synaptic neuron. Global parameters/variables return a single value (dendrite.tau) and local ones return a list (dendrite.w).\nYou can even omit the .dendrites part of the iterator:\nfor dendrite in proj:\n    print(dendrite.pre_ranks)\n    print(dendrite.size)\n    print(dendrite.tau)\n    print(dendrite.alpha)\n    print(dendrite.w)\nYou can also access the dendrites individually, either by specifying the rank of the post-synaptic neuron:\ndendrite = proj.dendrite(13)\nprint(dendrite.w)\nor its coordinates:\ndendrite = proj.dendrite(5, 5)\nprint(dendrite.w)\nWhen using ranks, you can also directly address the projection as an array:\ndendrite = proj[13]\nprint(dendrite.w)\n\n\n\n\n\n\nWarning\n\n\n\nYou should make sure that the dendrite actually exists before accessing it through its rank, because it is otherwise a None object.\n\n\n\n\n\nIf you have defined a function inside a Synapse definition:\nBCM = ann.Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100. : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic\n        dw/dt = eta * BCMRule(pre.r, post.r, theta) : min=0.0\n    \"\"\",\n    functions = \"\"\"\n        BCMRule(pre, post, theta) = post * (post - theta) * pre\n    \"\"\"\n)\nyou can use this function in Python as if it were a method of the corresponding object:\nproj = ann.Projection(pop1, pop2, 'exc', BCM).connect_xxx()\n\npre = np.linspace(0., 1., 100)\npost = np.linspace(0., 1., 100)\ntheta = 0.01 * np.ones(100)\n\nweight_change = proj.BCMRule(pre, post, theta)\nYou can pass either a list or a 1D Numpy array to each argument (not a single value, nor a multidimensional array!).\nThe size of the arrays passed for each argument is arbitrary (it must not match the projection’s size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#connecting-population-views",
    "href": "manual/Projections.html#connecting-population-views",
    "title": "Projections",
    "section": "",
    "text": "Projections are usually understood as a connectivity pattern between two populations. Complex connectivity patterns have to specifically designed (see Connectivity).\nIn some cases, it can be much simpler to connect subsets of neurons directly, using built-in connector methods. To this end, the Projection object also accepts PopulationView objects for the pre and post arguments.\nLet’s suppose we want to connect the (8,8) populations pop1 and pop2 in a all-to-all manner, but only for the (4,4) neurons in the center of these populations. The first step is to create the PopulationView objects using the slice operator:\npop1_center = pop1[2:7, 2:7]\npop2_center = pop2[2:7, 2:7]\nThey can then be simply used to create a projection:\nproj = ann.Projection(\n    pre = pop1_center,\n    post = pop2_center,\n    target = \"exc\",\n    synapse = BCM)\nproj.connect_all_to_all( weights = 1.0 )\nEach neuron of pop2_center will receive synapses from all neurons of pop1_center, and only them. Neurons of pop2 which are not in pop2_center will not receive any synapse.\n\n\n\n\n\n\nWarning\n\n\n\nIf you define your own connector method and want to use PopulationViews, you will need to iterate over the ranks attribute of the PopulationView object.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#specifying-delays-in-synaptic-transmission",
    "href": "manual/Projections.html#specifying-delays-in-synaptic-transmission",
    "title": "Projections",
    "section": "",
    "text": "By default, synaptic transmission is considered to be instantaneous (or more precisely, it takes one simulation step (dt) for a newly computed firing rate to be taken into account by post-synaptic neurons).\nIn order to take longer propagation times into account in the transmission of information between two populations, one has the possibility to define synaptic delays for a projection. All the built-in connector methods take an argument delays (default=dt), which can be a float (in milliseconds) or a random number generator.\nproj.connect_all_to_all( weights = 1.0, delays = 10.0)\nproj.connect_all_to_all( weights = 1.0, delays = Uniform(1.0, 10.0))\nIf the delay is not a multiple of the simulation time step (dt = 1.0 by default), it will be rounded to the closest multiple. The same is true for the values returned by a random number generator.\nNote: Per design, the minimal possible delay is equal to dt: values smaller than dt will be replaced by dt. Negative values do not make any sense and are ignored.\n\n\n\n\n\n\nWarning\n\n\n\nNon-uniform delays are not available on CUDA.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#controlling-projections",
    "href": "manual/Projections.html#controlling-projections",
    "title": "Projections",
    "section": "",
    "text": "Synaptic transmission, update and plasticity\nIt is possible to selectively control synaptic transmission and plasticity at the projection level. The boolean flags transmission, update and plasticity can be set for that purpose:\nproj.transmission = False\nproj.update = False\nproj.plasticity = False\n\nIf transmission is False, the projection is totally shut down: it does not transmit any information to the post-synaptic population (the corresponding weighted sums or conductances are constantly 0) and all synaptic variables are frozen to their current value (including the synaptic weights w).\nIf update is False, synaptic transmission occurs normally, but the synaptic variables are not updated. For spiking synapses, this includes traces when they are computed at each step, but not when they are integrated in an event-driven manner (flag event-driven). Beware: continous synaptic transmission as in NMDA synapses will not work in this mode, as internal variables are not updated.\nIf only plasticity is False, synaptic transmission and synaptic variable updates occur normally, but changes to the synaptic weight w are ignored.\n\nDisabling learning\nAlternatively, one can use the enable_learning() and disable_learning() methods of Projection. The effect of disable_learning() depends on the type of the projection:\n\nfor rate-coded projections, disable_learning() is equivalent to update=False: no synaptic variables is updated.\nfor spiking projections, it is equivalent to plasticity=False: only the weights are blocked.\n\nThe reason of this difference is to allow continuous synaptic transmission and computation of traces. Calling enable_learning() without arguments resumes the default learning behaviour.\nPeriodic learning\nenable_learning() also accepts two arguments period and offset. period defines the interval in ms between two evaluations of the synaptic variables. This can be useful when learning should only occur once at the end of a trial. It is recommended not to use ODEs in the equations in this case, as they are numerized according to a fixed time step. offset defines the time inside the period at which the evaluation should occur. By default, it is 0, so the variable updates will occur at the next step, then after period ms, and so on. Setting it to -1 will shift the update at the end of the period.\nNote that spiking synapses using online evaluation will not be affected by these parameters, as they are event-driven.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Projections.html#multiple-targets",
    "href": "manual/Projections.html#multiple-targets",
    "title": "Projections",
    "section": "",
    "text": "For spiking neurons, it may be desirable that a single synapses activates different currents (or conductances) in the post-synaptic neuron. One example are AMPA/NMDA synapses, where a single spike generates a “classical” AMPA current, plus a voltage-gated slower NMDA current. The following conductance-based Izhikevich is an example:\nRSNeuron = ann.Neuron(\n    parameters = \"\"\"\n        a = 0.02\n        b = 0.2\n        c = -65.\n        d = 8.\n        tau_ampa = 5.\n        tau_nmda = 150.\n        vrev = 0.0\n    \"\"\" ,\n    equations=\"\"\"\n        I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)\n        dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init=-65., midpoint\n        du/dt = a * (b*v - u) : init=-13.\n        tau_ampa * dg_ampa/dt = -g_ampa\n        tau_nmda * dg_nmda/dt = -g_nmda\n    \"\"\" ,\n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\",\n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\"\n)\nHowever, g_ampa and g_nmda collect by default spikes from different projections, so the weights will not be shared between the \"ampa\" projection and the \"nmda\" one. It is therefore possible to specify a list of targets when building a projection, meaning that a single pre-synaptic spike will increase both g_ampa and g_nmda from the same weight:\nproj = Projection(pop1, pop2, ['ampa', 'nmda'], STDP)\nAn example is provided in /notebooks/Ramp.ipynb.\n\n\n\n\n\n\nWarning\n\n\n\nMultiple targets are not available on CUDA yet.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Projections"
    ]
  },
  {
    "objectID": "manual/Connector.html",
    "href": "manual/Connector.html",
    "title": "Connectivity",
    "section": "",
    "text": "There are basically four methods to instantiate projections:\n\nBy using a built-in connector method.\nBy using a saved projection.\nBy loading dense or sparse matrices.\nBy defining a custom connector method.\n\n\n\nFor further detailed information about these connectors, please refer to the library reference Projections.\n\n\nAll neurons of the post-synaptic population form connections with all neurons of the pre-synaptic population (dense connectivity). Self-connections are avoided by default, but the parameter allow_self_connections can be set to True:\nproj.connect_all_to_all(weights=1.0, delays=2.0, allow_self_connections=False) \nThe weights and delays arguments accept both single float values (all synapses will take this initial value), as well as random objects allowing to randomly select the initial values for different synapses:\nproj.connect_all_to_all(weights=Uniform(0.0, 0.5)) \n\n\n\nA neuron of the post-synaptic population forms a connection with only one neuron of the pre-synaptic population. The order depends on the ranks: neuron 0 is connected with neuron 0 and so on. It is advised that the pre- and post-populations have the same size/geometry, especially when using population views.\npop1 = Population((20, 20), Neuron(parameters=\"r=0.0\"))\npop2 = Population((10, 10), Neuron(equations=\"r=sum(exc)\"))\n\nproj = Projection(pop1[5:15, 5:15], pop2, 'exc')\nproj.connect_one_to_one(weights=1.0) \nWeights and delays also accept random distributions.\nBelow is a graphical representation of the difference between all_to_all and one_to_one:\n\n\n\n\nA neuron of the post-synaptic population forms a connection with a limited region of the pre-synaptic population, centered around the neuron with the same normalized position. Weight values are initialized using a Gaussian function, with a maximal value amp for the neuron of same position and decreasing with distance (standard deviation sigma):\n\nw(x, y) = A \\, \\exp(-\\dfrac{1}{2}\\dfrac{(x-x_c)^2+(y-y_c)^2}{\\sigma^2})\n\nwhere (x, y) is the position of the pre-synaptic neuron (normalized to [0, 1]^d) and (x_c, y_c) is the position of the post-synaptic neuron (normalized to [0, 1]^d). A = amp, sigma = \\sigma.\nIn order to void creating useless synapses, the parameter limit can be set to restrict the creation of synapses to the cases where the value of the weight would be superior to limit*abs(amp). Default is 0.01 (1%).\nSelf-connections are avoided by default (parameter allow_self_connections).\nThe two populations must have the same number of dimensions, but the number of neurons can vary as the positions of each neuron are normalized in [0, 1]^d:\nproj.connect_gaussian( amp=1.0, sigma=0.2, limit=0.001) \n\n\n\nThe same as connect_gaussian, except weight values are computed using a Difference-of-Gaussians (DoG), usually positive in the center, negative a bit further away and small at long distances.\n\n\\begin{aligned}\nw(x, y) &= A^+ \\, \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_+^2}) \\\\\n    &-  A^- \\, \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_-^2}) \\\\\n\\end{aligned}\n\nWeights smaller than limit * abs(amp_pos - amp_neg) are not created and self-connections are avoided by default (parameter allow_self_connections):\nproj.connect_dog(\n    amp_pos=1.0, sigma_pos=0.2, \n    amp_neg=0.3, sigma_neg=0.7, \n    limit=0.001\n) \nThe following figure shows the example of a neuron of coordinates (10, 10) in the post-synaptic population, which is connected through the gaussian (left) and dog (right) projections to a population of geometry 30*30. The X and Y axis denote the coordinates of the pre-synaptic neurons, while the Z axis is the weight value.\n\n\n\n\nEach neuron in the post-synaptic population receives connections from a fixed number of neurons of the pre-synaptic population chosen randomly. It may happen that two post-synaptic neurons are connected to the same pre-synaptic neuron and that some pre-synaptic neurons are connected to nothing:\nproj.connect_fixed_number_pre(number = 20, weights=1.0) \nweights and delays can also take a random object.\n\n\n\nEach neuron in the pre-synaptic population sends a connection to a fixed number of neurons of the post-synaptic population chosen randomly. It may happen that two pre-synaptic neurons are connected to the same post-synaptic neuron and that some post-synaptic neurons receive no connection at all:\nproj.connect_fixed_number_post(number = 20, weights=1.0) \nThe following figure shows the fixed_number_pre (left) and fixed_number_post projections between two populations of 4 neurons, with number=2. In fixed_number_pre, each post-synaptic neuron receives exactly 2 connections, while in fixed_number_post, each pre-synaptic neuron send exactly two connections:\n\n\n\n\nFor each post-synaptic neuron, there is a fixed probability that it forms a connection with a neuron of the pre-synaptic population. It is basically a all_to_all projection, except some synapses are not created, making the projection sparser:\nproj.connect_fixed_probability(probability = 0.2, weights=1.0) \n\n\n\n\n\n\nImportant\n\n\n\nIf a single value is used for the weights argument of connect_all_to_all, connect_one_to_one, connect_fixed_probability, connect_fixed_number_pre and connect_fixed_number_post, and the default synapse is used (no synaptic plasticity), ANNarchy will generate a single weight value for all the synapses of the projection, not one per synapse.\nThis allows to save a lot of memory and improve performance. However, if you wish to manually change the weights of some of the synapses after the creation, you need to force the creation of one value per synapse by setting force_multiple_weights=True in the call to the connector.\n\n\n\n\n\n\nIt is also possible to build a connection pattern using data saved during a precedent simulation. This is useful when:\n\npre-learning is done in another context;\na connector method for static synapses is particularly slow (e.g. DoG), but loading the result from a file is faster.\n\nThe connectivity of a projection can be saved (after compile()) using:\nproj.save_connectivity(filename='proj.npz')\nThe filename can used relative or absolute paths. The data is saved in a binary format:\n\nCompressed Numpy format when the filename ends with .npz.\nCompressed binary file format when the filename ends with .gz.\nBinary file format otherwise.\n\nIt can then be used to instantiate another projection:\nproj.connect_from_file(filename='proj.npz')\nOnly the connectivity (which neurons are connected), the weights and delays are loaded. Other synaptic variables are left untouched. The pre- and post-synaptic population must have the same size during saving and loading.\n\n\n\nOne can also create connections using Numpy dense matrices or Scipy sparse matrices.\n\n\nThis method accepts a Numpy array to define the weights of the projection (and optionally the delays). By default, the matrix should have the size (post.size, pre.size), so that the first index represents a post-synaptic neuron and the second the pre-synaptic neurons. If your matrix is defined in the reversed order, you can either transpose it or set the pre_post argument to True.\nThis method is useful for dense connectivity matrices (all-to-all). If you do not want to create some synapses, the weight value should be set to None.\nThe following code creates a synfire chain inside a population of 100 neurons:\nN = 100\nproj = ann.Projection(pop, pop, 'exc')\n\n# Initialize an empty connectivity matrix\nw = np.array([[None]*N]*N)\n\n# Connect each post-synaptic neuron to its predecessor\nfor i in range(N):\n    w[i, (i-1)%N] = 1.0\n\n# Create the connections\nproj.connect_from_matrix(w)\nConnectivity matrices can not work with multi-dimensional coordinates, only ranks are used. Population views can be used in the projection, but the connection matrix must have the corresponding size:\nproj = ann.Projection(pop[10:20], pop[50:60], 'exc')\n\n# Create the connectivity matrix\nw = np.ones((10, 10))\n\n# Create the connections\nproj.connect_from_matrix(w)\n\n\n\nFor sparse connection matrices, the Numpy array format may have a huge memory overhead if most of its values are None. It is possible to use Scipy sparse matrices in that case. The previous synfire chain example becomes:\nfrom scipy.sparse import lil_matrix\n\nproj = ann.Projection(pop, pop, 'exc')\n\nw = lil_matrix((N, N))\nfor i in range(N):\n    w[i, (i+1)%N] = 1.0\n\nproj.connect_from_sparse(w)\n\n\n\n\n\n\nNote\n\n\n\nContrary to connect_from_matrix(), the first index of the sparse matrix represents the pre-synaptic neurons, not the post-synaptic ones. This is for compatibility with other neural simulators.\n\n\nconnect_from_sparse() accepts lil_matrix, csr_matrix and csc_matrix objects, although lil_matrix should be preferred for its simplicity of element access.\n\n\n\n\nThis section describes the creation of user-specific connection patterns in ANNarchy, if the available patterns are not enough. A connection pattern is simply implemented as a method returning a LILConnectivity (list-of-list) object containing all the necessary information to create the synapses.\nA connector method must take on the first position the pre-synaptic population (or a subset of it) and on the second one the post-synaptic population. Other arguments are free, but should be passed when creating the projection.\nprobabilistic_pattern(pre, post, &lt;other arguments&gt;)\nAs an example, we will recreate the fixed_probability connector method, building synapses with a given probability. For this new pattern we need a weight value (common for all synapses) and a probability value as additional arguments. We consider that no delay is introduced in the synaptic transmission.\ndef probabilistic_pattern(pre, post, weight, probability):\n\n    synapses = ann.LILConnectivity()\n\n    ... pattern code comes here ...\n\n    return synapses\n\n\nThe connector method needs to return a LILConnectivity object storing the connectivity. For each post-synaptic neuron receiving synapses, a list of pre-synaptic ranks, weight values and delays must be added to the structure. If you use 2D or 3D populations you need to transform the coordinates into ranks with the rank_from_coordinates function.\nimport random\nimport ANNarchy as ann\n\ndef probabilistic_pattern(pre, post, weight, probability):\n    # Create a LIL structure for the connectivity matrix\n    synapses = ann.LILConnectivity()\n    # For all neurons in the post-synaptic population\n    for post_rank in xrange(post.size):\n        # Decide which pre-synaptic neurons should form synapses\n        ranks = []\n        for pre_rank in xrange(pre.size):\n            if random.random() &lt; probability:\n                ranks.append(pre_rank)\n        # Create weights and delays arrays of the same size\n        values = [weight for i in xrange(len(ranks)) ]\n        delays = [0 for i in xrange(len(ranks)) ]\n        # Add this information to the LIL matrix\n        synapses.add(post_rank, ranks, values, delays)\n\n    return synapses\nThe first for - loop iterates over all post-synaptic neurons in the projection. The inner for loop decides for each of these neurons if a synapse with a pre-synaptic neuron should be created, based on the value probability provided as argument to the function.\nThe lists values and delays are then created with the same size as ranks (important!), and filled with the desired value. All this information is then fed into the LIL matrix using the add(post_rank, ranks, values, delays) method.\n\n\n\n\n\n\nNote\n\n\n\nBuilding such connectivity matrices in Python can be extremely slow, as Python is not made for tight nested loops. If the construction of your network lasts too long, you should definitely write this function in Cython.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe add() should be only called once per post-synaptic neuron! If not, ANNarchy will have to reorder its internal representations and this will be really slow.\n\n\nUsage of the pattern\nTo use the pattern within a projection you provide the pattern method to the connect_with_func method of Projection\nproj = ann.Projection(\n    pre = pop1, \n    post = pop2, \n    target = 'inh' \n)\n\nproj.connect_with_func(\n    method=probabilistic_pattern, \n    weight=1.0, \n    probability=0.3\n)   \nmethod is the method you just wrote. Extra arguments (other than pre and post) should be passed with the same name.\n\n\n\nFor this example, we will create a Cython file CustomPatterns.pyx in the same directory as the script. Its content should be relatively similar to the Python version, except some type definitions:\n# distutils: language = c++\nimport random\nimport ANNarchy\ncimport ANNarchy.cython_ext.Connector as Connector\n\ndef probabilistic_pattern(pre, post, weight, probability):\n    # Typedefs\n    cdef Connector.LILConnectivity synapses\n    cdef int post_rank, pre_rank\n    cdef list ranks, values, delays\n\n    # Create a LILConnectivity structure for the connectivity matrix\n    synapses = Connector.LILConnectivity()\n    # For all neurons in the post-synaptic population\n    for post_rank in xrange(post.size):\n        # Decide which pre-synaptic neurons should form synapses\n        ranks = []\n        for pre_rank in xrange(pre.size):\n            if random.random() &lt; probability:\n                ranks.append(pre_rank)\n        # Create weights and delays arrays of the same size\n        values = [weight for i in xrange(len(ranks)) ]\n        delays = [0 for i in xrange(len(ranks)) ]\n        # Add this information to the LILConnectivity matrix\n        synapses.add(post_rank, ranks, values, delays)\n\n    return synapses\nThe only differences with the Python code are:\n\nThe module Connector where the LILConnectivity connection matrix class is defined should be cimported with:\n\ncimport ANNarchy.cython_ext.Connector as Connector\n\nData structures should be declared with cdef at the beginning of the method:\n\n# Typedefs\ncdef Connector.LILConnectivity synapses\ncdef int post_rank, pre_rank\ncdef list ranks, values, delays \nTo allow Cython to compile this file, we also need to provide with a kind of \"Makefile\" specifying that the code should be generated in C++, not C. This file should have the same name as the Cython file but end with .pyxbld, here : CustomPatterns.pyxbld.\nfrom distutils.extension import Extension\nimport ANNarchy\n\ndef make_ext(modname, pyxfilename):\n    return Extension(name=modname,\n                     sources=[pyxfilename],\n                     include_dirs = ANNarchy.include_path(),\n                     extra_compile_args=['-std=c++11'],\n                     language=\"c++\")\n\n\n\n\n\n\nNote\n\n\n\nThis .pyxbld is generic, you don’t need to modify anything, except its name.\n\n\nNow you can import the method probabilistic_pattern() into your Python code using the pyximport module of Cython and build the Projection normally:\nimport pyximport; pyximport.install()\nfrom CustomPatterns import probabilistic_pattern\n\nproj.connect_with_func(method=probabilistic_pattern, weight=1.0, probability=0.3)\nWriting the connector in Cython can bring speedups up to 100x compared to Python if the projection has a lot of synapses.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Connectivity"
    ]
  },
  {
    "objectID": "manual/Connector.html#available-connector-methods",
    "href": "manual/Connector.html#available-connector-methods",
    "title": "Connectivity",
    "section": "",
    "text": "For further detailed information about these connectors, please refer to the library reference Projections.\n\n\nAll neurons of the post-synaptic population form connections with all neurons of the pre-synaptic population (dense connectivity). Self-connections are avoided by default, but the parameter allow_self_connections can be set to True:\nproj.connect_all_to_all(weights=1.0, delays=2.0, allow_self_connections=False) \nThe weights and delays arguments accept both single float values (all synapses will take this initial value), as well as random objects allowing to randomly select the initial values for different synapses:\nproj.connect_all_to_all(weights=Uniform(0.0, 0.5)) \n\n\n\nA neuron of the post-synaptic population forms a connection with only one neuron of the pre-synaptic population. The order depends on the ranks: neuron 0 is connected with neuron 0 and so on. It is advised that the pre- and post-populations have the same size/geometry, especially when using population views.\npop1 = Population((20, 20), Neuron(parameters=\"r=0.0\"))\npop2 = Population((10, 10), Neuron(equations=\"r=sum(exc)\"))\n\nproj = Projection(pop1[5:15, 5:15], pop2, 'exc')\nproj.connect_one_to_one(weights=1.0) \nWeights and delays also accept random distributions.\nBelow is a graphical representation of the difference between all_to_all and one_to_one:\n\n\n\n\nA neuron of the post-synaptic population forms a connection with a limited region of the pre-synaptic population, centered around the neuron with the same normalized position. Weight values are initialized using a Gaussian function, with a maximal value amp for the neuron of same position and decreasing with distance (standard deviation sigma):\n\nw(x, y) = A \\, \\exp(-\\dfrac{1}{2}\\dfrac{(x-x_c)^2+(y-y_c)^2}{\\sigma^2})\n\nwhere (x, y) is the position of the pre-synaptic neuron (normalized to [0, 1]^d) and (x_c, y_c) is the position of the post-synaptic neuron (normalized to [0, 1]^d). A = amp, sigma = \\sigma.\nIn order to void creating useless synapses, the parameter limit can be set to restrict the creation of synapses to the cases where the value of the weight would be superior to limit*abs(amp). Default is 0.01 (1%).\nSelf-connections are avoided by default (parameter allow_self_connections).\nThe two populations must have the same number of dimensions, but the number of neurons can vary as the positions of each neuron are normalized in [0, 1]^d:\nproj.connect_gaussian( amp=1.0, sigma=0.2, limit=0.001) \n\n\n\nThe same as connect_gaussian, except weight values are computed using a Difference-of-Gaussians (DoG), usually positive in the center, negative a bit further away and small at long distances.\n\n\\begin{aligned}\nw(x, y) &= A^+ \\, \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_+^2}) \\\\\n    &-  A^- \\, \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_-^2}) \\\\\n\\end{aligned}\n\nWeights smaller than limit * abs(amp_pos - amp_neg) are not created and self-connections are avoided by default (parameter allow_self_connections):\nproj.connect_dog(\n    amp_pos=1.0, sigma_pos=0.2, \n    amp_neg=0.3, sigma_neg=0.7, \n    limit=0.001\n) \nThe following figure shows the example of a neuron of coordinates (10, 10) in the post-synaptic population, which is connected through the gaussian (left) and dog (right) projections to a population of geometry 30*30. The X and Y axis denote the coordinates of the pre-synaptic neurons, while the Z axis is the weight value.\n\n\n\n\nEach neuron in the post-synaptic population receives connections from a fixed number of neurons of the pre-synaptic population chosen randomly. It may happen that two post-synaptic neurons are connected to the same pre-synaptic neuron and that some pre-synaptic neurons are connected to nothing:\nproj.connect_fixed_number_pre(number = 20, weights=1.0) \nweights and delays can also take a random object.\n\n\n\nEach neuron in the pre-synaptic population sends a connection to a fixed number of neurons of the post-synaptic population chosen randomly. It may happen that two pre-synaptic neurons are connected to the same post-synaptic neuron and that some post-synaptic neurons receive no connection at all:\nproj.connect_fixed_number_post(number = 20, weights=1.0) \nThe following figure shows the fixed_number_pre (left) and fixed_number_post projections between two populations of 4 neurons, with number=2. In fixed_number_pre, each post-synaptic neuron receives exactly 2 connections, while in fixed_number_post, each pre-synaptic neuron send exactly two connections:\n\n\n\n\nFor each post-synaptic neuron, there is a fixed probability that it forms a connection with a neuron of the pre-synaptic population. It is basically a all_to_all projection, except some synapses are not created, making the projection sparser:\nproj.connect_fixed_probability(probability = 0.2, weights=1.0) \n\n\n\n\n\n\nImportant\n\n\n\nIf a single value is used for the weights argument of connect_all_to_all, connect_one_to_one, connect_fixed_probability, connect_fixed_number_pre and connect_fixed_number_post, and the default synapse is used (no synaptic plasticity), ANNarchy will generate a single weight value for all the synapses of the projection, not one per synapse.\nThis allows to save a lot of memory and improve performance. However, if you wish to manually change the weights of some of the synapses after the creation, you need to force the creation of one value per synapse by setting force_multiple_weights=True in the call to the connector.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Connectivity"
    ]
  },
  {
    "objectID": "manual/Connector.html#saved-connectivity",
    "href": "manual/Connector.html#saved-connectivity",
    "title": "Connectivity",
    "section": "",
    "text": "It is also possible to build a connection pattern using data saved during a precedent simulation. This is useful when:\n\npre-learning is done in another context;\na connector method for static synapses is particularly slow (e.g. DoG), but loading the result from a file is faster.\n\nThe connectivity of a projection can be saved (after compile()) using:\nproj.save_connectivity(filename='proj.npz')\nThe filename can used relative or absolute paths. The data is saved in a binary format:\n\nCompressed Numpy format when the filename ends with .npz.\nCompressed binary file format when the filename ends with .gz.\nBinary file format otherwise.\n\nIt can then be used to instantiate another projection:\nproj.connect_from_file(filename='proj.npz')\nOnly the connectivity (which neurons are connected), the weights and delays are loaded. Other synaptic variables are left untouched. The pre- and post-synaptic population must have the same size during saving and loading.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Connectivity"
    ]
  },
  {
    "objectID": "manual/Connector.html#from-connectivity-matrices",
    "href": "manual/Connector.html#from-connectivity-matrices",
    "title": "Connectivity",
    "section": "",
    "text": "One can also create connections using Numpy dense matrices or Scipy sparse matrices.\n\n\nThis method accepts a Numpy array to define the weights of the projection (and optionally the delays). By default, the matrix should have the size (post.size, pre.size), so that the first index represents a post-synaptic neuron and the second the pre-synaptic neurons. If your matrix is defined in the reversed order, you can either transpose it or set the pre_post argument to True.\nThis method is useful for dense connectivity matrices (all-to-all). If you do not want to create some synapses, the weight value should be set to None.\nThe following code creates a synfire chain inside a population of 100 neurons:\nN = 100\nproj = ann.Projection(pop, pop, 'exc')\n\n# Initialize an empty connectivity matrix\nw = np.array([[None]*N]*N)\n\n# Connect each post-synaptic neuron to its predecessor\nfor i in range(N):\n    w[i, (i-1)%N] = 1.0\n\n# Create the connections\nproj.connect_from_matrix(w)\nConnectivity matrices can not work with multi-dimensional coordinates, only ranks are used. Population views can be used in the projection, but the connection matrix must have the corresponding size:\nproj = ann.Projection(pop[10:20], pop[50:60], 'exc')\n\n# Create the connectivity matrix\nw = np.ones((10, 10))\n\n# Create the connections\nproj.connect_from_matrix(w)\n\n\n\nFor sparse connection matrices, the Numpy array format may have a huge memory overhead if most of its values are None. It is possible to use Scipy sparse matrices in that case. The previous synfire chain example becomes:\nfrom scipy.sparse import lil_matrix\n\nproj = ann.Projection(pop, pop, 'exc')\n\nw = lil_matrix((N, N))\nfor i in range(N):\n    w[i, (i+1)%N] = 1.0\n\nproj.connect_from_sparse(w)\n\n\n\n\n\n\nNote\n\n\n\nContrary to connect_from_matrix(), the first index of the sparse matrix represents the pre-synaptic neurons, not the post-synaptic ones. This is for compatibility with other neural simulators.\n\n\nconnect_from_sparse() accepts lil_matrix, csr_matrix and csc_matrix objects, although lil_matrix should be preferred for its simplicity of element access.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Connectivity"
    ]
  },
  {
    "objectID": "manual/Connector.html#user-defined-patterns",
    "href": "manual/Connector.html#user-defined-patterns",
    "title": "Connectivity",
    "section": "",
    "text": "This section describes the creation of user-specific connection patterns in ANNarchy, if the available patterns are not enough. A connection pattern is simply implemented as a method returning a LILConnectivity (list-of-list) object containing all the necessary information to create the synapses.\nA connector method must take on the first position the pre-synaptic population (or a subset of it) and on the second one the post-synaptic population. Other arguments are free, but should be passed when creating the projection.\nprobabilistic_pattern(pre, post, &lt;other arguments&gt;)\nAs an example, we will recreate the fixed_probability connector method, building synapses with a given probability. For this new pattern we need a weight value (common for all synapses) and a probability value as additional arguments. We consider that no delay is introduced in the synaptic transmission.\ndef probabilistic_pattern(pre, post, weight, probability):\n\n    synapses = ann.LILConnectivity()\n\n    ... pattern code comes here ...\n\n    return synapses\n\n\nThe connector method needs to return a LILConnectivity object storing the connectivity. For each post-synaptic neuron receiving synapses, a list of pre-synaptic ranks, weight values and delays must be added to the structure. If you use 2D or 3D populations you need to transform the coordinates into ranks with the rank_from_coordinates function.\nimport random\nimport ANNarchy as ann\n\ndef probabilistic_pattern(pre, post, weight, probability):\n    # Create a LIL structure for the connectivity matrix\n    synapses = ann.LILConnectivity()\n    # For all neurons in the post-synaptic population\n    for post_rank in xrange(post.size):\n        # Decide which pre-synaptic neurons should form synapses\n        ranks = []\n        for pre_rank in xrange(pre.size):\n            if random.random() &lt; probability:\n                ranks.append(pre_rank)\n        # Create weights and delays arrays of the same size\n        values = [weight for i in xrange(len(ranks)) ]\n        delays = [0 for i in xrange(len(ranks)) ]\n        # Add this information to the LIL matrix\n        synapses.add(post_rank, ranks, values, delays)\n\n    return synapses\nThe first for - loop iterates over all post-synaptic neurons in the projection. The inner for loop decides for each of these neurons if a synapse with a pre-synaptic neuron should be created, based on the value probability provided as argument to the function.\nThe lists values and delays are then created with the same size as ranks (important!), and filled with the desired value. All this information is then fed into the LIL matrix using the add(post_rank, ranks, values, delays) method.\n\n\n\n\n\n\nNote\n\n\n\nBuilding such connectivity matrices in Python can be extremely slow, as Python is not made for tight nested loops. If the construction of your network lasts too long, you should definitely write this function in Cython.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe add() should be only called once per post-synaptic neuron! If not, ANNarchy will have to reorder its internal representations and this will be really slow.\n\n\nUsage of the pattern\nTo use the pattern within a projection you provide the pattern method to the connect_with_func method of Projection\nproj = ann.Projection(\n    pre = pop1, \n    post = pop2, \n    target = 'inh' \n)\n\nproj.connect_with_func(\n    method=probabilistic_pattern, \n    weight=1.0, \n    probability=0.3\n)   \nmethod is the method you just wrote. Extra arguments (other than pre and post) should be passed with the same name.\n\n\n\nFor this example, we will create a Cython file CustomPatterns.pyx in the same directory as the script. Its content should be relatively similar to the Python version, except some type definitions:\n# distutils: language = c++\nimport random\nimport ANNarchy\ncimport ANNarchy.cython_ext.Connector as Connector\n\ndef probabilistic_pattern(pre, post, weight, probability):\n    # Typedefs\n    cdef Connector.LILConnectivity synapses\n    cdef int post_rank, pre_rank\n    cdef list ranks, values, delays\n\n    # Create a LILConnectivity structure for the connectivity matrix\n    synapses = Connector.LILConnectivity()\n    # For all neurons in the post-synaptic population\n    for post_rank in xrange(post.size):\n        # Decide which pre-synaptic neurons should form synapses\n        ranks = []\n        for pre_rank in xrange(pre.size):\n            if random.random() &lt; probability:\n                ranks.append(pre_rank)\n        # Create weights and delays arrays of the same size\n        values = [weight for i in xrange(len(ranks)) ]\n        delays = [0 for i in xrange(len(ranks)) ]\n        # Add this information to the LILConnectivity matrix\n        synapses.add(post_rank, ranks, values, delays)\n\n    return synapses\nThe only differences with the Python code are:\n\nThe module Connector where the LILConnectivity connection matrix class is defined should be cimported with:\n\ncimport ANNarchy.cython_ext.Connector as Connector\n\nData structures should be declared with cdef at the beginning of the method:\n\n# Typedefs\ncdef Connector.LILConnectivity synapses\ncdef int post_rank, pre_rank\ncdef list ranks, values, delays \nTo allow Cython to compile this file, we also need to provide with a kind of \"Makefile\" specifying that the code should be generated in C++, not C. This file should have the same name as the Cython file but end with .pyxbld, here : CustomPatterns.pyxbld.\nfrom distutils.extension import Extension\nimport ANNarchy\n\ndef make_ext(modname, pyxfilename):\n    return Extension(name=modname,\n                     sources=[pyxfilename],\n                     include_dirs = ANNarchy.include_path(),\n                     extra_compile_args=['-std=c++11'],\n                     language=\"c++\")\n\n\n\n\n\n\nNote\n\n\n\nThis .pyxbld is generic, you don’t need to modify anything, except its name.\n\n\nNow you can import the method probabilistic_pattern() into your Python code using the pyximport module of Cython and build the Projection normally:\nimport pyximport; pyximport.install()\nfrom CustomPatterns import probabilistic_pattern\n\nproj.connect_with_func(method=probabilistic_pattern, weight=1.0, probability=0.3)\nWriting the connector in Cython can bring speedups up to 100x compared to Python if the projection has a lot of synapses.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Connectivity"
    ]
  },
  {
    "objectID": "manual/SpikeNeuron.html",
    "href": "manual/SpikeNeuron.html",
    "title": "Spiking neurons",
    "section": "",
    "text": "Contrary to rate-coded neurons, the use of spiking neurons requires the additional definition of a spike condition (the criteria defining the emission of a spike, typically when the membrane potential exceeds a threshold) and reset equations, governing the evolution of all variables after a spike is emitted.\n\n\nANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN. Their definition (parameters, equations) are described in Specific Neurons. The classes can be used directly when creating the populations (no need to implement them). Example:\npop = ann.Population(geometry = 1000, neuron = Izhikevich)\n\n\n\nLet’s consider a simple leaky integrate-and-fire spiking neuron model (LIF) using a voltage-gated excitatory conductance:\n\\tau \\cdot  \\frac{ d v(t) }{ dt } = (E_r - v(t) ) + g_\\text{exc}(t) \\cdot (E_e -  v(t) )\nwhere v(t) is the membrane potential, \\tau is the membrane time constant (in milliseconds), E_r the resting potential, E_e the target potential for excitatory synapses and g_\\text{exc}(t) the total current induced by excitatory synapses.\nThis neural model can be defined in ANNarchy by:\nLIF = ann.Neuron(\n    parameters=\"\"\"\n        tau = 10.0  : population\n        Er = -60.0  : population\n        Ee = 0.0    : population\n        T = -45.0   : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0\n    \"\"\",\n    spike = \"v &gt; T\",\n    reset = \"v = Er\",\n    refractory = 5.0\n)\nAs for rate-coded neurons, the parameters are defined in the parameters description, here globally for the population. equations contains the description of the ODE followed by the membrane potential. The additional information to provide is:\n\nspike : a boolean condition on a single variable (typically the membrane potential) deciding when a spike is emitted.\nreset : the modifications to the neuron’s variables after a spike is emitted (typically, clamping the membrane potential to its reset potential).\nrefractory: optionally a refractory period in ms.\n\n\n\nThe spike condition is a single constraint definition. You may use the different available comparison operators (&gt;, &lt;, ==, etc) on a single neuron variable, using as many parameters as you want.\nThe use of assignment statements or ODEs will lead to an error. Conditional statements can be used. Example:\nparameters=\"\"\"\n    ...\n    T = -45.0 \n\"\"\",\nequations=\"\"\"\n    prev_v = v\n    noise = Uniform (-5.0, 5.0)\n    tau*dv/dt = E - v + g_exc\n\"\"\",\nspike = \"\"\"\n    (v &gt; T + noise) and (prev_v &lt; T + noise)\n\"\"\"\n\n\n\nHere you define the variables which should be set to certain values after a spike occured. Any assignment statements is allowed (=, +=, etc), but the use of ODEs is not possible, as the reset is performed only once at the end of the time step.\nExample:\nreset = \"\"\"\n    v = Er \n    u += 0.1 \n\"\"\"\n\n\n\nContrary to rate-coded neurons, spiking neurons use conductance variables to encode the received inputs, not weighted sums. In ANNarchy, the conductances are defined by g_ followed by the target name. For example, if a population receives excitatory input (target exc) from another one, you can access the total conductance provoked by exc spikes with:\ntau * dv/dt + v = g_exc\nThe dynamics of the conductance can be specified after its usage in the membrane potential equation.\n\nThe default behaviour for conductances is an instantaneous reset (or infinitely fast exponential decay). In practice, this means that all incoming spikes are summed up (weighted by the synaptic efficiency) at the beginning of a simulation step, and the resulting conductance is reset to 0.0 at the end of the step. This default behaviour is equivalent to :\n\nLIF = ann.Neuron(\n    parameters=\"\"\" ... \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0\n        g_exc = 0.0\n    \"\"\",\n    spike = \" ... \",\n    reset = \" ... \"\n)\nIncoming spikes increase g_exc and can provoke a post-synaptic spike at the next step, but leave no trace beyond that point.\n\nMost models however use exponentially decaying synapses, where the conductance decays with a short time constant after a spike is received. This behavior should be explicitely specified in the neuron’s equations:\n\nLIF = ann.Neuron(\n    parameters=\"\"\" ... \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0\n        tau_exc * dg_exc/dt = - g_exc\n    \"\"\",\n    spike = \" ... \",\n    reset = \" ... \"\n)\ng_exc is increased by incoming spikes, and slowly decays back to 0.0 until the next spikes arrive.\n\n\n\nThe refractory period in milliseconds is specified by the refractory parameter of Neuron.\nLIF = ann.Neuron (\n    parameters = \"\"\" ... \"\"\",\n    equations = \"\"\" ... \"\"\",\n    spike = \"\"\" ... \"\"\",\n    reset = \"\"\" \n        v = c\n        u += d\n    \"\"\",\n    refractory = 5.0\n)\nThe refractory argument can be a floating value or the name of a parameter/variable (string).\nIf dt = 0.1, this means that the equations will not be evaluated for 50 consecutive steps after a spike is emitted, except for the conductances (starting with g_) which are evaluated normally during the refractory period (the neuron is not \"deaf\", it only is frozen in a refractory state).\nrefractory becomes an attribute of a spiking Population object, so it can be set specifically for a population even when omitted in the neuron definition:\nLIF = ann.Neuron (\n    parameters = \" ... \",\n    equations = \" ... \",\n    spike = \" ... \",\n    reset = \"\"\" \n        v = c\n        u += d\n    \"\"\"\n)\n\npop = ann.Population(geometry = 1000, neuron = LIF)\npop.refractory = Uniform(1.0, 10.0)\nIt can be either a single value, a RandomDistribution object or a Numpy array of the same size/geometry as the population.\n\n\n\nMethod 1: ISI\nSpiking neurons define an additional variable t_last which represents the timestamp (in ms) of the last emitted spike (updated at the end of the reset statement). The time elapsed since the last spike is then t - t_last.\nThis can be used to update the instantaneous firing rate of a neuron, by inverting the inter-spike interval (ISI) during the reset statement following the emission of a spike:\nneuron = ann.Neuron(\n    parameters = \"tau = 20.0; tauf = 1000.\",\n    equations = \"\"\"\n        tau * dv/dt + v = ...\n        tauf * df/dt = -f\n    \"\"\",\n    spike = \"v &gt; 1.0\",\n    reset = \"\"\"\n        v = 0.0\n        f = 1000./(t - t_last)\n    \"\"\"\n)\nHere, a leaky integrator on f is needed to 1) smooth the firing rate and 2) slowly decay to 0 when the neuron stops firing. This method reflects very fast changes in the firing rate, but is also very sensible to noise.\nMethod 2: Window\nA more stable way to compute the firing rate of a neuron is to count at each time step the number of spikes emitted during a sliding temporal window (of 100 ms or 1s for example). By default, spiking neurons only record the time of the last spike they emitted (t_last), so this mechanism has to be explicitely enabled by calling the compute_firing_rate() method of the desired population:\npop = ann.Population(100, Izhikevich)\npop.compute_firing_rate(window=1000.0)\nThe window argument represents the period in milliseconds over which the spikes will be counted. The resulting firing rate (in Hz) will be stored in the local variable r (as for rate-coded neurons), which can be accessed by the neuron itself or by incoming and outgoing synapse (pre.r and post.r).\nIf the method has not been called, the variable r of a spiking neuron will be constantly 0.0.\n\n\n\n\n\n\nWarning\n\n\n\nThe window method is not available on CUDA yet.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Spiking neurons"
    ]
  },
  {
    "objectID": "manual/SpikeNeuron.html#built-in-neurons",
    "href": "manual/SpikeNeuron.html#built-in-neurons",
    "title": "Spiking neurons",
    "section": "",
    "text": "ANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN. Their definition (parameters, equations) are described in Specific Neurons. The classes can be used directly when creating the populations (no need to implement them). Example:\npop = ann.Population(geometry = 1000, neuron = Izhikevich)",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Spiking neurons"
    ]
  },
  {
    "objectID": "manual/SpikeNeuron.html#user-defined-neurons",
    "href": "manual/SpikeNeuron.html#user-defined-neurons",
    "title": "Spiking neurons",
    "section": "",
    "text": "Let’s consider a simple leaky integrate-and-fire spiking neuron model (LIF) using a voltage-gated excitatory conductance:\n\\tau \\cdot  \\frac{ d v(t) }{ dt } = (E_r - v(t) ) + g_\\text{exc}(t) \\cdot (E_e -  v(t) )\nwhere v(t) is the membrane potential, \\tau is the membrane time constant (in milliseconds), E_r the resting potential, E_e the target potential for excitatory synapses and g_\\text{exc}(t) the total current induced by excitatory synapses.\nThis neural model can be defined in ANNarchy by:\nLIF = ann.Neuron(\n    parameters=\"\"\"\n        tau = 10.0  : population\n        Er = -60.0  : population\n        Ee = 0.0    : population\n        T = -45.0   : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0\n    \"\"\",\n    spike = \"v &gt; T\",\n    reset = \"v = Er\",\n    refractory = 5.0\n)\nAs for rate-coded neurons, the parameters are defined in the parameters description, here globally for the population. equations contains the description of the ODE followed by the membrane potential. The additional information to provide is:\n\nspike : a boolean condition on a single variable (typically the membrane potential) deciding when a spike is emitted.\nreset : the modifications to the neuron’s variables after a spike is emitted (typically, clamping the membrane potential to its reset potential).\nrefractory: optionally a refractory period in ms.\n\n\n\nThe spike condition is a single constraint definition. You may use the different available comparison operators (&gt;, &lt;, ==, etc) on a single neuron variable, using as many parameters as you want.\nThe use of assignment statements or ODEs will lead to an error. Conditional statements can be used. Example:\nparameters=\"\"\"\n    ...\n    T = -45.0 \n\"\"\",\nequations=\"\"\"\n    prev_v = v\n    noise = Uniform (-5.0, 5.0)\n    tau*dv/dt = E - v + g_exc\n\"\"\",\nspike = \"\"\"\n    (v &gt; T + noise) and (prev_v &lt; T + noise)\n\"\"\"\n\n\n\nHere you define the variables which should be set to certain values after a spike occured. Any assignment statements is allowed (=, +=, etc), but the use of ODEs is not possible, as the reset is performed only once at the end of the time step.\nExample:\nreset = \"\"\"\n    v = Er \n    u += 0.1 \n\"\"\"\n\n\n\nContrary to rate-coded neurons, spiking neurons use conductance variables to encode the received inputs, not weighted sums. In ANNarchy, the conductances are defined by g_ followed by the target name. For example, if a population receives excitatory input (target exc) from another one, you can access the total conductance provoked by exc spikes with:\ntau * dv/dt + v = g_exc\nThe dynamics of the conductance can be specified after its usage in the membrane potential equation.\n\nThe default behaviour for conductances is an instantaneous reset (or infinitely fast exponential decay). In practice, this means that all incoming spikes are summed up (weighted by the synaptic efficiency) at the beginning of a simulation step, and the resulting conductance is reset to 0.0 at the end of the step. This default behaviour is equivalent to :\n\nLIF = ann.Neuron(\n    parameters=\"\"\" ... \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0\n        g_exc = 0.0\n    \"\"\",\n    spike = \" ... \",\n    reset = \" ... \"\n)\nIncoming spikes increase g_exc and can provoke a post-synaptic spike at the next step, but leave no trace beyond that point.\n\nMost models however use exponentially decaying synapses, where the conductance decays with a short time constant after a spike is received. This behavior should be explicitely specified in the neuron’s equations:\n\nLIF = ann.Neuron(\n    parameters=\"\"\" ... \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0\n        tau_exc * dg_exc/dt = - g_exc\n    \"\"\",\n    spike = \" ... \",\n    reset = \" ... \"\n)\ng_exc is increased by incoming spikes, and slowly decays back to 0.0 until the next spikes arrive.\n\n\n\nThe refractory period in milliseconds is specified by the refractory parameter of Neuron.\nLIF = ann.Neuron (\n    parameters = \"\"\" ... \"\"\",\n    equations = \"\"\" ... \"\"\",\n    spike = \"\"\" ... \"\"\",\n    reset = \"\"\" \n        v = c\n        u += d\n    \"\"\",\n    refractory = 5.0\n)\nThe refractory argument can be a floating value or the name of a parameter/variable (string).\nIf dt = 0.1, this means that the equations will not be evaluated for 50 consecutive steps after a spike is emitted, except for the conductances (starting with g_) which are evaluated normally during the refractory period (the neuron is not \"deaf\", it only is frozen in a refractory state).\nrefractory becomes an attribute of a spiking Population object, so it can be set specifically for a population even when omitted in the neuron definition:\nLIF = ann.Neuron (\n    parameters = \" ... \",\n    equations = \" ... \",\n    spike = \" ... \",\n    reset = \"\"\" \n        v = c\n        u += d\n    \"\"\"\n)\n\npop = ann.Population(geometry = 1000, neuron = LIF)\npop.refractory = Uniform(1.0, 10.0)\nIt can be either a single value, a RandomDistribution object or a Numpy array of the same size/geometry as the population.\n\n\n\nMethod 1: ISI\nSpiking neurons define an additional variable t_last which represents the timestamp (in ms) of the last emitted spike (updated at the end of the reset statement). The time elapsed since the last spike is then t - t_last.\nThis can be used to update the instantaneous firing rate of a neuron, by inverting the inter-spike interval (ISI) during the reset statement following the emission of a spike:\nneuron = ann.Neuron(\n    parameters = \"tau = 20.0; tauf = 1000.\",\n    equations = \"\"\"\n        tau * dv/dt + v = ...\n        tauf * df/dt = -f\n    \"\"\",\n    spike = \"v &gt; 1.0\",\n    reset = \"\"\"\n        v = 0.0\n        f = 1000./(t - t_last)\n    \"\"\"\n)\nHere, a leaky integrator on f is needed to 1) smooth the firing rate and 2) slowly decay to 0 when the neuron stops firing. This method reflects very fast changes in the firing rate, but is also very sensible to noise.\nMethod 2: Window\nA more stable way to compute the firing rate of a neuron is to count at each time step the number of spikes emitted during a sliding temporal window (of 100 ms or 1s for example). By default, spiking neurons only record the time of the last spike they emitted (t_last), so this mechanism has to be explicitely enabled by calling the compute_firing_rate() method of the desired population:\npop = ann.Population(100, Izhikevich)\npop.compute_firing_rate(window=1000.0)\nThe window argument represents the period in milliseconds over which the spikes will be counted. The resulting firing rate (in Hz) will be stored in the local variable r (as for rate-coded neurons), which can be accessed by the neuron itself or by incoming and outgoing synapse (pre.r and post.r).\nIf the method has not been called, the variable r of a spiking neuron will be constantly 0.0.\n\n\n\n\n\n\nWarning\n\n\n\nThe window method is not available on CUDA yet.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Spiking neurons"
    ]
  },
  {
    "objectID": "manual/Reporting.html",
    "href": "manual/Reporting.html",
    "title": "Reporting",
    "section": "",
    "text": "ANNarchy includes an utility allowing to automatically generate a report based on the current structure of the network:\nann.report(filename=\"model_description.tex\")\nann.report(filename=\"model_description.qmd\")\nIf the filename ends with .tex, the LaTeX report will be generated based on the specifications provided in:\n\nNordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8):e1000456.\n\nIf the filename ends with .qmd, the report will be generated in Markdown, so it can later be exported to pdf or html using pandoc.\nreport() accepts several arguments:\n\nfilename: name of the file where the report will be written (default: \"./report.tex\")\nstandalone: tells if the generated TeX file should be directly compilable or only includable. Ignored in Markdown.\ngather_subprojections: if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False).\ntitle: title of the document (Markdown only)\nauthor: author of the document (Markdown only)\ndate: date of the document (Markdown only)\nnet_id: id of the network to be used for reporting (default: 0, everything that was declared)\n\n\n\nreport() produces a .tex file (by default report.tex in the current directory, but this can be changed by passing the filename argument) which can be directly compiled with pdflatex or integrated into a larger file:\npdflatex model_description.tex\nThis report consists of different tables describing several aspects of the model:\n\nSummary: A summary of the network, with a list of populations, neuron and synapse models, topologies, etc. This section may have to be adapted, as for example, ANNarchy does not make a distinction between synapse and plasticity models.\nPopulations: A list of populations, with their respective neural models and geometries.\nProjections: A list of projections, with the pre- and post-synaptic populations, the target, the synapse model if any, and a description of the connection pattern.\nNeuron models: For each neuron model, a description of its dynamics with equations parsed using SymPy and translated to the LaTeX mathematical language.\nSynapse models: For each synapse model, a description of its dynamics if any.\nParameters: The initial value (before the call to compile()) of the parameters of each population and projection (if any).\nInput: Inputs set to the network (has to be filled manually).\nMeasurements: Measurements done in the network (has to be filled manually).\n\n\n\n\nThe generated Mardown file is globally similar to the LaTeX one, with additional information that make it more useful for debugging (locality of attributes, type...). The Markown file is readable by design, but it can be translated to many markup languages (html, epub, latex, pdf...) using pandoc.\nTo obtain a pdf from the Markdown file (supposing you have a LaTeX distribution available), just type:\npandoc model_description.qmd -sN -V geometry:margin=1in -o model_description.pdf\nThe -V argument tells LaTex to use the full page instead of the default booklet format.\nTo obtain a html file, use:\npandoc model_description.qmd -sSN --mathjax -o model_description.html\nYou can omit the -S option if you only want to include the code into a webpage, otherwise it is a standalone file. --mathjax is needed to display mathematical equations using the javascript library MathJax.\nBy default, the html file has no styling, and tables can be very ugly. With a simple css file like this one, the html page looks nicer (feel free to edit):\npandoc model_description.qmd -sSN --mathjax --css=simple.css -o model_description.html\nIf you upload your model to a github-like service (bitbucket, gitlab, gogs...), it could be a good idea to generate the README.qmd directly with report(). Do not forget to set a title+author+date then.\n\n\n\nThe report is generated based entirely on the Python script. For it to make sense, the user has to provide the necessary information while defining the network:\n\nPopulations must be assigned a unique name. If no name is given, generic names such as pop0 or pop1 will be used. If two populations have the same name, the connectivity will be unreadable:\npop1 = ann.Population(geometry=(100, 100), neuron=ann.Izhikevich, name=\"Excitatory\")\npop2 = ann.Population(geometry=(20, 20), neuron=ann.Izhikevich, name=\"Inhibitory\")\nUser-defined neuron and synapse models should be assigned a name and description. The name should be relatively short and generic (e.g. “Izhikevich”, “BCM learning rule”), while the description should be more specific. They can contain LaTeX code, but remember to double the \\ which is the escape symbol in Python strings:\nLIF = ann.Neuron(\n    parameters = \"\"\"\n        tau = 10.0\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt + v = g_exc\n    \"\"\",\n    spike = \"v &gt; 30.0\",\n    reset = \"v = 0.0\"\n    name = \"LIF\",\n    description = \"Leaky Integrate-and-Fire spiking neuron with time constant $\\\\tau$.\" \n)\n\nOja = ann.Synapse(\n    parameters = \"\"\"\n        eta = 10.0 \n        tau = 10.0 : postsynaptic\n    \"\"\",\n    equations = \"\"\"\n        tau * dalpha/dt + alpha = pos(post.r - 1.0) : postsynaptic\n        eta * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=0.0\n    \"\"\", \n    name=\"Oja learning rule\",\n    description= \"\"\"Oja learning rule ensuring regularization of the synaptic weights.\"\"\"\n) \nChoose simple parameter and variable names for the description of equations. If a parameter/variable name uses only one character, it will be treated as a mathematical variable in the equations (ex: v becomes v), otherwise the plain text representation will be used (ugly). If the name corresponds to a greek letter (alpha, tau, etc.), it will be represented by the corresponding greek letter (\\alpha, \\tau). If the name is composed of two terms separated by an underscore (tau_exc), a subscript will be used (\\tau_\\text{exc}). If more than one underscore is used, the text representation is used instead (LaTeX does not allow multiple subscripts).\n\n\n\n\nLet’s take the homeostatic STDP ramp example provided in examples/homeostatic_stdp/Ramp.py and add names/descriptions to the objects:\nimport numpy as np\nimport ANNarchy as ann\n\n# Izhikevich RS neuron\nRSNeuron = ann.Neuron(\n    parameters = \"\"\"\n        a = 0.02 : population\n        b = 0.2 : population\n        c = -65. : population\n        d = 8. : population\n        tau_ampa = 5. : population\n        tau_nmda = 150. : population\n        vrev = 0.0 : population\n    \"\"\" ,\n    equations=\"\"\"\n        # Inputs\n        I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)  \n        # Midpoint scheme      \n        dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint\n        du/dt = a * (b*v - u) : init=-13., midpoint\n        # Izhikevich scheme\n        # new_v = v + 0.5*(0.04 * v^2 + 5.0 * v + 140.0 - u + I) : init=-65.\n        # v = new_v + 0.5*(0.04 * new_v^2 + 5.0 * new_v + 140.0 - u + I) : init=-65.\n        # u += a * (b*v - u) : init=-13.\n        # Conductances\n        tau_ampa * dg_ampa/dt = -g_ampa : exponential\n        tau_nmda * dg_nmda/dt = -g_nmda : exponential\n    \"\"\" , \n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\", \n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\",\n    name = \"Regular-spiking Izhikevich\",\n    description = \"Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\"\n)\n\n# Input population\ninp = ann.PoissonPopulation(100, rates=np.linspace(0.2, 20., 100), name=\"Poisson input\")\n\n# RS neuron without homeostatic mechanism\npop1 = ann.Population(1, RSNeuron, name=\"RS neuron without homeostasis\")\npop1.compute_firing_rate(5000.)\n\n# RS neuron with homeostatic mechanism\npop2 = ann.Population(1, RSNeuron, name=\"RS neuron with homeostasis\")\npop2.compute_firing_rate(5000.)\n\n# Nearest Neighbour STDP\nnearest_neighbour_stdp = ann.Synapse(\n    parameters=\"\"\"\n        tau_plus = 20. : projection\n        tau_minus = 60. : projection\n        A_plus = 0.0002 : projection\n        A_minus = 0.000066 : projection\n        w_max = 0.03 : projection\n    \"\"\",\n    equations = \"\"\"\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n        # Nearest-neighbour\n        w += if t_post &gt;= t_pre: ltp else: - ltd : min=0.0, max=w_max\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\",\n    name = \"Nearest-neighbour STDP\",\n    description = \"Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\"\n)\n\n# STDP with homeostatic regulation\nhomeo_stdp = ann.Synapse(\n    parameters=\"\"\"\n        # STDP\n        tau_plus = 20. : projection\n        tau_minus = 60. : projection\n        A_plus = 0.0002 : projection\n        A_minus = 0.000066 : projection\n        w_min = 0.0 : projection\n        w_max = 0.03 : projection\n\n        # Homeostatic regulation\n        alpha = 0.1 : projection\n        beta = 1.0 : projection\n        gamma = 50. : projection\n        Rtarget = 35. : projection\n        T = 5000. : projection\n    \"\"\",\n    equations = \"\"\"\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n        # Homeostatic values\n        R = post.r : postsynaptic\n        K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic\n        # Nearest-neighbour\n        stdp = if t_post &gt;= t_pre: ltp else: - ltd \n        w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\" ,\n    name = \"Nearest-neighbour STDP with homeostasis\",\n    description = \"Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \"\n)\n\n# Projection without homeostatic mechanism\nproj1 = ann.Projection(inp, pop1, ['ampa', 'nmda'], synapse=nearest_neighbour_stdp)\nproj1.connect_all_to_all(Uniform(0.01, 0.03))\n\n# Projection with homeostatic mechanism\nproj2 = ann.Projection(inp, pop2, ['ampa', 'nmda'], synapse=homeo_stdp)\nproj2.connect_all_to_all(weights=Uniform(0.01, 0.03))\n\n\n# Record\nm1 = ann.Monitor(pop1, 'r')\nm2 = ann.Monitor(pop2, 'r')\n\nann.report('ramp.qmd', \n        title=\"Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\", \n        author=\"Carlson, Richert, Dutt and Krichmar\",\n        date=\"Neural Networks (IJCNN) 2013\")\nThis generates the following Markdown file:\n---\ntitle: Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\nauthor: Carlson, Richert, Dutt and Krichmar\ndate: Neural Networks (IJCNN) 2013\n---\n\n# Structure of the network\n\n* ANNarchy 4.8.0 using the default backend.\n* Numerical step size: 1.0 ms.\n\n## Populations\n\n| **Population**                | **Size** | **Neuron type**            | \n| ----------------------------- | -------- | -------------------------- | \n| Poisson input                 | 100      | Poisson                    | \n| RS neuron without homeostasis | 1        | Regular-spiking Izhikevich | \n| RS neuron with homeostasis    | 1        | Regular-spiking Izhikevich | \n\n\n## Projections\n\n| **Source**    | **Destination**               | **Target**  | **Synapse type**                        | **Pattern**                                               | \n| ------------- | ----------------------------- | ----------- | --------------------------------------- | --------------------------------------------------------- | \n| Poisson input | RS neuron without homeostasis | ampa / nmda | Nearest-neighbour STDP                  | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | \n| Poisson input | RS neuron with homeostasis    | ampa / nmda | Nearest-neighbour STDP with homeostasis | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | \n\n\n## Monitors\n\n| **Object**                    | **Variables** | **Period** | \n| ----------------------------- | ------------- | ---------- | \n| RS neuron without homeostasis | r             | 1.0        | \n| RS neuron with homeostasis    | r             | 1.0        | \n\n\n# Neuron models\n\n## Regular-spiking Izhikevich\n\nRegular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\n\n**Parameters:**\n\n| **Name**             | **Default value** | **Locality**   | **Type** | \n| -------------------- | ----------------- | -------------- | -------- | \n| $a$                  | 0.02              | per population | double   | \n| $b$                  | 0.2               | per population | double   | \n| $c$                  | -65.0             | per population | double   | \n| $d$                  | 8.0               | per population | double   | \n| $\\tau_{\\text{ampa}}$ | 5.0               | per population | double   | \n| $\\tau_{\\text{nmda}}$ | 150.0             | per population | double   | \n| ${\\text{vrev}}$      | 0.0               | per population | double   | \n\n**Equations:**\n\n* Variable $I$ : per neuron, initial value: 0.0\n\n$$\n{I}(t) = {g_{\\text{ampa}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) + {g_{\\text{nmda}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) \\cdot \\operatorname{nmda}{\\left ({v}(t),-80.0,60.0 \\right )}\n$$\n\n* Variable $v$ : per neuron, initial value: -65.0, midpoint numerical method\n\n$$\n\\frac{d{v}(t)}{dt} = {I}(t) - {u}(t) + {v}(t) \\cdot \\left(0.04 \\cdot {v}(t) + 5.0\\right) + 140.0\n$$\n\n* Variable $u$ : per neuron, initial value: -13.0, midpoint numerical method\n\n$$\n\\frac{d{u}(t)}{dt} = a \\cdot \\left(b \\cdot {v}(t) - {u}(t)\\right)\n$$\n\n* Variable $g_{\\text{ampa}}$ : per neuron, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{g_{\\text{ampa}}}(t)}{dt} \\cdot \\tau_{\\text{ampa}} = - {g_{\\text{ampa}}}(t)\n$$\n\n* Variable $g_{\\text{nmda}}$ : per neuron, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{g_{\\text{nmda}}}(t)}{dt} \\cdot \\tau_{\\text{nmda}} = - {g_{\\text{nmda}}}(t)\n$$\n\n**Spike emission:**\n\nif ${v}(t) \\geq 30.0$ :\n\n* Emit a spike a time $t$.\n* ${v}(t) = c$\n* ${u}(t) \\mathrel{+}= d$\n\n\n**Functions**\n\n$${\\text{nmda}}(v, t, s) = \\frac{\\left(- t + v\\right)^{2}}{s^{2} \\cdot \\left(1.0 + \\frac{1}{s^{2}} \\cdot \\left(- t + v\\right)^{2}\\right)}$$\n\n\n## Poisson\n\nSpiking neuron with spikes emitted according to a Poisson distribution.\n\n**Parameters:**\n\n| **Name**         | **Default value** | **Locality** | **Type** | \n| ---------------- | ----------------- | ------------ | -------- | \n| ${\\text{rates}}$ | 10.0              | per neuron   | double   | \n\n**Equations:**\n\n* Variable $p$ : per neuron, initial value: 0.0\n\n$$\n{p}(t) = \\frac{1000.0}{\\Delta t} \\cdot \\mathcal{U}{\\left (0.0,1.0 \\right )}\n$$\n\n**Spike emission:**\n\nif ${p}(t) &lt; {\\text{rates}}$ :\n\n* Emit a spike a time $t$.\n\n# Synapse models\n\n## Nearest-neighbour STDP\n\nNearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\n\n**Parameters:**\n\n| **Name**              | **Default value** | **Locality**   | **Type** | \n| --------------------- | ----------------- | -------------- | -------- | \n| $\\tau_{\\text{plus}}$  | 20.0              | per projection | double   | \n| $\\tau_{\\text{minus}}$ | 60.0              | per projection | double   | \n| $A_{\\text{plus}}$     | 0.0002            | per projection | double   | \n| $A_{\\text{minus}}$    | 6.6e-05           | per projection | double   | \n| $w_{\\text{max}}$      | 0.03              | per projection | double   | \n\n**Equations:**\n\n* Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n$$\n\n* Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n$$\n\n* Variable $w$ : per synapse, initial value: 0.0, minimum: 0.0, maximum: w_max\n\n$$\n{w}(t) \\mathrel{+}= \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n$$\n\n**Pre-synaptic event at $t_\\text{pre} + d$:**\n$$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$\n$${{\\text{ltp}}}(t) = A_{\\text{plus}}$$\n\n**Post-synaptic event at $t_\\text{post}$:**\n$${{\\text{ltd}}}(t) = A_{\\text{minus}}$$\n\n## Nearest-neighbour STDP with homeostasis\n\nNearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \n\n**Parameters:**\n\n| **Name**              | **Default value** | **Locality**   | **Type** | \n| --------------------- | ----------------- | -------------- | -------- | \n| $\\tau_{\\text{plus}}$  | 20.0              | per projection | double   | \n| $\\tau_{\\text{minus}}$ | 60.0              | per projection | double   | \n| $A_{\\text{plus}}$     | 0.0002            | per projection | double   | \n| $A_{\\text{minus}}$    | 6.6e-05           | per projection | double   | \n| $w_{\\text{min}}$      | 0.0               | per projection | double   | \n| $w_{\\text{max}}$      | 0.03              | per projection | double   | \n| $\\alpha$              | 0.1               | per projection | double   | \n| $\\beta$               | 1.0               | per projection | double   | \n| $\\gamma$              | 50.0              | per projection | double   | \n| ${\\text{Rtarget}}$    | 35.0              | per projection | double   | \n| $T$                   | 5000.0            | per projection | double   | \n\n**Equations:**\n\n* Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n$$\n\n* Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n$$\n\n* Variable $R$ : per post-synaptic neuron, initial value: 0.0\n\n$$\n{R}(t) = {r}^{\\text{post}}(t)\n$$\n\n* Variable $K$ : per post-synaptic neuron, initial value: 0.0\n\n$$\n{K}(t) = \\frac{{R}(t)}{T \\cdot \\left(\\gamma \\cdot \\left|{f}\\right|{\\left (- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1.0 \\right )} + 1.0\\right)}\n$$\n\n* Variable ${\\text{stdp}}$ : per synapse, initial value: 0.0\n\n$$\n{{\\text{stdp}}}(t) = \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n$$\n\n* Variable $w$ : per synapse, initial value: 0.0, minimum: w_min, maximum: w_max\n\n$$\n{w}(t) \\mathrel{+}= {K}(t) \\cdot \\left(\\alpha \\cdot {w}(t) \\cdot \\left(- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1\\right) + \\beta \\cdot {{\\text{stdp}}}(t)\\right)\n$$\n\n**Pre-synaptic event at $t_\\text{pre} + d$:**\n$$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$\n$${{\\text{ltp}}}(t) = A_{\\text{plus}}$$\n\n**Post-synaptic event at $t_\\text{post}$:**\n$${{\\text{ltd}}}(t) = A_{\\text{minus}}$$\n\n# Parameters\n\n## Population parameters\n\n| **Population**                | **Neuron type**            | **Name**             | **Value**     | \n| ----------------------------- | -------------------------- | -------------------- | ------------- | \n| Poisson input                 | Poisson                    | ${\\text{rates}}$     | $[0.2, 20.0]$ | \n| RS neuron without homeostasis | Regular-spiking Izhikevich | $a$                  | 0.02          | \n|                               |                            | $b$                  | 0.2           | \n|                               |                            | $c$                  | -65.0         | \n|                               |                            | $d$                  | 8.0           | \n|                               |                            | $\\tau_{\\text{ampa}}$ | 5.0           | \n|                               |                            | $\\tau_{\\text{nmda}}$ | 150.0         | \n|                               |                            | ${\\text{vrev}}$      | 0.0           | \n| RS neuron with homeostasis    | Regular-spiking Izhikevich | $a$                  | 0.02          | \n|                               |                            | $b$                  | 0.2           | \n|                               |                            | $c$                  | -65.0         | \n|                               |                            | $d$                  | 8.0           | \n|                               |                            | $\\tau_{\\text{ampa}}$ | 5.0           | \n|                               |                            | $\\tau_{\\text{nmda}}$ | 150.0         | \n|                               |                            | ${\\text{vrev}}$      | 0.0           | \n\n\n\n## Projection parameters\n\n| **Projection**                                                                     | **Synapse type**                        | **Name**              | **Value** | \n| ---------------------------------------------------------------------------------- | --------------------------------------- | --------------------- | --------- | \n| Poisson input  $\\rightarrow$ RS neuron without homeostasis with target ampa / nmda | Nearest-neighbour STDP                  | $\\tau_{\\text{plus}}$  | 20.0      | \n|                                                                                    |                                         | $\\tau_{\\text{minus}}$ | 60.0      | \n|                                                                                    |                                         | $A_{\\text{plus}}$     | 0.0002    | \n|                                                                                    |                                         | $A_{\\text{minus}}$    | 6.6e-05   | \n|                                                                                    |                                         | $w_{\\text{max}}$      | 0.03      | \n| Poisson input  $\\rightarrow$ RS neuron with homeostasis with target ampa / nmda    | Nearest-neighbour STDP with homeostasis | $\\tau_{\\text{plus}}$  | 20.0      | \n|                                                                                    |                                         | $\\tau_{\\text{minus}}$ | 60.0      | \n|                                                                                    |                                         | $A_{\\text{plus}}$     | 0.0002    | \n|                                                                                    |                                         | $A_{\\text{minus}}$    | 6.6e-05   | \n|                                                                                    |                                         | $w_{\\text{min}}$      | 0.0       | \n|                                                                                    |                                         | $w_{\\text{max}}$      | 0.03      | \n|                                                                                    |                                         | $\\alpha$              | 0.1       | \n|                                                                                    |                                         | $\\beta$               | 1.0       | \n|                                                                                    |                                         | $\\gamma$              | 50.0      | \n|                                                                                    |                                         | ${\\text{Rtarget}}$    | 35.0      | \n|                                                                                    |                                         | $T$                   | 5000.0    |",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Reporting"
    ]
  },
  {
    "objectID": "manual/Reporting.html#content-of-the-tex-file",
    "href": "manual/Reporting.html#content-of-the-tex-file",
    "title": "Reporting",
    "section": "",
    "text": "report() produces a .tex file (by default report.tex in the current directory, but this can be changed by passing the filename argument) which can be directly compiled with pdflatex or integrated into a larger file:\npdflatex model_description.tex\nThis report consists of different tables describing several aspects of the model:\n\nSummary: A summary of the network, with a list of populations, neuron and synapse models, topologies, etc. This section may have to be adapted, as for example, ANNarchy does not make a distinction between synapse and plasticity models.\nPopulations: A list of populations, with their respective neural models and geometries.\nProjections: A list of projections, with the pre- and post-synaptic populations, the target, the synapse model if any, and a description of the connection pattern.\nNeuron models: For each neuron model, a description of its dynamics with equations parsed using SymPy and translated to the LaTeX mathematical language.\nSynapse models: For each synapse model, a description of its dynamics if any.\nParameters: The initial value (before the call to compile()) of the parameters of each population and projection (if any).\nInput: Inputs set to the network (has to be filled manually).\nMeasurements: Measurements done in the network (has to be filled manually).",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Reporting"
    ]
  },
  {
    "objectID": "manual/Reporting.html#content-of-the-markdown-file",
    "href": "manual/Reporting.html#content-of-the-markdown-file",
    "title": "Reporting",
    "section": "",
    "text": "The generated Mardown file is globally similar to the LaTeX one, with additional information that make it more useful for debugging (locality of attributes, type...). The Markown file is readable by design, but it can be translated to many markup languages (html, epub, latex, pdf...) using pandoc.\nTo obtain a pdf from the Markdown file (supposing you have a LaTeX distribution available), just type:\npandoc model_description.qmd -sN -V geometry:margin=1in -o model_description.pdf\nThe -V argument tells LaTex to use the full page instead of the default booklet format.\nTo obtain a html file, use:\npandoc model_description.qmd -sSN --mathjax -o model_description.html\nYou can omit the -S option if you only want to include the code into a webpage, otherwise it is a standalone file. --mathjax is needed to display mathematical equations using the javascript library MathJax.\nBy default, the html file has no styling, and tables can be very ugly. With a simple css file like this one, the html page looks nicer (feel free to edit):\npandoc model_description.qmd -sSN --mathjax --css=simple.css -o model_description.html\nIf you upload your model to a github-like service (bitbucket, gitlab, gogs...), it could be a good idea to generate the README.qmd directly with report(). Do not forget to set a title+author+date then.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Reporting"
    ]
  },
  {
    "objectID": "manual/Reporting.html#documenting-the-network",
    "href": "manual/Reporting.html#documenting-the-network",
    "title": "Reporting",
    "section": "",
    "text": "The report is generated based entirely on the Python script. For it to make sense, the user has to provide the necessary information while defining the network:\n\nPopulations must be assigned a unique name. If no name is given, generic names such as pop0 or pop1 will be used. If two populations have the same name, the connectivity will be unreadable:\npop1 = ann.Population(geometry=(100, 100), neuron=ann.Izhikevich, name=\"Excitatory\")\npop2 = ann.Population(geometry=(20, 20), neuron=ann.Izhikevich, name=\"Inhibitory\")\nUser-defined neuron and synapse models should be assigned a name and description. The name should be relatively short and generic (e.g. “Izhikevich”, “BCM learning rule”), while the description should be more specific. They can contain LaTeX code, but remember to double the \\ which is the escape symbol in Python strings:\nLIF = ann.Neuron(\n    parameters = \"\"\"\n        tau = 10.0\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt + v = g_exc\n    \"\"\",\n    spike = \"v &gt; 30.0\",\n    reset = \"v = 0.0\"\n    name = \"LIF\",\n    description = \"Leaky Integrate-and-Fire spiking neuron with time constant $\\\\tau$.\" \n)\n\nOja = ann.Synapse(\n    parameters = \"\"\"\n        eta = 10.0 \n        tau = 10.0 : postsynaptic\n    \"\"\",\n    equations = \"\"\"\n        tau * dalpha/dt + alpha = pos(post.r - 1.0) : postsynaptic\n        eta * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=0.0\n    \"\"\", \n    name=\"Oja learning rule\",\n    description= \"\"\"Oja learning rule ensuring regularization of the synaptic weights.\"\"\"\n) \nChoose simple parameter and variable names for the description of equations. If a parameter/variable name uses only one character, it will be treated as a mathematical variable in the equations (ex: v becomes v), otherwise the plain text representation will be used (ugly). If the name corresponds to a greek letter (alpha, tau, etc.), it will be represented by the corresponding greek letter (\\alpha, \\tau). If the name is composed of two terms separated by an underscore (tau_exc), a subscript will be used (\\tau_\\text{exc}). If more than one underscore is used, the text representation is used instead (LaTeX does not allow multiple subscripts).",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Reporting"
    ]
  },
  {
    "objectID": "manual/Reporting.html#example",
    "href": "manual/Reporting.html#example",
    "title": "Reporting",
    "section": "",
    "text": "Let’s take the homeostatic STDP ramp example provided in examples/homeostatic_stdp/Ramp.py and add names/descriptions to the objects:\nimport numpy as np\nimport ANNarchy as ann\n\n# Izhikevich RS neuron\nRSNeuron = ann.Neuron(\n    parameters = \"\"\"\n        a = 0.02 : population\n        b = 0.2 : population\n        c = -65. : population\n        d = 8. : population\n        tau_ampa = 5. : population\n        tau_nmda = 150. : population\n        vrev = 0.0 : population\n    \"\"\" ,\n    equations=\"\"\"\n        # Inputs\n        I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)  \n        # Midpoint scheme      \n        dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint\n        du/dt = a * (b*v - u) : init=-13., midpoint\n        # Izhikevich scheme\n        # new_v = v + 0.5*(0.04 * v^2 + 5.0 * v + 140.0 - u + I) : init=-65.\n        # v = new_v + 0.5*(0.04 * new_v^2 + 5.0 * new_v + 140.0 - u + I) : init=-65.\n        # u += a * (b*v - u) : init=-13.\n        # Conductances\n        tau_ampa * dg_ampa/dt = -g_ampa : exponential\n        tau_nmda * dg_nmda/dt = -g_nmda : exponential\n    \"\"\" , \n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\", \n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\",\n    name = \"Regular-spiking Izhikevich\",\n    description = \"Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\"\n)\n\n# Input population\ninp = ann.PoissonPopulation(100, rates=np.linspace(0.2, 20., 100), name=\"Poisson input\")\n\n# RS neuron without homeostatic mechanism\npop1 = ann.Population(1, RSNeuron, name=\"RS neuron without homeostasis\")\npop1.compute_firing_rate(5000.)\n\n# RS neuron with homeostatic mechanism\npop2 = ann.Population(1, RSNeuron, name=\"RS neuron with homeostasis\")\npop2.compute_firing_rate(5000.)\n\n# Nearest Neighbour STDP\nnearest_neighbour_stdp = ann.Synapse(\n    parameters=\"\"\"\n        tau_plus = 20. : projection\n        tau_minus = 60. : projection\n        A_plus = 0.0002 : projection\n        A_minus = 0.000066 : projection\n        w_max = 0.03 : projection\n    \"\"\",\n    equations = \"\"\"\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n        # Nearest-neighbour\n        w += if t_post &gt;= t_pre: ltp else: - ltd : min=0.0, max=w_max\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\",\n    name = \"Nearest-neighbour STDP\",\n    description = \"Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\"\n)\n\n# STDP with homeostatic regulation\nhomeo_stdp = ann.Synapse(\n    parameters=\"\"\"\n        # STDP\n        tau_plus = 20. : projection\n        tau_minus = 60. : projection\n        A_plus = 0.0002 : projection\n        A_minus = 0.000066 : projection\n        w_min = 0.0 : projection\n        w_max = 0.03 : projection\n\n        # Homeostatic regulation\n        alpha = 0.1 : projection\n        beta = 1.0 : projection\n        gamma = 50. : projection\n        Rtarget = 35. : projection\n        T = 5000. : projection\n    \"\"\",\n    equations = \"\"\"\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n        # Homeostatic values\n        R = post.r : postsynaptic\n        K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic\n        # Nearest-neighbour\n        stdp = if t_post &gt;= t_pre: ltp else: - ltd \n        w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\" ,\n    name = \"Nearest-neighbour STDP with homeostasis\",\n    description = \"Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \"\n)\n\n# Projection without homeostatic mechanism\nproj1 = ann.Projection(inp, pop1, ['ampa', 'nmda'], synapse=nearest_neighbour_stdp)\nproj1.connect_all_to_all(Uniform(0.01, 0.03))\n\n# Projection with homeostatic mechanism\nproj2 = ann.Projection(inp, pop2, ['ampa', 'nmda'], synapse=homeo_stdp)\nproj2.connect_all_to_all(weights=Uniform(0.01, 0.03))\n\n\n# Record\nm1 = ann.Monitor(pop1, 'r')\nm2 = ann.Monitor(pop2, 'r')\n\nann.report('ramp.qmd', \n        title=\"Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\", \n        author=\"Carlson, Richert, Dutt and Krichmar\",\n        date=\"Neural Networks (IJCNN) 2013\")\nThis generates the following Markdown file:\n---\ntitle: Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\nauthor: Carlson, Richert, Dutt and Krichmar\ndate: Neural Networks (IJCNN) 2013\n---\n\n# Structure of the network\n\n* ANNarchy 4.8.0 using the default backend.\n* Numerical step size: 1.0 ms.\n\n## Populations\n\n| **Population**                | **Size** | **Neuron type**            | \n| ----------------------------- | -------- | -------------------------- | \n| Poisson input                 | 100      | Poisson                    | \n| RS neuron without homeostasis | 1        | Regular-spiking Izhikevich | \n| RS neuron with homeostasis    | 1        | Regular-spiking Izhikevich | \n\n\n## Projections\n\n| **Source**    | **Destination**               | **Target**  | **Synapse type**                        | **Pattern**                                               | \n| ------------- | ----------------------------- | ----------- | --------------------------------------- | --------------------------------------------------------- | \n| Poisson input | RS neuron without homeostasis | ampa / nmda | Nearest-neighbour STDP                  | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | \n| Poisson input | RS neuron with homeostasis    | ampa / nmda | Nearest-neighbour STDP with homeostasis | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | \n\n\n## Monitors\n\n| **Object**                    | **Variables** | **Period** | \n| ----------------------------- | ------------- | ---------- | \n| RS neuron without homeostasis | r             | 1.0        | \n| RS neuron with homeostasis    | r             | 1.0        | \n\n\n# Neuron models\n\n## Regular-spiking Izhikevich\n\nRegular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\n\n**Parameters:**\n\n| **Name**             | **Default value** | **Locality**   | **Type** | \n| -------------------- | ----------------- | -------------- | -------- | \n| $a$                  | 0.02              | per population | double   | \n| $b$                  | 0.2               | per population | double   | \n| $c$                  | -65.0             | per population | double   | \n| $d$                  | 8.0               | per population | double   | \n| $\\tau_{\\text{ampa}}$ | 5.0               | per population | double   | \n| $\\tau_{\\text{nmda}}$ | 150.0             | per population | double   | \n| ${\\text{vrev}}$      | 0.0               | per population | double   | \n\n**Equations:**\n\n* Variable $I$ : per neuron, initial value: 0.0\n\n$$\n{I}(t) = {g_{\\text{ampa}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) + {g_{\\text{nmda}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) \\cdot \\operatorname{nmda}{\\left ({v}(t),-80.0,60.0 \\right )}\n$$\n\n* Variable $v$ : per neuron, initial value: -65.0, midpoint numerical method\n\n$$\n\\frac{d{v}(t)}{dt} = {I}(t) - {u}(t) + {v}(t) \\cdot \\left(0.04 \\cdot {v}(t) + 5.0\\right) + 140.0\n$$\n\n* Variable $u$ : per neuron, initial value: -13.0, midpoint numerical method\n\n$$\n\\frac{d{u}(t)}{dt} = a \\cdot \\left(b \\cdot {v}(t) - {u}(t)\\right)\n$$\n\n* Variable $g_{\\text{ampa}}$ : per neuron, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{g_{\\text{ampa}}}(t)}{dt} \\cdot \\tau_{\\text{ampa}} = - {g_{\\text{ampa}}}(t)\n$$\n\n* Variable $g_{\\text{nmda}}$ : per neuron, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{g_{\\text{nmda}}}(t)}{dt} \\cdot \\tau_{\\text{nmda}} = - {g_{\\text{nmda}}}(t)\n$$\n\n**Spike emission:**\n\nif ${v}(t) \\geq 30.0$ :\n\n* Emit a spike a time $t$.\n* ${v}(t) = c$\n* ${u}(t) \\mathrel{+}= d$\n\n\n**Functions**\n\n$${\\text{nmda}}(v, t, s) = \\frac{\\left(- t + v\\right)^{2}}{s^{2} \\cdot \\left(1.0 + \\frac{1}{s^{2}} \\cdot \\left(- t + v\\right)^{2}\\right)}$$\n\n\n## Poisson\n\nSpiking neuron with spikes emitted according to a Poisson distribution.\n\n**Parameters:**\n\n| **Name**         | **Default value** | **Locality** | **Type** | \n| ---------------- | ----------------- | ------------ | -------- | \n| ${\\text{rates}}$ | 10.0              | per neuron   | double   | \n\n**Equations:**\n\n* Variable $p$ : per neuron, initial value: 0.0\n\n$$\n{p}(t) = \\frac{1000.0}{\\Delta t} \\cdot \\mathcal{U}{\\left (0.0,1.0 \\right )}\n$$\n\n**Spike emission:**\n\nif ${p}(t) &lt; {\\text{rates}}$ :\n\n* Emit a spike a time $t$.\n\n# Synapse models\n\n## Nearest-neighbour STDP\n\nNearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\n\n**Parameters:**\n\n| **Name**              | **Default value** | **Locality**   | **Type** | \n| --------------------- | ----------------- | -------------- | -------- | \n| $\\tau_{\\text{plus}}$  | 20.0              | per projection | double   | \n| $\\tau_{\\text{minus}}$ | 60.0              | per projection | double   | \n| $A_{\\text{plus}}$     | 0.0002            | per projection | double   | \n| $A_{\\text{minus}}$    | 6.6e-05           | per projection | double   | \n| $w_{\\text{max}}$      | 0.03              | per projection | double   | \n\n**Equations:**\n\n* Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n$$\n\n* Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n$$\n\n* Variable $w$ : per synapse, initial value: 0.0, minimum: 0.0, maximum: w_max\n\n$$\n{w}(t) \\mathrel{+}= \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n$$\n\n**Pre-synaptic event at $t_\\text{pre} + d$:**\n$$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$\n$${{\\text{ltp}}}(t) = A_{\\text{plus}}$$\n\n**Post-synaptic event at $t_\\text{post}$:**\n$${{\\text{ltd}}}(t) = A_{\\text{minus}}$$\n\n## Nearest-neighbour STDP with homeostasis\n\nNearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \n\n**Parameters:**\n\n| **Name**              | **Default value** | **Locality**   | **Type** | \n| --------------------- | ----------------- | -------------- | -------- | \n| $\\tau_{\\text{plus}}$  | 20.0              | per projection | double   | \n| $\\tau_{\\text{minus}}$ | 60.0              | per projection | double   | \n| $A_{\\text{plus}}$     | 0.0002            | per projection | double   | \n| $A_{\\text{minus}}$    | 6.6e-05           | per projection | double   | \n| $w_{\\text{min}}$      | 0.0               | per projection | double   | \n| $w_{\\text{max}}$      | 0.03              | per projection | double   | \n| $\\alpha$              | 0.1               | per projection | double   | \n| $\\beta$               | 1.0               | per projection | double   | \n| $\\gamma$              | 50.0              | per projection | double   | \n| ${\\text{Rtarget}}$    | 35.0              | per projection | double   | \n| $T$                   | 5000.0            | per projection | double   | \n\n**Equations:**\n\n* Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n$$\n\n* Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n$$\n\n* Variable $R$ : per post-synaptic neuron, initial value: 0.0\n\n$$\n{R}(t) = {r}^{\\text{post}}(t)\n$$\n\n* Variable $K$ : per post-synaptic neuron, initial value: 0.0\n\n$$\n{K}(t) = \\frac{{R}(t)}{T \\cdot \\left(\\gamma \\cdot \\left|{f}\\right|{\\left (- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1.0 \\right )} + 1.0\\right)}\n$$\n\n* Variable ${\\text{stdp}}$ : per synapse, initial value: 0.0\n\n$$\n{{\\text{stdp}}}(t) = \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n$$\n\n* Variable $w$ : per synapse, initial value: 0.0, minimum: w_min, maximum: w_max\n\n$$\n{w}(t) \\mathrel{+}= {K}(t) \\cdot \\left(\\alpha \\cdot {w}(t) \\cdot \\left(- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1\\right) + \\beta \\cdot {{\\text{stdp}}}(t)\\right)\n$$\n\n**Pre-synaptic event at $t_\\text{pre} + d$:**\n$$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$\n$${{\\text{ltp}}}(t) = A_{\\text{plus}}$$\n\n**Post-synaptic event at $t_\\text{post}$:**\n$${{\\text{ltd}}}(t) = A_{\\text{minus}}$$\n\n# Parameters\n\n## Population parameters\n\n| **Population**                | **Neuron type**            | **Name**             | **Value**     | \n| ----------------------------- | -------------------------- | -------------------- | ------------- | \n| Poisson input                 | Poisson                    | ${\\text{rates}}$     | $[0.2, 20.0]$ | \n| RS neuron without homeostasis | Regular-spiking Izhikevich | $a$                  | 0.02          | \n|                               |                            | $b$                  | 0.2           | \n|                               |                            | $c$                  | -65.0         | \n|                               |                            | $d$                  | 8.0           | \n|                               |                            | $\\tau_{\\text{ampa}}$ | 5.0           | \n|                               |                            | $\\tau_{\\text{nmda}}$ | 150.0         | \n|                               |                            | ${\\text{vrev}}$      | 0.0           | \n| RS neuron with homeostasis    | Regular-spiking Izhikevich | $a$                  | 0.02          | \n|                               |                            | $b$                  | 0.2           | \n|                               |                            | $c$                  | -65.0         | \n|                               |                            | $d$                  | 8.0           | \n|                               |                            | $\\tau_{\\text{ampa}}$ | 5.0           | \n|                               |                            | $\\tau_{\\text{nmda}}$ | 150.0         | \n|                               |                            | ${\\text{vrev}}$      | 0.0           | \n\n\n\n## Projection parameters\n\n| **Projection**                                                                     | **Synapse type**                        | **Name**              | **Value** | \n| ---------------------------------------------------------------------------------- | --------------------------------------- | --------------------- | --------- | \n| Poisson input  $\\rightarrow$ RS neuron without homeostasis with target ampa / nmda | Nearest-neighbour STDP                  | $\\tau_{\\text{plus}}$  | 20.0      | \n|                                                                                    |                                         | $\\tau_{\\text{minus}}$ | 60.0      | \n|                                                                                    |                                         | $A_{\\text{plus}}$     | 0.0002    | \n|                                                                                    |                                         | $A_{\\text{minus}}$    | 6.6e-05   | \n|                                                                                    |                                         | $w_{\\text{max}}$      | 0.03      | \n| Poisson input  $\\rightarrow$ RS neuron with homeostasis with target ampa / nmda    | Nearest-neighbour STDP with homeostasis | $\\tau_{\\text{plus}}$  | 20.0      | \n|                                                                                    |                                         | $\\tau_{\\text{minus}}$ | 60.0      | \n|                                                                                    |                                         | $A_{\\text{plus}}$     | 0.0002    | \n|                                                                                    |                                         | $A_{\\text{minus}}$    | 6.6e-05   | \n|                                                                                    |                                         | $w_{\\text{min}}$      | 0.0       | \n|                                                                                    |                                         | $w_{\\text{max}}$      | 0.03      | \n|                                                                                    |                                         | $\\alpha$              | 0.1       | \n|                                                                                    |                                         | $\\beta$               | 1.0       | \n|                                                                                    |                                         | $\\gamma$              | 50.0      | \n|                                                                                    |                                         | ${\\text{Rtarget}}$    | 35.0      | \n|                                                                                    |                                         | $T$                   | 5000.0    |",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Reporting"
    ]
  },
  {
    "objectID": "manual/Parser.html",
    "href": "manual/Parser.html",
    "title": "Parser",
    "section": "",
    "text": "A Neuron or Synapse type is primarily defined by two sets of values which must be specified in its constructor:\n\nParameters are values such as time constants which are constant during the simulation. They can be the same throughout the population/projection, or take different values.\nVariables are neuronal variables (for example the membrane potential or firing rate) or synaptic variables (the synaptic efficiency) whose value evolve with time during the simulation. The equation (whether it is an ordinary differential equation or not) ruling their evolution can be described using a specific meta-language.\n\n\n\nParameters are defined by a multi-string consisting of one or more parameter definitions:\nparameters = \"\"\"\n    tau = 10.0\n    eta = 0.5\n\"\"\"\nEach parameter should be defined on a single line, with its name on the left side of the equal sign, and its value on the right side. The given value corresponds to the initial value of the parameter (but it can be changed at any further point of the simulation).\nAs a neuron/synapse type is likely to be reused in different populations/projections, it is good practice to set reasonable initial values in the neuron/synapse type, and eventually adapt them to the corresponding populations/projections later on.\nLocal vs. global parameters\nBy default, a neural parameter will be unique to each neuron (i.e. each neuron instance will hold a copy of the parameter) or synapse. In order to save memory space, one can force ANNarchy to store only one parameter value for a whole population by specifying the population flag after a : symbol following the parameter definition:\nparameters = \"\"\"\n    tau = 10.0\n    eta = 0.5 : population\n\"\"\"\nIn this case, there will be only only one instance of the eta parameter for the whole population. eta is called a global parameter, in opposition to local parameters which are the default.\nThe same is true for synapses, whose parameters are by default unique to each synapse in a given projection. If the post-synaptic flag is passed, the parameter will be common to all synapses of a post-synaptic neuron, but can differ from one post-synaptic neuron to another. If the projection flag is passed, the parameter will be common to all synapses of a projection (e.g. the learning rate).\nType of the variable\nParameters have floating-point precision by default. If you want to force the parameter to be an integer or boolean, you can also pass the int and bool flags, separated by commas:\nparameters = \"\"\"\n    tau = 10.0\n    eta = 1 : population, int\n\"\"\"\nConstants\nAlternatively, it is possible to use constants in the parameter definition (see later):\ntau_exc = ann.Constant('tau_exc', 10.0)\n\nneuron = ann.Neuron(\n    parameters = \"\"\"\n        tau = tau_exc\n    \"\"\",\n)\nThe advantage of this method is that if a parameter value is \"shared\" across several neuron/synapse types, you only need to change the value once, instead of in each neuron/synapse definition.\n\n\n\nTime-varying variables are also defined using a multi-line description:\nequations = \"\"\"\n    noise = Uniform(0.0, 0.2)\n    tau * dv/dt  + v = baseline + sum(exc) + noise\n    r = pos(v)\n\"\"\"\nThe evolution of each variable with time can be described through a simple equation or an ordinary differential equation (ODE). ANNarchy provides a simple parser for mathematical expressions, whose role is to translate a high-level description of the equation into an optimized C++ code snippet.\nThe equation for one variable can depend on parameters, other variables (even when declared later) or constants. Variables are updated in the same order as their declaration in the multistring (see Numerical methods, as it influences how ODEs are solved).\nThe declaration of a single variable can extend on multiple lines:\nequations = \"\"\"\n    noise = Uniform(0.0, 0.2)\n    tau * dv/dt  = baseline - v\n                    + sum(exc) + noise : max = 1.0\n    r = pos(v)\n\"\"\"\nAs it is only a parser and not a solver, some limitations exist:\n\nSimple equations must hold only the name of the variable on the left sign of the equation. Variable definitions such as r + v = noise are forbidden, as it would be impossible to guess which variable should be updated.\nODEs are more free regarding the left side, but only one variable should hold the gradient: the one which will be updated. The following definitions are equivalent and will lead to the same C++ code:\n\ntau * dv/dt  = baseline - v\n\ntau * dv/dt  + v = baseline\n\ntau * dv/dt  + v -  baseline = 0\n\ndv/dt  = (baseline - v) / tau\nIn practice, ODEs are transformed using Sympy into the last form (only the gradient stays on the left) and numerized using the chosen numerical method (see Numerical methods).\n\n\nLocality and type\nLike the parameters, variables also accept the population, postsynaptic and projection flags to define the local/global character of the variable, as well as the int or bool flags for their type.\nInitial value\nThe initial value of the variable (before the first simulation starts) can also be specified using the init keyword followed by the desired value:\nequations = \"\"\"\n    tau * dv/dt + v = baseline : init = 0.2\n\"\"\"\nIt must be a single value (the same for all neurons in the population or all synapses in the projection) and should not depend on other parameters and variables. This initial value can be specifically changed after the Population or Projection objects are created (see Populations).\nIt is also possible to use constants for the initial value:\ninit_v = Constant('init_v', 0.2)\n\nneuron = Neuron(\n    equations = \"\"\"\n        tau * dv/dt + v = baseline : init = init_v\n    \"\"\",\n)\nMin and Max values of a variable\nUpper- and lower-bounds can be set using the min and max keywords:\nequations = \"\"\"\n    tau * dv/dt  + v = baseline : min = -0.2, max = 1.0\n\"\"\"\nAt each step of the simulation, after the update rule is calculated for v, the new value will be compared to the min and max value, and clamped if necessary.\nmin and max can be single values, constants, parameters, variables or functions of all these:\nparameters = \"\"\"\n    tau = 10.0\n    min_v = -1.0 : population\n    max_v = 1.0\n\"\"\",\nequations = \"\"\"\n    variance = Uniform(0.0, 1.0)\n    tau * dv/dt  + v = sum(exc) : min = min_v, max = max_v + variance\n    r = v : min = 0.0 # Equivalent to r = pos(mp)\n\"\"\"\nNumerical method\nThe numerization method for a single ODEs can be explicitely set by specifying a flag:\ntau * dv/dt  + v = sum(exc) : exponential\nThe available numerical methods are described in Numerical methods.\nSummary of allowed flags for variables:\n\ninit: defines the initialization value at begin of simulation and after a network reset (default: 0.0)\nmin: minimum allowed value (unset by default)\nmax: maximum allowed value (unset by default)\npopulation: the attribute is shared by all neurons of a population.\npostsynaptic: the attribute is shared by all synapses of a post-synaptic neuron.\nprojection: the attribute is shared by all synapses of a projection.\nexplicit, implicit, exponential, midpoint, event-driven: the numerical method to be used.\n\n\n\n\n\nGlobal constants can be created by the user and used inside any equation. They must define an unique name and a floating point value:\ntau = ann.Constant('tau', 10.0)\n\nneuron = ann.Neuron(\n    equations = \"tau * dr/dt + r = sum(exc)\"\n)\nIn this example, a Neuron or Synapse does not have to define the parameter tau to use it: it is available everywhere. If the Neuron/Synapse redefines a parameter called tau, the constant is not visible anymore to that object.\nConstants can be manipulated as normal floats to define complex values:\ntau = ann.Constant('tau', 20)\nfactor = ann.Constant('factor', 0.1)\nreal_tau = ann.Constant('real_tau', tau*factor)\n\nneuron = ann.Neuron(\n    equations='''\n        real_tau*dr/dt + r =1.0\n    '''\n)\nNote that constants are only global, changing their value impacts all objects using them. Changing the value of a constant can only be done through the set() method (before or after compile()):\ntau = ann.Constant('tau', 20)\ntau.set(10.0)\n\n\n\nThe mathematical parser relies heavily on the one provided by SymPy.\n\n\nAll parameters and variables use implicitly the floating-point double precision, except when stated otherwise with the int or bool keywords. You can use numerical constants within the equation, noting that they will be automatically converted to this precision:\ntau * dv / dt  = 1 / pos(v) + 1\nThe constant \\pi is available under the literal form pi.\n\n\n\n\nAdditions (+), substractions (-), multiplications (*), divisions (/) and power functions (^) are of course allowed.\nGradients are allowed only for the variable currently described. They take the form:\n\ndv / dt  = A\nwith a d preceding the variable’s name and terminated by /dt (with or without spaces). Gradients must be on the left side of the equation.\n\nTo update the value of a variable at each time step, the operators =, +=, -=, *=, and /= are allowed.\n\n\n\n\nAny parameter or variable defined in the same Neuron/Synapse can be used inside an equation. User-defined constants can also be used. Additionally, the following variables are pre-defined:\n\ndt : the discretization time step for the simulation. Using this variable, you can define the numerical method by yourself. For example:\n\ntau * dv / dt  + v = baseline\nwith backward Euler would be equivalent to:\nv += dt/tau * (baseline - v)\n\nt : the time in milliseconds elapsed since the creation of the network. This allows to generate oscillating variables:\n\nf = 10.0 # Frequency of 10 Hz\nphi = pi/4 # Phase\nts = t / 1000.0 # ts is in seconds\nr = 10.0 * (sin(2*pi*f*ts + phi) + 1.0)\n\n\n\nSeveral random generators are available and can be used within an equation, for example:\n\nUniform(min, max) generates random numbers from a uniform distribution in the range [\\text{min}, \\text{max}].\nNormal(mu, sigma) generates random numbers from a normal distribution with mean mu and standard deviation sigma.\n\nSee the reference for Random Distributions for more distributions. For example:\nnoise = Uniform(-0.5, 0.5)\nThe arguments to the random distributions can be either fixed values or (functions of) global parameters.\nmin_val = -0.5 : population\nmax_val = 0.5 : population\nnoise = Uniform(min_val, max_val)\nIt is not allowed to use local parameters (with different values per neuron) or variables, as the random number generators are initialized only once at network creation (doing otherwise would impair performance too much). If a global parameter is used, changing its value will not affect the generator after compilation.\nIt is therefore better practice to use normalized random generators and scale their outputs:\nmin_val = -0.5 : population\nmax_val = 0.5 : population\nnoise = min_val + (max_val - min_val) * Uniform(0.0, 1.0)\n\n\n\n\nMost mathematical functions of the cmath library are understood by the parser, for example:\n\ncos, sin, tan, acos, asin, atan, exp, abs, fabs, sqrt, log, ln\n\nThe positive and negative parts of a term are also defined, with short and long versions:\n\nr = pos(mp)\nr = positive(mp)\nr = neg(mp)\nr = negative(mp)\n\nA piecewise linear function is also provided (linear when x is between a and b, saturated at a or b otherwise):\n\nr = clip(x, a, b)\n\nFor integer variables, the modulo operator is defined:\n\nx += 1 : int\ny = modulo(x, 10)\n\nWhen using the power function (r = x^2 or r = pow(x, 2)), the cmath pow(double, int) method is used. For small exponents (quadratic or cubic functions), it can be extremely slow, compared to r = x*x or r = x*x*x. Unfortunately, Sympy transforms automatically r = x*x into r = pow(x, 2). We therefore advise to use the built-in power(double, int) function instead:\n\nr = power(x, 3)\nThese functions must be followed by a set of matching brackets:\ntau * dmp / dt + mp = exp( - cos(2*pi*f*t + pi/4 ) + 1)\n\n\n\nPython-style\nIt is possible to use Python-style conditional statements as the right term of an equation or ODE. They follow the form:\nif condition : statement1 else : statement2\nFor example, to define a piecewise linear function, you can nest different conditionals:\nr = if mp &lt; 1. :\n        if mp &gt; 0.:\n            mp\n        else:\n            0.\n    else:\n        1.\nwhich is equivalent to:\nr = clip(mp, 0.0, 1.0)\nThe condition can use the following vocabulary:\nTrue, False, and, or, not, is, is not, ==, !=, &gt;, &lt;, &gt;=, &lt;=\n\n\n\n\n\n\nNote\n\n\n\nThe and, or and not logical operators must be used with parentheses around their terms. Example:\nvar = if (mp &gt; 0) and ( (noise &lt; 0.1) or (not(condition)) ):\n            1.0\n        else:\n            0.0\nis is equivalent to ==, is not is equivalent to !=.\n\n\nWhen a conditional statement is split over multiple lines, the flags must be set after the last line:\nrate = if mp &lt; 1.0 :\n          if mp &lt; 0.0 :\n              0.0\n          else:\n              mp\n       else:\n          1.0 : init = 0.6\nAn if a: b else:c statement must be exactly the right term of an equation. It is for example NOT possible to write:\nr = 1.0 + (if mp&gt; 0.0: mp else: 0.0) + b\nTernary operator\nThe ternary operator ite(cond, then, else) (ite stands for if-then-else) is available to ease the combination of conditionals with other terms:\nr = ite(mp&gt;0.0, mp, 0.0)\n# is exactly the same as:\nr = if mp &gt; 0.0: mp else: 0.0\nThe advantage is that the conditional term is not restricted to the right term of the equation, and can be used multiple times:\nr = ite(mp &gt; 0.0, ite(mp &lt; 1.0, mp, 1.0), 0.0) + ite(stimulated, 1.0, 0.0)\n\n\n\n\nTo simplify the writing of equations, custom functions can be defined either globally (usable by all neurons and synapses) or locally (only for the particular type of neuron/synapse) using the same mathematical parser.\nGlobal functions can be defined using the add_function() method:\nadd_function('sigmoid(x) = 1.0 / (1.0 + exp(-x))')\nWith this declaration, sigmoid() can be used in the declaration of any variable, for example:\nneuron = ann.Neuron(\n    equations = \"\"\"\n        r = sigmoid(sum(exc))\n    \"\"\"\n)\nFunctions must be one-liners, i.e. they should have only one return value. They can use as many arguments as needed, but are totally unaware of the context: all the needed information should be passed as an argument (except constants which are visible to the function).\nThe types of the arguments (including the return value) are by default floating-point. If other types should be used, they should be specified at the end of the definition, after the : sign, with the type of the return value first, followed by the type of all arguments separated by commas:\nann.add_function('conditional_increment(c, v, t) = if v &gt; t : c + 1 else: c : int, int, float, float')\nAfter compilation, the function can be called using arbitrary list of values for the arguments using the functions() method and the name of the function:\nann.add_function('sigmoid(x) = 1.0 / (1.0 + exp(-x))')\n\ncompile()\n\nx = np.linspace(-10., 10., 1000)\ny = ann.functions('sigmoid')(x)\nYou can pass a list or a 1D Numpy array as argument, but not a single value or a multidimensional array. When several arguemnts are passed, they must have the same size.\nLocal functions are specific to a Neuron or Synapse class and can only be used within this context (if they have the same name as global variables, they will override them). They can be passed as a multi-line argument to the constructor of a neuron or synapse (see later):\nfunctions == \"\"\"\n    sigmoid(x) = 1.0 / (1.0 + exp(-x))\n    conditional_increment(c, v, t) = if v &gt; t : c + 1 else: c : int, int, float, float\n\"\"\"",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Parser"
    ]
  },
  {
    "objectID": "manual/Parser.html#parameters",
    "href": "manual/Parser.html#parameters",
    "title": "Parser",
    "section": "",
    "text": "Parameters are defined by a multi-string consisting of one or more parameter definitions:\nparameters = \"\"\"\n    tau = 10.0\n    eta = 0.5\n\"\"\"\nEach parameter should be defined on a single line, with its name on the left side of the equal sign, and its value on the right side. The given value corresponds to the initial value of the parameter (but it can be changed at any further point of the simulation).\nAs a neuron/synapse type is likely to be reused in different populations/projections, it is good practice to set reasonable initial values in the neuron/synapse type, and eventually adapt them to the corresponding populations/projections later on.\nLocal vs. global parameters\nBy default, a neural parameter will be unique to each neuron (i.e. each neuron instance will hold a copy of the parameter) or synapse. In order to save memory space, one can force ANNarchy to store only one parameter value for a whole population by specifying the population flag after a : symbol following the parameter definition:\nparameters = \"\"\"\n    tau = 10.0\n    eta = 0.5 : population\n\"\"\"\nIn this case, there will be only only one instance of the eta parameter for the whole population. eta is called a global parameter, in opposition to local parameters which are the default.\nThe same is true for synapses, whose parameters are by default unique to each synapse in a given projection. If the post-synaptic flag is passed, the parameter will be common to all synapses of a post-synaptic neuron, but can differ from one post-synaptic neuron to another. If the projection flag is passed, the parameter will be common to all synapses of a projection (e.g. the learning rate).\nType of the variable\nParameters have floating-point precision by default. If you want to force the parameter to be an integer or boolean, you can also pass the int and bool flags, separated by commas:\nparameters = \"\"\"\n    tau = 10.0\n    eta = 1 : population, int\n\"\"\"\nConstants\nAlternatively, it is possible to use constants in the parameter definition (see later):\ntau_exc = ann.Constant('tau_exc', 10.0)\n\nneuron = ann.Neuron(\n    parameters = \"\"\"\n        tau = tau_exc\n    \"\"\",\n)\nThe advantage of this method is that if a parameter value is \"shared\" across several neuron/synapse types, you only need to change the value once, instead of in each neuron/synapse definition.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Parser"
    ]
  },
  {
    "objectID": "manual/Parser.html#variables",
    "href": "manual/Parser.html#variables",
    "title": "Parser",
    "section": "",
    "text": "Time-varying variables are also defined using a multi-line description:\nequations = \"\"\"\n    noise = Uniform(0.0, 0.2)\n    tau * dv/dt  + v = baseline + sum(exc) + noise\n    r = pos(v)\n\"\"\"\nThe evolution of each variable with time can be described through a simple equation or an ordinary differential equation (ODE). ANNarchy provides a simple parser for mathematical expressions, whose role is to translate a high-level description of the equation into an optimized C++ code snippet.\nThe equation for one variable can depend on parameters, other variables (even when declared later) or constants. Variables are updated in the same order as their declaration in the multistring (see Numerical methods, as it influences how ODEs are solved).\nThe declaration of a single variable can extend on multiple lines:\nequations = \"\"\"\n    noise = Uniform(0.0, 0.2)\n    tau * dv/dt  = baseline - v\n                    + sum(exc) + noise : max = 1.0\n    r = pos(v)\n\"\"\"\nAs it is only a parser and not a solver, some limitations exist:\n\nSimple equations must hold only the name of the variable on the left sign of the equation. Variable definitions such as r + v = noise are forbidden, as it would be impossible to guess which variable should be updated.\nODEs are more free regarding the left side, but only one variable should hold the gradient: the one which will be updated. The following definitions are equivalent and will lead to the same C++ code:\n\ntau * dv/dt  = baseline - v\n\ntau * dv/dt  + v = baseline\n\ntau * dv/dt  + v -  baseline = 0\n\ndv/dt  = (baseline - v) / tau\nIn practice, ODEs are transformed using Sympy into the last form (only the gradient stays on the left) and numerized using the chosen numerical method (see Numerical methods).\n\n\nLocality and type\nLike the parameters, variables also accept the population, postsynaptic and projection flags to define the local/global character of the variable, as well as the int or bool flags for their type.\nInitial value\nThe initial value of the variable (before the first simulation starts) can also be specified using the init keyword followed by the desired value:\nequations = \"\"\"\n    tau * dv/dt + v = baseline : init = 0.2\n\"\"\"\nIt must be a single value (the same for all neurons in the population or all synapses in the projection) and should not depend on other parameters and variables. This initial value can be specifically changed after the Population or Projection objects are created (see Populations).\nIt is also possible to use constants for the initial value:\ninit_v = Constant('init_v', 0.2)\n\nneuron = Neuron(\n    equations = \"\"\"\n        tau * dv/dt + v = baseline : init = init_v\n    \"\"\",\n)\nMin and Max values of a variable\nUpper- and lower-bounds can be set using the min and max keywords:\nequations = \"\"\"\n    tau * dv/dt  + v = baseline : min = -0.2, max = 1.0\n\"\"\"\nAt each step of the simulation, after the update rule is calculated for v, the new value will be compared to the min and max value, and clamped if necessary.\nmin and max can be single values, constants, parameters, variables or functions of all these:\nparameters = \"\"\"\n    tau = 10.0\n    min_v = -1.0 : population\n    max_v = 1.0\n\"\"\",\nequations = \"\"\"\n    variance = Uniform(0.0, 1.0)\n    tau * dv/dt  + v = sum(exc) : min = min_v, max = max_v + variance\n    r = v : min = 0.0 # Equivalent to r = pos(mp)\n\"\"\"\nNumerical method\nThe numerization method for a single ODEs can be explicitely set by specifying a flag:\ntau * dv/dt  + v = sum(exc) : exponential\nThe available numerical methods are described in Numerical methods.\nSummary of allowed flags for variables:\n\ninit: defines the initialization value at begin of simulation and after a network reset (default: 0.0)\nmin: minimum allowed value (unset by default)\nmax: maximum allowed value (unset by default)\npopulation: the attribute is shared by all neurons of a population.\npostsynaptic: the attribute is shared by all synapses of a post-synaptic neuron.\nprojection: the attribute is shared by all synapses of a projection.\nexplicit, implicit, exponential, midpoint, event-driven: the numerical method to be used.",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Parser"
    ]
  },
  {
    "objectID": "manual/Parser.html#constants",
    "href": "manual/Parser.html#constants",
    "title": "Parser",
    "section": "",
    "text": "Global constants can be created by the user and used inside any equation. They must define an unique name and a floating point value:\ntau = ann.Constant('tau', 10.0)\n\nneuron = ann.Neuron(\n    equations = \"tau * dr/dt + r = sum(exc)\"\n)\nIn this example, a Neuron or Synapse does not have to define the parameter tau to use it: it is available everywhere. If the Neuron/Synapse redefines a parameter called tau, the constant is not visible anymore to that object.\nConstants can be manipulated as normal floats to define complex values:\ntau = ann.Constant('tau', 20)\nfactor = ann.Constant('factor', 0.1)\nreal_tau = ann.Constant('real_tau', tau*factor)\n\nneuron = ann.Neuron(\n    equations='''\n        real_tau*dr/dt + r =1.0\n    '''\n)\nNote that constants are only global, changing their value impacts all objects using them. Changing the value of a constant can only be done through the set() method (before or after compile()):\ntau = ann.Constant('tau', 20)\ntau.set(10.0)",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Parser"
    ]
  },
  {
    "objectID": "manual/Parser.html#allowed-vocabulary",
    "href": "manual/Parser.html#allowed-vocabulary",
    "title": "Parser",
    "section": "",
    "text": "The mathematical parser relies heavily on the one provided by SymPy.\n\n\nAll parameters and variables use implicitly the floating-point double precision, except when stated otherwise with the int or bool keywords. You can use numerical constants within the equation, noting that they will be automatically converted to this precision:\ntau * dv / dt  = 1 / pos(v) + 1\nThe constant \\pi is available under the literal form pi.\n\n\n\n\nAdditions (+), substractions (-), multiplications (*), divisions (/) and power functions (^) are of course allowed.\nGradients are allowed only for the variable currently described. They take the form:\n\ndv / dt  = A\nwith a d preceding the variable’s name and terminated by /dt (with or without spaces). Gradients must be on the left side of the equation.\n\nTo update the value of a variable at each time step, the operators =, +=, -=, *=, and /= are allowed.\n\n\n\n\nAny parameter or variable defined in the same Neuron/Synapse can be used inside an equation. User-defined constants can also be used. Additionally, the following variables are pre-defined:\n\ndt : the discretization time step for the simulation. Using this variable, you can define the numerical method by yourself. For example:\n\ntau * dv / dt  + v = baseline\nwith backward Euler would be equivalent to:\nv += dt/tau * (baseline - v)\n\nt : the time in milliseconds elapsed since the creation of the network. This allows to generate oscillating variables:\n\nf = 10.0 # Frequency of 10 Hz\nphi = pi/4 # Phase\nts = t / 1000.0 # ts is in seconds\nr = 10.0 * (sin(2*pi*f*ts + phi) + 1.0)\n\n\n\nSeveral random generators are available and can be used within an equation, for example:\n\nUniform(min, max) generates random numbers from a uniform distribution in the range [\\text{min}, \\text{max}].\nNormal(mu, sigma) generates random numbers from a normal distribution with mean mu and standard deviation sigma.\n\nSee the reference for Random Distributions for more distributions. For example:\nnoise = Uniform(-0.5, 0.5)\nThe arguments to the random distributions can be either fixed values or (functions of) global parameters.\nmin_val = -0.5 : population\nmax_val = 0.5 : population\nnoise = Uniform(min_val, max_val)\nIt is not allowed to use local parameters (with different values per neuron) or variables, as the random number generators are initialized only once at network creation (doing otherwise would impair performance too much). If a global parameter is used, changing its value will not affect the generator after compilation.\nIt is therefore better practice to use normalized random generators and scale their outputs:\nmin_val = -0.5 : population\nmax_val = 0.5 : population\nnoise = min_val + (max_val - min_val) * Uniform(0.0, 1.0)\n\n\n\n\nMost mathematical functions of the cmath library are understood by the parser, for example:\n\ncos, sin, tan, acos, asin, atan, exp, abs, fabs, sqrt, log, ln\n\nThe positive and negative parts of a term are also defined, with short and long versions:\n\nr = pos(mp)\nr = positive(mp)\nr = neg(mp)\nr = negative(mp)\n\nA piecewise linear function is also provided (linear when x is between a and b, saturated at a or b otherwise):\n\nr = clip(x, a, b)\n\nFor integer variables, the modulo operator is defined:\n\nx += 1 : int\ny = modulo(x, 10)\n\nWhen using the power function (r = x^2 or r = pow(x, 2)), the cmath pow(double, int) method is used. For small exponents (quadratic or cubic functions), it can be extremely slow, compared to r = x*x or r = x*x*x. Unfortunately, Sympy transforms automatically r = x*x into r = pow(x, 2). We therefore advise to use the built-in power(double, int) function instead:\n\nr = power(x, 3)\nThese functions must be followed by a set of matching brackets:\ntau * dmp / dt + mp = exp( - cos(2*pi*f*t + pi/4 ) + 1)\n\n\n\nPython-style\nIt is possible to use Python-style conditional statements as the right term of an equation or ODE. They follow the form:\nif condition : statement1 else : statement2\nFor example, to define a piecewise linear function, you can nest different conditionals:\nr = if mp &lt; 1. :\n        if mp &gt; 0.:\n            mp\n        else:\n            0.\n    else:\n        1.\nwhich is equivalent to:\nr = clip(mp, 0.0, 1.0)\nThe condition can use the following vocabulary:\nTrue, False, and, or, not, is, is not, ==, !=, &gt;, &lt;, &gt;=, &lt;=\n\n\n\n\n\n\nNote\n\n\n\nThe and, or and not logical operators must be used with parentheses around their terms. Example:\nvar = if (mp &gt; 0) and ( (noise &lt; 0.1) or (not(condition)) ):\n            1.0\n        else:\n            0.0\nis is equivalent to ==, is not is equivalent to !=.\n\n\nWhen a conditional statement is split over multiple lines, the flags must be set after the last line:\nrate = if mp &lt; 1.0 :\n          if mp &lt; 0.0 :\n              0.0\n          else:\n              mp\n       else:\n          1.0 : init = 0.6\nAn if a: b else:c statement must be exactly the right term of an equation. It is for example NOT possible to write:\nr = 1.0 + (if mp&gt; 0.0: mp else: 0.0) + b\nTernary operator\nThe ternary operator ite(cond, then, else) (ite stands for if-then-else) is available to ease the combination of conditionals with other terms:\nr = ite(mp&gt;0.0, mp, 0.0)\n# is exactly the same as:\nr = if mp &gt; 0.0: mp else: 0.0\nThe advantage is that the conditional term is not restricted to the right term of the equation, and can be used multiple times:\nr = ite(mp &gt; 0.0, ite(mp &lt; 1.0, mp, 1.0), 0.0) + ite(stimulated, 1.0, 0.0)",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Parser"
    ]
  },
  {
    "objectID": "manual/Parser.html#custom-functions",
    "href": "manual/Parser.html#custom-functions",
    "title": "Parser",
    "section": "",
    "text": "To simplify the writing of equations, custom functions can be defined either globally (usable by all neurons and synapses) or locally (only for the particular type of neuron/synapse) using the same mathematical parser.\nGlobal functions can be defined using the add_function() method:\nadd_function('sigmoid(x) = 1.0 / (1.0 + exp(-x))')\nWith this declaration, sigmoid() can be used in the declaration of any variable, for example:\nneuron = ann.Neuron(\n    equations = \"\"\"\n        r = sigmoid(sum(exc))\n    \"\"\"\n)\nFunctions must be one-liners, i.e. they should have only one return value. They can use as many arguments as needed, but are totally unaware of the context: all the needed information should be passed as an argument (except constants which are visible to the function).\nThe types of the arguments (including the return value) are by default floating-point. If other types should be used, they should be specified at the end of the definition, after the : sign, with the type of the return value first, followed by the type of all arguments separated by commas:\nann.add_function('conditional_increment(c, v, t) = if v &gt; t : c + 1 else: c : int, int, float, float')\nAfter compilation, the function can be called using arbitrary list of values for the arguments using the functions() method and the name of the function:\nann.add_function('sigmoid(x) = 1.0 / (1.0 + exp(-x))')\n\ncompile()\n\nx = np.linspace(-10., 10., 1000)\ny = ann.functions('sigmoid')(x)\nYou can pass a list or a 1D Numpy array as argument, but not a single value or a multidimensional array. When several arguemnts are passed, they must have the same size.\nLocal functions are specific to a Neuron or Synapse class and can only be used within this context (if they have the same name as global variables, they will override them). They can be passed as a multi-line argument to the constructor of a neuron or synapse (see later):\nfunctions == \"\"\"\n    sigmoid(x) = 1.0 / (1.0 + exp(-x))\n    conditional_increment(c, v, t) = if v &gt; t : c + 1 else: c : int, int, float, float\n\"\"\"",
    "crumbs": [
      "Manual",
      "**Model structure**",
      "Parser"
    ]
  },
  {
    "objectID": "manual/index.html",
    "href": "manual/index.html",
    "title": "Manual",
    "section": "",
    "text": "Manual"
  },
  {
    "objectID": "manual/Network.html",
    "href": "manual/Network.html",
    "title": "Parallel simulations and networks",
    "section": "",
    "text": "A typical ANNarchy script represents a single network of populations and projections. Most of the work in computational neuroscience consists in running the same network again and again, varying some free parameters each time, until the fit to the data is publishable. The reset() allows to return the network to its state before compilation, but this is particularly tedious to implement.\nIn order to run different networks using the same script, the Network object can be used to make copies of existing objects (populations, projections and monitors) and simulate them in parallel using parallel_run().\nLet’s suppose the following dummy network is defined:\npop1 = ann.PoissonPopulation(100, rates=10.0)\npop2 = ann.Population(100, Izhikevich)\nproj = ann.Projection(pop1, pop2, 'exc')\nproj.connect_fixed_probability(weights=5.0, probability=0.2)\nm = ann.Monitor(pop2, 'spike')\n\nann.compile()\nOne would like to compare the firing patterns in pop2 when:\n\nThere is no input to pop2.\nThe Poisson input is at 10 Hz.\nThe Poisson input is at 20 Hz.\n\n\n\n\n\n\n\nNote\n\n\n\nparallel_run() uses the multiprocessing module to start parallel processes. On Linux, it should work directly, but there is an issue on OSX. Since Python 3.8, the ‘spawn’ method is the default way to start processes, but it does not work on MacOS. The following code should fix the issue, but it should only be ran once in the script.\nimport platform\nif platform.system() == \"Darwin\":\n    import multiprocessing as mp\n    mp.set_start_method('fork')\n\n\n\n\n\n\nThe most simple method is to create a single network and to iterate over some parameters values to run identical simulations multiple times using parallel_run():\npop1 = ann.PoissonPopulation(100, rates=10.0)\npop2 = ann.Population(100, Izhikevich)\nproj = ann.Projection(pop1, pop2, 'exc')\nproj.connect_fixed_probability(weights=5.0, probability=0.2)\nm = ann.Monitor(pop2, 'spike')\n\nann.compile()\n\ndef simulation(idx, net):\n    net.get(pop1).rates = 10. * idx\n    net.simulate(1000.)\n    return net.get(m).raster_plot()\n\nresults = ann.parallel_run(method=simulation, number=3)\n\nt1, n1 = results[0]\nt2, n2 = results[1]\nt3, n3 = results[2]\nThe simulation() method will be called over three copies of the network (in different processes). The first argument to this method must be an integer corresponding to the index of a network (here idx = [0, 1, 2]), the second must be a Network object (class Network).\nPopulations, projections and monitors of a network must be accessed with:\nnet.get(pop1)\nnet.get(pop2)\nnet.get(proj)\nnet.get(m)\nNetworks only work on copies of the corresponding objects at the time they are added to the network. It is no use to modify the rates parameter of pop1 after the network are created: the network is now independent. Only net.get(pop1).rates allows to change rates for the current simulation. Similarly, it is useless to read variables from the original objects when the networks are simulated: they would still have their original values.\nYou do not have access on the internally-created networks after the simulation (they are in a separate memory space). The method must the data you want to analyze (here the raster plot) or write them to disk (in separate files).\nparallel_run() returns a list of the values returned by the passed method:\nresults = parallel_run(method=simulation, networks=[net1, net2, net3])\n\nt1, n1 = results[0]\nt2, n2 = results[1]\nt3, n3 = results[2]\nIf you initialize some variables randomly in the main network, for example:\npop2.v = -60. + 10. * np.random.random(100)\nthey will have the same value in all networks, they are not drawn again. You need to perform random initialization on each network inside the simulation method:\ndef simulation(idx, net):\n    net.get(pop1).rates = 10. * idx\n    net.get(pop2).v = -60. + 10. * np.random.random(100)\n    net.simulate(1000.)\n    return net.get(m).raster_plot()\nOppositely, connection methods having a random components (e.g. connect_fixed_probability() or using weights=Uniform(0.0, 1.0)) will be redrawn for each network.\nGlobal simulation methods should not be called directly, even with the net_id parameter. The Network class overrides them:\nnet.step()\nnet.simulate()\nnet.simulate_until()\nnet.reset()\nnet.get_time()\nnet.set_time(t)\nnet.get_current_step()\nnet.set_current_step(t)\nnet.set_seed(seed)\nnet.enable_learning()\nnet.disable_learning()\nnet.get_population(name)\n\n\n\nThe two first obligatory arguments of the simulation callback are idx, the index of the network in the simulation, and net, the network object. You can of course use other names, but these two arguments will be passed.\nidx can be used for example to access arrays of parameter values:\nrates = [0.0, 0.1, 0.2, 0.3, 0.4]\ndef simulation(idx, net):\n    net.get(pop1).rates = rates[idx]\n    ...\n\nresults = ann.parallel_run(method=simulation, number=len(rates))\nAnother option is to provide additional arguments to the simulation callback during the parallel_run() call:\ndef simulation(idx, net, rates):\n    net.get(pop1).rates = rates\n    ...\n\nrates = [0.0, 0.1, 0.2, 0.3, 0.4]\nresults = ann.parallel_run(method=simulation, number=len(rates), rates=rates)\nThese additional arguments must be lists of the same size as the number of networks (number or len(networks)). You can use as many additional arguments as you want:\ndef simulation(idx, net, a, b, c, d):\n    ...\nresults = ann.parallel_run(method=simulation, number=10, a=..., b=..., c=..., d=...)\nIn parallel_run(), the arguments can be passed in any order, but they must be named (e.g. , a=list(range(0)),, not , list(range(10)),).\n\n\n\n\nOne can also create three different Network objects to implement the three conditions:\nnet1 = Network()\nnet1.add([pop2, m])\nnet1.compile()\nThe network is created empty, and the population pop2 as well as the attached monitor are added to it through the add() method. This method takes a list of objects (populations, projections and monitors).\nThe network has then to be compiled by calling the compile() method specifically on the network. The network can be simulated independently by calling simulate() or simulate_until() on the network.\nThe basic network, with inputs at 10 Hz, can be simulated directly using the normal methods, or copied into a new network:\nnet2 = Network()\nnet2.add([pop1, pop2, proj, m])\nnet2.compile()\nHere, all defined objects are added to the network. It would be easier to pass the everything argument of the Network constructor as True, which has the same effect. We can use this for the third network:\nnet3 = Network(everything=True)\nnet3.get(pop1).rates = 20.0\nnet3.compile()\nHere, the population pop1 of the third network has to be accessed though the get() method. The data corresponding to pop1 will not be the same as for net3.get(pop1), only the geometry and neuron models are the same.\nOnce a network is compiled, it can be simulated (but it does not matter if the other networks are also compiled, including the “original” network):\nnet1.simulate(1000.)\nnet2.simulate(1000.)\nnet3.simulate(1000.)\nSpike recordings have to be accessed per network, through the copies of the monitor m:\nt1, n1 = net1.get(m).raster_plot()\nt2, n2 = net2.get(m).raster_plot()\nt3, n3 = net3.get(m).raster_plot()\nOne can also call the parallel_run() method and pass it a list of networks instead of number:\nann.parallel_run(method=simulation, networks=[net1, net2, net3])\nThis will apply simulation() in parallel on the 3 networks, reducing the total computation time.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Parallel simulations and networks"
    ]
  },
  {
    "objectID": "manual/Network.html#parallel-simulation",
    "href": "manual/Network.html#parallel-simulation",
    "title": "Parallel simulations and networks",
    "section": "",
    "text": "The most simple method is to create a single network and to iterate over some parameters values to run identical simulations multiple times using parallel_run():\npop1 = ann.PoissonPopulation(100, rates=10.0)\npop2 = ann.Population(100, Izhikevich)\nproj = ann.Projection(pop1, pop2, 'exc')\nproj.connect_fixed_probability(weights=5.0, probability=0.2)\nm = ann.Monitor(pop2, 'spike')\n\nann.compile()\n\ndef simulation(idx, net):\n    net.get(pop1).rates = 10. * idx\n    net.simulate(1000.)\n    return net.get(m).raster_plot()\n\nresults = ann.parallel_run(method=simulation, number=3)\n\nt1, n1 = results[0]\nt2, n2 = results[1]\nt3, n3 = results[2]\nThe simulation() method will be called over three copies of the network (in different processes). The first argument to this method must be an integer corresponding to the index of a network (here idx = [0, 1, 2]), the second must be a Network object (class Network).\nPopulations, projections and monitors of a network must be accessed with:\nnet.get(pop1)\nnet.get(pop2)\nnet.get(proj)\nnet.get(m)\nNetworks only work on copies of the corresponding objects at the time they are added to the network. It is no use to modify the rates parameter of pop1 after the network are created: the network is now independent. Only net.get(pop1).rates allows to change rates for the current simulation. Similarly, it is useless to read variables from the original objects when the networks are simulated: they would still have their original values.\nYou do not have access on the internally-created networks after the simulation (they are in a separate memory space). The method must the data you want to analyze (here the raster plot) or write them to disk (in separate files).\nparallel_run() returns a list of the values returned by the passed method:\nresults = parallel_run(method=simulation, networks=[net1, net2, net3])\n\nt1, n1 = results[0]\nt2, n2 = results[1]\nt3, n3 = results[2]\nIf you initialize some variables randomly in the main network, for example:\npop2.v = -60. + 10. * np.random.random(100)\nthey will have the same value in all networks, they are not drawn again. You need to perform random initialization on each network inside the simulation method:\ndef simulation(idx, net):\n    net.get(pop1).rates = 10. * idx\n    net.get(pop2).v = -60. + 10. * np.random.random(100)\n    net.simulate(1000.)\n    return net.get(m).raster_plot()\nOppositely, connection methods having a random components (e.g. connect_fixed_probability() or using weights=Uniform(0.0, 1.0)) will be redrawn for each network.\nGlobal simulation methods should not be called directly, even with the net_id parameter. The Network class overrides them:\nnet.step()\nnet.simulate()\nnet.simulate_until()\nnet.reset()\nnet.get_time()\nnet.set_time(t)\nnet.get_current_step()\nnet.set_current_step(t)\nnet.set_seed(seed)\nnet.enable_learning()\nnet.disable_learning()\nnet.get_population(name)\n\n\n\nThe two first obligatory arguments of the simulation callback are idx, the index of the network in the simulation, and net, the network object. You can of course use other names, but these two arguments will be passed.\nidx can be used for example to access arrays of parameter values:\nrates = [0.0, 0.1, 0.2, 0.3, 0.4]\ndef simulation(idx, net):\n    net.get(pop1).rates = rates[idx]\n    ...\n\nresults = ann.parallel_run(method=simulation, number=len(rates))\nAnother option is to provide additional arguments to the simulation callback during the parallel_run() call:\ndef simulation(idx, net, rates):\n    net.get(pop1).rates = rates\n    ...\n\nrates = [0.0, 0.1, 0.2, 0.3, 0.4]\nresults = ann.parallel_run(method=simulation, number=len(rates), rates=rates)\nThese additional arguments must be lists of the same size as the number of networks (number or len(networks)). You can use as many additional arguments as you want:\ndef simulation(idx, net, a, b, c, d):\n    ...\nresults = ann.parallel_run(method=simulation, number=10, a=..., b=..., c=..., d=...)\nIn parallel_run(), the arguments can be passed in any order, but they must be named (e.g. , a=list(range(0)),, not , list(range(10)),).",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Parallel simulations and networks"
    ]
  },
  {
    "objectID": "manual/Network.html#multiple-network-instances",
    "href": "manual/Network.html#multiple-network-instances",
    "title": "Parallel simulations and networks",
    "section": "",
    "text": "One can also create three different Network objects to implement the three conditions:\nnet1 = Network()\nnet1.add([pop2, m])\nnet1.compile()\nThe network is created empty, and the population pop2 as well as the attached monitor are added to it through the add() method. This method takes a list of objects (populations, projections and monitors).\nThe network has then to be compiled by calling the compile() method specifically on the network. The network can be simulated independently by calling simulate() or simulate_until() on the network.\nThe basic network, with inputs at 10 Hz, can be simulated directly using the normal methods, or copied into a new network:\nnet2 = Network()\nnet2.add([pop1, pop2, proj, m])\nnet2.compile()\nHere, all defined objects are added to the network. It would be easier to pass the everything argument of the Network constructor as True, which has the same effect. We can use this for the third network:\nnet3 = Network(everything=True)\nnet3.get(pop1).rates = 20.0\nnet3.compile()\nHere, the population pop1 of the third network has to be accessed though the get() method. The data corresponding to pop1 will not be the same as for net3.get(pop1), only the geometry and neuron models are the same.\nOnce a network is compiled, it can be simulated (but it does not matter if the other networks are also compiled, including the “original” network):\nnet1.simulate(1000.)\nnet2.simulate(1000.)\nnet3.simulate(1000.)\nSpike recordings have to be accessed per network, through the copies of the monitor m:\nt1, n1 = net1.get(m).raster_plot()\nt2, n2 = net2.get(m).raster_plot()\nt3, n3 = net3.get(m).raster_plot()\nOne can also call the parallel_run() method and pass it a list of networks instead of number:\nann.parallel_run(method=simulation, networks=[net1, net2, net3])\nThis will apply simulation() in parallel on the 3 networks, reducing the total computation time.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Parallel simulations and networks"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html",
    "href": "manual/ConvolutionalNetworks.html",
    "title": "Convolution and pooling",
    "section": "",
    "text": "Projections use by default a set of weights per post-synaptic neuron. Some networks, including convolutional networks, define a single operation (convolution or pooling) to be applied systematically on all pre-synaptic neurons. In such cases, it would be a waste of resources to allocate weights for each post-synaptic neuron.\nThe extension convolution (see its API) allows to implement such specific projections. It has to be imported explicitly at the beginning of the script:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.convolution import *\n\n\n\n\n\n\nWarning\n\n\n\nShared weights are only implemented for rate-coded networks. The only possible backend is currently OpenMP, CUDA will be implemented later.\n\n\n\n\nThe simplest case of convolution is when the pre- and post-synaptic population have the same number of dimensions, for example:\npre = ann.Population(geometry=(100, 100), \n        neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = ann.Population(geometry=(100, 100), \n        neuron=ann.Neuron(equations=\"r = sum(exc)\"))\nContrary to normal projections, the geometry of the populations (number of dimensions and neurons in each dimension) has a great influence on the operation to be performed. In particular the number of dimensions will define how the convolution will be applied.\nIf for example the pre-synaptic population represents an 2D image, you may want to apply a vertical edge detector to it and get the result in the post-synaptic population. Such a filter can be defined by the following Numpy array:\nvertical_filter = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ]\n)\nWith 2 dimensions, the convolution operation (or more exactly, cross-correlation) with a 3*3 filter is defined for all neurons in the post-synaptic population by:\n\n\\text{post}[i, j] = \\sum_{c_i=-1}^1 \\sum_{c_j=-1}^1 \\text{filter}[c_i][c_j] \\cdot \\text{pre}[i + c_i, j + c_j]\n\nSuch a convolution is achieved by creating a Convolution object and using the connect_filter() method to create the connection pattern:\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filter(weights=vertical_filter)\nEach neuron of the post-synaptic population will then receive in sum('exc') (or whatever target name is used) the convolution between the kernel and a sub-region of the pre-syanptic population. ANNarchy defines the convolution operation for populations having 1, 2, 3, or 4 dimensions.\nSeveral options can be passed to the convolve() method:\n\npadding defines the value of the pre-synaptic firing rates which will be used when the coordinates are out-of-bounds. By default zero-padding is used, but you can specify another value with this argument. You can also use the 'border' value to repeat the firing rate of the neurons on the border (for example, if the filter tries to reach a neuron of coordinates (-1, -1), the firing rate of the neuron (0, 0) will be used instead).\nsubsampling. In convolutional networks, the convolution operation is often coupled with a reduction in the number of neurons in each dimension. In the example above, the post-synaptic population could be defined with a geometry (50, 50). For each post-synaptic neuron, the coordinates of the center of the applied kernel would be automatically shifted from two pre-synaptic neurons compared to the previous one. However, if the number of neurons in one dimension of the pre-synaptic population is not exactly a multiple of the number of post-synaptic neurons in the same dimension, ANNarchy can not guess what the correct correspondance should be. In this case, you have to specify this mapping by providing to the subsampling argument a list of pre-synaptic coordinates defining the position of the center of the kernel for each post-synaptic neuron. The list is indexed by the rank of the post-synaptic neurons (use the rank_from_coordinates() method) and must have the same size as the population. Each element should be a list of coordinates in the pre-synaptic population’s geometry (with as many elements as dimensions). It is possible to provide a Numpy array instead of a list of lists.\n\nOne can access the coordinates in the pre-synaptic geometry of the center of the filter corresponding to a particular post-synaptic neuron by calling the center() method of Convolution with the rank or coordinates of the post neuron:\npre = ann.Population(geometry=(100, 100), neuron = Whatever)\npost = ann.Population(geometry=(50, 50), neuron = Whatever)\n\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filter(weights=vertical_filter)\n\npre_coordinates = proj.center(10, 10) # returns (20, 20)\nIn some cases, the post-synaptic population can have less dimensions than the pre-synaptic one. An example would be when the pre-synaptic population has three dimensions (e.g. (100, 100, 3)), the last representing the R, G and B components of an image. A 3D filter, with 3 components in the last dimension, would result in a (100, 100, 1) post-synaptic population (or any subsampling of it). ANNarchy accepts in this case the use of a 2D population (100, 100), but it will be checked that the number of elements in the last dimension of the filter equals the number of pre-synaptic neurons in the last dimension:\npre = ann.Population(geometry=(100, 100, 3), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = ann.Population(geometry=(100, 100), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nred_filter = np.array(\n    [\n        [\n            [2.0, -1.0, -1.0]\n        ]\n    ]\n)\n\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filter(weights=red_filter)\n\n\n\nConvolution uses by default a regular cross-correlation, summing w * pre.r over the extent of the kernel. As for regular synapses, you can change this behavior when creating the projection:\n\nthe psp argument defines what will be summed. It is w*pre.r by default but can be changed to any combination of w and pre.r,such as w * log(1+pre.r):\nproj = Convolution(pre=pre, post=post, target='exc', psp='w*log(1+pre.r)')\nthe operation argument allows to change the summation operation. You can set it to ‘max’ (the maximum value of w*pre.r over the extent of the filter will be returned), ‘min’ (minimum) or ‘mean’ (same as ‘sum’, but normalized by the number of elements in the filter). The default is ‘sum’:\nproj = Convolution(pre=pre, post=post, target='exc', operation='max')\n\n\n\n\nIt is possible to define kernels with less dimensions than the pre-synaptic population. A 2D filter can for example be applied on each color component independently:\npre = ann.Population(geometry=(100, 100, 3), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = ann.Population(geometry=(50, 50, 3), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nvertical_filter = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ]\n)\n\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filter(weights=vertical_filter, keep_last_dimension=True)\nThe important parameter in this case is keep_last_dimension which tells the code generator that the last dimension of the input should not be used for convolution. The important constraint is that the post-synaptic population must have the same number of neurons in the last dimension than the pre-synaptic one (no subsampling is possible by definition).\n\n\n\nConvolutional networks often use banks of filters to perform different operations (such as edge detection with various orientations). It is possible to specify this mode of functioning by using the connect_filters() method:\npre = ann.Population(geometry=(100, 100), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = ann.Population(geometry=(50, 50, 4), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nbank_filters = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ],\n    [\n        [-1.0, 0.0, 1.0],\n        [-1.0, 0.0, 1.0],\n        [-1.0, 0.0, 1.0]\n    ],\n    [\n        [-1.0, -1.0, -1.0],\n        [ 0.0,  0.0,  0.0],\n        [ 1.0,  1.0,  1.0]\n    ],\n    [\n        [ 1.0,  1.0,  1.0],\n        [ 0.0,  0.0,  0.0],\n        [-1.0, -1.0, -1.0]\n    ]\n)\n\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filters(weights=bank_filters)\nHere the filter has three dimensions. The first one must correspond to each filter. The last dimension of the post-synaptic population must correspond to the total number of filters. It cannot be combined with keep_last_dimension.\n\n\n\n\n\n\nNote\n\n\n\nCurrent limitation: Each filter must have the same size, it is not possible yet to convolve over multiple scales.\n\n\n\n\n\nAnother form of atypical projection for a neural network is the pooling operation. In max-pooling, each post-synaptic neuron is associated to a region of the pre-synaptic population and responds like the maximum firing rate in this region. This is already possible by defining the operation argument of the synapse type, but it would use instantiated synapses, what would be a waste of memory.\nThe Pooling class allows to define such an operation without defining any synapse:\npre = ann.Population(geometry=(100, 100), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = ann.Population(geometry=(50, 50), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = Pooling(pre=pre, post=post, target='exc', operation='max')\nproj.connect_pooling()\nThe pooling region of a post-synaptic region is automatically determined by comparing the dimensions of the two populations: here each post-synaptic neuron will cover an area of 2*2 neurons.\nIf the number of dimensions do not match, you have to specify the extent argument to pooling(). For example, you can pool completely over one dimension of the pre-synaptic population:\npre = ann.Population(geometry=(100, 100, 10), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = ann.Population(geometry=(50, 50), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = Pooling(pre=pre, post=post, target='exc', operation='max')\nproj.connect_pooling(extent=(2, 2, 10))\n\n\n\nA different possibility to share weights is between two projections. If your network is composed of populations of the same size, and the projection patterns are identical, it could save some memory to “share” the weights of one projection with another, so they are created only once.\nTo this end, you can use the Copy class and pass it an existing projection:\npop1 = ann.Population(geometry=(30, 30), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\npop2 = ann.Population(geometry=(20, 20), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\npop3 = ann.Population(geometry=(20, 20), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\n\nproj1 = ann.Projection(pop1, pop2, 'exc')\nproj1.connect_gaussian(amp = 1.0, sigma=0.3, delays=2.0)\n\nproj2 = Copy(pop1, pop3, 'exc')\nproj2.connect_copy(proj1)\nThis only works when the pre- and post-populations of each projection have the same geometry, but they can be different, of course. If the original projection is learnable, the copied projection will see the changes. However, it is not possible for the shared projection to learn on its own. Copy only accepts psp and operation as parameters, which can be different from the original projection.\nIt is only possible to copy regular projections, not other shared projections. The transmission delays will be identical between the two projections.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html#simple-convolutions",
    "href": "manual/ConvolutionalNetworks.html#simple-convolutions",
    "title": "Convolution and pooling",
    "section": "",
    "text": "The simplest case of convolution is when the pre- and post-synaptic population have the same number of dimensions, for example:\npre = ann.Population(geometry=(100, 100), \n        neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = ann.Population(geometry=(100, 100), \n        neuron=ann.Neuron(equations=\"r = sum(exc)\"))\nContrary to normal projections, the geometry of the populations (number of dimensions and neurons in each dimension) has a great influence on the operation to be performed. In particular the number of dimensions will define how the convolution will be applied.\nIf for example the pre-synaptic population represents an 2D image, you may want to apply a vertical edge detector to it and get the result in the post-synaptic population. Such a filter can be defined by the following Numpy array:\nvertical_filter = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ]\n)\nWith 2 dimensions, the convolution operation (or more exactly, cross-correlation) with a 3*3 filter is defined for all neurons in the post-synaptic population by:\n\n\\text{post}[i, j] = \\sum_{c_i=-1}^1 \\sum_{c_j=-1}^1 \\text{filter}[c_i][c_j] \\cdot \\text{pre}[i + c_i, j + c_j]\n\nSuch a convolution is achieved by creating a Convolution object and using the connect_filter() method to create the connection pattern:\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filter(weights=vertical_filter)\nEach neuron of the post-synaptic population will then receive in sum('exc') (or whatever target name is used) the convolution between the kernel and a sub-region of the pre-syanptic population. ANNarchy defines the convolution operation for populations having 1, 2, 3, or 4 dimensions.\nSeveral options can be passed to the convolve() method:\n\npadding defines the value of the pre-synaptic firing rates which will be used when the coordinates are out-of-bounds. By default zero-padding is used, but you can specify another value with this argument. You can also use the 'border' value to repeat the firing rate of the neurons on the border (for example, if the filter tries to reach a neuron of coordinates (-1, -1), the firing rate of the neuron (0, 0) will be used instead).\nsubsampling. In convolutional networks, the convolution operation is often coupled with a reduction in the number of neurons in each dimension. In the example above, the post-synaptic population could be defined with a geometry (50, 50). For each post-synaptic neuron, the coordinates of the center of the applied kernel would be automatically shifted from two pre-synaptic neurons compared to the previous one. However, if the number of neurons in one dimension of the pre-synaptic population is not exactly a multiple of the number of post-synaptic neurons in the same dimension, ANNarchy can not guess what the correct correspondance should be. In this case, you have to specify this mapping by providing to the subsampling argument a list of pre-synaptic coordinates defining the position of the center of the kernel for each post-synaptic neuron. The list is indexed by the rank of the post-synaptic neurons (use the rank_from_coordinates() method) and must have the same size as the population. Each element should be a list of coordinates in the pre-synaptic population’s geometry (with as many elements as dimensions). It is possible to provide a Numpy array instead of a list of lists.\n\nOne can access the coordinates in the pre-synaptic geometry of the center of the filter corresponding to a particular post-synaptic neuron by calling the center() method of Convolution with the rank or coordinates of the post neuron:\npre = ann.Population(geometry=(100, 100), neuron = Whatever)\npost = ann.Population(geometry=(50, 50), neuron = Whatever)\n\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filter(weights=vertical_filter)\n\npre_coordinates = proj.center(10, 10) # returns (20, 20)\nIn some cases, the post-synaptic population can have less dimensions than the pre-synaptic one. An example would be when the pre-synaptic population has three dimensions (e.g. (100, 100, 3)), the last representing the R, G and B components of an image. A 3D filter, with 3 components in the last dimension, would result in a (100, 100, 1) post-synaptic population (or any subsampling of it). ANNarchy accepts in this case the use of a 2D population (100, 100), but it will be checked that the number of elements in the last dimension of the filter equals the number of pre-synaptic neurons in the last dimension:\npre = ann.Population(geometry=(100, 100, 3), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = ann.Population(geometry=(100, 100), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nred_filter = np.array(\n    [\n        [\n            [2.0, -1.0, -1.0]\n        ]\n    ]\n)\n\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filter(weights=red_filter)",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html#non-linear-convolutions",
    "href": "manual/ConvolutionalNetworks.html#non-linear-convolutions",
    "title": "Convolution and pooling",
    "section": "",
    "text": "Convolution uses by default a regular cross-correlation, summing w * pre.r over the extent of the kernel. As for regular synapses, you can change this behavior when creating the projection:\n\nthe psp argument defines what will be summed. It is w*pre.r by default but can be changed to any combination of w and pre.r,such as w * log(1+pre.r):\nproj = Convolution(pre=pre, post=post, target='exc', psp='w*log(1+pre.r)')\nthe operation argument allows to change the summation operation. You can set it to ‘max’ (the maximum value of w*pre.r over the extent of the filter will be returned), ‘min’ (minimum) or ‘mean’ (same as ‘sum’, but normalized by the number of elements in the filter). The default is ‘sum’:\nproj = Convolution(pre=pre, post=post, target='exc', operation='max')",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html#layer-wise-convolutions",
    "href": "manual/ConvolutionalNetworks.html#layer-wise-convolutions",
    "title": "Convolution and pooling",
    "section": "",
    "text": "It is possible to define kernels with less dimensions than the pre-synaptic population. A 2D filter can for example be applied on each color component independently:\npre = ann.Population(geometry=(100, 100, 3), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = ann.Population(geometry=(50, 50, 3), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nvertical_filter = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ]\n)\n\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filter(weights=vertical_filter, keep_last_dimension=True)\nThe important parameter in this case is keep_last_dimension which tells the code generator that the last dimension of the input should not be used for convolution. The important constraint is that the post-synaptic population must have the same number of neurons in the last dimension than the pre-synaptic one (no subsampling is possible by definition).",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html#bank-of-filters",
    "href": "manual/ConvolutionalNetworks.html#bank-of-filters",
    "title": "Convolution and pooling",
    "section": "",
    "text": "Convolutional networks often use banks of filters to perform different operations (such as edge detection with various orientations). It is possible to specify this mode of functioning by using the connect_filters() method:\npre = ann.Population(geometry=(100, 100), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = ann.Population(geometry=(50, 50, 4), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nbank_filters = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ],\n    [\n        [-1.0, 0.0, 1.0],\n        [-1.0, 0.0, 1.0],\n        [-1.0, 0.0, 1.0]\n    ],\n    [\n        [-1.0, -1.0, -1.0],\n        [ 0.0,  0.0,  0.0],\n        [ 1.0,  1.0,  1.0]\n    ],\n    [\n        [ 1.0,  1.0,  1.0],\n        [ 0.0,  0.0,  0.0],\n        [-1.0, -1.0, -1.0]\n    ]\n)\n\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filters(weights=bank_filters)\nHere the filter has three dimensions. The first one must correspond to each filter. The last dimension of the post-synaptic population must correspond to the total number of filters. It cannot be combined with keep_last_dimension.\n\n\n\n\n\n\nNote\n\n\n\nCurrent limitation: Each filter must have the same size, it is not possible yet to convolve over multiple scales.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html#pooling",
    "href": "manual/ConvolutionalNetworks.html#pooling",
    "title": "Convolution and pooling",
    "section": "",
    "text": "Another form of atypical projection for a neural network is the pooling operation. In max-pooling, each post-synaptic neuron is associated to a region of the pre-synaptic population and responds like the maximum firing rate in this region. This is already possible by defining the operation argument of the synapse type, but it would use instantiated synapses, what would be a waste of memory.\nThe Pooling class allows to define such an operation without defining any synapse:\npre = ann.Population(geometry=(100, 100), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = ann.Population(geometry=(50, 50), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = Pooling(pre=pre, post=post, target='exc', operation='max')\nproj.connect_pooling()\nThe pooling region of a post-synaptic region is automatically determined by comparing the dimensions of the two populations: here each post-synaptic neuron will cover an area of 2*2 neurons.\nIf the number of dimensions do not match, you have to specify the extent argument to pooling(). For example, you can pool completely over one dimension of the pre-synaptic population:\npre = ann.Population(geometry=(100, 100, 10), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npost = ann.Population(geometry=(50, 50), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = Pooling(pre=pre, post=post, target='exc', operation='max')\nproj.connect_pooling(extent=(2, 2, 10))",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/ConvolutionalNetworks.html#sharing-weights-with-another-projection",
    "href": "manual/ConvolutionalNetworks.html#sharing-weights-with-another-projection",
    "title": "Convolution and pooling",
    "section": "",
    "text": "A different possibility to share weights is between two projections. If your network is composed of populations of the same size, and the projection patterns are identical, it could save some memory to “share” the weights of one projection with another, so they are created only once.\nTo this end, you can use the Copy class and pass it an existing projection:\npop1 = ann.Population(geometry=(30, 30), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\npop2 = ann.Population(geometry=(20, 20), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\npop3 = ann.Population(geometry=(20, 20), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\n\nproj1 = ann.Projection(pop1, pop2, 'exc')\nproj1.connect_gaussian(amp = 1.0, sigma=0.3, delays=2.0)\n\nproj2 = Copy(pop1, pop3, 'exc')\nproj2.connect_copy(proj1)\nThis only works when the pre- and post-populations of each projection have the same geometry, but they can be different, of course. If the original projection is learnable, the copied projection will see the changes. However, it is not possible for the shared projection to learn on its own. Copy only accepts psp and operation as parameters, which can be different from the original projection.\nIt is only possible to copy regular projections, not other shared projections. The transmission delays will be identical between the two projections.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Convolution and pooling"
    ]
  },
  {
    "objectID": "manual/Logging.html",
    "href": "manual/Logging.html",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "The tensorboard extension allows to visualize ANNarchy simulations using tensorboard. It requires the tensorboardX package:\npip install tensorboardX\nThe Logger class is a thin wrapper around tensorboardX.SummaryWriter, which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io. Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch.\nThe extension has to be imported explicitly:\nimport ANNarchy as ann\nfrom ANNarchy.extensions.tensorboard import Logger\nFor detailed examples of how to use the extension, refer to the examples Basal Ganglia and Bayesian optimization, which are available as notebooks in the folder examples/tensorboard.\n\n\nThe Logger class has to be closed properly at the end of the script, so it is advised to use a context:\nwith ann.Logger() as logger:\n    logger.add_scalar(\"Accuracy\", acc, trial)\nYou can also make sure to close it:\nlogger = ann.Logger()\nlogger.add_scalar(\"Accuracy\", acc, trial)\nlogger.close()\nBy default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine. You can control these two elements by passing arguments to Logger():\nwith ann.Logger(logdir=\"/tmp/annarchy\", experiment=\"trial1\"): # logs in /tmp/annarchy/trial1\n\n\n\nAfter (or while) logging data within your simulation, run tensorboard in the terminal by specifying the path to the log directory:\ntensorboard --logdir runs\nYou will then be asked to open localhost:6006 in your browser and will see a page similar to this:\n\n\n\nStructure of a neural network\n\n\nThe information logged will be available in the different tabs (scalars, images...). You can also visualize and compare several experiments at the same time.\n\n\n\nThe add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc.\nThe simplest information to log is a scalar, for example the accuracy at the end of a trial:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        accuracy = ...\n        logger.add_scalar(\"Accuracy\", accuracy, trial)\nA tag should be given for each plot as the first argument. In the example above, the figure with the accuracy will be labelled \"Accuracy\" in tensorboard. You can also group plots together with 2-levels tags such as:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        train_accuracy = ...\n        test_accuracy = ...\n        logger.add_scalar(\"Accuracy/Train\", train_accuracy, trial)\n        logger.add_scalar(\"Accuracy/Test\", test_accuracy, trial)\nThe second argument is the scalar, obviously. The third is the index of x-axis of the plot. It can be the index of the trial, the current time or whatever you prefer.\nIf you want to display several scalars on the same plot, you can use the method add_scalars() and provide a dictionary:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        train_accuracy = ...\n        test_accuracy = ...\n        logger.add_scalars(\"Accuracy\", {'train': train_accuracy, 'test': test_accuracy}, trial)\n\n\n\nIt is also possible to log images, for example an input image or the firing rate of a 2D population, with the add_image() method:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        img = pop.r.reshape((10, 10))\n        logger.add_image(\"Population/Firing rate\", img, trial)\nThe image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images.\nThe values must be floats between 0 and 1 or integers between 0 and 255 in order to be displayed correctly. You can either do it yourself, or pass equalize=True to the add_image():\nlogger.add_image(\"Population/Firing rate\", img, trial, equalize=True)\nThe min/max values in the array are internally used to rescale the image:\nimg = (img - img.min())/(img.max() - img.min())\nTo display several images together, for example the receptive fields of a population, an array of size (number, height, width) or (number, height, width, 3) can be passed to add_images(), where number is the number of images to display:\n\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons\n        logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True)\nequalize=True applies the same scaling to all images, but you additionally pass equalize_per_image=True to have indepent scalings per image.\n\n\n\nHistograms can also be logged, for example to visualize the statistics of weights in a projection:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        weights= proj.w.flatten()\n        logger.add_histogram(\"Weight distribution\", weights, trial)\n\n\n\nMatplotlib figures can also be logged:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        fig = plt.figure()\n        plt.plot(pop.r)\n        logger.add_figure(\"Activity\", fig, trial)\nadd_figure() will automatically close the figure, no need to call show().\nBeware that this is very slow and requires a lot of space.\n\n\n\nThe previous methods can be called multiple times during a simulation, in order to visualize the changes during learning.\nadd_parameters() is more useful in the context of hyperparameter optimization, where the same network with different parameters is run multiple times.\nOnly once per simulation, typically at the end, you can log the value of some important parameters together with some metrics such as accuracy, error rate or so. This will allow tensorboard to display over multiple runs the relation between the parameters and the metrics in the tab “HPARAMS”:\nwith ann.Logger() as logger:\n    # ...\n    logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy}) \nRefer to Bayesian optimization for an example using the hyperopt library.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#creating-the-logger",
    "href": "manual/Logging.html#creating-the-logger",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "The Logger class has to be closed properly at the end of the script, so it is advised to use a context:\nwith ann.Logger() as logger:\n    logger.add_scalar(\"Accuracy\", acc, trial)\nYou can also make sure to close it:\nlogger = ann.Logger()\nlogger.add_scalar(\"Accuracy\", acc, trial)\nlogger.close()\nBy default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine. You can control these two elements by passing arguments to Logger():\nwith ann.Logger(logdir=\"/tmp/annarchy\", experiment=\"trial1\"): # logs in /tmp/annarchy/trial1",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#launching-tensorboard",
    "href": "manual/Logging.html#launching-tensorboard",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "After (or while) logging data within your simulation, run tensorboard in the terminal by specifying the path to the log directory:\ntensorboard --logdir runs\nYou will then be asked to open localhost:6006 in your browser and will see a page similar to this:\n\n\n\nStructure of a neural network\n\n\nThe information logged will be available in the different tabs (scalars, images...). You can also visualize and compare several experiments at the same time.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#logging-scalars",
    "href": "manual/Logging.html#logging-scalars",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "The add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc.\nThe simplest information to log is a scalar, for example the accuracy at the end of a trial:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        accuracy = ...\n        logger.add_scalar(\"Accuracy\", accuracy, trial)\nA tag should be given for each plot as the first argument. In the example above, the figure with the accuracy will be labelled \"Accuracy\" in tensorboard. You can also group plots together with 2-levels tags such as:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        train_accuracy = ...\n        test_accuracy = ...\n        logger.add_scalar(\"Accuracy/Train\", train_accuracy, trial)\n        logger.add_scalar(\"Accuracy/Test\", test_accuracy, trial)\nThe second argument is the scalar, obviously. The third is the index of x-axis of the plot. It can be the index of the trial, the current time or whatever you prefer.\nIf you want to display several scalars on the same plot, you can use the method add_scalars() and provide a dictionary:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        train_accuracy = ...\n        test_accuracy = ...\n        logger.add_scalars(\"Accuracy\", {'train': train_accuracy, 'test': test_accuracy}, trial)",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#logging-images",
    "href": "manual/Logging.html#logging-images",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "It is also possible to log images, for example an input image or the firing rate of a 2D population, with the add_image() method:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        img = pop.r.reshape((10, 10))\n        logger.add_image(\"Population/Firing rate\", img, trial)\nThe image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images.\nThe values must be floats between 0 and 1 or integers between 0 and 255 in order to be displayed correctly. You can either do it yourself, or pass equalize=True to the add_image():\nlogger.add_image(\"Population/Firing rate\", img, trial, equalize=True)\nThe min/max values in the array are internally used to rescale the image:\nimg = (img - img.min())/(img.max() - img.min())\nTo display several images together, for example the receptive fields of a population, an array of size (number, height, width) or (number, height, width, 3) can be passed to add_images(), where number is the number of images to display:\n\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons\n        logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True)\nequalize=True applies the same scaling to all images, but you additionally pass equalize_per_image=True to have indepent scalings per image.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#logging-histograms",
    "href": "manual/Logging.html#logging-histograms",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "Histograms can also be logged, for example to visualize the statistics of weights in a projection:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        weights= proj.w.flatten()\n        logger.add_histogram(\"Weight distribution\", weights, trial)",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#logging-figures",
    "href": "manual/Logging.html#logging-figures",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "Matplotlib figures can also be logged:\nwith ann.Logger() as logger:\n    for trial in range(100):\n        ann.simulate(1000.0)\n        fig = plt.figure()\n        plt.plot(pop.r)\n        logger.add_figure(\"Activity\", fig, trial)\nadd_figure() will automatically close the figure, no need to call show().\nBeware that this is very slow and requires a lot of space.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Logging.html#logging-parameters",
    "href": "manual/Logging.html#logging-parameters",
    "title": "Logging with tensorboard",
    "section": "",
    "text": "The previous methods can be called multiple times during a simulation, in order to visualize the changes during learning.\nadd_parameters() is more useful in the context of hyperparameter optimization, where the same network with different parameters is run multiple times.\nOnly once per simulation, typically at the end, you can log the value of some important parameters together with some metrics such as accuracy, error rate or so. This will allow tensorboard to display over multiple runs the relation between the parameters and the metrics in the tab “HPARAMS”:\nwith ann.Logger() as logger:\n    # ...\n    logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy}) \nRefer to Bayesian optimization for an example using the hyperopt library.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Logging with tensorboard"
    ]
  },
  {
    "objectID": "manual/Hybrid.html",
    "href": "manual/Hybrid.html",
    "title": "Hybrid networks",
    "section": "",
    "text": "ANNarchy has the possibility to simulate either rate-coded or spiking networks. It is therefore possible to define hybrid networks mixing rate-coded and spiking populations.\nA typical application would be to define a rate-coded network to process visual inputs, which is used to feed a spiking network for action selection. A dummy example is provided in examples/hybrid.\n\n\nConverting a rate-coded population to a spiking network is straightforward. The PoissonPopulation (see its API) defines a population of spiking neurons emitting spikes following a Poisson distribution:\npop = ann.PoissonPopulation(1000, rates=50.)\nIn this case, the 1000 neurons emit spikes at a rate of 50 Hz (the rate of individual neurons can be later modified). It is possible to use a weighted sum of rate-coded synapses in order to determine the firing rate of each Poisson neuron. It requires to connect a rate-coded population to the PoissonPopulation with a given target:\npop1 = ann.Population(4, Neuron(parameters=\"r=0.0\"))\n\npop2 = ann.PoissonPopulation(1000, target='exc')\n\nproj = ann.Projection(pop1, pop2, 'exc')\nproj.connect_fixed_number_pre(weights=10.0, number=1)\nIn this example, each of the 4 pre-synaptic neurons “controls” the firing rate of one fourth (on average) of the post-synaptic ones. If target is used in the Poisson population, rates will be ignored.\nThe weights determine the scaling of the transmission: a presynaptic rate r of 1.0 generates a firing rate of w Hz in the post-synaptic neurons. Here setting pop1.r = 1.0 will make the post-synaptic neurons fire at 10 Hz.\n\n\n\nDecoding a spiking population is a harder process, because of the stochastic nature of spike trains. One can take advantage of the fact here that a rate-coded neuron usually represents an ensemble of spiking neurons, so the average firing rate in that ensemble can be more precisely decoded.\nIn order to do so, one needs to connect the spiking population to a rate-coded one with a many-to-one pattern using a DecodingProjection. A DecodingProjection heritates all methods of Projection (including the connection methods) but performs the necessary conversion from spike trains to a instantaneous rate:\npop1 = ann.PoissonPopulation(1000, rates=50.0)\npop2 = ann.Population(1, Neuron(equations=\"r=sum(exc)\"))\nproj = ann.DecodingProjection(pop1, pop2, 'exc', window=10.0)\nproj.connect_all_to_all(weights=1.0)\nIn this example, the spiking population fires at 50 Hz. The single rate-coded neuron decoding that population will count how many spikes arrived in the last T milliseconds and divide it by the total number of synapses in order to estimate the population firing rate in pop1.\nThis would be accessed in sum(exc) (or whatever target is used in the projection). Because of its simple definition, it will therefore have its rate r at 50.0 (with some variance due to the stochastic nature of spike trains).\nThe window argument defines the duration in milliseconds of the sliding temporal window used to estimate the firing rate. By default, it is equal to dt, which means spikes are counted in a very narrow period of time, what could lead to very big variations of the decoded firing rate. If the window is too big, it would introduce a noticeable lag for the decoded firing rate if the input varies too quickly. window = 10.0 is usually a good compromise, but this depends on the input firing rate.\nThe weights of the projection define the scaling of the decoded firing rate. If one wants a firing rate of 100 Hz to be represented by r=1.0, the weights should be set to 0.01.\nNo Synapse model can be used in a DecodingProjection.\n\n\n\n\n\n\nWarning\n\n\n\nDecodingProjection is not implemented on CUDA yet.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Hybrid networks"
    ]
  },
  {
    "objectID": "manual/Hybrid.html#rate-coded-to-spike",
    "href": "manual/Hybrid.html#rate-coded-to-spike",
    "title": "Hybrid networks",
    "section": "",
    "text": "Converting a rate-coded population to a spiking network is straightforward. The PoissonPopulation (see its API) defines a population of spiking neurons emitting spikes following a Poisson distribution:\npop = ann.PoissonPopulation(1000, rates=50.)\nIn this case, the 1000 neurons emit spikes at a rate of 50 Hz (the rate of individual neurons can be later modified). It is possible to use a weighted sum of rate-coded synapses in order to determine the firing rate of each Poisson neuron. It requires to connect a rate-coded population to the PoissonPopulation with a given target:\npop1 = ann.Population(4, Neuron(parameters=\"r=0.0\"))\n\npop2 = ann.PoissonPopulation(1000, target='exc')\n\nproj = ann.Projection(pop1, pop2, 'exc')\nproj.connect_fixed_number_pre(weights=10.0, number=1)\nIn this example, each of the 4 pre-synaptic neurons “controls” the firing rate of one fourth (on average) of the post-synaptic ones. If target is used in the Poisson population, rates will be ignored.\nThe weights determine the scaling of the transmission: a presynaptic rate r of 1.0 generates a firing rate of w Hz in the post-synaptic neurons. Here setting pop1.r = 1.0 will make the post-synaptic neurons fire at 10 Hz.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Hybrid networks"
    ]
  },
  {
    "objectID": "manual/Hybrid.html#spike-to-rate-coded",
    "href": "manual/Hybrid.html#spike-to-rate-coded",
    "title": "Hybrid networks",
    "section": "",
    "text": "Decoding a spiking population is a harder process, because of the stochastic nature of spike trains. One can take advantage of the fact here that a rate-coded neuron usually represents an ensemble of spiking neurons, so the average firing rate in that ensemble can be more precisely decoded.\nIn order to do so, one needs to connect the spiking population to a rate-coded one with a many-to-one pattern using a DecodingProjection. A DecodingProjection heritates all methods of Projection (including the connection methods) but performs the necessary conversion from spike trains to a instantaneous rate:\npop1 = ann.PoissonPopulation(1000, rates=50.0)\npop2 = ann.Population(1, Neuron(equations=\"r=sum(exc)\"))\nproj = ann.DecodingProjection(pop1, pop2, 'exc', window=10.0)\nproj.connect_all_to_all(weights=1.0)\nIn this example, the spiking population fires at 50 Hz. The single rate-coded neuron decoding that population will count how many spikes arrived in the last T milliseconds and divide it by the total number of synapses in order to estimate the population firing rate in pop1.\nThis would be accessed in sum(exc) (or whatever target is used in the projection). Because of its simple definition, it will therefore have its rate r at 50.0 (with some variance due to the stochastic nature of spike trains).\nThe window argument defines the duration in milliseconds of the sliding temporal window used to estimate the firing rate. By default, it is equal to dt, which means spikes are counted in a very narrow period of time, what could lead to very big variations of the decoded firing rate. If the window is too big, it would introduce a noticeable lag for the decoded firing rate if the input varies too quickly. window = 10.0 is usually a good compromise, but this depends on the input firing rate.\nThe weights of the projection define the scaling of the decoded firing rate. If one wants a firing rate of 100 Hz to be represented by r=1.0, the weights should be set to 0.01.\nNo Synapse model can be used in a DecodingProjection.\n\n\n\n\n\n\nWarning\n\n\n\nDecodingProjection is not implemented on CUDA yet.",
    "crumbs": [
      "Manual",
      "**Extensions**",
      "Hybrid networks"
    ]
  },
  {
    "objectID": "manual/NumericalMethods.html",
    "href": "manual/NumericalMethods.html",
    "title": "Equations and numerical methods",
    "section": "",
    "text": "First-order ordinary differential equations (ODE) can be solved using different numerical methods. The method can be declared globally in the setup() call and used in all ODEs of the network:\nimport ANNarchy as ann\n\nann.setup(method='exponential')\nor specified explicitely for each ODE by specifying a flag:\nequations = \"\"\"\n    tau * dV/dt  + V =  A : init = 0.0, exponential\n\"\"\"\nIf nothing is specified, the explicit Euler method will be used.\nDifferent numerical methods are available:\n\nExplicit Euler 'explicit'\nImplicit Euler 'implicit'\nExponential Euler 'exponential'\nMidpoint 'midpoint'\nFourth-order Runge-Kutta 'rk4'\nEvent-driven 'event-driven'\n\nEach method has advantages/drawbacks in term of numerical error, stability and computational cost.\nTo describe these methods, we will take the example of a system of two linear first-order ODEs:\n\\frac{dx(t)}{dt} = f(x(t), y(t)) = a_x \\cdot x(t) + b_x \\cdot y(t) + c_x\n\\frac{dy(t)}{dt} = g(x(t), y(t)) = a_y \\cdot x(t) + b_y \\cdot y(t) + c_y\nThe objective of a numerical method is to approximate the value of x and y at time t+h based on its value at time t, where h is the discretization time step (noted dt in ANNarchy):\nx(t + h) = F(x(t), y(t))\ny(t + h) = G(x(t), y(t))\nAt each step of the simulation, the new values for the variables are computed using this update rule and will be used for the following step.\nThe derivative of each variable is usually approximated by:\n\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h}\n\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h}\nThe different numerical methods mostly differ in the time at which the functions f and g are evaluated.\n\n\nThe explicit (forward) Euler method computes the next value for the variables by estimating their derivative at time t:\n\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t), y(t))\n\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t), y(t))\nso the solution is straightforward to obtain:\nx(t+h) =  x(t) + h \\cdot  f(x(t), y(t))\ny(t+h) = y(t) + h \\cdot g(x(t), y(t))\n\n\n\nThe implicit (backward) Euler method computes the next value for the variables by estimating their derivative at time t + h:\n\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t+h), y(t+h))\n\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t+h), y(t+h))\nThis leads to a system of equations which must be solved in order to find the update rule. With the linear equations defined above, we need to solve:\n\\frac{x(t+h) - x(t)}{h} = a_x \\cdot x(t + h) + b_x \\cdot y(t + h) + c_x\n\\frac{y(t+h) - y(t)}{h} = a_y \\cdot x(t + h) + b_y \\cdot y(t + h) + c_y\nwhat gives something like:\nx(t+h) =  x(t) - h \\cdot \\frac{ \\left(a_{x} x(t) + b_{x} y(t) + c_{x} + h \\left(- a_{x} b_{y} x(t) + a_{y} b_{x} x(t) + b_{x} c_{y} - b_{y} c_{x}\\right)\\right)}{h^{2} \\left(- a_{x} b_{y} + a_{y} b_{x}\\right) + h \\left(a_{x} + b_{y}\\right) - 1}\ny(t+h) = y(t) -h \\cdot  \\frac{ a_{y} \\left(c_{x} h + x(t)\\right) + y(t) \\left(- a_{y} b_{x} h^{2} + \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)\\right) + \\left(a_{x} h - 1\\right) \\left(c_{y} h + y(t)\\right)}{a_{y} b_{x} h^{2} - \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)}\nANNarchy relies on Sympy to solve and simplify this system of equations and generate the update rule.\n\n\n\n\n\n\nNote\n\n\n\nThis method is obviously much more computationally expensive than the explicit Euler method, although more stable. The midpoint method is a better trade-off between complexity and stability than the implicit Euler method.\n\n\n\n\n\nThe exponential Euler method is particularly stable for single first-order linear equations, of the type:\n\\tau(t) \\cdot \\frac{dx(t)}{dt}  + x(t) =  A(t)\nThe update rule is then given by:\nx(t+h) = x(t) + (1 - \\exp(- \\frac{h}{\\tau(t)}) ) \\cdot (A(t) - x(t))\nThe difference with the explicit Euler method is the step size, which is an exponential function of the ratio \\frac{\\tau}{h}. The accurary of the exponential Euler method on linear first-order ODEs is close to perfect, compared to the other Euler methods. As it is an explicit method, systems of equations are solved very easily with the same rule.\nWhen the exponential method is used, ANNarchy first tries to reduce the ODE to its canonical form above (with the time constant being possibly dependent on time or inputs) and then generates the update rule accordingly.\nFor example, the description:\ntau * dv/dt = (E - v) + g_exc * (Ee - v) + g_inh * (v - Ei)\nwould be first transformed in:\n(1 + g_exc - g_inh) * dv/dt + v = (E + g_exc * Ee - g_inh * Ei) / (1 + g_exc - g_inh)\nbefore being transformed into an update rule, with \\tau(t) = 1 + g_\\text{exc} - g_\\text{inh}:\nv(t+h) = v(t) + (1 - \\exp(- \\frac{h}{1 + g_\\text{exc} - g_\\text{inh}}) ) \\cdot (\\frac{E + g_\\text{exc} \\cdot E_e - g_\\text{inh} \\cdot E_i}{1 + g_\\text{exc} - g_\\text{inh}} - v(t))\nThe exponential method can only be applied to first-order linear ODEs. Any other form of ODE will be rejected by the parser.\nImportant note: The step size 1 - \\exp(- \\frac{h}{\\tau(t)}) is computationally expensive because of the exponential function. If the time constant \\tau is a global parameter of the population or projection, ANNarchy can pre-compute the step size outside of the for loop over all neurons/synapses, which leads to huge increases in performance. The exponential method should therefore be reserved to first-order linear ODEs with the same time constant for all neurons/synapses:\nneuron = nn.Neuron(\n    parameters = \"tau = 10. : population\",\n    equations = \"tau * dr/dt + r = sum(exc) : min=0.0, exponential\"\n)\n\n\n\nThe midpoint method is a Runge-Kutta method of order 2. It estimates the derivative in the middle of the interval t + \\frac{h}{2}.\nk_x = f(x(t), y(t))\nk_y = g(x(t), y(t))\nx(t+h) =  x(t) + h \\cdot  f(x(t) + k_x \\cdot \\frac{h}{2}, y(t) +  k_y \\cdot \\frac{h}{2})\ny(t+h) = y(t) + h \\cdot g(x(t) + k_x \\cdot \\frac{h}{2}, y(t) +  k_y \\cdot \\frac{h}{2})\n\n\n\nThe fourth-order Runge-Kutta method estimates the derivative at four different points and combines them:\n k_1 =  f(x(t)) \n k_2 =  f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_1) \n k_3 =  f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_2) \n k_4 =  f(x(t + h) + h \\cdot k_3) \n x(t+h) = x(t) + \\frac{h}{6} \\cdot (k_1 + 2 \\cdot k_2 + 2 \\cdot k_3 + k_4 ) \n\n\n\nEvent-driven integration is only available for spiking synapses with variables following linear first-order dynamics. Let’s consider the following STDP synapse (see Spiking Synapse for explanations):\nSTDP = ann.Synapse(\n    parameters = \"\"\"\n        tau_pre = 10.0 : postsynaptic\n        tau_post = 10.0 : postsynaptic\n    \"\"\",\n    equations = \"\"\"\n        tau_pre * dApre/dt = - Apre : event-driven\n        tau_post * dApost/dt = - Apost : event-driven\n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre\n        w = clip(w + Apost, 0.0 , 1.0)\n    \"\"\",\n    post_spike = \"\"\"\n        Apost += cApost\n        w = clip(w + Apre, 0.0 , 1.0)\n    \"\"\"\n)\nThe value of Apost and Apre is only needed when a pre- or post-synaptic spike occurs at the synapse, so there is no need to integrate the corresponding equations between two such events. First-order linear ODEs have the nice property that their analytical solution is easy to obtain. Let’s consider an equation of the form:\n\\tau  \\frac{dv}{dt} = E - v\nIf v has the value V_0 at time t, its value at time t + \\Delta t is given by:\nv(t + \\Delta t) = V_0 \\cdot \\exp(-\\frac{\\Delta t}{\\tau})\n\n\n\n\n\n\nNote\n\n\n\nIf the synapse defines a psp argument (synaptic transmission is continuous), or if another continuous variable depends on the value of an event-driven one, it is not possible to use event-driven integration.\n\n\n\n\n\n\nThe values of variables are stored in a single array in order to save some memory. Special care therefore has to be taken on whether the update of a variable depends on the value of another variable at the previous time step or in the same step.\n\n\nSystems of ODEs are integrated concurrently, which means that the following system:\ntau * dv/dt = I - v - u\ntau * du/dt = v - u\nwould be numerized using the explicit Euler method as:\nv[t+1] = v[t] + dt*(I - v[t] - u[t])/tau\nu[t+1] = u[t] + dt*(v[t] - u[t])/tau\nAs we use a single array, the generated code is similar to:\nnew_v = v + dt*(I - v - u)/tau\nnew_u = u + dt*(v - u)/tau\n\nv = new_v\nu = new_u\nThis way, we ensure that the interdependent ODEs use the correct value for the other variables.\n\n\n\nWhen assignments (=, +=...) are used in an equations field, the order of valuation is different:\n\nAssigments occurring before or after a system of ODEs are updated sequentially.\nSystems of ODEs are updated concurrently.\n\nLet’s consider the following dummy equations:\n# Process the inputs\nExc = some_function(sum(exc))\nInh = another_function(sum(inh))\nI = Exc - Inh\n# ODE for the membrane potential, with a recovery variable\ntau*dv/dt = I - v - u\ntau*du/dt = v - u\n# Firing rate is the positive part of v\nr = pos(v)\nHere, Exc and Inh represent the inputs to the neuron at the current time t. The new values should be immediately available for updating I, whose value should similarly be immediately used in the ODE of v. Similarly, the value of r should be the positive part of the value of v that was just calculated, not at the previous time step. Doing otherwise would introduce a lag in the neuron: changes in sum(exc) at t would be reflected in Exc at t+1, in I at t+2, in v at t+3 and finally in r at t+4. This is generally unwanted.\nThe generated code is therefore equivalent to:\n# Process the inputs\nExc = some_function(sum(exc))\nInh = another_function(sum(inh))\nI = Exc - Inh\n# ODE for the membrane potential, with a recovery variable\nnew_v = v + dt*(I - v - u)/tau\nnew_u = u + dt*(v - u)/tau\nv = new_v\nu = new_u\n# Firing rate is the positive part of v\nr = pos(v)\nOne can even define multiple groups of assignments and systems of ODEs: systems of ODEs separated by at least one assignment will be evaluated sequentially (but concurrently inside each system). For example, in:\ntau*du/dt = v - u\nI = g_exc - g_inh\ntau*dk/dt = v - k\ntau*dv/dt = I - v - u + k\nu and k are updated using the previous value of v, while v uses the new values of both I and u, but the previous one of k.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Equations and numerical methods"
    ]
  },
  {
    "objectID": "manual/NumericalMethods.html#numerical-methods",
    "href": "manual/NumericalMethods.html#numerical-methods",
    "title": "Equations and numerical methods",
    "section": "",
    "text": "First-order ordinary differential equations (ODE) can be solved using different numerical methods. The method can be declared globally in the setup() call and used in all ODEs of the network:\nimport ANNarchy as ann\n\nann.setup(method='exponential')\nor specified explicitely for each ODE by specifying a flag:\nequations = \"\"\"\n    tau * dV/dt  + V =  A : init = 0.0, exponential\n\"\"\"\nIf nothing is specified, the explicit Euler method will be used.\nDifferent numerical methods are available:\n\nExplicit Euler 'explicit'\nImplicit Euler 'implicit'\nExponential Euler 'exponential'\nMidpoint 'midpoint'\nFourth-order Runge-Kutta 'rk4'\nEvent-driven 'event-driven'\n\nEach method has advantages/drawbacks in term of numerical error, stability and computational cost.\nTo describe these methods, we will take the example of a system of two linear first-order ODEs:\n\\frac{dx(t)}{dt} = f(x(t), y(t)) = a_x \\cdot x(t) + b_x \\cdot y(t) + c_x\n\\frac{dy(t)}{dt} = g(x(t), y(t)) = a_y \\cdot x(t) + b_y \\cdot y(t) + c_y\nThe objective of a numerical method is to approximate the value of x and y at time t+h based on its value at time t, where h is the discretization time step (noted dt in ANNarchy):\nx(t + h) = F(x(t), y(t))\ny(t + h) = G(x(t), y(t))\nAt each step of the simulation, the new values for the variables are computed using this update rule and will be used for the following step.\nThe derivative of each variable is usually approximated by:\n\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h}\n\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h}\nThe different numerical methods mostly differ in the time at which the functions f and g are evaluated.\n\n\nThe explicit (forward) Euler method computes the next value for the variables by estimating their derivative at time t:\n\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t), y(t))\n\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t), y(t))\nso the solution is straightforward to obtain:\nx(t+h) =  x(t) + h \\cdot  f(x(t), y(t))\ny(t+h) = y(t) + h \\cdot g(x(t), y(t))\n\n\n\nThe implicit (backward) Euler method computes the next value for the variables by estimating their derivative at time t + h:\n\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t+h), y(t+h))\n\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t+h), y(t+h))\nThis leads to a system of equations which must be solved in order to find the update rule. With the linear equations defined above, we need to solve:\n\\frac{x(t+h) - x(t)}{h} = a_x \\cdot x(t + h) + b_x \\cdot y(t + h) + c_x\n\\frac{y(t+h) - y(t)}{h} = a_y \\cdot x(t + h) + b_y \\cdot y(t + h) + c_y\nwhat gives something like:\nx(t+h) =  x(t) - h \\cdot \\frac{ \\left(a_{x} x(t) + b_{x} y(t) + c_{x} + h \\left(- a_{x} b_{y} x(t) + a_{y} b_{x} x(t) + b_{x} c_{y} - b_{y} c_{x}\\right)\\right)}{h^{2} \\left(- a_{x} b_{y} + a_{y} b_{x}\\right) + h \\left(a_{x} + b_{y}\\right) - 1}\ny(t+h) = y(t) -h \\cdot  \\frac{ a_{y} \\left(c_{x} h + x(t)\\right) + y(t) \\left(- a_{y} b_{x} h^{2} + \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)\\right) + \\left(a_{x} h - 1\\right) \\left(c_{y} h + y(t)\\right)}{a_{y} b_{x} h^{2} - \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)}\nANNarchy relies on Sympy to solve and simplify this system of equations and generate the update rule.\n\n\n\n\n\n\nNote\n\n\n\nThis method is obviously much more computationally expensive than the explicit Euler method, although more stable. The midpoint method is a better trade-off between complexity and stability than the implicit Euler method.\n\n\n\n\n\nThe exponential Euler method is particularly stable for single first-order linear equations, of the type:\n\\tau(t) \\cdot \\frac{dx(t)}{dt}  + x(t) =  A(t)\nThe update rule is then given by:\nx(t+h) = x(t) + (1 - \\exp(- \\frac{h}{\\tau(t)}) ) \\cdot (A(t) - x(t))\nThe difference with the explicit Euler method is the step size, which is an exponential function of the ratio \\frac{\\tau}{h}. The accurary of the exponential Euler method on linear first-order ODEs is close to perfect, compared to the other Euler methods. As it is an explicit method, systems of equations are solved very easily with the same rule.\nWhen the exponential method is used, ANNarchy first tries to reduce the ODE to its canonical form above (with the time constant being possibly dependent on time or inputs) and then generates the update rule accordingly.\nFor example, the description:\ntau * dv/dt = (E - v) + g_exc * (Ee - v) + g_inh * (v - Ei)\nwould be first transformed in:\n(1 + g_exc - g_inh) * dv/dt + v = (E + g_exc * Ee - g_inh * Ei) / (1 + g_exc - g_inh)\nbefore being transformed into an update rule, with \\tau(t) = 1 + g_\\text{exc} - g_\\text{inh}:\nv(t+h) = v(t) + (1 - \\exp(- \\frac{h}{1 + g_\\text{exc} - g_\\text{inh}}) ) \\cdot (\\frac{E + g_\\text{exc} \\cdot E_e - g_\\text{inh} \\cdot E_i}{1 + g_\\text{exc} - g_\\text{inh}} - v(t))\nThe exponential method can only be applied to first-order linear ODEs. Any other form of ODE will be rejected by the parser.\nImportant note: The step size 1 - \\exp(- \\frac{h}{\\tau(t)}) is computationally expensive because of the exponential function. If the time constant \\tau is a global parameter of the population or projection, ANNarchy can pre-compute the step size outside of the for loop over all neurons/synapses, which leads to huge increases in performance. The exponential method should therefore be reserved to first-order linear ODEs with the same time constant for all neurons/synapses:\nneuron = nn.Neuron(\n    parameters = \"tau = 10. : population\",\n    equations = \"tau * dr/dt + r = sum(exc) : min=0.0, exponential\"\n)\n\n\n\nThe midpoint method is a Runge-Kutta method of order 2. It estimates the derivative in the middle of the interval t + \\frac{h}{2}.\nk_x = f(x(t), y(t))\nk_y = g(x(t), y(t))\nx(t+h) =  x(t) + h \\cdot  f(x(t) + k_x \\cdot \\frac{h}{2}, y(t) +  k_y \\cdot \\frac{h}{2})\ny(t+h) = y(t) + h \\cdot g(x(t) + k_x \\cdot \\frac{h}{2}, y(t) +  k_y \\cdot \\frac{h}{2})\n\n\n\nThe fourth-order Runge-Kutta method estimates the derivative at four different points and combines them:\n k_1 =  f(x(t)) \n k_2 =  f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_1) \n k_3 =  f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_2) \n k_4 =  f(x(t + h) + h \\cdot k_3) \n x(t+h) = x(t) + \\frac{h}{6} \\cdot (k_1 + 2 \\cdot k_2 + 2 \\cdot k_3 + k_4 ) \n\n\n\nEvent-driven integration is only available for spiking synapses with variables following linear first-order dynamics. Let’s consider the following STDP synapse (see Spiking Synapse for explanations):\nSTDP = ann.Synapse(\n    parameters = \"\"\"\n        tau_pre = 10.0 : postsynaptic\n        tau_post = 10.0 : postsynaptic\n    \"\"\",\n    equations = \"\"\"\n        tau_pre * dApre/dt = - Apre : event-driven\n        tau_post * dApost/dt = - Apost : event-driven\n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre\n        w = clip(w + Apost, 0.0 , 1.0)\n    \"\"\",\n    post_spike = \"\"\"\n        Apost += cApost\n        w = clip(w + Apre, 0.0 , 1.0)\n    \"\"\"\n)\nThe value of Apost and Apre is only needed when a pre- or post-synaptic spike occurs at the synapse, so there is no need to integrate the corresponding equations between two such events. First-order linear ODEs have the nice property that their analytical solution is easy to obtain. Let’s consider an equation of the form:\n\\tau  \\frac{dv}{dt} = E - v\nIf v has the value V_0 at time t, its value at time t + \\Delta t is given by:\nv(t + \\Delta t) = V_0 \\cdot \\exp(-\\frac{\\Delta t}{\\tau})\n\n\n\n\n\n\nNote\n\n\n\nIf the synapse defines a psp argument (synaptic transmission is continuous), or if another continuous variable depends on the value of an event-driven one, it is not possible to use event-driven integration.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Equations and numerical methods"
    ]
  },
  {
    "objectID": "manual/NumericalMethods.html#order-of-evaluation",
    "href": "manual/NumericalMethods.html#order-of-evaluation",
    "title": "Equations and numerical methods",
    "section": "",
    "text": "The values of variables are stored in a single array in order to save some memory. Special care therefore has to be taken on whether the update of a variable depends on the value of another variable at the previous time step or in the same step.\n\n\nSystems of ODEs are integrated concurrently, which means that the following system:\ntau * dv/dt = I - v - u\ntau * du/dt = v - u\nwould be numerized using the explicit Euler method as:\nv[t+1] = v[t] + dt*(I - v[t] - u[t])/tau\nu[t+1] = u[t] + dt*(v[t] - u[t])/tau\nAs we use a single array, the generated code is similar to:\nnew_v = v + dt*(I - v - u)/tau\nnew_u = u + dt*(v - u)/tau\n\nv = new_v\nu = new_u\nThis way, we ensure that the interdependent ODEs use the correct value for the other variables.\n\n\n\nWhen assignments (=, +=...) are used in an equations field, the order of valuation is different:\n\nAssigments occurring before or after a system of ODEs are updated sequentially.\nSystems of ODEs are updated concurrently.\n\nLet’s consider the following dummy equations:\n# Process the inputs\nExc = some_function(sum(exc))\nInh = another_function(sum(inh))\nI = Exc - Inh\n# ODE for the membrane potential, with a recovery variable\ntau*dv/dt = I - v - u\ntau*du/dt = v - u\n# Firing rate is the positive part of v\nr = pos(v)\nHere, Exc and Inh represent the inputs to the neuron at the current time t. The new values should be immediately available for updating I, whose value should similarly be immediately used in the ODE of v. Similarly, the value of r should be the positive part of the value of v that was just calculated, not at the previous time step. Doing otherwise would introduce a lag in the neuron: changes in sum(exc) at t would be reflected in Exc at t+1, in I at t+2, in v at t+3 and finally in r at t+4. This is generally unwanted.\nThe generated code is therefore equivalent to:\n# Process the inputs\nExc = some_function(sum(exc))\nInh = another_function(sum(inh))\nI = Exc - Inh\n# ODE for the membrane potential, with a recovery variable\nnew_v = v + dt*(I - v - u)/tau\nnew_u = u + dt*(v - u)/tau\nv = new_v\nu = new_u\n# Firing rate is the positive part of v\nr = pos(v)\nOne can even define multiple groups of assignments and systems of ODEs: systems of ODEs separated by at least one assignment will be evaluated sequentially (but concurrently inside each system). For example, in:\ntau*du/dt = v - u\nI = g_exc - g_inh\ntau*dk/dt = v - k\ntau*dv/dt = I - v - u + k\nu and k are updated using the previous value of v, while v uses the new values of both I and u, but the previous one of k.",
    "crumbs": [
      "Manual",
      "**Simulation**",
      "Equations and numerical methods"
    ]
  },
  {
    "objectID": "Installation.html",
    "href": "Installation.html",
    "title": "Installation",
    "section": "",
    "text": "ANNarchy is designed to run on GNU/Linux and OSX. It relies mostly on a C++ compiler (g++ or clang++), Cython (C for Python extension) and Python (Numpy, Sympy) libraries.\nInstallation on Windows is only possible using the WSL (Windows Subsystem for Linux) or a VM.\n\n\nThe source code of ANNarchy can be downloaded on github:\ngit clone https://github.com/ANNarchy/ANNarchy.git\n\n\n\n\n\nANNarchy depends on a number of packages that should be easily accessible on recent GNU/Linux distributions. The classical way to install these dependencies is through your package manager, or using full Python distributions such as Anaconda. Older versions of these packages may work but have not been tested.\n\npython &gt;= 3.8 (with the development files, e.g. python-dev or python-devel)\ng++ &gt;= 7.4 or clang++ &gt;= 3.4\nmake &gt;= 3.0\ncython &gt;= 3.0\nnumpy &gt;= 1.13\nsympy &gt;= 1.11\nscipy &gt;= 1.9\nmatplotlib &gt;= 3.0\n\nAdditionally, the following packages are optional but strongly recommended:\n\npyqtgraph (to visualize some of the provided examples. The OpenGL backend can also be needed).\nlxml (to save the networks in .xml format).\npandoc (for reporting).\ntensorboardX (for the logging extension).\n\nTo use the CUDA backend:\n\nthe CUDA-SDK is available on the official website (we recommend to use at least a SDK version &gt; 6.x). For further details on installation etc., please consider the corresponding Quickstart guides (Quickstart_8.0 for the SDK 8.x).\n\nANNarchy works with full Python distributions such as Anaconda, as well as in virtual environments and Jupyter notebooks.\n\n\n\n\n\n\nNote\n\n\n\nOn a fresh install of Ubuntu 22.04, here are the minimal system packages to install before ANNarchy:\nsudo apt install build-essential git python3-dev python3-setuptools python3-pip \nThe minimal Python packages can be installed with:\npip install numpy scipy matplotlib cython sympy\n\n\n\n\n\n\n\nStable releases of ANNarchy are available on PyPi:\npip install ANNarchy\nor:\npip install ANNarchy --user\nif you do not have administrator permissions. Omit --user in a virtual environment.\nYou may also install directly the latest commit in the master (stable) or develop branches with:\npip install git+https://github.com/ANNarchy/ANNarchy.git@master\n\n\n\nInstallation of ANNarchy from source is possible using pip in the top-level directory:\npip install .\nor in development mode:\npip install -e .\nUsing python setup.py install is deprecated, but still works.\n\n\n\nBy default, ANNarchy will use the GNU C++ compiler g++, which should be in your PATH. If you want to use another compiler (clang++, icc), you can edit the configuration file located at $HOME/.config/ANNarchy/annarchy.json (created during installation) accordingly. By default, it is:\n{\n    \"openmp\": {\n        \"compiler\": \"g++\",\n        \"flags\": \"-march=native -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\nThe (path to the) compiler can be changed in the openmp section (ignore the cuda section if you do not have a GPU).\nYou can also change the compiler flags if you know what you are doing. -O3 does not always lead to faster simulation times.\n\n\n\nIf ANNarchy detects the CUDA SDK during installation, it will prepare the required modules. You need to make sure that the CUDA compiler nvcc is accessible in your path.\nThe main problem with CUDA is that the binaries, headers and libraries are installed at different locations depending on the version: /usr/local/cuda, /usr/local/cuda-7.0 or /usr/local/cuda-8.0. There is unfortunately no way for ANNarchy to guess the installation path.\nA first thing to help ANNarchy find the CUDA libraries is to define the LD_LIBRARY_PATH environment variable and have point at the lib64/ subfolder:\nexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64/:$LD_LIBRARY_PATH\nThis should in most cases work if you have only one CUDA installation. Otherwise, it is needed that you indicate where the CUDA libraries are, by modifying the ANNarchy configuration file located at $HOME/.config/ANNarchy/annarchy.json:\n{\n    \"openmp\": {\n        \"compiler\": \"g++\",\n        \"flags\": \"-march=native -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\nSimply point the ['cuda']['path'] field to the right location (without lib64/). If the nvcc compiler binary is at a different location, the absolute path to the nvcc can be provided by ['cuda']['compiler'] field.\nIt can happen that the detection of CUDA fails during installation, as some environment variables are not set. In this case try:\nenv \"PATH=$PATH\" \"LIBRARY_PATH=$LIBRARY_PATH\" pip install .\n\n\n\n\n\nInstallation on MacOS X is in principle similar to GNU/Linux:\npip install ANNarchy\nWe strongly advise using a full Python distribution like Miniforge, or a package manager like Homebrew rather than the default python provided by Apple.\nThe main issue is the choice of the C++ compiler:\n\n\nIf not done already, you should first install the Xcode Command Line Tools, either through Apple’s website or through Homebrew (see https://mac.install.guide/commandlinetools/ for a guide) to get the LLVM clang++ compiler.\nThe major drawback is that Apple’s clang++ still does not support OpenMP for parallel computing. Any attempt to use OpenMP with ANNarchy using this compiler will crash.\nIf you have a M1 arm64 processor, it might be beneficial to tell clang++ to use optimizations for that hardware. Open the configuration file at $HOME/.config/ANNarchy/annarchy.json and add the following compiler flag (Xcode &gt; 13.0):\n{\n    \"openmp\": {\n        \"compiler\": \"clang++\",\n        \"flags\": \"-mcpu=apple-m1 -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\n\n\n\nIn order to benefit from OpenMP parallelization, you should install gcc, the GNU C compiler, using Homebrew:\nbrew install gcc\nYou will get the command-line C++ compiler with a version number, e.g.:\ng++-11\nThe g++ executable is a symlink to Apple’s clang++, do not use it…\nYou now have to tell ANNarchy which compiler to use, even if it is in your PATH. After installing ANNarchy, a config file is created in $HOME/.config/ANNarchy/annarchy.json. Open it and change the openmp entry to:\n{\n    \"openmp\": {\n        \"compiler\": \"g++-11\",\n        \"flags\": \"-march=native -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\n\n\n\n\n\n\nNote\n\n\n\nA potential problem with Anaconda/miniforge is that the compiler will use by default the Python shared library provided by Apple, leading to the following crash when simulating:\nFatal Python error: PyThreadState_Get: no current thread\nAbort trap: 6\nThe solution is to set the environment variable DYLD_FALLBACK_LIBRARY_PATH to point at the correct library libpython3.6.dylib in your .bash_profile. For a standard Anaconda installation, this should be:\nexport DYLD_FALLBACK_LIBRARY_PATH=$HOME/anaconda/lib:$DYLD_FALLBACK_LIBRARY_PATH\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe CUDA backend is not available on OS X.",
    "crumbs": [
      "ANNarchy",
      "Installation"
    ]
  },
  {
    "objectID": "Installation.html#download",
    "href": "Installation.html#download",
    "title": "Installation",
    "section": "",
    "text": "The source code of ANNarchy can be downloaded on github:\ngit clone https://github.com/ANNarchy/ANNarchy.git",
    "crumbs": [
      "ANNarchy",
      "Installation"
    ]
  },
  {
    "objectID": "Installation.html#installation-on-gnulinux",
    "href": "Installation.html#installation-on-gnulinux",
    "title": "Installation",
    "section": "",
    "text": "ANNarchy depends on a number of packages that should be easily accessible on recent GNU/Linux distributions. The classical way to install these dependencies is through your package manager, or using full Python distributions such as Anaconda. Older versions of these packages may work but have not been tested.\n\npython &gt;= 3.8 (with the development files, e.g. python-dev or python-devel)\ng++ &gt;= 7.4 or clang++ &gt;= 3.4\nmake &gt;= 3.0\ncython &gt;= 3.0\nnumpy &gt;= 1.13\nsympy &gt;= 1.11\nscipy &gt;= 1.9\nmatplotlib &gt;= 3.0\n\nAdditionally, the following packages are optional but strongly recommended:\n\npyqtgraph (to visualize some of the provided examples. The OpenGL backend can also be needed).\nlxml (to save the networks in .xml format).\npandoc (for reporting).\ntensorboardX (for the logging extension).\n\nTo use the CUDA backend:\n\nthe CUDA-SDK is available on the official website (we recommend to use at least a SDK version &gt; 6.x). For further details on installation etc., please consider the corresponding Quickstart guides (Quickstart_8.0 for the SDK 8.x).\n\nANNarchy works with full Python distributions such as Anaconda, as well as in virtual environments and Jupyter notebooks.\n\n\n\n\n\n\nNote\n\n\n\nOn a fresh install of Ubuntu 22.04, here are the minimal system packages to install before ANNarchy:\nsudo apt install build-essential git python3-dev python3-setuptools python3-pip \nThe minimal Python packages can be installed with:\npip install numpy scipy matplotlib cython sympy\n\n\n\n\n\n\n\nStable releases of ANNarchy are available on PyPi:\npip install ANNarchy\nor:\npip install ANNarchy --user\nif you do not have administrator permissions. Omit --user in a virtual environment.\nYou may also install directly the latest commit in the master (stable) or develop branches with:\npip install git+https://github.com/ANNarchy/ANNarchy.git@master\n\n\n\nInstallation of ANNarchy from source is possible using pip in the top-level directory:\npip install .\nor in development mode:\npip install -e .\nUsing python setup.py install is deprecated, but still works.\n\n\n\nBy default, ANNarchy will use the GNU C++ compiler g++, which should be in your PATH. If you want to use another compiler (clang++, icc), you can edit the configuration file located at $HOME/.config/ANNarchy/annarchy.json (created during installation) accordingly. By default, it is:\n{\n    \"openmp\": {\n        \"compiler\": \"g++\",\n        \"flags\": \"-march=native -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\nThe (path to the) compiler can be changed in the openmp section (ignore the cuda section if you do not have a GPU).\nYou can also change the compiler flags if you know what you are doing. -O3 does not always lead to faster simulation times.\n\n\n\nIf ANNarchy detects the CUDA SDK during installation, it will prepare the required modules. You need to make sure that the CUDA compiler nvcc is accessible in your path.\nThe main problem with CUDA is that the binaries, headers and libraries are installed at different locations depending on the version: /usr/local/cuda, /usr/local/cuda-7.0 or /usr/local/cuda-8.0. There is unfortunately no way for ANNarchy to guess the installation path.\nA first thing to help ANNarchy find the CUDA libraries is to define the LD_LIBRARY_PATH environment variable and have point at the lib64/ subfolder:\nexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64/:$LD_LIBRARY_PATH\nThis should in most cases work if you have only one CUDA installation. Otherwise, it is needed that you indicate where the CUDA libraries are, by modifying the ANNarchy configuration file located at $HOME/.config/ANNarchy/annarchy.json:\n{\n    \"openmp\": {\n        \"compiler\": \"g++\",\n        \"flags\": \"-march=native -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\nSimply point the ['cuda']['path'] field to the right location (without lib64/). If the nvcc compiler binary is at a different location, the absolute path to the nvcc can be provided by ['cuda']['compiler'] field.\nIt can happen that the detection of CUDA fails during installation, as some environment variables are not set. In this case try:\nenv \"PATH=$PATH\" \"LIBRARY_PATH=$LIBRARY_PATH\" pip install .",
    "crumbs": [
      "ANNarchy",
      "Installation"
    ]
  },
  {
    "objectID": "Installation.html#installation-on-macos-x",
    "href": "Installation.html#installation-on-macos-x",
    "title": "Installation",
    "section": "",
    "text": "Installation on MacOS X is in principle similar to GNU/Linux:\npip install ANNarchy\nWe strongly advise using a full Python distribution like Miniforge, or a package manager like Homebrew rather than the default python provided by Apple.\nThe main issue is the choice of the C++ compiler:\n\n\nIf not done already, you should first install the Xcode Command Line Tools, either through Apple’s website or through Homebrew (see https://mac.install.guide/commandlinetools/ for a guide) to get the LLVM clang++ compiler.\nThe major drawback is that Apple’s clang++ still does not support OpenMP for parallel computing. Any attempt to use OpenMP with ANNarchy using this compiler will crash.\nIf you have a M1 arm64 processor, it might be beneficial to tell clang++ to use optimizations for that hardware. Open the configuration file at $HOME/.config/ANNarchy/annarchy.json and add the following compiler flag (Xcode &gt; 13.0):\n{\n    \"openmp\": {\n        \"compiler\": \"clang++\",\n        \"flags\": \"-mcpu=apple-m1 -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\n\n\n\nIn order to benefit from OpenMP parallelization, you should install gcc, the GNU C compiler, using Homebrew:\nbrew install gcc\nYou will get the command-line C++ compiler with a version number, e.g.:\ng++-11\nThe g++ executable is a symlink to Apple’s clang++, do not use it…\nYou now have to tell ANNarchy which compiler to use, even if it is in your PATH. After installing ANNarchy, a config file is created in $HOME/.config/ANNarchy/annarchy.json. Open it and change the openmp entry to:\n{\n    \"openmp\": {\n        \"compiler\": \"g++-11\",\n        \"flags\": \"-march=native -O3\"\n    },\n    \"cuda\": {\n        \"compiler\": \"nvcc\",\n        \"flags\": \"\",\n        \"device\": 0,\n        \"path\": \"/usr/local/cuda\"\n    }\n}\n\n\n\n\n\n\nNote\n\n\n\nA potential problem with Anaconda/miniforge is that the compiler will use by default the Python shared library provided by Apple, leading to the following crash when simulating:\nFatal Python error: PyThreadState_Get: no current thread\nAbort trap: 6\nThe solution is to set the environment variable DYLD_FALLBACK_LIBRARY_PATH to point at the correct library libpython3.6.dylib in your .bash_profile. For a standard Anaconda installation, this should be:\nexport DYLD_FALLBACK_LIBRARY_PATH=$HOME/anaconda/lib:$DYLD_FALLBACK_LIBRARY_PATH\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe CUDA backend is not available on OS X.",
    "crumbs": [
      "ANNarchy",
      "Installation"
    ]
  },
  {
    "objectID": "notebooks/RC.html",
    "href": "notebooks/RC.html",
    "title": "Echo state networks",
    "section": "",
    "text": "If you run this notebook in colab, first uncomment and run this cell to install ANNarchy:\n\n#!pip install ANNarchy\n\nThis notebook demonstrates how to implement a simple Echo state network (ESN) with ANNarchy. It is a simple rate-coded network, with a population of recurrently-connected neurons and a readout layer which will be learned offline using scikit-learn. The task will be a simple univariate regression.\n\nLet’s start by importing ANNarchy.\nThe clear() command is necessary in notebooks when recreating a network. If you re-run the cells creating a network without calling clear() first, populations will add up, and the results may not be what you expect.\nsetup() sets various parameters, such as the step size dt in milliseconds. By default, dt is 1.0, so the call is not necessary here.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ANNarchy as ann\n\nann.clear()\nann.setup(dt=1.0)\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nEach neuron in the reservoir follows the following equations:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + g \\,  \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nwhere \\xi(t) is a uniform noise.\nThe neuron has three parameters and two variables:\n\nESN_Neuron = ann.Neuron(\n    parameters = \"\"\"\n        tau = 30.0 : population\n        g = 1.0 : population\n        noise = 0.01\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1)\n\n        r = tanh(x)\n    \"\"\"\n)\n\nThe echo-state network will be a population of 400 neuron.\n\nN = 400\npop = ann.Population(N, ESN_Neuron)\n\nWe can specify the value of the parameters from Python, this will override the value defined in the neuron description. We can give single float values or numpy arrays of the correct shape:\n\npop.tau = 30.0\npop.g = 1.4\npop.noise = 0.01\n\nThe input to the reservoir is a single value, we create a special population InputArray that does nothing except storing a variable called r that can be set externally.\n\ninp = ann.InputArray(1)\ninp.r = 0.0\n\nInput weights are uniformly distributed between -1 and 1.\n\nWi = ann.Projection(inp, pop, 'in')\nWi.connect_all_to_all(weights=ann.Uniform(-1.0, 1.0))\n\n&lt;ANNarchy.core.Projection.Projection at 0x127bce6d0&gt;\n\n\nRecurrent weights are sampled from the normal distribution with mean 0 and variance g^2 / N. Here, we put the synaptic scaling g inside the neuron.\n\nWrec = ann.Projection(pop, pop, 'exc')\nWrec.connect_all_to_all(weights=ann.Normal(0., 1/np.sqrt(N)))\n\n&lt;ANNarchy.core.Projection.Projection at 0x127bd5450&gt;\n\n\n\nann.compile()\n\nCompiling ...  OK \n\n\nWe create a monitor to record the evolution of the firing rates in the reservoir during the simulation.\n\nm = ann.Monitor(pop, 'r')\nn = ann.Monitor(inp, 'r')\n\nA single trial lasts 3 second by default, with a step input between 100 and 200 ms. We define the trial in a method, so we can run it multiple times.\n\ndef trial(T=3000.):\n    \"Runs two trials for a given spectral radius.\"\n\n    # Reset firing rates\n    inp.r = 0.0\n    pop.x = 0.0\n    pop.r = 0.0\n    \n    # Run the trial\n    ann.simulate(100.)\n    inp.r = 1.0\n    ann.simulate(100.0) # initial stimulation\n    inp.r = 0.0\n    ann.simulate(T - 200.)\n    \n    return m.get('r')\n\nWe run two trials successively to look at the chaoticity depending on g.\n\npop.g = 1.5\ndata1 = trial()\ndata2 = trial()\n\n\nplt.figure(figsize=(12, 12))\nplt.subplot(311)\nplt.title(\"First trial\")\nfor i in range(5):\n    plt.plot(data1[:, i], lw=2)\nplt.subplot(312)\nplt.title(\"Second trial\")\nfor i in range(5):\n    plt.plot(data2[:, i], lw=2)\nplt.subplot(313)\nplt.title(\"Difference\")\nfor i in range(5):\n    plt.plot(data1[:, i] - data2[:, i], lw=2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can now train the readout neurons to reproduce a step signal after 2 seconds.\nFor simplicity, we just train a L1-regularized linear regression (LASSO) on the reservoir activity.\n\ntarget = np.zeros(3000)\ntarget[2000:2500] = 1.0\n\n\nfrom sklearn import linear_model\nreg = linear_model.Lasso(alpha=0.001, max_iter=10000)\nreg.fit(data1, target)\npred = reg.predict(data2)\n\n\nplt.figure(figsize=(12, 8))\nplt.plot(pred, lw=3)\nplt.plot(target, lw=3)\n\nsns.despine()\nplt.tight_layout()\nplt.show()\n\nNameError: name 'sns' is not defined",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Echo-state networks"
    ]
  },
  {
    "objectID": "notebooks/COBA.html",
    "href": "notebooks/COBA.html",
    "title": "COBA and CUBA networks",
    "section": "",
    "text": "#!pip install ANNarchy\nThis notebook reproduces the benchmarks used in:\nThey are based on the balanced network proposed by:\nEach network is composed of 4000 neurons (3200 excitatory and 800 inhibitory), reciprocally connected with a probability of 0.02 (sparse connection).\nThe CUBA network uses a current-based integrate-and-fire neuron model:\n\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) - g_\\text{inh} (t)\nwhile the COBA model uses conductance-based IF neurons:\n\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) * (E_\\text{exc} - v(t)) + g_\\text{inh} (t) * (E_\\text{inh} - v(t)) + I(t)\nApart from the neuron model and synaptic weights, both networks are equal, so we’ll focus on the COBA network here.\nThe discretization step has to be set to 0.1 ms:\nimport numpy as np\nimport ANNarchy as ann\n\nann.setup(dt=0.1) \n\nANNarchy 4.8 (4.8.0) on darwin (posix).",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "COBA/CUBA"
    ]
  },
  {
    "objectID": "notebooks/COBA.html#neuron-definition",
    "href": "notebooks/COBA.html#neuron-definition",
    "title": "COBA and CUBA networks",
    "section": "Neuron definition",
    "text": "Neuron definition\n\nCOBA = ann.Neuron(\n    parameters=\"\"\"\n        El = -60.0          : population\n        Vr = -60.0          : population\n        Erev_exc = 0.0      : population\n        Erev_inh = -80.0    : population\n        Vt = -50.0          : population\n        tau = 20.0          : population\n        tau_exc = 5.0       : population\n        tau_inh = 10.0      : population\n        I = 20.0            : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I\n\n        tau_exc * dg_exc/dt = - g_exc\n        tau_inh * dg_inh/dt = - g_inh\n    \"\"\",\n    spike = \"v &gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\n\nCUBA = ann.Neuron(\n    parameters=\"\"\"\n        El = -49.0      : population\n        Vr = -60.0      : population\n        Vt = -50.0      : population\n        tau_m = 20.0    : population\n        tau_exc = 5.0   : population\n        tau_inh = 10.0  : population\n    \"\"\",\n    equations=\"\"\"\n        tau_m * dv/dt = (El - v) + g_exc - g_inh \n\n        tau_exc * dg_exc/dt = - g_exc \n        tau_inh * dg_inh/dt = - g_inh \n    \"\"\",\n    spike = \"v &gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\nThe neurons define exponentially-decreasing conductance g_exc and g_inh for the excitatory and inhibitory conductances/currents, respectively. They also define a refractory period of 5 ms.",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "COBA/CUBA"
    ]
  },
  {
    "objectID": "notebooks/COBA.html#population",
    "href": "notebooks/COBA.html#population",
    "title": "COBA and CUBA networks",
    "section": "Population",
    "text": "Population\n\n# COBA network\nP_COBA = ann.Population(geometry=4000, neuron=COBA)\nP_COBA_E = P_COBA[:3200] ; P_COBA_I = P_COBA[3200:]\n\n# CUBA network\nP_CUBA = ann.Population(geometry=4000, neuron=CUBA)\nP_CUBA_E = P_CUBA[:3200] ; P_CUBA_I = P_CUBA[3200:]\n\nFor both networks, we create a population of COBA neurons, and assign the 3200 first ones to an excitatory sub-population and the 800 last ones to an inhibitory sub-population.\nIt would have been equivalent to declare two separate populations as:\nP_COBA_E = ann.Population(geometry=3200, neuron=COBA)\nP_COBA_I = ann.Population(geometry= 800, neuron=COBA)\nbut splitting a global population allows to apply methods to all neurons, for example when recording all spikes with a single monitor, or when initializing populations parameters uniformly.\nWe now initialize the variables of both populations:\n\n# COBA\nP_COBA.v = ann.Normal(-55.0, 5.0)\nP_COBA.g_exc = ann.Normal(4.0, 1.5)\nP_COBA.g_inh = ann.Normal(20.0, 12.0)\n\n# CUBA\nP_CUBA.v = ann.Uniform(-60.0, -50.0)",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "COBA/CUBA"
    ]
  },
  {
    "objectID": "notebooks/COBA.html#connections",
    "href": "notebooks/COBA.html#connections",
    "title": "COBA and CUBA networks",
    "section": "Connections",
    "text": "Connections\nThe neurons are randomly connected with a probability of 0.02. Excitatory neurons project on all other neurons with the target “exc” and a weight of 0.6 (COBA) or , while the inhibitory neurons have the target “inh” and a weight of 6.7.\n\n# COBA\nwe_COBA = 0.6\nwi_COBA = 6.7\nCe_COBA = ann.Projection(pre=P_COBA_E, post=P_COBA, target='exc')\nCe_COBA.connect_fixed_probability(weights=we_COBA, probability=0.02)\n\nCi_COBA = ann.Projection(pre=P_COBA_I, post=P_COBA, target='inh')\nCi_COBA.connect_fixed_probability(weights=wi_COBA, probability=0.02)\n\n# CUBA\nwe_CUBA = 0.27 * 60.0 / 10.0 # 0.7 * (Vmean - E_rev_exc) / gL (mV)\nwi_CUBA = 4.5 * 20.0 / 10.0 # 4.5 * (Vmean - E_rev_inh) / gL (mV)\nCe_CUBA = ann.Projection(pre=P_CUBA_E, post=P_CUBA, target='exc')\nCe_CUBA.connect_fixed_probability(weights=we_CUBA, probability=0.02)\n\nCi_CUBA = ann.Projection(pre=P_CUBA_I, post=P_CUBA, target='inh')\nCi_CUBA.connect_fixed_probability(weights=wi_CUBA, probability=0.02)\n\n&lt;ANNarchy.core.Projection.Projection at 0x12975bcd0&gt;\n\n\n\nann.compile()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "COBA/CUBA"
    ]
  },
  {
    "objectID": "notebooks/COBA.html#simulation",
    "href": "notebooks/COBA.html#simulation",
    "title": "COBA and CUBA networks",
    "section": "Simulation",
    "text": "Simulation\nWe first define a monitor to record the spikes emitted in the two populations:\n\nm_COBA = ann.Monitor(P_COBA, ['spike'])\nm_CUBA = ann.Monitor(P_CUBA, ['spike'])\n\nWe can then simulate for 1 second:\n\nann.simulate(1000.)\n\nWe retrieve the recorded spikes from the monitor:\n\ndata_COBA = m_COBA.get('spike')\ndata_CUBA = m_CUBA.get('spike')\n\nand compute a raster plot from the data:\n\nt_COBA, n_COBA = m_COBA.raster_plot(data_COBA)\nt_CUBA, n_CUBA = m_CUBA.raster_plot(data_CUBA)\n\nt and n are lists representing for each spike emitted during the simulation the time at which it was emitted and the index the neuron which fired. The length of this list represents the total number of spikes in the popultion, so we can compute the population mean firing rate:\n\nprint('Mean firing rate in the COBA population: ' + str(len(t_COBA) / 4000.) + 'Hz')\nprint('Mean firing rate in the CUBA population: ' + str(len(t_CUBA) / 4000.) + 'Hz')\n\nMean firing rate in the COBA population: 20.8135Hz\nMean firing rate in the CUBA population: 5.623Hz\n\n\nFinally, we can show the raster plot with matplotlib:\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.title(\"COBA\")\nplt.plot(t_COBA, n_COBA, '.', markersize=0.5)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.subplot(122)\nplt.title(\"CUBA\")\nplt.plot(t_CUBA, n_CUBA, '.', markersize=0.5)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\n\n\n\n\n\nMore detailed information about the activity of the population is provided by the inter-spike interval and the coefficient of variation, for both of which values we offer methods provided by the Monitor class.\n\nisi_COBA = m_COBA.inter_spike_interval(data_COBA)\nisi_CUBA = m_COBA.inter_spike_interval(data_CUBA)\n\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.title(\"COBA\")\nplt.hist(isi_COBA)\nplt.xlabel('ISI (ms)')\nplt.ylabel('n in bin')\nplt.subplot(122)\nplt.title(\"CUBA\")\nplt.hist(isi_CUBA)\nplt.xlabel('ISI (ms)')\nplt.ylabel('n in bin')\nplt.show()\n\n\n\n\n\n\n\n\n\ncov_COBA = m_COBA.coefficient_of_variation(data_COBA)\ncov_CUBA = m_COBA.coefficient_of_variation(data_CUBA)\n\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.title(\"COBA\")\nplt.hist(cov_COBA)\nplt.xlabel('ISI CV')\nplt.ylabel('n in bin')\nplt.subplot(122)\nplt.title(\"CUBA\")\nplt.hist(cov_CUBA)\nplt.xlabel('ISI CV')\nplt.ylabel('n in bin')\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "COBA/CUBA"
    ]
  },
  {
    "objectID": "notebooks/Miconi.html",
    "href": "notebooks/Miconi.html",
    "title": "Miconi network",
    "section": "",
    "text": "#!pip install ANNarchy\n\nReward-modulated recurrent network based on:\n\nMiconi T. (2017). Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks. eLife 6:e20899. doi:10.7554/eLife.20899\n\n\nimport numpy as np\nimport ANNarchy as ann\nann.clear()\nann.setup(dt=1.0)\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nEach neuron in the reservoir follows the following equations:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nwhere \\xi(t) is a random perturbation at 3 Hz, with an amplitude randomly sampled between -A and +A.\nWe additionally keep track of the mean firing rate with a sliding average:\n\n    \\tilde{x}(t) = \\alpha \\, \\tilde{x}(t) + (1 - \\alpha) \\, x(t)\n\nThe three first neurons keep a constant rate throughout learning (1 or -1) to provide some bias to the other neurons.\n\nneuron = ann.Neuron(\n    parameters = \"\"\"\n        tau = 30.0 : population # Time constant\n        constant = 0.0 # The four first neurons have constant rates\n        alpha = 0.05 : population # To compute the sliding mean\n        f = 3.0 : population # Frequency of the perturbation\n        A = 16. : population # Perturbation amplitude. dt*A/tau should be 0.5...\n    \"\"\",\n    equations=\"\"\"\n        # Perturbation\n        perturbation = if Uniform(0.0, 1.0) &lt; f/1000.: 1.0 else: 0.0 \n        noise = if perturbation &gt; 0.5: A*Uniform(-1.0, 1.0) else: 0.0\n\n        # ODE for x\n        x += dt*(sum(in) + sum(exc) - x + noise)/tau\n\n        # Output r\n        rprev = r\n        r = if constant == 0.0: tanh(x) else: tanh(constant)\n\n        # Sliding mean\n        delta_x = x - x_mean\n        x_mean = alpha * x_mean + (1 - alpha) * x\n    \"\"\"\n)\n\nThe learning rule is defined by a trace e_{i, j}(t) for each synapse i \\rightarrow j incremented at each time step with:\n\n    e_{i, j}(t) = e_{i, j}(t-1) + (r_i (t) \\, x_j(t))^3\n\nAt the end T of a trial, a normalized reward (R -R_\\text{mean}) is delivered and all weights are updated using:\n\n    \\Delta w_{i, j} = - \\eta \\,  e_{i, j}(T) \\, (R -R_\\text{mean})\n\nAll traces are then reset to 0 for the next trial. Weight changes are clamped between -0.0003 and 0.0003.\nAs ANNarchy applies the synaptic equations at each time step, we need to introduce a boolean learning_phase which performs trace integration when 0, weight update when 1.\n\nsynapse = ann.Synapse(\n    parameters=\"\"\"\n        eta = 0.5 : projection # Learning rate\n        learning_phase = 0.0 : projection # Flag to allow learning only at the end of a trial\n        error = 0.0 : projection # Reward received\n        mean_error = 0.0 : projection # Mean Reward received\n        max_weight_change = 0.0003 : projection # Clip the weight changes\n    \"\"\",\n    equations=\"\"\"\n        # Trace\n        trace += if learning_phase &lt; 0.5:\n                    power(pre.rprev * (post.delta_x), 3)\n                 else:\n                    0.0\n\n        # Weight update only at the end of the trial\n        delta_w = if learning_phase &gt; 0.5:\n                eta * trace * (mean_error) * (error - mean_error)\n             else:\n                 0.0 : min=-max_weight_change, max=max_weight_change\n        w -= if learning_phase &gt; 0.5:\n                delta_w\n             else:\n                 0.0\n    \"\"\"\n)\n\nWe model the DNMS task of Miconi. The RC network has two inputs A and B. The reservoir has 200 neurons, 3 of which have constant rates.\n\n# Input population\ninp = ann.Population(2, ann.Neuron(parameters=\"r=0.0\"))\n\n# Recurrent population\nN = 200\npop = ann.Population(N, neuron)\npop[0].constant = 1.0\npop[1].constant = 1.0\npop[2].constant = -1.0\npop.x = ann.Uniform(-0.1, 0.1)\n\nInput weights are uniformly distributed between -1 and 1.\nRecurrent weights and normally distributed, with a coupling strength of g=1.5 (edge of chaos).\nConnections are all-to-all (fully connected).\n\n# Input weights\nWi = ann.Projection(inp, pop, 'in')\nWi.connect_all_to_all(weights=ann.Uniform(-1.0, 1.0))\n\n# Recurrent weights\ng = 1.5\nWrec = ann.Projection(pop, pop, 'exc', synapse)\nWrec.connect_all_to_all(weights=ann.Normal(0., g/np.sqrt(N)), allow_self_connections=False)\n\n&lt;ANNarchy.core.Projection.Projection at 0x130d9f990&gt;\n\n\n\nann.compile()\n\nCompiling ...  OK \n\n\nThe output of the reservoir is chosen to be the neuron of index 100.\n\noutput_neuron = 100\n\nWe record the rates inside the reservoir:\n\nm = ann.Monitor(pop, ['r'])\n\nParameters defining the task:\n\n# Compute the mean reward per trial\nR_mean = np.zeros((2, 2))\nalpha = 0.75 # 0.33\n\n# Durations\nd_stim = 200\nd_delay= 200\nd_response = 400\nd_execution= 200\n\nDefinition of a DNMS trial (AA, AB, BA, BB):\n\ndef dnms_trial(trial, first, second, printing=False):\n    global R_mean\n    traces = []\n\n    # Reinitialize network\n    pop.x = ann.Uniform(-0.1, 0.1).get_values(N)\n    pop.r = np.tanh(pop.x)\n    pop[0].r = np.tanh(1.0)\n    pop[1].r = np.tanh(1.0)\n    pop[2].r = np.tanh(-1.0)\n\n    # First input\n    inp[first].r = 1.0\n    ann.simulate(d_stim)\n    \n    # Delay\n    inp.r = 0.0\n    ann.simulate(d_delay)\n    \n    # Second input\n    inp[second].r = 1.0\n    ann.simulate(d_stim)\n    \n    # Relaxation\n    inp.r = 0.0\n    ann.simulate(d_response)\n    \n    # Read the output\n    rec = m.get()\n    \n    # Compute the target\n    target = 0.98 if first != second else -0.98\n    \n    # Response is over the last 200 ms\n    output = rec['r'][-int(d_execution):, output_neuron] # neuron 100 over the last 200 ms\n    \n    # Compute the error\n    error = np.mean(np.abs(target - output))\n    if printing:\n        print('Target:', target, '\\tOutput:', \"%0.3f\" % np.mean(output), '\\tError:',  \"%0.3f\" % error, '\\tMean:', \"%0.3f\" % R_mean[first, second])\n    \n    # The first 25 trial do not learn, to let R_mean get realistic values\n    if trial &gt; 25:\n\n        # Apply the learning rule\n        Wrec.learning_phase = 1.0\n        Wrec.error = error\n        Wrec.mean_error = R_mean[first, second]\n\n        # Learn for one step\n        ann.step()\n        \n        # Reset the traces\n        Wrec.learning_phase = 0.0\n        Wrec.trace = 0.0\n        _ = m.get() # to flush the recording of the last step\n\n    # Update the mean reward\n    R_mean[first, second] = alpha * R_mean[first, second] + (1.- alpha) * error\n\n    return rec, traces\n\n\nfrom IPython.display import clear_output\nprinting = False\n\n# Many trials of each type\nmean_rewards = []\ntry:\n    for trial in range(1500):\n        if printing:\n            clear_output(wait=True)\n            print('Trial', trial)\n\n        # Perform the four different trials successively\n        recordsAA, tracesAA = dnms_trial(trial, 0, 0, printing)\n        recordsAB, tracesAB = dnms_trial(trial, 0, 1, printing)\n        recordsBA, tracesBA = dnms_trial(trial, 1, 0, printing)\n        recordsBB, tracesBB = dnms_trial(trial, 1, 1, printing)\n\n        # Record the initial trial\n        if trial == 0:\n            initialAA = recordsAA['r']\n            initialAB = recordsAB['r']\n            initialBA = recordsBA['r']\n            initialBB = recordsBB['r']\n\n        mean_rewards.append(R_mean.copy())\n\nexcept KeyboardInterrupt:\n    pass\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nmean_rewards = np.array(mean_rewards)\n\nplt.figure(figsize=(10, 6))\nplt.plot(mean_rewards.mean(axis=(1,2)), label='mean')\nplt.plot(mean_rewards[:, 0, 0], label='AA')\nplt.plot(mean_rewards[:, 0, 1], label='AB')\nplt.plot(mean_rewards[:, 1, 0], label='BA')\nplt.plot(mean_rewards[:, 1, 1], label='BB')\nplt.xlabel(\"Trials\")\nplt.ylabel(\"Mean error\")\nplt.legend()\n\nplt.figure(figsize=(12, 10))\n\nax = plt.subplot(221)\nax.plot(np.mean(initialAA[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsAA['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.legend()\nax.set_title('Output AA -1')\nax = plt.subplot(222)\nax.plot(np.mean(initialBA[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsBA['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.legend()\nax.set_title('Output BA +1')\nax = plt.subplot(223)\nax.plot(np.mean(initialAB[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsAB['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.set_title('Output AB +1')\nax = plt.subplot(224)\nax.plot(np.mean(initialBB[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsBB['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.set_title('Output BB -1')\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Miconi network"
    ]
  },
  {
    "objectID": "notebooks/BayesianOptimization.html",
    "href": "notebooks/BayesianOptimization.html",
    "title": "Hyperparameter optimization",
    "section": "",
    "text": "#!pip install ANNarchy\n\nMost of the work in computational neuroscience is to guess the values of parameters which are not constrained by the biology. The most basic approach is to simply try out different values, run the simulation, reason about why the results are not what you want, change some parameters, run again, etc. It is very easy to get lost in this process and it requires a great deal of intuition about how the model works.\nIf you are able to define an objective function for your model (a single number that tells how well your model performs), you can use search algorithms to find this hyperparameters automatically, at the cost of running your model multiple times.\nLet’s take the example of a rate-coded model depending on two hyperparameters a and b, where is the objective is to have a minimal activity after 1 s of simulation (dummy example):\nimport ANNarchy as ann\n\npop = ann.Population(...)\n...\nann.compile()\n\ndef run(a, b):\n    pop.a = a\n    pop.b = b\n    \n    ann.simulate(1000.)\n    \n    return (pop.r)**2\nGrid search would iterate over all possible values of the parameters to perform the search:\nmin_loss = 1000.\nfor a in np.linspace(0.0, 1.0, 100):\n    for b in np.linspace(0.0, 1.0, 100):\n        loss = run(a, b)\n        if loss &lt; min_loss:\n            min_loss = loss\n            a_best = a ; b_best = b\nIf you try 100 values for each parameters, you need 10000 simulations to find your parameters. The number of simulations explodes with the number of free parameters. Moreover, you cannot stop the search before the end, as you could miss the interesting region.\nRandom search samples blindly values for the hyperparameters:\nmin_loss = 1000.\nfor _ in range(1000):\n    a = np.random.uniform(0.0, 1.0)\n    b = np.random.uniform(0.0, 1.0)\n    loss = run(a, b)\n    if loss &lt; min_loss:\n        min_loss = loss\n        a_best = a ; b_best = b\nIf you are lucky, you may find a good solution quite early in the search, so you can stop it when the loss is below a desired threshold. The main drawback is that the search may spend a lot of time in uninteresting regions: it does not learn anything between two samples.\nAn often much more efficient search method is Bayesian optimization (also called sequential model-based optimization - SMBO). It is a form of random search that updates beliefs on the hyperparameters. In short, if some parameter values do not lead to good values of the objective function in early samples, they will not be used in later samples. The search becomes more and more focused on the interesting regions of the hyperparameter space.\nAs always with Python, there are many libraries for that, including:\n\nhyperopt https://github.com/hyperopt/hyperopt\noptuna https://github.com/pfnet/optuna\ntalos (for keras models) https://github.com/autonomio/talos\n\nThis notebook demonstrates how to use hyperopt to find some hyperparameters of the COBA models already included in the ANNarchy examples:\nhttps://annarchy.github.io/notebooks/COBA.html\nAdditionally, we will use the tensorboard extension to visualize the dependency between the parameters and the objective function.\n\nimport numpy as np\n\nimport ANNarchy as ann\nfrom ANNarchy.extensions.tensorboard import Logger\nann.clear()\nann.setup(dt=0.1)\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\n\nCOBA = ann.Neuron(\n    parameters=\"\"\"\n        El = -60.0          : population\n        Vr = -60.0          : population\n        Erev_exc = 0.0      : population\n        Erev_inh = -80.0    : population\n        Vt = -50.0          : population\n        tau = 20.0          : population\n        tau_exc = 5.0       : population\n        tau_inh = 10.0      : population\n        I = 20.0            : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I\n\n        tau_exc * dg_exc/dt = - g_exc\n        tau_inh * dg_inh/dt = - g_inh\n    \"\"\",\n    spike = \"v &gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\nP = ann.Population(geometry=4000, neuron=COBA)\nPe = P[:3200]\nPi = P[3200:]\nP.v = ann.Normal(-55.0, 5.0)\nP.g_exc = ann.Normal(4.0, 1.5)\nP.g_inh = ann.Normal(20.0, 12.0)\n\nCe = ann.Projection(pre=Pe, post=P, target='exc')\nCe.connect_fixed_probability(weights=0.6, probability=0.02)\nCi = ann.Projection(pre=Pi, post=P, target='inh')\nCi.connect_fixed_probability(weights=6.7, probability=0.02)\n\nann.compile()\n\nm = ann.Monitor(P, ['spike'])\n\nCompiling ...  OK \n\n\nWith the default parameters, the COBA network fires at around 20 Hz:\n\nann.simulate(1000.0)\ndata = m.get('spike')\nfr = m.mean_fr(data)\nprint(fr)\n\n20.947999999999997\n\n\nLet’s suppose we now want the network to fire at 30 Hz. Which parameters should we change to obtain that value?\nMany parameters might influence the firing rate of the network (if not all). Here, we make the assumption that the weight values for the excitatory connections (0.6) and inhibitory ones (6.7) are the most critical ones.\nLet’s start by importing hyperopt (after installing it with pip install hyperopt):\n\nfrom hyperopt import fmin, tpe, hp, STATUS_OK\n\nWe define a trial() method taking values for the two hyperparameters as inputs. It starts by resetting the network, sets the excitatory and inhibitory weights to the desired value, simulates for one second, computes the mean firing rate of the population, logs the parameters and finally returns the objective function: the squared error between the recorded firing rate and 30 Hz.\n\nlogger = Logger()\n\ndef trial(args):\n    \n    # Retrieve the parameters\n    w_exc = args[0]\n    w_inh = args[1]\n    \n    # Reset the network\n    ann.reset()\n    \n    # Set the hyperparameters\n    Ce.w = w_exc\n    Ci.w = w_inh\n    \n    # Simulate 1 second\n    ann.simulate(1000.0)\n\n    # Retrieve the spike recordings and the membrane potential\n    spikes = m.get('spike')\n\n    # Compute the population firing rate\n    fr = m.mean_fr(spikes)\n    \n    # Compute a quadratic loss around 30 Hz\n    loss = 0.001*(fr - 30.0)**2   \n    \n    # Log the parameters\n    logger.add_parameters({'w_exc': w_exc, 'w_inh': w_inh},\n                         {'loss': loss, 'firing_rate': fr})\n    \n    return {\n        'loss': loss,\n        'status': STATUS_OK,\n        # -- store other results like this\n        'fr': fr,\n        }\n\nLogging in runs/Apr18_12-02-46_Juliens-MBP\n\n\nWe can check that the default parameters indeed lead to a firing rate of 20 Hz:\n\ntrial([0.6, 6.7])\n\n{'loss': 0.08193870400000006, 'status': 'ok', 'fr': 20.947999999999997}\n\n\nWe can now use hyperopt to find the hyperparameters making the network fire at 30 Hz.\nThe fmin() function takes:\n\nfn: the objective function for a set of parameters.\nspace: the search space for the hyperparameters (the prior).\nalgo: which algorithm to use, either tpe.suggest or random.suggest\nmax_evals: number of samples (simulations) to make.\n\nHere, we will sample the excitatory weights between 0.1 and 1, the inhibitory ones between 1 and 10. Of course, the smaller the range, the better. Refer to the doc of hyperopt for other sampling priors.\n\nbest = fmin(\n    fn=trial,\n    space=[\n        hp.uniform('w_exc', 0.1, 1.0), \n        hp.uniform('w_inh', 1.0, 10.0)\n    ],\n    algo=tpe.suggest,\n    max_evals=100)\nprint(best)\n\n100%|██████████| 100/100 [00:14&lt;00:00,  7.04trial/s, best loss: 4.171806249999784e-05]\n{'w_exc': 0.9464537856379768, 'w_inh': 7.772957927545114}\n\n\nAfter 100 simulations, hyperopt returns a set of hyperparameter values that make the network fire at 30Hz. We can check that it is true with:\n\ntrial([best['w_exc'], best['w_inh']])\n\n{'loss': 4.171806249999784e-05, 'status': 'ok', 'fr': 30.204249999999995}\n\n\nThere are plenty of options to hyperopt (check Trials or the parallel search using MongoDB), but this simple example should get you started.\nIf we start tensorboard in the default directory runs/, we can additionally visualize how the firing rate depends on w_exc and w_inh in the HPARAMS tab.\n\nlogger.close()\n%load_ext tensorboard\n%tensorboard --logdir runs\n\n\n      \n      \n      \n    \n\n\n\n\n\nScreen Capture_select-area_20200529161854.png",
    "crumbs": [
      "Notebooks",
      "**Advanced features**",
      "Bayesian optimization"
    ]
  },
  {
    "objectID": "notebooks/BoldMonitoring2.html",
    "href": "notebooks/BoldMonitoring2.html",
    "title": "Recording BOLD signals - Davis model",
    "section": "",
    "text": "#!pip install ANNarchy\nimport numpy as np\n\nimport ANNarchy as ann\nfrom ANNarchy.extensions.bold import *\n\nANNarchy 4.8 (4.8.0) on darwin (posix).",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "BOLD monitor II"
    ]
  },
  {
    "objectID": "notebooks/BoldMonitoring2.html#davis-model",
    "href": "notebooks/BoldMonitoring2.html#davis-model",
    "title": "Recording BOLD signals - Davis model",
    "section": "Davis model",
    "text": "Davis model\nLet’s now demonstrate how to define a custom BOLD model. The default Ballon model is defined by the following code:\nballoon_RN = BoldModel(\n    parameters = \"\"\"\n        phi       = 1.0         ;   kappa     = 1/1.54\n        gamma     = 1/2.46      ;   E_0       = 0.34\n        tau       = 0.98        ;   alpha     = 0.33\n        V_0       = 0.02        ;   v_0       = 40.3\n        TE        = 40/1000.    ;   epsilon   = 1.43\n        r_0       = 25.         ;   second    = 1000.0\n    \"\"\",\n    equations = \"\"\"\n        # CBF input\n        I_CBF          = sum(I_CBF)       \n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  \n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        # Balloon model\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear BOLD equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))\n    \"\"\",\n    inputs=['I_CBF']\n)\nIt is very similar to the interface of a Neuron model, with parameters and equations defined in two multi-line strings. The input signal I_CBF has to be explicitly defined in the inputs argument to help the BOLD monitor create the mapping.\nTo demonstrate how to create a custom BOLD model, let’s suppose we want a model that computes both the BOLD signal of the Balloon model and the one of the Davis model:\n\nDavis, T. L., Kwong, K. K., Weisskoff, R. M., and Rosen, B. R. (1998). Calibrated functional MRI: mapping the dynamics of oxidative metabolism. Proceedings of the National Academy of Sciences 95, 1834–1839\n\nWithout going into too many details, the Davis model computes the BOLD signal directly using f_in and E, without introducing a differential equation for the BOLD signal. Its implementation using the BOLD model would be:\nDavisModel = BoldModel(\n    parameters = \"\"\"\n        second = 1000.0\n        \n        phi    = 1.0    # Friston et al. (2000)\n        kappa  = 1/1.54\n        gamma  = 1/2.46\n        E_0    = 0.34\n        \n        M      = 0.149   # Griffeth & Buxton (2011)\n        alpha  = 0.14\n        beta   = 0.91\n    \"\"\",\n    equations = \"\"\"\n        # CBF-driving input as in Friston et al. (2000)\n        I_CBF    = sum(I_CBF)                                             : init=0\n        ds/dt    = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  : init=0\n        df_in/dt = s  / second                                            : init=1, min=0.01\n    ​\n        # Using part of the Balloon model to calculate r (normalized CMRO2) as in Buxton et al. (2004)\n        E        = 1 - (1 - E_0)**(1 / f_in)                              : init=0.34\n        r        = f_in * E / E_0\n        \n        # Davis model\n        BOLD     = M * (1 - f_in**alpha * (r / f_in)**beta)               : init=0\n    \"\"\",\n    inputs=['I_CBF']\n)\nNote that we could simply define two BOLD monitors using different models, but let’s create a complex model that does both for the sake of demonstration.\nLet’s first redefine the populations of the previous section:\n\n# Two populations of 100 izhikevich neurons\npop0 = ann.Population(100, neuron=ann.Izhikevich)\npop1 = ann.Population(100, neuron=ann.Izhikevich)\n\n# Set noise to create some baseline activity\npop0.noise = 5.0; pop1.noise = 5.0\n\n# Compute mean firing rate in Hz on 100ms window\npop0.compute_firing_rate(window=100.0)\npop1.compute_firing_rate(window=100.0)\n\n# Create required monitors\nmon_pop0 = ann.Monitor(pop0, [\"r\"], start=False)\nmon_pop1 = ann.Monitor(pop1, [\"r\"], start=False)\n\nWe can now create a hybrid model computing both the Balloon RN model of Stephan et al. (2007) and the Davis model:\n\nballoon_Davis = BoldModel(\n    parameters = \"\"\"\n        phi       = 1.0         ;   kappa     = 1/1.54\n        gamma     = 1/2.46      ;   E_0       = 0.34\n        tau       = 0.98        ;   alpha     = 0.33\n        V_0       = 0.02        ;   v_0       = 40.3\n        TE        = 40/1000.    ;   epsilon   = 1.43\n        r_0       = 25.         ;   second    = 1000.0\n        M         = 0.062       ;   alpha2    = 0.14\n        beta      = 0.91\n    \"\"\",\n    equations = \"\"\"\n        # CBF input\n        I_CBF          = sum(I_CBF)       \n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  \n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        # Balloon model\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear BOLD equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))\n        \n        # Davis model\n        r = f_in * E / E_0                                                         : init=1, min=0.01\n        BOLD_Davis =  M * (1 - f_in**alpha2 * (r / f_in)**beta) \n    \"\"\",\n    inputs=['I_CBF']\n)\n\nWe now only need to pass that new object to the BOLD monitor, and specify that we want to record both BOLD and BOLD_Davis:\n\nm_bold = BoldMonitor(\n    \n    populations = [pop0, pop1],  \n    \n    bold_model = balloon_Davis,\n    \n    mapping={'I_CBF': 'r'},            \n    \n    normalize_input=2000, \n    \n    recorded_variables=[\"I_CBF\", \"BOLD\", \"BOLD_Davis\"]\n)\n\nann.compile()\n\nCompiling ...  OK \n\n\nWe run the same simulation protocol and compare the two BOLD signals. Note that the value of M has been modified to give a similar amplitude to both signals:\n\n# Ramp up time\nann.simulate(1000)\n\n# Start recording\nmon_pop0.start()\nmon_pop1.start()\nm_bold.start()\n\n# we manipulate the noise for the half of the neurons\nann.simulate(5000)      # 5s with low noise\npop0.noise = 7.5\nann.simulate(5000)      # 5s with higher noise (one population)\npop0.noise = 5\nann.simulate(10000)     # 10s with low noise\n\n# retrieve the recordings\nmean_fr1 = np.mean(mon_pop0.get(\"r\"), axis=1)\nmean_fr2 = np.mean(mon_pop1.get(\"r\"), axis=1)\n\nIf_data = m_bold.get(\"I_CBF\")\nbold_data = m_bold.get(\"BOLD\")\ndavis_data = m_bold.get(\"BOLD_Davis\")\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,8))\ngrid = plt.GridSpec(1, 2, left=0.05, right=0.95)\n\n# mean firing rate\nax1 = plt.subplot(grid[0, 0])\nax1.plot(mean_fr1, label=\"pop0\")\nax1.plot(mean_fr2, label=\"pop1\")\nplt.legend()\nax1.set_ylabel(\"Average firing rate [Hz]\", fontweight=\"bold\", fontsize=18)\n\n# BOLD input signal as percent\nax2 = plt.subplot(grid[0, 1])\nax2.plot(bold_data*100.0, label=\"Balloon_RN\")\nax2.plot(davis_data*100.0, label=\"Davis\")\nplt.legend()\nax2.set_ylabel(\"BOLD [%]\", fontweight=\"bold\", fontsize=18)\n\n# x-axis labels as seconds\nfor ax in [ax1, ax2]:\n    ax.set_xticks(np.arange(0,21,2)*1000)\n    ax.set_xticklabels(np.arange(0,21,2))\n    ax.set_xlabel(\"time [s]\", fontweight=\"bold\", fontsize=18)\n\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "BOLD monitor II"
    ]
  },
  {
    "objectID": "notebooks/HodgkinHuxley.html",
    "href": "notebooks/HodgkinHuxley.html",
    "title": "Hodgkin Huxley neuron",
    "section": "",
    "text": "#!pip install ANNarchy\n\nSimple Hodgkin-Huxley neuron.\n\nimport numpy as np\nimport ANNarchy as ann\nann.clear()\n\ndt=0.01\nann.setup(dt=dt)\n\nHH = ann.Neuron(\n\n    parameters = \"\"\"\n    C = 1.0 # Capacitance\n    VL = -59.387 # Leak voltage\n    VK = -82.0 # Potassium reversal voltage\n    VNa = 45.0 # Sodium reveral voltage\n    gK = 36.0 # Maximal Potassium conductance\n    gNa = 120.0 # Maximal Sodium conductance\n    gL = 0.3 # Leak conductance\n    vt = 30.0 # Threshold for spike emission\n    I = 0.0 # External current\n    \"\"\",\n\n    equations = \"\"\"\n    # Previous membrane potential\n    prev_V = V\n\n    # Voltage-dependency parameters\n    an = 0.01 * (V + 60.0) / (1.0 - exp(-0.1* (V + 60.0) ) )\n    am = 0.1 * (V + 45.0) / (1.0 - exp (- 0.1 * ( V + 45.0 )))\n    ah = 0.07 * exp(- 0.05 * ( V + 70.0 ))\n\n    bn = 0.125 * exp (- 0.0125 * (V + 70.0))\n    bm = 4.0 *  exp (- (V + 70.0) / 80.0)\n    bh = 1.0/(1.0 + exp (- 0.1 * ( V + 40.0 )) )\n\n    # Alpha/Beta functions\n    dn/dt = an * (1.0 - n) - bn * n : init = 0.3, midpoint\n    dm/dt = am * (1.0 - m) - bm * m : init = 0.0, midpoint\n    dh/dt = ah * (1.0 - h) - bh * h : init = 0.6, midpoint\n\n    # Membrane equation\n    C * dV/dt = gL * (VL - V ) + gK * n**4 * (VK - V) + gNa * m**3 * h * (VNa - V) + I : midpoint\n\n    \"\"\",\n\n    spike = \"\"\"\n    # Spike is emitted when the membrane potential crosses the threshold from below\n    (V &gt; vt) and (prev_V &lt;= vt)    \n    \"\"\",\n\n    reset = \"\"\"\n    # Nothing to do, it is built-in...\n    \"\"\"\n)\n\npop = ann.Population(neuron=HH, geometry=1)\npop.V = -50.0\n\nann.compile()\n\nm = ann.Monitor(pop, ['spike', 'V', 'n', 'm', 'h'])\n\n# Preparation\nann.simulate(100.0)\n# Current impulse for 1 ms\npop.I = 200.0\nann.simulate(1.0)\n# Reset\npop.I = 0.0\nann.simulate(100.0)\n\ndata = m.get()\n\ntstart = int(90.0/dt)\ntstop  = int(120.0/dt)\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\nplt.subplot(2,2,1)\nplt.plot(90.0 + dt*np.arange(tstop-tstart), data['V'][tstart:tstop, 0])\nplt.title('V')\nplt.subplot(2,2,2)\nplt.plot(90.0 + dt*np.arange(tstop-tstart), data['n'][tstart:tstop, 0])\nplt.title('n')\nplt.ylim((0.0, 1.0))\nplt.subplot(2,2,3)\nplt.plot(90.0 + dt*np.arange(tstop-tstart), data['m'][tstart:tstop, 0])\nplt.title('m')\nplt.ylim((0.0, 1.0))\nplt.subplot(2,2,4)\nplt.plot(90.0 + dt*np.arange(tstop-tstart), data['h'][tstart:tstop, 0])\nplt.title('h')\nplt.ylim((0.0, 1.0))\nplt.show()\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\nCompiling ...  OK",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Hodgkin-Huxley"
    ]
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "List of notebooks",
    "section": "",
    "text": "This section provides a list of the sample models provided in the examples/ directory of the source code.\nThe Jupyter notebooks can be downloaded from:\nhttps://github.com/ANNarchy/ANNarchy.github.io/tree/master/notebooks\n\n\n\nEcho-state networks: echo-state networks are the rate-coded version of reservoir computing (Jaeger, 2001).\nNeural Field: a simple rate-coded model without learning using neural fields.\nBCM learning rule: basic demonstration of the Intrator & Cooper BCM learning rule.\nBar learning: an implementation of the bar learning problem, illustrating synaptic plasticity in rate-coded networks.\nMiconi: Reward-modulated recurrent network based on Miconi (2017).\nStructural Plasticity: a dummy example demonstrating structural plasticity.\n\n\n\n\n\nAdEx: how the AdEx neuron model (adaptive exponential) can reproduce various spiking patterns in vivo (Naud et al. 2008).\nPyNN/Brian: a set of single neuron models reproducing various examples from PyNN and Brian.\nIzhikevich: an implementation of the simple pulse-coupled network described in (Izhikevich, 2003). It shows how to build a simple spiking network without synaptic plasticity.\nGap Junctions: an example using gap junctions.\nHodgkinHuxley: a single Hodgkin-Huxley neuron.\nCOBA and CUBA: an implementation of the balanced network described in (Vogels and Abbott, 2005). It shows how to build a simple spiking network using integrate-and-fire neurons and sparse connectivity.\nSTP: an example of short-term plasticity based on the model of Tsodyks, Uziel and Markram (2000).\nSTDP I and II: two simple examples using spike-timing dependent plasticity (STDP).\nRamp: an example of homeostatic STDP based on the model of Carlson, Richert, Dutt and Krichmar (2013).\n\n\n\n\n\nHybrid networks: a simple hybrid network with both rate-coded and spiking sub-parts.\nParallel simulations: shows how to call parallel_run to run several networks in parallel.\nBayesian optimization: a demo showing how to use hyperopt to search for hyperparameters of a model.\n\n\n\n\n\nImage and Convolution: shows how to use the ImagePopulation class of the image extension to clamp directly images and video streams into a rate-coded network. Also demonstrates the convolution extension.\nLogging with tensorboard: a simple basal ganglia model to show how to use the tensorboard extension.\nBOLD monitoring I and II: a showcase of the bold extension allowing to record BOLD signals fron a network.\nANN2SNN: demonstrates the ANN-to-SNN conversion tool using the MNIST dataset.",
    "crumbs": [
      "Notebooks",
      "List of notebooks"
    ]
  },
  {
    "objectID": "notebooks/index.html#rate-coded-networks",
    "href": "notebooks/index.html#rate-coded-networks",
    "title": "List of notebooks",
    "section": "",
    "text": "Echo-state networks: echo-state networks are the rate-coded version of reservoir computing (Jaeger, 2001).\nNeural Field: a simple rate-coded model without learning using neural fields.\nBCM learning rule: basic demonstration of the Intrator & Cooper BCM learning rule.\nBar learning: an implementation of the bar learning problem, illustrating synaptic plasticity in rate-coded networks.\nMiconi: Reward-modulated recurrent network based on Miconi (2017).\nStructural Plasticity: a dummy example demonstrating structural plasticity.",
    "crumbs": [
      "Notebooks",
      "List of notebooks"
    ]
  },
  {
    "objectID": "notebooks/index.html#spiking-networks",
    "href": "notebooks/index.html#spiking-networks",
    "title": "List of notebooks",
    "section": "",
    "text": "AdEx: how the AdEx neuron model (adaptive exponential) can reproduce various spiking patterns in vivo (Naud et al. 2008).\nPyNN/Brian: a set of single neuron models reproducing various examples from PyNN and Brian.\nIzhikevich: an implementation of the simple pulse-coupled network described in (Izhikevich, 2003). It shows how to build a simple spiking network without synaptic plasticity.\nGap Junctions: an example using gap junctions.\nHodgkinHuxley: a single Hodgkin-Huxley neuron.\nCOBA and CUBA: an implementation of the balanced network described in (Vogels and Abbott, 2005). It shows how to build a simple spiking network using integrate-and-fire neurons and sparse connectivity.\nSTP: an example of short-term plasticity based on the model of Tsodyks, Uziel and Markram (2000).\nSTDP I and II: two simple examples using spike-timing dependent plasticity (STDP).\nRamp: an example of homeostatic STDP based on the model of Carlson, Richert, Dutt and Krichmar (2013).",
    "crumbs": [
      "Notebooks",
      "List of notebooks"
    ]
  },
  {
    "objectID": "notebooks/index.html#advanced-features",
    "href": "notebooks/index.html#advanced-features",
    "title": "List of notebooks",
    "section": "",
    "text": "Hybrid networks: a simple hybrid network with both rate-coded and spiking sub-parts.\nParallel simulations: shows how to call parallel_run to run several networks in parallel.\nBayesian optimization: a demo showing how to use hyperopt to search for hyperparameters of a model.",
    "crumbs": [
      "Notebooks",
      "List of notebooks"
    ]
  },
  {
    "objectID": "notebooks/index.html#extensions",
    "href": "notebooks/index.html#extensions",
    "title": "List of notebooks",
    "section": "",
    "text": "Image and Convolution: shows how to use the ImagePopulation class of the image extension to clamp directly images and video streams into a rate-coded network. Also demonstrates the convolution extension.\nLogging with tensorboard: a simple basal ganglia model to show how to use the tensorboard extension.\nBOLD monitoring I and II: a showcase of the bold extension allowing to record BOLD signals fron a network.\nANN2SNN: demonstrates the ANN-to-SNN conversion tool using the MNIST dataset.",
    "crumbs": [
      "Notebooks",
      "List of notebooks"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html",
    "href": "notebooks/PyNN.html",
    "title": "PyNN and Brian examples",
    "section": "",
    "text": "#!pip install ANNarchy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ANNarchy as ann\n\nann.setup(dt=0.1)\n\nANNarchy 4.8 (4.8.0) on darwin (posix).",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html#if_curr_alpha",
    "href": "notebooks/PyNN.html#if_curr_alpha",
    "title": "PyNN and Brian examples",
    "section": "IF_curr_alpha",
    "text": "IF_curr_alpha\nSimple network with a Poisson spike source projecting to a pair of IF_curr_alpha neurons.\nThis is a reimplementation of the PyNN example:\nhttp://www.neuralensemble.org/trac/PyNN/wiki/Examples/simpleNetwork\n\n# Parameters\ntstop = 1000.0\nrate = 100.0\n\n# Create the Poisson spikes\nnumber = int(2*tstop*rate/1000.0)\nnp.random.seed(26278342)\nspike_times = np.add.accumulate(np.random.exponential(1000.0/rate, size=number))\n\n# Input population\ninp  = ann.SpikeSourceArray(list(spike_times))\n\n# Output population\npop = ann.Population(2, ann.IF_curr_alpha)\npop.tau_refrac = 2.0,\npop.v_thresh = -50.0,\npop.tau_syn_E = 2.0,\npop.tau_syn_I = 2.0\n\n# Excitatory projection\nproj = ann.Projection(inp, pop, 'exc')\nproj.connect_all_to_all(weights=1.0)\n\n# Monitor\nm = ann.Monitor(pop, ['spike', 'v'])\n\n# Compile the network\nnet = ann.Network()\nnet.add([inp, pop, proj, m])\nnet.compile()\n\n# Simulate\nnet.simulate(tstop)\ndata = net.get(m).get()\n\n# Plot the results\nplt.figure(figsize=(12, 8))\nplt.plot(ann.dt()*np.arange(tstop/ann.dt()), data['v'][:, 0])\nplt.xlabel('Time (ms)')\nplt.ylabel('Vm (mV)')\nplt.ylim([-66.0, -48.0])\nplt.title('Simple Network')\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html#if_cond_exp",
    "href": "notebooks/PyNN.html#if_cond_exp",
    "title": "PyNN and Brian examples",
    "section": "IF_cond_exp",
    "text": "IF_cond_exp\nA single IF neuron with exponential, conductance-based synapses, fed by two spike sources.\nThis is a reimplementation of the PyNN example:\nhttp://www.neuralensemble.org/trac/PyNN/wiki/Examples/IF_cond_exp\n\n# Parameters\ntstop = 200.0\n\n# Input populations with predetermined spike times\nspike_sourceE = ann.SpikeSourceArray(spike_times= [float(i) for i in range(5,105,10)] )\nspike_sourceI = ann.SpikeSourceArray(spike_times= [float(i) for i in range(155,255,10)])\n\n# Population with one IF_cond_exp neuron\nifcell = ann.Population(1, ann.IF_cond_exp)\nifcell.set(\n    {   'i_offset' : 0.1,    'tau_refrac' : 3.0,\n        'v_thresh' : -51.0,  'tau_syn_E'  : 2.0,\n        'tau_syn_I': 5.0,    'v_reset'    : -70.0,\n        'e_rev_E'  : 0.,     'e_rev_I'    : -80.0 } )\n\n\n# Projections\nconnE = ann.Projection(spike_sourceE, ifcell, 'exc')\nconnE.connect_all_to_all(weights=0.006, delays=2.0)\n\nconnI = ann.Projection(spike_sourceI, ifcell, 'inh')\nconnI.connect_all_to_all(weights=0.02,  delays=4.0)\n\n# Monitor\nm = ann.Monitor(ifcell, ['spike', 'v'])\n\n# Compile the network\nnet = ann.Network()\nnet.add([spike_sourceE, spike_sourceI, ifcell, connE, connI, m])\nnet.compile()\n\n# Simulate\nnet.simulate(tstop)\ndata = net.get(m).get()\n\n# Show the result\nplt.figure(figsize=(12, 8))\nplt.plot(ann.dt()*np.arange(tstop/ann.dt()), data['v'][:, 0])\nplt.xlabel('Time (ms)')\nplt.ylabel('Vm (mV)')\nplt.ylim([-66.0, -61.0])\nplt.title('IF_cond_exp')\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html#eif_cond_exp",
    "href": "notebooks/PyNN.html#eif_cond_exp",
    "title": "PyNN and Brian examples",
    "section": "EIF_cond_exp",
    "text": "EIF_cond_exp\nNetwork of EIF neurons with exponentially decreasing conductance-based synapses.\nThis is a reimplementation of the Brian example:\nhttp://brian.readthedocs.org/en/1.4.1/examples-misc_expIF_network.html\n\nEIF = ann.Neuron(\n    parameters = \"\"\"\n        v_rest = -70.0 : population\n        cm = 0.2 : population\n        tau_m = 10.0 : population\n        tau_syn_E = 5.0 : population\n        tau_syn_I = 10.0 : population\n        e_rev_E = 0.0 : population\n        e_rev_I = -80.0 : population\n        delta_T = 3.0 : population\n        v_thresh = -55.0 : population\n        v_reset = -70.0 : population\n        v_spike = -20.0 : population\n    \"\"\",\n    equations=\"\"\"\n        dv/dt = (v_rest - v +  delta_T * exp( (v-v_thresh)/delta_T) )/tau_m + ( g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) )/cm\n\n        tau_syn_E * dg_exc/dt = - g_exc\n        tau_syn_I * dg_inh/dt = - g_inh\n    \"\"\",\n    spike = \"\"\"\n        v &gt; v_spike\n    \"\"\",\n    reset = \"\"\"\n        v = v_reset\n    \"\"\",\n    refractory = 2.0\n\n)\n\n# Poisson inputs\ni_exc = ann.PoissonPopulation(geometry=200, rates=\"if t &lt; 200.0 : 2000.0 else : 0.0\")\ni_inh = ann.PoissonPopulation(geometry=200, rates=\"if t &lt; 100.0 : 2000.0 else : 0.0\")\n\n# Main population\nP = ann.Population(geometry=4000, neuron=EIF)\n\n# Subpopulations\nPe = P[:3200]\nPi = P[3200:]\n\n# Projections\nwe = 1.5 / 1000.0 # excitatory synaptic weight\nwi = 2.5 * we # inhibitory synaptic weight\n\nCe = ann.Projection(Pe, P, 'exc')\nCe.connect_fixed_probability(weights=we, probability=0.05)\n\nCi = ann.Projection(Pi, P, 'inh')\nCi.connect_fixed_probability(weights=wi, probability=0.05)\n\nIe = ann.Projection(i_exc, P[:200], 'exc') # inputs to excitatory cells\nIe.connect_one_to_one(weights=we)\n\nIi = ann.Projection(i_inh, P[3200:3400], 'exc')# inputs to inhibitory cells\nIi.connect_one_to_one(weights=we)\n\n# Initialization of variables\nP.v = -70.0 + 10.0 * np.random.rand(P.size)\nP.g_exc = (np.random.randn(P.size) * 2.0 + 5.0) * we\nP.g_inh = (np.random.randn(P.size) * 2.0 + 5.0) * wi\n\n# Monitor\nm = ann.Monitor(P, 'spike')\n\n# Compile the Network\nnet = ann.Network()\nnet.add([P, i_exc, i_inh, Ce, Ci, Ie, Ii, m])\nnet.compile()\n\n# Simulate\nnet.simulate(500.0, measure_time=True)\n\n# Retrieve recordings\ndata = net.get(m).get()\nt, n = net.get(m).raster_plot(data['spike'])\n\nplt.figure(figsize=(12, 8))\nplt.plot(t, n, '.', markersize=0.2)\nplt.show()\n\nCompiling network 3...  OK \nSimulating 0.5 seconds of the network 3 took 1.0771634578704834 seconds.",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html#aeif_cond_exp",
    "href": "notebooks/PyNN.html#aeif_cond_exp",
    "title": "PyNN and Brian examples",
    "section": "AEIF_cond_exp",
    "text": "AEIF_cond_exp\nAdaptive exponential integrate-and-fire model.\nhttp://www.scholarpedia.org/article/Adaptive_exponential_integrate-and-fire_model\nModel introduced in:\n\nBrette R. and Gerstner W. (2005), Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity, J. Neurophysiol. 94: 3637 - 3642.\n\nThis is a reimplementation of the Brian example:\nhttps://brian.readthedocs.io/en/stable/examples-frompapers_Brette_Gerstner_2005.html\n\n# Create a population with one AdEx neuron\npop = ann.Population(geometry=1, neuron=ann.EIF_cond_exp_isfa_ista)\n\n# Regular spiking (paper)\npop.tau_w, pop.a, pop.b, pop.v_reset = 144.0,  4.0, 0.0805, -70.6\n\n# Bursting\n#pop.tau_w, pop.a, pop.b, pop.v_reset = 20.0, 4.0, 0.5, pop.v_thresh + 5.0\n\n# Fast spiking\n#pop.tau_w, pop.a, pop.b, pop.v_reset = 144.0, 2000.0*pop.cm/144.0, 0.0, -70.6\n\n# Monitor\nm = ann.Monitor(pop, ['spike', 'v', 'w'])\n\n# Compile the network\nnet = ann.Network()\nnet.add([pop, m])\nnet.compile()\n\n\n# Add current of 1 nA and simulate\nnet.simulate(20.0)\nnet.get(pop).i_offset = 1.0\nnet.simulate(100.0)\nnet.get(pop).i_offset = 0.0\nnet.simulate(20.0)\n\n# Retrieve the results\ndata = net.get(m).get()\nspikes = data['spike'][0]\nv = data['v'][:, 0]\nw = data['w'][:, 0]\nif len(spikes)&gt;0:\n    v[spikes] = 20.0\n\n# Plot the activity\nplt.figure(figsize=(12, 8))\nplt.subplot(2,1,1)\nplt.plot(ann.dt()*np.arange(140.0/ann.dt()), v)\nplt.ylabel('v')\nplt.title('Adaptive exponential integrate-and-fire')\nplt.subplot(2,1,2)\nplt.plot(ann.dt()*np.arange(140.0/ann.dt()), w)\nplt.xlabel('Time (ms)')\nplt.ylabel('w')\nplt.show()\n\nCompiling network 4...  OK",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html#non-linear-synapses",
    "href": "notebooks/PyNN.html#non-linear-synapses",
    "title": "PyNN and Brian examples",
    "section": "Non-linear synapses",
    "text": "Non-linear synapses\nA single IF neuron with two non-linear NMDA synapses.\nThis is a reimplementation of the Brian example:\nhttp://brian.readthedocs.org/en/latest/examples-synapses_nonlinear_synapses.html\n\n# Neurons\nLinear = ann.Neuron(equations=\"dv/dt = 0.1\", spike=\"v&gt;1.0\", reset=\"v=0.0\")\nIntegrator = ann.Neuron(equations=\"dv/dt = 0.1*(g_exc -v)\", spike=\"v&gt;2.0\", reset=\"v=0.0\")\n\n# Non-linear synapse\nNMDA = ann.Synapse(\n    parameters = \"\"\"\n        tau = 10.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dx/dt = -x\n        tau * dg/dt = -g +  x * (1 -g)\n    \"\"\",\n    pre_spike = \"x += w\",\n    psp = \"g\"\n)\n\n# Populations\ninput = ann.Population(geometry=2, neuron=Linear)\ninput.v = [0.0, 0.5]\npop = ann.Population(geometry=1, neuron=Integrator)\n\n# Projection\nproj = ann.Projection(\n    pre=input, post=pop, target='exc',\n    synapse=NMDA)\nproj.connect_from_matrix(weights=[[1.0, 10.0]])\n\n# Monitors\nm = ann.Monitor(pop, 'v')\nw = ann.Monitor(proj, 'g')\n\n# Compile the network\nnet = ann.Network()\nnet.add([input, pop, proj, m, w])\nnet.compile()\n\n# Simulate for 100 ms\nnet.simulate(100.0)\n\n# Retrieve recordings\nv = net.get(m).get('v')[:, 0]\ns = net.get(w).get('g')[:, 0, :]\n\n# Plot the recordings\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2,1,1)\nplt.plot(s[:, 0])\nplt.plot(s[:, 1])\nplt.xlabel(\"Time (ms)\")\nplt.xlabel(\"Conductance\")\n\nplt.subplot(2,1,2)\nplt.plot(v)\nplt.xlabel(\"Time (ms)\")\nplt.xlabel(\"Membrane potential\")\nplt.show()\n\nWARNING: Monitor(): it is a bad idea to record synaptic variables of a projection at each time step! \nWARNING: Monitor(): it is a bad idea to record synaptic variables of a projection at each time step! \nCompiling network 5...  OK",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/PyNN.html#stp",
    "href": "notebooks/PyNN.html#stp",
    "title": "PyNN and Brian examples",
    "section": "STP",
    "text": "STP\nNetwork (CUBA) with short-term synaptic plasticity for excitatory synapses (depressing at long timescales, facilitating at short timescales).\nAdapted from :\nhttps://brian.readthedocs.io/en/stable/examples-synapses_short_term_plasticity2.html\n\nduration = 1000.0\n\nLIF = ann.Neuron(\n    parameters = \"\"\"\n        tau_m = 20.0 : population\n        tau_e = 5.0 : population\n        tau_i = 10.0 : population\n        E_rest = -49.0 : population\n        E_thresh = -50.0 : population\n        E_reset = -60.0 : population\n    \"\"\",\n    equations = \"\"\"\n        tau_m * dv/dt = E_rest -v + g_exc - g_inh\n        tau_e * dg_exc/dt = -g_exc\n        tau_i * dg_inh/dt = -g_inh\n    \"\"\",\n    spike = \"v &gt; E_thresh\",\n    reset = \"v = E_reset\"\n)\n\nSTP = ann.Synapse(\n    parameters = \"\"\"\n        tau_rec = 200.0 : projection\n        tau_facil = 20.0 : projection\n        U = 0.2 : projection\n    \"\"\",\n    equations = \"\"\"\n        dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n        du/dt = (U - u)/tau_facil : init = 0.2, event-driven\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w * u * x\n        x *= (1 - u)\n        u += U * (1 - u)\n    \"\"\"\n)\n\n# Population\nP = ann.Population(geometry=4000, neuron=LIF)\nP.v = ann.Uniform(-60.0, -50.0)\nPe = P[:3200]\nPi = P[3200:]\n\n# Projections\ncon_e = ann.Projection(pre=Pe, post=P, target='exc', synapse = STP)\ncon_e.connect_fixed_probability(weights=1.62, probability=0.02)\n\ncon_i = ann.Projection(pre=Pi, post=P, target='inh')\ncon_i.connect_fixed_probability(weights=9.0, probability=0.02)\n\n# Monitor\nm = ann.Monitor(P, 'spike')\n\n# Compile the network\nnet = ann.Network()\nnet.add([P, con_e, con_i, m])\nnet.compile()\n\n# Simulate without plasticity\nnet.simulate(duration, measure_time=True)\n\ndata = net.get(m).get()\nt, n = net.get(m).raster_plot(data['spike'])\nrates = net.get(m).population_rate(data['spike'], 5.0)\nprint('Total number of spikes: ' + str(len(t)))\n\nplt.figure(figsize=(12, 8))\nplt.subplot(211)\nplt.plot(t, n, '.')\nplt.xlabel('Time (ms)')\nplt.ylabel('Neuron number')\nplt.subplot(212)\nplt.plot(np.arange(rates.size)*ann.dt(), rates)\nplt.show()\n\nCompiling network 6...  OK \nSimulating 1.0 seconds of the network 6 took 0.31792426109313965 seconds. \nTotal number of spikes: 14115",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "PyNN/Brian"
    ]
  },
  {
    "objectID": "notebooks/StructuralPlasticity.html",
    "href": "notebooks/StructuralPlasticity.html",
    "title": "Structural plasticity",
    "section": "",
    "text": "#!pip install ANNarchy\n\nAs simple example showing how to use structural plasticity (creation/pruning of synapses) in a rate-coded network (spiking networks work similarly).\nFirst, the structural plasticity mechanisms must be allowed in setup():\n\nimport numpy as np\n\nimport ANNarchy as ann\nann.clear()\n\n# Compulsory to allow structural plasticity\nann.setup(structural_plasticity=True)\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nWe define a leaky integrator rate-coded neuron and a small population:\n\nLeakyIntegratorNeuron = ann.Neuron(\n    parameters=\"\"\"\n        tau = 10.0 : population\n        baseline = 0.0 \n    \"\"\",\n    equations = \"\"\"\n        tau * dr/dt + r = baseline + sum(exc) : min=0.0\n    \"\"\"\n)\npop = ann.Population(100, LeakyIntegratorNeuron)\n\nStructural plasticity has two components: creation of synapses and pruning (removal) under certain conditions. These conditions are defined in the synapse type itself in the pruning and creating arguments:\n\nStructuralPlasticSynapse = ann.Synapse(\n    parameters = \" T = 10000 : int, projection \",\n    equations = \"\"\"\n        age = if pre.r * post.r &gt; 1.0 :\n                0\n              else :\n                age + 1 : init = 0, int\"\"\",\n    pruning = \"age &gt; T : proba = 0.2\",\n    creating = \"pre.r * post.r &gt; 1.0 : proba = 0.1, w = 0.01\",\n)\n\nproj = ann.Projection(pop, pop, 'exc', StructuralPlasticSynapse)\nproj.connect_fixed_probability(weights = 0.01, probability=0.1)\n\n&lt;ANNarchy.core.Projection.Projection at 0x14298b450&gt;\n\n\nThese conditions must be boolean values, which when True may trigger the creation/pruning of a synapse. The flag proba gives the probability by which the synapse will actually be created/pruned.\n\nWhen creating is True, a synapse that did not exist will be created with the provided probability. Its weight will take the value provided by the flag w (0.01), the other variables take their default value.\nWhen pruning is True, a synapse that exists will be deleted with the given probability.\n\nThe pruning condition can depend on any pre-synaptic, post-synaptic or synaptic variable. The creating condition can only depend on pre- or post-synaptic conditions, as the synapse does not exist yet.\nApart from these two fields, the synapse is a regular synapse, one could also define synaptic plasticity mechanisms and so on.\nWe finally create a sparse projection within the population, with 10% connectivity.\n\nann.compile()\n\nCompiling ...  OK \n\n\nThe creation and pruning have to be explicitly started before a simulation, as they are very expensive computationally. The period argument states how often the conditions will be checked (avoid using dt):\n\nproj.start_creating(period=100.0)\nproj.start_pruning(period=100.0)\n\nTo see the effect of structural plasticity, one alternatively activates one half of the population by setting a high baseline (mimicking corrrelated inputs). As neurons in one half will be activated at the same time, they will create synapses between each other. Between the two halves, the neurons are never co-activated, so the existing synapses will slowly die out.\n\n# Save the initial connectivity matrix\ninitial_weights = proj.connectivity_matrix()\n\n# Let structural plasticity over several trials\nnum_trials = 100\nfor trial in range(num_trials):\n    # Activate the first subpopulation\n    pop[:50].baseline = 1.0\n    # Simulate for 1s\n    ann.simulate(1000.)\n    # Reset the population\n    pop.baseline = 0.0\n    ann.simulate(100.)\n    # Activate the second subpopulation\n    pop[50:].baseline = 1.0\n    # Simulate for 1s\n    ann.simulate(1000.)\n    # Reset the population\n    pop.baseline = 0.0\n    ann.simulate(100.)\n\n# Inspect the final connectivity matrix\nfinal_weights = proj.connectivity_matrix()\n\nWe can check the effect of structural plasticity by looking at the connectivity matrix before and after the stimulation:\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 15))\nplt.subplot(121)\nplt.imshow(initial_weights)\nplt.title('Connectivity matrix before')\nplt.subplot(122)\nplt.imshow(final_weights)\nplt.title('Connectivity matrix after')\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Rate-coded networks**",
      "Structural plasticity"
    ]
  },
  {
    "objectID": "notebooks/Hybrid.html",
    "href": "notebooks/Hybrid.html",
    "title": "Hybrid network",
    "section": "",
    "text": "#!pip install ANNarchy\n\nSimple example showing hybrid spike/rate-coded networks.\nReproduces Fig.4 of (Vitay, Dinkelbach and Hamker, 2015)\n\nimport numpy as np\nimport ANNarchy as ann\n\nann.clear()\nann.setup(dt=0.1)\n\n# Rate-coded input neuron\ninput_neuron = ann.Neuron(\n    parameters = \"baseline = 0.0\",\n    equations = \"r = baseline\"\n)\n# Rate-coded output neuron\nsimple_neuron = ann.Neuron(\n    equations = \"r = sum(exc)\"\n)\n\n# Rate-coded population for input\npop1 = ann.Population(geometry=1, neuron=input_neuron)\n\n# Poisson Population to encode\npop2 = ann.PoissonPopulation(geometry=1000, target=\"exc\")\nproj = ann.Projection(pop1, pop2, 'exc').connect_all_to_all(weights=1.)\n\n# Rate-coded population to decode\npop3 = ann.Population(geometry=1000, neuron =simple_neuron)\nproj = ann.DecodingProjection(pop2, pop3, 'exc', window=10.0)\n\ndef diagonal(pre, post, weights):\n    \"\"\"\n    Simple connector pattern to progressively connect each post-synaptic neuron to a growing number of pre-synaptic neurons.\n    \"\"\"\n    lil = ann.LILConnectivity()\n    for rk_post in range(post.size):\n        lil.add(rk_post, range((rk_post+1)), [weights], [0] )\n    return lil\nproj.connect_with_func(method=diagonal, weights=1.)\n\nann.compile()\n\n# Monitors\nm1 = ann.Monitor(pop1, 'r')\nm2 = ann.Monitor(pop2, 'spike')\nm3 = ann.Monitor(pop3, 'r')\n\n# Simulate\nduration = 250.\n# 0 Hz\npop1.baseline = 0.0\nann.simulate(duration)\n# 10 Hz\npop1.baseline = 10.0\nann.simulate(duration)\n# 50 Hz\npop1.baseline = 50.0\nann.simulate(duration)\n# 100 Hz\npop1.baseline = 100.0\nann.simulate(duration)\n\n# Get recordings\ndata1 = m1.get()\ndata2 = m2.get()\ndata3 = m3.get()\n\n# Raster plot of the spiking population\nt, n = m2.raster_plot(data2['spike'])\n\n# Variance of the the decoded firing rate\ndata_10 = data3['r'][int(1.0*duration/ann.dt()):int(2*duration/ann.dt()), :]\ndata_50 = data3['r'][int(2.0*duration/ann.dt()):int(3*duration/ann.dt()), :]\ndata_100 = data3['r'][int(3.0*duration/ann.dt()):int(4*duration/ann.dt()), :]\nvar_10 = np.mean(np.abs((data_10 - 10.)/10.), axis=0)\nvar_50 = np.mean(np.abs((data_50 - 50.)/50.), axis=0)\nvar_100 = np.mean(np.abs((data_100 - 100.)/100.), axis=0)\n\n### Plot the results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(t, n, '.', markersize=0.5)\nplt.title('a) Raster plot')\nplt.xlabel('Time (ms)')\nplt.ylabel('# neurons')\nplt.xlim((0, 4*duration))\n\nplt.subplot(3,1,2)\nplt.plot(np.arange(0, 4*duration, 0.1), data1['r'][:, 0], label='Original firing rate')\nplt.plot(np.arange(0, 4*duration, 0.1), data3['r'][:, 999], label='Decoded firing rate')\nplt.legend(frameon=False, loc=2)\nplt.title('b) Decoded firing rate')\nplt.xlabel('Time (ms)')\nplt.ylabel('Activity (Hz)')\n\nplt.subplot(3,1,3)\nplt.plot(var_10, label='10 Hz')\nplt.plot(var_50, label='50 Hz')\nplt.plot(var_100, label='100 Hz')\nplt.legend(frameon=False)\nplt.title('c) Precision')\nplt.xlabel('# neurons used for decoding')\nplt.ylabel('Normalized error')\nplt.ylim((0,1))\n\nplt.show()\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\nCompiling ...  OK",
    "crumbs": [
      "Notebooks",
      "**Advanced features**",
      "Hybrid networks"
    ]
  },
  {
    "objectID": "notebooks/Image.html",
    "href": "notebooks/Image.html",
    "title": "Convolutions and pooling",
    "section": "",
    "text": "#!pip install ANNarchy\n\nThis simple example in examples/image demonstrates how to load images directly into the firing rates of a population and apply basic linear filters on it.\nIt relies on the ANNarchy extensions image and convolution which must be explicitly imported:\n\nimport numpy as np\nimport ANNarchy as ann\n\nfrom ANNarchy.extensions.image import ImagePopulation\nfrom ANNarchy.extensions.convolution import Convolution, Pooling\n\nann.clear()\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nANNarchy.extensions.image depends on the Python bindings of OpenCV, they must be installed before running the script.\nWe first create an ImagePopulation that will load images:\n\nimage = ImagePopulation(geometry=(480, 640, 3))\n\nIts geometry specifies the size of the images that can be loaded, here 640x480 RGB images. Note the geometry must be of the form (height, width, channels), where channels is 1 for grayscale images and 3 for color images.\nThe next step is to reduce the size of the image, what can be done by using the Pooling class of the convolution extension.\nWe define a dummy artificial neuron, whose firing rate r will simply be the sum of excitatory connections /ensured to be positive, but this should always be the case). We then create a smaller population pooled with this neuron type, and connect it to the ImagePopulation using mean-pooling:\n\n# Simple ANN\nLinearNeuron = ann.Neuron(equations=\"r=sum(exc): min=0.0\")\n\n# Subsampling population\npooled = ann.Population(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Mean-pooling projection\npool_proj = Pooling(pre=image, post=pooled, target='exc', operation='mean')\npool_proj.connect_pooling()\n\n&lt;ANNarchy.extensions.convolution.Pooling.Pooling at 0x137862890&gt;\n\n\nThe pooled population reduces the size of the image by a factor ten (defined by the size of the population) by averaging the pixels values over 10x10 regions (operation is set to 'mean', but one could use 'max' or 'min'). The connect_pooling() connector creates the “fake” connection pattern (as no weights are involved).\nLet’s apply now a 3x3 box filter on each channel of the pooled population:\n\n# Smoothing population\nsmoothed = ann.Population(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Box filter projection\nbox_filter = np.ones((3, 3, 1))/9.\nsmooth_proj = Convolution(pre=pooled, post=smoothed, target='exc')\nsmooth_proj.connect_filter(weights=box_filter)\n\n&lt;ANNarchy.extensions.convolution.Convolve.Convolution at 0x107e8e910&gt;\n\n\nTo perform a convolution operation on the population (or more precisely a cross-correlation), we call the connect_filter() connector method of the Convolution projection. It requires to define a kernel (weights) that will be convolved over the input population. Here we use a simple box filter, but any filter can be used.\nAs the pooled population has three dimensions and we want to smooth the activities per color channel, we need to define a (3, 3, 1) kernel. If we wanted to smooth also over the color channels, we could have used a (3, 3) filter: the resulting population would have the shape (48, 64).\nWe now apply a bank of three filters, each selective to a particular color (red/green/blue). This filters do not have a spatial extent (1x1 convolution), but sum over the third dimension (the color channels):\n\n# Convolution population    \nfiltered = ann.Population(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Red/Green/Blue filter bank\nfilter_bank = np.array([ \n    [[ [2.0, -1.0, -1.0] ]] , # Red filter \n    [[ [-1.0, 2.0, -1.0] ]] , # Blue filter\n    [[ [-1.0, -1.0, 2.0] ]]   # Green filter\n])\nfilter_proj = Convolution(pre=smoothed, post=filtered, target='exc')\nfilter_proj.connect_filters(weights=filter_bank)\n\n&lt;ANNarchy.extensions.convolution.Convolve.Convolution at 0x107f063d0&gt;\n\n\nEach of the three filter has the shape (1, 1, 3). The result of each convolution would then be (48, 64), but as there are three filters, the output population is (48, 64, 3). The last dimension does not correspond to the number of color channels, but to the number of filters in the bank: if you add a filter, the population will have to be (48, 64, 4).\nBanks of filters require to use connect_filters() instead of connect_filter().\n\nann.compile()\n\nCompiling ...  OK \n\n\nAfter compilation, we can load an image into the input population:\n\nimage.set_image('test.jpg')\n\nTo see the result, we need to simulate for four time steps (4 milliseconds, as dt=1.0).\n\nStep 1: The image population loads the image.\nStep 2: The pooled population subsamples the image.\nStep 3: The smoothed population filters the pooled image.\nStep 4: The bank of filters are applied by filtered.\n\n\nann.simulate(4.0)\n\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20.0, 20.0))\n\nplt.subplot(532)\nplt.imshow(image.r)\nplt.title('Original')\n\nplt.subplot(534)\nplt.imshow(image.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('image R')\nplt.subplot(535)\nplt.imshow(image.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('image G')\nplt.subplot(536)\nplt.imshow(image.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('image B')\n\nplt.subplot(537)\nplt.imshow(pooled.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('pooled R')\nplt.subplot(538)\nplt.imshow(pooled.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('pooled G')\nplt.subplot(539)\nplt.imshow(pooled.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('pooled B')\n\nplt.subplot(5, 3, 10)\nplt.imshow(smoothed.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('smoothed R')\nplt.subplot(5, 3, 11)\nplt.imshow(smoothed.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('smoothed G')\nplt.subplot(5, 3, 12)\nplt.imshow(smoothed.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('smoothed B')\n\nplt.subplot(5, 3, 13)\nplt.imshow(filtered.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('filtered R')\nplt.subplot(5, 3, 14)\nplt.imshow(filtered.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('filtered G')\nplt.subplot(5, 3, 15)\nplt.imshow(filtered.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('filtered B')\n\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Extensions**",
      "Image"
    ]
  },
  {
    "objectID": "notebooks/SORF.html",
    "href": "notebooks/SORF.html",
    "title": "Homeostatic STDP: SORF model",
    "section": "",
    "text": "#!pip install ANNarchy\n\nReimplementation of the SORF model published in:\n\nCarlson, K.D.; Richert, M.; Dutt, N.; Krichmar, J.L., “Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks,” in Neural Networks (IJCNN), The 2013 International Joint Conference on , vol., no., pp.1-8, 4-9 Aug. 2013. doi: 10.1109/IJCNN.2013.6706961\n\n\nimport numpy as np\nimport ANNarchy as ann\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\n\nnb_neuron = 4 # Number of exc and inh neurons\nsize = (32, 32) # input size\nfreq = 1.2 # nb_cycles/half-image\nnb_stim = 40 # Number of grating per epoch\nnb_epochs = 20 # Number of epochs\nmax_freq = 28. # Max frequency of the poisson neurons\nT = 10000. # Period for averaging the firing rate\n\n\n# Izhikevich Coba neuron with AMPA, NMDA and GABA receptors\nRSNeuron = ann.Neuron(\n    parameters = \"\"\"\n        a = 0.02 : population\n        b = 0.2 : population\n        c = -65. : population\n        d = 8. : population\n        tau_ampa = 5. : population\n        tau_nmda = 150. : population\n        tau_gabaa = 6. : population\n        tau_gabab = 150. : population\n        vrev_ampa = 0.0 : population\n        vrev_nmda = 0.0 : population\n        vrev_gabaa = -70.0 : population\n        vrev_gabab = -90.0 : population\n    \"\"\" ,\n    equations=\"\"\"\n        # Inputs\n        I = g_ampa * (vrev_ampa - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev_nmda -v) + g_gabaa * (vrev_gabaa - v) + g_gabab * (vrev_gabab -v)\n        # Midpoint scheme\n        dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., min=-90., midpoint\n        du/dt = a * (b*v - u) : init=-13., midpoint\n        # Conductances\n        tau_ampa * dg_ampa/dt = -g_ampa : exponential\n        tau_nmda * dg_nmda/dt = -g_nmda : exponential\n        tau_gabaa * dg_gabaa/dt = -g_gabaa : exponential\n        tau_gabab * dg_gabab/dt = -g_gabab : exponential\n    \"\"\" ,\n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\",\n    reset = \"\"\"\n        v = c\n        u += d\n        g_ampa = 0.0\n        g_nmda = 0.0\n        g_gabaa = 0.0\n        g_gabab = 0.0\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\",\n    refractory=1.0\n)\n\n\n# STDP with homeostatic regulation\nhomeo_stdp = ann.Synapse(\n    parameters=\"\"\"\n        # STDP\n        tau_plus  = 60. : projection\n        tau_minus = 90. : projection\n        A_plus  = 0.000045 : projection\n        A_minus = 0.00003 : projection\n\n        # Homeostatic regulation\n        alpha = 0.1 : projection\n        beta = 50.0 : projection # &lt;- Difference with the original implementation\n        gamma = 50.0 : projection\n        Rtarget = 10. : projection\n        T = 10000. : projection\n    \"\"\",\n    equations = \"\"\"\n        # Homeostatic values\n        R = post.r : postsynaptic\n        K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic\n        # Nearest-neighbour\n        stdp = if t_post &gt;= t_pre: ltp else: - ltd \n        w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=0.0, max=10.0\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",\n    post_spike=\"\"\"\n        ltd = A_minus\n    \"\"\"\n)\n\n\n# Input population\nOnPoiss = ann.PoissonPopulation(size, rates=1.0)\nOffPoiss = ann.PoissonPopulation(size, rates=1.0)\n\n# RS neuron for the input buffers\nOnBuffer = ann.Population(size, RSNeuron)\nOffBuffer = ann.Population(size, RSNeuron)\n\n# Connect the buffers\nOnPoissBuffer = ann.Projection(OnPoiss, OnBuffer, ['ampa', 'nmda'])\nOnPoissBuffer.connect_one_to_one(ann.Uniform(0.2, 0.6))\nOffPoissBuffer = ann.Projection(OffPoiss, OffBuffer, ['ampa', 'nmda'])\nOffPoissBuffer.connect_one_to_one(ann.Uniform(0.2, 0.6))\n\n# Excitatory and inhibitory neurons\nExc = ann.Population(nb_neuron, RSNeuron)\nInh = ann.Population(nb_neuron, RSNeuron)\nExc.compute_firing_rate(T)\nInh.compute_firing_rate(T)\n\n# Input connections\nOnBufferExc = ann.Projection(OnBuffer, Exc, ['ampa', 'nmda'], homeo_stdp)\nOnBufferExc.connect_all_to_all(ann.Uniform(0.004, 0.015))\nOffBufferExc = ann.Projection(OffBuffer, Exc, ['ampa', 'nmda'], homeo_stdp)\nOffBufferExc.connect_all_to_all(ann.Uniform(0.004, 0.015))\n\n# Competition\nExcInh = ann.Projection(Exc, Inh, ['ampa', 'nmda'], homeo_stdp)\nExcInh.connect_all_to_all(ann.Uniform(0.116, 0.403))\n\nExcInh.Rtarget = 75.\nExcInh.tau_plus = 51.\nExcInh.tau_minus = 78.\nExcInh.A_plus = -0.000041\nExcInh.A_minus = -0.000015\n\nInhExc = ann.Projection(Inh, Exc, ['gabaa', 'gabab'])\nInhExc.connect_all_to_all(ann.Uniform(0.065, 0.259))\n\nann.compile()\n\nCompiling ...  OK \n\n\n\n# Inputs\ndef get_grating(theta):\n    x = np.linspace(-1., 1., size[0])\n    y = np.linspace(-1., 1., size[1])\n    xx, yy = np.meshgrid(x, y)\n    z = np.sin(2.*np.pi*(np.cos(theta)*xx + np.sin(theta)*yy)*freq)\n    return np.maximum(z, 0.), -np.minimum(z, 0.0)\n\n# Initial weights\nw_on_start = OnBufferExc.w\nw_off_start = OffBufferExc.w\n\n# Monitors\nm = ann.Monitor(Exc, 'r')\nn = ann.Monitor(Inh, 'r')\no = ann.Monitor(OnBufferExc[0], 'w', period=1000.)\np = ann.Monitor(ExcInh[0], 'w', period=1000.)\n\n# Learning procedure\nfrom time import time\nimport random\ntstart = time()\nstim_order = list(range(nb_stim))\ntry:\n    for epoch in range(nb_epochs):\n        random.shuffle(stim_order)\n        for stim in stim_order:\n            # Generate a grating randomly\n            rates_on, rates_off = get_grating(np.pi*stim/float(nb_stim))\n            # Set it as input to the poisson neurons\n            OnPoiss.rates  = max_freq * rates_on\n            OffPoiss.rates = max_freq * rates_off\n            # Simulate for 2s\n            ann.simulate(2000.)\n            # Relax the Poisson inputs\n            OnPoiss.rates  = 1.\n            OffPoiss.rates = 1.\n            # Simulate for 500ms\n            ann.simulate(500.)\n        print('Epoch', epoch+1, 'done.')\nexcept KeyboardInterrupt:\n    print('Simulation stopped')\n    \nprint('Done in ', time()-tstart)\n\n# Recordings\ndatae = m.get('r')\ndatai = n.get('r')\ndataw = o.get('w')\ndatal = p.get('w')\n\nEpoch 1 done.\nEpoch 2 done.\nEpoch 3 done.\nEpoch 4 done.\nEpoch 5 done.\nEpoch 6 done.\nEpoch 7 done.\nEpoch 8 done.\nEpoch 9 done.\nEpoch 10 done.\nEpoch 11 done.\nEpoch 12 done.\nEpoch 13 done.\nEpoch 14 done.\nEpoch 15 done.\nEpoch 16 done.\nEpoch 17 done.\nEpoch 18 done.\nEpoch 19 done.\nEpoch 20 done.\nDone in  122.07915997505188\n\n\n\n# Final weights\nw_on_end = OnBufferExc.w\nw_off_end = OffBufferExc.w\n\n# Plot\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 12))\nplt.title('Feedforward weights before and after learning')\nfor i in range(nb_neuron):\n    plt.subplot(3, nb_neuron, i+1)\n    plt.imshow((np.array(w_on_start[i])).reshape((32,32)), aspect='auto', cmap='hot')\n    plt.subplot(3, nb_neuron, nb_neuron + i +1)\n    plt.imshow((np.array(w_on_end[i])).reshape((32,32)), aspect='auto', cmap='hot')\n    plt.subplot(3, nb_neuron, 2*nb_neuron + i +1)\n    plt.imshow((np.array(w_off_end[i])).reshape((32,32)), aspect='auto', cmap='hot')\n\nplt.figure(figsize=(12, 8))\nplt.plot(datae[:, 0], label='Exc')\nplt.plot(datai[:, 0], label='Inh')\nplt.title('Mean FR of the Exc and Inh neurons')\nplt.legend()\n\nplt.figure(figsize=(12, 8))\nplt.subplot(121)\nplt.imshow(np.array(dataw, dtype='float').T, aspect='auto', cmap='hot')\nplt.title('Timecourse of feedforward weights')\nplt.colorbar()\nplt.subplot(122)\nplt.imshow(np.array(datal, dtype='float').T, aspect='auto', cmap='hot')\nplt.title('Timecourse of inhibitory weights')\nplt.colorbar()\nplt.show()\n\n/var/folders/6w/6msx49ws7k13cc0bbys0tt4m0000gn/T/ipykernel_8956/2229657718.py:11: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  plt.subplot(3, nb_neuron, i+1)",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "Homeostatic STDP - SORF"
    ]
  },
  {
    "objectID": "notebooks/STP.html",
    "href": "notebooks/STP.html",
    "title": "Short-term Plasticity and Synchrony",
    "section": "",
    "text": "#!pip install ANNarchy\n\nImplementation of the recurrent network with short-term plasticity (STP) proposed in:\n\nTsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses, The Journal of Neuroscience, 20(50).\n\n\nimport numpy as np\nimport ANNarchy as ann\nann.clear()\n\ndt=0.25\nann.setup(dt=dt)\n\nANNarchy 4.8 (4.8.0) on darwin (posix).\n\n\nThis network uses simple leaky integrate-and-fire (LIF) neurons:\n\nLIF = ann.Neuron(\n    parameters = \"\"\"\n    tau = 30.0 : population\n    I = 15.0\n    tau_I = 3.0 : population\n    \"\"\",\n    equations = \"\"\"\n    tau * dv/dt = -v + g_exc - g_inh + I : init=13.5\n    tau_I * dg_exc/dt = -g_exc\n    tau_I * dg_inh/dt = -g_inh\n    \"\"\",\n    spike = \"v &gt; 15.0\",\n    reset = \"v = 13.5\",\n    refractory = 3.0\n)\n\nP = ann.Population(geometry=500, neuron=LIF)\nP.I = np.sort(ann.Uniform(14.625, 15.375).get_values(500))\nP.v = ann.Uniform(0.0, 15.0)\nExc = P[:400]\nInh = P[400:]\n\nShort-term plasticity can be defined by dynamical changes of synaptic efficiency, based on pre- or post-synaptic activity.\nWe define a STP synapse, whose post-pynaptic potential (psp, define by g_target) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u:\n\nSTP = ann.Synapse(\n    parameters = \"\"\"\n    w=0.0\n    tau_rec = 1.0\n    tau_facil = 1.0\n    U = 0.1\n    \"\"\",\n    equations = \"\"\"\n    dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n    du/dt = (U - u)/tau_facil : init = 0.1, event-driven   \n    \"\"\",\n    pre_spike=\"\"\"\n    g_target += w * u * x\n    x *= (1 - u)\n    u += U * (1 - u)\n    \"\"\"\n)\n\nCreating the projection between the excitatory and inhibitory is straightforward when the right parameters are chosen:\n\n# Parameters for the synapses\nAee = 1.8\nAei = 5.4\nAie = 7.2\nAii = 7.2\n\nUee = 0.5\nUei = 0.5\nUie = 0.04\nUii = 0.04\n\ntau_rec_ee = 800.0\ntau_rec_ei = 800.0\ntau_rec_ie = 100.0\ntau_rec_ii = 100.0\n\ntau_facil_ie = 1000.0\ntau_facil_ii = 1000.0\n\n# Create projections\nproj_ee = ann.Projection(pre=Exc, post=Exc, target='exc', synapse=STP)\nproj_ee.connect_fixed_probability(probability=0.1, weights=ann.Normal(Aee, (Aee/2.0), min=0.2*Aee, max=2.0*Aee)) \nproj_ee.U = ann.Normal(Uee, (Uee/2.0), min=0.1, max=0.9)\nproj_ee.tau_rec = ann.Normal(tau_rec_ee, (tau_rec_ee/2.0), min=5.0)\nproj_ee.tau_facil = dt # Cannot be 0!\n\nproj_ei = ann.Projection(pre=Inh, post=Exc, target='inh', synapse=STP)\nproj_ei.connect_fixed_probability(probability=0.1, weights=ann.Normal(Aei, (Aei/2.0), min=0.2*Aei, max=2.0*Aei))\nproj_ei.U = ann.Normal(Uei, (Uei/2.0), min=0.1, max=0.9)\nproj_ei.tau_rec = ann.Normal(tau_rec_ei, (tau_rec_ei/2.0), min=5.0)\nproj_ei.tau_facil = dt # Cannot be 0!\n\nproj_ie = ann.Projection(pre=Exc, post=Inh, target='exc', synapse=STP)\nproj_ie.connect_fixed_probability(probability=0.1, weights=ann.Normal(Aie, (Aie/2.0), min=0.2*Aie, max=2.0*Aie))\nproj_ie.U = ann.Normal(Uie, (Uie/2.0), min=0.001, max=0.07)\nproj_ie.tau_rec = ann.Normal(tau_rec_ie, (tau_rec_ie/2.0), min=5.0)\nproj_ie.tau_facil = ann.Normal(tau_facil_ie, (tau_facil_ie/2.0), min=5.0)\n\nproj_ii = ann.Projection(pre=Inh, post=Inh, target='inh', synapse=STP)\nproj_ii.connect_fixed_probability(probability=0.1, weights=ann.Normal(Aii, (Aii/2.0), min=0.2*Aii, max=2.0*Aii))\nproj_ii.U = ann.Normal(Uii, (Uii/2.0), min=0.001, max=0.07)\nproj_ii.tau_rec = ann.Normal(tau_rec_ii, (tau_rec_ii/2.0), min=5.0)\nproj_ii.tau_facil = ann.Normal(tau_facil_ii, (tau_facil_ii/2.0), min=5.0)\n\nWe compile and simulate for 10 seconds:\n\nann.compile()\n\n# Record\nMe = ann.Monitor(Exc, 'spike')\nMi = ann.Monitor(Inh, 'spike')\n\n# Simulate\nduration = 10000.0\nann.simulate(duration, measure_time=True)\n\nCompiling ...  OK \nSimulating 10.0 seconds of the network took 0.07375288009643555 seconds. \n\n\nWe retrieve the recordings and plot them:\n\n# Retrieve recordings\ndata_exc = Me.get()\ndata_inh = Mi.get()\nte, ne = Me.raster_plot(data_exc['spike'])\nti, ni = Mi.raster_plot(data_inh['spike'])\n\n# Histogram of the exc population\nh = Me.histogram(data_exc['spike'], bins=1.0)\n\n# Mean firing rate of each excitatory neuron\nrates = []\nfor neur in data_exc['spike'].keys():\n    rates.append(len(data_exc['spike'][neur])/duration*1000.0)\n\n# Plot\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(te, ne, 'b.', markersize=1.0)\nplt.plot(ti, ni, 'b.', markersize=1.0)\nplt.xlim((0, duration)); plt.ylim((0,500))\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\n\nplt.subplot(3,1,2)\nplt.plot(h/400.)\nplt.xlabel('Time (ms)')\nplt.ylabel('Net activity')\n\nplt.subplot(3,1,3)\nplt.plot(sorted(rates))\nplt.ylabel('Spikes / sec')\nplt.xlabel('# neuron')\nplt.show()",
    "crumbs": [
      "Notebooks",
      "**Spiking networks**",
      "STP"
    ]
  },
  {
    "objectID": "reference/smoothed_rate.html",
    "href": "reference/smoothed_rate.html",
    "title": "smoothed_rate",
    "section": "",
    "text": "core.Monitor.smoothed_rate(spikes, smooth=0.0)\nComputes the smoothed firing rate of the recorded spiking neurons.\nThe first axis is the neuron index, the second is time.\nExample:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nspikes = m.get('spike')\nr = smoothed_rate(smooth=100.)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nrequired\n\n\nsmooth\nfloat\nsmoothing time constant. Default: 0.0 (no smoothing).\n0.0",
    "crumbs": [
      "Reference",
      "**Plotting**",
      "smoothed_rate"
    ]
  },
  {
    "objectID": "reference/smoothed_rate.html#parameters",
    "href": "reference/smoothed_rate.html#parameters",
    "title": "smoothed_rate",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nrequired\n\n\nsmooth\nfloat\nsmoothing time constant. Default: 0.0 (no smoothing).\n0.0",
    "crumbs": [
      "Reference",
      "**Plotting**",
      "smoothed_rate"
    ]
  },
  {
    "objectID": "reference/every.html",
    "href": "reference/every.html",
    "title": "every",
    "section": "",
    "text": "every\ncore.Simulate.every(self, period, offset=0.0, wait=0.0, net_id=0)\nDecorator to declare a callback method that will be called periodically during the simulation.\nExample of setting increasing inputs to a population every 100 ms, with an offset of 90 ms (or -10 ms relative to the period):\n@every(period=100., offset=-10.)\ndef step_input(n):\n    pop.I = float(n) / 100.\n\nsimulate(10000.)\nstep_input() will be called at times 90, 190, …, 9990 ms during the call to simulate().\nThe method must accept only n as parameter (an integer being 0 the first time the method is called, and incremented afterwards) and can not return anything.\nThe times at which the method is called are relative to the time when simulate() is called (if t is already 150 before calling simulate(), the first call will then be made at t=240 with the previous example).\nIf multiple callbacks are defined, they will be called in the order of their declaration if they occur at the same time.",
    "crumbs": [
      "Reference",
      "**Callbacks**",
      "every"
    ]
  },
  {
    "objectID": "reference/IF_cond_exp.html",
    "href": "reference/IF_cond_exp.html",
    "title": "IF_cond_exp",
    "section": "",
    "text": "IF_cond_exp\nmodels.Neurons.IF_cond_exp(self, v_rest=-65.0, cm=1.0, tau_m=20.0, tau_refrac=0.0, tau_syn_E=5.0, tau_syn_I=5.0, e_rev_E=0.0, e_rev_I=-70.0, v_thresh=-50.0, v_reset=-65.0, i_offset=0.0)\nLeaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance.\nParameters:\n\nv_rest = -65.0 : Resting membrane potential (mV)\ncm = 1.0 : Capacity of the membrane (nF)\ntau_m = 20.0 : Membrane time constant (ms)\ntau_refrac = 0.0 : Duration of refractory period (ms)\ntau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms)\ntau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms)\ne_rev_E = 0.0 : Reversal potential for excitatory input (mV)\ne_rev_I = -70.0 : Reversal potential for inhibitory input (mv)\ni_offset = 0.0 : Offset current (nA)\nv_reset = -65.0 : Reset potential after a spike (mV)\nv_thresh = -50.0 : Spike threshold (mV)\n\nVariables:\n\nv : membrane potential in mV (init=-65.0):\ncm * dv/dt = cm/tau_m(v_rest -v) + g_exc  (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset\ng_exc : excitatory current (init = 0.0):\ntau_syn_E * dg_exc/dt = - g_exc\ng_inh : inhibitory current (init = 0.0):\ntau_syn_I * dg_inh/dt = - g_inh\n\nSpike emission:\nv &gt; v_thresh\nReset:\nv = v_reset\nThe ODEs are solved using the exponential Euler method.\nEquivalent code:\n\nIF_cond_exp = Neuron(\n    parameters = \"\"\"\n        v_rest = -65.0\n        cm  = 1.0\n        tau_m  = 20.0\n        tau_syn_E = 5.0\n        tau_syn_I = 5.0\n        e_rev_E = 0.0\n        e_rev_I = -70.0\n        v_thresh = -50.0\n        v_reset = -65.0\n        i_offset = 0.0\n    \"\"\", \n    equations = \"\"\"\n        cm * dv/dt = cm/tau_m*(v_rest -v)   + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0\n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n    \"\"\",\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = v_reset\",\n    refractory = 0.0\n)",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_cond_exp"
    ]
  },
  {
    "objectID": "reference/projections.html",
    "href": "reference/projections.html",
    "title": "projections",
    "section": "",
    "text": "core.Global.projections(net_id=0, post=None, pre=None, target=None, suppress_error=False)\nReturns a list of all declared populations. By default, the method returns all connections which were defined. By setting one of the arguments, post, pre and target one can select a subset accordingly.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npost\nANNarchy.core.Population\nall returned projections should have this population as post.\nNone\n\n\npre\nANNarchy.core.Population\nall returned projections should have this population as pre.\nNone\n\n\ntarget\nstr\nall returned projections should have this target.\nNone\n\n\nsuppress_error\nbool\nby default, ANNarchy throws an error if the list of assigned projections is empty. If this flag is set to True, the error message is suppressed.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[ANNarchy.core.Projection]\nA list of all assigned projections in this network, or a subset according to the arguments.",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "projections"
    ]
  },
  {
    "objectID": "reference/projections.html#parameters",
    "href": "reference/projections.html#parameters",
    "title": "projections",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npost\nANNarchy.core.Population\nall returned projections should have this population as post.\nNone\n\n\npre\nANNarchy.core.Population\nall returned projections should have this population as pre.\nNone\n\n\ntarget\nstr\nall returned projections should have this target.\nNone\n\n\nsuppress_error\nbool\nby default, ANNarchy throws an error if the list of assigned projections is empty. If this flag is set to True, the error message is suppressed.\nFalse",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "projections"
    ]
  },
  {
    "objectID": "reference/projections.html#returns",
    "href": "reference/projections.html#returns",
    "title": "projections",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nlist[ANNarchy.core.Projection]\nA list of all assigned projections in this network, or a subset according to the arguments.",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "projections"
    ]
  },
  {
    "objectID": "reference/clear.html",
    "href": "reference/clear.html",
    "title": "clear",
    "section": "",
    "text": "core.Global.clear(functions=True, neurons=True, synapses=True, constants=True)\nClears all variables (erasing already defined populations, projections, monitors and constants), as if you had just imported ANNarchy.\nUseful when re-running Jupyter/IPython notebooks multiple times:\nimport ANNarchy as ann\nann.clear()\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunctions\nbool\nif True (default), all functions defined with add_function are erased.\nTrue\n\n\nneurons\nbool\nif True (default), all neurons defined with Neuron are erased.\nTrue\n\n\nsynapses\nbool\nif True (default), all synapses defined with Synapse are erased.\nTrue\n\n\nconstants\nbool\nif True (default), all constants defined with Constant are erased.\nTrue",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "clear"
    ]
  },
  {
    "objectID": "reference/clear.html#parameters",
    "href": "reference/clear.html#parameters",
    "title": "clear",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfunctions\nbool\nif True (default), all functions defined with add_function are erased.\nTrue\n\n\nneurons\nbool\nif True (default), all neurons defined with Neuron are erased.\nTrue\n\n\nsynapses\nbool\nif True (default), all synapses defined with Synapse are erased.\nTrue\n\n\nconstants\nbool\nif True (default), all constants defined with Constant are erased.\nTrue",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "clear"
    ]
  },
  {
    "objectID": "reference/compile.html",
    "href": "reference/compile.html",
    "title": "compile",
    "section": "",
    "text": "generator.compile(directory='annarchy', clean=False, populations=None, projections=None, compiler='default', compiler_flags='default', add_sources='', extra_libs='', cuda_config={'device': 0}, annarchy_json='', silent=False, debug_build=False, profile_enabled=False, net_id=0)\nThis method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation.\nThe compiler, compiler_flags and part of cuda_config take their default value from the configuration file ~/.config/ANNarchy/annarchy.json.\nThe following arguments are for internal development use only:\n\ndebug_build: creates a debug version of ANNarchy, which logs the creation of objects and some other data (default: False).\nprofile_enabled: creates a profilable version of ANNarchy, which logs several computation timings (default: False).\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndirectory\n\nname of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: “annarchy/”.\n'annarchy'\n\n\nclean\n\nboolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False).\nFalse\n\n\npopulations\n\nlist of populations which should be compiled. If set to None, all available populations will be used.\nNone\n\n\nprojections\n\nlist of projection which should be compiled. If set to None, all available projections will be used.\nNone\n\n\ncompiler\n\nC++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++].\n'default'\n\n\ncompiler_flags\n\nplatform-specific flags to pass to the compiler. Default: “-march=native -O2”. Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended.\n'default'\n\n\ncuda_config\n\ndictionary defining the CUDA configuration for each population and projection.\n{'device': 0}\n\n\nannarchy_json\n\ncompiler flags etc can be stored in a .json file normally placed in the home directory (see comment below). With this flag one can directly assign a file location.\n''\n\n\nsilent\n\ndefines if status message like “Compiling… OK” should be printed.\nFalse",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "compile"
    ]
  },
  {
    "objectID": "reference/compile.html#parameters",
    "href": "reference/compile.html#parameters",
    "title": "compile",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndirectory\n\nname of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: “annarchy/”.\n'annarchy'\n\n\nclean\n\nboolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False).\nFalse\n\n\npopulations\n\nlist of populations which should be compiled. If set to None, all available populations will be used.\nNone\n\n\nprojections\n\nlist of projection which should be compiled. If set to None, all available projections will be used.\nNone\n\n\ncompiler\n\nC++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++].\n'default'\n\n\ncompiler_flags\n\nplatform-specific flags to pass to the compiler. Default: “-march=native -O2”. Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended.\n'default'\n\n\ncuda_config\n\ndictionary defining the CUDA configuration for each population and projection.\n{'device': 0}\n\n\nannarchy_json\n\ncompiler flags etc can be stored in a .json file normally placed in the home directory (see comment below). With this flag one can directly assign a file location.\n''\n\n\nsilent\n\ndefines if status message like “Compiling… OK” should be printed.\nFalse",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "compile"
    ]
  },
  {
    "objectID": "reference/histogram.html",
    "href": "reference/histogram.html",
    "title": "histogram",
    "section": "",
    "text": "core.Monitor.histogram(spikes, bins=None, per_neuron=False, recording_window=None)\nReturns a histogram for the recorded spikes in the population.\nExample:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nspikes = m.get('spike')\nhisto = histogram(spikes)\nplt.plot(histo)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike').\nrequired\n\n\nbins\nfloat\nthe bin size in ms (default: dt).\nNone",
    "crumbs": [
      "Reference",
      "**Plotting**",
      "histogram"
    ]
  },
  {
    "objectID": "reference/histogram.html#parameters",
    "href": "reference/histogram.html#parameters",
    "title": "histogram",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike').\nrequired\n\n\nbins\nfloat\nthe bin size in ms (default: dt).\nNone",
    "crumbs": [
      "Reference",
      "**Plotting**",
      "histogram"
    ]
  },
  {
    "objectID": "reference/balloon_CL.html",
    "href": "reference/balloon_CL.html",
    "title": "balloon_CL",
    "section": "",
    "text": "extensions.bold.PredefinedModels.balloon_CL(self, phi=1.0, kappa=1 / 1.54, gamma=1 / 2.46, E_0=0.34, tau=0.98, alpha=0.33, V_0=0.02, v_0=40.3, TE=40 / 1000.0, epsilon=1.43)\nA balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007).\nEquivalent code:\nballoon_CL = BoldModel(\n    parameters = '''\n        second    = 1000.0\n        phi       = 1.0\n        kappa     = 1/1.54\n        gamma     = 1/2.46\n        E_0       = 0.34\n        tau       = 0.98\n        alpha     = 0.33\n        V_0       = 0.02\n        v_0       = 40.3\n        TE        = 40/1000.\n        epsilon   = 1.43\n    ''',\n    equations = '''\n        # Single input\n        I_CBF          = sum(I_CBF)                                                : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Classic coefficients\n        k_1            = (1 - V_0) * 4.3 * v_0 * E_0 * TE\n        k_2            = 2 * E_0\n        k_3            = 1 - epsilon\n\n        # Linear equation\n        BOLD           = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v))        : init=0\n    ''',\n    inputs=\"I_CBF\",\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_CL"
    ]
  },
  {
    "objectID": "reference/balloon_CL.html#parameters",
    "href": "reference/balloon_CL.html#parameters",
    "title": "balloon_CL",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_CL"
    ]
  },
  {
    "objectID": "reference/BoldModel.html",
    "href": "reference/BoldModel.html",
    "title": "BoldModel",
    "section": "",
    "text": "extensions.bold.BoldModel.BoldModel(self, parameters, equations, inputs, output=['BOLD'], name='Custom BOLD model', description='')\nBase class to define a BOLD model to be used in a BOLD monitor.\nA BOLD model is quite similar to a regular rate-coded neuron. It gets a weighted sum of inputs with a specific target (e.g. I_CBF) and compute a single output variable (called BOLD in the predefined models, but it could be anything).\nThe main difference is that a BOLD model should also declare which targets are used for the input signal:\nbold_model = BoldModel(\n    parameters = '''\n        tau = 1000.\n    ''',\n    equations = '''\n        I_CBF = sum(I_CBF)\n        # ...\n        tau * dBOLD/dt = I_CBF - BOLD\n    ''',\n    inputs = \"I_CBF\"\n)\nThe provided BOLD models follow the Balloon model (Buxton et al., 1998) with the different variations studied in (Stephan et al., 2007). Those models all compute the vascular response to neural activity through a dampened oscillator:\n\n    \\frac{ds}{dt} = \\phi \\, I_\\text{CBF} - \\kappa \\, s - \\gamma \\, (f_{in} - 1)\n\n\n    \\frac{df_{in}}{dt} = s\n\nThis allows to compute the oxygen extraction fraction:\n\n    E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} }\n\nThe (normalized) venous blood volume is computed as:\n\n    \\tau_0 \\, \\frac{dv}{dt} = (f_{in} - f_{out})\n\n\n    f_{out} = v^{\\frac{1}{\\alpha}}\n\nThe level of deoxyhemoglobin into the venous compartment is computed by:\n\n    \\tau_0 \\, \\frac{dq}{dt} = f_{in} \\, \\frac{E}{E_0} - \\frac{q}{v} \\, f_{out}\n\nUsing the two signals v and q, there are two ways to compute the corresponding BOLD signal:\n\nN: Non-linear BOLD equation:\n\n\n    BOLD = v_0 \\, ( k_1 \\, (1-q) + k_2 \\, (1- \\dfrac{q}{v}) + k_3 \\, (1 - v) )\n\n\nL: Linear BOLD equation:\n\n\n    BOLD = v_0 \\, ((k_1 + k_2) \\, (1 - q) + (k_3 - k_2) \\, (1 - v))\n\nAdditionally, the three coefficients k_1, k_2, k_3 can be computed in two different ways:\n\nC: classical coefficients from (Buxton et al., 1998):\n\nk_1            = (1 - v_0) \\, 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\nk_2            = 2 \\, E_0\nk_3            = 1 - \\epsilon\n\nR: revised coefficients from (Obata et al., 2004):\n\nk_1            = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\nk_2            = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\nk_3            = 1 - \\epsilon\nThis makes a total of four different BOLD model (balloon_RN, balloon_RL, balloon_CN, balloon_CL) which are provided by the extension. The different parameters can be modified in the constructor. Additionally, we also provide the model that was used in (Maith et al., 2021) and the two-inputs model of (Maith et al, 2022).\n\nBuxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855-864. doi:10.1002/mrm.1910390602\n\n\nFriston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466-477\n\n\nBuxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, 220-233. doi:10.1016/j.neuroimage.2004.07.013\n\n\nStephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387-401. doi:10.1016/j.neuroimage.2007.07.040\n\n\nMaith et al. (2021) A computational model-based analysis of basal ganglia pathway changes in Parkinson’s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278-2295. doi:10.1111/ejn.14868\n\n\nMaith et al. (2022) BOLD monitoring in the neural simulator ANNarchy. Frontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameters\n\nparameters of the model and their initial value.\nrequired\n\n\nequations\n\nequations defining the temporal evolution of variables.\nrequired\n\n\ninputs\n\nsingle variable or list of input signals (e.g. ‘I_CBF’ or [‘I_CBF’, ‘I_CMRO2’]).\nrequired\n\n\noutput\n\noutput variable of the model (default is ‘BOLD’).\n['BOLD']\n\n\nname\n\noptional model name.\n'Custom BOLD model'\n\n\ndescription\n\noptional model description.\n''",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "BoldModel"
    ]
  },
  {
    "objectID": "reference/BoldModel.html#parameters",
    "href": "reference/BoldModel.html#parameters",
    "title": "BoldModel",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nparameters\n\nparameters of the model and their initial value.\nrequired\n\n\nequations\n\nequations defining the temporal evolution of variables.\nrequired\n\n\ninputs\n\nsingle variable or list of input signals (e.g. ‘I_CBF’ or [‘I_CBF’, ‘I_CMRO2’]).\nrequired\n\n\noutput\n\noutput variable of the model (default is ‘BOLD’).\n['BOLD']\n\n\nname\n\noptional model name.\n'Custom BOLD model'\n\n\ndescription\n\noptional model description.\n''",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "BoldModel"
    ]
  },
  {
    "objectID": "reference/STDP.html",
    "href": "reference/STDP.html",
    "title": "STDP",
    "section": "",
    "text": "STDP\nmodels.Synapses.STDP(self, tau_plus=20.0, tau_minus=20.0, A_plus=0.01, A_minus=0.01, w_min=0.0, w_max=1.0)\nSpike-timing dependent plasticity, online version.\n\nSong, S., and Abbott, L.F. (2001). Cortical development and remapping through spike timing-dependent plasticity. Neuron 32, 339-350.\n\nParameters (global):\n\ntau_plus = 20.0 : time constant of the pre-synaptic trace (ms)\ntau_minus = 20.0 : time constant of the pre-synaptic trace (ms)\nA_plus = 0.01 : increase of the pre-synaptic trace after a spike.\nA_minus = 0.01 : decrease of the post-synaptic trace after a spike.\nw_min = 0.0 : minimal value of the weight w.\nw_max = 1.0 : maximal value of the weight w.\n\nVariables:\n\nx : pre-synaptic trace:\n\ntau_plus  * dx/dt = -x\n\ny: post-synaptic trace:\n\ntau_minus * dy/dt = -y\nBoth variables are evaluated event-driven.\nPre-spike events:\ng_target += w\n\nx += A_plus * w_max\n\nw = clip(w + y, w_min , w_max)\nPost-spike events::\ny -= A_minus * w_max\n\nw = clip(w + x, w_min , w_max)\nEquivalent code:\n\nSTDP = ann.Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection\n        tau_minus = 20.0 : projection\n        A_plus = 0.01 : projection\n        A_minus = 0.01 : projection\n        w_min = 0.0 : projection\n        w_max = 1.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_plus  * dx/dt = -x : event-driven\n        tau_minus * dy/dt = -y : event-driven\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        x += A_plus * w_max\n        w = clip(w + y, w_min , w_max)\n    \"\"\",\n    post_spike=\"\"\"\n        y -= A_minus * w_max\n        w = clip(w + x, w_min , w_max)\n    \"\"\"\n)",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "STDP"
    ]
  },
  {
    "objectID": "reference/reset.html",
    "href": "reference/reset.html",
    "title": "reset",
    "section": "",
    "text": "core.Global.reset(populations=True, projections=False, synapses=False, monitors=True, net_id=0)\nReinitialises the network to its state before the call to compile. The network time will be set to 0ms.\nAll monitors are emptied.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npopulations\nbool\nif True (default), the neural parameters and variables will be reset to their initial value.\nTrue\n\n\nprojections\nbool\nif True, the synaptic parameters and variables (except the connections) will be reset (default=False).\nFalse\n\n\nsynapses\nbool\nif True, the synaptic weights will be erased and recreated (default=False).\nFalse\n\n\nmonitors\nbool\nif True, the monitors will be emptied and reset (default=True).\nTrue",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "reset"
    ]
  },
  {
    "objectID": "reference/reset.html#parameters",
    "href": "reference/reset.html#parameters",
    "title": "reset",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npopulations\nbool\nif True (default), the neural parameters and variables will be reset to their initial value.\nTrue\n\n\nprojections\nbool\nif True, the synaptic parameters and variables (except the connections) will be reset (default=False).\nFalse\n\n\nsynapses\nbool\nif True, the synaptic weights will be erased and recreated (default=False).\nFalse\n\n\nmonitors\nbool\nif True, the monitors will be emptied and reset (default=True).\nTrue",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "reset"
    ]
  },
  {
    "objectID": "reference/disable_learning.html",
    "href": "reference/disable_learning.html",
    "title": "disable_learning",
    "section": "",
    "text": "core.Global.disable_learning(projections=None, net_id=0)\nDisables learning for all projections.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojections\n\nthe projections whose learning should be disabled. By default, all the existing projections are disabled.\nNone",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "disable_learning"
    ]
  },
  {
    "objectID": "reference/disable_learning.html#parameters",
    "href": "reference/disable_learning.html#parameters",
    "title": "disable_learning",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nprojections\n\nthe projections whose learning should be disabled. By default, all the existing projections are disabled.\nNone",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "disable_learning"
    ]
  },
  {
    "objectID": "reference/set_time.html",
    "href": "reference/set_time.html",
    "title": "set_time",
    "section": "",
    "text": "set_time\ncore.Global.set_time(t, net_id=0)\nSets the current time in ms.\nWarning: can be dangerous for some spiking models.",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "set_time"
    ]
  },
  {
    "objectID": "reference/populations.html",
    "href": "reference/populations.html",
    "title": "populations",
    "section": "",
    "text": "populations\ncore.Global.populations(net_id=0)\nReturns a list of all declared populations.\n:retruns: a list of all populations.",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "populations"
    ]
  },
  {
    "objectID": "reference/HH_cond_exp.html",
    "href": "reference/HH_cond_exp.html",
    "title": "HH_cond_exp",
    "section": "",
    "text": "HH_cond_exp\nmodels.Neurons.HH_cond_exp(self, gbar_Na=20.0, gbar_K=6.0, gleak=0.01, cm=0.2, v_offset=-63.0, e_rev_Na=50.0, e_rev_K=-90.0, e_rev_leak=-65.0, e_rev_E=0.0, e_rev_I=-80.0, tau_syn_E=0.2, tau_syn_I=2.0, i_offset=0.0, v_thresh=0.0)\nSingle-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub.\nParameters:\n\ngbar_Na = 20.0 : Maximal conductance of the Sodium current.\ngbar_K = 6.0 : Maximal conductance of the Potassium current.\ngleak = 0.01 : Conductance of the leak current (nF)\n\ncm = 0.2 : Capacity of the membrane (nF)\nv_offset = -63.0 : Threshold for the rate constants (mV)\n\ne_rev_Na = 50.0 : Reversal potential for the Sodium current (mV)\ne_rev_K = -90.0 : Reversal potential for the Potassium current (mV)\n\ne_rev_leak = -65.0 : Reversal potential for the leak current (mV)\n\ne_rev_E = 0.0 : Reversal potential for excitatory input (mV)\n\ne_rev_I = -80.0 : Reversal potential for inhibitory input (mV)\n\ntau_syn_E = 0.2 : Decay time of excitatory synaptic current (ms)\n\ntau_syn_I = 2.0 : Decay time of inhibitory synaptic current (ms)\n\ni_offset = 0.0 : Offset current (nA)\nv_thresh = 0.0 : Threshold for spike emission\n\nVariables:\n\nVoltage-dependent rate constants an, bn, am, bm, ah, bh:\nan = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0)\nbn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) )\nActivation variables n, m, h (h is initialized to 1.0, n and m to 0.0):\ndn/dt = an * (1.0 - n) - bn * n dm/dt = am * (1.0 - m) - bm * m dh/dt = ah * (1.0 - h) - bh * h\nv : membrane potential in mV (init=-65.0):\ncm * dv/dt = gleak(e_rev_leak -v) + gbar_K  n4 * (e_rev_K - v) + gbar_Na * m3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset\ng_exc : excitatory conductance (init = 0.0):\ntau_syn_E * dg_exc/dt = - g_exc\ng_inh : inhibitory conductance (init = 0.0):\ntau_syn_I * dg_inh/dt = - g_inh\n\nSpike emission (the spike is emitted only once when v crosses the threshold from below):\nv &gt; v_thresh and v(t-1) &lt; v_thresh\nThe ODEs for n, m, h and v are solved using the midpoint method, while the conductances g_exc and g_inh are solved using the exponential Euler method.\nEquivalent code:\n\nHH_cond_exp = Neuron(\n    parameters = \"\"\"\n        gbar_Na = 20.0\n        gbar_K = 6.0\n        gleak = 0.01\n        cm = 0.2 \n        v_offset = -63.0 \n        e_rev_Na = 50.0\n        e_rev_K = -90.0 \n        e_rev_leak = -65.0\n        e_rev_E = 0.0\n        e_rev_I = -80.0 \n        tau_syn_E = 0.2\n        tau_syn_I = 2.0\n        i_offset = 0.0\n        v_thresh = 0.0\n    \"\"\", \n    equations = \"\"\"\n        # Previous membrane potential\n        prev_v = v\n\n        # Voltage-dependent rate constants\n        an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0)\n        am = 0.32  * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0)\n        ah = 0.128 * exp((17.0 - v + v_offset)/18.0) \n\n        bn = 0.5   * exp ((10.0 - v + v_offset)/40.0)\n        bm = 0.28  * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0)\n        bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) )\n\n        # Activation variables\n        dn/dt = an * (1.0 - n) - bn * n : init = 0.0, exponential\n        dm/dt = am * (1.0 - m) - bm * m : init = 0.0, exponential\n        dh/dt = ah * (1.0 - h) - bh * h : init = 1.0, exponential\n\n        # Membrane equation\n        cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v)\n                        + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset: exponential, init=-65.0\n\n        # Exponentially-decaying conductances\n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n    \"\"\",\n    spike = \"(v &gt; v_thresh) and (prev_v &lt;= v_thresh)\",\n    reset = \"\"\n)",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "HH_cond_exp"
    ]
  },
  {
    "objectID": "reference/BoldMonitor.html",
    "href": "reference/BoldMonitor.html",
    "title": "BoldMonitor",
    "section": "",
    "text": "extensions.bold.BoldMonitor.BoldMonitor(self, populations=None, bold_model=balloon_RN, mapping={'I_CBF': 'r'}, scale_factor=None, normalize_input=None, recorded_variables=None, start=False, net_id=0, copied=False)\nMonitors the BOLD signal for several populations using a computational model.\nThe BOLD monitor transforms one or two input population variables (such as the mean firing rate) into a recordable BOLD signal according to a computational model (for example a variation of the Balloon model).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npopulations\nlist\nlist of recorded populations.\nNone\n\n\nbold_model\nANNarchy.extensions.bold.BoldModel\ncomputational model for BOLD signal defined as a BoldModel object. Default is balloon_RN.\nballoon_RN\n\n\nmapping\ndict\nmapping dictionary between the inputs of the BOLD model (I_CBF for single inputs, I_CBF and I_CMRO2 for double inputs in the provided examples) and the variables of the input populations. By default, {'I_CBF': 'r'} maps the firing rate r of the input population(s) to the variable I_CBF of the BOLD model.\n{'I_CBF': 'r'}\n\n\nscale_factor\nlist[float]\nlist of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region.\nNone\n\n\nnormalize_input\nlist[int]\nlist of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time).\nNone\n\n\nrecorded_variables\nlist[str]\nwhich variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [“BOLD”] for the provided examples).\nNone\n\n\nstart\nbool\nwhether to start recording directly.\nFalse\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget\nRetrieves recordings as in ANNarchy.core.Monitor.get().\n\n\nstart\nStarts recording as in ANNarchy.core.Monitor.start().\n\n\nstop\nStops recording as in `ANNarchy.core.Monitor.stop().\n\n\n\n\n\nextensions.bold.BoldMonitor.BoldMonitor.get(variable)\nRetrieves recordings as in ANNarchy.core.Monitor.get().\n\n\n\nextensions.bold.BoldMonitor.BoldMonitor.start()\nStarts recording as in ANNarchy.core.Monitor.start().\n\n\n\nextensions.bold.BoldMonitor.BoldMonitor.stop()\nStops recording as in `ANNarchy.core.Monitor.stop().",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "BoldMonitor"
    ]
  },
  {
    "objectID": "reference/BoldMonitor.html#parameters",
    "href": "reference/BoldMonitor.html#parameters",
    "title": "BoldMonitor",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npopulations\nlist\nlist of recorded populations.\nNone\n\n\nbold_model\nANNarchy.extensions.bold.BoldModel\ncomputational model for BOLD signal defined as a BoldModel object. Default is balloon_RN.\nballoon_RN\n\n\nmapping\ndict\nmapping dictionary between the inputs of the BOLD model (I_CBF for single inputs, I_CBF and I_CMRO2 for double inputs in the provided examples) and the variables of the input populations. By default, {'I_CBF': 'r'} maps the firing rate r of the input population(s) to the variable I_CBF of the BOLD model.\n{'I_CBF': 'r'}\n\n\nscale_factor\nlist[float]\nlist of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region.\nNone\n\n\nnormalize_input\nlist[int]\nlist of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time).\nNone\n\n\nrecorded_variables\nlist[str]\nwhich variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [“BOLD”] for the provided examples).\nNone\n\n\nstart\nbool\nwhether to start recording directly.\nFalse",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "BoldMonitor"
    ]
  },
  {
    "objectID": "reference/BoldMonitor.html#methods",
    "href": "reference/BoldMonitor.html#methods",
    "title": "BoldMonitor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget\nRetrieves recordings as in ANNarchy.core.Monitor.get().\n\n\nstart\nStarts recording as in ANNarchy.core.Monitor.start().\n\n\nstop\nStops recording as in `ANNarchy.core.Monitor.stop().\n\n\n\n\n\nextensions.bold.BoldMonitor.BoldMonitor.get(variable)\nRetrieves recordings as in ANNarchy.core.Monitor.get().\n\n\n\nextensions.bold.BoldMonitor.BoldMonitor.start()\nStarts recording as in ANNarchy.core.Monitor.start().\n\n\n\nextensions.bold.BoldMonitor.BoldMonitor.stop()\nStops recording as in `ANNarchy.core.Monitor.stop().",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "BoldMonitor"
    ]
  },
  {
    "objectID": "reference/raster_plot.html",
    "href": "reference/raster_plot.html",
    "title": "raster_plot",
    "section": "",
    "text": "core.Monitor.raster_plot(spikes)\nReturns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.\nExample:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nspikes = m.get('spike')\nt, n = raster_plot(spikes)\n\nplt.plot(t, n, '.')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike').\nrequired",
    "crumbs": [
      "Reference",
      "**Plotting**",
      "raster_plot"
    ]
  },
  {
    "objectID": "reference/raster_plot.html#parameters",
    "href": "reference/raster_plot.html#parameters",
    "title": "raster_plot",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike').\nrequired",
    "crumbs": [
      "Reference",
      "**Plotting**",
      "raster_plot"
    ]
  },
  {
    "objectID": "reference/ANNtoSNNConverter.html",
    "href": "reference/ANNtoSNNConverter.html",
    "title": "ANNtoSNNConverter",
    "section": "",
    "text": "extensions.ann_to_snn_conversion.ANNtoSNNConverter.ANNtoSNNConverter(self, input_encoding='poisson', hidden_neuron='IaF', read_out='spike_count', **kwargs)\nConverts a pre-trained Keras model into a spiking neural network.\nThe implementation of the present module is inspired by the SNNToolbox (Rueckauer et al. 2017), and is largely based on the work of Diehl et al. (2015). We provide several input encodings, as suggested in the work of Park et al. (2019) and Auge et al. (2021).\nProcessing Queue\nThe pre-trained ANN model to be converted should be saved in keras as an h5py file (extension .h5). The saved model is transformed layer by layer into a feed-forward ANNarchy spiking network. The structure of the network remains the same as in the original ANN, while the weights are normalised. Please note that the current implementation focuses primarily on the correctness of the conversion. Computational performance, especially of the converted CNNs, will be improved in future releases.\n\n\n\n\n\n\nNote\n\n\n\nWhile the neurons are conceptually spiking neurons, there is one specialty: next to the spike event (stored automatically in ANNarchy), each event will be stored in an additional mask array. This mask value decays in absence of further spike events exponentially. The decay can be controlled by the mask_tau parameter of the population. The projections (either dense or convolution) will use this mask as pre-synaptic input, not the generated list of spike events.\n\n\nInput Encoding\nIn the following we provide brief descriptions of the available input encodings. The abbreviations in brackets denote the name provided to the conversion tool constructor.\n\nIntrinsically Bursting (“IB”)\n\nThis encoding bases on the Izhikevich (2003) model that comprises two ODEs:\n\n\\begin{cases}\n\\frac{dv}{dt} = 0.04 \\cdot v^2 + 5.0 \\cdot v + 140.0 - u + I \\\\\n\\\\\n\\frac{du}{dt} = a \\cdot (b \\cdot v - u) \\\\\n\\end{cases}\n\nThe parameters for a - d are selected accordingly to Izhikevich (2003). The provided input images will be set as I.\n\nPoisson (“CPN”)\n\nThis encoding uses a Poisson distribution where the pixel values of the image will be used as probability for each individual neuron.\n\nPhase Shift Oscillation (“PSO”)\n\nBased on the description by Park et al. (2019), the spiking threshold v_\\text{th} is modulated by a oscillation function \\Pi, whereas the membrane potential follows simply the input current.\n\n\\begin{cases}\n\\Pi(t) = 2^{-(1+ \\text{mod}(t,k))}\\\\\n\\\\\nv_\\text{th}(t) = \\Pi(t) \\, v_\\text{th}(t)\\\\\n\\end{cases}\n\n\nUser-defined input encodings\n\nIn addition to the pre-defined models, one can opt for individual models using the Neuron class of ANNarchy. Please note that a mask variable need to be defined, which is fed into the subsequent projections.\nRead-out Methods\nIn a classification task, the neuron with the highest activity corresponds corresponds to the decision to which class the presented input belongs. However, the highest activity can be determined in different ways. We support currently three methods, defined by the read_out parameter of the constructor:\n\nMaximum Spike Count\n\nread_out = 'spike_count' : the number of spikes emitted by each neuron is recorded and the index of the neuron(s) with the maximum number is returned.\n\nTime to Number of Spikes\n\nread_out = 'time_to_first_spike' or read_out = 'time_to_k_spikes': when the first or first k spikes are emitted by a single neuron, the simulation is stopped and the neuron rank(s) is returned. For the second mode, an additional k argument need to be also provided.\n\nMembrane potential\n\nread_out = 'membrane_potential': pre-synaptic events are accumulated in the membrane potential of each output neuron. The index of the neuron(s) with the highest membrane potential is returned.\n\nIzhikevich (2003) Simple Model of Spiking Neurons. IEEE transactions on neural networks 14(6). doi: 10.1109/TNN.2003.820440\n\n\nDiehl PU, Neil D, Binas J,et al. (2015) Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing, 2015 International Joint Conference on Neural Networks (IJCNN), 1-8, doi: 10.1109/IJCNN.2015.7280696.\n\n\nRueckauer B, Lungu I, Hu Y, et al. (2017) Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification., Front. Neurosci., 2017, 11. doi: 10.3389/fnins.2017.00682\n\n\nPark S, Kim S, Choe H, et al. (2019) Fast and Efficient Information Transmission with Burst Spikes in Deep Spiking Neural Networks.\n\n\nAuge D, Hille J, Mueller E et al. (2021) A Survey of Encoding Techniques for Signal Processing in Spiking Neural Networks. Neural Processing Letters. 2021; 53:4963-4710. doi:10.1007/s11063-021-10562-2\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput_encoding\nstr\na string representing which input encoding should be used: custom poisson, PSO, IB or CH.\n'poisson'\n\n\nhidden_neuron\nstr\nneuron model used in the hidden layers. Either the default integrate-and-fire (‘IaF’) or an ANNarchy Neuron object.\n'IaF'\n\n\nread_out\nstr\na string which of the following read-out method should be used: spike_count, time_to_first_spike, membrane_potential.\n'spike_count'\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_annarchy_network\nReturns the ANNarchy.Network instance.\n\n\ninit_from_keras_model\nLoads the pre-trained model provided as a .h5 file.\n\n\npredict\nPerforms the prediction for a given input series.\n\n\n\n\n\nextensions.ann_to_snn_conversion.ANNtoSNNConverter.ANNtoSNNConverter.get_annarchy_network()\nReturns the ANNarchy.Network instance.\n\n\n\nextensions.ann_to_snn_conversion.ANNtoSNNConverter.ANNtoSNNConverter.init_from_keras_model(filename, directory='annarchy', scale_factor=None, show_info=True)\nLoads the pre-trained model provided as a .h5 file.\nIn tf.keras, the weights can be saved using:\nmodel.save(\"model.h5\")\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\n\nstored model as a .h5 file.\nrequired\n\n\ndirectory\n\nsub-directory where the generated code should be stored (default: “annarchy”)\n'annarchy'\n\n\nscale_factor\n\nallows a fine-grained control of the weight scale factor. By default (None), with each layer-depth the factor increases by one. If a scalar value is provided the same value is used for each layer. Otherwise a list can be provided to assign the scale factors individually.\nNone\n\n\nshow_info\n\nwhether the network structure should be printed on console (default: True)\nTrue\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nANNarchy.core.Network.Network\nAn ANNarchy.Network instance.\n\n\n\n\n\n\n\nextensions.ann_to_snn_conversion.ANNtoSNNConverter.ANNtoSNNConverter.predict(samples, duration_per_sample=1000, measure_time=False)\nPerforms the prediction for a given input series.\nParameters:\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsamples\n\nset of inputs to present to the network. The function expects a 2-dimensional array (num_samples, input_size).\nrequired\n\n\nduration_per_sample\n\nthe number of simulation steps for one input sample (default: 1000, 1 second biological time)\n1000\n\n\nmeasure_time\n\nprint out the computation time spent for one input sample (default: False)\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[int]\nA list of predicted class indices. If multiple neurons fulfill the condition, all candidate indices are returned.",
    "crumbs": [
      "Reference",
      "**ANN-to-SNN conversion**",
      "ANNtoSNNConverter"
    ]
  },
  {
    "objectID": "reference/ANNtoSNNConverter.html#parameters",
    "href": "reference/ANNtoSNNConverter.html#parameters",
    "title": "ANNtoSNNConverter",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ninput_encoding\nstr\na string representing which input encoding should be used: custom poisson, PSO, IB or CH.\n'poisson'\n\n\nhidden_neuron\nstr\nneuron model used in the hidden layers. Either the default integrate-and-fire (‘IaF’) or an ANNarchy Neuron object.\n'IaF'\n\n\nread_out\nstr\na string which of the following read-out method should be used: spike_count, time_to_first_spike, membrane_potential.\n'spike_count'",
    "crumbs": [
      "Reference",
      "**ANN-to-SNN conversion**",
      "ANNtoSNNConverter"
    ]
  },
  {
    "objectID": "reference/ANNtoSNNConverter.html#methods",
    "href": "reference/ANNtoSNNConverter.html#methods",
    "title": "ANNtoSNNConverter",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_annarchy_network\nReturns the ANNarchy.Network instance.\n\n\ninit_from_keras_model\nLoads the pre-trained model provided as a .h5 file.\n\n\npredict\nPerforms the prediction for a given input series.\n\n\n\n\n\nextensions.ann_to_snn_conversion.ANNtoSNNConverter.ANNtoSNNConverter.get_annarchy_network()\nReturns the ANNarchy.Network instance.\n\n\n\nextensions.ann_to_snn_conversion.ANNtoSNNConverter.ANNtoSNNConverter.init_from_keras_model(filename, directory='annarchy', scale_factor=None, show_info=True)\nLoads the pre-trained model provided as a .h5 file.\nIn tf.keras, the weights can be saved using:\nmodel.save(\"model.h5\")\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\n\nstored model as a .h5 file.\nrequired\n\n\ndirectory\n\nsub-directory where the generated code should be stored (default: “annarchy”)\n'annarchy'\n\n\nscale_factor\n\nallows a fine-grained control of the weight scale factor. By default (None), with each layer-depth the factor increases by one. If a scalar value is provided the same value is used for each layer. Otherwise a list can be provided to assign the scale factors individually.\nNone\n\n\nshow_info\n\nwhether the network structure should be printed on console (default: True)\nTrue\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nANNarchy.core.Network.Network\nAn ANNarchy.Network instance.\n\n\n\n\n\n\n\nextensions.ann_to_snn_conversion.ANNtoSNNConverter.ANNtoSNNConverter.predict(samples, duration_per_sample=1000, measure_time=False)\nPerforms the prediction for a given input series.\nParameters:\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsamples\n\nset of inputs to present to the network. The function expects a 2-dimensional array (num_samples, input_size).\nrequired\n\n\nduration_per_sample\n\nthe number of simulation steps for one input sample (default: 1000, 1 second biological time)\n1000\n\n\nmeasure_time\n\nprint out the computation time spent for one input sample (default: False)\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[int]\nA list of predicted class indices. If multiple neurons fulfill the condition, all candidate indices are returned.",
    "crumbs": [
      "Reference",
      "**ANN-to-SNN conversion**",
      "ANNtoSNNConverter"
    ]
  },
  {
    "objectID": "reference/IF_curr_exp.html",
    "href": "reference/IF_curr_exp.html",
    "title": "IF_curr_exp",
    "section": "",
    "text": "models.Neurons.IF_curr_exp(self, v_rest=-65.0, cm=1.0, tau_m=20.0, tau_refrac=0.0, tau_syn_E=5.0, tau_syn_I=5.0, v_thresh=-50.0, v_reset=-65.0, i_offset=0.0)\nLeaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current.\n(Separate synaptic currents for excitatory and inhibitory synapses).\nParameters:\n\nv_rest = -65.0 : Resting membrane potential (mV)\ncm = 1.0 : Capacity of the membrane (nF)\ntau_m = 20.0 : Membrane time constant (ms)\ntau_refrac = 0.0 : Duration of refractory period (ms)\ntau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms)\ntau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms)\ni_offset = 0.0 : Offset current (nA)\nv_reset = -65.0 : Reset potential after a spike (mV)\nv_thresh = -50.0 : Spike threshold (mV)\n\nVariables:\n\nv : membrane potential in mV (init=-65.0):\ncm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset\ng_exc : excitatory current (init = 0.0):\ntau_syn_E * dg_exc/dt = - g_exc\ng_inh : inhibitory current (init = 0.0):\ntau_syn_I * dg_inh/dt = - g_inh\n\nSpike emission:\nv &gt; v_thresh\nReset:\nv = v_reset\nThe ODEs are solved using the exponential Euler method.\nEquivalent code:\n\nIF_curr_exp = Neuron(\n    parameters = \"\"\"\n        v_rest = -65.0\n        cm  = 1.0\n        tau_m  = 20.0\n        tau_syn_E = 5.0\n        tau_syn_I = 5.0\n        v_thresh = -50.0\n        v_reset = -65.0\n        i_offset = 0.0\n    \"\"\", \n    equations = \"\"\"\n        cm * dv/dt = cm/tau_m*(v_rest -v)   + g_exc - g_inh + i_offset : exponential, init=-65.0\n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n    \"\"\",\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = v_reset\",\n    refractory = 0.0\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv_rest\nfloat\nResting membrane potential (mV)\n-65.0\n\n\ncm\nfloat\nCapacity of the membrane (nF)\n1.0\n\n\ntau_m\nfloat\nMembrane time constant (ms)\n20.0\n\n\ntau_refrac\nfloat\nDuration of refractory period (ms)\n0.0\n\n\ntau_syn_E\nfloat\nDecay time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\nfloat\nDecay time of inhibitory synaptic current (ms)\n5.0\n\n\ni_offset\nfloat\nOffset current (nA)\n0.0\n\n\nv_reset\nfloat\nReset potential after a spike (mV)\n-65.0\n\n\nv_thresh\nfloat\nSpike threshold (mV)\n-50.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_curr_exp"
    ]
  },
  {
    "objectID": "reference/IF_curr_exp.html#parameters",
    "href": "reference/IF_curr_exp.html#parameters",
    "title": "IF_curr_exp",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nv_rest\nfloat\nResting membrane potential (mV)\n-65.0\n\n\ncm\nfloat\nCapacity of the membrane (nF)\n1.0\n\n\ntau_m\nfloat\nMembrane time constant (ms)\n20.0\n\n\ntau_refrac\nfloat\nDuration of refractory period (ms)\n0.0\n\n\ntau_syn_E\nfloat\nDecay time of excitatory synaptic current (ms)\n5.0\n\n\ntau_syn_I\nfloat\nDecay time of inhibitory synaptic current (ms)\n5.0\n\n\ni_offset\nfloat\nOffset current (nA)\n0.0\n\n\nv_reset\nfloat\nReset potential after a spike (mV)\n-65.0\n\n\nv_thresh\nfloat\nSpike threshold (mV)\n-50.0",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "IF_curr_exp"
    ]
  },
  {
    "objectID": "reference/functions.html",
    "href": "reference/functions.html",
    "title": "functions",
    "section": "",
    "text": "core.Global.functions(name, net_id=0)\nAllows to access a global function declared with add_function and use it from Python using arrays after compilation.\nThe name of the function is not added to the global namespace to avoid overloading.\nadd_function(\"logistic(x) = 1. / (1. + exp(-x))\") \n\nann.compile()  \n\nresult = functions('logistic')([0., 1., 2., 3., 4.])\nOnly lists or 1D Numpy arrays can be passed as arguments, not single values nor multidimensional arrays.\nWhen passing several arguments, make sure they have the same size.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the function.\nrequired",
    "crumbs": [
      "Reference",
      "**Functions and Constants**",
      "functions"
    ]
  },
  {
    "objectID": "reference/functions.html#parameters",
    "href": "reference/functions.html#parameters",
    "title": "functions",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the function.\nrequired",
    "crumbs": [
      "Reference",
      "**Functions and Constants**",
      "functions"
    ]
  },
  {
    "objectID": "reference/simulate.html",
    "href": "reference/simulate.html",
    "title": "simulate",
    "section": "",
    "text": "core.Simulate.simulate(duration, measure_time=False, progress_bar=False, callbacks=True, net_id=0)\nSimulates the network for the given duration in milliseconds.\nThe number of simulation steps is computed relative to the discretization step dt declared in setup() (default: 1ms):\nsimulate(1000.0)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nduration\nfloat\nthe duration in milliseconds.\nrequired\n\n\nmeasure_time\nbool\ndefines whether the simulation time should be printed.\nFalse\n\n\nprogress_bar\nbool\ndefines whether a progress bar should be printed.\nFalse\n\n\ncallbacks\nbool\ndefines if the callback method (decorator every should be called).\nTrue\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nNone",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "simulate"
    ]
  },
  {
    "objectID": "reference/simulate.html#parameters",
    "href": "reference/simulate.html#parameters",
    "title": "simulate",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nduration\nfloat\nthe duration in milliseconds.\nrequired\n\n\nmeasure_time\nbool\ndefines whether the simulation time should be printed.\nFalse\n\n\nprogress_bar\nbool\ndefines whether a progress bar should be printed.\nFalse\n\n\ncallbacks\nbool\ndefines if the callback method (decorator every should be called).\nTrue",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "simulate"
    ]
  },
  {
    "objectID": "reference/simulate.html#returns",
    "href": "reference/simulate.html#returns",
    "title": "simulate",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nNone",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "simulate"
    ]
  },
  {
    "objectID": "reference/Exponential.html",
    "href": "reference/Exponential.html",
    "title": "Exponential",
    "section": "",
    "text": "core.Random.Exponential(self, Lambda, min=None, max=None)\nExponential distribution, according to the density function:\nP(x | \\lambda) = \\lambda e^{(-\\lambda x )}\nNote: Lambda is capitalized, otherwise it would be a reserved Python keyword.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nLambda\nfloat\nrate parameter.\nrequired\n\n\nmin\nfloat\nminimum value (default: unlimited).\nNone\n\n\nmax\nfloat\nmaximum value (default: unlimited).\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.Exponential.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Exponential"
    ]
  },
  {
    "objectID": "reference/Exponential.html#parameters",
    "href": "reference/Exponential.html#parameters",
    "title": "Exponential",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nLambda\nfloat\nrate parameter.\nrequired\n\n\nmin\nfloat\nminimum value (default: unlimited).\nNone\n\n\nmax\nfloat\nmaximum value (default: unlimited).\nNone",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Exponential"
    ]
  },
  {
    "objectID": "reference/Exponential.html#methods",
    "href": "reference/Exponential.html#methods",
    "title": "Exponential",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.Exponential.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Exponential"
    ]
  },
  {
    "objectID": "reference/Convolution.html",
    "href": "reference/Convolution.html",
    "title": "Convolution",
    "section": "",
    "text": "extensions.convolution.Convolve.Convolution(self, pre, post, target, psp='pre.r * w', operation='sum', name=None, copied=False)\nPerforms a convolution of a weight kernel on the pre-synaptic population.\nDespite its name, the operation performed is actually a cross-correlation, as is usual in computer vision and convolutional neural networks:\ng(x) = \\sum_{k=-n}^n h(k) \\, f(x + k)\nThe convolution operation benefits from giving a multi-dimensional geometry to the populations and filters, for example in 2D:\ninp = ann.Population(geometry=(100, 100), neuron=ann.Neuron(parameters=\"r = 0.0\"))\npop = ann.Population(geometry=(100, 100), neuron=ann.Neuron(equations=\"r = sum(exc)\"))\n\nproj = Convolution(inp, pop, 'exc')\nproj.connect_filter(\n    [\n        [-1., 0., 1.],\n        [-1., 0., 1.],\n        [-1., 0., 1.]\n    ])\nThe maximum number of dimensions for populations and filters is 4, an error is thrown otherwise.\nDepending on the number of dimensions of the pre- and post-synaptic populations, as well as of the kernel, the convolution is implemented differentely.\nMethod connect_filter()\n\nIf the pre- and post-populations have the same dimension as the kernel, the convolution is regular. Example:\n(100, 100) * (3, 3) -&gt; (100, 100)\nIf the post-population has one dimension less than the pre-synaptic one, the last dimension of the kernel must match the last one of the pre-synaptic population. Example:\n(100, 100, 3) * (3, 3, 3) -&gt; (100, 100)\nIf the kernel has less dimensions than the two populations, the number of neurons in the last dimension of the populations must be the same. The convolution will be calculated for each feature map in the last dimension. In this case, you must set keep_last_dimension to True. Example:\n(100, 100, 16) * (3, 3) -&gt; (100, 100, 16)\n\nMethod connect_filters()\n\nIf the kernel has more dimensions than the pre-synaptic population, this means a bank of different filters will be applied on the pre-synaptic population (like a convolutional layer in a CNN). Attention: the first index of weights corresponds to the different filters, while the result will be accessible in the last dimension of the post-synaptic population. You must set the multiple argument to True. Example:\n(100, 100) * (16, 3, 3) -&gt; (100, 100, 16)\n\nThe convolution always uses padding for elements that would be outside the array (no equivalent of valid in tensorflow). It is 0.0 by default, but can be changed using the padding argument. Setting padding to the string border will repeat the value of the border elements.\nSub-sampling will be automatically performed according to the populations’ geometry. If these geometries do not match, an error will be thrown. Example:\n(100, 100) * (3, 3) -&gt; (50, 50)\nYou can redefine the sub-sampling by providing a list subsampling as argument, defining for each post-synaptic neuron the coordinates of the pre-synaptic neuron which will be the center of the filter/kernel.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\n\npre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\n\npost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\n\ntype of the connection\nrequired\n\n\npsp\n\ncontinuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r).\n'pre.r * w'\n\n\noperation\n\noperation (sum, max, min, mean) performed by the kernel (default: sum).\n'sum'\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nconnect_filter\nApplies a single filter on the pre-synaptic population.\n\n\nconnect_filters\nApplies a set of different filters on the pre-synaptic population.\n\n\nconnectivity_matrix\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nsave\nNot available.\n\n\nsave_connectivity\nNot available.\n\n\n\n\n\nextensions.convolution.Convolve.Convolution.connect_filter(weights, delays=0.0, keep_last_dimension=False, padding=0.0, subsampling=None)\nApplies a single filter on the pre-synaptic population.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\n\nnumpy array or list of lists representing the matrix of weights for the filter.\nrequired\n\n\ndelays\n\ndelay in synaptic transmission (default: dt). Can only be the same value for all neurons.\n0.0\n\n\nkeep_last_dimension\n\ndefines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.\nFalse\n\n\npadding\n\nvalue to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to ‘border’, the values on the boundaries are repeated. Default: 0.0.\n0.0\n\n\nsubsampling\n\nlist for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.\nNone\n\n\n\n\n\n\n\nextensions.convolution.Convolve.Convolution.connect_filters(weights, delays=0.0, keep_last_dimension=False, padding=0.0, subsampling=None)\nApplies a set of different filters on the pre-synaptic population.\nThe weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\n\nnumpy array or list of lists representing the matrix of weights for the filter.\nrequired\n\n\ndelays\n\ndelay in synaptic transmission (default: dt). Can only be the same value for all neurons.\n0.0\n\n\nkeep_last_dimension\n\ndefines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.\nFalse\n\n\npadding\n\nvalue to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to ‘border’, the values on the boundaries are repeated. Default: 0.0.\n0.0\n\n\nsubsampling\n\nlist for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.\nNone\n\n\n\n\n\n\n\nextensions.convolution.Convolve.Convolution.connectivity_matrix(fill=0.0)\nNot available.\n\n\n\nextensions.convolution.Convolve.Convolution.load(filename)\nNot available.\n\n\n\nextensions.convolution.Convolve.Convolution.receptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nextensions.convolution.Convolve.Convolution.save(filename)\nNot available.\n\n\n\nextensions.convolution.Convolve.Convolution.save_connectivity(filename)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Convolution"
    ]
  },
  {
    "objectID": "reference/Convolution.html#parameters",
    "href": "reference/Convolution.html#parameters",
    "title": "Convolution",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\n\npre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\n\npost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\n\ntype of the connection\nrequired\n\n\npsp\n\ncontinuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r).\n'pre.r * w'\n\n\noperation\n\noperation (sum, max, min, mean) performed by the kernel (default: sum).\n'sum'",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Convolution"
    ]
  },
  {
    "objectID": "reference/Convolution.html#methods",
    "href": "reference/Convolution.html#methods",
    "title": "Convolution",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nconnect_filter\nApplies a single filter on the pre-synaptic population.\n\n\nconnect_filters\nApplies a set of different filters on the pre-synaptic population.\n\n\nconnectivity_matrix\nNot available.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nsave\nNot available.\n\n\nsave_connectivity\nNot available.\n\n\n\n\n\nextensions.convolution.Convolve.Convolution.connect_filter(weights, delays=0.0, keep_last_dimension=False, padding=0.0, subsampling=None)\nApplies a single filter on the pre-synaptic population.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\n\nnumpy array or list of lists representing the matrix of weights for the filter.\nrequired\n\n\ndelays\n\ndelay in synaptic transmission (default: dt). Can only be the same value for all neurons.\n0.0\n\n\nkeep_last_dimension\n\ndefines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.\nFalse\n\n\npadding\n\nvalue to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to ‘border’, the values on the boundaries are repeated. Default: 0.0.\n0.0\n\n\nsubsampling\n\nlist for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.\nNone\n\n\n\n\n\n\n\nextensions.convolution.Convolve.Convolution.connect_filters(weights, delays=0.0, keep_last_dimension=False, padding=0.0, subsampling=None)\nApplies a set of different filters on the pre-synaptic population.\nThe weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\n\nnumpy array or list of lists representing the matrix of weights for the filter.\nrequired\n\n\ndelays\n\ndelay in synaptic transmission (default: dt). Can only be the same value for all neurons.\n0.0\n\n\nkeep_last_dimension\n\ndefines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.\nFalse\n\n\npadding\n\nvalue to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to ‘border’, the values on the boundaries are repeated. Default: 0.0.\n0.0\n\n\nsubsampling\n\nlist for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.\nNone\n\n\n\n\n\n\n\nextensions.convolution.Convolve.Convolution.connectivity_matrix(fill=0.0)\nNot available.\n\n\n\nextensions.convolution.Convolve.Convolution.load(filename)\nNot available.\n\n\n\nextensions.convolution.Convolve.Convolution.receptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nextensions.convolution.Convolve.Convolution.save(filename)\nNot available.\n\n\n\nextensions.convolution.Convolve.Convolution.save_connectivity(filename)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Convolution"
    ]
  },
  {
    "objectID": "reference/STP.html",
    "href": "reference/STP.html",
    "title": "STP",
    "section": "",
    "text": "STP\nmodels.Synapses.STP(self, tau_rec=100.0, tau_facil=0.01, U=0.5)\nSynapse exhibiting short-term facilitation and depression.\nImplemented using the model of Tsodyks, Markram et al.:\n\nTsodyks, Uziel and Markram (2000) Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses. Journal of Neuroscience 20:RC50\n\nNote that the time constant of the post-synaptic current is set in the neuron model, not here.\nParameters (global):\n\ntau_rec = 100.0 : depression time constant (ms).\ntau_facil = 0.01 : facilitation time constant (ms).\nU = 0.5 : use parameter.\n\nVariables:\n\nx : recovery variable::\n\ndx/dt = (1 - x)/tau_rec \n\nu : facilitation variable::\n\ndu/dt = (U - u)/tau_facil \nBoth variables are integrated event-driven.\nPre-spike events:\ng_target += w * u * x\nx *= (1 - u)\nu += U * (1 - u)\nEquivalent code:\nSTP = ann.Synapse(\n    parameters = \"\"\"\n        tau_rec = 100.0 : projection\n        tau_facil = 0.01 : projection\n        U = 0.5\n    \"\"\",\n    equations = \"\"\"\n        dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n        du/dt = (U - u)/tau_facil : init = 0.5, event-driven\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w * u * x\n        x *= (1 - u)\n        u += U * (1 - u)\n    \"\"\"\n)",
    "crumbs": [
      "Reference",
      "**Synapse models**",
      "STP"
    ]
  },
  {
    "objectID": "reference/enable_learning.html",
    "href": "reference/enable_learning.html",
    "title": "enable_learning",
    "section": "",
    "text": "core.Global.enable_learning(projections=None, period=None, offset=None, net_id=0)\nEnables learning for all projections. Optionally period and offset can be changed for all projections.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojections\nlist\nthe projections whose learning should be enabled. By default, all the existing projections are enabled.\nNone\n\n\nperiod\nlist\ndetermines how often the synaptic variables will be updated.\nNone\n\n\noffset\nfloat\ndetermines the offset at which the synaptic variables will be updated relative to the current time.\nNone",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "enable_learning"
    ]
  },
  {
    "objectID": "reference/enable_learning.html#parameters",
    "href": "reference/enable_learning.html#parameters",
    "title": "enable_learning",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nprojections\nlist\nthe projections whose learning should be enabled. By default, all the existing projections are enabled.\nNone\n\n\nperiod\nlist\ndetermines how often the synaptic variables will be updated.\nNone\n\n\noffset\nfloat\ndetermines the offset at which the synaptic variables will be updated relative to the current time.\nNone",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "enable_learning"
    ]
  },
  {
    "objectID": "reference/load_parameters.html",
    "href": "reference/load_parameters.html",
    "title": "load_parameters",
    "section": "",
    "text": "core.IO.load_parameters(filename, global_only=True, verbose=False, net_id=0)\nLoads the global parameters of a network (flag population for neurons, projection for synapses) from a JSON file.\nIt is advised to generate the JSON file first with save_parameters() and later edit it manually.\nA strong restriction is that population/projection names cannot change between saving and loading. By default, they take names such as pop0 or proj2, we advise setting explicitly a name in their constructor for readability.\nIf you add a parameter name to the JSON file but it does not exist in te neuron/synapse, it will be silently skipped. Enable verbose=True to see which parameters are effectively changed.\nIf you set global_only to True, you will be able to set values for non-global parameters (e.g. synapse-specific), but a single value will be loaded for all. The JSON file cannot contain arrays.\nIf you want to save/load the value of variables after a simulation, please refer to save() or load().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\npath to the JSON file.\nrequired\n\n\nglobal_only\nbool\nTrue if only global parameters (flags population and projection) should be loaded, the other values are ignored. (default: True)\nTrue\n\n\nverbose\nbool\nTrue if the old and new values of the parameters should be printed (default: False).\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\n\na dictionary of additional parameters not related to populations or projections (keyword network in the JSON file).",
    "crumbs": [
      "Reference",
      "**IO**",
      "load_parameters"
    ]
  },
  {
    "objectID": "reference/load_parameters.html#parameters",
    "href": "reference/load_parameters.html#parameters",
    "title": "load_parameters",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\npath to the JSON file.\nrequired\n\n\nglobal_only\nbool\nTrue if only global parameters (flags population and projection) should be loaded, the other values are ignored. (default: True)\nTrue\n\n\nverbose\nbool\nTrue if the old and new values of the parameters should be printed (default: False).\nFalse",
    "crumbs": [
      "Reference",
      "**IO**",
      "load_parameters"
    ]
  },
  {
    "objectID": "reference/load_parameters.html#returns",
    "href": "reference/load_parameters.html#returns",
    "title": "load_parameters",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\n\na dictionary of additional parameters not related to populations or projections (keyword network in the JSON file).",
    "crumbs": [
      "Reference",
      "**IO**",
      "load_parameters"
    ]
  },
  {
    "objectID": "reference/balloon_maith2021.html",
    "href": "reference/balloon_maith2021.html",
    "title": "balloon_maith2021",
    "section": "",
    "text": "balloon_maith2021\nextensions.bold.PredefinedModels.balloon_maith2021(self)\nThe balloon model as used in Maith et al. (2021).",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_maith2021"
    ]
  },
  {
    "objectID": "reference/Network.html",
    "href": "reference/Network.html",
    "title": "Network",
    "section": "",
    "text": "core.Network.Network(self, everything=False)\nA network gathers already defined populations, projections and monitors in order to run them independently.\nThis is particularly useful when varying single parameters of a network and comparing the results (see the parallel_run() method).\nOnly objects declared before the creation of the network can be used. Global methods such as simulate() must be used on the network object. The objects must be accessed through the get() method, as the original ones will not be part of the network (a copy is made).\nEach network must be individually compiled, but it does not matter if the original objects were already compiled.\nWhen passing everything=True to the constructor, all populations/projections/monitors already defined at the global level will be added to the network.\nIf not, you can select which object will be added to network with the add() method.\nExample with everything=True:\npop = ann.Population(100, Izhikevich)\nproj = ann.Projection(pop, pop, 'exc')\nproj.connect_all_to_all(1.0)\nm = ann.Monitor(pop, 'spike')\n\nann.compile() # Optional\n\nnet = ann.Network(everything=True)\nnet.get(pop).a = 0.02\nnet.compile()\nnet.simulate(1000.)\n\nnet2 = ann.Network(everything=True)\nnet2.get(pop).a = 0.05\nnet2.compile()\nnet2.simulate(1000.)\n\nt, n = net.get(m).raster_plot()\nt2, n2 = net2.get(m).raster_plot()\nExample with everything=False (the default):\npop = ann.Population(100, Izhikevich)\nproj1 = ann.Projection(pop, pop, 'exc')\nproj1.connect_all_to_all(1.0)\nproj2 = ann.Projection(pop, pop, 'exc')\nproj2.connect_all_to_all(2.0)\nm = ann.Monitor(pop, 'spike')\n\nnet = ann.Network()\nnet.add([pop, proj1, m])\nnet.compile()\nnet.simulate(1000.)\n\nnet2 = ann.Network()\nnet2.add([pop, proj2, m])\nnet2.compile()\nnet2.simulate(1000.)\n\nt, n = net.get(m).raster_plot()\nt2, n2 = net2.get(m).raster_plot()\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\neverything\nbool\ndefines if all existing populations and projections should be automatically added (default: False).\nFalse\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd\nAdds a Population, Projection or Monitor to the network.\n\n\ncompile\nCompiles the network.\n\n\ndisable_learning\nDisables learning for all projections.\n\n\nenable_learning\nEnables learning for all projections.\n\n\nget\nReturns the local Population, Projection or Monitor corresponding to the provided argument.\n\n\nget_current_step\nReturns the current simulation step.\n\n\nget_population\nReturns the population with the given name.\n\n\nget_populations\nReturns a list of all declared populations in this network.\n\n\nget_projection\nReturns the projection with the given name.\n\n\nget_projections\nGet a list of declared projections for the current network. By default,\n\n\nget_time\nReturns the current time in ms.\n\n\nload\nLoads a saved state of the current network by calling ANNarchy.core.IO.load().\n\n\nreset\nReinitialises the network to its state before the call to compile.\n\n\nsave\nSaves the current network by calling ANNarchy.core.IO.save().\n\n\nset_current_step\nSets the current simulation step.\n\n\nset_seed\nSets the seed of the random number generators for this network.\n\n\nset_time\nSets the current time in ms.\n\n\nsimulate\nRuns the network for the given duration in milliseconds.\n\n\nsimulate_until\nRuns the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped.\n\n\nstep\nPerforms a single simulation step (duration = dt).\n\n\n\n\n\ncore.Network.Network.add(objects)\nAdds a Population, Projection or Monitor to the network.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobjects\nlist\nA single object or a list to add to the network.\nrequired\n\n\n\n\n\n\n\ncore.Network.Network.compile(directory='annarchy', clean=False, compiler='default', compiler_flags='default', add_sources='', extra_libs='', cuda_config={'device': 0}, annarchy_json='', silent=False, debug_build=False, profile_enabled=False)\nCompiles the network.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndirectory\nstr\nname of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: “annarchy/”.\n'annarchy'\n\n\nclean\nbool\nboolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False).\nFalse\n\n\ncompiler\nstr\nC++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++].\n'default'\n\n\ncompiler_flags\nlist[str]\nplatform-specific flags to pass to the compiler. Default: “-march=native -O2”. Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended.\n'default'\n\n\ncuda_config\ndict\ndictionary defining the CUDA configuration for each population and projection.\n{'device': 0}\n\n\nannarchy_json\nstr\ncompiler flags etc are stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location.\n''\n\n\nsilent\nbool\ndefines if the “Compiling… OK” should be printed.\nFalse\n\n\n\n\n\n\n\ncore.Network.Network.disable_learning(projections=None)\nDisables learning for all projections.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojections\nlist\nthe projections whose learning should be disabled. By default, all the existing projections are disabled.\nNone\n\n\n\n\n\n\n\ncore.Network.Network.enable_learning(projections=None, period=None, offset=None)\nEnables learning for all projections.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojections\nlist\nthe projections whose learning should be enabled. By default, all the existing projections are disabled.\nNone\n\n\n\n\n\n\n\ncore.Network.Network.get(obj)\nReturns the local Population, Projection or Monitor corresponding to the provided argument.\nobj is for example a top-level poopulation, while net.get(pop)is the copy local to the network.\nExample:\npop = ann.Population(100, Izhikevich)\nnet = ann.Network()\nnet.add(pop)\nnet.compile()\n\nprint(net.get(pop).v)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobj\n\nA single object or a list of objects.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\n\nThe corresponding object or list of objects.\n\n\n\n\n\n\n\ncore.Network.Network.get_current_step()\nReturns the current simulation step.\n\n\n\ncore.Network.Network.get_population(name)\nReturns the population with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the population\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nANNarchy.core.Population.Population\nThe requested Population object if existing, None otherwise.\n\n\n\n\n\n\n\ncore.Network.Network.get_populations()\nReturns a list of all declared populations in this network.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[ANNarchy.core.Population.Population]\nthe list of all populations in the network.\n\n\n\n\n\n\n\ncore.Network.Network.get_projection(name)\nReturns the projection with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the projection\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nANNarchy.core.Projection.Projection\nThe requested Projection object if existing, None otherwise.\n\n\n\n\n\n\n\ncore.Network.Network.get_projections(post=None, pre=None, target=None, suppress_error=False)\nGet a list of declared projections for the current network. By default, the method returns all connections within the network.\nBy setting the arguments, post, pre and target one can select a subset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npost\n\nall returned projections should have this population as post.\nNone\n\n\npre\n\nall returned projections should have this population as pre.\nNone\n\n\ntarget\n\nall returned projections should have this target.\nNone\n\n\nsuppress_error\n\nby default, ANNarchy throws an error if the list of assigned projections is empty. If this flag is set to True, the error message is suppressed.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[ANNarchy.core.Projection.Projection]\nthe list of all assigned projections in this network or a subset according to the arguments.\n\n\n\n\n\n\n\ncore.Network.Network.get_time()\nReturns the current time in ms.\n\n\n\ncore.Network.Network.load(filename, populations=True, projections=True, pickle_encoding=None)\nLoads a saved state of the current network by calling ANNarchy.core.IO.load().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfilename, may contain relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be saved (by default True)\nTrue\n\n\nprojections\nbool\nif True, projection data will be saved (by default True)\nTrue\n\n\npickle_encoding\nstr\noptional parameter provided to the pickle.load() method. If set to None the default is used.\nNone\n\n\n\n\n\n\n\ncore.Network.Network.reset(populations=True, projections=False, monitors=True, synapses=False)\nReinitialises the network to its state before the call to compile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npopulations\nbool\nif True (default), the neural parameters and variables will be reset to their initial value.\nTrue\n\n\nprojections\nbool\nif True, the synaptic parameters and variables (except the connections) will be reset (default=False).\nFalse\n\n\nsynapses\nbool\nif True, the synaptic weights will be erased and recreated (default=False).\nFalse\n\n\n\n\n\n\n\ncore.Network.Network.save(filename, populations=True, projections=True)\nSaves the current network by calling ANNarchy.core.IO.save().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfilename, may contain relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be saved (by default True)\nTrue\n\n\nprojections\nbool\nif True, projection data will be saved (by default True)\nTrue\n\n\n\n\n\n\n\ncore.Network.Network.set_current_step(t)\nSets the current simulation step.\nWarning: can be dangerous for some spiking models.\n\n\n\ncore.Network.Network.set_seed(seed, use_seed_seq=True)\nSets the seed of the random number generators for this network.\n\n\n\ncore.Network.Network.set_time(t, net_id=0)\nSets the current time in ms.\nWarning: can be dangerous for some spiking models.\n\n\n\ncore.Network.Network.simulate(duration, measure_time=False)\nRuns the network for the given duration in milliseconds.\nThe number of simulation steps is computed relative to the discretization step dt declared in setup() (default: 1ms):\nnet.simulate(1000.0)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nduration\nfloat\nthe duration in milliseconds.\nrequired\n\n\nmeasure_time\nbool\ndefines whether the simulation time should be printed (default=False).\nFalse\n\n\n\n\n\n\n\ncore.Network.Network.simulate_until(max_duration, population, operator='and', measure_time=False)\nRuns the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped.\nOne can specify several populations. If the stop condition is true for any of the populations, the simulation will stop (‘or’ function).\nExample:\npop1 = ann.Population( ..., stop_condition = \"r &gt; 1.0 : any\")\n...\nnet.compile()\nnet.simulate_until(max_duration=1000.0. population=pop1)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmax_duration\nfloat\nthe maximum duration of the simulation in milliseconds.\nrequired\n\n\npopulation\nANNarchy.core.Population.Population\nthe (list of) population whose stop_condition should be checked to stop the simulation.\nrequired\n\n\noperator\nstr\noperator to be used (‘and’ or ‘or’) when multiple populations are provided (default: ‘and’).\n'and'\n\n\nmeasure_time\nbool\ndefines whether the simulation time should be printed (default=False).\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nthe actual duration of the simulation in milliseconds.\n\n\n\n\n\n\n\ncore.Network.Network.step()\nPerforms a single simulation step (duration = dt).",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Network"
    ]
  },
  {
    "objectID": "reference/Network.html#parameters",
    "href": "reference/Network.html#parameters",
    "title": "Network",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\neverything\nbool\ndefines if all existing populations and projections should be automatically added (default: False).\nFalse",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Network"
    ]
  },
  {
    "objectID": "reference/Network.html#methods",
    "href": "reference/Network.html#methods",
    "title": "Network",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nadd\nAdds a Population, Projection or Monitor to the network.\n\n\ncompile\nCompiles the network.\n\n\ndisable_learning\nDisables learning for all projections.\n\n\nenable_learning\nEnables learning for all projections.\n\n\nget\nReturns the local Population, Projection or Monitor corresponding to the provided argument.\n\n\nget_current_step\nReturns the current simulation step.\n\n\nget_population\nReturns the population with the given name.\n\n\nget_populations\nReturns a list of all declared populations in this network.\n\n\nget_projection\nReturns the projection with the given name.\n\n\nget_projections\nGet a list of declared projections for the current network. By default,\n\n\nget_time\nReturns the current time in ms.\n\n\nload\nLoads a saved state of the current network by calling ANNarchy.core.IO.load().\n\n\nreset\nReinitialises the network to its state before the call to compile.\n\n\nsave\nSaves the current network by calling ANNarchy.core.IO.save().\n\n\nset_current_step\nSets the current simulation step.\n\n\nset_seed\nSets the seed of the random number generators for this network.\n\n\nset_time\nSets the current time in ms.\n\n\nsimulate\nRuns the network for the given duration in milliseconds.\n\n\nsimulate_until\nRuns the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped.\n\n\nstep\nPerforms a single simulation step (duration = dt).\n\n\n\n\n\ncore.Network.Network.add(objects)\nAdds a Population, Projection or Monitor to the network.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobjects\nlist\nA single object or a list to add to the network.\nrequired\n\n\n\n\n\n\n\ncore.Network.Network.compile(directory='annarchy', clean=False, compiler='default', compiler_flags='default', add_sources='', extra_libs='', cuda_config={'device': 0}, annarchy_json='', silent=False, debug_build=False, profile_enabled=False)\nCompiles the network.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndirectory\nstr\nname of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: “annarchy/”.\n'annarchy'\n\n\nclean\nbool\nboolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False).\nFalse\n\n\ncompiler\nstr\nC++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++].\n'default'\n\n\ncompiler_flags\nlist[str]\nplatform-specific flags to pass to the compiler. Default: “-march=native -O2”. Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended.\n'default'\n\n\ncuda_config\ndict\ndictionary defining the CUDA configuration for each population and projection.\n{'device': 0}\n\n\nannarchy_json\nstr\ncompiler flags etc are stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location.\n''\n\n\nsilent\nbool\ndefines if the “Compiling… OK” should be printed.\nFalse\n\n\n\n\n\n\n\ncore.Network.Network.disable_learning(projections=None)\nDisables learning for all projections.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojections\nlist\nthe projections whose learning should be disabled. By default, all the existing projections are disabled.\nNone\n\n\n\n\n\n\n\ncore.Network.Network.enable_learning(projections=None, period=None, offset=None)\nEnables learning for all projections.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojections\nlist\nthe projections whose learning should be enabled. By default, all the existing projections are disabled.\nNone\n\n\n\n\n\n\n\ncore.Network.Network.get(obj)\nReturns the local Population, Projection or Monitor corresponding to the provided argument.\nobj is for example a top-level poopulation, while net.get(pop)is the copy local to the network.\nExample:\npop = ann.Population(100, Izhikevich)\nnet = ann.Network()\nnet.add(pop)\nnet.compile()\n\nprint(net.get(pop).v)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobj\n\nA single object or a list of objects.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\n\nThe corresponding object or list of objects.\n\n\n\n\n\n\n\ncore.Network.Network.get_current_step()\nReturns the current simulation step.\n\n\n\ncore.Network.Network.get_population(name)\nReturns the population with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the population\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nANNarchy.core.Population.Population\nThe requested Population object if existing, None otherwise.\n\n\n\n\n\n\n\ncore.Network.Network.get_populations()\nReturns a list of all declared populations in this network.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[ANNarchy.core.Population.Population]\nthe list of all populations in the network.\n\n\n\n\n\n\n\ncore.Network.Network.get_projection(name)\nReturns the projection with the given name.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nname of the projection\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nANNarchy.core.Projection.Projection\nThe requested Projection object if existing, None otherwise.\n\n\n\n\n\n\n\ncore.Network.Network.get_projections(post=None, pre=None, target=None, suppress_error=False)\nGet a list of declared projections for the current network. By default, the method returns all connections within the network.\nBy setting the arguments, post, pre and target one can select a subset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npost\n\nall returned projections should have this population as post.\nNone\n\n\npre\n\nall returned projections should have this population as pre.\nNone\n\n\ntarget\n\nall returned projections should have this target.\nNone\n\n\nsuppress_error\n\nby default, ANNarchy throws an error if the list of assigned projections is empty. If this flag is set to True, the error message is suppressed.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[ANNarchy.core.Projection.Projection]\nthe list of all assigned projections in this network or a subset according to the arguments.\n\n\n\n\n\n\n\ncore.Network.Network.get_time()\nReturns the current time in ms.\n\n\n\ncore.Network.Network.load(filename, populations=True, projections=True, pickle_encoding=None)\nLoads a saved state of the current network by calling ANNarchy.core.IO.load().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfilename, may contain relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be saved (by default True)\nTrue\n\n\nprojections\nbool\nif True, projection data will be saved (by default True)\nTrue\n\n\npickle_encoding\nstr\noptional parameter provided to the pickle.load() method. If set to None the default is used.\nNone\n\n\n\n\n\n\n\ncore.Network.Network.reset(populations=True, projections=False, monitors=True, synapses=False)\nReinitialises the network to its state before the call to compile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npopulations\nbool\nif True (default), the neural parameters and variables will be reset to their initial value.\nTrue\n\n\nprojections\nbool\nif True, the synaptic parameters and variables (except the connections) will be reset (default=False).\nFalse\n\n\nsynapses\nbool\nif True, the synaptic weights will be erased and recreated (default=False).\nFalse\n\n\n\n\n\n\n\ncore.Network.Network.save(filename, populations=True, projections=True)\nSaves the current network by calling ANNarchy.core.IO.save().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfilename, may contain relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be saved (by default True)\nTrue\n\n\nprojections\nbool\nif True, projection data will be saved (by default True)\nTrue\n\n\n\n\n\n\n\ncore.Network.Network.set_current_step(t)\nSets the current simulation step.\nWarning: can be dangerous for some spiking models.\n\n\n\ncore.Network.Network.set_seed(seed, use_seed_seq=True)\nSets the seed of the random number generators for this network.\n\n\n\ncore.Network.Network.set_time(t, net_id=0)\nSets the current time in ms.\nWarning: can be dangerous for some spiking models.\n\n\n\ncore.Network.Network.simulate(duration, measure_time=False)\nRuns the network for the given duration in milliseconds.\nThe number of simulation steps is computed relative to the discretization step dt declared in setup() (default: 1ms):\nnet.simulate(1000.0)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nduration\nfloat\nthe duration in milliseconds.\nrequired\n\n\nmeasure_time\nbool\ndefines whether the simulation time should be printed (default=False).\nFalse\n\n\n\n\n\n\n\ncore.Network.Network.simulate_until(max_duration, population, operator='and', measure_time=False)\nRuns the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped.\nOne can specify several populations. If the stop condition is true for any of the populations, the simulation will stop (‘or’ function).\nExample:\npop1 = ann.Population( ..., stop_condition = \"r &gt; 1.0 : any\")\n...\nnet.compile()\nnet.simulate_until(max_duration=1000.0. population=pop1)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmax_duration\nfloat\nthe maximum duration of the simulation in milliseconds.\nrequired\n\n\npopulation\nANNarchy.core.Population.Population\nthe (list of) population whose stop_condition should be checked to stop the simulation.\nrequired\n\n\noperator\nstr\noperator to be used (‘and’ or ‘or’) when multiple populations are provided (default: ‘and’).\n'and'\n\n\nmeasure_time\nbool\ndefines whether the simulation time should be printed (default=False).\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nthe actual duration of the simulation in milliseconds.\n\n\n\n\n\n\n\ncore.Network.Network.step()\nPerforms a single simulation step (duration = dt).",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Network"
    ]
  },
  {
    "objectID": "reference/Neuron.html",
    "href": "reference/Neuron.html",
    "title": "Neuron",
    "section": "",
    "text": "core.Neuron.Neuron(self, parameters='', equations='', spike=None, axon_spike=None, reset=None, axon_reset=None, refractory=None, functions=None, name='', description='', extra_values={})\nBase class to define a neuron.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameters\nstr\nparameters of the neuron and their initial value.\n''\n\n\nequations\nstr\nequations defining the temporal evolution of variables.\n''\n\n\nfunctions\nstr\nadditional functions used in the variables’ equations.\nNone\n\n\nspike\nstr\ncondition to emit a spike (only for spiking neurons).\nNone\n\n\naxon_spike\nstr\ncondition to emit an axonal spike (only for spiking neurons and optional). The axonal spike can appear additional to the spike and is independent from refractoriness of a neuron.\nNone\n\n\nreset\nstr\nchanges to the variables after a spike (only for spiking neurons).\nNone\n\n\naxon_reset\nstr\nchanges to the variables after an axonal spike (only for spiking neurons).\nNone\n\n\nrefractory\nstr\nrefractory period of a neuron after a spike (only for spiking neurons).\nNone\n\n\nname\nstr\nname of the neuron type (used for reporting only).\n''\n\n\ndescription\nstr\nshort description of the neuron type (used for reporting).\n''",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Neuron"
    ]
  },
  {
    "objectID": "reference/Neuron.html#parameters",
    "href": "reference/Neuron.html#parameters",
    "title": "Neuron",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nparameters\nstr\nparameters of the neuron and their initial value.\n''\n\n\nequations\nstr\nequations defining the temporal evolution of variables.\n''\n\n\nfunctions\nstr\nadditional functions used in the variables’ equations.\nNone\n\n\nspike\nstr\ncondition to emit a spike (only for spiking neurons).\nNone\n\n\naxon_spike\nstr\ncondition to emit an axonal spike (only for spiking neurons and optional). The axonal spike can appear additional to the spike and is independent from refractoriness of a neuron.\nNone\n\n\nreset\nstr\nchanges to the variables after a spike (only for spiking neurons).\nNone\n\n\naxon_reset\nstr\nchanges to the variables after an axonal spike (only for spiking neurons).\nNone\n\n\nrefractory\nstr\nrefractory period of a neuron after a spike (only for spiking neurons).\nNone\n\n\nname\nstr\nname of the neuron type (used for reporting only).\n''\n\n\ndescription\nstr\nshort description of the neuron type (used for reporting).\n''",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Neuron"
    ]
  },
  {
    "objectID": "reference/Synapse.html",
    "href": "reference/Synapse.html",
    "title": "Synapse",
    "section": "",
    "text": "core.Synapse.Synapse(self, parameters='', equations='', psp=None, operation='sum', pre_spike=None, post_spike=None, pre_axon_spike=None, functions=None, pruning=None, creating=None, name=None, description=None, extra_values={})\nBase class to define a synapse.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparameters\nstr\nparameters of the neuron and their initial value.\n''\n\n\nequations\nstr\nequations defining the temporal evolution of variables.\n''\n\n\npsp\nstr\ncontinuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r). Synaptic transmission in spiking synapses occurs in pre_spike.\nNone\n\n\noperation\nstr\noperation (sum, max, min, mean) performed by the post-synaptic neuron on the individual psp (rate-coded only, default=sum).\n'sum'\n\n\npre_spike\nstr\nupdating of variables when a pre-synaptic spike is received (spiking only).\nNone\n\n\npost_spike\nstr\nupdating of variables when a post-synaptic spike is emitted (spiking only).\nNone\n\n\npre_axon_spike\nstr\nupdating of variables when an axonal spike was emitted (spiking only, default None). The usage of this arguments prevents the application of learning rules.\nNone\n\n\nfunctions\nstr\nadditional functions used in the equations.\nNone\n\n\nname\nstr\nname of the synapse type (used for reporting only).\nNone\n\n\ndescription\nstr\nshort description of the synapse type (used for reporting).\nNone",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Synapse"
    ]
  },
  {
    "objectID": "reference/Synapse.html#parameters",
    "href": "reference/Synapse.html#parameters",
    "title": "Synapse",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nparameters\nstr\nparameters of the neuron and their initial value.\n''\n\n\nequations\nstr\nequations defining the temporal evolution of variables.\n''\n\n\npsp\nstr\ncontinuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r). Synaptic transmission in spiking synapses occurs in pre_spike.\nNone\n\n\noperation\nstr\noperation (sum, max, min, mean) performed by the post-synaptic neuron on the individual psp (rate-coded only, default=sum).\n'sum'\n\n\npre_spike\nstr\nupdating of variables when a pre-synaptic spike is received (spiking only).\nNone\n\n\npost_spike\nstr\nupdating of variables when a post-synaptic spike is emitted (spiking only).\nNone\n\n\npre_axon_spike\nstr\nupdating of variables when an axonal spike was emitted (spiking only, default None). The usage of this arguments prevents the application of learning rules.\nNone\n\n\nfunctions\nstr\nadditional functions used in the equations.\nNone\n\n\nname\nstr\nname of the synapse type (used for reporting only).\nNone\n\n\ndescription\nstr\nshort description of the synapse type (used for reporting).\nNone",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Synapse"
    ]
  },
  {
    "objectID": "reference/simulate_until.html",
    "href": "reference/simulate_until.html",
    "title": "simulate_until",
    "section": "",
    "text": "core.Simulate.simulate_until(max_duration, population, operator='and', measure_time=False, net_id=0)\nRuns the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped.\nOne can specify several populations. If the stop condition is true for any of the populations, the simulation will stop (‘or’ function).\nExample:\npop1 = Population( ..., stop_condition = \"r &gt; 1.0 : any\")\ncompile()\nsimulate_until(max_duration=1000.0, population=pop1)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmax_duration\nfloat\nMaximum duration of the simulation in milliseconds.\nrequired\n\n\npopulation\nANNarchy.core.Population.Population | list[ANNarchy.core.Population.Population]\n(list of) population(s) whose stop_condition should be checked to stop the simulation.\nrequired\n\n\noperator\n\nOperator to be used (‘and’ or ‘or’) when multiple populations are provided (default: ‘and’).\n'and'\n\n\nmeasure_time\nbool\nDefines whether the simulation time should be printed (default=False).\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\n\nthe actual duration of the simulation in milliseconds.",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "simulate_until"
    ]
  },
  {
    "objectID": "reference/simulate_until.html#parameters",
    "href": "reference/simulate_until.html#parameters",
    "title": "simulate_until",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmax_duration\nfloat\nMaximum duration of the simulation in milliseconds.\nrequired\n\n\npopulation\nANNarchy.core.Population.Population | list[ANNarchy.core.Population.Population]\n(list of) population(s) whose stop_condition should be checked to stop the simulation.\nrequired\n\n\noperator\n\nOperator to be used (‘and’ or ‘or’) when multiple populations are provided (default: ‘and’).\n'and'\n\n\nmeasure_time\nbool\nDefines whether the simulation time should be printed (default=False).\nFalse",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "simulate_until"
    ]
  },
  {
    "objectID": "reference/simulate_until.html#returns",
    "href": "reference/simulate_until.html#returns",
    "title": "simulate_until",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\n\nthe actual duration of the simulation in milliseconds.",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "simulate_until"
    ]
  },
  {
    "objectID": "reference/balloon_RN.html",
    "href": "reference/balloon_RN.html",
    "title": "balloon_RN",
    "section": "",
    "text": "extensions.bold.PredefinedModels.balloon_RN(self, phi=1.0, kappa=1 / 1.54, gamma=1 / 2.46, E_0=0.34, tau=0.98, alpha=0.33, V_0=0.02, v_0=40.3, TE=40 / 1000.0, epsilon=1.43, r_0=25)\nA balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\nEquivalent code:\nballoon_RN = BoldModel(\n    parameters = '''\n        second    = 1000.0\n        phi       = 1.0\n        kappa     = 1/1.54\n        gamma     = 1/2.46\n        E_0       = 0.34\n        tau       = 0.98\n        alpha     = 0.33\n        V_0       = 0.02\n        v_0       = 40.3\n        TE        = 40/1000.\n        epsilon   = 1.43\n        r_0       = 25.\n    ''',\n    equations = '''\n        # Single input\n        I_CBF          = sum(I_CBF)                                                : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))  : init=0\n    ''',\n    inputs=\"I_CBF\",\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43\n\n\nr_0\n\nslope of the relation between the intravascular relaxation rate and oxygen saturation\n25",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_RN"
    ]
  },
  {
    "objectID": "reference/balloon_RN.html#parameters",
    "href": "reference/balloon_RN.html#parameters",
    "title": "balloon_RN",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43\n\n\nr_0\n\nslope of the relation between the intravascular relaxation rate and oxygen saturation\n25",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_RN"
    ]
  },
  {
    "objectID": "reference/Copy.html",
    "href": "reference/Copy.html",
    "title": "Copy",
    "section": "",
    "text": "extensions.convolution.Copy.Copy(self, pre, post, target, psp='pre.r * w', operation='sum', name=None, copied=False)\nCreates a virtual projection reusing the weights and delays of an already-defined projection.\nAlthough the original projection can be learnable, this one can not. Changes in the original weights will be reflected in this projection. The only possible modifications are psp and operation.\nThe pre- and post-synaptic populations of both projections must have the same geometry.\nExample:\nproj = ann.Projection(pop1, pop2, \"exc\")\nproj.connect_fixed_probability(0.1, 0.5)\n\ncopy_proj = Copy(pop1, pop3, \"exc\")\ncopy_proj.connect_copy(proj)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\n\npre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\n\npost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\n\ntype of the connection\nrequired\n\n\npsp\n\ncontinuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r).\n'pre.r * w'\n\n\noperation\n\noperation (sum, max, min, mean) performed by the kernel (default: sum).\n'sum'\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nconnect_copy\n\n\n\nconnectivity_matrix\nNot available.\n\n\ngenerate_omp\nCode generation of CopyProjection object for the openMP paradigm.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nsave\nNot available.\n\n\nsave_connectivity\nNot available.\n\n\n\n\n\nextensions.convolution.Copy.Copy.connect_copy(projection)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojection\n\nExisting projection to copy.\nrequired\n\n\n\n\n\n\n\nextensions.convolution.Copy.Copy.connectivity_matrix(fill=0.0)\nNot available.\n\n\n\nextensions.convolution.Copy.Copy.generate_omp()\nCode generation of CopyProjection object for the openMP paradigm.\n\n\n\nextensions.convolution.Copy.Copy.load(filename)\nNot available.\n\n\n\nextensions.convolution.Copy.Copy.receptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nextensions.convolution.Copy.Copy.save(filename)\nNot available.\n\n\n\nextensions.convolution.Copy.Copy.save_connectivity(filename)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Copy"
    ]
  },
  {
    "objectID": "reference/Copy.html#parameters",
    "href": "reference/Copy.html#parameters",
    "title": "Copy",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\n\npre-synaptic population (either its name or a Population object).\nrequired\n\n\npost\n\npost-synaptic population (either its name or a Population object).\nrequired\n\n\ntarget\n\ntype of the connection\nrequired\n\n\npsp\n\ncontinuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r).\n'pre.r * w'\n\n\noperation\n\noperation (sum, max, min, mean) performed by the kernel (default: sum).\n'sum'",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Copy"
    ]
  },
  {
    "objectID": "reference/Copy.html#methods",
    "href": "reference/Copy.html#methods",
    "title": "Copy",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nconnect_copy\n\n\n\nconnectivity_matrix\nNot available.\n\n\ngenerate_omp\nCode generation of CopyProjection object for the openMP paradigm.\n\n\nload\nNot available.\n\n\nreceptive_fields\nNot available.\n\n\nsave\nNot available.\n\n\nsave_connectivity\nNot available.\n\n\n\n\n\nextensions.convolution.Copy.Copy.connect_copy(projection)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprojection\n\nExisting projection to copy.\nrequired\n\n\n\n\n\n\n\nextensions.convolution.Copy.Copy.connectivity_matrix(fill=0.0)\nNot available.\n\n\n\nextensions.convolution.Copy.Copy.generate_omp()\nCode generation of CopyProjection object for the openMP paradigm.\n\n\n\nextensions.convolution.Copy.Copy.load(filename)\nNot available.\n\n\n\nextensions.convolution.Copy.Copy.receptive_fields(variable='w', in_post_geometry=True)\nNot available.\n\n\n\nextensions.convolution.Copy.Copy.save(filename)\nNot available.\n\n\n\nextensions.convolution.Copy.Copy.save_connectivity(filename)\nNot available.",
    "crumbs": [
      "Reference",
      "**Convolution**",
      "Copy"
    ]
  },
  {
    "objectID": "reference/clear_all_callbacks.html",
    "href": "reference/clear_all_callbacks.html",
    "title": "clear_all_callbacks",
    "section": "",
    "text": "clear_all_callbacks\ncore.Simulate.clear_all_callbacks(net_id=0)\nClears the list of declared callbacks for the network.\nCannot be undone!",
    "crumbs": [
      "Reference",
      "**Callbacks**",
      "clear_all_callbacks"
    ]
  },
  {
    "objectID": "reference/save.html",
    "href": "reference/save.html",
    "title": "save",
    "section": "",
    "text": "core.IO.save(filename, populations=True, projections=True, net_id=0)\nSave the current network state (parameters and variables) to a file.\n\nIf the extension is ‘.npz’, the data will be saved and compressed using np.savez_compressed (recommended).\nIf the extension is ‘.mat’, the data will be saved as a Matlab 7.2 file. Scipy must be installed.\nIf the extension ends with ‘.gz’, the data will be pickled into a binary file and compressed using gzip.\nOtherwise, the data will be pickled into a simple binary text file using cPickle.\n\nWarning: The ‘.mat’ data will not be loadable by ANNarchy, it is only for external analysis purpose.\nExample:\nann.save('results/init.npz')\n\nann.save('results/init.data')\n\nann.save('results/init.txt.gz')\n\nann.save('1000_trials.mat')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfilename, may contain relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be saved (by default True)\nTrue\n\n\nprojections\nbool\nif True, projection data will be saved (by default True)\nTrue",
    "crumbs": [
      "Reference",
      "**IO**",
      "save"
    ]
  },
  {
    "objectID": "reference/save.html#parameters",
    "href": "reference/save.html#parameters",
    "title": "save",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nfilename, may contain relative or absolute path.\nrequired\n\n\npopulations\nbool\nif True, population data will be saved (by default True)\nTrue\n\n\nprojections\nbool\nif True, projection data will be saved (by default True)\nTrue",
    "crumbs": [
      "Reference",
      "**IO**",
      "save"
    ]
  },
  {
    "objectID": "reference/report.html",
    "href": "reference/report.html",
    "title": "report",
    "section": "",
    "text": "parser.report.Report.report(filename='./report.tex', standalone=True, gather_subprojections=False, title=None, author=None, date=None, net_id=0)\nGenerates a report describing the network.\nIf the filename ends with .tex, the TeX file is generated according to:\n\nNordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8): e1000456.\n\nIf the filename ends with .md, a (more complete) Markdown file is generated, which can be converted to pdf or html by pandoc::\npandoc report.md -o report.pdf\npandoc report.md -o report.html\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nname of the file where the report will be written.\n'./report.tex'\n\n\nstandalone\nbool\ntells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown.\nTrue\n\n\ngather_subprojections\nbool\nif a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary.\nFalse\n\n\ntitle\nstr\ntitle of the document (Markdown only).\nNone\n\n\nauthor\nstr\nauthor of the document (Markdown only).\nNone\n\n\ndate\nstr\ndate of the document (Markdown only).\nNone",
    "crumbs": [
      "Reference",
      "**Utilities**",
      "report"
    ]
  },
  {
    "objectID": "reference/report.html#parameters",
    "href": "reference/report.html#parameters",
    "title": "report",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nname of the file where the report will be written.\n'./report.tex'\n\n\nstandalone\nbool\ntells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown.\nTrue\n\n\ngather_subprojections\nbool\nif a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary.\nFalse\n\n\ntitle\nstr\ntitle of the document (Markdown only).\nNone\n\n\nauthor\nstr\nauthor of the document (Markdown only).\nNone\n\n\ndate\nstr\ndate of the document (Markdown only).\nNone",
    "crumbs": [
      "Reference",
      "**Utilities**",
      "report"
    ]
  },
  {
    "objectID": "reference/balloon_RL.html",
    "href": "reference/balloon_RL.html",
    "title": "balloon_RL",
    "section": "",
    "text": "extensions.bold.PredefinedModels.balloon_RL(self, phi=1.0, kappa=1 / 1.54, gamma=1 / 2.46, E_0=0.34, tau=0.98, alpha=0.33, V_0=0.02, v_0=40.3, TE=40 / 1000.0, epsilon=1.43, r_0=25)\nA balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007).\nEquivalent code:\nballoon_RL = BoldModel(\n    parameters = '''\n        second    = 1000.0\n        phi       = 1.0\n        kappa     = 1/1.54\n        gamma     = 1/2.46\n        E_0       = 0.34\n        tau       = 0.98\n        alpha     = 0.33\n        V_0       = 0.02\n        v_0       = 40.3\n        TE        = 40/1000.\n        epsilon   = 1.43\n        r_0       = 25.\n    ''',\n    equations = '''\n        # Single input\n        I_CBF          = sum(I_CBF)                                                : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Linear equation\n        BOLD           = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v))         : init=0\n    ''',\n    inputs=\"I_CBF\",\n)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43\n\n\nr_0\n\nslope of the relation between the intravascular relaxation rate and oxygen saturation\n25",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_RL"
    ]
  },
  {
    "objectID": "reference/balloon_RL.html#parameters",
    "href": "reference/balloon_RL.html#parameters",
    "title": "balloon_RL",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nphi\n\ninput coefficient\n1.0\n\n\nkappa\n\nsignal decay\n1 / 1.54\n\n\ngamma\n\nfeedback regulation\n1 / 2.46\n\n\nE_0\n\noxygen extraction fraction at rest\n0.34\n\n\ntau\n\ntime constant (in s!)\n0.98\n\n\nalpha\n\nvessel stiffness\n0.33\n\n\nV_0\n\nresting venous blood volume fraction\n0.02\n\n\nv_0\n\nfrequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n40.3\n\n\nTE\n\necho time\n40 / 1000.0\n\n\nepsilon\n\nratio of intra- and extravascular signal\n1.43\n\n\nr_0\n\nslope of the relation between the intravascular relaxation rate and oxygen saturation\n25",
    "crumbs": [
      "Reference",
      "**BOLD monitoring**",
      "balloon_RL"
    ]
  },
  {
    "objectID": "reference/get_constant.html",
    "href": "reference/get_constant.html",
    "title": "get_constant",
    "section": "",
    "text": "get_constant\ncore.Global.get_constant(name, net_id=0)\nReturns the Constant object with the given name, None otherwise.",
    "crumbs": [
      "Reference",
      "**Functions and Constants**",
      "get_constant"
    ]
  },
  {
    "objectID": "reference/save_parameters.html",
    "href": "reference/save_parameters.html",
    "title": "save_parameters",
    "section": "",
    "text": "core.IO.save_parameters(filename, net_id=0)\nSaves the global parameters of a network (flag population for neurons, projection for synapses) to a JSON file.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\npath to the JSON file.\nrequired",
    "crumbs": [
      "Reference",
      "**IO**",
      "save_parameters"
    ]
  },
  {
    "objectID": "reference/save_parameters.html#parameters",
    "href": "reference/save_parameters.html#parameters",
    "title": "save_parameters",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\npath to the JSON file.\nrequired",
    "crumbs": [
      "Reference",
      "**IO**",
      "save_parameters"
    ]
  },
  {
    "objectID": "reference/Monitor.html",
    "href": "reference/Monitor.html",
    "title": "Monitor",
    "section": "",
    "text": "core.Monitor.Monitor(self, obj, variables=[], period=None, period_offset=None, start=True, net_id=0)\nMonitoring class allowing to record easily parameters or variables from Population, PopulationView, Dendrite or Projection objects.\nExample:\nm = Monitor(pop, ['g_exc', 'v', 'spike'], period=10.0)\nIt is also possible to record the sum of inputs to each neuron in a rate-coded population:\nm = Monitor(pop, ['sum(exc)', 'r'])\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobj\ntyping.Any\nobject to monitor. Must be a Population, PopulationView, Dendrite or Projection object.\nrequired\n\n\nvariables\nlist\nsingle variable name or list of variable names to record (default: []).\n[]\n\n\nperiod\nfloat\ndelay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View).\nNone\n\n\nperiod_offset\nfloat\ndetermine the moment in ms of recording within the period (default 0). Must be smaller than period.\nNone\n\n\nstart\nbool\ndefines if the recording should start immediately (default: True). If not, you should later start the recordings with the start() method.\nTrue\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nperiod\nPeriod of recording in ms\n\n\nperiod_offset\nShift of moment of time of recording in ms within a period\n\n\nvariables\nReturns a copy of the current variable list.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncoefficient_of_variation\nComputes the coefficient of variation for the recorded spikes in the population.\n\n\nget\nReturns the recorded variables as a Numpy array (first dimension is time, second is neuron index).\n\n\nhistogram\nReturns a histogram for the recorded spikes in the population.\n\n\ninter_spike_interval\nComputes the inter-spike interval for the recorded spikes in the population.\n\n\nmean_fr\nComputes the mean firing rate in the population during the recordings.\n\n\npause\nPauses the recordings.\n\n\npopulation_rate\nTakes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons.\n\n\nraster_plot\nReturns two numpy arrays representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.\n\n\nreset\nReset the monitor to its initial state.\n\n\nresume\nResumes the recordings.\n\n\nsize_in_bytes\nGet the size of allocated memory on C++ side. Please note, this is only valid if compile() was invoked.\n\n\nsmoothed_rate\nComputes the smoothed firing rate of the recorded spiking neurons.\n\n\nstart\nStarts recording the variables.\n\n\nstop\nStops the recording.\n\n\ntimes\nReturns the start and stop times (in ms) of the recorded variables.\n\n\n\n\n\ncore.Monitor.Monitor.coefficient_of_variation(spikes=None, ranks=None)\nComputes the coefficient of variation for the recorded spikes in the population.\n:ranks: a list of neurons that should be evaluated. By default (None), all neurons are evaluated.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\ncore.Monitor.Monitor.get(variables=None, keep=False, reshape=False, force_dict=False)\nReturns the recorded variables as a Numpy array (first dimension is time, second is neuron index).\nIf a single variable name is provided, the recorded values for this variable are directly returned. If a list is provided or the argument left empty, a dictionary with all recorded variables is returned.\nThe spike variable of a population will be returned as a dictionary of lists, where the spike times (in steps) for each recorded neurons are returned.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nstr | list[str]\n(list of) variables. By default, a dictionary with all variables is returned.\nNone\n\n\nkeep\nbool\ndefines if the content in memory for each variable should be kept (default: False).\nFalse\n\n\nreshape\nbool\ntransforms the second axis of the array to match the population’s geometry (default: False).\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict\nRecorded variables\n\n\n\n\n\n\n\ncore.Monitor.Monitor.histogram(spikes=None, bins=None, per_neuron=False, recording_window=None)\nReturns a histogram for the recorded spikes in the population.\nExample:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nhisto = m.histogram()\nplt.plot(histo)\nor:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nspikes = m.get('spike')\nhisto = m.histogram(spikes)\nplt.plot(histo)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\nbins\n\nthe bin size in ms (default: dt).\nNone\n\n\n\n\n\n\n\ncore.Monitor.Monitor.inter_spike_interval(spikes=None, ranks=None, per_neuron=False)\nComputes the inter-spike interval for the recorded spikes in the population.\n:ranks: a list of neurons that should be evaluated. By default (None), all neurons are evaluated. :per_neuron: if set to True, the computed inter-spike intervals are stored per neuron (analog to spikes), otherwise all values are stored in one huge vector (default: False).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\ncore.Monitor.Monitor.mean_fr(spikes=None)\nComputes the mean firing rate in the population during the recordings.\nExample:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nfr = m.mean_fr()\nor:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nspikes = m.get('spike')\nfr = m.mean_fr(spikes)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\ncore.Monitor.Monitor.pause()\nPauses the recordings.\n\n\n\ncore.Monitor.Monitor.population_rate(spikes=None, smooth=0.0)\nTakes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons.\nThis method is faster than calling smoothed_rate and then averaging.\nThe first axis is the neuron index, the second is time.\nIf spikes is left empty, get('spike') will be called. Beware: this erases the data from memory.\nExample:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nr = m.population_rate(smooth=100.)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike').\nNone\n\n\nsmooth\n\nsmoothing time constant. Default: 0.0 (no smoothing).\n0.0\n\n\n\n\n\n\n\ncore.Monitor.Monitor.raster_plot(spikes=None)\nReturns two numpy arrays representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.\nExample:\nm = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nt, n = m.raster_plot()\nplt.plot(t, n, '.')\nor:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nspikes = m.get('spike')\nt, n = m.raster_plot(spikes)\nplt.plot(t, n, '.')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple\nspike times and neuron indices as numpy arrays..\n\n\n\n\n\n\n\ncore.Monitor.Monitor.reset()\nReset the monitor to its initial state.\n\n\n\ncore.Monitor.Monitor.resume()\nResumes the recordings.\n\n\n\ncore.Monitor.Monitor.size_in_bytes()\nGet the size of allocated memory on C++ side. Please note, this is only valid if compile() was invoked.\n\n\n\n\n\nType\nDescription\n\n\n\n\nint\nsize in bytes of all allocated C++ data.\n\n\n\n\n\n\n\ncore.Monitor.Monitor.smoothed_rate(spikes=None, smooth=0.0)\nComputes the smoothed firing rate of the recorded spiking neurons.\nThe first axis is the neuron index, the second is time.\nExample:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nr = m.smoothed_rate(smooth=100.)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\nsmooth\n\nsmoothing time constant. Default: 0.0 (no smoothing).\n0.0\n\n\n\n\n\n\n\ncore.Monitor.Monitor.start(variables=None, period=None)\nStarts recording the variables.\nIt is called automatically after compile() if the flag start was not passed to the constructor.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nlist\nsingle variable name or list of variable names to start recording (default: the variables argument passed to the constructor).\nNone\n\n\nperiod\nfloat\ndelay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View).\nNone\n\n\n\n\n\n\n\ncore.Monitor.Monitor.stop()\nStops the recording.\nWarning: This will delete the content of the C++ object and all data not previously retrieved is lost.\n\n\n\ncore.Monitor.Monitor.times(variables=None)\nReturns the start and stop times (in ms) of the recorded variables.\nIt should only be called after a call to get(), so that it describes when the variables have been recorded.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nlist[str]\n(list of) variables. By default, the times for all variables is returned.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict\ndictionary of start and stop times.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Monitor"
    ]
  },
  {
    "objectID": "reference/Monitor.html#parameters",
    "href": "reference/Monitor.html#parameters",
    "title": "Monitor",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nobj\ntyping.Any\nobject to monitor. Must be a Population, PopulationView, Dendrite or Projection object.\nrequired\n\n\nvariables\nlist\nsingle variable name or list of variable names to record (default: []).\n[]\n\n\nperiod\nfloat\ndelay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View).\nNone\n\n\nperiod_offset\nfloat\ndetermine the moment in ms of recording within the period (default 0). Must be smaller than period.\nNone\n\n\nstart\nbool\ndefines if the recording should start immediately (default: True). If not, you should later start the recordings with the start() method.\nTrue",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Monitor"
    ]
  },
  {
    "objectID": "reference/Monitor.html#attributes",
    "href": "reference/Monitor.html#attributes",
    "title": "Monitor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nperiod\nPeriod of recording in ms\n\n\nperiod_offset\nShift of moment of time of recording in ms within a period\n\n\nvariables\nReturns a copy of the current variable list.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Monitor"
    ]
  },
  {
    "objectID": "reference/Monitor.html#methods",
    "href": "reference/Monitor.html#methods",
    "title": "Monitor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncoefficient_of_variation\nComputes the coefficient of variation for the recorded spikes in the population.\n\n\nget\nReturns the recorded variables as a Numpy array (first dimension is time, second is neuron index).\n\n\nhistogram\nReturns a histogram for the recorded spikes in the population.\n\n\ninter_spike_interval\nComputes the inter-spike interval for the recorded spikes in the population.\n\n\nmean_fr\nComputes the mean firing rate in the population during the recordings.\n\n\npause\nPauses the recordings.\n\n\npopulation_rate\nTakes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons.\n\n\nraster_plot\nReturns two numpy arrays representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.\n\n\nreset\nReset the monitor to its initial state.\n\n\nresume\nResumes the recordings.\n\n\nsize_in_bytes\nGet the size of allocated memory on C++ side. Please note, this is only valid if compile() was invoked.\n\n\nsmoothed_rate\nComputes the smoothed firing rate of the recorded spiking neurons.\n\n\nstart\nStarts recording the variables.\n\n\nstop\nStops the recording.\n\n\ntimes\nReturns the start and stop times (in ms) of the recorded variables.\n\n\n\n\n\ncore.Monitor.Monitor.coefficient_of_variation(spikes=None, ranks=None)\nComputes the coefficient of variation for the recorded spikes in the population.\n:ranks: a list of neurons that should be evaluated. By default (None), all neurons are evaluated.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\ncore.Monitor.Monitor.get(variables=None, keep=False, reshape=False, force_dict=False)\nReturns the recorded variables as a Numpy array (first dimension is time, second is neuron index).\nIf a single variable name is provided, the recorded values for this variable are directly returned. If a list is provided or the argument left empty, a dictionary with all recorded variables is returned.\nThe spike variable of a population will be returned as a dictionary of lists, where the spike times (in steps) for each recorded neurons are returned.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nstr | list[str]\n(list of) variables. By default, a dictionary with all variables is returned.\nNone\n\n\nkeep\nbool\ndefines if the content in memory for each variable should be kept (default: False).\nFalse\n\n\nreshape\nbool\ntransforms the second axis of the array to match the population’s geometry (default: False).\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict\nRecorded variables\n\n\n\n\n\n\n\ncore.Monitor.Monitor.histogram(spikes=None, bins=None, per_neuron=False, recording_window=None)\nReturns a histogram for the recorded spikes in the population.\nExample:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nhisto = m.histogram()\nplt.plot(histo)\nor:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nspikes = m.get('spike')\nhisto = m.histogram(spikes)\nplt.plot(histo)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\nbins\n\nthe bin size in ms (default: dt).\nNone\n\n\n\n\n\n\n\ncore.Monitor.Monitor.inter_spike_interval(spikes=None, ranks=None, per_neuron=False)\nComputes the inter-spike interval for the recorded spikes in the population.\n:ranks: a list of neurons that should be evaluated. By default (None), all neurons are evaluated. :per_neuron: if set to True, the computed inter-spike intervals are stored per neuron (analog to spikes), otherwise all values are stored in one huge vector (default: False).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\ncore.Monitor.Monitor.mean_fr(spikes=None)\nComputes the mean firing rate in the population during the recordings.\nExample:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nfr = m.mean_fr()\nor:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nspikes = m.get('spike')\nfr = m.mean_fr(spikes)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\ncore.Monitor.Monitor.pause()\nPauses the recordings.\n\n\n\ncore.Monitor.Monitor.population_rate(spikes=None, smooth=0.0)\nTakes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons.\nThis method is faster than calling smoothed_rate and then averaging.\nThe first axis is the neuron index, the second is time.\nIf spikes is left empty, get('spike') will be called. Beware: this erases the data from memory.\nExample:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nr = m.population_rate(smooth=100.)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike').\nNone\n\n\nsmooth\n\nsmoothing time constant. Default: 0.0 (no smoothing).\n0.0\n\n\n\n\n\n\n\ncore.Monitor.Monitor.raster_plot(spikes=None)\nReturns two numpy arrays representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.\nExample:\nm = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nt, n = m.raster_plot()\nplt.plot(t, n, '.')\nor:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nspikes = m.get('spike')\nt, n = m.raster_plot(spikes)\nplt.plot(t, n, '.')\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple\nspike times and neuron indices as numpy arrays..\n\n\n\n\n\n\n\ncore.Monitor.Monitor.reset()\nReset the monitor to its initial state.\n\n\n\ncore.Monitor.Monitor.resume()\nResumes the recordings.\n\n\n\ncore.Monitor.Monitor.size_in_bytes()\nGet the size of allocated memory on C++ side. Please note, this is only valid if compile() was invoked.\n\n\n\n\n\nType\nDescription\n\n\n\n\nint\nsize in bytes of all allocated C++ data.\n\n\n\n\n\n\n\ncore.Monitor.Monitor.smoothed_rate(spikes=None, smooth=0.0)\nComputes the smoothed firing rate of the recorded spiking neurons.\nThe first axis is the neuron index, the second is time.\nExample:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nr = m.smoothed_rate(smooth=100.)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\n\nthe dictionary of spikes returned by get('spike'). If left empty, get('spike') will be called. Beware: this erases the data from memory.\nNone\n\n\nsmooth\n\nsmoothing time constant. Default: 0.0 (no smoothing).\n0.0\n\n\n\n\n\n\n\ncore.Monitor.Monitor.start(variables=None, period=None)\nStarts recording the variables.\nIt is called automatically after compile() if the flag start was not passed to the constructor.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nlist\nsingle variable name or list of variable names to start recording (default: the variables argument passed to the constructor).\nNone\n\n\nperiod\nfloat\ndelay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View).\nNone\n\n\n\n\n\n\n\ncore.Monitor.Monitor.stop()\nStops the recording.\nWarning: This will delete the content of the C++ object and all data not previously retrieved is lost.\n\n\n\ncore.Monitor.Monitor.times(variables=None)\nReturns the start and stop times (in ms) of the recorded variables.\nIt should only be called after a call to get(), so that it describes when the variables have been recorded.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\nlist[str]\n(list of) variables. By default, the times for all variables is returned.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict\ndictionary of start and stop times.",
    "crumbs": [
      "Reference",
      "**Core components**",
      "Monitor"
    ]
  },
  {
    "objectID": "reference/set_current_step.html",
    "href": "reference/set_current_step.html",
    "title": "set_current_step",
    "section": "",
    "text": "set_current_step\ncore.Global.set_current_step(t, net_id=0)\nSets the current simulation step (integer).\nWarning: can be dangerous for some spiking models.",
    "crumbs": [
      "Reference",
      "**Simulation**",
      "set_current_step"
    ]
  },
  {
    "objectID": "reference/disable_callbacks.html",
    "href": "reference/disable_callbacks.html",
    "title": "disable_callbacks",
    "section": "",
    "text": "disable_callbacks\ncore.Simulate.disable_callbacks(net_id=0)\nDisables all callbacks for the network.",
    "crumbs": [
      "Reference",
      "**Callbacks**",
      "disable_callbacks"
    ]
  },
  {
    "objectID": "reference/setup.html",
    "href": "reference/setup.html",
    "title": "setup",
    "section": "",
    "text": "setup\nintern.ConfigManagement.setup(**keyValueArgs)\nThe setup function is used to configure ANNarchy simulation environment. It takes various optional arguments:\n\ndt: simulation step size (default: 1.0 ms).\nparadigm: parallel framework for code generation. Accepted values: “openmp” or “cuda” (default: “openmp”).\nmethod: default method to numerize ODEs. Default is the explicit forward Euler method (‘explicit’).\nsparse_matrix_format: the default matrix format for projections in ANNarchy (by default: List-In-List for CPUs and Compressed Sparse Row). Note that affects only the C++ data structures.\nsparse_matrix_storage_order: encodes whether the row in a connectivity matrix encodes pre-synaptic neurons (post_to_pre, default) or post-synaptic neurons (pre_to_post). Note that affects only the C++ data structures.\nprecision: default floating precision for variables in ANNarchy. Accepted values: “float” or “double” (default: “double”)\nonly_int_idx_type: if set to True (default) only signed integers are used to store pre-/post-synaptic ranks which was default until 4.7. If set to False, the index type used in a single projection is selected based on the size of the corresponding populations.\nnum_threads: number of treads used by openMP (overrides the environment variable OMP_NUM_THREADS when set, default = None).\nvisible_cores: allows a fine-grained control which cores are useable for the created threads (default = [] for no limitation). It can be used to limit created openMP threads to a physical socket.\nstructural_plasticity: allows synapses to be dynamically added/removed during the simulation (default: False).\nseed: the seed (integer) to be used in the random number generators (default = -1 is equivalent to time(NULL)).\n\nThe following parameters are mainly for debugging and profiling, and should be ignored by most users:\n\nverbose: shows details about compilation process on console (by default False). Additional some information of the network construction will be shown.\nsuppress_warnings: if True, warnings (e. g. from the mathematical parser) are suppressed.\nshow_time: if True, initialization times are shown. Attention: verbose should be set to True additionally.\ndisable_shared_library_time_offset: by default False. If set to True, the shared library generated by ANNarchy will not be extended by time offset.\n\nNote:\nThis function should be used before any other functions of ANNarchy (including importing a network definition), right after import ANNarchy:\nimport ANNarchyas ann\nann.setup(dt=1.0, method='midpoint', num_threads=2)",
    "crumbs": [
      "Reference",
      "**Configuration**",
      "setup"
    ]
  },
  {
    "objectID": "reference/population_rate.html",
    "href": "reference/population_rate.html",
    "title": "population_rate",
    "section": "",
    "text": "core.Monitor.population_rate(spikes, smooth=0.0)\nTakes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons.\nThis method is faster than calling smoothed_rate and then averaging.\nThe first axis is the neuron index, the second is time.\nExample:\nm = ann.Monitor(P[:1000], 'spike')\nann.simulate(1000.0)\nspikes = m.get('spike')\nr = population_rate(smooth=100.)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike').\nrequired\n\n\nsmooth\nfloat\nsmoothing time constant. Default: 0.0 (no smoothing).\n0.0",
    "crumbs": [
      "Reference",
      "**Plotting**",
      "population_rate"
    ]
  },
  {
    "objectID": "reference/population_rate.html#parameters",
    "href": "reference/population_rate.html#parameters",
    "title": "population_rate",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nspikes\ndict\nthe dictionary of spikes returned by get('spike').\nrequired\n\n\nsmooth\nfloat\nsmoothing time constant. Default: 0.0 (no smoothing).\n0.0",
    "crumbs": [
      "Reference",
      "**Plotting**",
      "population_rate"
    ]
  },
  {
    "objectID": "reference/Normal.html",
    "href": "reference/Normal.html",
    "title": "Normal",
    "section": "",
    "text": "core.Random.Normal(self, mu, sigma, min=None, max=None)\nNormal distribution.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmu\nfloat\nMean of the distribution.\nrequired\n\n\nsigma\nfloat\nStandard deviation of the distribution.\nrequired\n\n\nmin\nfloat\nMinimum value (default: unlimited).\nNone\n\n\nmax\nfloat\nMaximum value (default: unlimited).\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.Normal.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Normal"
    ]
  },
  {
    "objectID": "reference/Normal.html#parameters",
    "href": "reference/Normal.html#parameters",
    "title": "Normal",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmu\nfloat\nMean of the distribution.\nrequired\n\n\nsigma\nfloat\nStandard deviation of the distribution.\nrequired\n\n\nmin\nfloat\nMinimum value (default: unlimited).\nNone\n\n\nmax\nfloat\nMaximum value (default: unlimited).\nNone",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Normal"
    ]
  },
  {
    "objectID": "reference/Normal.html#methods",
    "href": "reference/Normal.html#methods",
    "title": "Normal",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_values\nReturns a Numpy array with the given shape.\n\n\n\n\n\ncore.Random.Normal.get_values(shape)\nReturns a Numpy array with the given shape.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nshape\ntuple\nShape of the array.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnumpy.numpy.ndarray\nArray.",
    "crumbs": [
      "Reference",
      "**Random Distributions**",
      "Normal"
    ]
  },
  {
    "objectID": "reference/EIF_cond_exp_isfa_ista.html",
    "href": "reference/EIF_cond_exp_isfa_ista.html",
    "title": "EIF_cond_exp_isfa_ista",
    "section": "",
    "text": "EIF_cond_exp_isfa_ista\nmodels.Neurons.EIF_cond_exp_isfa_ista(self, v_rest=-70.6, cm=0.281, tau_m=9.3667, tau_refrac=0.1, tau_syn_E=5.0, tau_syn_I=5.0, e_rev_E=0.0, e_rev_I=-80.0, tau_w=144.0, a=4.0, b=0.0805, i_offset=0.0, delta_T=2.0, v_thresh=-50.4, v_reset=-70.6, v_spike=-40.0)\nExponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.).\nDefinition according to:\n\nBrette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642\n\nParameters:\n\nv_rest = -70.6 : Resting membrane potential (mV)\ncm = 0.281 : Capacity of the membrane (nF)\ntau_m = 9.3667 : Membrane time constant (ms)\ntau_refrac = 0.1 : Duration of refractory period (ms)\ntau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms)\ntau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms)\ne_rev_E = 0.0 : Reversal potential for excitatory input (mV)\ne_rev_I = -80.0 : Reversal potential for inhibitory input (mv)\ntau_w = 144.0 : Time constant of the adaptation variable (ms)\na = 4.0 : Scaling of the adaptation variable\nb = 0.0805 : Increment on the adaptation variable after a spike\ni_offset = 0.0 : Offset current (nA)\ndelta_T = 2.0 : Speed of the exponential (mV)\nv_thresh = -50.4 : Spike threshold for the exponential (mV)\nv_reset = -70.6 : Reset potential after a spike (mV)\nv_spike = -40.0 : Spike threshold (mV)\n\nVariables:\n\nI : input current (nA):\nI = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset\nv : membrane potential in mV (init=-70.6):\ntau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w)\nw : adaptation variable (init=0.0):\ntau_w * dw/dt = a * (v - v_rest) / 1000.0 - w\ng_exc : excitatory current (init = 0.0):\ntau_syn_E * dg_exc/dt = - g_exc\ng_inh : inhibitory current (init = 0.0):\ntau_syn_I * dg_inh/dt = - g_inh\n\nSpike emission:\nv &gt; v_thresh\nReset:\nv = v_reset\nu += b\nThe ODEs are solved using the explicit Euler method.\nEquivalent code:\n\nEIF_cond_exp_isfa_ista = Neuron(\n    parameters = \"\"\"\n        v_rest = -70.6\n        cm = 0.281 \n        tau_m = 9.3667 \n        tau_syn_E = 5.0 \n        tau_syn_I = 5.0 \n        e_rev_E = 0.0 \n        e_rev_I = -80.0\n        tau_w = 144.0 \n        a = 4.0\n        b = 0.0805\n        i_offset = 0.0\n        delta_T = 2.0 \n        v_thresh = -50.4\n        v_reset = -70.6\n        v_spike = -40.0 \n    \"\"\", \n    equations = \"\"\"\n        I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset            \n        tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6          \n        tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w           \n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n    \"\"\",\n    spike = \"v &gt; v_spike\",\n    reset = \"\"\"\n        v = v_reset\n        w += b\n    \"\"\",\n    refractory = 0.1\n)",
    "crumbs": [
      "Reference",
      "**Neuron models**",
      "EIF_cond_exp_isfa_ista"
    ]
  },
  {
    "objectID": "reference/DecodingProjection.html",
    "href": "reference/DecodingProjection.html",
    "title": "DecodingProjection",
    "section": "",
    "text": "inputs.DecodingProjection.DecodingProjection(self, pre, post, target, window=0.0, name=None, copied=False)\nDecoding projection to transform spike trains into firing rates.\nThe pre-synaptic population must be a spiking population, while the post-synaptic one must be rate-coded.\nPre-synaptic spikes are accumulated for each post-synaptic neuron. A sliding window can be used to smoothen the results with the window parameter.\nThe decoded firing rate is accessible in the post-synaptic neurons with sum(target).\nThe projection can be connected using any method available in Projection (although all-to-all or many-to-one makes mostly sense). Delays are ignored.\nThe weight value allows to scale the firing rate: if you want a pre-synaptic firing rate of 100 Hz to correspond to a post-synaptic rate of 1.0, use w = 1./100..\nExample:\npop1 = ann.PoissonPopulation(1000, rates=100.)\npop2 = ann.Population(1, ann.Neuron(equations=\"r=sum(exc)\"))\nproj = DecodingProjection(pop1, pop2, 'exc', window=10.0)\nproj.connect_all_to_all(1.0, force_multiple_weights=True)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npre\nANNarchy.core.Population.Population\npre-synaptic population.\nrequired\n\n\npost\nANNarchy.core.Population.Population\npost-synaptic population.\nrequired\n\n\ntarget\nstr\ntype of the connection.\nrequired\n\n\nwindow\nfloat\nduration of the time window to collect spikes (default: dt).\n0.0",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "DecodingProjection"
    ]
  },
  {
    "objectID": "reference/DecodingProjection.html#parameters",
    "href": "reference/DecodingProjection.html#parameters",
    "title": "DecodingProjection",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npre\nANNarchy.core.Population.Population\npre-synaptic population.\nrequired\n\n\npost\nANNarchy.core.Population.Population\npost-synaptic population.\nrequired\n\n\ntarget\nstr\ntype of the connection.\nrequired\n\n\nwindow\nfloat\nduration of the time window to collect spikes (default: dt).\n0.0",
    "crumbs": [
      "Reference",
      "**Inputs**",
      "DecodingProjection"
    ]
  }
]