@article{Kriegeskorte2018a,
  title = {Cognitive Computational Neuroscience},
  author = {Kriegeskorte, Nikolaus and Douglas, Pamela K.},
  year = {2018},
  month = sep,
  journal = {Nature Neuroscience},
  volume = {21},
  number = {9},
  pages = {1148--1160},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0210-5},
  abstract = {To learn how cognition is implemented in the brain, we must build computational models that can perform cognitive tasks, and test such models with brain and behavioral experiments. Cognitive science has developed computational models that decompose cognition into functional components. Computational neuroscience has modeled how interacting neurons can implement elementary components of cognition. It is time to assemble the pieces of the puzzle of brain computation and to better integrate these separate disciplines. Modern technologies enable us to measure and manipulate brain activity in unprecedentedly rich ways in animals and humans. However, experiments will yield theoretical insight only when employed to test brain-computational models. Here we review recent work in the intersection of cognitive science, computational neuroscience and artificial intelligence. Computational models that mimic brain information processing during perceptual, cognitive and control tasks are beginning to be developed and tested with brain and behavioral data.},
}

@article{Izhikevich2003,
  title = {Simple Model of Spiking Neurons.},
  author = {Izhikevich, E M},
  year = {2003},
  month = jan,
  journal = {IEEE transactions on neural networks},
  volume = {14},
  number = {6},
  pages = {1569--72},
  doi = {10.1109/TNN.2003.820440},
  abstract = {A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.}
}

@article{Brette2005,
  title = {Adaptive {{Exponential Integrate-and-Fire Model}} as an {{Effective Description}} of {{Neuronal Activity}}},
  author = {Brette, Romain and Gerstner, Wulfram},
  year = {2005},
  month = nov,
  journal = {Journal of Neurophysiology},
  volume = {94},
  number = {5},
  pages = {3637--3642},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.00686.2005},
  abstract = {We introduce a two-dimensional integrate-and-fire model that combines an exponential spike mechanism with an adaptation equation, based on recent theoretical findings. We describe a systematic method to estimate its parameters with simple electrophysiological protocols (current-clamp injection of pulses and ramps) and apply it to a detailed conductance-based model of a regular spiking neuron. Our simple model predicts correctly the timing of 96\% of the spikes ({$\pm$}2 ms) of the detailed model in response to injection of noisy synaptic conductances. The model is especially reliable in high-conductance states, typical of cortical activity in vivo, in which intrinsic conductances were found to have a reduced role in shaping spike trains. These results are promising because this simple model has enough expressive power to reproduce qualitatively several electrophysiological classes described in vitro.},
}

@article{Bienenstock1982,
  title = {Theory for the Development of Neuron Selectivity: Orientation Specificity and Binocular Interaction in Visual Cortex.},
  author = {Bienenstock, E L and Cooper, L N and Munro, P W},
  year = {1982},
  month = jan,
  journal = {Journal of Neuroscience},
  volume = {2},
  number = {1},
  pages = {32--48},
  abstract = {The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form for this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further.},
  keywords = {Animals,Cats,Cerebral,Dominance,Mathematics,Models,Neurological,Neurons,Orientation,Retina,Senso}
}

@article{Intrator1992,
  title = {Objective Function Formulation of the {{BCM}} Theory of Visual Cortical Plasticity: {{Statistical}} Connections, Stability Conditions},
  author = {Intrator, Nathan and Cooper, Leon N},
  year = {1992},
  month = jan,
  journal = {Neural Networks},
  volume = {5},
  number = {1},
  pages = {3--17},
  doi = {10.1016/S0893-6080(05)80003-6},
  abstract = {In this paper, we present an objective function formulation of the Bienenstock, Cooper, and Munro (BCM) theory of visual cortical plasticity that permits us to demonstrate the connection between the unsupervised BCM learning procedure and various statistical methods, in particular, that of Projection Pursuit. This formulation provides a general method for stability analysis of the fixed points of the theory and enables us to analyze the behavior and the evolution of the network under various visual rearing conditions. It also allows comparison with many existing unsupervised methods. This model has been shown successful in various applications such as phoneme and 3D object recognition. We thus have the striking and possibly highly significant result that a biological neuron is performing a sophisticated statistical procedure.},
  keywords = {Dimensionality reduction,Feature extraction,Unsupervised learning}
}

@book{Dayan2001,
  title = {Theoretical {{Neuroscience}}: {{Computational}} and {{Mathematical Modeling}} of {{Neural Systems}}},
  author = {Dayan, Peter and Abbott, L. F.},
  year = {2001},
  month = sep,
  publisher = {{The MIT Press}},
  isbn = {0-262-54185-8},
}

@article{Oja1982,
  title = {A Simplified Neuron Model as a Principal Component Analyzer.},
  author = {Oja, E},
  year = {1982},
  month = jan,
  journal = {Journal of Mathematical Biology},
  volume = {15},
  number = {3},
  pages = {267--73},
  keywords = {Animals,Mathematics,Models; Neurological,Neurons,Neurons: physiology,Synapses,Synapses: physiology}
}

@article{Vitay2010,
  title = {A Computational Model of {{Basal Ganglia}} and Its Role in Memory Retrieval in Rewarded Visual Memory Tasks.},
  author = {Vitay, Julien and Hamker, Fred H},
  year = {2010},
  month = jan,
  journal = {Frontiers in computational neuroscience},
  volume = {4},
  number = {13},
  doi = {10.3389/fncom.2010.00013},
  abstract = {Visual working memory (WM) tasks involve a network of cortical areas such as inferotemporal, medial temporal and prefrontal cortices. We suggest here to investigate the role of the basal ganglia (BG) in the learning of delayed rewarded tasks through the selective gating of thalamocortical loops. We designed a computational model of the visual loop linking the perirhinal cortex, the BG and the thalamus, biased by sustained representations in prefrontal cortex. This model learns concurrently different delayed rewarded tasks that require to maintain a visual cue and to associate it to itself or to another visual object to obtain reward. The retrieval of visual information is achieved through thalamic stimulation of the perirhinal cortex. The input structure of the BG, the striatum, learns to represent visual information based on its association to reward, while the output structure, the substantia nigra pars reticulata, learns to link striatal representations to the disinhibition of the correct thalamocortical loop. In parallel, a dopaminergic cell learns to associate striatal representations to reward and modulates learning of connections within the BG. The model provides testable predictions about the behavior of several areas during such tasks, while providing a new functional organization of learning within the BG, putting emphasis on the learning of the striatonigral connections as well as the lateral connections within the substantia nigra pars reticulata. It suggests that the learning of visual WM tasks is achieved rapidly in the BG and used as a teacher for feedback connections from prefrontal cortex to posterior cortices.},
}

@article{Bi2001,
  title = {Synaptic {{Modification}} by {{Correlated Activity}}: {{Hebb}}'s {{Postulate Revisited}}},
  shorttitle = {Synaptic {{Modification}} by {{Correlated Activity}}},
  author = {Bi, Guo-qiang and Poo, Mu-ming},
  year = {2001},
  journal = {Annual Review of Neuroscience},
  volume = {24},
  number = {1},
  pages = {139--166},
  doi = {10.1146/annurev.neuro.24.1.139},
  abstract = {Correlated spiking of pre- and postsynaptic neurons can result in strengthening or weakening of synapses, depending on the temporal order of spiking. Recent findings indicate that there are narrow and cell type\textendash specific temporal windows for such synaptic modification and that the generally accepted input- (or synapse-) specific rule for modification appears not to be strictly adhered to. Spike timing\textendash dependent modifications, together with selective spread of synaptic changes, provide a set of cellular mechanisms that are likely to be important for the development and functioning of neural networks. When an axon of cell A is near enough to excite cell B or repeatedly or consistently takes part in firing it, some growth or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased. Donald Hebb (1949)},
}

@article{Villagrasa2018,
  title = {On the {{Role}} of {{Cortex-Basal Ganglia Interactions}} for {{Category Learning}}: {{A Neurocomputational Approach}}},
  shorttitle = {On the {{Role}} of {{Cortex-Basal Ganglia Interactions}} for {{Category Learning}}},
  author = {Villagrasa, Francesc and Baladron, Javier and Vitay, Julien and Schroll, Henning and Antzoulatos, Evan G. and Miller, Earl K. and Hamker, Fred H.},
  year = {2018},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {38},
  number = {44},
  pages = {9551--9562},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0874-18.2018},
  abstract = {In addition to the prefrontal cortex (PFC), the basal ganglia (BG) have been increasingly often reported to play a fundamental role in category learning, but the circuit mechanisms mediating their interaction remain to be explored. We developed a novel neurocomputational model of category learning that particularly addresses the BG\textendash PFC interplay. We propose that the BG bias PFC activity by removing the inhibition of cortico\textendash thalamo\textendash cortical loop and thereby provide a teaching signal to guide the acquisition of category representations in the corticocortical associations to the PFC. Our model replicates key behavioral and physiological data of macaque monkey learning a prototype distortion task from Antzoulatos and Miller (2011). Our simulations allowed us to gain a deeper insight into the observed drop of category selectivity in striatal neurons seen in the experimental data and in the model. The simulation results and a new analysis of the experimental data based on the model's predictions show that the drop in category selectivity of the striatum emerges as the variability of responses in the striatum rises when confronting the BG with an increasingly larger number of stimuli to be classified. The neurocomputational model therefore provides new testable insights of systems-level brain circuits involved in category learning that may also be generalized to better understand other cortico\textendash BG\textendash cortical loops. SIGNIFICANCE STATEMENT Inspired by the idea that basal ganglia (BG) teach the prefrontal cortex (PFC) to acquire category representations, we developed a novel neurocomputational model and tested it on a task that was recently applied in monkey experiments. As an advantage over previous models of category learning, our model allows to compare simulation data with single-cell recordings in PFC and BG. We not only derived model predictions, but already verified a prediction to explain the observed drop in striatal category selectivity. When testing our model with a simple, real-world face categorization task, we observed that the fast striatal learning with a performance of 85\% correct responses can teach the slower PFC learning to push the model performance up to almost 100\%.},
  copyright = {Copyright \textcopyright{} 2018 the authors 0270-6474/18/389551-13\$15.00/0},
  langid = {english},
  pmid = {30228231},
  keywords = {basal ganglia,category learning,computational model,physiological data,prefrontal cortex},
}

@article{Goenner2017,
  title = {Predictive {{Place-Cell Sequences}} for {{Goal-Finding Emerge}} from {{Goal Memory}} and the {{Cognitive Map}}: {{A Computational Model}}},
  author = {G{\"o}nner, Lorenz and Vitay, Julien and Hamker, Fred H.},
  year = {2017},
  month = oct,
  journal = {Frontiers in Computational Neuroscience},
  volume = {11},
  pages = {84--84},
  doi = {10.3389/fncom.2017.00084},
  abstract = {Hippocampal place-cell sequences observed during awake immobility often represent previous experience, suggesting a role in memory processes. However, recent reports of goals being overrepresented in sequential activity suggest a role in short-term planning, although a detailed understanding of the origins of hippocampal sequential activity and of its functional role is still lacking. In particular, it is unknown which mechanism could support efficient planning by generating place-cell sequences biased toward known goal locations, in an adaptive and constructive fashion. To address these questions, we propose a model of spatial learning and sequence generation as interdependent processes, integrating cortical contextual coding, synaptic plasticity and neuromodulatory mechanisms into a map-based approach. Following goal learning, sequential activity emerges from continuous attractor network dynamics biased by goal memory inputs. We apply Bayesian decoding on the resulting spike trains, allowing a direct comparison with experimental data. Simulations show that this model (1) explains the generation of never-experienced sequence trajectories in familiar environments, without requiring virtual self-motion signals, (2) accounts for the bias in place-cell sequences toward goal locations, (3) highlights their utility in flexible route planning, and (4) provides specific testable predictions.},
  copyright = {All rights reserved},
  keywords = {Bayesian decoding,contextual bias,Contextual bias,continuous attractor network,Continuous attractor network,goal memory,Goal memory,memory recall,Memory recall,reward-based learning,Reward-based learning,sequential activity,Sequential activity},
}

@article{Vitay2014,
  title = {Timing and Expectation of Reward: {{A}} Neuro-Computational Model of the Afferents to the Ventral Tegmental Area},
  author = {Vitay, Julien and Hamker, Fred H.},
  year = {2014},
  journal = {Frontiers in Neurorobotics},
  volume = {8},
  number = {4},
  issn = {1662-5218 (Electronic)\textbackslash r1662-5218 (Linking)},
  doi = {10.3389/fnbot.2014.00004},
  abstract = {Neural activity in dopaminergic areas such as the ventral tegmental area is influenced by timing processes, in particular by the temporal expectation of rewards during Pavlovian conditioning. Receipt of a reward at the expected time allows to compute reward-prediction errors which can drive learning in motor or cognitive structures. Reciprocally, dopamine plays an important role in the timing of external events. Several models of the dopaminergic system exist, but the substrate of temporal learning is rather unclear. In this article, we propose a neuro-computational model of the afferent network to the ventral tegmental area, including the lateral hypothalamus, the pedunculopontine nucleus, the amygdala, the ventromedial prefrontal cortex, the ventral basal ganglia (including the nucleus accumbens and the ventral pallidum), as well as the lateral habenula and the rostromedial tegmental nucleus. Based on a plausible connectivity and realistic learning rules, this neuro-computational model reproduces several experimental observations, such as the progressive cancelation of dopaminergic bursts at reward delivery, the appearance of bursts at the onset of reward-predicting cues or the influence of reward magnitude on activity in the amygdala and ventral tegmental area. While associative learning occurs primarily in the amygdala, learning of the temporal relationship between the cue and the associated reward is implemented as a dopamine-modulated coincidence detection mechanism in the nucleus accumbens.},
  copyright = {All rights reserved},
  keywords = {Amygdala,Basal ganglia,Classical conditioning,Dopamine,Timing,VTA},
}

@article{Vitay2015,
  title = {{{ANNarchy}}: A Code Generation Approach to Neural Simulations on Parallel Hardware},
  author = {Vitay, Julien and Dinkelbach, Helge {\"U}. and Hamker, Fred H.},
  year = {2015},
  journal = {Frontiers in Neuroinformatics},
  volume = {9},
  doi = {10.3389/fninf.2015.00019},
  abstract = {Many modern neural simulators focus on the simulation of networks of spiking neurons on parallel hardware. Another important framework in computational neuroscience, rate-coded neural networks, is mostly difficult or impossible to implement using these simulators. We present here the ANNarchy (Artificial Neural Networks architect) neural simulator, which allows to easily define and simulate rate-coded and spiking networks, as well as combinations of both. The interface in Python has been designed to be close to the PyNN interface, while the definition of neuron and synapse models can be specified using an equation-oriented mathematical description similar to the Brian neural simulator. This information is used to generate C++ code that will efficiently perform the simulation on the chosen parallel hardware (multi-core system or graphical processing unit). Several numerical methods are available to transform ordinary differential equations into an efficient C++code. We compare the parallel performance of the simulator to existing solutions.},
}

@techreport{Jaeger2001,
  title = {The "Echo State" Approach to Analysing and Training Recurrent Neural Networks},
  author = {Jaeger, Herbert},
  year = {2001},
  pages = {GMD Report 148-GMD Report 148},
  institution = {{Jacobs Universit\"at Bremen}}
}

@article{Miconi2017,
  title = {Biologically Plausible Learning in Recurrent Neural Networks Reproduces Neural Dynamics Observed during Cognitive Tasks},
  author = {Miconi, Thomas},
  year = {2017},
  month = feb,
  journal = {eLife},
  volume = {6},
  doi = {10.7554/elife.20899},
}

@article{Teichmann2021,
  title = {Performance of Biologically Grounded Models of the Early Visual System on Standard Object Recognition Tasks},
  author = {Teichmann, Michael and Larisch, Ren{\'e} and Hamker, Fred H.},
  year = {2021},
  month = dec,
  journal = {Neural Networks},
  volume = {144},
  pages = {210--228},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.08.009},
  abstract = {Computational neuroscience models of vision and neural network models for object recognition are often framed by different research agendas. Computational neuroscience mainly aims at replicating experimental data, while (artificial) neural networks target high performance on classification tasks. However, we propose that models of vision should be validated on object recognition tasks. At some point, mechanisms of realistic neuro-computational models of the visual cortex have to convince in object recognition as well. In order to foster this idea, we report the recognition accuracy for two different neuro-computational models of the visual cortex on several object recognition datasets. The models were trained using unsupervised Hebbian learning rules on natural scene inputs for the emergence of receptive fields comparable to their biological counterpart. We assume that the emerged receptive fields result in a general codebook of features, which should be applicable to a variety of visual scenes. We report the performances on datasets with different levels of difficulty, ranging from the simple MNIST to the more complex CIFAR-10 or ETH-80. We found that both networks show good results on simple digit recognition, comparable with previously published biologically plausible models. We also observed that our deeper layer neurons provide for naturalistic datasets a better recognition codebook. As for most datasets, recognition results of biologically grounded models are not available yet, our results provide a broad basis of performance values to compare methodologically similar models.},
  langid = {english},
  keywords = {Brain-inspired neural networks,Hebbian learning,Object recognition,Spike timing-dependent plasticity,Spiking neural networks},
}


@article{Vogels2005,
  title = {Signal Propagation and Logic Gating in Networks of Integrate-and-Fire Neurons.},
  author = {Vogels, Tim P and Abbott, L F},
  date = {2005-11},
  journaltitle = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
  volume = {25},
  number = {46},
  eprint = {16291952},
  eprinttype = {pubmed},
  pages = {10786--95},
  doi = {10.1523/JNEUROSCI.3508-05.2005},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/16291952},
  abstract = {Transmission of signals within the brain is essential for cognitive function, but it is not clear how neural circuits support reliable and accurate signal propagation over a sufficiently large dynamic range. Two modes of propagation have been studied: synfire chains, in which synchronous activity travels through feedforward layers of a neuronal network, and the propagation of fluctuations in firing rate across these layers. In both cases, a sufficient amount of noise, which was added to previous models from an external source, had to be included to support stable propagation. Sparse, randomly connected networks of spiking model neurons can generate chaotic patterns of activity. We investigate whether this activity, which is a more realistic noise source, is sufficient to allow for signal transmission. We find that, for rate-coded signals but not for synfire chains, such networks support robust and accurate signal reproduction through up to six layers if appropriate adjustments are made in synaptic strengths. We investigate the factors affecting transmission and show that multiple signals can propagate simultaneously along different pathways. Using this feature, we show how different types of logic gates can arise within the architecture of the random network through the strengthening of specific synapses.},
  keywords = {Action Potentials,Action Potentials: physiology,Ion Channel Gating,Ion Channel Gating: physiology,Logic,Models Neurological,Neural Networks (Computer),Neural Pathways,Neural Pathways: physiology,Neurons,Neurons: physiology,Signal Transduction,Signal Transduction: physiology}
}

@article{Tsodyks1997,
  title = {The Neural Code between Neocortical Pyramidal Neurons Depends on Neurotransmitter Release Probability},
  author = {Tsodyks, M. V. and Markram, H.},
  date = {1997-01},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {94},
  number = {2},
  pages = {719--723},
  doi = {10.1073/pnas.94.2.719},
  url = {http://www.pnas.org/content/94/2/719},
  abstract = {Although signaling between neurons is central to the functioning of the brain, we still do not understand how the code used in signaling depends on the properties of synaptic transmission. Theoretical analysis combined with patch clamp recordings from pairs of neocortical pyramidal neurons revealed that the rate of synaptic depression, which depends on the probability of neurotransmitter release, dictates the extent to which firing rate and temporal coherence of action potentials within a presynaptic population are signaled to the postsynaptic neuron. The postsynaptic response primarily reflects rates of firing when depression is slow and temporal coherence when depression is fast. A wide range of rates of synaptic depression between different pairs of pyramidal neurons was found, suggesting that the relative contribution of rate and temporal signals varies along a continuum. We conclude that by setting the rate of synaptic depression, release probability is an important factor in determining the neural code.}
}



