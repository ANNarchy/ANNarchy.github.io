{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN-to-SNN conversion\n",
    "\n",
    "Download the Jupyter notebook : [ANN2SNN.ipynb](https://raw.githubusercontent.com/ANNarchy/ANNarchy.github.io/master/docs/example/ANN2SNN.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to transform a neural network trained using tensorflow/keras into an SNN network usable in ANNarchy.\n",
    "\n",
    "The models are adapted from the original models used in:\n",
    "\n",
    "> Diehl et al. (2015) \"Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing\" Proceedings of IJCNN. doi: 10.1109/IJCNN.2015.7280696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to download and process the MNIST dataset provided by tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "(X_train, t_train), (X_test, t_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize inputs\n",
    "X_train = X_train.reshape(X_train.shape[0], 784).astype('float32') / 255.\n",
    "X_test = X_test.reshape(X_test.shape[0], 784).astype('float32') / 255.\n",
    "\n",
    "# One-hot output vectors\n",
    "T_train = tf.keras.utils.to_categorical(t_train, 10)\n",
    "T_test = tf.keras.utils.to_categorical(t_test, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an ANN in tensorflow/keras\n",
    "\n",
    "The `tensorflow` networks are build using the functional API. \n",
    "\n",
    "The fully-connected network has two fully connected layers with ReLU, **no bias**, dropout at 0.5, and a softmax output layer with 10 neurons. We use the standard SGD optimizer and the categorical crossentropy loss for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp():\n",
    "    # Model\n",
    "    inputs = tf.keras.layers.Input(shape=(784,))\n",
    "    x= tf.keras.layers.Dense(128, use_bias=False, activation='relu')(inputs)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x= tf.keras.layers.Dense(128, use_bias=False, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x=tf.keras.layers.Dense(10, use_bias=False, activation='softmax')(x)\n",
    "\n",
    "    model= tf.keras.Model(inputs, x)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "\n",
    "    # Loss function\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', # loss function\n",
    "        optimizer=optimizer, # learning rule\n",
    "        metrics=['accuracy'] # show accuracy\n",
    "    )\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the network and save the weights in the HDF5 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_mlp()\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, T_train,       # training data\n",
    "    batch_size=128,          # batch size\n",
    "    epochs=20,              # Maximum number of epochs\n",
    "    validation_split=0.1,   # Percentage of training data used for validation\n",
    ")\n",
    "model.save(\"runs/mlp.h5\")\n",
    "\n",
    "# Test model\n",
    "predictions_keras = model.predict(X_test, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, T_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'], '-r', label=\"Training\")\n",
    "plt.plot(history.history['val_loss'], '-b', label=\"Validation\")\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'], '-r', label=\"Training\")\n",
    "plt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the ANN-to-SNN converter\n",
    "\n",
    "We first create an instance of the ANN-to-SNN conversion object. The function receives the *input_encoding* parameter, which is the type of input encoding we want to use. \n",
    "\n",
    "By default, there are *intrinsically bursting* (`IB`), *phase shift oscillation* (`PSO`) and *Poisson* (`poisson`) available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNarchy 4.7 (4.7.3) on darwin (posix).\n"
     ]
    }
   ],
   "source": [
    "from ANNarchy.extensions.ann_to_snn_conversion import ANNtoSNNConverter\n",
    "\n",
    "snn_converter = ANNtoSNNConverter(\n",
    "    input_encoding='IB',\n",
    "    hidden_neuron='IaF', \n",
    "    read_out='spike_count',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we provide the TensorFlow model stored as h5py file to the conversion tool. The print-out of the network structure of the imported network is suppressed when *show_info*=False is provided to *init_from_keras_model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Dense representation is an experimental feature for spiking models, we greatly appreciate bug reports. \n",
      "Parameters\n",
      "----------------------\n",
      "* input encoding: IB\n",
      "* hidden neuron: IaF neuron\n",
      "* read-out method: spike_count\n",
      "\n",
      "Layers\n",
      "----------------------\n",
      "* name=dense, dense layer, geometry=128\n",
      "* name=dense_1, dense layer, geometry=128\n",
      "* name=dense_2, dense layer, geometry=10\n",
      "\n",
      "Projections\n",
      "----------------------\n",
      "* input_1 (784,) -> dense (128,)\n",
      "    weight matrix size (128, 784)\n",
      "    mean -0.0021713783498853445, std 0.052132513374090195\n",
      "    min -0.3114379048347473, max 0.2092430740594864\n",
      "* dense (128,) -> dense_1 (128,)\n",
      "    weight matrix size (128, 128)\n",
      "    mean 0.007305823266506195, std 0.10028184950351715\n",
      "    min -0.33722999691963196, max 0.5378042459487915\n",
      "* dense_1 (128,) -> dense_2 (10,)\n",
      "    weight matrix size (10, 128)\n",
      "    mean -0.005615146365016699, std 0.20395728945732117\n",
      "    min -0.5734260678291321, max 0.5247938632965088\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = snn_converter.init_from_keras_model(\"runs/mlp.h5\", show_distributions=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the network has been built successfully, we can perform a test using all MNIST training samples. Using `duration_per_sample`, the number of steps simulated for each image can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9900/10000\r"
     ]
    }
   ],
   "source": [
    "predictions_snn = snn_converter.predict(X_test, duration_per_sample=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the selected read-out method, it can happen that multiple neurons/classes are selected as a winner for an example. For example, if `duration_per_sample` is too low, several output neurons might output the same number of spikes. \n",
    "\n",
    "In the following cell, we force the predictions to keep only one of the winning neurons by using `np.random.choice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_snn = [ [np.random.choice(p)] for p in predictions_snn ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the recorded predictions, we can now compute the accuracy using scikit-learn for all presented samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       980\n",
      "           1       0.98      0.99      0.98      1135\n",
      "           2       0.97      0.94      0.95      1032\n",
      "           3       0.93      0.97      0.95      1010\n",
      "           4       0.95      0.96      0.96       982\n",
      "           5       0.96      0.94      0.95       892\n",
      "           6       0.96      0.97      0.96       958\n",
      "           7       0.96      0.96      0.96      1028\n",
      "           8       0.95      0.95      0.95       974\n",
      "           9       0.95      0.93      0.94      1009\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n",
      "Test accuracy of the SNN: 0.9583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(t_test, predictions_snn))\n",
    "print(\"Test accuracy of the SNN:\", accuracy_score(t_test, predictions_snn))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, here is the performance of the original SNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       980\n",
      "           1       0.98      0.98      0.98      1135\n",
      "           2       0.98      0.95      0.96      1032\n",
      "           3       0.94      0.97      0.96      1010\n",
      "           4       0.97      0.97      0.97       982\n",
      "           5       0.97      0.96      0.97       892\n",
      "           6       0.97      0.97      0.97       958\n",
      "           7       0.97      0.96      0.96      1028\n",
      "           8       0.96      0.96      0.96       974\n",
      "           9       0.96      0.95      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "Test accuracy of the ANN: 0.9671\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(t_test, predictions_keras.argmax(axis=1)))\n",
    "print(\"Test accuracy of the ANN:\", accuracy_score(t_test, predictions_keras.argmax(axis=1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison of the predictions made by the ANN and the SNN on each class may reveal different behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       989\n",
      "           1       0.98      0.99      0.99      1139\n",
      "           2       0.97      0.97      0.97      1006\n",
      "           3       0.96      0.97      0.97      1045\n",
      "           4       0.97      0.97      0.97       986\n",
      "           5       0.97      0.96      0.97       877\n",
      "           6       0.98      0.98      0.98       964\n",
      "           7       0.98      0.98      0.98      1025\n",
      "           8       0.97      0.97      0.97       978\n",
      "           9       0.97      0.97      0.97       991\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions_keras.argmax(axis=1), predictions_snn))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d24234067c217f49dc985cbc60012ce72928059d528f330ba9cb23ce737906d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
