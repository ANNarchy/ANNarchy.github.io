---
title: ANNarchy
subtitle: Artificial Neural Networks architect
authors:
  - name: Julien Vitay 
    email: julien.vitay@informatik.tu-chemnitz.de
    url: https://julien-vitay.net
    affiliation: Chemnitz University of Technology
    affiliation-url: https://tu-chemnitz.de
    orcid: 0000-0001-5229-2349
  - name: Helge Ülo Dinkelbach
    email: helge-uelo.dinkelbach@informatik.tu-chemnitz.de
    url: https://www.tu-chemnitz.de/informatik/KI
    affiliation: Chemnitz University of Technology
    affiliation-url: https://tu-chemnitz.de
    orcid: 0000-0002-8871-8177
  - name: Fred Hamker
    email: fred.hamker@informatik.tu-chemnitz.de
    url: https://www.tu-chemnitz.de/informatik/KI
    affiliation: Chemnitz University of Technology
    affiliation-url: https://tu-chemnitz.de
    orcid: 0000-0001-9104-7143

format:
  revealjs: 
    
    theme: [default, ../assets/slides.scss]

    slide-number: c/t
    html-math-method: katex
    chalkboard: 
      buttons: false

    auto-stretch: false
    touch: true
    fig-align: "center"

    center-title-slide: true
    title-slide-attributes:
        data-background-image: "img/tuc-new-large.png"
        data-background-size: 30%
        data-background-opacity: "1"
        data-background-position: top

    bibliography: ../assets/references.bib
    csl: ../assets/frontiers.csl

    highlight-style: github
    code-line-numbers: false
    callout-appearance: minimal
---

# Outline

1. Neurocomputational models

2. Neuro-simulator ANNarchy

3. Rate-coded networks

4. Spiking networks

# 1 - Neurocomputational models

# Computational neuroscience

::: {.columns}
::: {.column width=50%}

* Computational neuroscience is about explaining brain functioning at various levels (neural activity patterns, behavior, etc.) using biologically realistic neuro-computational models. 

* Different types of neural and synaptic mathematical models are used in the field, abstracting biological complexity at different levels.

* There is no "right" level of biological plausibility for a model - you can always add more details -, but you have to find a paradigm that allows you to:

1. Explain the current experimental data.
2. Make predictions that can be useful to experimentalists.

::: 
::: {.column width=50%}

![Source: @Kriegeskorte2018a](img/neurocomp.png)

:::
:::

# Rate-coded and spiking neurons

::: {.columns}
::: {.column width=50%}


* **Rate-coded** neurons only represent the instantaneous firing rate of a neuron:

$$
    \tau \, \frac{d v(t)}{dt} + v(t) = \sum_{i=1}^d w_{i, j} \, r_i(t) + b
$$

$$
    r(t) = f(v(t))
$$


![](img/ratecoded-simple.png){width=80%}

:::
::: {.column width=50%}

* **Spiking** neurons emit binary spikes when their membrane potential exceeds a threshold (leaky integrate-and-fire, LIF):

$$
    C \, \frac{d v(t)}{dt} = - g_L \, (v(t) - V_L) + I(t)
$$

$$
    \text{if} \; v(t) > V_T \; \text{emit a spike and reset.}
$$

![](img/LIF-threshold.png)


:::
:::



# Many different spiking neuron models are possible

::: {.columns}
::: {.column width=50%}

* Izhikevich quadratic IF [@Izhikevich2003].

$$\begin{cases}
    \displaystyle\frac{dv}{dt} = 0.04 \, v^2 + 5 \, v + 140 - u + I \\
    \\
    \displaystyle\frac{du}{dt} = a \, (b \, v - u) \\
\end{cases}$$

:::
::: {.column width=50%}

* Adaptive exponential IF [AdEx, @Brette2005].

$$
\begin{cases}
\begin{aligned}
    C \, \frac{dv}{dt} = -g_L \ (v - E_L) + & g_L \, \Delta_T \, \exp(\frac{v - v_T}{\Delta_T}) \\
                                            & + I - w
\end{aligned}\\
\\
    \tau_w \, \displaystyle\frac{dw}{dt} = a \, (v - E_L) - w\\
\end{cases}$$

:::
:::


![](img/LIF-Izhi-AdEx.png)

# Realistic neuron models can reproduce a variety of dynamics


* Biological neurons do not all respond the same to an input current: Some fire regularly, some slow down with time., some emit bursts of spikes...

* Modern spiking neuron models allow to recreate these dynamics by changing just a few parameters.

![](img/adex.png){width=60% fig-align="center"}


# Populations of neurons


* Recurrent neural networks (e.g. randomly connected populations of neurons) can exhibit very rich **dynamics** even in the absence of inputs:

::: {.columns}
::: {.column width=50%}


![](img/rc-network.jpg)

![](img/reservoir-simple.png){width=50%}

:::
::: {.column width=50%}


* Oscillations at the population level.

* Excitatory/inhibitory balance. 

* Spatio-temporal separation of inputs (**reservoir computing**).

![](img/ratecoded-izhikevich.png){width=80%}

:::
:::



# Synaptic plasticity: Hebbian learning

* **Hebbian learning** postulates that synapses strengthen based on the **correlation** between the activity of the pre- and post-synaptic neurons:


> When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased. 
>
> **Donald Hebb**, 1949



* Weights increase proportionally to the the product of the pre- and post-synaptic firing rates:

$$\frac{dw}{dt} = \eta \, r^\text{pre} \, r^\text{post}$$


![Source: <https://slideplayer.com/slide/11511675/>](img/hebb.png){width=60%}


# Synaptic plasticity: Hebbian-based learning


* The **BCM** [@Bienenstock1982;@Intrator1992] plasticity rule allows LTP and LTD depending on the post-synaptic activity:


$$\frac{dw}{dt} = \eta \, r^\text{pre} \, r^\text{post}  \,  (r^\text{post} - \mathbb{E}[(r^\text{post})^2])$$


::: {.columns}
::: {.column width=70%}

* **Covariance** learning rule [@Dayan2001]:

$$\frac{dw}{dt} = \eta \, r^\text{pre} \, (r^\text{post} - \mathbb{E}[r^\text{post}])$$

* **Oja** learning rule [@Oja1982]:

$$\frac{dw}{dt}= \eta \, r^\text{pre} \, r^\text{post} - \alpha \, (r^\text{post})^2 \, w$$


:::
::: {.column width=30%}

![Source: <http://www.scholarpedia.org/article/BCM_theory>](img/bcm.png)



:::
:::


or virtually anything depending only on the pre- and post-synaptic firing rates, e.g.  @Vitay2010:


$$\begin{aligned}
    \frac{dw}{dt}  & = \eta \, ( \text{DA}(t) - \overline{\text{DA}}) \, (r^\text{post} - \mathbb{E}[r^\text{post}] )^+  \, (r^\text{pre} - \mathbb{E}[r^\text{pre}])- \alpha(t) \,  ((r^\text{post} - \mathbb{E}[r^\text{post}] )^+ )^2  \, w
\end{aligned}
$$

# STDP: Spike-timing dependent plasticity

* Synaptic efficiencies actually evolve depending on the the **causation** between the neuron's firing patterns:

    * If the pre-synaptic neuron fires **before** the post-synaptic one, the weight is increased (**long-term potentiation**). Pre causes Post to fire.

    * If it fires **after**, the weight is decreased (**long-term depression**). Pre does not cause Post to fire.

![ ](img/stdp.jpg){width=70% fig-align="center"}

:::footer
@Bi2001 Synaptic Modification by Correlated Activity: Hebb’s Postulate Revisited. Annual Review of Neuroscience 24.
:::

# STDP: Spike-timing dependent plasticity

::: {.columns}
::: {.column width=50%}


* The STDP [spike-timing dependent plasticity, @Bi2001] plasticity rule describes how the weight of a synapse evolves when the pre-synaptic neuron fires at $t_\text{pre}$ and the post-synaptic one fires at $t_\text{post}$.

$$ \frac{dw}{dt} = \begin{cases} A^+ \, \exp - \frac{t_\text{pre} - t_\text{post}}{\tau^+} \; \text{if} \; t_\text{post} > t_\text{pre}\\  
    \\
    A^- \, \exp - \frac{t_\text{pre} - t_\text{post}}{\tau^-} \; \text{if} \; t_\text{pre} > t_\text{post}\\ \end{cases}
$$

* STDP can be implemented online using traces.

* More complex variants of STDP (triplet STDP) exist, but this is the main model of synaptic plasticity in spiking networks.

:::
::: {.column width=50%}


![](img/stdp2.png)

:::
:::


:::footer
@Bi2001 Synaptic Modification by Correlated Activity: Hebb’s Postulate Revisited. Annual Review of Neuroscience 24.
:::

# Neuro-computational modeling

* Populations of neurons can be combined in functional **neuro-computational models** learning to solve various tasks.

* Need to implement one (or more) equations per neuron and synapse. There can thousands of neurons and millions of synapses, see @Teichmann2021.

::: {.columns}
::: {.column width=33%}

**Basal Ganglia** 

![Source: @Villagrasa2018](img/BG.png)

::: 
::: {.column width=33%}


**Hippocampus** 

![Source: @Goenner2017](img/Hippocampus.png)

:::
::: {.column width=33%}

**Dopaminergic system** 

![Source: @Vitay2014](img/Dopamine.png)


:::
:::



# 2 - Neuro-simulator ANNarchy


# Neuro-simulators

::: {.columns}
::: {.column width=50%}


**Fixed libraries of models**

* **NEURON**\
<https://neuron.yale.edu/neuron/>

    * Multi-compartmental models, spiking neurons (CPU)

<!-- * **GENESIS** <http://genesis-sim.org/>

    * Multi-compartmental models, spiking neurons (CPU) -->

* **NEST**\
<https://nest-initiative.org/>

    * Spiking neurons (CPU)

* **GeNN**\
<https://genn-team.github.io/genn/>

    * Spiking neurons (GPU)

* **Auryn**\
<https://fzenke.net/auryn/doku.php>

    * Spiking neurons (CPU)

:::
::: {.column width=50%}


**Code generation**

* **Brian**\
<https://briansimulator.org/>

    * Spiking neurons (CPU)

* **Brian2CUDA**\
<https://github.com/brian-team/brian2cuda>

    * Spiking neurons (GPU)

* **ANNarchy**\
<https://github.com/ANNarchy/ANNarchy>

    * Rate-coded and spiking neurons (CPU, GPU)

:::
:::


# ANNarchy (Artificial Neural Networks architect)

<br>

@Vitay2015\
**ANNarchy: a code generation approach to neural simulations on parallel hardware.**\
*Frontiers in Neuroinformatics* 9. [doi:10.3389/fninf.2015.00019](https://doi.org/10.3389/fninf.2015.00019)

<br>

::: {.columns}
::: {.column width=50%}


![](img/drawing.svg)

:::
::: {.column width=50%}



* Source code:

<https://github.com/ANNarchy/ANNarchy>

* Documentation:

<https://annarchy.github.io>

* Forum:

<https://groups.google.com/forum/#!forum/annarchy>


:::
:::





# Installation

Installation guide: <https://annarchy.github.io/Installation/>

Using pip:

```bash
pip install ANNarchy
```

From source:

```bash
git clone https://github.com/ANNarchy/ANNarchy.git
cd annarchy
pip install -e .
```

Requirements (Linux and MacOS):

* g++/clang++
* python >= 3.10
* numpy
* sympy
* nanobind


# Features

::: {.columns}
::: {.column width=50%}


* Simulation of both **rate-coded** and **spiking** neural networks.

* Only local biologically realistic mechanisms are possible (no backpropagation).

* **Equation-oriented** description of neural/synaptic dynamics (à la Brian).

* **Code generation** in C++, parallelized using OpenMP on CPU and CUDA on GPU (MPI is coming).

* Synaptic, intrinsic and structural plasticity mechanisms.

:::
::: {.column width=50%}


![](img/drawing.svg){width=100%}

:::
:::


---

![](img/drawing.svg){width=100%}


# Structure of a script

```python
import ANNarchy as ann

# Create network
net = ann.Network()

# Create neuron and  synapse types for transmission and/or plasticity
neuron = ann.Neuron(...) 
stdp = ann.Synapse(...) 

# Create populations of neurons
pop = net.create(1000, neuron) 

# Connect the populations through projections
proj = net.connect(pop, pop, 'exc', stdp) 
proj.connect_fixed_probability(weights=ann.Uniform(0.0, 1.0), probability=0.1)

# Generate and compile the code
net.compile() 

# Simulate for 1 second
net.simulate(1000.)
```

# 3 - Rate-coded networks

# Echo-State Network

::: {.columns}
::: {.column width=40%}

![](img/rc.jpg)

::: 
::: {.column width=60%}

* ESN rate-coded neurons follow first-order ODEs:

$$
    \tau \frac{dx(t)}{dt} + x(t) = \sum w^\text{in} \, r^\text{in}(t) + g \, \sum w^\text{rec} \, r(t) + \xi(t)
$$

$$
    r(t) = \tanh(x(t))
$$

:::
:::

* Neural dynamics are described by the equation-oriented interface:

```python
import ANNarchy as ann

ESN_Neuron = ann.Neuron(

    parameters = dict(tau=30.0, g=1.0 , noise=0.01),
    
    equations = [
        'tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1)',
        'r = tanh(x)'
    ]
)
```

# Parameters

* The parameters are provided as a dictionary:

```python
parameters = dict(tau=30.0, g=1.0 , noise=0.01),
```


* Parameters can have one value per neuron in the population (**local**) or be common to all neurons of the population (**global**).

* The parameters are considered global by default. To define a local parameters, use `ann.Parameter()`:

```python
parameters = dict(tau=30.0, g=1.0 , noise=ann.Parameter(0.01, locality='local'))
```


* Parameters and variables are floats by default, but the type can also be specified (`int`, `bool`).

```python
parameters = dict(
    tau = 30.0, g = 1.0, noise=0.01, 
    activated= ann.Parameter(False, type=bool, locality='local'),
)
```

# Variables


* Variables are evaluated at each time step **in the order of their declaration**.

```python
equations = [
    'tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1)',
    'r = tanh(x)',
]
```

* The output variable of a rate-coded neuron must be named `r`.

* Variables can be updated with assignments (`=`, `+=`, etc) or by defining first order ODEs.

* The math C library symbols can be used (`tanh`, `cos`, `exp`, etc).

* Initial values can be specified by using an `ann.Variable()` object (default: 0.0). 

```python
equations = [
    ann.Variable('tau * dx/dt + x = sum(exc)', init=-1.0),
    ann.Variable('r = tanh(x)'),
]
```

* Lower/higher bounds on the values of the variables can be set with the `min`/`max` flags:

```python
equations = [
    ann.Variable('tau * dx/dt + x = sum(exc)', init=-1.0, max=1.0),
    ann.Variable('r = x', min=0.0), # ReLU
]
```

* Additive noise can be drawn from several distributions, including `Uniform`, `Normal`, `LogNormal`, `Exponential`, `Gamma`...


# ODEs

::: {.columns}
::: {.column width=50%}


* First-order ODEs are parsed and manipulated using `sympy`:

```python
# All equivalent:
tau * dx/dt + x = I
tau * dx/dt = I - x
dx/dt = (I - x)/tau
```


* The generated C++ code applies a numerical method (fixed step size `dt`) for all neurons:

```c++
#pragma omp simd
for(unsigned int i = 0; i < size; i++){
    double _x = (I[i] - x[i])/tau;
    x[i] += dt*_x ;
    r[i] = tanh(x[i]);
}
```


:::
::: {.column width=50%}


* Several numerical methods are available:

    * Explicit (forward) Euler (default): 
    
    ```python
    tau * dx/dt + x = I : explicit
    ```

    * Implicit (backward) Euler: 
    
    ```python
    tau * dx/dt + x = I : implicit
    ```

    * Exponential Euler (exact for linear ODE):  
    
    ```python
    tau * dx/dt + x = I : exponential
    ```

    * Midpoint (RK2): 
    
    ```python
    tau * dx/dt + x = I : midpoint
    ```

    * Runge-Kutta (RK4): 
    
    ```python
    tau * dx/dt + x = I : rk4
    ```

    * Event-driven (spiking synapses): 
    
    ```python
    tau * dx/dt + x = I : event-driven
    ```
:::
:::


::: footer
<https://annarchy.github.io/manual/NumericalMethods>
:::

# Network

:::columns
:::column

* `Network` is the main object in ANNarchy:

```python
net = ann.Network()
```

* It is the container for all populations and projections of the network:

```python
pop = net.create(
    geometry=1000, 
    neuron=ESN_Neuron
)

proj = net.connect(
    pre=pop, 
    post=pop, 
    target='exc'
)
```

* It stores the configuration of the simulation, including the numerical time step `dt` and the seed of the RNG:

```python
net = ann.Network(dt=0.1, seed=42)
net.config(num_threads=8)
```

:::
:::column

* For better re-usability, you can create your own `Network` subclasses:

```python
class SimpleNetwork(ann.Network):
    def __init__(self, size:int):

        self.pop = self.create(
            geometry=size, 
            neuron=ESN_Neuron
        )
        
        self.proj = self.connect(
            pre=self.pop, 
            post=self.pop, 
            target='exc'
        )

net = SimpleNetwork(1000)
```

* `Network` subclasses allow to run simulations in parallel.

:::
:::


# Populations

* Populations are creating by specifying a number of neurons and a neuron type:

```python
pop = net.create(geometry=1000, neuron=ESN_Neuron)
```

* For visualization purposes or when using convolutional layers, a tuple geometry can be passed instead of the size:

```python
pop = net.create(geometry=(100, 100), neuron=ESN_Neuron)
```

* All parameters and variables become attributes of the population (read and write) as numpy arrays:

```python
pop.tau = np.linspace(20.0, 40.0, 1000)
pop.r = np.tanh(pop.v)
```

* Slices of populations are called `PopulationView` and can be addressed separately:

```python
pop = net.create(geometry=1000, neuron=ESN_Neuron)
E = pop[:800]
I = pop[800:]
```

# Projections

* Projections connect two populations (or views) in a uni-directional way.

```python
proj_exc = net.connect(E, pop, 'exc')
proj_inh = net.connect(I, pop, 'inh')
```

* Each target (`'exc', 'inh', 'AMPA', 'NMDA', 'GABA'`) can be defined as needed and will be treated differently by the post-synaptic neurons.

* The weighted sum of inputs for a specific target is accessed in the equations by `sum(target)`:

```python
equations = [
    ann.Variable('tau * dx/dt + x = sum(exc) / (1 + sum(inh))'),
    ann.Variable('r = tanh(x)'),
]
```

* It is therefore possible to model modulatory effects, divisive inhibition, etc.

# Connection methods

* Projections must be populated with a connectivity matrix (who is connected to who), a weight `w` and optionally a delay `d` (uniform or variable).

* Several patterns are predefined:

```python
proj.connect_all_to_all(weights=ann.Normal(0.0, 1.0), delays=2.0, allow_self_connections=False)

proj.connect_fixed_probability(probability=0.2, weights=1.0)

proj.connect_one_to_one(weights=1.0, delays=ann.Uniform(1.0, 10.0))

proj.connect_fixed_number_pre(number=20, weights=1.0)

proj.connect_fixed_number_post(number=20, weights=1.0)

proj.connect_gaussian(amp=1.0, sigma=0.2, limit=0.001)

proj.connect_dog(amp_pos=1.0, sigma_pos=0.2, amp_neg=0.3, sigma_neg=0.7, limit=0.001)
```

* But you can also load Numpy arrays or Scipy sparse matrices. Example for synfire chains:

::: {.columns}
::: {.column width=50%}


```python
w = np.array([[None]*pre.size]*post.size)

for i in range(post.size):
    w[i, (i-1)%pre.size] = 1.0

proj.connect_from_matrix(w)
```

:::
::: {.column width=50%}


```python
w = lil_matrix((pre.size, post.size))

for i in range(pre.size):
    w[pre.size, (i+1)%post.size] = 1.0

proj.connect_from_sparse(w)
```

:::
:::


# Compiling and running the simulation

* Once all populations and projections are created inside a network, you have to generate to the C++ code and compile it:

```python
net.compile()
```

* You can now manipulate all parameters/variables from Python.

* A simulation is simply run for a fixed duration in milliseconds with:

```python
net.simulate(1000.) # 1 second
```

* You can also run a simulation until a criteria is filled (e.g. first-spike), see:

<https://annarchy.github.io/manual/Network.html#early-stopping>

# Monitoring

* By default, a simulation is run in C++ without interaction with Python.

* You may want to record some variables (neural or synaptic) during the simulation with a `Monitor`:

```python
m = net.monitor(pop, ['v', 'r'])

n = net.monitor(proj, ['w'])
```

* After the simulation, you can retrieve the recordings with:

```python
recorded_v = m.get('v')

recorded_r = m.get('r')

recorded_w = n.get('w')
```

::: {.callout-warning}
Calling `get()` flushes the underlying arrays.
:::


::: {.callout-warning}
Recording projections can quickly fill up the RAM...
:::

# Notebook: Echo-State Network

[![Download JupyterNotebook](https://img.shields.io/badge/Download-Notebook-orange?style=for-the-badge&logo=Jupyter)](https://raw.githubusercontent.com/ANNarchy/ANNarchy.github.io/master/notebooks/RC.ipynb){target="_blank"} [![Open in Colab](https://img.shields.io/badge/Open_in-Colab-blue?style=for-the-badge&logo=Jupyter)](https://colab.research.google.com/github/ANNarchy/ANNarchy.github.io/blob/master/notebooks/RC.ipynb){target="_blank"}

$$
    \tau \frac{dx(t)}{dt} + x(t) = \sum_\text{input} W^\text{IN} \, r^\text{IN}(t) + g \,  \sum_\text{rec} W^\text{REC} \, r(t) + \xi(t)
$$

$$
    r(t) = \tanh(x(t))
$$


![](img/rc.jpg){width=60% fig-align="center"}


:::footer
@Jaeger2001 The "echo state" approach to analysing and training recurrent neural networks. Jacobs Universität Bremen.
:::

# Plasticity in rate-coded networks

* Synapses can also implement plasticity rules that will be evaluated after each neural update.

* Example of the Intrator & Cooper BCM learning rule:

$$\Delta w = \eta \, r^\text{pre} \, r^\text{post}  \,  (r^\text{post} - \mathbb{E}[(r^\text{post})^2])$$

```python
IBCM = ann.Synapse(
    parameters = dict(eta = 0.01, tau = 2000.0),
    equations = [
        ann.Variable(
            'tau * dtheta/dt + theta = post.r^2', 
            locality='semiglobal', method='exponential'),

        ann.Variable('dw/dt = eta * post.r * (post.r - theta) * pre.r', min=0.0),
    ],
    psp = "w * pre.r"
)
```

* Each synapse can access pre- and post-synaptic variables with `pre.` and `post.`.

* The `semiglobal` locality allows to do computations only once per post-synaptic neuron.

* `psp` optionally defines what will be summed by the post-synaptic neuron (e.g. `psp = "w * log(pre.r)"`).

# Plastic projections

* The synapse type just has to be passed to `Network.connect()`:

```python
proj = net.connect(inp, pop, 'exc', synapse=IBCM)
```

* Synaptic variables can be accessed as lists of lists for the whole projection:

```python
proj.w
proj.theta
```

or for a single post-synaptic neuron:

```python
proj[10].w
```


# Notebook: IBCM learning rule

[![Download JupyterNotebook](https://img.shields.io/badge/Download-Notebook-orange?style=for-the-badge&logo=Jupyter)](https://raw.githubusercontent.com/ANNarchy/ANNarchy.github.io/master/notebooks/BCM.ipynb){target="_blank"} [![Download JupyterNotebook](https://img.shields.io/badge/Open_in-Colab-blue?style=for-the-badge&logo=Jupyter)](https://colab.research.google.com/github/ANNarchy/ANNarchy.github.io/blob/master/notebooks/BCM.ipynb){target="_blank"}

:::columns
:::column

* Variant of the BCM (**B**ienenstock, **C**ooper, **M**unro, 1982) learning rule.

* The LTP and LTD depend on post-synaptic activity: homeostasis.

$$\Delta w = \eta \, r^\text{pre} \, r^\text{post}  \,  (r^\text{post} - \mathbb{E}[(r^\text{post})^2])$$

* See <http://www.scholarpedia.org/article/BCM_theory>

:::
:::column
![Source: <http://www.scholarpedia.org/article/BCM_theory>](img/bcm.png)
:::
:::

:::footer
@Intrator1992 Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. Neural Networks, 5(1).
:::


# Notebook: Reward-modulated RC network of Miconi (2017)

[![Download JupyterNotebook](https://img.shields.io/badge/Download-Notebook-orange?style=for-the-badge&logo=Jupyter)](https://raw.githubusercontent.com/ANNarchy/ANNarchy.github.io/master/notebooks/Miconi.ipynb){target="_blank"} [![Download JupyterNotebook](https://img.shields.io/badge/Open_in-Colab-blue?style=for-the-badge&logo=Jupyter)](https://colab.research.google.com/github/ANNarchy/ANNarchy.github.io/blob/master/notebooks/Miconi.ipynb){target="_blank"}

$$
    e_{i, j}(t) = e_{i, j}(t-1) + (r_i (t) \, x_j(t))^3
$$

$$
    \Delta w_{i, j} = - \eta \,  e_{i, j}(T) \, (R - R_\text{mean})
$$


![](img/miconi.png){width=80% fig-align="center"}


:::footer
@Miconi2017 Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks. eLife. doi:10.7554/eLife.20899
:::

# 4 - Spiking networks

# Spiking neurons

::: {.columns}
::: {.column width=45%}

* Spiking neurons must also define two additional fields:

    * `spike`: condition for emitting a spike.

    * `reset`: what happens after a spike is emitted (at the start of the refractory period).

* A refractory period in ms can also be specified.

![](img/LIF-threshold.png){width=100%}

:::
::: {.column width=55%}

* Example of the Leaky Integrate-and-Fire:

$$
    C \, \frac{d v(t)}{dt} = - g_L \, (v(t) - V_L) + I(t)
$$

$$
    \text{if} \; v(t) > V_T \; \text{emit a spike and reset.}
$$


```python
LIF = ann.Neuron(
    parameters = dict(
        C = 200.,
        g_L = 10.,
        E_L = -70.,
        v_T = 0.,
        v_r = -58.,
        I = 0.25,
    ),
    equations = [
        ann.Variable(
            'C * dv/dt = g_L * (E_L - v) + I', 
            init='E_L'
        ),     
    ],
    spike = "v >= v_T",
    reset = "v = v_r",
    refractory = 2.0
)
```

:::
:::

# Notebook: AdEx neuron - Adaptive exponential Integrate-and-fire

[![Download JupyterNotebook](https://img.shields.io/badge/Download-Notebook-orange?style=for-the-badge&logo=Jupyter)](https://raw.githubusercontent.com/ANNarchy/ANNarchy.github.io/master/notebooks/AdEx.ipynb){target="_blank"} [![Download JupyterNotebook](https://img.shields.io/badge/Open_in-Colab-blue?style=for-the-badge&logo=Jupyter)](https://colab.research.google.com/github/ANNarchy/ANNarchy.github.io/blob/master/notebooks/AdEx.ipynb){target="_blank"}



$$\tau \cdot \frac{dv (t)}{dt} = E_l - v(t) + g_\text{exc} (t) \, (E_\text{exc} - v(t)) + g_\text{inh} (t) \, (E_\text{inh} - v(t)) + I(t)$$

![](img/adex.png){width=50% fig-align="center"}

:::footer
@Brette2005 Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. Journal of Neurophysiology 94.
:::

# Conductances / currents

::: {.columns}
::: {.column width=60%}

* A pre-synaptic spike arriving to a spiking neuron increases the conductance/current `g_target` (e.g. `g_exc` or `g_inh`, depending on the projection).

```python
LIF = ann.Neuron(
    parameters = {'tau': 20.0},
    equations = [
        'tau * dv/dt + v = g_exc',     
    ],
    spike = "v >= 1",
    reset = "v = 0"
)
```

* Each spike increments instantaneously `g_target` from the synaptic efficiency `w` of the corresponding synapse.

* This can be changed through the `pre_spike` argument of the synapse model:

```python
default_synapse = ann.Synapse(
    pre_spike="g_target += w"
)
```

:::
::: {.column width=40%}

![](img/synaptictransmission.png)

:::
:::


# Conductances / currents

::: {.columns}
::: {.column width=60%}

* For **exponentially-decreasing** or **alpha-shaped** synapses, ODEs have to be introduced for the conductance/current.

```python
LIF = ann.Neuron(
    parameters = {'tau'=20.0},
    equations = [
        'tau * dv/dt + v = g_exc',     
        'tau_exc * dg_exc/dt = - g_exc',
    ],
    spike = "v >= 1",
    reset = "v = 0"
)
```

* The exponential numerical method should be preferred, as integration is exact.

```python
ann.Variable(
    'tau_exc * dg_exc/dt = - g_exc', 
        method='exponential'
)
```

:::
::: {.column width=40%}

![](img/synaptictransmission.png)

:::
:::


# Notebook: Synaptic transmission

:::columns
::: {.column width=60%}

[![Download JupyterNotebook](https://img.shields.io/badge/Download-Notebook-orange?style=for-the-badge&logo=Jupyter)](https://raw.githubusercontent.com/ANNarchy/ANNarchy.github.io/master/notebooks/SynapticTransmission.ipynb){target="_blank"} [![Download JupyterNotebook](https://img.shields.io/badge/Open_in-Colab-blue?style=for-the-badge&logo=Jupyter)](https://colab.research.google.com/github/ANNarchy/ANNarchy.github.io/blob/master/notebooks/SynapticTransmission.ipynb){target="_blank"}


```python
equations = [
    # Membrane potential
    'tau*dv/dt = (E_L- v) + g_a + g_b + alpha_c',
    
    # Exponentially decreasing
    ann.Variable(
        'tau_b * dg_b/dt = -g_b', 
        method='exponential'
    ),
    
    # Alpha-shaped
    ann.Variable(
        'tau_c * dg_c/dt = -g_c', 
        method='exponential'
    ),
    ann.Variable('''
        tau_c * dalpha_c/dt = 
            exp((tau_c - dt/2.0)/tau_c) * g_c 
                                    - alpha_c''', 
        method='exponential'
    ),
],
```

:::
::: {.column width=40%}
![](img/synaptictransmission.png)
:::
:::

# Notebook: COBA - Conductance-based E/I network

[![Download JupyterNotebook](https://img.shields.io/badge/Download-Notebook-orange?style=for-the-badge&logo=Jupyter)](https://raw.githubusercontent.com/ANNarchy/ANNarchy.github.io/master/notebooks/COBA.ipynb){target="_blank"} [![Download JupyterNotebook](https://img.shields.io/badge/Open_in-Colab-blue?style=for-the-badge&logo=Jupyter)](https://colab.research.google.com/github/ANNarchy/ANNarchy.github.io/blob/master/notebooks/COBA.ipynb){target="_blank"}


![](img/COBA.png){width=40% fig-align="center"}


$$\tau \, \frac{dv (t)}{dt} = E_l - v(t) + g_\text{exc} (t) \, (E_\text{exc} - v(t)) + g_\text{inh} (t) \, (E_\text{inh} - v(t)) + I(t)$$

:::footer
@Vogels2005 Signal propagation and logic gating in networks of integrate-and-fire neurons. The Journal of neuroscience 25.
:::


# Spiking synapses : Short-term plasticity (STP)

* Spiking synapses can define a `pre_spike` field, defining what happens when a pre-synaptic spike arrives at the synapse.

* `g_target` is an alias for the corresponding post-synaptic conductance: it will be replaced by `g_exc` or `g_inh` depending on how the synapse is used.

* By default, a pre-synaptic spike increments the post-synaptic conductance from `w`, but this can be changed.

```python
STP = ann.Synapse(
    parameters = dict(
        tau_rec = 100.0,
        tau_facil = 0.01,
        U = 0.5
    ),
    equations = [
        ann.Variable('dx/dt = (1 - x)/tau_rec', init = 1.0, method='event-driven'),
        ann.Variable('du/dt = (U - u)/tau_facil', init = 0.5, method='event-driven'),
    ],
    pre_spike="""
        g_target += w * u * x
        x *= (1 - u)
        u += U * (1 - u)
    """
)
```


# Notebook: STP

[![Download JupyterNotebook](https://img.shields.io/badge/Download-Notebook-orange?style=for-the-badge&logo=Jupyter)](https://raw.githubusercontent.com/ANNarchy/ANNarchy.github.io/master/notebooks/STP.ipynb){target="_blank"} [![Download JupyterNotebook](https://img.shields.io/badge/Open_in-Colab-blue?style=for-the-badge&logo=Jupyter)](https://colab.research.google.com/github/ANNarchy/ANNarchy.github.io/blob/master/notebooks/STP.ipynb){target="_blank"}

![](img/stp.png){width=70% fig-align="center"}

:::footer
@Tsodyks1997 The neural code between neocortical pyramidal neurons depends on neurotransmitter release probability. PNAS 94.
:::


# Spiking synapses : Example of Spike-Timing Dependent plasticity (STDP)

* `post_spike` similarly defines what happens when a post-synaptic spike is emitted.

```python
STDP = ann.Synapse(
    parameters = dict(
        tau_plus = 20.0, tau_minus = 20.0,
        A_plus = 0.01, A_minus = 0.01,
        w_min = 0.0, w_max = 1.0,
    ),
    equations = [
        ann.Variable('tau_plus  * dx/dt = -x', method='event-driven'), # pre-synaptic trace
        ann.Variable('tau_minus * dy/dt = -y', method='event-driven'), # post-synaptic trace
    ],
    pre_spike="""
        g_target += w
        x += A_plus * w_max
        w = clip(w + y, w_min , w_max)
    """,
    post_spike="""
        y -= A_minus * w_max
        w = clip(w + x, w_min , w_max)
    """)
```

# Notebook: STDP

[![Download JupyterNotebook](https://img.shields.io/badge/Download-Notebook-orange?style=for-the-badge&logo=Jupyter)](https://raw.githubusercontent.com/ANNarchy/ANNarchy.github.io/master/notebooks/STDP1.ipynb){target="_blank"} [![Download JupyterNotebook](https://img.shields.io/badge/Open_in-Colab-blue?style=for-the-badge&logo=Jupyter)](https://colab.research.google.com/github/ANNarchy/ANNarchy.github.io/blob/master/notebooks/STDP1.ipynb){target="_blank"}

$$
\begin{cases}
\tau^+ \, \dfrac{d x(t)}{dt} = - x(t) \\
\\
\tau^- \, \dfrac{d y(t)}{dt} = - y(t) \\
\end{cases}
$$

::: {.columns}
::: {.column width=50%}


![](img/stdp3.png)

:::
::: {.column width=50%}


![](img/stdp.png)

:::
:::

:::footer
@Bi2001 Synaptic Modification by Correlated Activity: Hebb’s Postulate Revisited. Annual Review of Neuroscience 24.
:::


# And much more...

* Standard populations (`SpikeSourceArray`, `TimedArray`, `PoissonPopulation`, `HomogeneousCorrelatedSpikeTrains`), OpenCV bindings.

* Standard neurons:

    * LeakyIntegrator, Izhikevich, IF_curr_exp, IF_cond_exp, IF_curr_alpha, IF_cond_alpha, HH_cond_exp, EIF_cond_exp_isfa_ista, EIF_cond_alpha_isfa_ista

* Standard synapses:

    * Hebb, Oja, IBCM, STP, STDP

* Parallel simulations with `parallel_run`.

* Convolutional and pooling layers.

* Hybrid rate-coded / spiking networks.

* Structural plasticity.

* BOLD monitors.

* Tensorboard visualization.

* ANN (keras) to SNN conversion tool.

RTFM: <https://annarchy.github.io>


# References (scroll down for the full list)


