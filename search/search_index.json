{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation for ANNarchy # ANNarchy (Artificial Neural Networks architect) is a neural simulator designed for distributed rate-coded or spiking neural networks. The core of the library is written in C++ and distributed using openMP or CUDA. It provides an interface in Python for the definition of the networks. It is released under the GNU GPL v2 or later . The source code of ANNarchy can be downloaded at: http://bitbucket.org/annarchy/annarchy The documentation can be found online at: http://annarchy.readthedocs.io A forum for discussion is set at: https://groups.google.com/forum/#!forum/annarchy Bug reports should be done through the Issue Tracker of ANNarchy on Bitbucket . You will need to create a free account. Citation If you use ANNarchy for your research, we would appreciate if you cite the following paper: Vitay J, Dinkelbach H\u00dc and Hamker FH (2015). ANNarchy: a code generation approach to neural simulations on parallel hardware. Frontiers in Neuroinformatics 9:19. doi:10.3389/fninf.2015.00019","title":"Home"},{"location":"#documentation-for-annarchy","text":"ANNarchy (Artificial Neural Networks architect) is a neural simulator designed for distributed rate-coded or spiking neural networks. The core of the library is written in C++ and distributed using openMP or CUDA. It provides an interface in Python for the definition of the networks. It is released under the GNU GPL v2 or later . The source code of ANNarchy can be downloaded at: http://bitbucket.org/annarchy/annarchy The documentation can be found online at: http://annarchy.readthedocs.io A forum for discussion is set at: https://groups.google.com/forum/#!forum/annarchy Bug reports should be done through the Issue Tracker of ANNarchy on Bitbucket . You will need to create a free account. Citation If you use ANNarchy for your research, we would appreciate if you cite the following paper: Vitay J, Dinkelbach H\u00dc and Hamker FH (2015). ANNarchy: a code generation approach to neural simulations on parallel hardware. Frontiers in Neuroinformatics 9:19. doi:10.3389/fninf.2015.00019","title":"Documentation for ANNarchy"},{"location":"Installation/","text":"Installation of ANNarchy # ANNarchy is designed to run on GNU/Linux and OSX. It relies mostly on a C++ compiler (g++ or clang++), Cython (C for Python extension) and Python (Numpy, Sympy) libraries. Installation on Windows is not possible. Download # The source code of ANNarchy can be downloaded on Bitbucket: git clone http://bitbucket.org/annarchy/annarchy.git Installation on GNU/Linux # Dependencies # ANNarchy depends on a number of packages which should be easily accessible on recent GNU/Linux distributions. The classical way to install these dependencies is through your package manager, or using full Python distributions such as Anaconda. Older versions of these packages may work but have not been tested. g++ >= 4.8 make >= 3.0 python >= 3.6 (with the development files, e.g. python-dev or python-devel ) cython >= 0.19 setuptools >= 0.6 numpy >= 1.8 sympy >= 0.7.4 scipy >= 0.12 matplotlib >= 2.0 Additionally, the following packages are optional but strongly recommended: pyqtgraph >= 0.9.8 (to visualize some of the provided examples. The OpenGL backend can also be needed). lxml >= 3.0 (to save the networks in .xml format). pandoc >= 2.0 (for reporting). tensorboardX (for the logging extension). To use the CUDA backend: the CUDA-SDK is available on the official website (we recommend to use at least a SDK version > 6.x). For further details on installation etc., please consider the corresponding Quickstart guides ( Quickstart_8.0 for the SDK 8.x). ANNarchy works with full Python distributions such as Anaconda, as well as in virtual environments. On a fresh install of Ubuntu 18.10, here are the minimal packages to install before ANNarchy (using python 3): sudo apt install build-essential git python3-dev python3-setuptools python3-scipy python3-matplotlib python3-sympy cython3 sudo apt install python3-pyqtgraph python3-pyqt4.qtopengl python3-lxml Installation # Using pip # Stable releases of ANNarchy are available on PyPi: sudo pip install ANNarchy or: pip install ANNarchy --user if you do not have administrator permissions. Omit --user in a virtual environment. You may also install directly the latest commit in the master (stable) or develop branches with: pip install git+https://bitbucket.org/annarchy/annarchy.git@master Using the source code # Installation of ANNarchy is possible using one of the three following methods: Local installation in home directory If you want to install ANNarchy in your home directory, type: python setup.py install --user The ANNarchy code will be installed in $HOME/.local/lib/pythonx.y/site-packages/ (replace \\'x.y\\' with your Python version) and automatically added to your PYTHONPATH . Global installation If you have administrator permissions, you can install ANNarchy in /usr/local by typing in the top-level directory: sudo python setup.py install This simply installs the code in /usr/local/lib/pythonx.y/dist-packages/ . Custom installation If you want to install ANNarchy in another directory (let's say in /path/to/repertory ), you should first set your Python path to this directory: export PYTHONPATH = $PYTHONPATH :/path/to/repertory/lib/pythonx.y/site-packages Again, replace 'x.y' with your Python version. If this directory does not exist, you should create it now. Don't forget to set this value in your ~/.bash_profile or ~/.bashrc to avoid typing this command before every session. You can then install ANNarchy by typing: python setup.py install --prefix = /path/to/repertory If you have multiple Python installations on your system (e.g. through Anaconda), you should update your LD_LIBRARY_PATH environment variable in .bashrc or bash_profile to point at the location of libpython3.6.so (or whatever version): export LD_LIBRARY_PATH = $HOME /anaconda3/lib: $LD_LIBRARY_PATH ANNarchy normally detects which python installation you are currently using, but helping it does not hurt... CUDA # If ANNarchy detects the CUDA SDK during installation, it will prepare the required modules. You need to make sure that the CUDA compiler nvcc is accessible in your path. The main problem with CUDA is that the binaries, headers and libraries are installed at different locations depending on the version: /usr/local/cuda , /usr/local/cuda-7.0 or /usr/local/cuda-8.0 . There is unfortunately no way for ANNarchy to guess the installation path. A first thing to help ANNarchy find the CUDA libraries is to define the LD_LIBRARY_PATH environment variable and have point at the lib64/ subfolder: export LD_LIBRARY_PATH = /usr/local/cuda-8.0/lib64/: $LD_LIBRARY_PATH This should in most cases work if you have only one CUDA installation. Otherwise, it is needed that you indicate where the CUDA libraries are, by modifying the ANNarchy configuration file located at ~/.config/ANNarchy/annarchy.json : { \"openmp\" : { \"compiler\" : \"g++\" , \"flags\" : \"-march=native -O2\" }, \"cuda\" : { \"compiler\" : \"nvcc\" , \"flags\" : \"\" , \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } Simply point the ['cuda']['path'] field to the right location (without lib64/ ). If the nvcc compiler binary is at a different location, the absolute path to the nvcc can be provided by ['cuda']['compiler'] field. It can happen that the detection of CUDA fails during installation, as some environment variables are not set. In this case try: sudo env \"PATH= $PATH \" \"LIBRARY_PATH= $LIBRARY_PATH \" python setup.py install Installation on MacOS X # Installation on MacOS X is in principle similar to GNU/Linux: python setup.py install ( --user or --prefix ) We advise using a full Python distribution such as Anaconda , which installs automatically all dependencies of ANNarchy, rather than using the old python provided by Apple. The only problem with Anaconda is that the compiler will use by default the Python shared library provided by Apple, leading to the following crash when simulating: Fatal Python error: PyThreadState_Get: no current thread Abort trap: 6 The solution is to set the environment variable DYLD_FALLBACK_LIBRARY_PATH to point at the correct library libpython3.6.dylib in your .bash_profile . For a standard Anaconda installation, this should be: export DYLD_FALLBACK_LIBRARY_PATH = $HOME /anaconda/lib: $DYLD_FALLBACK_LIBRARY_PATH Note The default compiler on OS X is clang-llvm. You should install the command_line_tools together with XCode in order to use it. For some reasons, this compiler is not compatible with OpenMP, so the models will only run sequentially. The CUDA backend is not available on OS X.","title":"Installation"},{"location":"Installation/#installation-of-annarchy","text":"ANNarchy is designed to run on GNU/Linux and OSX. It relies mostly on a C++ compiler (g++ or clang++), Cython (C for Python extension) and Python (Numpy, Sympy) libraries. Installation on Windows is not possible.","title":"Installation of ANNarchy"},{"location":"Installation/#download","text":"The source code of ANNarchy can be downloaded on Bitbucket: git clone http://bitbucket.org/annarchy/annarchy.git","title":"Download"},{"location":"Installation/#installation-on-gnulinux","text":"","title":"Installation on GNU/Linux"},{"location":"Installation/#dependencies","text":"ANNarchy depends on a number of packages which should be easily accessible on recent GNU/Linux distributions. The classical way to install these dependencies is through your package manager, or using full Python distributions such as Anaconda. Older versions of these packages may work but have not been tested. g++ >= 4.8 make >= 3.0 python >= 3.6 (with the development files, e.g. python-dev or python-devel ) cython >= 0.19 setuptools >= 0.6 numpy >= 1.8 sympy >= 0.7.4 scipy >= 0.12 matplotlib >= 2.0 Additionally, the following packages are optional but strongly recommended: pyqtgraph >= 0.9.8 (to visualize some of the provided examples. The OpenGL backend can also be needed). lxml >= 3.0 (to save the networks in .xml format). pandoc >= 2.0 (for reporting). tensorboardX (for the logging extension). To use the CUDA backend: the CUDA-SDK is available on the official website (we recommend to use at least a SDK version > 6.x). For further details on installation etc., please consider the corresponding Quickstart guides ( Quickstart_8.0 for the SDK 8.x). ANNarchy works with full Python distributions such as Anaconda, as well as in virtual environments. On a fresh install of Ubuntu 18.10, here are the minimal packages to install before ANNarchy (using python 3): sudo apt install build-essential git python3-dev python3-setuptools python3-scipy python3-matplotlib python3-sympy cython3 sudo apt install python3-pyqtgraph python3-pyqt4.qtopengl python3-lxml","title":"Dependencies"},{"location":"Installation/#installation","text":"","title":"Installation"},{"location":"Installation/#using-pip","text":"Stable releases of ANNarchy are available on PyPi: sudo pip install ANNarchy or: pip install ANNarchy --user if you do not have administrator permissions. Omit --user in a virtual environment. You may also install directly the latest commit in the master (stable) or develop branches with: pip install git+https://bitbucket.org/annarchy/annarchy.git@master","title":"Using pip"},{"location":"Installation/#using-the-source-code","text":"Installation of ANNarchy is possible using one of the three following methods: Local installation in home directory If you want to install ANNarchy in your home directory, type: python setup.py install --user The ANNarchy code will be installed in $HOME/.local/lib/pythonx.y/site-packages/ (replace \\'x.y\\' with your Python version) and automatically added to your PYTHONPATH . Global installation If you have administrator permissions, you can install ANNarchy in /usr/local by typing in the top-level directory: sudo python setup.py install This simply installs the code in /usr/local/lib/pythonx.y/dist-packages/ . Custom installation If you want to install ANNarchy in another directory (let's say in /path/to/repertory ), you should first set your Python path to this directory: export PYTHONPATH = $PYTHONPATH :/path/to/repertory/lib/pythonx.y/site-packages Again, replace 'x.y' with your Python version. If this directory does not exist, you should create it now. Don't forget to set this value in your ~/.bash_profile or ~/.bashrc to avoid typing this command before every session. You can then install ANNarchy by typing: python setup.py install --prefix = /path/to/repertory If you have multiple Python installations on your system (e.g. through Anaconda), you should update your LD_LIBRARY_PATH environment variable in .bashrc or bash_profile to point at the location of libpython3.6.so (or whatever version): export LD_LIBRARY_PATH = $HOME /anaconda3/lib: $LD_LIBRARY_PATH ANNarchy normally detects which python installation you are currently using, but helping it does not hurt...","title":"Using the source code"},{"location":"Installation/#cuda","text":"If ANNarchy detects the CUDA SDK during installation, it will prepare the required modules. You need to make sure that the CUDA compiler nvcc is accessible in your path. The main problem with CUDA is that the binaries, headers and libraries are installed at different locations depending on the version: /usr/local/cuda , /usr/local/cuda-7.0 or /usr/local/cuda-8.0 . There is unfortunately no way for ANNarchy to guess the installation path. A first thing to help ANNarchy find the CUDA libraries is to define the LD_LIBRARY_PATH environment variable and have point at the lib64/ subfolder: export LD_LIBRARY_PATH = /usr/local/cuda-8.0/lib64/: $LD_LIBRARY_PATH This should in most cases work if you have only one CUDA installation. Otherwise, it is needed that you indicate where the CUDA libraries are, by modifying the ANNarchy configuration file located at ~/.config/ANNarchy/annarchy.json : { \"openmp\" : { \"compiler\" : \"g++\" , \"flags\" : \"-march=native -O2\" }, \"cuda\" : { \"compiler\" : \"nvcc\" , \"flags\" : \"\" , \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } Simply point the ['cuda']['path'] field to the right location (without lib64/ ). If the nvcc compiler binary is at a different location, the absolute path to the nvcc can be provided by ['cuda']['compiler'] field. It can happen that the detection of CUDA fails during installation, as some environment variables are not set. In this case try: sudo env \"PATH= $PATH \" \"LIBRARY_PATH= $LIBRARY_PATH \" python setup.py install","title":"CUDA"},{"location":"Installation/#installation-on-macos-x","text":"Installation on MacOS X is in principle similar to GNU/Linux: python setup.py install ( --user or --prefix ) We advise using a full Python distribution such as Anaconda , which installs automatically all dependencies of ANNarchy, rather than using the old python provided by Apple. The only problem with Anaconda is that the compiler will use by default the Python shared library provided by Apple, leading to the following crash when simulating: Fatal Python error: PyThreadState_Get: no current thread Abort trap: 6 The solution is to set the environment variable DYLD_FALLBACK_LIBRARY_PATH to point at the correct library libpython3.6.dylib in your .bash_profile . For a standard Anaconda installation, this should be: export DYLD_FALLBACK_LIBRARY_PATH = $HOME /anaconda/lib: $DYLD_FALLBACK_LIBRARY_PATH Note The default compiler on OS X is clang-llvm. You should install the command_line_tools together with XCode in order to use it. For some reasons, this compiler is not compatible with OpenMP, so the models will only run sequentially. The CUDA backend is not available on OS X.","title":"Installation on MacOS X"},{"location":"License/","text":"License # GNU GENERAL PUBLIC LICENSE # Version 2, June 1991 Copyright (C) 1989, 1991 Free Software Foundation, Inc. 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble # The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Lesser General Public License instead.) You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things. To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it. For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software. Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations. Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION # 0. This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The \"Program\", below, refers to any such program or work, and a \"work based on the Program\" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term \"modification\".) Each licensee is addressed as \"you\". Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does. 1. You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program. You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee. 2. You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions: a) You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change. b) You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License. c) If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.) These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it. Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program. In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License. 3. You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following: a) Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, b) Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, c) Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.) The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable. If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code. 4. You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. 5. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it. 6. Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License. 7. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program. If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances. It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice. This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License. 8. If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License. 9. The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation. 10. If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally. NO WARRANTY 11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. 12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.","title":"License"},{"location":"License/#license","text":"","title":"License"},{"location":"License/#gnu-general-public-license","text":"Version 2, June 1991 Copyright (C) 1989, 1991 Free Software Foundation, Inc. 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.","title":"GNU GENERAL PUBLIC LICENSE"},{"location":"License/#preamble","text":"The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Lesser General Public License instead.) You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things. To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it. For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software. Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations. Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all. The precise terms and conditions for copying, distribution and modification follow.","title":"Preamble"},{"location":"License/#terms-and-conditions-for-copying-distribution-and-modification","text":"0. This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The \"Program\", below, refers to any such program or work, and a \"work based on the Program\" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term \"modification\".) Each licensee is addressed as \"you\". Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does. 1. You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program. You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee. 2. You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions: a) You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change. b) You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License. c) If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.) These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it. Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program. In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License. 3. You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following: a) Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, b) Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, c) Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.) The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable. If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code. 4. You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. 5. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it. 6. Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License. 7. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program. If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances. It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice. This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License. 8. If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License. 9. The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation. 10. If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally. NO WARRANTY 11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. 12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.","title":"TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION"},{"location":"API/ANNarchy/","text":"Top-level methods # These methods are directly available in the main namespace when importing ANNarhcy: from ANNarchy import * Note that numpy is automatically imported as: import numpy as np Configuration and compilation # Contrary to other simulators, ANNarchy is entirely based on code generation. It provides a set of first level functions to ensure the network is correctly created. It is important to call these functions in the right order. setup ( ** keyValueArgs ) # The setup function is used to configure ANNarchy simulation environment. It takes various optional arguments: dt: simulation step size (default: 1.0 ms). paradigm: parallel framework for code generation. Accepted values: \"openmp\" or \"cuda\" (default: \"openmp\"). method: default method to numerize ODEs. Default is the explicit forward Euler method ('explicit'). precision: default floating precision for variables in ANNarchy. Accepted values: \"float\" or \"double\" (default: \"double\") only_int_idx_type: if set to True (default) only signed integers are used to store pre-/post-synaptic ranks which was default until 4.7. If set to False, the index type used in a single projection is selected based on the size of the corresponding populations. num_threads: number of treads used by openMP (overrides the environment variable OMP_NUM_THREADS when set, default = None). visible_cores: allows a fine-grained control which cores are useable for the created threads (default = [] for no limitation). It can be used to limit created openMP threads to a physical socket. structural_plasticity: allows synapses to be dynamically added/removed during the simulation (default: False). seed: the seed (integer) to be used in the random number generators (default = -1 is equivalent to time(NULL)). disable_parallel_rng: determines if random numbers drawn from distributions are generated from a single source (default: True). If this flag is set to true only one RNG source is used und the values are drawn by one thread which reduces parallel performance (this is the behavior of all ANNarchy versions prior to 4.7). If set to false a seed sequence is generated to allow usage of one RNG per thread. Please note, that this flag won't effect the GPUs which draw from multiple sources anyways. use_seed_seq: If parallel RNGs are used the single generators need to be initialized. By default (use_seed_seq == True) we use the STL seed sequence to generate a list of seeds from the given master seed ( seed argument). If set to False, we use a simpler initialization strategy adapted from NEST. use_cpp_connectors: For some of the default connectivity methods of ANNarchy we offer a CPP-side construction of the pattern to improve the initialization time (default=False). For maximum performance the disable_parallel_rng should be set to False to allow a parallel construction of the pattern. disable_split_matrix: determines if projections can use thread-local allocation. If set to True (default) no thread local allocation is allowed. This equals the behavior of ANNarchy until 4.7. If set to False the code generator can use sliced versions if they are available. disable_SIMD_SpMV: determines if the hand-written implementation is used (by default True) if the current hardware platform and used sparse matrix format does support the vectorization). Disabling is intended for performance analysis. The following parameters are mainly for debugging and profiling, and should be ignored by most users: verbose: shows details about compilation process on console (by default False). Additional some information of the network construction will be shown. suppress_warnings: if True, warnings (e. g. from the mathematical parser) are suppressed. show_time: if True, initialization times are shown. Attention: verbose should be set to True additionally. Note: This function should be used before any other functions of ANNarchy (including importing a network definition), right after from ANNarchy import * : from ANNarchy import * setup ( dt = 1.0 , method = 'midpoint' , cores = 2 ) Source code in ANNarchy/core/Global.py def setup ( ** keyValueArgs ): \"\"\" The setup function is used to configure ANNarchy simulation environment. It takes various optional arguments: * dt: simulation step size (default: 1.0 ms). * paradigm: parallel framework for code generation. Accepted values: \"openmp\" or \"cuda\" (default: \"openmp\"). * method: default method to numerize ODEs. Default is the explicit forward Euler method ('explicit'). * precision: default floating precision for variables in ANNarchy. Accepted values: \"float\" or \"double\" (default: \"double\") * only_int_idx_type: if set to True (default) only signed integers are used to store pre-/post-synaptic ranks which was default until 4.7. If set to False, the index type used in a single projection is selected based on the size of the corresponding populations. * num_threads: number of treads used by openMP (overrides the environment variable ``OMP_NUM_THREADS`` when set, default = None). * visible_cores: allows a fine-grained control which cores are useable for the created threads (default = [] for no limitation). It can be used to limit created openMP threads to a physical socket. * structural_plasticity: allows synapses to be dynamically added/removed during the simulation (default: False). * seed: the seed (integer) to be used in the random number generators (default = -1 is equivalent to time(NULL)). * disable_parallel_rng: determines if random numbers drawn from distributions are generated from a single source (default: True). If this flag is set to true only one RNG source is used und the values are drawn by one thread which reduces parallel performance (this is the behavior of all ANNarchy versions prior to 4.7). If set to false a seed sequence is generated to allow usage of one RNG per thread. Please note, that this flag won't effect the GPUs which draw from multiple sources anyways. * use_seed_seq: If parallel RNGs are used the single generators need to be initialized. By default (use_seed_seq == True) we use the STL seed sequence to generate a list of seeds from the given master seed (*seed* argument). If set to False, we use a simpler initialization strategy adapted from NEST. * use_cpp_connectors: For some of the default connectivity methods of ANNarchy we offer a CPP-side construction of the pattern to improve the initialization time (default=False). For maximum performance the disable_parallel_rng should be set to False to allow a parallel construction of the pattern. * disable_split_matrix: determines if projections can use thread-local allocation. If set to *True* (default) no thread local allocation is allowed. This equals the behavior of ANNarchy until 4.7. If set to *False* the code generator can use sliced versions if they are available. * disable_SIMD_SpMV: determines if the hand-written implementation is used (by default True) if the current hardware platform and used sparse matrix format does support the vectorization). Disabling is intended for performance analysis. The following parameters are mainly for debugging and profiling, and should be ignored by most users: * verbose: shows details about compilation process on console (by default False). Additional some information of the network construction will be shown. * suppress_warnings: if True, warnings (e. g. from the mathematical parser) are suppressed. * show_time: if True, initialization times are shown. Attention: verbose should be set to True additionally. **Note:** This function should be used before any other functions of ANNarchy (including importing a network definition), right after `from ANNarchy import *`: ```python from ANNarchy import * setup(dt=1.0, method='midpoint', cores=2) ``` \"\"\" if len ( _network [ 0 ][ 'populations' ]) > 0 or len ( _network [ 0 ][ 'projections' ]) > 0 or len ( _network [ 0 ][ 'monitors' ]) > 0 : if 'dt' in keyValueArgs : _warning ( 'setup(): populations or projections have already been created. Changing dt now might lead to strange behaviors with the synaptic delays (internally generated in steps, not ms)...' ) if 'precision' in keyValueArgs : _warning ( 'setup(): populations or projections have already been created. Changing precision now might lead to strange behaviors...' ) for key in keyValueArgs : if key in config . keys (): config [ key ] = keyValueArgs [ key ] else : _warning ( 'setup(): unknown key:' , key ) if key == 'seed' : # also seed numpy np . random . seed ( keyValueArgs [ key ]) compile ( directory = 'annarchy' , clean = False , populations = None , projections = None , compiler = 'default' , compiler_flags = 'default' , add_sources = '' , extra_libs = '' , cuda_config = { 'device' : 0 }, annarchy_json = '' , silent = False , debug_build = False , profile_enabled = False , net_id = 0 ) # This method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation. The compiler , compiler_flags and part of cuda_config take their default value from the configuration file ~/.config/ANNarchy/annarchy.json . The following arguments are for internal development use only: debug_build : creates a debug version of ANNarchy, which logs the creation of objects and some other data (default: False). profile_enabled : creates a profilable version of ANNarchy, which logs several computation timings (default: False). Parameters: Name Type Description Default directory name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\". 'annarchy' clean boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False). False populations list of populations which should be compiled. If set to None, all available populations will be used. None projections list of projection which should be compiled. If set to None, all available projections will be used. None compiler C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++]. 'default' compiler_flags platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended. 'default' cuda_config dictionary defining the CUDA configuration for each population and projection. {'device': 0} annarchy_json compiler flags etc can be stored in a .json file normally placed in the home directory (see comment below). With this flag one can directly assign a file location. '' silent defines if status message like \"Compiling... OK\" should be printed. False Source code in ANNarchy/generator/Compiler.py def compile ( directory = 'annarchy' , clean = False , populations = None , projections = None , compiler = \"default\" , compiler_flags = \"default\" , add_sources = \"\" , extra_libs = \"\" , cuda_config = { 'device' : 0 }, annarchy_json = \"\" , silent = False , debug_build = False , profile_enabled = False , net_id = 0 ): \"\"\" This method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation. The ``compiler``, ``compiler_flags`` and part of ``cuda_config`` take their default value from the configuration file ``~/.config/ANNarchy/annarchy.json``. The following arguments are for internal development use only: * **debug_build**: creates a debug version of ANNarchy, which logs the creation of objects and some other data (default: False). * **profile_enabled**: creates a profilable version of ANNarchy, which logs several computation timings (default: False). :param directory: name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\". :param clean: boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False). :param populations: list of populations which should be compiled. If set to None, all available populations will be used. :param projections: list of projection which should be compiled. If set to None, all available projections will be used. :param compiler: C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++]. :param compiler_flags: platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended. :param cuda_config: dictionary defining the CUDA configuration for each population and projection. :param annarchy_json: compiler flags etc can be stored in a .json file normally placed in the home directory (see comment below). With this flag one can directly assign a file location. :param silent: defines if status message like \"Compiling... OK\" should be printed. \"\"\" # Check if the network has already been compiled if Global . _network [ net_id ][ 'compiled' ]: Global . _print ( \"\"\"compile(): the network has already been compiled, doing nothing. If you are re-running a Jupyter notebook, you should call `clear()` right after importing ANNarchy in order to reset everything.\"\"\" ) return # Get the command-line arguments parser = setup_parser () options , unknown = parser . parse_known_args () if len ( unknown ) > 0 and Global . config [ 'verbose' ]: Global . _warning ( 'unrecognized command-line arguments:' , unknown ) # if the parameters set on command-line they overwrite Global.config if options . num_threads is not None : Global . config [ 'num_threads' ] = options . num_threads if options . visible_cores is not None : try : core_list = [ int ( x ) for x in options . visible_cores . split ( \",\" )] Global . config [ 'visible_cores' ] = core_list except : Global . _error ( \"As argument for 'visible_cores' a comma-seperated list of integers is expected.\" ) # Get CUDA configuration if options . gpu_device >= 0 : Global . config [ 'paradigm' ] = \"cuda\" cuda_config [ 'device' ] = int ( options . gpu_device ) # Check that a single backend is chosen if ( options . num_threads != None ) and ( options . gpu_device >= 0 ): Global . _error ( 'CUDA and openMP can not be active at the same time, please check your command line arguments.' ) # Verbose if options . verbose is not None : Global . config [ 'verbose' ] = options . verbose # Precision if options . precision is not None : Global . config [ 'precision' ] = options . precision # Profiling if options . profile != None : profile_enabled = options . profile Global . config [ 'profiling' ] = options . profile Global . config [ 'profile_out' ] = options . profile_out if profile_enabled != False and options . profile == None : # Profiling enabled due compile() Global . config [ 'profiling' ] = True # Debug if not debug_build : debug_build = options . debug # debug build Global . config [ \"debug\" ] = debug_build # Clean clean = options . clean or clean # enforce rebuild # Populations to compile if populations is None : # Default network populations = Global . _network [ net_id ][ 'populations' ] # Projections to compile if projections is None : # Default network projections = Global . _network [ net_id ][ 'projections' ] # Compiling directory annarchy_dir = os . getcwd () + '/' + directory if not annarchy_dir . endswith ( '/' ): annarchy_dir += '/' # Turn OMP off for MacOS if ( Global . _check_paradigm ( \"openmp\" ) and Global . config [ 'num_threads' ] > 1 and sys . platform == \"darwin\" ): Global . _warning ( \"OpenMP is not supported on Mac OS yet\" ) Global . config [ 'num_threads' ] = 1 # Test if the current ANNarchy version is newer than what was used to create the subfolder from pkg_resources import parse_version if os . path . isfile ( annarchy_dir + '/release' ): with open ( annarchy_dir + '/release' , 'r' ) as rfile : prev_release = rfile . read () . strip () prev_paradigm = '' # HD (03.08.2016): # in ANNarchy 4.5.7b I added also the paradigm to the release tag. # This if clause can be removed in later releases (TODO) if prev_release . find ( ',' ) != - 1 : prev_paradigm , prev_release = prev_release . split ( ', ' ) else : # old release tag clean = True if parse_version ( prev_release ) < parse_version ( ANNarchy . __release__ ): clean = True elif prev_paradigm != Global . config [ 'paradigm' ]: clean = True else : clean = True # for very old versions # Check if the last compilation was successful if os . path . isfile ( annarchy_dir + '/compilation' ): with open ( annarchy_dir + '/compilation' , 'r' ) as rfile : res = rfile . read () if res . strip () == \"0\" : # the last compilation failed clean = True else : clean = True # Manage the compilation subfolder _folder_management ( annarchy_dir , profile_enabled , clean , net_id ) # Create a Compiler object compiler = Compiler ( annarchy_dir = annarchy_dir , clean = clean , compiler = compiler , compiler_flags = compiler_flags , add_sources = add_sources , extra_libs = extra_libs , path_to_json = annarchy_json , silent = silent , cuda_config = cuda_config , debug_build = debug_build , profile_enabled = profile_enabled , populations = populations , projections = projections , net_id = net_id ) # Code Generation compiler . generate () if Global . config [ 'verbose' ]: net_str = \"\" if compiler . net_id == 0 else str ( compiler . net_id ) + \" \" Global . _print ( 'Construct network ' + net_str + '...' , end = \" \" ) # Create the Python objects _instantiate ( compiler . net_id , cuda_config = compiler . cuda_config , user_config = compiler . user_config ) # NormProjections require an update of afferent projections _update_num_aff_connections ( compiler . net_id ) if Global . config [ 'verbose' ]: Global . _print ( 'OK' ) clear () # Clears all variables (erasing already defined populations, projections, monitors and constants), as if you had just imported ANNarchy. Useful when re-running Jupyter/IPython notebooks multiple times: from ANNarchy import * clear () Source code in ANNarchy/core/Global.py def clear (): \"\"\" Clears all variables (erasing already defined populations, projections, monitors and constants), as if you had just imported ANNarchy. Useful when re-running Jupyter/IPython notebooks multiple times: ```python from ANNarchy import * clear() ``` \"\"\" # Reset objects global _objects _objects = { 'functions' : [], 'neurons' : [], 'synapses' : [], 'constants' : [], } # Reinitialize initial state global _network _network . clear () # # Configuration # config = dict( # { # 'dt' : 1.0, # 'verbose': False, # 'show_time': False, # 'suppress_warnings': False, # 'num_threads': 1, # 'paradigm': \"openmp\", # 'method': \"explicit\", # 'precision': \"double\", # 'seed': -1, # 'structural_plasticity': False, # 'profiling': False, # 'profile_out': None # } # ) Simulation # Different methods are available to run the simulation: simulate ( duration , measure_time = False , progress_bar = False , callbacks = True , net_id = 0 ) # Simulates the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step dt declared in setup() (default: 1ms): simulate ( 1000.0 ) Parameters: Name Type Description Default duration the duration in milliseconds. required measure_time defines whether the simulation time should be printed. Default: False. False progress_bar defines whether a progress bar should be printed. Default: False False callbacks defines if the callback method (decorator every should be called). Default: True. True Source code in ANNarchy/core/Simulate.py def simulate ( duration , measure_time = False , progress_bar = False , callbacks = True , net_id = 0 ): \"\"\" Simulates the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step ``dt`` declared in ``setup()`` (default: 1ms): ```python simulate(1000.0) ``` :param duration: the duration in milliseconds. :param measure_time: defines whether the simulation time should be printed. Default: False. :param progress_bar: defines whether a progress bar should be printed. Default: False :param callbacks: defines if the callback method (decorator ``every`` should be called). Default: True. \"\"\" if Global . _profiler : t0 = time . time () if not _network [ net_id ][ 'instance' ]: _error ( 'simulate(): the network is not compiled yet.' ) # Compute the number of steps nb_steps = ceil ( float ( duration ) / dt ()) if measure_time : tstart = time . time () if callbacks and _callbacks_enabled [ net_id ] and len ( _callbacks [ net_id ]) > 0 : _simulate_with_callbacks ( duration , net_id ) else : _network [ net_id ][ 'instance' ] . pyx_run ( nb_steps , progress_bar ) if measure_time : if net_id > 0 : _print ( 'Simulating' , duration / 1000.0 , 'seconds of the network' , net_id , 'took' , time . time () - tstart , 'seconds.' ) else : _print ( 'Simulating' , duration / 1000.0 , 'seconds of the network took' , time . time () - tstart , 'seconds.' ) if Global . _profiler : t1 = time . time () print ( Global . _profiler ) Global . _profiler . add_entry ( t0 , t1 , \"simulate\" , \"simulate\" ) simulate_until ( max_duration , population , operator = 'and' , measure_time = False , net_id = 0 ) # Runs the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: pop1 = Population ( ... , stop_condition = \"r > 1.0 : any\" ) compile () simulate_until ( max_duration = 1000.0 . population = pop1 ) Parameters: Name Type Description Default max_duration the maximum duration of the simulation in milliseconds. required population the (list of) population whose stop_condition should be checked to stop the simulation. required operator operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). 'and' measure_time defines whether the simulation time should be printed (default=False). False Returns: Type Description the actual duration of the simulation in milliseconds. Source code in ANNarchy/core/Simulate.py def simulate_until ( max_duration , population , operator = 'and' , measure_time = False , net_id = 0 ): \"\"\" Runs the network for the maximal duration in milliseconds. If the ``stop_condition`` defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: ```python pop1 = Population( ..., stop_condition = \"r > 1.0 : any\") compile() simulate_until(max_duration=1000.0. population=pop1) ``` :param max_duration: the maximum duration of the simulation in milliseconds. :param population: the (list of) population whose ``stop_condition`` should be checked to stop the simulation. :param operator: operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). :param measure_time: defines whether the simulation time should be printed (default=False). :return: the actual duration of the simulation in milliseconds. \"\"\" if not _network [ net_id ][ 'instance' ]: _error ( 'simulate_until(): the network is not compiled yet.' ) nb_steps = ceil ( float ( max_duration ) / dt ()) if not isinstance ( population , list ): population = [ population ] if measure_time : tstart = time . time () nb = _network [ net_id ][ 'instance' ] . pyx_run_until ( nb_steps , [ pop . id for pop in population ], True if operator == 'and' else False ) sim_time = float ( nb ) / dt () if measure_time : _print ( 'Simulating' , nb / dt () / 1000.0 , 'seconds of the network took' , time . time () - tstart , 'seconds.' ) return sim_time step ( net_id = 0 ) # Performs a single simulation step (duration = dt ). Source code in ANNarchy/core/Simulate.py def step ( net_id = 0 ): \"\"\" Performs a single simulation step (duration = ``dt``). \"\"\" if not _network [ net_id ][ 'instance' ]: _error ( 'simulate_until(): the network is not compiled yet.' ) _network [ net_id ][ 'instance' ] . pyx_step () every # Decorator to declare a callback method that will be called periodically during the simulation. Example of setting increasing inputs to a population every 100 ms, with an offset of 90 ms (or -10 ms relative to the period): @every ( period = 100. , offset =- 10. ) def step_input ( n ): pop . I = float ( n ) / 100. simulate ( 10000. ) step_input() will be called at times 90, 190, ..., 9990 ms during the call to simulate() . The method must accept only n as parameter (an integer being 0 the first time the method is called, and incremented afterwards) and can not return anything. The times at which the method is called are relative to the time when simulate() is called (if t is already 150 before calling simulate() , the first call will then be made at t=240 with the previous example). If multiple callbacks are defined, they will be called in the order of their declaration if they occur at the same time. __init__ ( self , period , offset = 0.0 , wait = 0.0 , net_id = 0 ) special # Parameters: Name Type Description Default period interval in ms between two calls to the function. If less than dt , will be called every step. required offset by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period. 0.0 wait allows to wait for a certain amount of time (in ms) before starting to call the method. wait can be combined with offset , so if period=100. , offset=50. and wait=500. , the first call will be made 550 ms after the call to simulate() 0.0 Source code in ANNarchy/core/Simulate.py def __init__ ( self , period , offset = 0. , wait = 0.0 , net_id = 0 ): \"\"\" :param period: interval in ms between two calls to the function. If less than ``dt``, will be called every step. :param offset: by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period. :param wait: allows to wait for a certain amount of time (in ms) before starting to call the method. ``wait`` can be combined with ``offset``, so if ``period=100.``, ``offset=50.`` and ``wait=500.``, the first call will be made 550 ms after the call to ``simulate()`` \"\"\" self . period = max ( float ( period ), dt ()) self . offset = min ( float ( offset ), self . period ) self . wait = max ( float ( wait ), 0.0 ) _callbacks [ net_id ] . append ( self ) enable_callbacks ( net_id = 0 ) # Enables all declared callbacks for the network. Source code in ANNarchy/core/Simulate.py def enable_callbacks ( net_id = 0 ): \"\"\" Enables all declared callbacks for the network. \"\"\" _callbacks_enabled [ net_id ] = True disable_callbacks ( net_id = 0 ) # Disables all callbacks for the network. Source code in ANNarchy/core/Simulate.py def disable_callbacks ( net_id = 0 ): \"\"\" Disables all callbacks for the network. \"\"\" _callbacks_enabled [ net_id ] = False clear_all_callbacks ( net_id = 0 ) # Clears the list of declared callbacks for the network. Cannot be undone! Source code in ANNarchy/core/Simulate.py def clear_all_callbacks ( net_id = 0 ): \"\"\" Clears the list of declared callbacks for the network. Cannot be undone! \"\"\" _callbacks [ net_id ] . clear () Reset the network # If you want to run multiple experiments with the same network, or if your experiment setup requires a pre learning phase, you can reset selectively neural or synaptic variables to their initial values. reset ( populations = True , projections = False , synapses = False , net_id = 0 ) # Reinitialises the network to its state before the call to compile. The network time will be set to 0ms. Parameters: Name Type Description Default populations if True (default), the neural parameters and variables will be reset to their initial value. True projections if True, the synaptic parameters and variables (except the connections) will be reset (default=False). False synapses if True, the synaptic weights will be erased and recreated (default=False). False Source code in ANNarchy/core/Global.py def reset ( populations = True , projections = False , synapses = False , net_id = 0 ): \"\"\" Reinitialises the network to its state before the call to compile. The network time will be set to 0ms. :param populations: if True (default), the neural parameters and variables will be reset to their initial value. :param projections: if True, the synaptic parameters and variables (except the connections) will be reset (default=False). :param synapses: if True, the synaptic weights will be erased and recreated (default=False). \"\"\" if populations : for pop in _network [ net_id ][ 'populations' ]: pop . reset () # pop.reset only clears spike container with no or uniform delay for proj in _network [ net_id ][ 'projections' ]: if hasattr ( proj . cyInstance , 'reset_ring_buffer' ): proj . cyInstance . reset_ring_buffer () if synapses and not projections : _warning ( \"reset(): if synapses is set to true this automatically enables projections==true\" ) projections = True if projections : for proj in _network [ net_id ][ 'projections' ]: proj . reset ( attributes =- 1 , synapses = synapses ) _network [ net_id ][ 'instance' ] . set_time ( 0 ) Access to populations # get_population ( name , net_id = 0 ) # Returns the population with the given name . Parameters: Name Type Description Default name name of the population. required Returns: Type Description The requested Population object if existing, None otherwise. Source code in ANNarchy/core/Global.py def get_population ( name , net_id = 0 ): \"\"\" Returns the population with the given ``name``. :param name: name of the population. :return: The requested ``Population`` object if existing, ``None`` otherwise. \"\"\" for pop in _network [ net_id ][ 'populations' ]: if pop . name == name : return pop _warning ( \"get_population(): the population\" , name , \"does not exist.\" ) return None get_projection ( name , net_id = 0 ) # Returns the projection with the given name . Parameters: Name Type Description Default name name of the projection. required Returns: Type Description The requested Projection object if existing, None otherwise. Source code in ANNarchy/core/Global.py def get_projection ( name , net_id = 0 ): \"\"\" Returns the projection with the given *name*. :param name: name of the projection. :return: The requested ``Projection`` object if existing, ``None`` otherwise. \"\"\" for proj in _network [ net_id ][ 'projections' ]: if proj . name == name : return proj _warning ( \"get_projection(): the projection\" , name , \"does not exist.\" ) return None Functions # add_function ( function ) # Defines a global function which can be used by all neurons and synapses. The function must have only one return value and use only the passed arguments. Examples of valid functions: logistic ( x ) = 1 / ( 1 + exp ( - x )) piecewise ( x , a , b ) = if x < a : a else : if x > b : b else : x Please refer to the manual to know the allowed mathematical functions. Source code in ANNarchy/core/Global.py def add_function ( function ): \"\"\" Defines a global function which can be used by all neurons and synapses. The function must have only one return value and use only the passed arguments. Examples of valid functions: ```python logistic(x) = 1 / (1 + exp(-x)) piecewise(x, a, b) = if x < a: a else: if x > b : b else: x ``` Please refer to the manual to know the allowed mathematical functions. \"\"\" name = function . split ( '(' )[ 0 ] _objects [ 'functions' ] . append ( ( name , function )) functions ( name , net_id = 0 ) # Allows to access a global function defined with add_function and use it from Python using arrays after compilation . The name of the function is not added to the global namespace to avoid overloading. add_function ( \"logistic(x) = 1. / (1. + exp(-x))\" ) compile () result = functions ( 'logistic' )([ 0. , 1. , 2. , 3. , 4. ]) Only lists or 1D Numpy arrays can be passed as arguments, not single values nor multidimensional arrays. When passing several arguments, make sure they have the same size. Source code in ANNarchy/core/Global.py def functions ( name , net_id = 0 ): \"\"\" Allows to access a global function defined with ``add_function`` and use it from Python using arrays **after compilation**. The name of the function is not added to the global namespace to avoid overloading. ```python add_function(\"logistic(x) = 1. / (1. + exp(-x))\") compile() result = functions('logistic')([0., 1., 2., 3., 4.]) ``` Only lists or 1D Numpy arrays can be passed as arguments, not single values nor multidimensional arrays. When passing several arguments, make sure they have the same size. \"\"\" try : func = getattr ( _network [ net_id ][ 'instance' ], 'func_' + name ) except : _error ( 'call to' , name , ': the function is not compiled yet.' ) return func Constants # Constant # Constant parameter that can be used by all neurons and synapses. The class Constant derives from float , so any legal operation on floats (addition, multiplication) can be used. If a Neuron/Synapse defines a parameter with the same name, the constant parameters will not be visible. Example: tau = Constant ( 'tau' , 20 ) factor = Constant ( 'factor' , 0.1 ) real_tau = Constant ( 'real_tau' , tau * factor ) neuron = Neuron ( equations = ''' real_tau*dr/dt + r =1.0 ''' ) The value of the constant can be changed anytime with the set() method. Assignments will have no effect (e.g. tau = 10.0 only creates a new float). The value of constants defined as combination of other constants ( real_tau ) is not updated if the value of these constants changes (changing tau with tau.set(10.0) will not modify the value of real_tau ). __init__ ( self , name , value , net_id = 0 ) special # Parameters: Name Type Description Default name name of the constant (unique), which can be used in equations. required value the value of the constant, which must be a float, or a combination of Constants. required Source code in ANNarchy/core/Global.py def __init__ ( self , name , value , net_id = 0 ): \"\"\" :param name: name of the constant (unique), which can be used in equations. :param value: the value of the constant, which must be a float, or a combination of Constants. \"\"\" self . name = name self . value = value self . net_id = net_id for obj in _objects [ 'constants' ]: if obj . name == name : _error ( 'the constant' , name , 'is already defined.' ) _objects [ 'constants' ] . append ( self ) __new__ ( cls , name , value , net_id = 0 ) special staticmethod # Create and return a new object. See help(type) for accurate signature. Source code in ANNarchy/core/Global.py def __new__ ( cls , name , value , net_id = 0 ): return float . __new__ ( cls , value ) set ( self , value ) # Changes the value of the constant. Source code in ANNarchy/core/Global.py def set ( self , value ): \"Changes the value of the constant.\" self . value = value if _network [ self . net_id ][ 'compiled' ]: getattr ( _network [ self . net_id ][ 'instance' ], '_set_' + self . name )( self . value ) Learning # enable_learning ( projections = None , period = None , offset = None , net_id = 0 ) # Enables learning for all projections. Optionally period and offset can be changed for all projections. Parameters: Name Type Description Default projections the projections whose learning should be enabled. By default, all the existing projections are enabled. None period determines how often the synaptic variables will be updated. None offset determines the offset at which the synaptic variables will be updated relative to the current time. None Source code in ANNarchy/core/Global.py def enable_learning ( projections = None , period = None , offset = None , net_id = 0 ): \"\"\" Enables learning for all projections. Optionally *period* and *offset* can be changed for all projections. :param projections: the projections whose learning should be enabled. By default, all the existing projections are enabled. :param period: determines how often the synaptic variables will be updated. :param offset: determines the offset at which the synaptic variables will be updated relative to the current time. \"\"\" if not projections : projections = _network [ net_id ][ 'projections' ] for proj in projections : proj . enable_learning ( period , offset ) disable_learning ( projections = None , net_id = 0 ) # Disables learning for all projections. Parameters: Name Type Description Default projections the projections whose learning should be disabled. By default, all the existing projections are disabled. None Source code in ANNarchy/core/Global.py def disable_learning ( projections = None , net_id = 0 ): \"\"\" Disables learning for all projections. :param projections: the projections whose learning should be disabled. By default, all the existing projections are disabled. \"\"\" if not projections : projections = _network [ net_id ][ 'projections' ] for proj in projections : proj . disable_learning () Access to simulation times # get_time ( net_id = 0 ) # Returns the current time in ms. Source code in ANNarchy/core/Global.py def get_time ( net_id = 0 ): \"Returns the current time in ms.\" try : t = _network [ net_id ][ 'instance' ] . get_time () * config [ 'dt' ] except : t = 0.0 return t set_time ( t , net_id = 0 ) # Sets the current time in ms. Warning: can be dangerous for some spiking models. Source code in ANNarchy/core/Global.py def set_time ( t , net_id = 0 ): \"\"\" Sets the current time in ms. **Warning:** can be dangerous for some spiking models. \"\"\" try : _network [ net_id ][ 'instance' ] . set_time ( int ( t / config [ 'dt' ])) except : _warning ( 'Time can only be set when the network is compiled.' ) get_current_step ( net_id = 0 ) # Returns the current simulation step. Source code in ANNarchy/core/Global.py def get_current_step ( net_id = 0 ): \"Returns the current simulation step.\" try : t = _network [ net_id ][ 'instance' ] . get_time () except : t = 0 return t set_current_step ( t , net_id = 0 ) # Sets the current simulation step (integer). Warning: can be dangerous for some spiking models. Source code in ANNarchy/core/Global.py def set_current_step ( t , net_id = 0 ): \"\"\" Sets the current simulation step (integer). **Warning:** can be dangerous for some spiking models. \"\"\" try : _network [ net_id ][ 'instance' ] . set_time ( int ( t )) except : _warning ( 'Time can only be set when the network is compiled.' ) dt () # Returns the simulation step size dt used in the simulation. Source code in ANNarchy/core/Global.py def dt (): \"Returns the simulation step size `dt` used in the simulation.\" return config [ 'dt' ]","title":"Top-level methods"},{"location":"API/ANNarchy/#top-level-methods","text":"These methods are directly available in the main namespace when importing ANNarhcy: from ANNarchy import * Note that numpy is automatically imported as: import numpy as np","title":"Top-level methods"},{"location":"API/ANNarchy/#configuration-and-compilation","text":"Contrary to other simulators, ANNarchy is entirely based on code generation. It provides a set of first level functions to ensure the network is correctly created. It is important to call these functions in the right order.","title":"Configuration and compilation"},{"location":"API/ANNarchy/#ANNarchy.core.Global.setup","text":"The setup function is used to configure ANNarchy simulation environment. It takes various optional arguments: dt: simulation step size (default: 1.0 ms). paradigm: parallel framework for code generation. Accepted values: \"openmp\" or \"cuda\" (default: \"openmp\"). method: default method to numerize ODEs. Default is the explicit forward Euler method ('explicit'). precision: default floating precision for variables in ANNarchy. Accepted values: \"float\" or \"double\" (default: \"double\") only_int_idx_type: if set to True (default) only signed integers are used to store pre-/post-synaptic ranks which was default until 4.7. If set to False, the index type used in a single projection is selected based on the size of the corresponding populations. num_threads: number of treads used by openMP (overrides the environment variable OMP_NUM_THREADS when set, default = None). visible_cores: allows a fine-grained control which cores are useable for the created threads (default = [] for no limitation). It can be used to limit created openMP threads to a physical socket. structural_plasticity: allows synapses to be dynamically added/removed during the simulation (default: False). seed: the seed (integer) to be used in the random number generators (default = -1 is equivalent to time(NULL)). disable_parallel_rng: determines if random numbers drawn from distributions are generated from a single source (default: True). If this flag is set to true only one RNG source is used und the values are drawn by one thread which reduces parallel performance (this is the behavior of all ANNarchy versions prior to 4.7). If set to false a seed sequence is generated to allow usage of one RNG per thread. Please note, that this flag won't effect the GPUs which draw from multiple sources anyways. use_seed_seq: If parallel RNGs are used the single generators need to be initialized. By default (use_seed_seq == True) we use the STL seed sequence to generate a list of seeds from the given master seed ( seed argument). If set to False, we use a simpler initialization strategy adapted from NEST. use_cpp_connectors: For some of the default connectivity methods of ANNarchy we offer a CPP-side construction of the pattern to improve the initialization time (default=False). For maximum performance the disable_parallel_rng should be set to False to allow a parallel construction of the pattern. disable_split_matrix: determines if projections can use thread-local allocation. If set to True (default) no thread local allocation is allowed. This equals the behavior of ANNarchy until 4.7. If set to False the code generator can use sliced versions if they are available. disable_SIMD_SpMV: determines if the hand-written implementation is used (by default True) if the current hardware platform and used sparse matrix format does support the vectorization). Disabling is intended for performance analysis. The following parameters are mainly for debugging and profiling, and should be ignored by most users: verbose: shows details about compilation process on console (by default False). Additional some information of the network construction will be shown. suppress_warnings: if True, warnings (e. g. from the mathematical parser) are suppressed. show_time: if True, initialization times are shown. Attention: verbose should be set to True additionally. Note: This function should be used before any other functions of ANNarchy (including importing a network definition), right after from ANNarchy import * : from ANNarchy import * setup ( dt = 1.0 , method = 'midpoint' , cores = 2 ) Source code in ANNarchy/core/Global.py def setup ( ** keyValueArgs ): \"\"\" The setup function is used to configure ANNarchy simulation environment. It takes various optional arguments: * dt: simulation step size (default: 1.0 ms). * paradigm: parallel framework for code generation. Accepted values: \"openmp\" or \"cuda\" (default: \"openmp\"). * method: default method to numerize ODEs. Default is the explicit forward Euler method ('explicit'). * precision: default floating precision for variables in ANNarchy. Accepted values: \"float\" or \"double\" (default: \"double\") * only_int_idx_type: if set to True (default) only signed integers are used to store pre-/post-synaptic ranks which was default until 4.7. If set to False, the index type used in a single projection is selected based on the size of the corresponding populations. * num_threads: number of treads used by openMP (overrides the environment variable ``OMP_NUM_THREADS`` when set, default = None). * visible_cores: allows a fine-grained control which cores are useable for the created threads (default = [] for no limitation). It can be used to limit created openMP threads to a physical socket. * structural_plasticity: allows synapses to be dynamically added/removed during the simulation (default: False). * seed: the seed (integer) to be used in the random number generators (default = -1 is equivalent to time(NULL)). * disable_parallel_rng: determines if random numbers drawn from distributions are generated from a single source (default: True). If this flag is set to true only one RNG source is used und the values are drawn by one thread which reduces parallel performance (this is the behavior of all ANNarchy versions prior to 4.7). If set to false a seed sequence is generated to allow usage of one RNG per thread. Please note, that this flag won't effect the GPUs which draw from multiple sources anyways. * use_seed_seq: If parallel RNGs are used the single generators need to be initialized. By default (use_seed_seq == True) we use the STL seed sequence to generate a list of seeds from the given master seed (*seed* argument). If set to False, we use a simpler initialization strategy adapted from NEST. * use_cpp_connectors: For some of the default connectivity methods of ANNarchy we offer a CPP-side construction of the pattern to improve the initialization time (default=False). For maximum performance the disable_parallel_rng should be set to False to allow a parallel construction of the pattern. * disable_split_matrix: determines if projections can use thread-local allocation. If set to *True* (default) no thread local allocation is allowed. This equals the behavior of ANNarchy until 4.7. If set to *False* the code generator can use sliced versions if they are available. * disable_SIMD_SpMV: determines if the hand-written implementation is used (by default True) if the current hardware platform and used sparse matrix format does support the vectorization). Disabling is intended for performance analysis. The following parameters are mainly for debugging and profiling, and should be ignored by most users: * verbose: shows details about compilation process on console (by default False). Additional some information of the network construction will be shown. * suppress_warnings: if True, warnings (e. g. from the mathematical parser) are suppressed. * show_time: if True, initialization times are shown. Attention: verbose should be set to True additionally. **Note:** This function should be used before any other functions of ANNarchy (including importing a network definition), right after `from ANNarchy import *`: ```python from ANNarchy import * setup(dt=1.0, method='midpoint', cores=2) ``` \"\"\" if len ( _network [ 0 ][ 'populations' ]) > 0 or len ( _network [ 0 ][ 'projections' ]) > 0 or len ( _network [ 0 ][ 'monitors' ]) > 0 : if 'dt' in keyValueArgs : _warning ( 'setup(): populations or projections have already been created. Changing dt now might lead to strange behaviors with the synaptic delays (internally generated in steps, not ms)...' ) if 'precision' in keyValueArgs : _warning ( 'setup(): populations or projections have already been created. Changing precision now might lead to strange behaviors...' ) for key in keyValueArgs : if key in config . keys (): config [ key ] = keyValueArgs [ key ] else : _warning ( 'setup(): unknown key:' , key ) if key == 'seed' : # also seed numpy np . random . seed ( keyValueArgs [ key ])","title":"setup()"},{"location":"API/ANNarchy/#ANNarchy.generator.Compiler.compile","text":"This method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation. The compiler , compiler_flags and part of cuda_config take their default value from the configuration file ~/.config/ANNarchy/annarchy.json . The following arguments are for internal development use only: debug_build : creates a debug version of ANNarchy, which logs the creation of objects and some other data (default: False). profile_enabled : creates a profilable version of ANNarchy, which logs several computation timings (default: False). Parameters: Name Type Description Default directory name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\". 'annarchy' clean boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False). False populations list of populations which should be compiled. If set to None, all available populations will be used. None projections list of projection which should be compiled. If set to None, all available projections will be used. None compiler C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++]. 'default' compiler_flags platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended. 'default' cuda_config dictionary defining the CUDA configuration for each population and projection. {'device': 0} annarchy_json compiler flags etc can be stored in a .json file normally placed in the home directory (see comment below). With this flag one can directly assign a file location. '' silent defines if status message like \"Compiling... OK\" should be printed. False Source code in ANNarchy/generator/Compiler.py def compile ( directory = 'annarchy' , clean = False , populations = None , projections = None , compiler = \"default\" , compiler_flags = \"default\" , add_sources = \"\" , extra_libs = \"\" , cuda_config = { 'device' : 0 }, annarchy_json = \"\" , silent = False , debug_build = False , profile_enabled = False , net_id = 0 ): \"\"\" This method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation. The ``compiler``, ``compiler_flags`` and part of ``cuda_config`` take their default value from the configuration file ``~/.config/ANNarchy/annarchy.json``. The following arguments are for internal development use only: * **debug_build**: creates a debug version of ANNarchy, which logs the creation of objects and some other data (default: False). * **profile_enabled**: creates a profilable version of ANNarchy, which logs several computation timings (default: False). :param directory: name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\". :param clean: boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False). :param populations: list of populations which should be compiled. If set to None, all available populations will be used. :param projections: list of projection which should be compiled. If set to None, all available projections will be used. :param compiler: C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++]. :param compiler_flags: platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended. :param cuda_config: dictionary defining the CUDA configuration for each population and projection. :param annarchy_json: compiler flags etc can be stored in a .json file normally placed in the home directory (see comment below). With this flag one can directly assign a file location. :param silent: defines if status message like \"Compiling... OK\" should be printed. \"\"\" # Check if the network has already been compiled if Global . _network [ net_id ][ 'compiled' ]: Global . _print ( \"\"\"compile(): the network has already been compiled, doing nothing. If you are re-running a Jupyter notebook, you should call `clear()` right after importing ANNarchy in order to reset everything.\"\"\" ) return # Get the command-line arguments parser = setup_parser () options , unknown = parser . parse_known_args () if len ( unknown ) > 0 and Global . config [ 'verbose' ]: Global . _warning ( 'unrecognized command-line arguments:' , unknown ) # if the parameters set on command-line they overwrite Global.config if options . num_threads is not None : Global . config [ 'num_threads' ] = options . num_threads if options . visible_cores is not None : try : core_list = [ int ( x ) for x in options . visible_cores . split ( \",\" )] Global . config [ 'visible_cores' ] = core_list except : Global . _error ( \"As argument for 'visible_cores' a comma-seperated list of integers is expected.\" ) # Get CUDA configuration if options . gpu_device >= 0 : Global . config [ 'paradigm' ] = \"cuda\" cuda_config [ 'device' ] = int ( options . gpu_device ) # Check that a single backend is chosen if ( options . num_threads != None ) and ( options . gpu_device >= 0 ): Global . _error ( 'CUDA and openMP can not be active at the same time, please check your command line arguments.' ) # Verbose if options . verbose is not None : Global . config [ 'verbose' ] = options . verbose # Precision if options . precision is not None : Global . config [ 'precision' ] = options . precision # Profiling if options . profile != None : profile_enabled = options . profile Global . config [ 'profiling' ] = options . profile Global . config [ 'profile_out' ] = options . profile_out if profile_enabled != False and options . profile == None : # Profiling enabled due compile() Global . config [ 'profiling' ] = True # Debug if not debug_build : debug_build = options . debug # debug build Global . config [ \"debug\" ] = debug_build # Clean clean = options . clean or clean # enforce rebuild # Populations to compile if populations is None : # Default network populations = Global . _network [ net_id ][ 'populations' ] # Projections to compile if projections is None : # Default network projections = Global . _network [ net_id ][ 'projections' ] # Compiling directory annarchy_dir = os . getcwd () + '/' + directory if not annarchy_dir . endswith ( '/' ): annarchy_dir += '/' # Turn OMP off for MacOS if ( Global . _check_paradigm ( \"openmp\" ) and Global . config [ 'num_threads' ] > 1 and sys . platform == \"darwin\" ): Global . _warning ( \"OpenMP is not supported on Mac OS yet\" ) Global . config [ 'num_threads' ] = 1 # Test if the current ANNarchy version is newer than what was used to create the subfolder from pkg_resources import parse_version if os . path . isfile ( annarchy_dir + '/release' ): with open ( annarchy_dir + '/release' , 'r' ) as rfile : prev_release = rfile . read () . strip () prev_paradigm = '' # HD (03.08.2016): # in ANNarchy 4.5.7b I added also the paradigm to the release tag. # This if clause can be removed in later releases (TODO) if prev_release . find ( ',' ) != - 1 : prev_paradigm , prev_release = prev_release . split ( ', ' ) else : # old release tag clean = True if parse_version ( prev_release ) < parse_version ( ANNarchy . __release__ ): clean = True elif prev_paradigm != Global . config [ 'paradigm' ]: clean = True else : clean = True # for very old versions # Check if the last compilation was successful if os . path . isfile ( annarchy_dir + '/compilation' ): with open ( annarchy_dir + '/compilation' , 'r' ) as rfile : res = rfile . read () if res . strip () == \"0\" : # the last compilation failed clean = True else : clean = True # Manage the compilation subfolder _folder_management ( annarchy_dir , profile_enabled , clean , net_id ) # Create a Compiler object compiler = Compiler ( annarchy_dir = annarchy_dir , clean = clean , compiler = compiler , compiler_flags = compiler_flags , add_sources = add_sources , extra_libs = extra_libs , path_to_json = annarchy_json , silent = silent , cuda_config = cuda_config , debug_build = debug_build , profile_enabled = profile_enabled , populations = populations , projections = projections , net_id = net_id ) # Code Generation compiler . generate () if Global . config [ 'verbose' ]: net_str = \"\" if compiler . net_id == 0 else str ( compiler . net_id ) + \" \" Global . _print ( 'Construct network ' + net_str + '...' , end = \" \" ) # Create the Python objects _instantiate ( compiler . net_id , cuda_config = compiler . cuda_config , user_config = compiler . user_config ) # NormProjections require an update of afferent projections _update_num_aff_connections ( compiler . net_id ) if Global . config [ 'verbose' ]: Global . _print ( 'OK' )","title":"compile()"},{"location":"API/ANNarchy/#ANNarchy.core.Global.clear","text":"Clears all variables (erasing already defined populations, projections, monitors and constants), as if you had just imported ANNarchy. Useful when re-running Jupyter/IPython notebooks multiple times: from ANNarchy import * clear () Source code in ANNarchy/core/Global.py def clear (): \"\"\" Clears all variables (erasing already defined populations, projections, monitors and constants), as if you had just imported ANNarchy. Useful when re-running Jupyter/IPython notebooks multiple times: ```python from ANNarchy import * clear() ``` \"\"\" # Reset objects global _objects _objects = { 'functions' : [], 'neurons' : [], 'synapses' : [], 'constants' : [], } # Reinitialize initial state global _network _network . clear () # # Configuration # config = dict( # { # 'dt' : 1.0, # 'verbose': False, # 'show_time': False, # 'suppress_warnings': False, # 'num_threads': 1, # 'paradigm': \"openmp\", # 'method': \"explicit\", # 'precision': \"double\", # 'seed': -1, # 'structural_plasticity': False, # 'profiling': False, # 'profile_out': None # } # )","title":"clear()"},{"location":"API/ANNarchy/#simulation","text":"Different methods are available to run the simulation:","title":"Simulation"},{"location":"API/ANNarchy/#ANNarchy.core.Simulate.simulate","text":"Simulates the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step dt declared in setup() (default: 1ms): simulate ( 1000.0 ) Parameters: Name Type Description Default duration the duration in milliseconds. required measure_time defines whether the simulation time should be printed. Default: False. False progress_bar defines whether a progress bar should be printed. Default: False False callbacks defines if the callback method (decorator every should be called). Default: True. True Source code in ANNarchy/core/Simulate.py def simulate ( duration , measure_time = False , progress_bar = False , callbacks = True , net_id = 0 ): \"\"\" Simulates the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step ``dt`` declared in ``setup()`` (default: 1ms): ```python simulate(1000.0) ``` :param duration: the duration in milliseconds. :param measure_time: defines whether the simulation time should be printed. Default: False. :param progress_bar: defines whether a progress bar should be printed. Default: False :param callbacks: defines if the callback method (decorator ``every`` should be called). Default: True. \"\"\" if Global . _profiler : t0 = time . time () if not _network [ net_id ][ 'instance' ]: _error ( 'simulate(): the network is not compiled yet.' ) # Compute the number of steps nb_steps = ceil ( float ( duration ) / dt ()) if measure_time : tstart = time . time () if callbacks and _callbacks_enabled [ net_id ] and len ( _callbacks [ net_id ]) > 0 : _simulate_with_callbacks ( duration , net_id ) else : _network [ net_id ][ 'instance' ] . pyx_run ( nb_steps , progress_bar ) if measure_time : if net_id > 0 : _print ( 'Simulating' , duration / 1000.0 , 'seconds of the network' , net_id , 'took' , time . time () - tstart , 'seconds.' ) else : _print ( 'Simulating' , duration / 1000.0 , 'seconds of the network took' , time . time () - tstart , 'seconds.' ) if Global . _profiler : t1 = time . time () print ( Global . _profiler ) Global . _profiler . add_entry ( t0 , t1 , \"simulate\" , \"simulate\" )","title":"simulate()"},{"location":"API/ANNarchy/#ANNarchy.core.Simulate.simulate_until","text":"Runs the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: pop1 = Population ( ... , stop_condition = \"r > 1.0 : any\" ) compile () simulate_until ( max_duration = 1000.0 . population = pop1 ) Parameters: Name Type Description Default max_duration the maximum duration of the simulation in milliseconds. required population the (list of) population whose stop_condition should be checked to stop the simulation. required operator operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). 'and' measure_time defines whether the simulation time should be printed (default=False). False Returns: Type Description the actual duration of the simulation in milliseconds. Source code in ANNarchy/core/Simulate.py def simulate_until ( max_duration , population , operator = 'and' , measure_time = False , net_id = 0 ): \"\"\" Runs the network for the maximal duration in milliseconds. If the ``stop_condition`` defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: ```python pop1 = Population( ..., stop_condition = \"r > 1.0 : any\") compile() simulate_until(max_duration=1000.0. population=pop1) ``` :param max_duration: the maximum duration of the simulation in milliseconds. :param population: the (list of) population whose ``stop_condition`` should be checked to stop the simulation. :param operator: operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). :param measure_time: defines whether the simulation time should be printed (default=False). :return: the actual duration of the simulation in milliseconds. \"\"\" if not _network [ net_id ][ 'instance' ]: _error ( 'simulate_until(): the network is not compiled yet.' ) nb_steps = ceil ( float ( max_duration ) / dt ()) if not isinstance ( population , list ): population = [ population ] if measure_time : tstart = time . time () nb = _network [ net_id ][ 'instance' ] . pyx_run_until ( nb_steps , [ pop . id for pop in population ], True if operator == 'and' else False ) sim_time = float ( nb ) / dt () if measure_time : _print ( 'Simulating' , nb / dt () / 1000.0 , 'seconds of the network took' , time . time () - tstart , 'seconds.' ) return sim_time","title":"simulate_until()"},{"location":"API/ANNarchy/#ANNarchy.core.Simulate.step","text":"Performs a single simulation step (duration = dt ). Source code in ANNarchy/core/Simulate.py def step ( net_id = 0 ): \"\"\" Performs a single simulation step (duration = ``dt``). \"\"\" if not _network [ net_id ][ 'instance' ]: _error ( 'simulate_until(): the network is not compiled yet.' ) _network [ net_id ][ 'instance' ] . pyx_step ()","title":"step()"},{"location":"API/ANNarchy/#ANNarchy.core.Simulate.every","text":"Decorator to declare a callback method that will be called periodically during the simulation. Example of setting increasing inputs to a population every 100 ms, with an offset of 90 ms (or -10 ms relative to the period): @every ( period = 100. , offset =- 10. ) def step_input ( n ): pop . I = float ( n ) / 100. simulate ( 10000. ) step_input() will be called at times 90, 190, ..., 9990 ms during the call to simulate() . The method must accept only n as parameter (an integer being 0 the first time the method is called, and incremented afterwards) and can not return anything. The times at which the method is called are relative to the time when simulate() is called (if t is already 150 before calling simulate() , the first call will then be made at t=240 with the previous example). If multiple callbacks are defined, they will be called in the order of their declaration if they occur at the same time.","title":"every"},{"location":"API/ANNarchy/#ANNarchy.core.Simulate.every.__init__","text":"Parameters: Name Type Description Default period interval in ms between two calls to the function. If less than dt , will be called every step. required offset by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period. 0.0 wait allows to wait for a certain amount of time (in ms) before starting to call the method. wait can be combined with offset , so if period=100. , offset=50. and wait=500. , the first call will be made 550 ms after the call to simulate() 0.0 Source code in ANNarchy/core/Simulate.py def __init__ ( self , period , offset = 0. , wait = 0.0 , net_id = 0 ): \"\"\" :param period: interval in ms between two calls to the function. If less than ``dt``, will be called every step. :param offset: by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period. :param wait: allows to wait for a certain amount of time (in ms) before starting to call the method. ``wait`` can be combined with ``offset``, so if ``period=100.``, ``offset=50.`` and ``wait=500.``, the first call will be made 550 ms after the call to ``simulate()`` \"\"\" self . period = max ( float ( period ), dt ()) self . offset = min ( float ( offset ), self . period ) self . wait = max ( float ( wait ), 0.0 ) _callbacks [ net_id ] . append ( self )","title":"__init__()"},{"location":"API/ANNarchy/#ANNarchy.core.Simulate.enable_callbacks","text":"Enables all declared callbacks for the network. Source code in ANNarchy/core/Simulate.py def enable_callbacks ( net_id = 0 ): \"\"\" Enables all declared callbacks for the network. \"\"\" _callbacks_enabled [ net_id ] = True","title":"enable_callbacks()"},{"location":"API/ANNarchy/#ANNarchy.core.Simulate.disable_callbacks","text":"Disables all callbacks for the network. Source code in ANNarchy/core/Simulate.py def disable_callbacks ( net_id = 0 ): \"\"\" Disables all callbacks for the network. \"\"\" _callbacks_enabled [ net_id ] = False","title":"disable_callbacks()"},{"location":"API/ANNarchy/#ANNarchy.core.Simulate.clear_all_callbacks","text":"Clears the list of declared callbacks for the network. Cannot be undone! Source code in ANNarchy/core/Simulate.py def clear_all_callbacks ( net_id = 0 ): \"\"\" Clears the list of declared callbacks for the network. Cannot be undone! \"\"\" _callbacks [ net_id ] . clear ()","title":"clear_all_callbacks()"},{"location":"API/ANNarchy/#reset-the-network","text":"If you want to run multiple experiments with the same network, or if your experiment setup requires a pre learning phase, you can reset selectively neural or synaptic variables to their initial values.","title":"Reset the network"},{"location":"API/ANNarchy/#ANNarchy.core.Global.reset","text":"Reinitialises the network to its state before the call to compile. The network time will be set to 0ms. Parameters: Name Type Description Default populations if True (default), the neural parameters and variables will be reset to their initial value. True projections if True, the synaptic parameters and variables (except the connections) will be reset (default=False). False synapses if True, the synaptic weights will be erased and recreated (default=False). False Source code in ANNarchy/core/Global.py def reset ( populations = True , projections = False , synapses = False , net_id = 0 ): \"\"\" Reinitialises the network to its state before the call to compile. The network time will be set to 0ms. :param populations: if True (default), the neural parameters and variables will be reset to their initial value. :param projections: if True, the synaptic parameters and variables (except the connections) will be reset (default=False). :param synapses: if True, the synaptic weights will be erased and recreated (default=False). \"\"\" if populations : for pop in _network [ net_id ][ 'populations' ]: pop . reset () # pop.reset only clears spike container with no or uniform delay for proj in _network [ net_id ][ 'projections' ]: if hasattr ( proj . cyInstance , 'reset_ring_buffer' ): proj . cyInstance . reset_ring_buffer () if synapses and not projections : _warning ( \"reset(): if synapses is set to true this automatically enables projections==true\" ) projections = True if projections : for proj in _network [ net_id ][ 'projections' ]: proj . reset ( attributes =- 1 , synapses = synapses ) _network [ net_id ][ 'instance' ] . set_time ( 0 )","title":"reset()"},{"location":"API/ANNarchy/#access-to-populations","text":"","title":"Access to populations"},{"location":"API/ANNarchy/#ANNarchy.core.Global.get_population","text":"Returns the population with the given name . Parameters: Name Type Description Default name name of the population. required Returns: Type Description The requested Population object if existing, None otherwise. Source code in ANNarchy/core/Global.py def get_population ( name , net_id = 0 ): \"\"\" Returns the population with the given ``name``. :param name: name of the population. :return: The requested ``Population`` object if existing, ``None`` otherwise. \"\"\" for pop in _network [ net_id ][ 'populations' ]: if pop . name == name : return pop _warning ( \"get_population(): the population\" , name , \"does not exist.\" ) return None","title":"get_population()"},{"location":"API/ANNarchy/#ANNarchy.core.Global.get_projection","text":"Returns the projection with the given name . Parameters: Name Type Description Default name name of the projection. required Returns: Type Description The requested Projection object if existing, None otherwise. Source code in ANNarchy/core/Global.py def get_projection ( name , net_id = 0 ): \"\"\" Returns the projection with the given *name*. :param name: name of the projection. :return: The requested ``Projection`` object if existing, ``None`` otherwise. \"\"\" for proj in _network [ net_id ][ 'projections' ]: if proj . name == name : return proj _warning ( \"get_projection(): the projection\" , name , \"does not exist.\" ) return None","title":"get_projection()"},{"location":"API/ANNarchy/#functions","text":"","title":"Functions"},{"location":"API/ANNarchy/#ANNarchy.core.Global.add_function","text":"Defines a global function which can be used by all neurons and synapses. The function must have only one return value and use only the passed arguments. Examples of valid functions: logistic ( x ) = 1 / ( 1 + exp ( - x )) piecewise ( x , a , b ) = if x < a : a else : if x > b : b else : x Please refer to the manual to know the allowed mathematical functions. Source code in ANNarchy/core/Global.py def add_function ( function ): \"\"\" Defines a global function which can be used by all neurons and synapses. The function must have only one return value and use only the passed arguments. Examples of valid functions: ```python logistic(x) = 1 / (1 + exp(-x)) piecewise(x, a, b) = if x < a: a else: if x > b : b else: x ``` Please refer to the manual to know the allowed mathematical functions. \"\"\" name = function . split ( '(' )[ 0 ] _objects [ 'functions' ] . append ( ( name , function ))","title":"add_function()"},{"location":"API/ANNarchy/#ANNarchy.core.Global.functions","text":"Allows to access a global function defined with add_function and use it from Python using arrays after compilation . The name of the function is not added to the global namespace to avoid overloading. add_function ( \"logistic(x) = 1. / (1. + exp(-x))\" ) compile () result = functions ( 'logistic' )([ 0. , 1. , 2. , 3. , 4. ]) Only lists or 1D Numpy arrays can be passed as arguments, not single values nor multidimensional arrays. When passing several arguments, make sure they have the same size. Source code in ANNarchy/core/Global.py def functions ( name , net_id = 0 ): \"\"\" Allows to access a global function defined with ``add_function`` and use it from Python using arrays **after compilation**. The name of the function is not added to the global namespace to avoid overloading. ```python add_function(\"logistic(x) = 1. / (1. + exp(-x))\") compile() result = functions('logistic')([0., 1., 2., 3., 4.]) ``` Only lists or 1D Numpy arrays can be passed as arguments, not single values nor multidimensional arrays. When passing several arguments, make sure they have the same size. \"\"\" try : func = getattr ( _network [ net_id ][ 'instance' ], 'func_' + name ) except : _error ( 'call to' , name , ': the function is not compiled yet.' ) return func","title":"functions()"},{"location":"API/ANNarchy/#constants","text":"","title":"Constants"},{"location":"API/ANNarchy/#ANNarchy.core.Global.Constant","text":"Constant parameter that can be used by all neurons and synapses. The class Constant derives from float , so any legal operation on floats (addition, multiplication) can be used. If a Neuron/Synapse defines a parameter with the same name, the constant parameters will not be visible. Example: tau = Constant ( 'tau' , 20 ) factor = Constant ( 'factor' , 0.1 ) real_tau = Constant ( 'real_tau' , tau * factor ) neuron = Neuron ( equations = ''' real_tau*dr/dt + r =1.0 ''' ) The value of the constant can be changed anytime with the set() method. Assignments will have no effect (e.g. tau = 10.0 only creates a new float). The value of constants defined as combination of other constants ( real_tau ) is not updated if the value of these constants changes (changing tau with tau.set(10.0) will not modify the value of real_tau ).","title":"Constant"},{"location":"API/ANNarchy/#ANNarchy.core.Global.Constant.__init__","text":"Parameters: Name Type Description Default name name of the constant (unique), which can be used in equations. required value the value of the constant, which must be a float, or a combination of Constants. required Source code in ANNarchy/core/Global.py def __init__ ( self , name , value , net_id = 0 ): \"\"\" :param name: name of the constant (unique), which can be used in equations. :param value: the value of the constant, which must be a float, or a combination of Constants. \"\"\" self . name = name self . value = value self . net_id = net_id for obj in _objects [ 'constants' ]: if obj . name == name : _error ( 'the constant' , name , 'is already defined.' ) _objects [ 'constants' ] . append ( self )","title":"__init__()"},{"location":"API/ANNarchy/#ANNarchy.core.Global.Constant.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in ANNarchy/core/Global.py def __new__ ( cls , name , value , net_id = 0 ): return float . __new__ ( cls , value )","title":"__new__()"},{"location":"API/ANNarchy/#ANNarchy.core.Global.Constant.set","text":"Changes the value of the constant. Source code in ANNarchy/core/Global.py def set ( self , value ): \"Changes the value of the constant.\" self . value = value if _network [ self . net_id ][ 'compiled' ]: getattr ( _network [ self . net_id ][ 'instance' ], '_set_' + self . name )( self . value )","title":"set()"},{"location":"API/ANNarchy/#learning","text":"","title":"Learning"},{"location":"API/ANNarchy/#ANNarchy.core.Global.enable_learning","text":"Enables learning for all projections. Optionally period and offset can be changed for all projections. Parameters: Name Type Description Default projections the projections whose learning should be enabled. By default, all the existing projections are enabled. None period determines how often the synaptic variables will be updated. None offset determines the offset at which the synaptic variables will be updated relative to the current time. None Source code in ANNarchy/core/Global.py def enable_learning ( projections = None , period = None , offset = None , net_id = 0 ): \"\"\" Enables learning for all projections. Optionally *period* and *offset* can be changed for all projections. :param projections: the projections whose learning should be enabled. By default, all the existing projections are enabled. :param period: determines how often the synaptic variables will be updated. :param offset: determines the offset at which the synaptic variables will be updated relative to the current time. \"\"\" if not projections : projections = _network [ net_id ][ 'projections' ] for proj in projections : proj . enable_learning ( period , offset )","title":"enable_learning()"},{"location":"API/ANNarchy/#ANNarchy.core.Global.disable_learning","text":"Disables learning for all projections. Parameters: Name Type Description Default projections the projections whose learning should be disabled. By default, all the existing projections are disabled. None Source code in ANNarchy/core/Global.py def disable_learning ( projections = None , net_id = 0 ): \"\"\" Disables learning for all projections. :param projections: the projections whose learning should be disabled. By default, all the existing projections are disabled. \"\"\" if not projections : projections = _network [ net_id ][ 'projections' ] for proj in projections : proj . disable_learning ()","title":"disable_learning()"},{"location":"API/ANNarchy/#access-to-simulation-times","text":"","title":"Access to simulation times"},{"location":"API/ANNarchy/#ANNarchy.core.Global.get_time","text":"Returns the current time in ms. Source code in ANNarchy/core/Global.py def get_time ( net_id = 0 ): \"Returns the current time in ms.\" try : t = _network [ net_id ][ 'instance' ] . get_time () * config [ 'dt' ] except : t = 0.0 return t","title":"get_time()"},{"location":"API/ANNarchy/#ANNarchy.core.Global.set_time","text":"Sets the current time in ms. Warning: can be dangerous for some spiking models. Source code in ANNarchy/core/Global.py def set_time ( t , net_id = 0 ): \"\"\" Sets the current time in ms. **Warning:** can be dangerous for some spiking models. \"\"\" try : _network [ net_id ][ 'instance' ] . set_time ( int ( t / config [ 'dt' ])) except : _warning ( 'Time can only be set when the network is compiled.' )","title":"set_time()"},{"location":"API/ANNarchy/#ANNarchy.core.Global.get_current_step","text":"Returns the current simulation step. Source code in ANNarchy/core/Global.py def get_current_step ( net_id = 0 ): \"Returns the current simulation step.\" try : t = _network [ net_id ][ 'instance' ] . get_time () except : t = 0 return t","title":"get_current_step()"},{"location":"API/ANNarchy/#ANNarchy.core.Global.set_current_step","text":"Sets the current simulation step (integer). Warning: can be dangerous for some spiking models. Source code in ANNarchy/core/Global.py def set_current_step ( t , net_id = 0 ): \"\"\" Sets the current simulation step (integer). **Warning:** can be dangerous for some spiking models. \"\"\" try : _network [ net_id ][ 'instance' ] . set_time ( int ( t )) except : _warning ( 'Time can only be set when the network is compiled.' )","title":"set_current_step()"},{"location":"API/ANNarchy/#ANNarchy.core.Global.dt","text":"Returns the simulation step size dt used in the simulation. Source code in ANNarchy/core/Global.py def dt (): \"Returns the simulation step size `dt` used in the simulation.\" return config [ 'dt' ]","title":"dt()"},{"location":"API/BOLD/","text":"BOLD monitoring # BOLD monitoring utilities are provided in the module ANNarchy.extensions.bold , which must be explicitely imported: from ANNarchy import * from ANNarchy.extensions.bold import BoldMonitor BoldMonitor # BoldMonitor # Monitors the BOLD signal for several populations using a computational model. The BOLD monitor transforms one or two input population variables (such as the mean firing rate) into a recordable BOLD signal according to a computational model (for example a variation of the Balloon model). __init__ ( self , populations = [], bold_model =< class ' ANNarchy . extensions . bold . PredefinedModels . balloon_RN '>, mapping={' I_CBF ': ' r '}, scale_factor=[], normalize_input=[], recorded_variables=[], start=False, net_id=0, copied=False) special # Parameters: Name Type Description Default populations list of recorded populations. [] bold_model computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is balloon_RN . <class 'ANNarchy.extensions.bold.PredefinedModels.balloon_RN'> mapping mapping dictionary between the inputs of the BOLD model ( I_CBF for single inputs, I_CBF and I_CMRO2 for double inputs in the provided examples) and the variables of the input populations. By default, {'I_CBF': 'r'} maps the firing rate r of the input population(s) to the variable I_CBF of the BOLD model. {'I_CBF': 'r'} scale_factor list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region. [] normalize_input list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time). [] recorded_variables which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples). [] Source code in ANNarchy/extensions/bold/BoldMonitor.py def __init__ ( self , populations = [], bold_model = balloon_RN , mapping = { 'I_CBF' : 'r' }, scale_factor = [], normalize_input = [], recorded_variables = [], start = False , net_id = 0 , copied = False ): \"\"\" :param populations: list of recorded populations. :param bold_model: computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is `balloon_RN`. :param mapping: mapping dictionary between the inputs of the BOLD model (`I_CBF` for single inputs, `I_CBF` and `I_CMRO2` for double inputs in the provided examples) and the variables of the input populations. By default, `{'I_CBF': 'r'}` maps the firing rate `r` of the input population(s) to the variable `I_CBF` of the BOLD model. :param scale_factor: list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region. :param normalize_input: list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time). :param recorded_variables: which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples). \"\"\" self . net_id = net_id # instantiate if necessary, please note # that population will make a deepcopy on this objects if inspect . isclass ( bold_model ): bold_model = bold_model () # for reporting bold_model . _model_instantiated = True # argument check if not ( isinstance ( populations , list )): populations = [ populations ] if not ( isinstance ( scale_factor , list )): scale_factor = [ scale_factor ] * len ( populations ) if not ( isinstance ( normalize_input , list )): normalize_input = [ normalize_input ] * len ( populations ) if isinstance ( recorded_variables , str ): recorded_variables = [ recorded_variables ] if len ( scale_factor ) > 0 : ###TODO: this leads to errors, because scale_factor is somehow globally set if len ( populations ) != len ( scale_factor ): Global . _error ( \"BoldMonitor: Length of scale_factor must be equal to number of populations\" ) if len ( normalize_input ) > 0 : if len ( populations ) != len ( normalize_input ): Global . _error ( \"BoldMonitor: Length of normalize_input must be equal to number of populations\" ) # Check mapping for target , input_var in mapping . items (): if not target in bold_model . _inputs : Global . _error ( \"BoldMonitor: the key \" + target + \" of mapping is not part of the BOLD model.\" ) # Check recorded variables if len ( recorded_variables ) == 0 : recorded_variables = [ bold_model . _output ] if not copied : # Add the container to the object management Global . _network [ 0 ][ 'extensions' ] . append ( self ) self . id = len ( Global . _network [ self . net_id ][ 'extensions' ]) # create the population self . _bold_pop = Population ( 1 , neuron = bold_model , name = bold_model . name ) self . _bold_pop . enabled = start # create the monitor self . _monitor = Monitor ( self . _bold_pop , recorded_variables , start = start ) # create the projection(s) self . _acc_proj = [] if len ( scale_factor ) == 0 : pop_overall_size = 0 for _ , pop in enumerate ( populations ): pop_overall_size += pop . size # the conductance is normalized between [0 .. 1]. This scale factor # should balance different population sizes for _ , pop in enumerate ( populations ): scale_factor_conductance = float ( pop . size ) / float ( pop_overall_size ) scale_factor . append ( scale_factor_conductance ) if len ( normalize_input ) == 0 : normalize_input = [ 0 ] * len ( populations ) # TODO: can we check if users used NormProjections? If not, this will crash ... for target , input_var in mapping . items (): for pop , scale , normalize in zip ( populations , scale_factor , normalize_input ): tmp_proj = AccProjection ( pre = pop , post = self . _bold_pop , target = target , variable = input_var , scale_factor = scale , normalize_input = normalize ) tmp_proj . connect_all_to_all ( weights = 1.0 ) self . _acc_proj . append ( tmp_proj ) else : # Add the container to the object management Global . _network [ net_id ][ 'extensions' ] . append ( self ) self . id = len ( Global . _network [ self . net_id ][ 'extensions' ]) # instances are assigned by the copying instance self . _bold_pop = None self . _monitor = None self . _acc_proj = [] self . name = \"bold_monitor\" # store arguments for copy self . _populations = populations self . _mapping = mapping self . _recorded_variables = recorded_variables self . _bold_model = bold_model self . _start = start # Finalize initialization self . _initialized = True if not copied else False get ( self , variable ) # Same as ANNarchy.core.Monitor.get() Source code in ANNarchy/extensions/bold/BoldMonitor.py def get ( self , variable ): \"\"\" Same as `ANNarchy.core.Monitor.get()` \"\"\" return self . _monitor . get ( variable ) start ( self ) # Same as ANNarchy.core.Monitor.start() Source code in ANNarchy/extensions/bold/BoldMonitor.py def start ( self ): \"\"\" Same as `ANNarchy.core.Monitor.start()` \"\"\" self . _monitor . start () # enable ODEs self . _bold_pop . cyInstance . activate ( True ) # check if we have projections with baseline for proj in self . _acc_proj : if proj . _normalize_input > 0 : proj . cyInstance . start ( proj . _normalize_input / Global . config [ \"dt\" ]) stop ( self ) # Same as ANNarchy.core.Monitor.stop() Source code in ANNarchy/extensions/bold/BoldMonitor.py def stop ( self ): \"\"\" Same as `ANNarchy.core.Monitor.stop()` \"\"\" self . _monitor . stop () # enable ODEs self . _bold_pop . cyInstance . activate ( False ) BOLD models # The provided BOLD models follow the Balloon model (Buxton et al., 1998) with the different variations studied in (Stephan et al., 2007). Those models all compute the vascular response to neural activity through a dampened oscillator: \\[ \\frac{ds}{dt} = \\phi \\, I_\\text{CBF} - \\kappa \\, s - \\gamma \\, (f_{in} - 1) \\] \\[ \\frac{df_{in}}{dt} = s \\] This allows to compute the oxygen extraction fraction: \\[ E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} } \\] The (normalized) venous blood volume is computed as: \\[ \\tau_0 \\, \\frac{dv}{dt} = (f_{in} - f_{out}) \\] \\[ f_{out} = v^{\\frac{1}{\\alpha}} \\] The level of deoxyhemoglobin into the venous compartment is computed by: \\[ \\tau_0 \\, \\frac{dq}{dt} = f_{in} \\, \\frac{E}{E_0} - \\frac{q}{v} \\, f_{out} \\] Using the two signals \\(v\\) and \\(q\\) , there are two ways to compute the corresponding BOLD signal: N: Non-linear BOLD equation: \\[ BOLD = v_0 \\, ( k_1 \\, (1-q) + k_2 \\, (1- \\dfrac{q}{v}) + k_3 \\, (1 - v) ) \\] L: Linear BOLD equation: \\[ BOLD = v_0 \\, ((k_1 + k_2) \\, (1 - q) + (k_3 - k_2) \\, (1 - v)) \\] Additionally, the three coefficients \\(k_1\\) , \\(k_2\\) , \\(k_3\\) can be computed in two different ways: C: classical coefficients from (Buxton et al., 1998): \\[k_1 = (1 - v_0) \\, 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2 = 2 \\, E_0\\] \\[k_3 = 1 - \\epsilon\\] R: revised coefficients from (Obata et al., 2004): \\[k_1 = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2 = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\\] \\[k_3 = 1 - \\epsilon\\] This makes a total of four different BOLD model (RN, RL, CN, CL) which are provided by the extension. The different parameters can be modified in the constructor. Additionally, we also provide the model that was used in (Maith et al., 2021) and the two-inputs model of (Maith et al, 2021b). BoldModel # BoldModel # Base class to define a BOLD model to be used in a BOLD monitor. A BOLD model is quite similar to a regular rate-coded neuron. It gets a weighted sum of inputs with a specific target (e.g. I_CBF) and compute a single output variable (called BOLD in the predefined models, but it could be r as well). The main difference is that a BOLD model should also declare which targets are used and which variable is the output: bold_model = BoldModel ( parameters = ''' tau = 1000. ''' , equations = ''' I_CBF = sum(I_CBF) # ... tau * dBOLD/dt = I_CBF - BOLD ''' , inputs = [ 'I_CBF' ], output = 'BOLD' ) __init__ ( self , parameters , equations , inputs , output , name = 'Custom BOLD model' , description = '' ) special # See ANNarchy.extensions.bold.PredefinedModels.py for some example models. Parameters: Name Type Description Default parameters parameters of the model and their initial value. required equations equations defining the temporal evolution of variables. required inputs list of input signals (e.g. ['I_CBF'] or ['I_CBF', 'I_CMRO2']). required output output variable of the model (e.g. 'BOLD'). required name optional model name. 'Custom BOLD model' description optional model description. '' Source code in ANNarchy/extensions/bold/BoldModel.py def __init__ ( self , parameters , equations , inputs , output , name = \"Custom BOLD model\" , description = \"\" ): \"\"\" See ANNarchy.extensions.bold.PredefinedModels.py for some example models. :param parameters: parameters of the model and their initial value. :param equations: equations defining the temporal evolution of variables. :param inputs: list of input signals (e.g. ['I_CBF'] or ['I_CBF', 'I_CMRO2']). :param output: output variable of the model (e.g. 'BOLD'). :param name: optional model name. :param description: optional model description. \"\"\" self . _inputs = inputs self . _output = output Neuron . __init__ ( self , parameters = parameters , equations = equations , name = name , description = description ) self . _model_instantiated = False # activated by BoldMonitor balloon_RN # A balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_RN = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 r_0 = 25. ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 ''' , inputs = [ 'I_CBF' ], output = \"BOLD\" , ) __init__ ( self , phi = 1.0 , kappa = 0.6493506493506493 , gamma = 0.4065040650406504 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 0.04 , epsilon = 1.43 , r_0 = 25 ) special # Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 0.6493506493506493 gamma feedback regulation 0.4065040650406504 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 0.04 epsilon ratio of intra- and extravascular signal 1.43 r_0 slope of the relation between the intravascular relaxation rate and oxygen saturation 25 Source code in ANNarchy/extensions/bold/PredefinedModels.py def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , r_0 = 25 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal :param r_0: slope of the relation between the intravascular relaxation rate and oxygen saturation \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population r_0 = %(r_0)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , 'r_0' : r_0 , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BOLD model RN\" description = \"BOLD computation with revised coefficients and non-linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = [ 'I_CBF' ], output = \"BOLD\" , name = name , description = description ) balloon_RL # A balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_RL = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 r_0 = 25. ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 ''' , inputs = [ 'I_CBF' ], output = \"BOLD\" , ) __init__ ( self , phi = 1.0 , kappa = 0.6493506493506493 , gamma = 0.4065040650406504 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 0.04 , epsilon = 1.43 , r_0 = 25 ) special # Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 0.6493506493506493 gamma feedback regulation 0.4065040650406504 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 0.04 epsilon ratio of intra- and extravascular signal 1.43 r_0 slope of the relation between the intravascular relaxation rate and oxygen saturation 25 Source code in ANNarchy/extensions/bold/PredefinedModels.py def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , r_0 = 25 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal :param r_0: slope of the relation between the intravascular relaxation rate and oxygen saturation \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population r_0 = %(r_0)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , 'r_0' : r_0 , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 tau*dq/dt = f_in * E / E_0 - (q / v) * f_out : init=1, min=0.01 tau*dv/dt = f_in - f_out : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coeeficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 \"\"\" name = \"BOLD model RL\" description = \"BOLD computation with revised coefficients and linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = [ 'I_CBF' ], output = \"BOLD\" , name = name , description = description ) balloon_CL # A balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_CL = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 ''' , inputs = [ 'I_CBF' ], output = \"BOLD\" , ) __init__ ( self , phi = 1.0 , kappa = 0.6493506493506493 , gamma = 0.4065040650406504 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 0.04 , epsilon = 1.43 ) special # Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 0.6493506493506493 gamma feedback regulation 0.4065040650406504 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 0.04 epsilon ratio of intra- and extravascular signal 1.43 Source code in ANNarchy/extensions/bold/PredefinedModels.py def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 \"\"\" name = \"BoldNeuron CL\" description = \"BOLD computation with classic coefficients and linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = [ 'I_CBF' ], output = \"BOLD\" , name = name , description = description ) balloon_CN # A balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_CN = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 ''' , inputs = [ 'I_CBF' ], output = \"BOLD\" , ) __init__ ( self , phi = 1.0 , kappa = 0.6493506493506493 , gamma = 0.4065040650406504 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 0.04 , epsilon = 1.43 ) special # Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 0.6493506493506493 gamma feedback regulation 0.4065040650406504 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 0.04 epsilon ratio of intra- and extravascular signal 1.43 Source code in ANNarchy/extensions/bold/PredefinedModels.py def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BoldNeuron CN\" , description = \"BOLD computation with classic coefficients and non-linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = [ 'I_CBF' ], output = \"BOLD\" , name = name , description = description ) balloon_maith2021 # The balloon model as used in Maith et al. (2021). balloon_two_inputs # BOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007). References # Buxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855\u2013864. doi:10.1002/mrm.1910390602 Friston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466\u2013477 Buxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220\u2013S233. doi:10.1016/j.neuroimage.2004.07.013 Stephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387\u2013401. doi:10.1016/j.neuroimage.2007.07.040 Maith et al. (2021) A computational model-based analysis of basal ganglia pathway changes in Parkinson\u2019s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278\u2013 2295. doi:10.1111/ejn.14868 Maith et al. (2021b) BOLD monitoring in the neural simulator ANNarchy. submitted.","title":"BOLD monitoring"},{"location":"API/BOLD/#bold-monitoring","text":"BOLD monitoring utilities are provided in the module ANNarchy.extensions.bold , which must be explicitely imported: from ANNarchy import * from ANNarchy.extensions.bold import BoldMonitor","title":"BOLD monitoring"},{"location":"API/BOLD/#ANNarchy.extensions.bold.BoldMonitor","text":"","title":"BoldMonitor"},{"location":"API/BOLD/#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor","text":"Monitors the BOLD signal for several populations using a computational model. The BOLD monitor transforms one or two input population variables (such as the mean firing rate) into a recordable BOLD signal according to a computational model (for example a variation of the Balloon model).","title":"BoldMonitor"},{"location":"API/BOLD/#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor.__init__","text":"Parameters: Name Type Description Default populations list of recorded populations. [] bold_model computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is balloon_RN . <class 'ANNarchy.extensions.bold.PredefinedModels.balloon_RN'> mapping mapping dictionary between the inputs of the BOLD model ( I_CBF for single inputs, I_CBF and I_CMRO2 for double inputs in the provided examples) and the variables of the input populations. By default, {'I_CBF': 'r'} maps the firing rate r of the input population(s) to the variable I_CBF of the BOLD model. {'I_CBF': 'r'} scale_factor list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region. [] normalize_input list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time). [] recorded_variables which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples). [] Source code in ANNarchy/extensions/bold/BoldMonitor.py def __init__ ( self , populations = [], bold_model = balloon_RN , mapping = { 'I_CBF' : 'r' }, scale_factor = [], normalize_input = [], recorded_variables = [], start = False , net_id = 0 , copied = False ): \"\"\" :param populations: list of recorded populations. :param bold_model: computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is `balloon_RN`. :param mapping: mapping dictionary between the inputs of the BOLD model (`I_CBF` for single inputs, `I_CBF` and `I_CMRO2` for double inputs in the provided examples) and the variables of the input populations. By default, `{'I_CBF': 'r'}` maps the firing rate `r` of the input population(s) to the variable `I_CBF` of the BOLD model. :param scale_factor: list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region. :param normalize_input: list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time). :param recorded_variables: which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples). \"\"\" self . net_id = net_id # instantiate if necessary, please note # that population will make a deepcopy on this objects if inspect . isclass ( bold_model ): bold_model = bold_model () # for reporting bold_model . _model_instantiated = True # argument check if not ( isinstance ( populations , list )): populations = [ populations ] if not ( isinstance ( scale_factor , list )): scale_factor = [ scale_factor ] * len ( populations ) if not ( isinstance ( normalize_input , list )): normalize_input = [ normalize_input ] * len ( populations ) if isinstance ( recorded_variables , str ): recorded_variables = [ recorded_variables ] if len ( scale_factor ) > 0 : ###TODO: this leads to errors, because scale_factor is somehow globally set if len ( populations ) != len ( scale_factor ): Global . _error ( \"BoldMonitor: Length of scale_factor must be equal to number of populations\" ) if len ( normalize_input ) > 0 : if len ( populations ) != len ( normalize_input ): Global . _error ( \"BoldMonitor: Length of normalize_input must be equal to number of populations\" ) # Check mapping for target , input_var in mapping . items (): if not target in bold_model . _inputs : Global . _error ( \"BoldMonitor: the key \" + target + \" of mapping is not part of the BOLD model.\" ) # Check recorded variables if len ( recorded_variables ) == 0 : recorded_variables = [ bold_model . _output ] if not copied : # Add the container to the object management Global . _network [ 0 ][ 'extensions' ] . append ( self ) self . id = len ( Global . _network [ self . net_id ][ 'extensions' ]) # create the population self . _bold_pop = Population ( 1 , neuron = bold_model , name = bold_model . name ) self . _bold_pop . enabled = start # create the monitor self . _monitor = Monitor ( self . _bold_pop , recorded_variables , start = start ) # create the projection(s) self . _acc_proj = [] if len ( scale_factor ) == 0 : pop_overall_size = 0 for _ , pop in enumerate ( populations ): pop_overall_size += pop . size # the conductance is normalized between [0 .. 1]. This scale factor # should balance different population sizes for _ , pop in enumerate ( populations ): scale_factor_conductance = float ( pop . size ) / float ( pop_overall_size ) scale_factor . append ( scale_factor_conductance ) if len ( normalize_input ) == 0 : normalize_input = [ 0 ] * len ( populations ) # TODO: can we check if users used NormProjections? If not, this will crash ... for target , input_var in mapping . items (): for pop , scale , normalize in zip ( populations , scale_factor , normalize_input ): tmp_proj = AccProjection ( pre = pop , post = self . _bold_pop , target = target , variable = input_var , scale_factor = scale , normalize_input = normalize ) tmp_proj . connect_all_to_all ( weights = 1.0 ) self . _acc_proj . append ( tmp_proj ) else : # Add the container to the object management Global . _network [ net_id ][ 'extensions' ] . append ( self ) self . id = len ( Global . _network [ self . net_id ][ 'extensions' ]) # instances are assigned by the copying instance self . _bold_pop = None self . _monitor = None self . _acc_proj = [] self . name = \"bold_monitor\" # store arguments for copy self . _populations = populations self . _mapping = mapping self . _recorded_variables = recorded_variables self . _bold_model = bold_model self . _start = start # Finalize initialization self . _initialized = True if not copied else False","title":"__init__()"},{"location":"API/BOLD/#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor.get","text":"Same as ANNarchy.core.Monitor.get() Source code in ANNarchy/extensions/bold/BoldMonitor.py def get ( self , variable ): \"\"\" Same as `ANNarchy.core.Monitor.get()` \"\"\" return self . _monitor . get ( variable )","title":"get()"},{"location":"API/BOLD/#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor.start","text":"Same as ANNarchy.core.Monitor.start() Source code in ANNarchy/extensions/bold/BoldMonitor.py def start ( self ): \"\"\" Same as `ANNarchy.core.Monitor.start()` \"\"\" self . _monitor . start () # enable ODEs self . _bold_pop . cyInstance . activate ( True ) # check if we have projections with baseline for proj in self . _acc_proj : if proj . _normalize_input > 0 : proj . cyInstance . start ( proj . _normalize_input / Global . config [ \"dt\" ])","title":"start()"},{"location":"API/BOLD/#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor.stop","text":"Same as ANNarchy.core.Monitor.stop() Source code in ANNarchy/extensions/bold/BoldMonitor.py def stop ( self ): \"\"\" Same as `ANNarchy.core.Monitor.stop()` \"\"\" self . _monitor . stop () # enable ODEs self . _bold_pop . cyInstance . activate ( False )","title":"stop()"},{"location":"API/BOLD/#bold-models","text":"The provided BOLD models follow the Balloon model (Buxton et al., 1998) with the different variations studied in (Stephan et al., 2007). Those models all compute the vascular response to neural activity through a dampened oscillator: \\[ \\frac{ds}{dt} = \\phi \\, I_\\text{CBF} - \\kappa \\, s - \\gamma \\, (f_{in} - 1) \\] \\[ \\frac{df_{in}}{dt} = s \\] This allows to compute the oxygen extraction fraction: \\[ E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} } \\] The (normalized) venous blood volume is computed as: \\[ \\tau_0 \\, \\frac{dv}{dt} = (f_{in} - f_{out}) \\] \\[ f_{out} = v^{\\frac{1}{\\alpha}} \\] The level of deoxyhemoglobin into the venous compartment is computed by: \\[ \\tau_0 \\, \\frac{dq}{dt} = f_{in} \\, \\frac{E}{E_0} - \\frac{q}{v} \\, f_{out} \\] Using the two signals \\(v\\) and \\(q\\) , there are two ways to compute the corresponding BOLD signal: N: Non-linear BOLD equation: \\[ BOLD = v_0 \\, ( k_1 \\, (1-q) + k_2 \\, (1- \\dfrac{q}{v}) + k_3 \\, (1 - v) ) \\] L: Linear BOLD equation: \\[ BOLD = v_0 \\, ((k_1 + k_2) \\, (1 - q) + (k_3 - k_2) \\, (1 - v)) \\] Additionally, the three coefficients \\(k_1\\) , \\(k_2\\) , \\(k_3\\) can be computed in two different ways: C: classical coefficients from (Buxton et al., 1998): \\[k_1 = (1 - v_0) \\, 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2 = 2 \\, E_0\\] \\[k_3 = 1 - \\epsilon\\] R: revised coefficients from (Obata et al., 2004): \\[k_1 = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2 = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\\] \\[k_3 = 1 - \\epsilon\\] This makes a total of four different BOLD model (RN, RL, CN, CL) which are provided by the extension. The different parameters can be modified in the constructor. Additionally, we also provide the model that was used in (Maith et al., 2021) and the two-inputs model of (Maith et al, 2021b).","title":"BOLD models"},{"location":"API/BOLD/#ANNarchy.extensions.bold.BoldModel","text":"","title":"BoldModel"},{"location":"API/BOLD/#ANNarchy.extensions.bold.BoldModel.BoldModel","text":"Base class to define a BOLD model to be used in a BOLD monitor. A BOLD model is quite similar to a regular rate-coded neuron. It gets a weighted sum of inputs with a specific target (e.g. I_CBF) and compute a single output variable (called BOLD in the predefined models, but it could be r as well). The main difference is that a BOLD model should also declare which targets are used and which variable is the output: bold_model = BoldModel ( parameters = ''' tau = 1000. ''' , equations = ''' I_CBF = sum(I_CBF) # ... tau * dBOLD/dt = I_CBF - BOLD ''' , inputs = [ 'I_CBF' ], output = 'BOLD' )","title":"BoldModel"},{"location":"API/BOLD/#ANNarchy.extensions.bold.BoldModel.BoldModel.__init__","text":"See ANNarchy.extensions.bold.PredefinedModels.py for some example models. Parameters: Name Type Description Default parameters parameters of the model and their initial value. required equations equations defining the temporal evolution of variables. required inputs list of input signals (e.g. ['I_CBF'] or ['I_CBF', 'I_CMRO2']). required output output variable of the model (e.g. 'BOLD'). required name optional model name. 'Custom BOLD model' description optional model description. '' Source code in ANNarchy/extensions/bold/BoldModel.py def __init__ ( self , parameters , equations , inputs , output , name = \"Custom BOLD model\" , description = \"\" ): \"\"\" See ANNarchy.extensions.bold.PredefinedModels.py for some example models. :param parameters: parameters of the model and their initial value. :param equations: equations defining the temporal evolution of variables. :param inputs: list of input signals (e.g. ['I_CBF'] or ['I_CBF', 'I_CMRO2']). :param output: output variable of the model (e.g. 'BOLD'). :param name: optional model name. :param description: optional model description. \"\"\" self . _inputs = inputs self . _output = output Neuron . __init__ ( self , parameters = parameters , equations = equations , name = name , description = description ) self . _model_instantiated = False # activated by BoldMonitor","title":"__init__()"},{"location":"API/BOLD/#ANNarchy.extensions.bold.PredefinedModels.balloon_RN","text":"A balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_RN = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 r_0 = 25. ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 ''' , inputs = [ 'I_CBF' ], output = \"BOLD\" , )","title":"balloon_RN"},{"location":"API/BOLD/#ANNarchy.extensions.bold.PredefinedModels.balloon_RN.__init__","text":"Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 0.6493506493506493 gamma feedback regulation 0.4065040650406504 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 0.04 epsilon ratio of intra- and extravascular signal 1.43 r_0 slope of the relation between the intravascular relaxation rate and oxygen saturation 25 Source code in ANNarchy/extensions/bold/PredefinedModels.py def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , r_0 = 25 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal :param r_0: slope of the relation between the intravascular relaxation rate and oxygen saturation \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population r_0 = %(r_0)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , 'r_0' : r_0 , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BOLD model RN\" description = \"BOLD computation with revised coefficients and non-linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = [ 'I_CBF' ], output = \"BOLD\" , name = name , description = description )","title":"__init__()"},{"location":"API/BOLD/#ANNarchy.extensions.bold.PredefinedModels.balloon_RL","text":"A balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_RL = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 r_0 = 25. ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 ''' , inputs = [ 'I_CBF' ], output = \"BOLD\" , )","title":"balloon_RL"},{"location":"API/BOLD/#ANNarchy.extensions.bold.PredefinedModels.balloon_RL.__init__","text":"Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 0.6493506493506493 gamma feedback regulation 0.4065040650406504 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 0.04 epsilon ratio of intra- and extravascular signal 1.43 r_0 slope of the relation between the intravascular relaxation rate and oxygen saturation 25 Source code in ANNarchy/extensions/bold/PredefinedModels.py def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , r_0 = 25 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal :param r_0: slope of the relation between the intravascular relaxation rate and oxygen saturation \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population r_0 = %(r_0)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , 'r_0' : r_0 , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 tau*dq/dt = f_in * E / E_0 - (q / v) * f_out : init=1, min=0.01 tau*dv/dt = f_in - f_out : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coeeficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 \"\"\" name = \"BOLD model RL\" description = \"BOLD computation with revised coefficients and linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = [ 'I_CBF' ], output = \"BOLD\" , name = name , description = description )","title":"__init__()"},{"location":"API/BOLD/#ANNarchy.extensions.bold.PredefinedModels.balloon_CL","text":"A balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_CL = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 ''' , inputs = [ 'I_CBF' ], output = \"BOLD\" , )","title":"balloon_CL"},{"location":"API/BOLD/#ANNarchy.extensions.bold.PredefinedModels.balloon_CL.__init__","text":"Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 0.6493506493506493 gamma feedback regulation 0.4065040650406504 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 0.04 epsilon ratio of intra- and extravascular signal 1.43 Source code in ANNarchy/extensions/bold/PredefinedModels.py def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 \"\"\" name = \"BoldNeuron CL\" description = \"BOLD computation with classic coefficients and linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = [ 'I_CBF' ], output = \"BOLD\" , name = name , description = description )","title":"__init__()"},{"location":"API/BOLD/#ANNarchy.extensions.bold.PredefinedModels.balloon_CN","text":"A balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_CN = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 ''' , inputs = [ 'I_CBF' ], output = \"BOLD\" , )","title":"balloon_CN"},{"location":"API/BOLD/#ANNarchy.extensions.bold.PredefinedModels.balloon_CN.__init__","text":"Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 0.6493506493506493 gamma feedback regulation 0.4065040650406504 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 0.04 epsilon ratio of intra- and extravascular signal 1.43 Source code in ANNarchy/extensions/bold/PredefinedModels.py def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BoldNeuron CN\" , description = \"BOLD computation with classic coefficients and non-linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = [ 'I_CBF' ], output = \"BOLD\" , name = name , description = description )","title":"__init__()"},{"location":"API/BOLD/#ANNarchy.extensions.bold.PredefinedModels.balloon_maith2021","text":"The balloon model as used in Maith et al. (2021).","title":"balloon_maith2021"},{"location":"API/BOLD/#ANNarchy.extensions.bold.PredefinedModels.balloon_two_inputs","text":"BOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007).","title":"balloon_two_inputs"},{"location":"API/BOLD/#references","text":"Buxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855\u2013864. doi:10.1002/mrm.1910390602 Friston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466\u2013477 Buxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220\u2013S233. doi:10.1016/j.neuroimage.2004.07.013 Stephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387\u2013401. doi:10.1016/j.neuroimage.2007.07.040 Maith et al. (2021) A computational model-based analysis of basal ganglia pathway changes in Parkinson\u2019s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278\u2013 2295. doi:10.1111/ejn.14868 Maith et al. (2021b) BOLD monitoring in the neural simulator ANNarchy. submitted.","title":"References"},{"location":"API/Convolution/","text":"Convolution and Pooling # Convolution and pooling operations are provided in the module ANNarchy.extensions.convolution . They must be explicitely imported: from ANNarchy import * from ANNarchy.extensions.convolution import * ANNarchy.extensions.convolution.Convolve.Convolution # Performs a convolution of a weight kernel on the pre-synaptic population. Despite its name, the operation performed is actually a cross-correlation, as is usual in computer vision and convolutional neural networks: \\[g(x) = \\sum_{k=-n}^n h(k) \\, f(x + k)\\] The convolution operation benefits from giving a multi-dimensional geometry to the populations and filters, for example in 2D: inp = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) pop = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Convolution ( inp , pop , 'exc' ) proj . connect_filter ( [ [ - 1. , 0. , 1. ], [ - 1. , 0. , 1. ], [ - 1. , 0. , 1. ] ]) The maximum number of dimensions for populations and filters is 4, an error is thrown otherwise. Depending on the number of dimensions of the pre- and post-synaptic populations, as well as of the kernel, the convolution is implemented differentely. Method connect_filter() If the pre- and post-populations have the same dimension as the kernel, the convolution is regular. Example: (100, 100) * (3, 3) -> (100, 100) If the post-population has one dimension less than the pre-synaptic one, the last dimension of the kernel must match the last one of the pre-synaptic population. Example: (100, 100, 3) * (3, 3, 3) -> (100, 100) If the kernel has less dimensions than the two populations, the number of neurons in the last dimension of the populations must be the same. The convolution will be calculated for each feature map in the last dimension. In this case, you must set keep_last_dimension to True . Example: (100, 100, 16) * (3, 3) -> (100, 100, 16) Method connect_filters() If the kernel has more dimensions than the pre-synaptic population, this means a bank of different filters will be applied on the pre-synaptic population (like a convolutional layer in a CNN). Attention: the first index of weights corresponds to the different filters, while the result will be accessible in the last dimension of the post-synaptic population. You must set the multiple argument to True. Example: (100, 100) * (16, 3, 3) -> (100, 100, 16) The convolution always uses padding for elements that would be outside the array (no equivalent of valid in tensorflow). It is 0.0 by default, but can be changed using the padding argument. Setting padding to the string border will repeat the value of the border elements. Sub-sampling will be automatically performed according to the populations' geometry. If these geometries do not match, an error will be thrown. Example: (100, 100) * (3, 3) -> (50, 50) You can redefine the sub-sampling by providing a list subsampling as argument, defining for each post-synaptic neuron the coordinates of the pre-synaptic neuron which will be the center of the filter/kernel. __init__ ( self , pre , post , target , psp = 'pre.r * w' , operation = 'sum' , name = None , copied = False ) special # Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection required psp continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r ). 'pre.r * w' operation operation (sum, max, min, mean) performed by the kernel (default: sum). 'sum' Source code in ANNarchy/extensions/convolution/Convolve.py def __init__ ( self , pre , post , target , psp = \"pre.r * w\" , operation = \"sum\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum). \"\"\" # Create the description, but it will not be used for generation Projection . __init__ ( self , pre , post , target , synapse = SharedSynapse ( psp = psp , operation = operation , name = \"Convolution operation\" , description = \"Convoluted kernel over the pre-synaptic population.\" ), name = name , copied = copied ) # Disable saving self . _saveable = False connect_filter ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ) # Applies a single filter on the pre-synaptic population. Parameters: Name Type Description Default weights numpy array or list of lists representing the matrix of weights for the filter. required delays delay in synaptic transmission (default: dt). Can only be the same value for all neurons. 0.0 keep_last_dimension defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. False padding value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. 0.0 subsampling list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. None Source code in ANNarchy/extensions/convolution/Convolve.py def connect_filter ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ): \"\"\" Applies a single filter on the pre-synaptic population. :param weights: numpy array or list of lists representing the matrix of weights for the filter. :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons. :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. \"\"\" # Process the weights self . weights = np . array ( weights ) # Process the delays self . delays = float ( delays ) if not isinstance ( delays , ( int , float )): Global . _error ( 'Convolutions can only have constant delays.' ) self . subsampling = subsampling self . keep_last_dimension = keep_last_dimension self . padding = padding self . multiple = False # Check dimensions of populations and weight matrix self . dim_kernel = self . weights . ndim self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the pre-synaptic population (maximum 4).' ) if self . dim_kernel > 5 or ( not self . multiple and self . dim_kernel > 4 ): print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the kernel (maximum 4).' ) # Check if the last axes match for parallel convolution (e.g. 3-2-3) if self . dim_kernel < self . dim_pre : if not self . keep_last_dimension : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.' ) if self . pre . geometry [ - 1 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.' ) # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less. if self . dim_post < self . dim_pre : # OK, but check the last dimension of the kernel has the same size as the post-population if self . weights . shape [ - 1 ] != self . pre . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.' ) # Check if it is a bank of filters if self . dim_kernel > self . dim_pre : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has more dimensions than the pre-synaptic population, you need to use the connect_filters() method.' ) # Generate the pre-synaptic coordinates self . _generate_pre_coordinates () # Finish building the synapses self . _create () return self connect_filters ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ) # Applies a set of different filters on the pre-synaptic population. The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters. Parameters: Name Type Description Default weights numpy array or list of lists representing the matrix of weights for the filter. required delays delay in synaptic transmission (default: dt). Can only be the same value for all neurons. 0.0 keep_last_dimension defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. False padding value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. 0.0 subsampling list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. None Source code in ANNarchy/extensions/convolution/Convolve.py def connect_filters ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ): \"\"\" Applies a set of different filters on the pre-synaptic population. The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters. :param weights: numpy array or list of lists representing the matrix of weights for the filter. :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons. :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. \"\"\" # Process the weights self . weights = np . array ( weights ) # Process the delays self . delays = float ( delays ) if not isinstance ( delays , ( int , float )): Global . _error ( 'Convolutions can only have constant delays.' ) self . subsampling = subsampling self . keep_last_dimension = keep_last_dimension self . padding = padding self . multiple = True # Check dimensions of populations and weight matrix self . dim_kernel = self . weights . ndim self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the pre-synaptic population (maximum 4).' ) if self . dim_kernel > 5 or ( not self . multiple and self . dim_kernel > 4 ): print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the kernel (maximum 4).' ) # Check if the last axes match for parallel convolution (e.g. 3-2-3) if self . dim_kernel < self . dim_pre : if not self . keep_last_dimension : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.' ) if self . pre . geometry [ - 1 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.' ) # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less. if self . dim_post < self . dim_pre : # OK, but check the last dimension of the kernel has the same size as the post-population if self . weights . shape [ - 1 ] != self . pre . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.' ) # The last dimension of the post population must correspond to the number of filters if self . weights . shape [ 0 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: For multiple filters, the last dimension of the post-synaptic population must have as many neurons as there are filters.' ) # Generate the pre-synaptic coordinates self . _generate_pre_coordinates_bank () # Finish building the synapses self . _create () return self connectivity_matrix ( self , fill = 0.0 ) # Not available. Source code in ANNarchy/extensions/convolution/Convolve.py def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Convolutional projections can not display connectivity matrices.' ) load ( self , filename ) # Not available. Source code in ANNarchy/extensions/convolution/Convolve.py def load ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be loaded.' ) receptive_fields ( self , variable = 'w' , in_post_geometry = True ) # Not available. Source code in ANNarchy/extensions/convolution/Convolve.py def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Convolutional projections can not display receptive fields.' ) save ( self , filename ) # Not available. Source code in ANNarchy/extensions/convolution/Convolve.py def save ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be saved.' ) save_connectivity ( self , filename ) # Not available. Source code in ANNarchy/extensions/convolution/Convolve.py def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be saved.' ) ANNarchy.extensions.convolution.Pooling # Pooling # Performs a pooling operation (e.g. max.pooling) on the pre-synaptic population. Each post-synaptic neuron covers a specific region ( extent ) of the pre-synaptic population, over which the result of the operation on firing rates will be assigned to sum(target). The extent is automatically computed using the geometry of the populations, but can be specified in the `connect_pooling()`` methods. Example: inp = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) pop = Population ( geometry = ( 50 , 50 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Pooling ( inp , pop , 'exc' , operation = 'max' ) # max-pooling proj . connect_pooling () # extent=(2, 2) is implicit __init__ ( self , pre , post , target , operation = 'max' , name = None , copied = False ) special # Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection required operation pooling function to be applied (\"max\", \"min\", \"mean\") 'max' Source code in ANNarchy/extensions/convolution/Pooling.py def __init__ ( self , pre , post , target , operation = \"max\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param operation: pooling function to be applied (\"max\", \"min\", \"mean\") \"\"\" if not operation in [ \"max\" , \"mean\" , \"min\" ]: Global . _error ( \"Pooling: the operation must be either 'max', 'mean' or 'min'.\" ) self . operation = operation Projection . __init__ ( self , pre , post , target , synapse = SharedSynapse ( psp = \"pre.r\" , operation = operation , name = \"Pooling operation\" , description = operation + \"-pooling operation over the pre-synaptic population.\" ), name = name , copied = copied ) if not pre . neuron_type . type == 'rate' : Global . _error ( 'Pooling: only implemented for rate-coded populations.' ) # check dimensions of populations, should not exceed 4 self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : Global . _error ( 'Pooling: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : Global . _error ( 'Pooling: Too many dimensions for the pre-synaptic population (maximum 4).' ) # Disable saving self . _saveable = False connect_pooling ( self , extent = None , delays = 0.0 ) # Parameters: Name Type Description Default extent extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g (2, 2) ). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None. None delays synaptic delay in ms 0.0 Source code in ANNarchy/extensions/convolution/Pooling.py def connect_pooling ( self , extent = None , delays = 0.0 ): \"\"\" :param extent: extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g ``(2, 2)``). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None. :param delays: synaptic delay in ms \"\"\" # process extent self . extent_init = extent if extent is None : # compute the extent automatically if self . pre . dimension != self . post . dimension : Global . _error ( 'Pooling: If you do not provide the extent parameter, the two populations must have the same number of dimensions.' ) extent = list ( self . pre . geometry ) for dim in range ( self . pre . dimension ): extent [ dim ] /= self . post . geometry [ dim ] if self . pre . geometry [ dim ] != extent [ dim ] * self . post . geometry [ dim ]: Global . _error ( 'Pooling: Unable to compute the extent of the pooling area: the number of neurons do not match.' ) elif not isinstance ( extent , tuple ): Global . _error ( 'Pooling: You must provide a tuple for the extent of the pooling operation.' ) self . extent = list ( extent ) if len ( self . extent ) < self . pre . dimension : Global . _error ( 'Pooling: You must provide a tuple for the extent of the pooling operation.' ) # process delays self . delays = delays # Generate the pre-synaptic coordinates self . _generate_extent_coordinates () # create fake LIL self . _create () return self connectivity_matrix ( self , fill = 0.0 ) # Not available. Source code in ANNarchy/extensions/convolution/Pooling.py def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Pooling projections can not display connectivity matrices.' ) load ( self , filename ) # Not available. Source code in ANNarchy/extensions/convolution/Pooling.py def load ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be loaded.' ) receptive_fields ( self , variable = 'w' , in_post_geometry = True ) # Not available. Source code in ANNarchy/extensions/convolution/Pooling.py def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Pooling projections can not display receptive fields.' ) save ( self , filename ) # Not available. Source code in ANNarchy/extensions/convolution/Pooling.py def save ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be saved.' ) save_connectivity ( self , filename ) # Not available. Source code in ANNarchy/extensions/convolution/Pooling.py def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be saved.' ) ANNarchy.extensions.convolution.Copy # Copy # Creates a virtual projection reusing the weights and delays of an already-defined projection. Although the original projection can be learnable, this one can not. Changes in the original weights will be reflected in this projection. The only possible modifications are psp and operation . The pre- and post-synaptic populations of both projections must have the same geometry. Example: proj = Projection ( pop1 , pop2 , \"exc\" ) proj . connect_fixed_probability ( 0.1 , 0.5 ) copy_proj = Copy ( pop1 , pop3 , \"exc\" ) copy_proj . connect_copy ( proj ) __init__ ( self , pre , post , target , psp = 'pre.r * w' , operation = 'sum' , name = None , copied = False ) special # Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection required psp continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r ). 'pre.r * w' operation operation (sum, max, min, mean) performed by the kernel (default: sum). 'sum' Source code in ANNarchy/extensions/convolution/Copy.py def __init__ ( self , pre , post , target , psp = \"pre.r * w\" , operation = \"sum\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum). \"\"\" # Create the description, but it will not be used for generation Projection . __init__ ( self , pre = pre , post = post , target = target , synapse = SharedSynapse ( psp = psp , operation = operation ), name = name , copied = copied ) connect_copy ( self , projection ) # Parameters: Name Type Description Default projection Existing projection to copy. required Source code in ANNarchy/extensions/convolution/Copy.py def connect_copy ( self , projection ): \"\"\" :param projection: Existing projection to copy. \"\"\" self . projection = projection # Sanity checks if not isinstance ( self . projection , Projection ): Global . _error ( 'Copy: You must provide an existing projection to copy().' ) if isinstance ( self . projection , ( ConvolutionProjection , PoolingProjection )): Global . _error ( 'Copy: You can only copy regular projections, not shared projections.' ) if not self . pre . geometry == self . projection . pre . geometry or not self . post . geometry == self . projection . post . geometry : Global . _error ( 'Copy: When copying a projection, the geometries must be the same.' ) # Dummy weights self . weights = None self . pre_coordinates = [] # Finish building the synapses self . _create () return self connectivity_matrix ( self , fill = 0.0 ) # Not available. Source code in ANNarchy/extensions/convolution/Copy.py def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Copied projections can not display connectivity matrices.' ) generate_omp ( self ) # Code generation of CopyProjection object for the openMP paradigm. Source code in ANNarchy/extensions/convolution/Copy.py def generate_omp ( self ): \"\"\" Code generation of CopyProjection object for the openMP paradigm. \"\"\" # Set the projection specific parameters copy_proj_dict = deepcopy ( copy_proj_template ) for key , value in copy_proj_dict . items (): value = value % { 'id_proj' : self . id , 'id_copy' : self . projection . id , 'float_prec' : Global . config [ 'precision' ] } copy_proj_dict [ key ] = value # Update specific template self . _specific_template . update ( copy_proj_dict ) # OMP code if more then one thread if Global . config [ 'num_threads' ] > 1 : omp_code = '#pragma omp for private(sum)' if self . post . size > Global . OMP_MIN_NB_NEURONS else '' else : omp_code = \"\" # PSP psp = self . synapse_type . description [ 'psp' ][ 'cpp' ] % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'local_index' : '[i][j]' , 'global_index' : '[i]' , 'pre_index' : '[pre_rank[i][j]]' , 'post_index' : '[post_rank[i]]' , 'pre_prefix' : 'pop' + str ( self . pre . id ) + '.' , 'post_prefix' : 'pop' + str ( self . post . id ) + '.' } psp = psp . replace ( 'rk_pre' , 'pre_rank[i][j]' ) . replace ( ';' , '' ) # Take delays into account if any if self . delays > Global . config [ 'dt' ]: psp = psp . replace ( 'pop %(id_pre)s .r[rk_pre]' % { 'id_pre' : self . pre . id }, 'pop %(id_pre)s ._delayed_r[delay-1][rk_pre]' % { 'id_pre' : self . pre . id } # TODO HD: wouldn't it be much better to reduce delay globaly, instead of the substraction here??? ) # Select template for operation to be performed: sum, max, min, mean try : sum_code = copy_sum_template [ self . synapse_type . operation ] except KeyError : Global . _error ( \"CopyProjection: the operation \" , self . synapse_type . operation , ' is not available.' ) # Finalize code self . generator [ 'omp' ][ 'body_compute_psp' ] = sum_code % { 'id_proj' : self . id , 'target' : self . target , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'id_post' : self . post . id , 'name_post' : self . post . name , 'id' : self . projection . id , 'float_prec' : Global . config [ 'precision' ], 'omp_code' : omp_code , 'psp' : psp } load ( self , filename ) # Not available. Source code in ANNarchy/extensions/convolution/Copy.py def load ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be loaded.' ) receptive_fields ( self , variable = 'w' , in_post_geometry = True ) # Not available. Source code in ANNarchy/extensions/convolution/Copy.py def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Copied projections can not display receptive fields.' ) save ( self , filename ) # Not available. Source code in ANNarchy/extensions/convolution/Copy.py def save ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be saved.' ) save_connectivity ( self , filename ) # Not available. Source code in ANNarchy/extensions/convolution/Copy.py def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be saved.' )","title":"Convolution and Pooling"},{"location":"API/Convolution/#convolution-and-pooling","text":"Convolution and pooling operations are provided in the module ANNarchy.extensions.convolution . They must be explicitely imported: from ANNarchy import * from ANNarchy.extensions.convolution import *","title":"Convolution and Pooling"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Convolve.Convolution","text":"Performs a convolution of a weight kernel on the pre-synaptic population. Despite its name, the operation performed is actually a cross-correlation, as is usual in computer vision and convolutional neural networks: \\[g(x) = \\sum_{k=-n}^n h(k) \\, f(x + k)\\] The convolution operation benefits from giving a multi-dimensional geometry to the populations and filters, for example in 2D: inp = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) pop = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Convolution ( inp , pop , 'exc' ) proj . connect_filter ( [ [ - 1. , 0. , 1. ], [ - 1. , 0. , 1. ], [ - 1. , 0. , 1. ] ]) The maximum number of dimensions for populations and filters is 4, an error is thrown otherwise. Depending on the number of dimensions of the pre- and post-synaptic populations, as well as of the kernel, the convolution is implemented differentely. Method connect_filter() If the pre- and post-populations have the same dimension as the kernel, the convolution is regular. Example: (100, 100) * (3, 3) -> (100, 100) If the post-population has one dimension less than the pre-synaptic one, the last dimension of the kernel must match the last one of the pre-synaptic population. Example: (100, 100, 3) * (3, 3, 3) -> (100, 100) If the kernel has less dimensions than the two populations, the number of neurons in the last dimension of the populations must be the same. The convolution will be calculated for each feature map in the last dimension. In this case, you must set keep_last_dimension to True . Example: (100, 100, 16) * (3, 3) -> (100, 100, 16) Method connect_filters() If the kernel has more dimensions than the pre-synaptic population, this means a bank of different filters will be applied on the pre-synaptic population (like a convolutional layer in a CNN). Attention: the first index of weights corresponds to the different filters, while the result will be accessible in the last dimension of the post-synaptic population. You must set the multiple argument to True. Example: (100, 100) * (16, 3, 3) -> (100, 100, 16) The convolution always uses padding for elements that would be outside the array (no equivalent of valid in tensorflow). It is 0.0 by default, but can be changed using the padding argument. Setting padding to the string border will repeat the value of the border elements. Sub-sampling will be automatically performed according to the populations' geometry. If these geometries do not match, an error will be thrown. Example: (100, 100) * (3, 3) -> (50, 50) You can redefine the sub-sampling by providing a list subsampling as argument, defining for each post-synaptic neuron the coordinates of the pre-synaptic neuron which will be the center of the filter/kernel.","title":"Convolution"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Convolve.Convolution.__init__","text":"Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection required psp continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r ). 'pre.r * w' operation operation (sum, max, min, mean) performed by the kernel (default: sum). 'sum' Source code in ANNarchy/extensions/convolution/Convolve.py def __init__ ( self , pre , post , target , psp = \"pre.r * w\" , operation = \"sum\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum). \"\"\" # Create the description, but it will not be used for generation Projection . __init__ ( self , pre , post , target , synapse = SharedSynapse ( psp = psp , operation = operation , name = \"Convolution operation\" , description = \"Convoluted kernel over the pre-synaptic population.\" ), name = name , copied = copied ) # Disable saving self . _saveable = False","title":"__init__()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Convolve.Convolution.connect_filter","text":"Applies a single filter on the pre-synaptic population. Parameters: Name Type Description Default weights numpy array or list of lists representing the matrix of weights for the filter. required delays delay in synaptic transmission (default: dt). Can only be the same value for all neurons. 0.0 keep_last_dimension defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. False padding value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. 0.0 subsampling list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. None Source code in ANNarchy/extensions/convolution/Convolve.py def connect_filter ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ): \"\"\" Applies a single filter on the pre-synaptic population. :param weights: numpy array or list of lists representing the matrix of weights for the filter. :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons. :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. \"\"\" # Process the weights self . weights = np . array ( weights ) # Process the delays self . delays = float ( delays ) if not isinstance ( delays , ( int , float )): Global . _error ( 'Convolutions can only have constant delays.' ) self . subsampling = subsampling self . keep_last_dimension = keep_last_dimension self . padding = padding self . multiple = False # Check dimensions of populations and weight matrix self . dim_kernel = self . weights . ndim self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the pre-synaptic population (maximum 4).' ) if self . dim_kernel > 5 or ( not self . multiple and self . dim_kernel > 4 ): print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the kernel (maximum 4).' ) # Check if the last axes match for parallel convolution (e.g. 3-2-3) if self . dim_kernel < self . dim_pre : if not self . keep_last_dimension : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.' ) if self . pre . geometry [ - 1 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.' ) # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less. if self . dim_post < self . dim_pre : # OK, but check the last dimension of the kernel has the same size as the post-population if self . weights . shape [ - 1 ] != self . pre . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.' ) # Check if it is a bank of filters if self . dim_kernel > self . dim_pre : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has more dimensions than the pre-synaptic population, you need to use the connect_filters() method.' ) # Generate the pre-synaptic coordinates self . _generate_pre_coordinates () # Finish building the synapses self . _create () return self","title":"connect_filter()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Convolve.Convolution.connect_filters","text":"Applies a set of different filters on the pre-synaptic population. The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters. Parameters: Name Type Description Default weights numpy array or list of lists representing the matrix of weights for the filter. required delays delay in synaptic transmission (default: dt). Can only be the same value for all neurons. 0.0 keep_last_dimension defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. False padding value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. 0.0 subsampling list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. None Source code in ANNarchy/extensions/convolution/Convolve.py def connect_filters ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ): \"\"\" Applies a set of different filters on the pre-synaptic population. The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters. :param weights: numpy array or list of lists representing the matrix of weights for the filter. :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons. :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. \"\"\" # Process the weights self . weights = np . array ( weights ) # Process the delays self . delays = float ( delays ) if not isinstance ( delays , ( int , float )): Global . _error ( 'Convolutions can only have constant delays.' ) self . subsampling = subsampling self . keep_last_dimension = keep_last_dimension self . padding = padding self . multiple = True # Check dimensions of populations and weight matrix self . dim_kernel = self . weights . ndim self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the pre-synaptic population (maximum 4).' ) if self . dim_kernel > 5 or ( not self . multiple and self . dim_kernel > 4 ): print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the kernel (maximum 4).' ) # Check if the last axes match for parallel convolution (e.g. 3-2-3) if self . dim_kernel < self . dim_pre : if not self . keep_last_dimension : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.' ) if self . pre . geometry [ - 1 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.' ) # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less. if self . dim_post < self . dim_pre : # OK, but check the last dimension of the kernel has the same size as the post-population if self . weights . shape [ - 1 ] != self . pre . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.' ) # The last dimension of the post population must correspond to the number of filters if self . weights . shape [ 0 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: For multiple filters, the last dimension of the post-synaptic population must have as many neurons as there are filters.' ) # Generate the pre-synaptic coordinates self . _generate_pre_coordinates_bank () # Finish building the synapses self . _create () return self","title":"connect_filters()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Convolve.Convolution.connectivity_matrix","text":"Not available. Source code in ANNarchy/extensions/convolution/Convolve.py def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Convolutional projections can not display connectivity matrices.' )","title":"connectivity_matrix()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Convolve.Convolution.load","text":"Not available. Source code in ANNarchy/extensions/convolution/Convolve.py def load ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be loaded.' )","title":"load()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Convolve.Convolution.receptive_fields","text":"Not available. Source code in ANNarchy/extensions/convolution/Convolve.py def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Convolutional projections can not display receptive fields.' )","title":"receptive_fields()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Convolve.Convolution.save","text":"Not available. Source code in ANNarchy/extensions/convolution/Convolve.py def save ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be saved.' )","title":"save()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Convolve.Convolution.save_connectivity","text":"Not available. Source code in ANNarchy/extensions/convolution/Convolve.py def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be saved.' )","title":"save_connectivity()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Pooling","text":"","title":"Pooling"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Pooling.Pooling","text":"Performs a pooling operation (e.g. max.pooling) on the pre-synaptic population. Each post-synaptic neuron covers a specific region ( extent ) of the pre-synaptic population, over which the result of the operation on firing rates will be assigned to sum(target). The extent is automatically computed using the geometry of the populations, but can be specified in the `connect_pooling()`` methods. Example: inp = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) pop = Population ( geometry = ( 50 , 50 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Pooling ( inp , pop , 'exc' , operation = 'max' ) # max-pooling proj . connect_pooling () # extent=(2, 2) is implicit","title":"Pooling"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Pooling.Pooling.__init__","text":"Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection required operation pooling function to be applied (\"max\", \"min\", \"mean\") 'max' Source code in ANNarchy/extensions/convolution/Pooling.py def __init__ ( self , pre , post , target , operation = \"max\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param operation: pooling function to be applied (\"max\", \"min\", \"mean\") \"\"\" if not operation in [ \"max\" , \"mean\" , \"min\" ]: Global . _error ( \"Pooling: the operation must be either 'max', 'mean' or 'min'.\" ) self . operation = operation Projection . __init__ ( self , pre , post , target , synapse = SharedSynapse ( psp = \"pre.r\" , operation = operation , name = \"Pooling operation\" , description = operation + \"-pooling operation over the pre-synaptic population.\" ), name = name , copied = copied ) if not pre . neuron_type . type == 'rate' : Global . _error ( 'Pooling: only implemented for rate-coded populations.' ) # check dimensions of populations, should not exceed 4 self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : Global . _error ( 'Pooling: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : Global . _error ( 'Pooling: Too many dimensions for the pre-synaptic population (maximum 4).' ) # Disable saving self . _saveable = False","title":"__init__()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Pooling.Pooling.connect_pooling","text":"Parameters: Name Type Description Default extent extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g (2, 2) ). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None. None delays synaptic delay in ms 0.0 Source code in ANNarchy/extensions/convolution/Pooling.py def connect_pooling ( self , extent = None , delays = 0.0 ): \"\"\" :param extent: extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g ``(2, 2)``). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None. :param delays: synaptic delay in ms \"\"\" # process extent self . extent_init = extent if extent is None : # compute the extent automatically if self . pre . dimension != self . post . dimension : Global . _error ( 'Pooling: If you do not provide the extent parameter, the two populations must have the same number of dimensions.' ) extent = list ( self . pre . geometry ) for dim in range ( self . pre . dimension ): extent [ dim ] /= self . post . geometry [ dim ] if self . pre . geometry [ dim ] != extent [ dim ] * self . post . geometry [ dim ]: Global . _error ( 'Pooling: Unable to compute the extent of the pooling area: the number of neurons do not match.' ) elif not isinstance ( extent , tuple ): Global . _error ( 'Pooling: You must provide a tuple for the extent of the pooling operation.' ) self . extent = list ( extent ) if len ( self . extent ) < self . pre . dimension : Global . _error ( 'Pooling: You must provide a tuple for the extent of the pooling operation.' ) # process delays self . delays = delays # Generate the pre-synaptic coordinates self . _generate_extent_coordinates () # create fake LIL self . _create () return self","title":"connect_pooling()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Pooling.Pooling.connectivity_matrix","text":"Not available. Source code in ANNarchy/extensions/convolution/Pooling.py def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Pooling projections can not display connectivity matrices.' )","title":"connectivity_matrix()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Pooling.Pooling.load","text":"Not available. Source code in ANNarchy/extensions/convolution/Pooling.py def load ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be loaded.' )","title":"load()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Pooling.Pooling.receptive_fields","text":"Not available. Source code in ANNarchy/extensions/convolution/Pooling.py def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Pooling projections can not display receptive fields.' )","title":"receptive_fields()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Pooling.Pooling.save","text":"Not available. Source code in ANNarchy/extensions/convolution/Pooling.py def save ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be saved.' )","title":"save()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Pooling.Pooling.save_connectivity","text":"Not available. Source code in ANNarchy/extensions/convolution/Pooling.py def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be saved.' )","title":"save_connectivity()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Copy","text":"","title":"Copy"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Copy.Copy","text":"Creates a virtual projection reusing the weights and delays of an already-defined projection. Although the original projection can be learnable, this one can not. Changes in the original weights will be reflected in this projection. The only possible modifications are psp and operation . The pre- and post-synaptic populations of both projections must have the same geometry. Example: proj = Projection ( pop1 , pop2 , \"exc\" ) proj . connect_fixed_probability ( 0.1 , 0.5 ) copy_proj = Copy ( pop1 , pop3 , \"exc\" ) copy_proj . connect_copy ( proj )","title":"Copy"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Copy.Copy.__init__","text":"Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection required psp continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r ). 'pre.r * w' operation operation (sum, max, min, mean) performed by the kernel (default: sum). 'sum' Source code in ANNarchy/extensions/convolution/Copy.py def __init__ ( self , pre , post , target , psp = \"pre.r * w\" , operation = \"sum\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum). \"\"\" # Create the description, but it will not be used for generation Projection . __init__ ( self , pre = pre , post = post , target = target , synapse = SharedSynapse ( psp = psp , operation = operation ), name = name , copied = copied )","title":"__init__()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Copy.Copy.connect_copy","text":"Parameters: Name Type Description Default projection Existing projection to copy. required Source code in ANNarchy/extensions/convolution/Copy.py def connect_copy ( self , projection ): \"\"\" :param projection: Existing projection to copy. \"\"\" self . projection = projection # Sanity checks if not isinstance ( self . projection , Projection ): Global . _error ( 'Copy: You must provide an existing projection to copy().' ) if isinstance ( self . projection , ( ConvolutionProjection , PoolingProjection )): Global . _error ( 'Copy: You can only copy regular projections, not shared projections.' ) if not self . pre . geometry == self . projection . pre . geometry or not self . post . geometry == self . projection . post . geometry : Global . _error ( 'Copy: When copying a projection, the geometries must be the same.' ) # Dummy weights self . weights = None self . pre_coordinates = [] # Finish building the synapses self . _create () return self","title":"connect_copy()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Copy.Copy.connectivity_matrix","text":"Not available. Source code in ANNarchy/extensions/convolution/Copy.py def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Copied projections can not display connectivity matrices.' )","title":"connectivity_matrix()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Copy.Copy.generate_omp","text":"Code generation of CopyProjection object for the openMP paradigm. Source code in ANNarchy/extensions/convolution/Copy.py def generate_omp ( self ): \"\"\" Code generation of CopyProjection object for the openMP paradigm. \"\"\" # Set the projection specific parameters copy_proj_dict = deepcopy ( copy_proj_template ) for key , value in copy_proj_dict . items (): value = value % { 'id_proj' : self . id , 'id_copy' : self . projection . id , 'float_prec' : Global . config [ 'precision' ] } copy_proj_dict [ key ] = value # Update specific template self . _specific_template . update ( copy_proj_dict ) # OMP code if more then one thread if Global . config [ 'num_threads' ] > 1 : omp_code = '#pragma omp for private(sum)' if self . post . size > Global . OMP_MIN_NB_NEURONS else '' else : omp_code = \"\" # PSP psp = self . synapse_type . description [ 'psp' ][ 'cpp' ] % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'local_index' : '[i][j]' , 'global_index' : '[i]' , 'pre_index' : '[pre_rank[i][j]]' , 'post_index' : '[post_rank[i]]' , 'pre_prefix' : 'pop' + str ( self . pre . id ) + '.' , 'post_prefix' : 'pop' + str ( self . post . id ) + '.' } psp = psp . replace ( 'rk_pre' , 'pre_rank[i][j]' ) . replace ( ';' , '' ) # Take delays into account if any if self . delays > Global . config [ 'dt' ]: psp = psp . replace ( 'pop %(id_pre)s .r[rk_pre]' % { 'id_pre' : self . pre . id }, 'pop %(id_pre)s ._delayed_r[delay-1][rk_pre]' % { 'id_pre' : self . pre . id } # TODO HD: wouldn't it be much better to reduce delay globaly, instead of the substraction here??? ) # Select template for operation to be performed: sum, max, min, mean try : sum_code = copy_sum_template [ self . synapse_type . operation ] except KeyError : Global . _error ( \"CopyProjection: the operation \" , self . synapse_type . operation , ' is not available.' ) # Finalize code self . generator [ 'omp' ][ 'body_compute_psp' ] = sum_code % { 'id_proj' : self . id , 'target' : self . target , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'id_post' : self . post . id , 'name_post' : self . post . name , 'id' : self . projection . id , 'float_prec' : Global . config [ 'precision' ], 'omp_code' : omp_code , 'psp' : psp }","title":"generate_omp()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Copy.Copy.load","text":"Not available. Source code in ANNarchy/extensions/convolution/Copy.py def load ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be loaded.' )","title":"load()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Copy.Copy.receptive_fields","text":"Not available. Source code in ANNarchy/extensions/convolution/Copy.py def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Copied projections can not display receptive fields.' )","title":"receptive_fields()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Copy.Copy.save","text":"Not available. Source code in ANNarchy/extensions/convolution/Copy.py def save ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be saved.' )","title":"save()"},{"location":"API/Convolution/#ANNarchy.extensions.convolution.Copy.Copy.save_connectivity","text":"Not available. Source code in ANNarchy/extensions/convolution/Copy.py def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be saved.' )","title":"save_connectivity()"},{"location":"API/Dendrite/","text":"Dendrite class # A Dendrite is a sub-group of a Projection , gathering the synapses between the pre-synaptic population and a single post-synaptic neuron. Dendrite # A Dendrite is a sub-group of a Projection , gathering the synapses between the pre-synaptic population and a single post-synaptic neuron. It can not be created directly, only through a call to Projection.dendrite(rank) : dendrite = proj . dendrite ( 6 ) pre_ranks property readonly # List of ranks of pre-synaptic neurons. size property readonly # Number of synapses. synapses property readonly # Iteratively returns the synapses corresponding to this dendrite. create_synapse ( self , rank , w = 0.0 , delay = 0 ) # Creates a synapse for this dendrite with the given pre-synaptic neuron. Parameters: Name Type Description Default rank rank of the pre-synaptic neuron required w synaptic weight (defalt: 0.0). 0.0 delay synaptic delay (default = dt) 0 Source code in ANNarchy/core/Dendrite.py def create_synapse ( self , rank , w = 0.0 , delay = 0 ): \"\"\" Creates a synapse for this dendrite with the given pre-synaptic neuron. :param rank: rank of the pre-synaptic neuron :param w: synaptic weight (defalt: 0.0). :param delay: synaptic delay (default = dt) \"\"\" if not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), can not add the synapse.' ) return if self . proj . cyInstance . dendrite_index ( self . post_rank , rank ) != - 1 : Global . _error ( 'the synapse of rank ' + str ( rank ) + ' already exists.' ) return # Set default values for the additional variables extra_attributes = {} for var in self . proj . synapse_type . description [ 'parameters' ] + self . proj . synapse_type . description [ 'variables' ]: if not var [ 'name' ] in [ 'w' , 'delay' ] and var [ 'name' ] in self . proj . synapse_type . description [ 'local' ]: if not isinstance ( self . proj . init [ var [ 'name' ]], ( int , float , bool )): init = var [ 'init' ] else : init = self . proj . init [ var [ 'name' ]] extra_attributes [ var [ 'name' ]] = init try : self . proj . cyInstance . add_synapse ( self . post_rank , rank , w , int ( delay / Global . config [ 'dt' ]), ** extra_attributes ) except Exception as e : Global . _print ( e ) get ( self , name ) # Returns the value of a variable/parameter. Example: dendrite . get ( 'w' ) Parameters: Name Type Description Default name name of the parameter/variable. required Source code in ANNarchy/core/Dendrite.py def get ( self , name ): \"\"\" Returns the value of a variable/parameter. Example: ```python dendrite.get('w') ``` :param name: name of the parameter/variable. \"\"\" if name == 'rank' : Global . _warning ( \"Dendrite.get('rank'): the attribute is deprecated, use Dendrite.pre_ranks instead.\" ) return self . proj . cyInstance . pre_rank ( self . idx ) elif name == 'pre_ranks' : return self . proj . cyInstance . pre_rank ( self . idx ) elif name in self . attributes : return getattr ( self , name ) else : Global . _error ( \"Dendrite has no parameter/variable called\" , name ) prune_synapse ( self , rank ) # Removes the synapse with the given pre-synaptic neuron from the dendrite. Parameters: Name Type Description Default rank rank of the pre-synaptic neuron required Source code in ANNarchy/core/Dendrite.py def prune_synapse ( self , rank ): \"\"\" Removes the synapse with the given pre-synaptic neuron from the dendrite. :param rank: rank of the pre-synaptic neuron \"\"\" if not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), can not remove the synapse.' ) return if not rank in self . pre_ranks : Global . _error ( 'the synapse with the pre-synaptic neuron of rank ' + str ( rank ) + ' did not already exist.' ) return self . proj . cyInstance . remove_synapse ( self . post_rank , rank ) receptive_field ( self , variable = 'w' , fill = 0.0 ) # Returns the given variable as a receptive field. A Numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value fill ). Parameters: Name Type Description Default variable name of the variable (default = 'w') 'w' fill value to use when a synapse does not exist (default: 0.0). 0.0 Source code in ANNarchy/core/Dendrite.py def receptive_field ( self , variable = 'w' , fill = 0.0 ): \"\"\" Returns the given variable as a receptive field. A Numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value ``fill``). :param variable: name of the variable (default = 'w') :param fill: value to use when a synapse does not exist (default: 0.0). \"\"\" values = getattr ( self . proj . cyInstance , 'get_dendrite_' + variable )( self . idx ) pre_ranks = self . proj . cyInstance . pre_rank ( self . idx ) m = fill * np . ones ( self . pre . size ) m [ pre_ranks ] = values return m . reshape ( self . pre . geometry ) set ( self , value ) # Sets the value of a parameter/variable of all synapses. Example: dendrite . set ( { 'tau' : 20 , 'w' = Uniform ( 0.0 , 1.0 ) } ) Parameters: Name Type Description Default value a dictionary containing the parameter/variable names as keys. required Source code in ANNarchy/core/Dendrite.py def set ( self , value ): \"\"\" Sets the value of a parameter/variable of all synapses. Example: ```python dendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } ) ``` :param value: a dictionary containing the parameter/variable names as keys. \"\"\" for key , value in value . items (): # sanity check and then forward to __setattr__ if key in self . attributes : setattr ( self , key , value ) else : Global . _error ( \"Dendrite has no parameter/variable called\" , key ) synapse ( self , pos ) # Returns the synapse coming from the corresponding presynaptic neuron. Parameters: Name Type Description Default pos can be either the rank or the coordinates of the presynaptic neuron required Source code in ANNarchy/core/Dendrite.py def synapse ( self , pos ): \"\"\" Returns the synapse coming from the corresponding presynaptic neuron. :param pos: can be either the rank or the coordinates of the presynaptic neuron \"\"\" if isinstance ( pos , int ): rank = pos else : rank = self . proj . pre . rank_from_coordinates ( pos ) if rank in self . pre_ranks : return IndividualSynapse ( self , rank ) else : Global . _error ( \" The neuron of rank \" + str ( rank ) + \" has no synapse in this dendrite.\" ) return None","title":"Dendrite class"},{"location":"API/Dendrite/#dendrite-class","text":"A Dendrite is a sub-group of a Projection , gathering the synapses between the pre-synaptic population and a single post-synaptic neuron.","title":"Dendrite class"},{"location":"API/Dendrite/#ANNarchy.core.Dendrite.Dendrite","text":"A Dendrite is a sub-group of a Projection , gathering the synapses between the pre-synaptic population and a single post-synaptic neuron. It can not be created directly, only through a call to Projection.dendrite(rank) : dendrite = proj . dendrite ( 6 )","title":"Dendrite"},{"location":"API/Dendrite/#ANNarchy.core.Dendrite.Dendrite.pre_ranks","text":"List of ranks of pre-synaptic neurons.","title":"pre_ranks"},{"location":"API/Dendrite/#ANNarchy.core.Dendrite.Dendrite.size","text":"Number of synapses.","title":"size"},{"location":"API/Dendrite/#ANNarchy.core.Dendrite.Dendrite.synapses","text":"Iteratively returns the synapses corresponding to this dendrite.","title":"synapses"},{"location":"API/Dendrite/#ANNarchy.core.Dendrite.Dendrite.create_synapse","text":"Creates a synapse for this dendrite with the given pre-synaptic neuron. Parameters: Name Type Description Default rank rank of the pre-synaptic neuron required w synaptic weight (defalt: 0.0). 0.0 delay synaptic delay (default = dt) 0 Source code in ANNarchy/core/Dendrite.py def create_synapse ( self , rank , w = 0.0 , delay = 0 ): \"\"\" Creates a synapse for this dendrite with the given pre-synaptic neuron. :param rank: rank of the pre-synaptic neuron :param w: synaptic weight (defalt: 0.0). :param delay: synaptic delay (default = dt) \"\"\" if not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), can not add the synapse.' ) return if self . proj . cyInstance . dendrite_index ( self . post_rank , rank ) != - 1 : Global . _error ( 'the synapse of rank ' + str ( rank ) + ' already exists.' ) return # Set default values for the additional variables extra_attributes = {} for var in self . proj . synapse_type . description [ 'parameters' ] + self . proj . synapse_type . description [ 'variables' ]: if not var [ 'name' ] in [ 'w' , 'delay' ] and var [ 'name' ] in self . proj . synapse_type . description [ 'local' ]: if not isinstance ( self . proj . init [ var [ 'name' ]], ( int , float , bool )): init = var [ 'init' ] else : init = self . proj . init [ var [ 'name' ]] extra_attributes [ var [ 'name' ]] = init try : self . proj . cyInstance . add_synapse ( self . post_rank , rank , w , int ( delay / Global . config [ 'dt' ]), ** extra_attributes ) except Exception as e : Global . _print ( e )","title":"create_synapse()"},{"location":"API/Dendrite/#ANNarchy.core.Dendrite.Dendrite.get","text":"Returns the value of a variable/parameter. Example: dendrite . get ( 'w' ) Parameters: Name Type Description Default name name of the parameter/variable. required Source code in ANNarchy/core/Dendrite.py def get ( self , name ): \"\"\" Returns the value of a variable/parameter. Example: ```python dendrite.get('w') ``` :param name: name of the parameter/variable. \"\"\" if name == 'rank' : Global . _warning ( \"Dendrite.get('rank'): the attribute is deprecated, use Dendrite.pre_ranks instead.\" ) return self . proj . cyInstance . pre_rank ( self . idx ) elif name == 'pre_ranks' : return self . proj . cyInstance . pre_rank ( self . idx ) elif name in self . attributes : return getattr ( self , name ) else : Global . _error ( \"Dendrite has no parameter/variable called\" , name )","title":"get()"},{"location":"API/Dendrite/#ANNarchy.core.Dendrite.Dendrite.prune_synapse","text":"Removes the synapse with the given pre-synaptic neuron from the dendrite. Parameters: Name Type Description Default rank rank of the pre-synaptic neuron required Source code in ANNarchy/core/Dendrite.py def prune_synapse ( self , rank ): \"\"\" Removes the synapse with the given pre-synaptic neuron from the dendrite. :param rank: rank of the pre-synaptic neuron \"\"\" if not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), can not remove the synapse.' ) return if not rank in self . pre_ranks : Global . _error ( 'the synapse with the pre-synaptic neuron of rank ' + str ( rank ) + ' did not already exist.' ) return self . proj . cyInstance . remove_synapse ( self . post_rank , rank )","title":"prune_synapse()"},{"location":"API/Dendrite/#ANNarchy.core.Dendrite.Dendrite.receptive_field","text":"Returns the given variable as a receptive field. A Numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value fill ). Parameters: Name Type Description Default variable name of the variable (default = 'w') 'w' fill value to use when a synapse does not exist (default: 0.0). 0.0 Source code in ANNarchy/core/Dendrite.py def receptive_field ( self , variable = 'w' , fill = 0.0 ): \"\"\" Returns the given variable as a receptive field. A Numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value ``fill``). :param variable: name of the variable (default = 'w') :param fill: value to use when a synapse does not exist (default: 0.0). \"\"\" values = getattr ( self . proj . cyInstance , 'get_dendrite_' + variable )( self . idx ) pre_ranks = self . proj . cyInstance . pre_rank ( self . idx ) m = fill * np . ones ( self . pre . size ) m [ pre_ranks ] = values return m . reshape ( self . pre . geometry )","title":"receptive_field()"},{"location":"API/Dendrite/#ANNarchy.core.Dendrite.Dendrite.set","text":"Sets the value of a parameter/variable of all synapses. Example: dendrite . set ( { 'tau' : 20 , 'w' = Uniform ( 0.0 , 1.0 ) } ) Parameters: Name Type Description Default value a dictionary containing the parameter/variable names as keys. required Source code in ANNarchy/core/Dendrite.py def set ( self , value ): \"\"\" Sets the value of a parameter/variable of all synapses. Example: ```python dendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } ) ``` :param value: a dictionary containing the parameter/variable names as keys. \"\"\" for key , value in value . items (): # sanity check and then forward to __setattr__ if key in self . attributes : setattr ( self , key , value ) else : Global . _error ( \"Dendrite has no parameter/variable called\" , key )","title":"set()"},{"location":"API/Dendrite/#ANNarchy.core.Dendrite.Dendrite.synapse","text":"Returns the synapse coming from the corresponding presynaptic neuron. Parameters: Name Type Description Default pos can be either the rank or the coordinates of the presynaptic neuron required Source code in ANNarchy/core/Dendrite.py def synapse ( self , pos ): \"\"\" Returns the synapse coming from the corresponding presynaptic neuron. :param pos: can be either the rank or the coordinates of the presynaptic neuron \"\"\" if isinstance ( pos , int ): rank = pos else : rank = self . proj . pre . rank_from_coordinates ( pos ) if rank in self . pre_ranks : return IndividualSynapse ( self , rank ) else : Global . _error ( \" The neuron of rank \" + str ( rank ) + \" has no synapse in this dendrite.\" ) return None","title":"synapse()"},{"location":"API/Hybrid/","text":"Hybrid networks # Converting a rate-coded population to a spiking population requires connecting a PoissonPopulation with the rate-coded one. Converting a spiking population with a rate-coded one requires the use of a DecodingProjection , which can connected using any connector method available for Projection . DecodingProjection # Decoding projection to transform spike trains into firing rates. The pre-synaptic population must be a spiking population, while the post-synaptic one must be rate-coded. Pre-synaptic spikes are accumulated for each post-synaptic neuron. A sliding window can be used to smoothen the results with the window parameter. The decoded firing rate is accessible in the post-synaptic neurons with sum(target) . The projection can be connected using any method available in Projection (although all-to-all or many-to-one makes mostly sense). Delays are ignored. The weight value allows to scale the firing rate: if you want a pre-synaptic firing rate of 100 Hz to correspond to a post-synaptic rate of 1.0, use w = 1./100. . Example: pop1 = PoissonPopulation ( 1000 , rates = 100. ) pop2 = Population ( 1 , Neuron ( equations = \"r=sum(exc)\" )) proj = DecodingProjection ( pop1 , pop2 , 'exc' , window = 10.0 ) proj . connect_all_to_all ( 1.0 , force_multiple_weights = True ) __init__ ( self , pre , post , target , window = 0.0 , name = None , copied = False ) special # Parameters: Name Type Description Default pre pre-synaptic population. required post post-synaptic population. required target type of the connection. required window duration of the time window to collect spikes (default: dt). 0.0 Source code in ANNarchy/core/SpecificProjection.py def __init__ ( self , pre , post , target , window = 0.0 , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population. :param post: post-synaptic population. :param target: type of the connection. :param window: duration of the time window to collect spikes (default: dt). \"\"\" # Instantiate the projection SpecificProjection . __init__ ( self , pre , post , target , None , name , copied ) # Check populations if not self . pre . neuron_type . type == 'spike' : Global . _error ( 'The pre-synaptic population of a DecodingProjection must be spiking.' ) if not self . post . neuron_type . type == 'rate' : Global . _error ( 'The post-synaptic population of a DecodingProjection must be rate-coded.' ) # Process window argument if window == 0.0 : window = Global . config [ 'dt' ] self . window = window # Disable openMP post-synaptic matrix split self . _no_split_matrix = True # Not on CUDA if Global . _check_paradigm ( 'cuda' ): Global . _error ( 'DecodingProjections are not available on CUDA yet.' )","title":"Hybrid networks"},{"location":"API/Hybrid/#hybrid-networks","text":"Converting a rate-coded population to a spiking population requires connecting a PoissonPopulation with the rate-coded one. Converting a spiking population with a rate-coded one requires the use of a DecodingProjection , which can connected using any connector method available for Projection .","title":"Hybrid networks"},{"location":"API/Hybrid/#ANNarchy.core.SpecificProjection.DecodingProjection","text":"Decoding projection to transform spike trains into firing rates. The pre-synaptic population must be a spiking population, while the post-synaptic one must be rate-coded. Pre-synaptic spikes are accumulated for each post-synaptic neuron. A sliding window can be used to smoothen the results with the window parameter. The decoded firing rate is accessible in the post-synaptic neurons with sum(target) . The projection can be connected using any method available in Projection (although all-to-all or many-to-one makes mostly sense). Delays are ignored. The weight value allows to scale the firing rate: if you want a pre-synaptic firing rate of 100 Hz to correspond to a post-synaptic rate of 1.0, use w = 1./100. . Example: pop1 = PoissonPopulation ( 1000 , rates = 100. ) pop2 = Population ( 1 , Neuron ( equations = \"r=sum(exc)\" )) proj = DecodingProjection ( pop1 , pop2 , 'exc' , window = 10.0 ) proj . connect_all_to_all ( 1.0 , force_multiple_weights = True )","title":"DecodingProjection"},{"location":"API/Hybrid/#ANNarchy.core.SpecificProjection.DecodingProjection.__init__","text":"Parameters: Name Type Description Default pre pre-synaptic population. required post post-synaptic population. required target type of the connection. required window duration of the time window to collect spikes (default: dt). 0.0 Source code in ANNarchy/core/SpecificProjection.py def __init__ ( self , pre , post , target , window = 0.0 , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population. :param post: post-synaptic population. :param target: type of the connection. :param window: duration of the time window to collect spikes (default: dt). \"\"\" # Instantiate the projection SpecificProjection . __init__ ( self , pre , post , target , None , name , copied ) # Check populations if not self . pre . neuron_type . type == 'spike' : Global . _error ( 'The pre-synaptic population of a DecodingProjection must be spiking.' ) if not self . post . neuron_type . type == 'rate' : Global . _error ( 'The post-synaptic population of a DecodingProjection must be rate-coded.' ) # Process window argument if window == 0.0 : window = Global . config [ 'dt' ] self . window = window # Disable openMP post-synaptic matrix split self . _no_split_matrix = True # Not on CUDA if Global . _check_paradigm ( 'cuda' ): Global . _error ( 'DecodingProjections are not available on CUDA yet.' )","title":"__init__()"},{"location":"API/IO/","text":"Saving / Loading # Saving / loading the state of the network # To save or load the network state you can use the following methods: save ( filename , populations = True , projections = True , net_id = 0 ) # Save the current network state (parameters and variables) to a file. If the extension is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the extension is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. If the extension ends with '.gz', the data will be pickled into a binary file and compressed using gzip. Otherwise, the data will be pickled into a simple binary text file using cPickle. Warning: The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: save ( 'results/init.npz' ) save ( 'results/init.data' ) save ( 'results/init.txt.gz' ) save ( '1000_trials.mat' ) Parameters: Name Type Description Default filename filename, may contain relative or absolute path. required populations if True, population data will be saved (by default True) True projections if True, projection data will be saved (by default True) True Source code in ANNarchy/core/IO.py def save ( filename , populations = True , projections = True , net_id = 0 ): #, pure_data=True): \"\"\" Save the current network state (parameters and variables) to a file. * If the extension is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the extension is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * If the extension ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * Otherwise, the data will be pickled into a simple binary text file using cPickle. **Warning:** The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: ```python save('results/init.npz') save('results/init.data') save('results/init.txt.gz') save('1000_trials.mat') ``` :param filename: filename, may contain relative or absolute path. :param populations: if True, population data will be saved (by default True) :param projections: if True, projection data will be saved (by default True) \"\"\" data = _net_description ( populations , projections , net_id ) _save_data ( filename , data ) load ( filename , populations = True , projections = True , net_id = 0 ) # Loads a saved state of the network. Warning: Matlab data can not be loaded. Example: load ( 'results/network.npz' ) Parameters: Name Type Description Default filename the filename with relative or absolute path. required populations if True, population data will be loaded (by default True) True projections if True, projection data will be loaded (by default True) True Source code in ANNarchy/core/IO.py def load ( filename , populations = True , projections = True , net_id = 0 ): \"\"\" Loads a saved state of the network. **Warning:** Matlab data can not be loaded. Example: ```python load('results/network.npz') ``` :param filename: the filename with relative or absolute path. :param populations: if True, population data will be loaded (by default True) :param projections: if True, projection data will be loaded (by default True) \"\"\" desc = _load_data ( filename ) if desc is None : return if 'time_step' in desc . keys (): Global . set_current_step ( desc [ 'time_step' ], net_id ) if populations : # Over all populations for pop in Global . _network [ net_id ][ 'populations' ]: # check if the population is contained in save file if pop . name in desc . keys (): pop . _load_pop_data ( desc [ pop . name ]) if projections : for proj in Global . _network [ net_id ][ 'projections' ] : if proj . name in desc . keys (): proj . _load_proj_data ( desc [ proj . name ]) Please note that these functions are only usable after the call to ANNarchy.compile() . Saving / loading the parameters of the network # save_parameters ( filename , net_id = 0 ) # Saves the global parameters of a network (flag population for neurons, projection for synapses) to a JSON file. Parameters: Name Type Description Default filename path to the JSON file. required net_id ID of the network (default: 0, the global network). 0 Source code in ANNarchy/core/IO.py def save_parameters ( filename , net_id = 0 ): \"\"\" Saves the global parameters of a network (flag ``population`` for neurons, ``projection`` for synapses) to a JSON file. :param filename: path to the JSON file. :param net_id: ID of the network (default: 0, the global network). \"\"\" import json # Get the netowrk description network = Global . _network [ net_id ] # Dictionary of parameters description = { 'populations' : {}, 'projections' : {}, 'network' : {}, 'constants' : {}, } # Constants for constant in Global . _objects [ 'constants' ]: description [ 'constants' ][ constant . name ] = constant . value # Populations for pop in network [ 'populations' ]: # Get the neuron description neuron = pop . neuron_type pop_description = {} for param in neuron . description [ 'global' ]: pop_description [ param ] = pop . init [ param ] description [ 'populations' ][ pop . name ] = pop_description # Projections for proj in network [ 'projections' ]: # Get the synapse description synapse = proj . synapse_type proj_description = {} for param in synapse . description [ 'global' ]: proj_description [ param ] = proj . init [ param ] description [ 'projections' ][ proj . name ] = proj_description # Save the description in a json file with open ( filename , 'w' ) as wfile : json . dump ( description , wfile , indent = 4 ) load_parameters ( filename , global_only = True , verbose = False , net_id = 0 ) # Loads the global parameters of a network (flag population for neurons, projection for synapses) from a JSON file. It is advised to generate the JSON file first with save_parameters() and later edit it manually. A strong restriction is that population/projection names cannot change between saving and loading. By default, they take names such as pop0 or proj2 , we advise setting explicitly a name in their constructor for readability. If you add a parameter name to the JSON file but it does not exist in te neuron/synapse, it will be silently skipped. Enable verbose=True to see which parameters are effectively changed. If you set global_only to True, you will be able to set values for non-global parameters (e.g. synapse-specific), but a single value will be loaded for all. The JSON file cannot contain arrays. If you want to save/load the value of variables after a simulation, please refer to save() or load() . Parameters: Name Type Description Default filename path to the JSON file. required global_only True if only global parameters (flags population and projection ) should be loaded, the other values are ignored. (default: True) True verbose True if the old and new values of the parameters should be printed (default: False). False net_id ID of the network (default: 0, the global network). 0 Returns: Type Description a dictionary of additional parameters not related to populations or projections (keyword network in the JSON file). Source code in ANNarchy/core/IO.py def load_parameters ( filename , global_only = True , verbose = False , net_id = 0 ): \"\"\" Loads the global parameters of a network (flag ``population`` for neurons, ``projection`` for synapses) from a JSON file. It is advised to generate the JSON file first with ``save_parameters()`` and later edit it manually. A strong restriction is that population/projection names cannot change between saving and loading. By default, they take names such as ``pop0`` or ``proj2``, we advise setting explicitly a name in their constructor for readability. If you add a parameter name to the JSON file but it does not exist in te neuron/synapse, it will be silently skipped. Enable ``verbose=True`` to see which parameters are effectively changed. If you set ``global_only`` to True, you will be able to set values for non-global parameters (e.g. synapse-specific), but a single value will be loaded for all. The JSON file cannot contain arrays. If you want to save/load the value of variables after a simulation, please refer to ``save()`` or ``load()``. :param filename: path to the JSON file. :param global_only: True if only global parameters (flags ``population`` and ``projection``) should be loaded, the other values are ignored. (default: True) :param verbose: True if the old and new values of the parameters should be printed (default: False). :param net_id: ID of the network (default: 0, the global network). :return: a dictionary of additional parameters not related to populations or projections (keyword ``network`` in the JSON file). \"\"\" import json with open ( filename , 'r' ) as rfile : desc = json . load ( rfile ) if verbose : Global . _print ( 'Loading parameters from file' , filename ) Global . _print ( '-' * 40 ) # Populations try : populations = desc [ 'populations' ] except : populations = {} if verbose : Global . _print ( 'load_parameters(): no population parameters.' ) for name , parameters in populations . items (): # Get the population for pop in Global . _network [ net_id ][ 'populations' ]: if pop . name == name : population = pop break else : Global . _warning ( 'The population' , name , 'defined in the file' , filename , 'does not exist in the current network.' ) if verbose : Global . _print ( 'Population' , name ) # Set the parameters for name , val in parameters . items (): # Check that the variable indeed exists if not name in population . parameters : Global . _print ( ' ' , name , 'is not a global parameter of' , population . name , ', skipping.' ) continue if global_only and not name in population . neuron_type . description [ 'global' ]: Global . _print ( ' ' , name , 'is not a global parameter of' , population . name , ', skipping.' ) continue if verbose : Global . _print ( ' ' , name , ':' , population . get ( name ), '->' , val ) population . set ({ name : float ( val )}) # Projections try : projections = desc [ 'projections' ] except : projections = {} if verbose : Global . _print ( 'load_parameters(): no projection parameters.' ) for name , parameters in projections . items (): # Get the projection for proj in Global . _network [ net_id ][ 'projections' ]: if proj . name == name : projection = proj break else : Global . _warning ( 'The projection' , name , 'defined in the file' , filename , 'does not exist in the current network.' ) if verbose : Global . _print ( 'Projection' , name ) # Set the parameters for name , val in parameters . items (): # Check that the variable indeed exists if not name in projection . parameters : Global . _print ( ' ' , name , 'is not a global parameter of' , population . name , ', skipping.' ) continue if global_only and not name in projection . synapse_type . description [ 'global' ]: Global . _print ( ' ' , name , 'is not a global parameter of' , population . name , ', skipping.' ) continue if verbose : Global . _print ( ' ' , name , ':' , projection . get ( name ), '->' , val ) projection . set ({ name : float ( val )}) # Constants try : constants = desc [ 'constants' ] except : constants = {} if verbose : Global . _print ( 'load_parameters(): no constants.' ) for name , value in constants . items (): if name in Global . list_constants (): # modify it Global . get_constant ( name ) . value = value else : # create it _ = Global . Constant ( name , value ) # Global user-defined parameters try : network_parameters = {} for name , val in desc [ 'network' ] . items (): network_parameters [ name ] = float ( val ) except : network_parameters = {} return network_parameters","title":"Saving / Loading"},{"location":"API/IO/#saving-loading","text":"","title":"Saving / Loading"},{"location":"API/IO/#saving-loading-the-state-of-the-network","text":"To save or load the network state you can use the following methods:","title":"Saving / loading the state of the network"},{"location":"API/IO/#ANNarchy.core.IO.save","text":"Save the current network state (parameters and variables) to a file. If the extension is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the extension is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. If the extension ends with '.gz', the data will be pickled into a binary file and compressed using gzip. Otherwise, the data will be pickled into a simple binary text file using cPickle. Warning: The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: save ( 'results/init.npz' ) save ( 'results/init.data' ) save ( 'results/init.txt.gz' ) save ( '1000_trials.mat' ) Parameters: Name Type Description Default filename filename, may contain relative or absolute path. required populations if True, population data will be saved (by default True) True projections if True, projection data will be saved (by default True) True Source code in ANNarchy/core/IO.py def save ( filename , populations = True , projections = True , net_id = 0 ): #, pure_data=True): \"\"\" Save the current network state (parameters and variables) to a file. * If the extension is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the extension is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * If the extension ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * Otherwise, the data will be pickled into a simple binary text file using cPickle. **Warning:** The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: ```python save('results/init.npz') save('results/init.data') save('results/init.txt.gz') save('1000_trials.mat') ``` :param filename: filename, may contain relative or absolute path. :param populations: if True, population data will be saved (by default True) :param projections: if True, projection data will be saved (by default True) \"\"\" data = _net_description ( populations , projections , net_id ) _save_data ( filename , data )","title":"save()"},{"location":"API/IO/#ANNarchy.core.IO.load","text":"Loads a saved state of the network. Warning: Matlab data can not be loaded. Example: load ( 'results/network.npz' ) Parameters: Name Type Description Default filename the filename with relative or absolute path. required populations if True, population data will be loaded (by default True) True projections if True, projection data will be loaded (by default True) True Source code in ANNarchy/core/IO.py def load ( filename , populations = True , projections = True , net_id = 0 ): \"\"\" Loads a saved state of the network. **Warning:** Matlab data can not be loaded. Example: ```python load('results/network.npz') ``` :param filename: the filename with relative or absolute path. :param populations: if True, population data will be loaded (by default True) :param projections: if True, projection data will be loaded (by default True) \"\"\" desc = _load_data ( filename ) if desc is None : return if 'time_step' in desc . keys (): Global . set_current_step ( desc [ 'time_step' ], net_id ) if populations : # Over all populations for pop in Global . _network [ net_id ][ 'populations' ]: # check if the population is contained in save file if pop . name in desc . keys (): pop . _load_pop_data ( desc [ pop . name ]) if projections : for proj in Global . _network [ net_id ][ 'projections' ] : if proj . name in desc . keys (): proj . _load_proj_data ( desc [ proj . name ]) Please note that these functions are only usable after the call to ANNarchy.compile() .","title":"load()"},{"location":"API/IO/#saving-loading-the-parameters-of-the-network","text":"","title":"Saving / loading the parameters of the network"},{"location":"API/IO/#ANNarchy.core.IO.save_parameters","text":"Saves the global parameters of a network (flag population for neurons, projection for synapses) to a JSON file. Parameters: Name Type Description Default filename path to the JSON file. required net_id ID of the network (default: 0, the global network). 0 Source code in ANNarchy/core/IO.py def save_parameters ( filename , net_id = 0 ): \"\"\" Saves the global parameters of a network (flag ``population`` for neurons, ``projection`` for synapses) to a JSON file. :param filename: path to the JSON file. :param net_id: ID of the network (default: 0, the global network). \"\"\" import json # Get the netowrk description network = Global . _network [ net_id ] # Dictionary of parameters description = { 'populations' : {}, 'projections' : {}, 'network' : {}, 'constants' : {}, } # Constants for constant in Global . _objects [ 'constants' ]: description [ 'constants' ][ constant . name ] = constant . value # Populations for pop in network [ 'populations' ]: # Get the neuron description neuron = pop . neuron_type pop_description = {} for param in neuron . description [ 'global' ]: pop_description [ param ] = pop . init [ param ] description [ 'populations' ][ pop . name ] = pop_description # Projections for proj in network [ 'projections' ]: # Get the synapse description synapse = proj . synapse_type proj_description = {} for param in synapse . description [ 'global' ]: proj_description [ param ] = proj . init [ param ] description [ 'projections' ][ proj . name ] = proj_description # Save the description in a json file with open ( filename , 'w' ) as wfile : json . dump ( description , wfile , indent = 4 )","title":"save_parameters()"},{"location":"API/IO/#ANNarchy.core.IO.load_parameters","text":"Loads the global parameters of a network (flag population for neurons, projection for synapses) from a JSON file. It is advised to generate the JSON file first with save_parameters() and later edit it manually. A strong restriction is that population/projection names cannot change between saving and loading. By default, they take names such as pop0 or proj2 , we advise setting explicitly a name in their constructor for readability. If you add a parameter name to the JSON file but it does not exist in te neuron/synapse, it will be silently skipped. Enable verbose=True to see which parameters are effectively changed. If you set global_only to True, you will be able to set values for non-global parameters (e.g. synapse-specific), but a single value will be loaded for all. The JSON file cannot contain arrays. If you want to save/load the value of variables after a simulation, please refer to save() or load() . Parameters: Name Type Description Default filename path to the JSON file. required global_only True if only global parameters (flags population and projection ) should be loaded, the other values are ignored. (default: True) True verbose True if the old and new values of the parameters should be printed (default: False). False net_id ID of the network (default: 0, the global network). 0 Returns: Type Description a dictionary of additional parameters not related to populations or projections (keyword network in the JSON file). Source code in ANNarchy/core/IO.py def load_parameters ( filename , global_only = True , verbose = False , net_id = 0 ): \"\"\" Loads the global parameters of a network (flag ``population`` for neurons, ``projection`` for synapses) from a JSON file. It is advised to generate the JSON file first with ``save_parameters()`` and later edit it manually. A strong restriction is that population/projection names cannot change between saving and loading. By default, they take names such as ``pop0`` or ``proj2``, we advise setting explicitly a name in their constructor for readability. If you add a parameter name to the JSON file but it does not exist in te neuron/synapse, it will be silently skipped. Enable ``verbose=True`` to see which parameters are effectively changed. If you set ``global_only`` to True, you will be able to set values for non-global parameters (e.g. synapse-specific), but a single value will be loaded for all. The JSON file cannot contain arrays. If you want to save/load the value of variables after a simulation, please refer to ``save()`` or ``load()``. :param filename: path to the JSON file. :param global_only: True if only global parameters (flags ``population`` and ``projection``) should be loaded, the other values are ignored. (default: True) :param verbose: True if the old and new values of the parameters should be printed (default: False). :param net_id: ID of the network (default: 0, the global network). :return: a dictionary of additional parameters not related to populations or projections (keyword ``network`` in the JSON file). \"\"\" import json with open ( filename , 'r' ) as rfile : desc = json . load ( rfile ) if verbose : Global . _print ( 'Loading parameters from file' , filename ) Global . _print ( '-' * 40 ) # Populations try : populations = desc [ 'populations' ] except : populations = {} if verbose : Global . _print ( 'load_parameters(): no population parameters.' ) for name , parameters in populations . items (): # Get the population for pop in Global . _network [ net_id ][ 'populations' ]: if pop . name == name : population = pop break else : Global . _warning ( 'The population' , name , 'defined in the file' , filename , 'does not exist in the current network.' ) if verbose : Global . _print ( 'Population' , name ) # Set the parameters for name , val in parameters . items (): # Check that the variable indeed exists if not name in population . parameters : Global . _print ( ' ' , name , 'is not a global parameter of' , population . name , ', skipping.' ) continue if global_only and not name in population . neuron_type . description [ 'global' ]: Global . _print ( ' ' , name , 'is not a global parameter of' , population . name , ', skipping.' ) continue if verbose : Global . _print ( ' ' , name , ':' , population . get ( name ), '->' , val ) population . set ({ name : float ( val )}) # Projections try : projections = desc [ 'projections' ] except : projections = {} if verbose : Global . _print ( 'load_parameters(): no projection parameters.' ) for name , parameters in projections . items (): # Get the projection for proj in Global . _network [ net_id ][ 'projections' ]: if proj . name == name : projection = proj break else : Global . _warning ( 'The projection' , name , 'defined in the file' , filename , 'does not exist in the current network.' ) if verbose : Global . _print ( 'Projection' , name ) # Set the parameters for name , val in parameters . items (): # Check that the variable indeed exists if not name in projection . parameters : Global . _print ( ' ' , name , 'is not a global parameter of' , population . name , ', skipping.' ) continue if global_only and not name in projection . synapse_type . description [ 'global' ]: Global . _print ( ' ' , name , 'is not a global parameter of' , population . name , ', skipping.' ) continue if verbose : Global . _print ( ' ' , name , ':' , projection . get ( name ), '->' , val ) projection . set ({ name : float ( val )}) # Constants try : constants = desc [ 'constants' ] except : constants = {} if verbose : Global . _print ( 'load_parameters(): no constants.' ) for name , value in constants . items (): if name in Global . list_constants (): # modify it Global . get_constant ( name ) . value = value else : # create it _ = Global . Constant ( name , value ) # Global user-defined parameters try : network_parameters = {} for name , val in desc [ 'network' ] . items (): network_parameters [ name ] = float ( val ) except : network_parameters = {} return network_parameters","title":"load_parameters()"},{"location":"API/Logging/","text":"Logging with tensorboard # Logging utilities based on tensorboard are provided in the module ANNarchy.extensions.tensorboard , which must be explicitely imported: from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger The main object in that module is the Logger class. Logger # Logger # Logger class to use tensorboard to visualize ANNarchy simulations. Requires the tensorboardX package (pip install tensorboardX). The Logger class is a thin wrapper around tensorboardX.SummaryWriter, which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io/ . Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch. The extension has to be imported explictly: from ANNarchy.extensions.tensorboard import Logger The Logger class has to be closed properly at the end of the script, so it is advised to use a context: with Logger () as logger : logger . add_scalar ( \"Accuracy\" , acc , trial ) You can also make sure to close it: logger = Logger () logger . add_scalar ( \"Accuracy\" , acc , trial ) logger . close () By default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine . You can control these two elements by passing arguments to Logger() : with Logger ( logdir = \"/tmp/annarchy\" , experiment = \"trial1\" ): # logs in /tmp/annarchy/trial1 The add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc. A tag should be given to each plot. In the example above, the figure with the accuracy will be labelled \"Accuracy\" in tensorboard. You can also group plots together with tags such as \"Global performance/Accuracy\", \"Global performance/Error rate\", \"Neural activity/Population 1\", etc. After (or while) logging data within your simulation, run tensorboard in the terminal by specifying the log directory: tensorboard --logdir runs TensorboardX enqueues the data in memory before writing to disk. You can force flushing with: logger . flush () __init__ ( self , logdir = 'runs/' , experiment = None ) special # Parameters: Name Type Description Default logdir path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\" 'runs/' experiment name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended. None Source code in ANNarchy/extensions/tensorboard/Logger.py def __init__ ( self , logdir = \"runs/\" , experiment = None ): \"\"\" :param logdir: path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\" :param experiment: name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended. \"\"\" self . logdir = logdir self . experiment = experiment # Create the logdir if it does not exist if not os . path . exists ( self . logdir ): os . makedirs ( self . logdir ) if not experiment : current_time = datetime . now () . strftime ( '%b %d _%H-%M-%S' ) self . currentlogdir = os . path . join ( self . logdir , current_time + '_' + socket . gethostname ()) else : self . currentlogdir = self . logdir + \"/\" + self . experiment print ( \"Logging in\" , self . currentlogdir ) self . _create_summary_writer () add_figure ( self , tag , figure , step = None , close = True ) # Logs a Matplotlib figure. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) fig = plt . figure () plt . plot ( pop . r ) logger . add_figure ( \"Activity\" , fig , trial ) Parameters: Name Type Description Default tag name of the image in tensorboard. required figure a list or 1D numpy array of values. required step time index. None close whether the logger will close the figure when done (default: True). True Source code in ANNarchy/extensions/tensorboard/Logger.py def add_figure ( self , tag , figure , step = None , close = True ): \"\"\" Logs a Matplotlib figure. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) fig = plt.figure() plt.plot(pop.r) logger.add_figure(\"Activity\", fig, trial) ``` :param tag: name of the image in tensorboard. :param figure: a list or 1D numpy array of values. :param step: time index. :param close: whether the logger will close the figure when done (default: True). \"\"\" import matplotlib.pyplot as plt import matplotlib.backends.backend_agg as plt_backend_agg canvas = plt_backend_agg . FigureCanvasAgg ( figure ) canvas . draw () data = np . frombuffer ( canvas . buffer_rgba (), dtype = np . uint8 ) w , h = figure . canvas . get_width_height () image_hwc = data . reshape ([ h , w , 4 ])[:, :, 0 : 3 ] image_chw = np . moveaxis ( image_hwc , source = 2 , destination = 0 ) if close : plt . close ( figure ) self . _summary . add_image ( tag , image_chw , step ) add_histogram ( self , tag , hist , step = None ) # Logs an histogram. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . flatten () logger . add_histogram ( \"Weight distribution\" , weights , trial ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required hist a list or 1D numpy array of values. required step time index. None Source code in ANNarchy/extensions/tensorboard/Logger.py def add_histogram ( self , tag , hist , step = None ): \"\"\" Logs an histogram. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) weights= proj.w.flatten() logger.add_histogram(\"Weight distribution\", weights, trial) ``` :param tag: name of the figure in tensorboard. :param hist: a list or 1D numpy array of values. :param step: time index. \"\"\" self . _summary . add_histogram ( tag , hist , step ) add_image ( self , tag , img , step = None , equalize = False ) # Logs an image. The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values. Example:: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) img = pop . r . reshape (( 10 , 10 )) logger . add_image ( \"Population / Firing rate\" , img , trial , equalize = True ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required img array for the image. required step time index. None equalize rescales the pixels between 0 and 1 using the min and max values of the array. False Source code in ANNarchy/extensions/tensorboard/Logger.py def add_image ( self , tag , img , step = None , equalize = False ): \"\"\" Logs an image. The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values. Example:: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) img = pop.r.reshape((10, 10)) logger.add_image(\"Population / Firing rate\", img, trial, equalize=True) ``` :param tag: name of the figure in tensorboard. :param img: array for the image. :param step: time index. :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array. \"\"\" if img . ndim == 2 : if equalize : img = img . astype ( np . float ) img = ( img - img . min ()) / ( img . max () - img . min ()) self . _summary . add_image ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'HW' ) elif img . ndim == 3 : if not img . shape [ 2 ] == 3 : Global . _error ( \"Logger.add_image: color images must be of shape (H, W, 3).\" ) if equalize : img = np . array ( img ) . astype ( np . float ) img = ( img - img . min ()) / ( img . max () - img . min ()) self . _summary . add_image ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'HWC' ) else : Global . _error ( \"Logger.add_image: images must be of shape (H, W) or (H, W, 3).\" ) add_images ( self , tag , img , step = None , equalize = False , equalize_per_image = False ) # Logs a set of images (e.g. receptive fields). The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . reshape ( 100 , 10 , 10 ) # 100 post neurons, 10*10 pre neurons logger . add_images ( \"Projection/Receptive fields\" , weights , trial , equalize = True ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required img array for the images. required step time index. None equalize rescales the pixels between 0 and 1 using the min and max values of the array. False equalize_per_image whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False. False Source code in ANNarchy/extensions/tensorboard/Logger.py def add_images ( self , tag , img , step = None , equalize = False , equalize_per_image = False ): \"\"\" Logs a set of images (e.g. receptive fields). The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True) ``` :param tag: name of the figure in tensorboard. :param img: array for the images. :param step: time index. :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array. :param equalize_per_image: whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False. \"\"\" if img . ndim == 3 : img = np . expand_dims ( img , axis = 3 ) if equalize : img = np . array ( img ) . astype ( np . float ) if not equalize_per_image : img = ( img - img . min ()) / ( img . max () - img . min ()) else : for i in range ( img . shape [ 0 ]): img [ i , ... ] = ( img [ i , ... ] - img [ i , ... ] . min ()) / ( img [ i , ... ] . max () - img [ i , ... ] . min ()) self . _summary . add_images ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'NHWC' ) add_parameters ( self , params , metrics ) # Logs parameters of a simulation. This should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc. Example: with Logger () as logger : # ... logger . add_parameters ({ 'learning_rate' : lr , 'tau' : tau }, { 'accuracy' : accuracy }) Parameters: Name Type Description Default params dictionary of parameters. required metrics dictionary of metrics. required Source code in ANNarchy/extensions/tensorboard/Logger.py def add_parameters ( self , params , metrics ): \"\"\" Logs parameters of a simulation. This should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc. Example: ```python with Logger() as logger: # ... logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy}) ``` :param params: dictionary of parameters. :param metrics: dictionary of metrics. \"\"\" self . _summary . add_hparams ( params , metrics ) add_scalar ( self , tag , value , step = None ) # Logs a single scalar value, e.g. a success rate at various stages of learning. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) accuracy = ... logger . add_scalar ( \"Accuracy\" , accuracy , trial ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required value value. required step time index. None Source code in ANNarchy/extensions/tensorboard/Logger.py def add_scalar ( self , tag , value , step = None ): \"\"\" Logs a single scalar value, e.g. a success rate at various stages of learning. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) accuracy = ... logger.add_scalar(\"Accuracy\", accuracy, trial) ``` :param tag: name of the figure in tensorboard. :param value: value. :param step: time index. \"\"\" self . _summary . add_scalar ( tag = tag , scalar_value = value , global_step = step , walltime = None ) add_scalars ( self , tag , value , step = None ) # Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) act1 = pop . r [ 0 ] act2 = pop . r [ 1 ] logger . add_scalars ( \"Accuracy\" , { 'First neuron' : act1 , 'Second neuron' : act2 }, trial ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required value dictionary of values. required step time index. None Source code in ANNarchy/extensions/tensorboard/Logger.py def add_scalars ( self , tag , value , step = None ): \"\"\" Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) act1 = pop.r[0] act2 = pop.r[1] logger.add_scalars( \"Accuracy\", {'First neuron': act1, 'Second neuron': act2}, trial) ``` :param tag: name of the figure in tensorboard. :param value: dictionary of values. :param step: time index. \"\"\" self . _summary . add_scalars ( main_tag = tag , tag_scalar_dict = value , global_step = step , walltime = None ) close ( self ) # Closes the logger. Source code in ANNarchy/extensions/tensorboard/Logger.py def close ( self ): \"Closes the logger.\" self . _summary . close () flush ( self ) # Forces the logged data to be flushed to disk. Source code in ANNarchy/extensions/tensorboard/Logger.py def flush ( self ): \"Forces the logged data to be flushed to disk.\" self . _summary . flush ()","title":"Logging with tensorboard"},{"location":"API/Logging/#logging-with-tensorboard","text":"Logging utilities based on tensorboard are provided in the module ANNarchy.extensions.tensorboard , which must be explicitely imported: from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger The main object in that module is the Logger class.","title":"Logging with tensorboard"},{"location":"API/Logging/#ANNarchy.extensions.tensorboard.Logger","text":"","title":"Logger"},{"location":"API/Logging/#ANNarchy.extensions.tensorboard.Logger.Logger","text":"Logger class to use tensorboard to visualize ANNarchy simulations. Requires the tensorboardX package (pip install tensorboardX). The Logger class is a thin wrapper around tensorboardX.SummaryWriter, which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io/ . Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch. The extension has to be imported explictly: from ANNarchy.extensions.tensorboard import Logger The Logger class has to be closed properly at the end of the script, so it is advised to use a context: with Logger () as logger : logger . add_scalar ( \"Accuracy\" , acc , trial ) You can also make sure to close it: logger = Logger () logger . add_scalar ( \"Accuracy\" , acc , trial ) logger . close () By default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine . You can control these two elements by passing arguments to Logger() : with Logger ( logdir = \"/tmp/annarchy\" , experiment = \"trial1\" ): # logs in /tmp/annarchy/trial1 The add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc. A tag should be given to each plot. In the example above, the figure with the accuracy will be labelled \"Accuracy\" in tensorboard. You can also group plots together with tags such as \"Global performance/Accuracy\", \"Global performance/Error rate\", \"Neural activity/Population 1\", etc. After (or while) logging data within your simulation, run tensorboard in the terminal by specifying the log directory: tensorboard --logdir runs TensorboardX enqueues the data in memory before writing to disk. You can force flushing with: logger . flush ()","title":"Logger"},{"location":"API/Logging/#ANNarchy.extensions.tensorboard.Logger.Logger.__init__","text":"Parameters: Name Type Description Default logdir path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\" 'runs/' experiment name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended. None Source code in ANNarchy/extensions/tensorboard/Logger.py def __init__ ( self , logdir = \"runs/\" , experiment = None ): \"\"\" :param logdir: path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\" :param experiment: name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended. \"\"\" self . logdir = logdir self . experiment = experiment # Create the logdir if it does not exist if not os . path . exists ( self . logdir ): os . makedirs ( self . logdir ) if not experiment : current_time = datetime . now () . strftime ( '%b %d _%H-%M-%S' ) self . currentlogdir = os . path . join ( self . logdir , current_time + '_' + socket . gethostname ()) else : self . currentlogdir = self . logdir + \"/\" + self . experiment print ( \"Logging in\" , self . currentlogdir ) self . _create_summary_writer ()","title":"__init__()"},{"location":"API/Logging/#ANNarchy.extensions.tensorboard.Logger.Logger.add_figure","text":"Logs a Matplotlib figure. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) fig = plt . figure () plt . plot ( pop . r ) logger . add_figure ( \"Activity\" , fig , trial ) Parameters: Name Type Description Default tag name of the image in tensorboard. required figure a list or 1D numpy array of values. required step time index. None close whether the logger will close the figure when done (default: True). True Source code in ANNarchy/extensions/tensorboard/Logger.py def add_figure ( self , tag , figure , step = None , close = True ): \"\"\" Logs a Matplotlib figure. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) fig = plt.figure() plt.plot(pop.r) logger.add_figure(\"Activity\", fig, trial) ``` :param tag: name of the image in tensorboard. :param figure: a list or 1D numpy array of values. :param step: time index. :param close: whether the logger will close the figure when done (default: True). \"\"\" import matplotlib.pyplot as plt import matplotlib.backends.backend_agg as plt_backend_agg canvas = plt_backend_agg . FigureCanvasAgg ( figure ) canvas . draw () data = np . frombuffer ( canvas . buffer_rgba (), dtype = np . uint8 ) w , h = figure . canvas . get_width_height () image_hwc = data . reshape ([ h , w , 4 ])[:, :, 0 : 3 ] image_chw = np . moveaxis ( image_hwc , source = 2 , destination = 0 ) if close : plt . close ( figure ) self . _summary . add_image ( tag , image_chw , step )","title":"add_figure()"},{"location":"API/Logging/#ANNarchy.extensions.tensorboard.Logger.Logger.add_histogram","text":"Logs an histogram. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . flatten () logger . add_histogram ( \"Weight distribution\" , weights , trial ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required hist a list or 1D numpy array of values. required step time index. None Source code in ANNarchy/extensions/tensorboard/Logger.py def add_histogram ( self , tag , hist , step = None ): \"\"\" Logs an histogram. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) weights= proj.w.flatten() logger.add_histogram(\"Weight distribution\", weights, trial) ``` :param tag: name of the figure in tensorboard. :param hist: a list or 1D numpy array of values. :param step: time index. \"\"\" self . _summary . add_histogram ( tag , hist , step )","title":"add_histogram()"},{"location":"API/Logging/#ANNarchy.extensions.tensorboard.Logger.Logger.add_image","text":"Logs an image. The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values. Example:: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) img = pop . r . reshape (( 10 , 10 )) logger . add_image ( \"Population / Firing rate\" , img , trial , equalize = True ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required img array for the image. required step time index. None equalize rescales the pixels between 0 and 1 using the min and max values of the array. False Source code in ANNarchy/extensions/tensorboard/Logger.py def add_image ( self , tag , img , step = None , equalize = False ): \"\"\" Logs an image. The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values. Example:: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) img = pop.r.reshape((10, 10)) logger.add_image(\"Population / Firing rate\", img, trial, equalize=True) ``` :param tag: name of the figure in tensorboard. :param img: array for the image. :param step: time index. :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array. \"\"\" if img . ndim == 2 : if equalize : img = img . astype ( np . float ) img = ( img - img . min ()) / ( img . max () - img . min ()) self . _summary . add_image ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'HW' ) elif img . ndim == 3 : if not img . shape [ 2 ] == 3 : Global . _error ( \"Logger.add_image: color images must be of shape (H, W, 3).\" ) if equalize : img = np . array ( img ) . astype ( np . float ) img = ( img - img . min ()) / ( img . max () - img . min ()) self . _summary . add_image ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'HWC' ) else : Global . _error ( \"Logger.add_image: images must be of shape (H, W) or (H, W, 3).\" )","title":"add_image()"},{"location":"API/Logging/#ANNarchy.extensions.tensorboard.Logger.Logger.add_images","text":"Logs a set of images (e.g. receptive fields). The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . reshape ( 100 , 10 , 10 ) # 100 post neurons, 10*10 pre neurons logger . add_images ( \"Projection/Receptive fields\" , weights , trial , equalize = True ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required img array for the images. required step time index. None equalize rescales the pixels between 0 and 1 using the min and max values of the array. False equalize_per_image whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False. False Source code in ANNarchy/extensions/tensorboard/Logger.py def add_images ( self , tag , img , step = None , equalize = False , equalize_per_image = False ): \"\"\" Logs a set of images (e.g. receptive fields). The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True) ``` :param tag: name of the figure in tensorboard. :param img: array for the images. :param step: time index. :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array. :param equalize_per_image: whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False. \"\"\" if img . ndim == 3 : img = np . expand_dims ( img , axis = 3 ) if equalize : img = np . array ( img ) . astype ( np . float ) if not equalize_per_image : img = ( img - img . min ()) / ( img . max () - img . min ()) else : for i in range ( img . shape [ 0 ]): img [ i , ... ] = ( img [ i , ... ] - img [ i , ... ] . min ()) / ( img [ i , ... ] . max () - img [ i , ... ] . min ()) self . _summary . add_images ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'NHWC' )","title":"add_images()"},{"location":"API/Logging/#ANNarchy.extensions.tensorboard.Logger.Logger.add_parameters","text":"Logs parameters of a simulation. This should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc. Example: with Logger () as logger : # ... logger . add_parameters ({ 'learning_rate' : lr , 'tau' : tau }, { 'accuracy' : accuracy }) Parameters: Name Type Description Default params dictionary of parameters. required metrics dictionary of metrics. required Source code in ANNarchy/extensions/tensorboard/Logger.py def add_parameters ( self , params , metrics ): \"\"\" Logs parameters of a simulation. This should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc. Example: ```python with Logger() as logger: # ... logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy}) ``` :param params: dictionary of parameters. :param metrics: dictionary of metrics. \"\"\" self . _summary . add_hparams ( params , metrics )","title":"add_parameters()"},{"location":"API/Logging/#ANNarchy.extensions.tensorboard.Logger.Logger.add_scalar","text":"Logs a single scalar value, e.g. a success rate at various stages of learning. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) accuracy = ... logger . add_scalar ( \"Accuracy\" , accuracy , trial ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required value value. required step time index. None Source code in ANNarchy/extensions/tensorboard/Logger.py def add_scalar ( self , tag , value , step = None ): \"\"\" Logs a single scalar value, e.g. a success rate at various stages of learning. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) accuracy = ... logger.add_scalar(\"Accuracy\", accuracy, trial) ``` :param tag: name of the figure in tensorboard. :param value: value. :param step: time index. \"\"\" self . _summary . add_scalar ( tag = tag , scalar_value = value , global_step = step , walltime = None )","title":"add_scalar()"},{"location":"API/Logging/#ANNarchy.extensions.tensorboard.Logger.Logger.add_scalars","text":"Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) act1 = pop . r [ 0 ] act2 = pop . r [ 1 ] logger . add_scalars ( \"Accuracy\" , { 'First neuron' : act1 , 'Second neuron' : act2 }, trial ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required value dictionary of values. required step time index. None Source code in ANNarchy/extensions/tensorboard/Logger.py def add_scalars ( self , tag , value , step = None ): \"\"\" Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) act1 = pop.r[0] act2 = pop.r[1] logger.add_scalars( \"Accuracy\", {'First neuron': act1, 'Second neuron': act2}, trial) ``` :param tag: name of the figure in tensorboard. :param value: dictionary of values. :param step: time index. \"\"\" self . _summary . add_scalars ( main_tag = tag , tag_scalar_dict = value , global_step = step , walltime = None )","title":"add_scalars()"},{"location":"API/Logging/#ANNarchy.extensions.tensorboard.Logger.Logger.close","text":"Closes the logger. Source code in ANNarchy/extensions/tensorboard/Logger.py def close ( self ): \"Closes the logger.\" self . _summary . close ()","title":"close()"},{"location":"API/Logging/#ANNarchy.extensions.tensorboard.Logger.Logger.flush","text":"Forces the logged data to be flushed to disk. Source code in ANNarchy/extensions/tensorboard/Logger.py def flush ( self ): \"Forces the logged data to be flushed to disk.\" self . _summary . flush ()","title":"flush()"},{"location":"API/Monitor/","text":"Monitoring # Recording of neural or synaptic variables during the simulation is possible through a Monitor object. Monitor # Monitoring class allowing to record easily parameters or variables from Population, PopulationView and Dendrite objects. Example: m = Monitor ( pop , [ 'g_exc' , 'v' , 'spike' ], period = 10.0 ) It is also possible to record the sum of inputs to each neuron in a rate-coded population: m = Monitor ( pop , [ 'sum(exc)' , 'r' ]) period property writable # Period of recording in ms period_offset property writable # Shift of moment of time of recording in ms within a period variables property writable # Returns a copy of the current variable list. __init__ ( self , obj , variables = [], period = None , period_offset = None , start = True , net_id = 0 ) special # Parameters: Name Type Description Default obj object to monitor. Must be a Population, PopulationView, Dendrite or Projection object. required variables single variable name or list of variable names to record (default: []). [] period delay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View). None period_offset determine the moment in ms of recording within the period (default 0). Must be smaller than period . None start defines if the recording should start immediately (default: True). If not, you should later start the recordings with the start() method. True Source code in ANNarchy/core/Monitor.py def __init__ ( self , obj , variables = [], period = None , period_offset = None , start = True , net_id = 0 ): \"\"\" :param obj: object to monitor. Must be a Population, PopulationView, Dendrite or Projection object. :param variables: single variable name or list of variable names to record (default: []). :param period: delay in ms between two recording (default: dt). Not valid for the ``spike`` variable of a Population(View). :param period_offset: determine the moment in ms of recording within the period (default 0). Must be smaller than **period**. :param start: defines if the recording should start immediately (default: True). If not, you should later start the recordings with the ``start()`` method. \"\"\" # Object to record (Population, PopulationView, Dendrite) self . object = obj self . cyInstance = None self . net_id = net_id self . name = 'Monitor' # Check type of the object if not isinstance ( self . object , ( Population , PopulationView , Dendrite , Projection )): Global . _error ( 'Monitor: the object must be a Population, PopulationView, Dendrite or Projection object' ) # Variables to record if not isinstance ( variables , list ): self . _variables = [ variables ] else : self . _variables = variables # Check variables for var in self . _variables : if var in self . object . parameters : Global . _error ( 'Parameters are not recordable' ) if not var in self . object . variables and not var in [ 'spike' , 'axon_spike' ] and not var . startswith ( 'sum(' ): Global . _error ( 'Monitor: the object does not have an attribute named' , var ) # Period if not period : self . _period = Global . config [ 'dt' ] else : self . _period = float ( period ) # Period Offset if not period_offset : self . _period_offset = 0 else : # check validity if period_offset >= period : Global . _error ( \"Monitor(): value of period_offset must be smaller than period.\" ) else : self . _period_offset = period_offset # Warn users when recording projections if isinstance ( self . object , Projection ) and self . _period == Global . config [ 'dt' ]: Global . _warning ( 'Monitor(): it is a bad idea to record synaptic variables of a projection at each time step!' ) # Start self . _start = start self . _recorded_variables = {} # Add the monitor to the global variable self . id = len ( Global . _network [ self . net_id ][ 'monitors' ]) Global . _network [ self . net_id ][ 'monitors' ] . append ( self ) if Global . _network [ self . net_id ][ 'compiled' ]: # Already compiled self . _init_monitoring () get ( self , variables = None , keep = False , reshape = False , force_dict = False ) # Returns the recorded variables as a Numpy array (first dimension is time, second is neuron index). If a single variable name is provided, the recorded values for this variable are directly returned. If a list is provided or the argument left empty, a dictionary with all recorded variables is returned. The spike variable of a population will be returned as a dictionary of lists, where the spike times (in steps) for each recorded neurons are returned. Parameters: Name Type Description Default variables (list of) variables. By default, a dictionary with all variables is returned. None keep defines if the content in memory for each variable should be kept (default: False). False reshape transforms the second axis of the array to match the population's geometry (default: False). False Source code in ANNarchy/core/Monitor.py def get ( self , variables = None , keep = False , reshape = False , force_dict = False ): \"\"\" Returns the recorded variables as a Numpy array (first dimension is time, second is neuron index). If a single variable name is provided, the recorded values for this variable are directly returned. If a list is provided or the argument left empty, a dictionary with all recorded variables is returned. The ``spike`` variable of a population will be returned as a dictionary of lists, where the spike times (in steps) for each recorded neurons are returned. :param variables: (list of) variables. By default, a dictionary with all variables is returned. :param keep: defines if the content in memory for each variable should be kept (default: False). :param reshape: transforms the second axis of the array to match the population's geometry (default: False). \"\"\" def reshape_recording ( self , data ): if not reshape : return data else : return data . reshape (( data . shape [ 0 ],) + self . object . geometry ) def return_variable ( self , name , keep ): if isinstance ( self . object , ( Population , PopulationView )): return reshape_recording ( self , self . _get_population ( self . object , name , keep )) elif isinstance ( self . object , ( Dendrite , Projection )): data = self . _get_dendrite ( self . object , name , keep ) # Dendrites have one empty dimension if isinstance ( self . object , Dendrite ): data = data . squeeze () return data else : return None if variables : if not isinstance ( variables , list ): variables = [ variables ] else : variables = self . variables force_dict = True data = {} for var in variables : name = var # Sums of inputs for rate-coded populations if var . startswith ( 'sum(' ): target = re . findall ( r \"\\(([\\w]+)\\)\" , var )[ 0 ] name = '_sum_' + target # Retrieve the data data [ var ] = return_variable ( self , name , keep ) # Eventually reshape the array try : if not keep : if self . _recorded_variables [ var ][ 'stop' ][ - 1 ] != Global . get_current_step ( self . net_id ): self . _recorded_variables [ var ][ 'start' ][ - 1 ] = self . _recorded_variables [ var ][ 'stop' ][ - 1 ] self . _recorded_variables [ var ][ 'stop' ][ - 1 ] = Global . get_current_step ( self . net_id ) else : if self . _recorded_variables [ var ][ 'stop' ][ - 1 ] != Global . get_current_step ( self . net_id ): self . _recorded_variables [ var ][ 'stop' ][ - 1 ] = Global . get_current_step ( self . net_id ) except : Global . _warning ( 'Monitor.get(): you try to get recordings which do not exist:' , var ) if not force_dict and len ( variables ) == 1 : return data [ variables [ 0 ]] else : return data histogram ( self , spikes = None , bins = None ) # Returns a histogram for the recorded spikes in the population. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) histo = m . histogram () plt . plot ( histo ) or: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) histo = m . histogram ( spikes ) plt . plot ( histo ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None bins the bin size in ms (default: dt). None Source code in ANNarchy/core/Monitor.py def histogram ( self , spikes = None , bins = None ): \"\"\" Returns a histogram for the recorded spikes in the population. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) histo = m.histogram() plt.plot(histo) ``` or: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') histo = m.histogram(spikes) plt.plot(histo) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. If left empty, ``get('spike')`` will be called. Beware: this erases the data from memory. :param bins: the bin size in ms (default: dt). \"\"\" if not 'spike' in self . _variables : Global . _error ( 'Monitor: spike was not recorded' ) # Get data if not spikes : data = self . get ( 'spike' ) else : if 'spike' in spikes . keys (): data = spikes [ 'spike' ] else : data = spikes if not bins : bins = Global . config [ 'dt' ] # Compute the duration of the recordings t_start = self . _recorded_variables [ 'spike' ][ 'start' ][ - 1 ] duration = self . _recorded_variables [ 'spike' ][ 'stop' ][ - 1 ] - self . _recorded_variables [ 'spike' ][ 'start' ][ - 1 ] # Number of bins nb_bins = int ( duration * Global . config [ 'dt' ] / bins ) # Initialize histogram histo = [ 0 for t in range ( nb_bins )] # Compute histogram neurons = self . object . ranks if isinstance ( self . object , PopulationView ) else range ( self . object . size ) for neuron in neurons : for t in data [ neuron ]: histo [ int (( t - t_start ) / float ( bins / Global . config [ 'dt' ]))] += 1 return np . array ( histo ) mean_fr ( self , spikes = None ) # Computes the mean firing rate in the population during the recordings. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) fr = m . mean_fr () or: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) fr = m . mean_fr ( spikes ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None Source code in ANNarchy/core/Monitor.py def mean_fr ( self , spikes = None ): \"\"\" Computes the mean firing rate in the population during the recordings. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) fr = m.mean_fr() ``` or: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') fr = m.mean_fr(spikes) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. If left empty, ``get('spike')`` will be called. Beware: this erases the data from memory. \"\"\" if not 'spike' in self . _variables : Global . _error ( 'Monitor: spike was not recorded' ) # Get data if not spikes : data = self . get ( 'spike' ) else : if 'spike' in spikes . keys (): data = spikes [ 'spike' ] else : data = spikes # Compute the duration of the recordings duration = self . _recorded_variables [ 'spike' ][ 'stop' ][ - 1 ] - self . _recorded_variables [ 'spike' ][ 'start' ][ - 1 ] # Number of neurons neurons = self . object . ranks if isinstance ( self . object , PopulationView ) else range ( self . object . size ) # Compute fr fr = 0 for neuron in neurons : fr += len ( data [ neuron ]) return fr / float ( len ( neurons )) / duration / Global . dt () * 1000.0 pause ( self ) # Pauses the recordings. Source code in ANNarchy/core/Monitor.py def pause ( self ): \"Pauses the recordings.\" # Start recording the variables for var in self . variables : name = var # Sums of inputs for rate-coded populations if var . startswith ( 'sum(' ): target = re . findall ( r \"\\(([\\w]+)\\)\" , var )[ 0 ] name = '_sum_' + target try : setattr ( self . cyInstance , 'record_' + name , False ) except : obj_desc = '' if isinstance ( self . object , ( Population , PopulationView )): obj_desc = 'population ' + self . object . name elif isinstance ( self . object , Projection ): obj_desc = 'projection between ' + self . object . pre . name + ' and ' + self . object . post . name else : obj_desc = 'dendrite between ' + self . object . proj . pre . name + ' and ' + self . object . proj . post . name Global . _warning ( 'Monitor:' + var + ' can not be recorded (' + obj_desc + ')' ) self . _recorded_variables [ var ][ 'stop' ] . append ( Global . get_current_step ( self . net_id )) population_rate ( self , spikes = None , smooth = 0.0 ) # Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons. This method is faster than calling smoothed_rate and then averaging. The first axis is the neuron index, the second is time. If spikes is left empty, get('spike') will be called. Beware: this erases the data from memory. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) r = m . population_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . None smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0 Source code in ANNarchy/core/Monitor.py def population_rate ( self , spikes = None , smooth = 0. ): \"\"\" Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons. This method is faster than calling ``smoothed_rate`` and then averaging. The first axis is the neuron index, the second is time. If ``spikes`` is left empty, ``get('spike')`` will be called. Beware: this erases the data from memory. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) r = m.population_rate(smooth=100.) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. :param smooth: smoothing time constant. Default: 0.0 (no smoothing). \"\"\" if not 'spike' in self . _variables : Global . _error ( 'Monitor: spike was not recorded' ) # Get data if not spikes : data = self . get ( 'spike' ) else : if 'spike' in spikes . keys (): data = spikes [ 'spike' ] else : data = spikes import ANNarchy.core.cython_ext.Transformations as Transformations return Transformations . population_rate ( { 'data' : data , 'start' : self . _recorded_variables [ 'spike' ][ 'start' ][ - 1 ], 'stop' : self . _recorded_variables [ 'spike' ][ 'stop' ][ - 1 ] }, smooth ) raster_plot ( self , spikes = None ) # Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spike_times , spike_ranks = m . raster_plot () plt . plot ( spike_times , spike_ranks , '.' ) or: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) spike_times , spike_ranks = m . raster_plot ( spikes ) plt . plot ( spike_times , spike_ranks , '.' ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None Source code in ANNarchy/core/Monitor.py def raster_plot ( self , spikes = None ): \"\"\" Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spike_times, spike_ranks = m.raster_plot() plt.plot(spike_times, spike_ranks, '.') ``` or: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') spike_times, spike_ranks = m.raster_plot(spikes) plt.plot(spike_times, spike_ranks, '.') ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. If left empty, ``get('spike')`` will be called. Beware: this erases the data from memory. \"\"\" times = []; ranks = [] if not 'spike' in self . _variables : Global . _error ( 'Monitor: spike was not recorded' ) # Get data if not spikes : data = self . get ( 'spike' ) else : if 'spike' in spikes . keys (): data = spikes [ 'spike' ] elif 'axon_spike' in spikes . keys (): data = spikes [ 'axon_spike' ] else : data = spikes # Compute raster for n in data . keys (): for t in data [ n ]: times . append ( t ) ranks . append ( n ) return Global . dt () * np . array ( times ), np . array ( ranks ) resume ( self ) # Resumes the recordings. Source code in ANNarchy/core/Monitor.py def resume ( self ): \"Resumes the recordings.\" # Start recording the variables for var in self . variables : name = var # Sums of inputs for rate-coded populations if var . startswith ( 'sum(' ): target = re . findall ( r \"\\(([\\w]+)\\)\" , var )[ 0 ] name = '_sum_' + target try : setattr ( self . cyInstance , 'record_' + name , True ) except : obj_desc = '' if isinstance ( self . object , ( Population , PopulationView )): obj_desc = 'population ' + self . object . name elif isinstance ( self . object , Projection ): obj_desc = 'projection between ' + self . object . pre . name + ' and ' + self . object . post . name else : obj_desc = 'dendrite between ' + self . object . proj . pre . name + ' and ' + self . object . proj . post . name Global . _warning ( 'Monitor:' + var + ' can not be recorded (' + obj_desc + ')' ) self . _recorded_variables [ var ][ 'start' ] . append ( Global . get_current_step ( self . net_id )) size_in_bytes ( self ) # Get the size of allocated memory on C++ side. Please note, this is only valid if compile() was invoked. Returns: Type Description size in bytes of all allocated C++ data. Source code in ANNarchy/core/Monitor.py def size_in_bytes ( self ): \"\"\" Get the size of allocated memory on C++ side. Please note, this is only valid if compile() was invoked. :return: size in bytes of all allocated C++ data. \"\"\" if hasattr ( self . cyInstance , 'size_in_bytes' ): return self . cyInstance . size_in_bytes () smoothed_rate ( self , spikes = None , smooth = 0.0 ) # Computes the smoothed firing rate of the recorded spiking neurons. The first axis is the neuron index, the second is time. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) r = m . smoothed_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0 Source code in ANNarchy/core/Monitor.py def smoothed_rate ( self , spikes = None , smooth = 0. ): \"\"\" Computes the smoothed firing rate of the recorded spiking neurons. The first axis is the neuron index, the second is time. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) r = m.smoothed_rate(smooth=100.) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. If left empty, ``get('spike')`` will be called. Beware: this erases the data from memory. :param smooth: smoothing time constant. Default: 0.0 (no smoothing). \"\"\" if not 'spike' in self . _variables : Global . _error ( 'Monitor: spike was not recorded' ) # Get data if not spikes : data = self . get ( 'spike' ) else : if 'spike' in spikes . keys (): data = spikes [ 'spike' ] else : data = spikes import ANNarchy.core.cython_ext.Transformations as Transformations return Transformations . smoothed_rate ( { 'data' : data , 'start' : self . _recorded_variables [ 'spike' ][ 'start' ][ - 1 ], 'stop' : self . _recorded_variables [ 'spike' ][ 'stop' ][ - 1 ] }, smooth ) start ( self , variables = None , period = None ) # Starts recording the variables. It is called automatically after compile() if the flag start was not passed to the constructor. Parameters: Name Type Description Default variables single variable name or list of variable names to start recording (default: the variables argument passed to the constructor). None period delay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View). None Source code in ANNarchy/core/Monitor.py def start ( self , variables = None , period = None ): \"\"\"Starts recording the variables. It is called automatically after ``compile()`` if the flag ``start`` was not passed to the constructor. :param variables: single variable name or list of variable names to start recording (default: the ``variables`` argument passed to the constructor). :param period: delay in ms between two recording (default: dt). Not valid for the ``spike`` variable of a Population(View). \"\"\" if variables : if not isinstance ( variables , list ): self . _add_variable ( variables ) variables = [ variables ] else : for var in variables : self . _add_variable ( var ) else : variables = self . variables if period : self . _period = period self . cyInstance . period = int ( self . _period / Global . config [ 'dt' ]) self . cyInstance . offset = Global . get_current_step ( self . net_id ) for var in variables : name = var # Sums of inputs for rate-coded populations if var . startswith ( 'sum(' ): target = re . findall ( r \"\\(([\\w]+)\\)\" , var )[ 0 ] name = '_sum_' + target try : setattr ( self . cyInstance , 'record_' + name , True ) except : obj_desc = '' if isinstance ( self . object , ( Population , PopulationView )): obj_desc = 'population ' + self . object . name elif isinstance ( self . object , Projection ): obj_desc = 'projection between ' + self . object . pre . name + ' and ' + self . object . post . name else : obj_desc = 'dendrite between ' + self . object . proj . pre . name + ' and ' + self . object . proj . post . name if var in self . object . proj . parameters : Global . _print ( ' \\t ' , var , 'is a parameter, its value is constant' ) Global . _warning ( 'Monitor: ' + var + ' can not be recorded (' + obj_desc + ')' ) stop ( self ) # Stops the recording. Warning: This will delete the content of the C++ object and all not previously retrieved data is lost. Source code in ANNarchy/core/Monitor.py def stop ( self ): \"\"\" Stops the recording. Warning: This will delete the content of the C++ object and all not previously retrieved data is lost. \"\"\" try : self . _variables = [] self . _recorded_variables = {} self . cyInstance . clear () self . cyInstance = None except : obj_desc = '' if isinstance ( self . object , ( Population , PopulationView )): obj_desc = 'population ' + self . object . name elif isinstance ( self . object , Projection ): obj_desc = 'projection between ' + self . object . pre . name + ' and ' + self . object . post . name else : obj_desc = 'dendrite between ' + self . object . proj . pre . name + ' and ' + self . object . proj . post . name Global . _warning ( 'Monitor:' + var + ' can not be recorded (' + obj_desc + ')' ) times ( self , variables = None ) # Returns the start and stop times of the recorded variables. Parameters: Name Type Description Default variables (list of) variables. By default, the times for all variables is returned. None Source code in ANNarchy/core/Monitor.py def times ( self , variables = None ): \"\"\" Returns the start and stop times of the recorded variables. :param variables: (list of) variables. By default, the times for all variables is returned. \"\"\" t = {} if variables : if not isinstance ( variables , list ): variables = [ variables ] else : variables = self . _variables for var in variables : # check for spelling mistakes if not var in self . _variables : Global . _warning ( \"Variable '\" + str ( var ) + \"' is not monitored.\" ) continue t [ var ] = deepcopy ( self . _recorded_variables [ var ]) return t raster_plot ( spikes ) # Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) spike_times , spike_ranks = raster_plot ( spikes ) plt . plot ( spike_times , spike_ranks , '.' ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required Source code in ANNarchy/core/Monitor.py def raster_plot ( spikes ): \"\"\" Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') spike_times, spike_ranks = raster_plot(spikes) plt.plot(spike_times, spike_ranks, '.') ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. \"\"\" times = []; ranks = [] # Compute raster for n in spikes . keys (): for t in spikes [ n ]: times . append ( t ) ranks . append ( n ) return Global . dt () * np . array ( times ), np . array ( ranks ) histogram ( spikes , bins = None ) # Returns a histogram for the recorded spikes in the population. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) histo = histogram ( spikes ) plt . plot ( histo ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required bins the bin size in ms (default: dt). None Source code in ANNarchy/core/Monitor.py def histogram ( spikes , bins = None ): \"\"\" Returns a histogram for the recorded spikes in the population. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') histo = histogram(spikes) plt.plot(histo) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. :param bins: the bin size in ms (default: dt). \"\"\" if bins is None : bins = Global . config [ 'dt' ] bin_step = int ( bins / Global . config [ 'dt' ]) # Compute the duration of the recordings t_maxes = [] t_mines = [] for neuron in spikes . keys (): if len ( spikes [ neuron ]) == 0 : continue t_maxes . append ( np . max ( spikes [ neuron ])) t_mines . append ( np . min ( spikes [ neuron ])) t_max = np . max ( t_maxes ) t_min = np . min ( t_mines ) duration = t_max - t_min # Number of bins nb_bins = int ( duration / bin_step ) print ( t_min , t_max , duration , nb_bins ) # Initialize histogram histo = [ 0 for t in range ( nb_bins + 1 )] # Compute per step histogram for neuron in spikes . keys (): for t in spikes [ neuron ]: histo [ int (( t - t_min ) / float ( bin_step ))] += 1 return np . array ( histo ) mean_fr ( spikes , duration = None ) # Computes the mean firing rate in the population during the recordings. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) fr = mean_fr ( spikes ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required duration duration of the recordings. By default, the mean firing rate is computed between the first and last spikes of the recordings. None Source code in ANNarchy/core/Monitor.py def mean_fr ( spikes , duration = None ): \"\"\" Computes the mean firing rate in the population during the recordings. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') fr = mean_fr(spikes) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. :param duration: duration of the recordings. By default, the mean firing rate is computed between the first and last spikes of the recordings. \"\"\" if duration is None : # Compute the duration of the recordings t_maxes = [] t_mines = [] for neuron in spikes . keys (): if len ( spikes [ neuron ]) == 0 : continue t_maxes . append ( np . max ( spikes [ neuron ])) t_mines . append ( np . min ( spikes [ neuron ])) t_max = np . max ( t_maxes ) t_min = np . min ( t_mines ) duration = t_max - t_min nb_neurons = len ( spikes . keys ()) # Compute fr fr = 0 for neuron in spikes : fr += len ( spikes [ neuron ]) return fr / float ( nb_neurons ) / duration / Global . dt () * 1000.0 smoothed_rate ( spikes , smooth = 0.0 ) # Computes the smoothed firing rate of the recorded spiking neurons. The first axis is the neuron index, the second is time. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) r = smoothed_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. required smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0 Source code in ANNarchy/core/Monitor.py def smoothed_rate ( spikes , smooth = 0. ): \"\"\" Computes the smoothed firing rate of the recorded spiking neurons. The first axis is the neuron index, the second is time. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') r = smoothed_rate(smooth=100.) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. If left empty, ``get('spike')`` will be called. Beware: this erases the data from memory. :param smooth: smoothing time constant. Default: 0.0 (no smoothing). \"\"\" # Compute the duration of the recordings t_maxes = [] t_mines = [] for neuron in spikes . keys (): if len ( spikes [ neuron ]) == 0 : continue t_maxes . append ( np . max ( spikes [ neuron ])) t_mines . append ( np . min ( spikes [ neuron ])) t_max = np . max ( t_maxes ) t_min = np . min ( t_mines ) import ANNarchy.core.cython_ext.Transformations as Transformations return Transformations . smoothed_rate ( { 'data' : spikes , 'start' : t_min , 'stop' : t_max }, smooth ) population_rate ( spikes , smooth = 0.0 ) # Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons. This method is faster than calling smoothed_rate and then averaging. The first axis is the neuron index, the second is time. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) r = population_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0 Source code in ANNarchy/core/Monitor.py def population_rate ( spikes , smooth = 0.0 ): \"\"\" Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons. This method is faster than calling ``smoothed_rate`` and then averaging. The first axis is the neuron index, the second is time. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') r = population_rate(smooth=100.) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. :param smooth: smoothing time constant. Default: 0.0 (no smoothing). \"\"\" # Compute the duration of the recordings t_maxes = [] t_mines = [] for neuron in spikes . keys (): if len ( spikes [ neuron ]) == 0 : continue t_maxes . append ( np . max ( spikes [ neuron ])) t_mines . append ( np . min ( spikes [ neuron ])) t_max = np . max ( t_maxes ) t_min = np . min ( t_mines ) import ANNarchy.core.cython_ext.Transformations as Transformations return Transformations . population_rate ( { 'data' : spikes , 'start' : t_min , 'stop' : t_max }, smooth )","title":"Monitoring"},{"location":"API/Monitor/#monitoring","text":"Recording of neural or synaptic variables during the simulation is possible through a Monitor object.","title":"Monitoring"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor","text":"Monitoring class allowing to record easily parameters or variables from Population, PopulationView and Dendrite objects. Example: m = Monitor ( pop , [ 'g_exc' , 'v' , 'spike' ], period = 10.0 ) It is also possible to record the sum of inputs to each neuron in a rate-coded population: m = Monitor ( pop , [ 'sum(exc)' , 'r' ])","title":"Monitor"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.period","text":"Period of recording in ms","title":"period"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.period_offset","text":"Shift of moment of time of recording in ms within a period","title":"period_offset"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.variables","text":"Returns a copy of the current variable list.","title":"variables"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.__init__","text":"Parameters: Name Type Description Default obj object to monitor. Must be a Population, PopulationView, Dendrite or Projection object. required variables single variable name or list of variable names to record (default: []). [] period delay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View). None period_offset determine the moment in ms of recording within the period (default 0). Must be smaller than period . None start defines if the recording should start immediately (default: True). If not, you should later start the recordings with the start() method. True Source code in ANNarchy/core/Monitor.py def __init__ ( self , obj , variables = [], period = None , period_offset = None , start = True , net_id = 0 ): \"\"\" :param obj: object to monitor. Must be a Population, PopulationView, Dendrite or Projection object. :param variables: single variable name or list of variable names to record (default: []). :param period: delay in ms between two recording (default: dt). Not valid for the ``spike`` variable of a Population(View). :param period_offset: determine the moment in ms of recording within the period (default 0). Must be smaller than **period**. :param start: defines if the recording should start immediately (default: True). If not, you should later start the recordings with the ``start()`` method. \"\"\" # Object to record (Population, PopulationView, Dendrite) self . object = obj self . cyInstance = None self . net_id = net_id self . name = 'Monitor' # Check type of the object if not isinstance ( self . object , ( Population , PopulationView , Dendrite , Projection )): Global . _error ( 'Monitor: the object must be a Population, PopulationView, Dendrite or Projection object' ) # Variables to record if not isinstance ( variables , list ): self . _variables = [ variables ] else : self . _variables = variables # Check variables for var in self . _variables : if var in self . object . parameters : Global . _error ( 'Parameters are not recordable' ) if not var in self . object . variables and not var in [ 'spike' , 'axon_spike' ] and not var . startswith ( 'sum(' ): Global . _error ( 'Monitor: the object does not have an attribute named' , var ) # Period if not period : self . _period = Global . config [ 'dt' ] else : self . _period = float ( period ) # Period Offset if not period_offset : self . _period_offset = 0 else : # check validity if period_offset >= period : Global . _error ( \"Monitor(): value of period_offset must be smaller than period.\" ) else : self . _period_offset = period_offset # Warn users when recording projections if isinstance ( self . object , Projection ) and self . _period == Global . config [ 'dt' ]: Global . _warning ( 'Monitor(): it is a bad idea to record synaptic variables of a projection at each time step!' ) # Start self . _start = start self . _recorded_variables = {} # Add the monitor to the global variable self . id = len ( Global . _network [ self . net_id ][ 'monitors' ]) Global . _network [ self . net_id ][ 'monitors' ] . append ( self ) if Global . _network [ self . net_id ][ 'compiled' ]: # Already compiled self . _init_monitoring ()","title":"__init__()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.get","text":"Returns the recorded variables as a Numpy array (first dimension is time, second is neuron index). If a single variable name is provided, the recorded values for this variable are directly returned. If a list is provided or the argument left empty, a dictionary with all recorded variables is returned. The spike variable of a population will be returned as a dictionary of lists, where the spike times (in steps) for each recorded neurons are returned. Parameters: Name Type Description Default variables (list of) variables. By default, a dictionary with all variables is returned. None keep defines if the content in memory for each variable should be kept (default: False). False reshape transforms the second axis of the array to match the population's geometry (default: False). False Source code in ANNarchy/core/Monitor.py def get ( self , variables = None , keep = False , reshape = False , force_dict = False ): \"\"\" Returns the recorded variables as a Numpy array (first dimension is time, second is neuron index). If a single variable name is provided, the recorded values for this variable are directly returned. If a list is provided or the argument left empty, a dictionary with all recorded variables is returned. The ``spike`` variable of a population will be returned as a dictionary of lists, where the spike times (in steps) for each recorded neurons are returned. :param variables: (list of) variables. By default, a dictionary with all variables is returned. :param keep: defines if the content in memory for each variable should be kept (default: False). :param reshape: transforms the second axis of the array to match the population's geometry (default: False). \"\"\" def reshape_recording ( self , data ): if not reshape : return data else : return data . reshape (( data . shape [ 0 ],) + self . object . geometry ) def return_variable ( self , name , keep ): if isinstance ( self . object , ( Population , PopulationView )): return reshape_recording ( self , self . _get_population ( self . object , name , keep )) elif isinstance ( self . object , ( Dendrite , Projection )): data = self . _get_dendrite ( self . object , name , keep ) # Dendrites have one empty dimension if isinstance ( self . object , Dendrite ): data = data . squeeze () return data else : return None if variables : if not isinstance ( variables , list ): variables = [ variables ] else : variables = self . variables force_dict = True data = {} for var in variables : name = var # Sums of inputs for rate-coded populations if var . startswith ( 'sum(' ): target = re . findall ( r \"\\(([\\w]+)\\)\" , var )[ 0 ] name = '_sum_' + target # Retrieve the data data [ var ] = return_variable ( self , name , keep ) # Eventually reshape the array try : if not keep : if self . _recorded_variables [ var ][ 'stop' ][ - 1 ] != Global . get_current_step ( self . net_id ): self . _recorded_variables [ var ][ 'start' ][ - 1 ] = self . _recorded_variables [ var ][ 'stop' ][ - 1 ] self . _recorded_variables [ var ][ 'stop' ][ - 1 ] = Global . get_current_step ( self . net_id ) else : if self . _recorded_variables [ var ][ 'stop' ][ - 1 ] != Global . get_current_step ( self . net_id ): self . _recorded_variables [ var ][ 'stop' ][ - 1 ] = Global . get_current_step ( self . net_id ) except : Global . _warning ( 'Monitor.get(): you try to get recordings which do not exist:' , var ) if not force_dict and len ( variables ) == 1 : return data [ variables [ 0 ]] else : return data","title":"get()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.histogram","text":"Returns a histogram for the recorded spikes in the population. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) histo = m . histogram () plt . plot ( histo ) or: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) histo = m . histogram ( spikes ) plt . plot ( histo ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None bins the bin size in ms (default: dt). None Source code in ANNarchy/core/Monitor.py def histogram ( self , spikes = None , bins = None ): \"\"\" Returns a histogram for the recorded spikes in the population. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) histo = m.histogram() plt.plot(histo) ``` or: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') histo = m.histogram(spikes) plt.plot(histo) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. If left empty, ``get('spike')`` will be called. Beware: this erases the data from memory. :param bins: the bin size in ms (default: dt). \"\"\" if not 'spike' in self . _variables : Global . _error ( 'Monitor: spike was not recorded' ) # Get data if not spikes : data = self . get ( 'spike' ) else : if 'spike' in spikes . keys (): data = spikes [ 'spike' ] else : data = spikes if not bins : bins = Global . config [ 'dt' ] # Compute the duration of the recordings t_start = self . _recorded_variables [ 'spike' ][ 'start' ][ - 1 ] duration = self . _recorded_variables [ 'spike' ][ 'stop' ][ - 1 ] - self . _recorded_variables [ 'spike' ][ 'start' ][ - 1 ] # Number of bins nb_bins = int ( duration * Global . config [ 'dt' ] / bins ) # Initialize histogram histo = [ 0 for t in range ( nb_bins )] # Compute histogram neurons = self . object . ranks if isinstance ( self . object , PopulationView ) else range ( self . object . size ) for neuron in neurons : for t in data [ neuron ]: histo [ int (( t - t_start ) / float ( bins / Global . config [ 'dt' ]))] += 1 return np . array ( histo )","title":"histogram()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.mean_fr","text":"Computes the mean firing rate in the population during the recordings. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) fr = m . mean_fr () or: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) fr = m . mean_fr ( spikes ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None Source code in ANNarchy/core/Monitor.py def mean_fr ( self , spikes = None ): \"\"\" Computes the mean firing rate in the population during the recordings. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) fr = m.mean_fr() ``` or: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') fr = m.mean_fr(spikes) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. If left empty, ``get('spike')`` will be called. Beware: this erases the data from memory. \"\"\" if not 'spike' in self . _variables : Global . _error ( 'Monitor: spike was not recorded' ) # Get data if not spikes : data = self . get ( 'spike' ) else : if 'spike' in spikes . keys (): data = spikes [ 'spike' ] else : data = spikes # Compute the duration of the recordings duration = self . _recorded_variables [ 'spike' ][ 'stop' ][ - 1 ] - self . _recorded_variables [ 'spike' ][ 'start' ][ - 1 ] # Number of neurons neurons = self . object . ranks if isinstance ( self . object , PopulationView ) else range ( self . object . size ) # Compute fr fr = 0 for neuron in neurons : fr += len ( data [ neuron ]) return fr / float ( len ( neurons )) / duration / Global . dt () * 1000.0","title":"mean_fr()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.pause","text":"Pauses the recordings. Source code in ANNarchy/core/Monitor.py def pause ( self ): \"Pauses the recordings.\" # Start recording the variables for var in self . variables : name = var # Sums of inputs for rate-coded populations if var . startswith ( 'sum(' ): target = re . findall ( r \"\\(([\\w]+)\\)\" , var )[ 0 ] name = '_sum_' + target try : setattr ( self . cyInstance , 'record_' + name , False ) except : obj_desc = '' if isinstance ( self . object , ( Population , PopulationView )): obj_desc = 'population ' + self . object . name elif isinstance ( self . object , Projection ): obj_desc = 'projection between ' + self . object . pre . name + ' and ' + self . object . post . name else : obj_desc = 'dendrite between ' + self . object . proj . pre . name + ' and ' + self . object . proj . post . name Global . _warning ( 'Monitor:' + var + ' can not be recorded (' + obj_desc + ')' ) self . _recorded_variables [ var ][ 'stop' ] . append ( Global . get_current_step ( self . net_id ))","title":"pause()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.population_rate","text":"Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons. This method is faster than calling smoothed_rate and then averaging. The first axis is the neuron index, the second is time. If spikes is left empty, get('spike') will be called. Beware: this erases the data from memory. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) r = m . population_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . None smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0 Source code in ANNarchy/core/Monitor.py def population_rate ( self , spikes = None , smooth = 0. ): \"\"\" Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons. This method is faster than calling ``smoothed_rate`` and then averaging. The first axis is the neuron index, the second is time. If ``spikes`` is left empty, ``get('spike')`` will be called. Beware: this erases the data from memory. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) r = m.population_rate(smooth=100.) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. :param smooth: smoothing time constant. Default: 0.0 (no smoothing). \"\"\" if not 'spike' in self . _variables : Global . _error ( 'Monitor: spike was not recorded' ) # Get data if not spikes : data = self . get ( 'spike' ) else : if 'spike' in spikes . keys (): data = spikes [ 'spike' ] else : data = spikes import ANNarchy.core.cython_ext.Transformations as Transformations return Transformations . population_rate ( { 'data' : data , 'start' : self . _recorded_variables [ 'spike' ][ 'start' ][ - 1 ], 'stop' : self . _recorded_variables [ 'spike' ][ 'stop' ][ - 1 ] }, smooth )","title":"population_rate()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.raster_plot","text":"Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spike_times , spike_ranks = m . raster_plot () plt . plot ( spike_times , spike_ranks , '.' ) or: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) spike_times , spike_ranks = m . raster_plot ( spikes ) plt . plot ( spike_times , spike_ranks , '.' ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None Source code in ANNarchy/core/Monitor.py def raster_plot ( self , spikes = None ): \"\"\" Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spike_times, spike_ranks = m.raster_plot() plt.plot(spike_times, spike_ranks, '.') ``` or: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') spike_times, spike_ranks = m.raster_plot(spikes) plt.plot(spike_times, spike_ranks, '.') ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. If left empty, ``get('spike')`` will be called. Beware: this erases the data from memory. \"\"\" times = []; ranks = [] if not 'spike' in self . _variables : Global . _error ( 'Monitor: spike was not recorded' ) # Get data if not spikes : data = self . get ( 'spike' ) else : if 'spike' in spikes . keys (): data = spikes [ 'spike' ] elif 'axon_spike' in spikes . keys (): data = spikes [ 'axon_spike' ] else : data = spikes # Compute raster for n in data . keys (): for t in data [ n ]: times . append ( t ) ranks . append ( n ) return Global . dt () * np . array ( times ), np . array ( ranks )","title":"raster_plot()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.resume","text":"Resumes the recordings. Source code in ANNarchy/core/Monitor.py def resume ( self ): \"Resumes the recordings.\" # Start recording the variables for var in self . variables : name = var # Sums of inputs for rate-coded populations if var . startswith ( 'sum(' ): target = re . findall ( r \"\\(([\\w]+)\\)\" , var )[ 0 ] name = '_sum_' + target try : setattr ( self . cyInstance , 'record_' + name , True ) except : obj_desc = '' if isinstance ( self . object , ( Population , PopulationView )): obj_desc = 'population ' + self . object . name elif isinstance ( self . object , Projection ): obj_desc = 'projection between ' + self . object . pre . name + ' and ' + self . object . post . name else : obj_desc = 'dendrite between ' + self . object . proj . pre . name + ' and ' + self . object . proj . post . name Global . _warning ( 'Monitor:' + var + ' can not be recorded (' + obj_desc + ')' ) self . _recorded_variables [ var ][ 'start' ] . append ( Global . get_current_step ( self . net_id ))","title":"resume()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.size_in_bytes","text":"Get the size of allocated memory on C++ side. Please note, this is only valid if compile() was invoked. Returns: Type Description size in bytes of all allocated C++ data. Source code in ANNarchy/core/Monitor.py def size_in_bytes ( self ): \"\"\" Get the size of allocated memory on C++ side. Please note, this is only valid if compile() was invoked. :return: size in bytes of all allocated C++ data. \"\"\" if hasattr ( self . cyInstance , 'size_in_bytes' ): return self . cyInstance . size_in_bytes ()","title":"size_in_bytes()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.smoothed_rate","text":"Computes the smoothed firing rate of the recorded spiking neurons. The first axis is the neuron index, the second is time. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) r = m . smoothed_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0 Source code in ANNarchy/core/Monitor.py def smoothed_rate ( self , spikes = None , smooth = 0. ): \"\"\" Computes the smoothed firing rate of the recorded spiking neurons. The first axis is the neuron index, the second is time. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) r = m.smoothed_rate(smooth=100.) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. If left empty, ``get('spike')`` will be called. Beware: this erases the data from memory. :param smooth: smoothing time constant. Default: 0.0 (no smoothing). \"\"\" if not 'spike' in self . _variables : Global . _error ( 'Monitor: spike was not recorded' ) # Get data if not spikes : data = self . get ( 'spike' ) else : if 'spike' in spikes . keys (): data = spikes [ 'spike' ] else : data = spikes import ANNarchy.core.cython_ext.Transformations as Transformations return Transformations . smoothed_rate ( { 'data' : data , 'start' : self . _recorded_variables [ 'spike' ][ 'start' ][ - 1 ], 'stop' : self . _recorded_variables [ 'spike' ][ 'stop' ][ - 1 ] }, smooth )","title":"smoothed_rate()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.start","text":"Starts recording the variables. It is called automatically after compile() if the flag start was not passed to the constructor. Parameters: Name Type Description Default variables single variable name or list of variable names to start recording (default: the variables argument passed to the constructor). None period delay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View). None Source code in ANNarchy/core/Monitor.py def start ( self , variables = None , period = None ): \"\"\"Starts recording the variables. It is called automatically after ``compile()`` if the flag ``start`` was not passed to the constructor. :param variables: single variable name or list of variable names to start recording (default: the ``variables`` argument passed to the constructor). :param period: delay in ms between two recording (default: dt). Not valid for the ``spike`` variable of a Population(View). \"\"\" if variables : if not isinstance ( variables , list ): self . _add_variable ( variables ) variables = [ variables ] else : for var in variables : self . _add_variable ( var ) else : variables = self . variables if period : self . _period = period self . cyInstance . period = int ( self . _period / Global . config [ 'dt' ]) self . cyInstance . offset = Global . get_current_step ( self . net_id ) for var in variables : name = var # Sums of inputs for rate-coded populations if var . startswith ( 'sum(' ): target = re . findall ( r \"\\(([\\w]+)\\)\" , var )[ 0 ] name = '_sum_' + target try : setattr ( self . cyInstance , 'record_' + name , True ) except : obj_desc = '' if isinstance ( self . object , ( Population , PopulationView )): obj_desc = 'population ' + self . object . name elif isinstance ( self . object , Projection ): obj_desc = 'projection between ' + self . object . pre . name + ' and ' + self . object . post . name else : obj_desc = 'dendrite between ' + self . object . proj . pre . name + ' and ' + self . object . proj . post . name if var in self . object . proj . parameters : Global . _print ( ' \\t ' , var , 'is a parameter, its value is constant' ) Global . _warning ( 'Monitor: ' + var + ' can not be recorded (' + obj_desc + ')' )","title":"start()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.stop","text":"Stops the recording. Warning: This will delete the content of the C++ object and all not previously retrieved data is lost. Source code in ANNarchy/core/Monitor.py def stop ( self ): \"\"\" Stops the recording. Warning: This will delete the content of the C++ object and all not previously retrieved data is lost. \"\"\" try : self . _variables = [] self . _recorded_variables = {} self . cyInstance . clear () self . cyInstance = None except : obj_desc = '' if isinstance ( self . object , ( Population , PopulationView )): obj_desc = 'population ' + self . object . name elif isinstance ( self . object , Projection ): obj_desc = 'projection between ' + self . object . pre . name + ' and ' + self . object . post . name else : obj_desc = 'dendrite between ' + self . object . proj . pre . name + ' and ' + self . object . proj . post . name Global . _warning ( 'Monitor:' + var + ' can not be recorded (' + obj_desc + ')' )","title":"stop()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.Monitor.times","text":"Returns the start and stop times of the recorded variables. Parameters: Name Type Description Default variables (list of) variables. By default, the times for all variables is returned. None Source code in ANNarchy/core/Monitor.py def times ( self , variables = None ): \"\"\" Returns the start and stop times of the recorded variables. :param variables: (list of) variables. By default, the times for all variables is returned. \"\"\" t = {} if variables : if not isinstance ( variables , list ): variables = [ variables ] else : variables = self . _variables for var in variables : # check for spelling mistakes if not var in self . _variables : Global . _warning ( \"Variable '\" + str ( var ) + \"' is not monitored.\" ) continue t [ var ] = deepcopy ( self . _recorded_variables [ var ]) return t","title":"times()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.raster_plot","text":"Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) spike_times , spike_ranks = raster_plot ( spikes ) plt . plot ( spike_times , spike_ranks , '.' ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required Source code in ANNarchy/core/Monitor.py def raster_plot ( spikes ): \"\"\" Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') spike_times, spike_ranks = raster_plot(spikes) plt.plot(spike_times, spike_ranks, '.') ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. \"\"\" times = []; ranks = [] # Compute raster for n in spikes . keys (): for t in spikes [ n ]: times . append ( t ) ranks . append ( n ) return Global . dt () * np . array ( times ), np . array ( ranks )","title":"raster_plot()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.histogram","text":"Returns a histogram for the recorded spikes in the population. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) histo = histogram ( spikes ) plt . plot ( histo ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required bins the bin size in ms (default: dt). None Source code in ANNarchy/core/Monitor.py def histogram ( spikes , bins = None ): \"\"\" Returns a histogram for the recorded spikes in the population. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') histo = histogram(spikes) plt.plot(histo) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. :param bins: the bin size in ms (default: dt). \"\"\" if bins is None : bins = Global . config [ 'dt' ] bin_step = int ( bins / Global . config [ 'dt' ]) # Compute the duration of the recordings t_maxes = [] t_mines = [] for neuron in spikes . keys (): if len ( spikes [ neuron ]) == 0 : continue t_maxes . append ( np . max ( spikes [ neuron ])) t_mines . append ( np . min ( spikes [ neuron ])) t_max = np . max ( t_maxes ) t_min = np . min ( t_mines ) duration = t_max - t_min # Number of bins nb_bins = int ( duration / bin_step ) print ( t_min , t_max , duration , nb_bins ) # Initialize histogram histo = [ 0 for t in range ( nb_bins + 1 )] # Compute per step histogram for neuron in spikes . keys (): for t in spikes [ neuron ]: histo [ int (( t - t_min ) / float ( bin_step ))] += 1 return np . array ( histo )","title":"histogram()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.mean_fr","text":"Computes the mean firing rate in the population during the recordings. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) fr = mean_fr ( spikes ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required duration duration of the recordings. By default, the mean firing rate is computed between the first and last spikes of the recordings. None Source code in ANNarchy/core/Monitor.py def mean_fr ( spikes , duration = None ): \"\"\" Computes the mean firing rate in the population during the recordings. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') fr = mean_fr(spikes) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. :param duration: duration of the recordings. By default, the mean firing rate is computed between the first and last spikes of the recordings. \"\"\" if duration is None : # Compute the duration of the recordings t_maxes = [] t_mines = [] for neuron in spikes . keys (): if len ( spikes [ neuron ]) == 0 : continue t_maxes . append ( np . max ( spikes [ neuron ])) t_mines . append ( np . min ( spikes [ neuron ])) t_max = np . max ( t_maxes ) t_min = np . min ( t_mines ) duration = t_max - t_min nb_neurons = len ( spikes . keys ()) # Compute fr fr = 0 for neuron in spikes : fr += len ( spikes [ neuron ]) return fr / float ( nb_neurons ) / duration / Global . dt () * 1000.0","title":"mean_fr()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.smoothed_rate","text":"Computes the smoothed firing rate of the recorded spiking neurons. The first axis is the neuron index, the second is time. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) r = smoothed_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. required smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0 Source code in ANNarchy/core/Monitor.py def smoothed_rate ( spikes , smooth = 0. ): \"\"\" Computes the smoothed firing rate of the recorded spiking neurons. The first axis is the neuron index, the second is time. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') r = smoothed_rate(smooth=100.) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. If left empty, ``get('spike')`` will be called. Beware: this erases the data from memory. :param smooth: smoothing time constant. Default: 0.0 (no smoothing). \"\"\" # Compute the duration of the recordings t_maxes = [] t_mines = [] for neuron in spikes . keys (): if len ( spikes [ neuron ]) == 0 : continue t_maxes . append ( np . max ( spikes [ neuron ])) t_mines . append ( np . min ( spikes [ neuron ])) t_max = np . max ( t_maxes ) t_min = np . min ( t_mines ) import ANNarchy.core.cython_ext.Transformations as Transformations return Transformations . smoothed_rate ( { 'data' : spikes , 'start' : t_min , 'stop' : t_max }, smooth )","title":"smoothed_rate()"},{"location":"API/Monitor/#ANNarchy.core.Monitor.population_rate","text":"Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons. This method is faster than calling smoothed_rate and then averaging. The first axis is the neuron index, the second is time. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) r = population_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0 Source code in ANNarchy/core/Monitor.py def population_rate ( spikes , smooth = 0.0 ): \"\"\" Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons. This method is faster than calling ``smoothed_rate`` and then averaging. The first axis is the neuron index, the second is time. Example: ```python m = Monitor(P[:1000], 'spike') simulate(1000.0) spikes = m.get('spike') r = population_rate(smooth=100.) ``` :param spikes: the dictionary of spikes returned by ``get('spike')``. :param smooth: smoothing time constant. Default: 0.0 (no smoothing). \"\"\" # Compute the duration of the recordings t_maxes = [] t_mines = [] for neuron in spikes . keys (): if len ( spikes [ neuron ]) == 0 : continue t_maxes . append ( np . max ( spikes [ neuron ])) t_mines . append ( np . min ( spikes [ neuron ])) t_max = np . max ( t_maxes ) t_min = np . min ( t_mines ) import ANNarchy.core.cython_ext.Transformations as Transformations return Transformations . population_rate ( { 'data' : spikes , 'start' : t_min , 'stop' : t_max }, smooth )","title":"population_rate()"},{"location":"API/Network/","text":"Network class # A Network object holds copies of previously defined populations, projections or monitors in order to simulate them independently. The parallel_run() method can be used to simulate different networks in parallel. Network # A network gathers already defined populations, projections and monitors in order to run them independently. This is particularly useful when varying single parameters of a network and comparing the results (see the parallel_run() method). Only objects declared before the creation of the network can be used. Global methods such as simulate() must be used on the network object. The objects must be accessed through the get() method, as the original ones will not be part of the network (a copy is made). Each network must be individually compiled, but it does not matter if the original objects were already compiled. When passing everything=True to the constructor, all populations/projections/monitors already defined at the global level will be added to the network. If not, you can select which object will be added to network with the add() method. Example with everything=True : pop = Population ( 100 , Izhikevich ) proj = Projection ( pop , pop , 'exc' ) proj . connect_all_to_all ( 1.0 ) m = Monitor ( pop , 'spike' ) compile () # Optional net = Network ( everything = True ) net . get ( pop ) . a = 0.02 net . compile () net . simulate ( 1000. ) net2 = Network ( everything = True ) net2 . get ( pop ) . a = 0.05 net2 . compile () net2 . simulate ( 1000. ) t , n = net . get ( m ) . raster_plot () t2 , n2 = net2 . get ( m ) . raster_plot () Example with everything=False (the default): pop = Population ( 100 , Izhikevich ) proj1 = Projection ( pop , pop , 'exc' ) proj1 . connect_all_to_all ( 1.0 ) proj2 = Projection ( pop , pop , 'exc' ) proj2 . connect_all_to_all ( 2.0 ) m = Monitor ( pop , 'spike' ) net = Network () net . add ([ pop , proj1 , m ]) net . compile () net . simulate ( 1000. ) net2 = Network () net2 . add ([ pop , proj2 , m ]) net2 . compile () net2 . simulate ( 1000. ) t , n = net . get ( m ) . raster_plot () t2 , n2 = net2 . get ( m ) . raster_plot () __init__ ( self , everything = False ) special # Parameters: Name Type Description Default everything defines if all existing populations and projections should be automatically added (default: False). False Source code in ANNarchy/core/Network.py def __init__ ( self , everything = False ): \"\"\" :param everything: defines if all existing populations and projections should be automatically added (default: False). \"\"\" self . id = Global . _network . add_network ( self ) self . everything = everything Simulate . _callbacks . append ([]) Simulate . _callbacks_enabled . append ( True ) self . populations = [] self . projections = [] self . monitors = [] self . extensions = [] if everything : self . add ( Global . _network [ 0 ][ 'populations' ]) self . add ( Global . _network [ 0 ][ 'projections' ]) self . add ( Global . _network [ 0 ][ 'monitors' ]) self . add ( Global . _network [ 0 ][ 'extensions' ]) add ( self , objects ) # Adds a Population, Projection or Monitor to the network. Parameters: Name Type Description Default objects A single object or a list to add to the network. required Source code in ANNarchy/core/Network.py def add ( self , objects ): \"\"\" Adds a Population, Projection or Monitor to the network. :param objects: A single object or a list to add to the network. \"\"\" if isinstance ( objects , list ): for item in objects : self . _add_object ( item ) else : self . _add_object ( objects ) compile ( self , directory = 'annarchy' , clean = False , compiler = 'default' , compiler_flags = 'default' , add_sources = '' , extra_libs = '' , cuda_config = { 'device' : 0 }, annarchy_json = '' , silent = False , debug_build = False , profile_enabled = False ) # Compiles the network. Parameters: Name Type Description Default directory name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\". 'annarchy' clean boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False). False compiler C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++]. 'default' compiler_flags platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended. 'default' cuda_config dictionary defining the CUDA configuration for each population and projection. {'device': 0} annarchy_json compiler flags etc are stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location. '' silent defines if the \"Compiling... OK\" should be printed. False Source code in ANNarchy/core/Network.py def compile ( self , directory = 'annarchy' , clean = False , compiler = \"default\" , compiler_flags = \"default\" , add_sources = \"\" , extra_libs = \"\" , cuda_config = { 'device' : 0 }, annarchy_json = \"\" , silent = False , debug_build = False , profile_enabled = False ): \"\"\" Compiles the network. :param directory: name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\". :param clean: boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False). :param compiler: C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++]. :param compiler_flags: platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended. :param cuda_config: dictionary defining the CUDA configuration for each population and projection. :param annarchy_json: compiler flags etc are stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location. :param silent: defines if the \"Compiling... OK\" should be printed. \"\"\" Compiler . compile ( directory = directory , clean = clean , silent = silent , debug_build = debug_build , add_sources = add_sources , extra_libs = extra_libs , compiler = compiler , compiler_flags = compiler_flags , cuda_config = cuda_config , annarchy_json = annarchy_json , profile_enabled = profile_enabled , net_id = self . id ) disable_learning ( self , projections = None ) # Disables learning for all projections. Parameters: Name Type Description Default projections the projections whose learning should be disabled. By default, all the existing projections are disabled. None Source code in ANNarchy/core/Network.py def disable_learning ( self , projections = None ): \"\"\" Disables learning for all projections. :param projections: the projections whose learning should be disabled. By default, all the existing projections are disabled. \"\"\" if not projections : projections = self . projections for proj in projections : proj . disable_learning () enable_learning ( self , projections = None ) # Enables learning for all projections. Parameters: Name Type Description Default projections the projections whose learning should be enabled. By default, all the existing projections are disabled. None Source code in ANNarchy/core/Network.py def enable_learning ( self , projections = None ): \"\"\" Enables learning for all projections. :param projections: the projections whose learning should be enabled. By default, all the existing projections are disabled. \"\"\" if not projections : projections = self . projections for proj in projections : proj . enable_learning () get ( self , obj ) # Returns the local Population, Projection or Monitor identical to the provided argument. Example: pop = Population ( 100 , Izhikevich ) net = Network () net . add ( pop ) net . compile () net . simulate ( 100. ) print net . get ( pop ) . v Parameters: Name Type Description Default obj A single object or a list of objects. required Returns: Type Description The corresponding object or list of objects. Source code in ANNarchy/core/Network.py def get ( self , obj ): \"\"\" Returns the local Population, Projection or Monitor identical to the provided argument. Example: ```python pop = Population(100, Izhikevich) net = Network() net.add(pop) net.compile() net.simulate(100.) print net.get(pop).v ``` :param obj: A single object or a list of objects. :return: The corresponding object or list of objects. \"\"\" if isinstance ( obj , list ): return [ self . _get_object ( o ) for o in obj ] else : return self . _get_object ( obj ) get_current_step ( self ) # Returns the current simulation step. Source code in ANNarchy/core/Network.py def get_current_step ( self ): \"Returns the current simulation step.\" return Global . get_current_step ( self . id ) get_population ( self , name ) # Returns the population with the given name . Parameters: Name Type Description Default name name of the population required Returns: Type Description The requested Population object if existing, None otherwise. Source code in ANNarchy/core/Network.py def get_population ( self , name ): \"\"\" Returns the population with the given *name*. :param name: name of the population :return: The requested ``Population`` object if existing, ``None`` otherwise. \"\"\" for pop in self . populations : if pop . name == name : return pop Global . _print ( 'get_population(): the population' , name , 'does not exist in this network.' ) return None get_populations ( self ) # Returns a list of all declared populations in this network. Source code in ANNarchy/core/Network.py def get_populations ( self ): \"\"\" Returns a list of all declared populations in this network. \"\"\" if self . populations == []: Global . _warning ( \"Network.get_populations(): no populations attached to this network.\" ) return self . populations get_projection ( self , name ) # Returns the projection with the given name . Parameters: Name Type Description Default name name of the projection required Returns: Type Description The requested Projection object if existing, None otherwise. Source code in ANNarchy/core/Network.py def get_projection ( self , name ): \"\"\" Returns the projection with the given *name*. :param name: name of the projection :return: The requested ``Projection`` object if existing, ``None`` otherwise. \"\"\" for proj in self . projections : if proj . name == name : return proj Global . _print ( 'get_projection(): the projection' , name , 'does not exist in this network.' ) return None get_projections ( self , post = None , pre = None , target = None , suppress_error = False ) # Get a list of declared projections for the current network. By default, the method returns all connections within the network. By setting the arguments, post, pre and target one can select a subset. Parameters: Name Type Description Default post all returned projections should have this population as post. None pre all returned projections should have this population as pre. None target all returned projections should have this target. None suppress_error by default, ANNarchy throws an error if the list of assigned projections is empty. If this flag is set to True, the error message is suppressed. False Returns: Type Description A list of all assigned projections in this network or a subset according to the arguments. Source code in ANNarchy/core/Network.py def get_projections ( self , post = None , pre = None , target = None , suppress_error = False ): \"\"\" Get a list of declared projections for the current network. By default, the method returns all connections within the network. By setting the arguments, post, pre and target one can select a subset. :param post: all returned projections should have this population as post. :param pre: all returned projections should have this population as pre. :param target: all returned projections should have this target. :param suppress_error: by default, ANNarchy throws an error if the list of assigned projections is empty. If this flag is set to True, the error message is suppressed. :return: A list of all assigned projections in this network or a subset according to the arguments. \"\"\" if self . projections == []: if not suppress_error : Global . _error ( \"Network.get_projections(): no projections attached to this network.\" ) if post is None and pre is None and target is None : return self . projections else : res = [] if isinstance ( post , str ): post = self . get_population ( post ) if isinstance ( pre , str ): pre = self . get_population ( pre ) for proj in self . projections : if post is not None : # post is exclusionary if proj . post == post : res . append ( proj ) if pre is not None : raise NotImplementedError if target is not None : raise NotImplementedError return res get_time ( self ) # Returns the current time in ms. Source code in ANNarchy/core/Network.py def get_time ( self ): \"Returns the current time in ms.\" return Global . get_time ( self . id ) load ( self , filename , populations = True , projections = True ) # Loads a saved state of the current network by calling ANNarchy.core.IO.load(). Parameters: Name Type Description Default filename filename, may contain relative or absolute path. required populations if True, population data will be saved (by default True) True projections if True, projection data will be saved (by default True) True Source code in ANNarchy/core/Network.py def load ( self , filename , populations = True , projections = True ): \"\"\" Loads a saved state of the current network by calling ANNarchy.core.IO.load(). :param filename: filename, may contain relative or absolute path. :param populations: if True, population data will be saved (by default True) :param projections: if True, projection data will be saved (by default True) \"\"\" IO . load ( filename , populations , projections , self . id ) reset ( self , populations = True , projections = False , synapses = False ) # Reinitialises the network to its state before the call to compile. Parameters: Name Type Description Default populations if True (default), the neural parameters and variables will be reset to their initial value. True projections if True, the synaptic parameters and variables (except the connections) will be reset (default=False). False synapses if True, the synaptic weights will be erased and recreated (default=False). False Source code in ANNarchy/core/Network.py def reset ( self , populations = True , projections = False , synapses = False ): \"\"\" Reinitialises the network to its state before the call to compile. :param populations: if True (default), the neural parameters and variables will be reset to their initial value. :param projections: if True, the synaptic parameters and variables (except the connections) will be reset (default=False). :param synapses: if True, the synaptic weights will be erased and recreated (default=False). \"\"\" Global . reset ( populations , projections , synapses , self . id ) save ( self , filename , populations = True , projections = True ) # Saves the current network by calling ANNarchy.core.IO.save(). Parameters: Name Type Description Default filename filename, may contain relative or absolute path. required populations if True, population data will be saved (by default True) True projections if True, projection data will be saved (by default True) True Source code in ANNarchy/core/Network.py def save ( self , filename , populations = True , projections = True ): \"\"\" Saves the current network by calling ANNarchy.core.IO.save(). :param filename: filename, may contain relative or absolute path. :param populations: if True, population data will be saved (by default True) :param projections: if True, projection data will be saved (by default True) \"\"\" IO . save ( filename , populations , projections , self . id ) set_current_step ( self , t ) # Sets the current simulation step. Warning: can be dangerous for some spiking models. Source code in ANNarchy/core/Network.py def set_current_step ( self , t ): \"\"\" Sets the current simulation step. **Warning:** can be dangerous for some spiking models. \"\"\" Global . set_current_step ( t , self . id ) set_seed ( self , seed , use_seed_seq = True ) # Sets the seed of the random number generators for this network. Source code in ANNarchy/core/Network.py def set_seed ( self , seed , use_seed_seq = True ): \"\"\" Sets the seed of the random number generators for this network. \"\"\" Global . set_seed ( seed , use_seed_seq , self . id ) set_time ( self , t , net_id = 0 ) # Sets the current time in ms. Warning: can be dangerous for some spiking models. Source code in ANNarchy/core/Network.py def set_time ( self , t , net_id = 0 ): \"\"\" Sets the current time in ms. **Warning:** can be dangerous for some spiking models. \"\"\" Global . set_time ( t , self . id ) simulate ( self , duration , measure_time = False ) # Runs the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step dt declared in setup() (default: 1ms): simulate ( 1000.0 ) Parameters: Name Type Description Default duration the duration in milliseconds. required measure_time defines whether the simulation time should be printed (default=False). False Source code in ANNarchy/core/Network.py def simulate ( self , duration , measure_time = False ): \"\"\" Runs the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step ``dt`` declared in ``setup()`` (default: 1ms): ```python simulate(1000.0) ``` :param duration: the duration in milliseconds. :param measure_time: defines whether the simulation time should be printed (default=False). \"\"\" Simulate . simulate ( duration , measure_time , net_id = self . id ) simulate_until ( self , max_duration , population , operator = 'and' , measure_time = False ) # Runs the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: pop1 = Population ( ... , stop_condition = \"r > 1.0 : any\" ) compile () simulate_until ( max_duration = 1000.0 . population = pop1 ) Parameters: Name Type Description Default max_duration the maximum duration of the simulation in milliseconds. required population the (list of) population whose stop_condition should be checked to stop the simulation. required operator operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). 'and' measure_time defines whether the simulation time should be printed (default=False). False Returns: Type Description the actual duration of the simulation in milliseconds. Source code in ANNarchy/core/Network.py def simulate_until ( self , max_duration , population , operator = 'and' , measure_time = False ): \"\"\" Runs the network for the maximal duration in milliseconds. If the ``stop_condition`` defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: ```python pop1 = Population( ..., stop_condition = \"r > 1.0 : any\") compile() simulate_until(max_duration=1000.0. population=pop1) ``` :param max_duration: the maximum duration of the simulation in milliseconds. :param population: the (list of) population whose ``stop_condition`` should be checked to stop the simulation. :param operator: operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). :param measure_time: defines whether the simulation time should be printed (default=False). :return: the actual duration of the simulation in milliseconds. \"\"\" return Simulate . simulate_until ( max_duration , population , operator , measure_time , net_id = self . id ) step ( self ) # Performs a single simulation step (duration = dt ). Source code in ANNarchy/core/Network.py def step ( self ): \"\"\" Performs a single simulation step (duration = ``dt``). \"\"\" Simulate . step ( self . id ) parallel_run ( method , networks = None , number = 0 , max_processes =- 1 , measure_time = False , sequential = False , same_seed = False , ** args ) # Allows to run multiple networks in parallel using multiprocessing. If the networks argument is provided as a list of Network objects, the given method will be executed for each of these networks. If number is given instead, the same number of networks will be created and the method is applied. If number is used, the created networks are not returned, you should return what you need to analyse. Example: pop1 = PoissonPopulation ( 100 , rates = 10.0 ) pop2 = Population ( 100 , Izhikevich ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_probability ( weights = 5.0 , probability = 0.2 ) m = Monitor ( pop2 , 'spike' ) compile () def simulation ( idx , net ): net . get ( pop1 ) . rates = 10. * idx net . simulate ( 1000. ) return net . get ( m ) . raster_plot () results = parallel_run ( method = simulation , number = 3 ) t1 , n1 = results [ 0 ] t2 , n2 = results [ 1 ] t3 , n3 = results [ 2 ] Parameters: Name Type Description Default method a Python method which will be executed for each network. This function must accept an integer as first argument (id of the simulation) and a Network object as second argument. required networks a list of networks to simulate in parallel. None number the number of odentical networks to run in parallel. 0 max_processes maximal number of processes to start concurrently (default: the available number of cores on the machine). -1 measure_time if the total simulation time should be printed out. False sequential if True, runs the simulations sequentially instead of in parallel (default: False). False same_seed if True, all networks will use the same seed. If not, the seed will be randomly initialized with time(0) for each network (default). It has no influence when the networks argument is set (the seed has to be set individually for each network using net.set_seed() ), only when number is used. False args other named arguments you want to pass to the simulation method. {} Returns: Type Description a list of the values returned by method . Source code in ANNarchy/core/Network.py def parallel_run ( method , networks = None , number = 0 , max_processes =- 1 , measure_time = False , sequential = False , same_seed = False , ** args ): \"\"\" Allows to run multiple networks in parallel using multiprocessing. If the ``networks`` argument is provided as a list of Network objects, the given method will be executed for each of these networks. If ``number`` is given instead, the same number of networks will be created and the method is applied. If ``number`` is used, the created networks are not returned, you should return what you need to analyse. Example: ```python pop1 = PoissonPopulation(100, rates=10.0) pop2 = Population(100, Izhikevich) proj = Projection(pop1, pop2, 'exc') proj.connect_fixed_probability(weights=5.0, probability=0.2) m = Monitor(pop2, 'spike') compile() def simulation(idx, net): net.get(pop1).rates = 10. * idx net.simulate(1000.) return net.get(m).raster_plot() results = parallel_run(method=simulation, number = 3) t1, n1 = results[0] t2, n2 = results[1] t3, n3 = results[2] ``` :param method: a Python method which will be executed for each network. This function must accept an integer as first argument (id of the simulation) and a Network object as second argument. :param networks: a list of networks to simulate in parallel. :param number: the number of odentical networks to run in parallel. :param max_processes: maximal number of processes to start concurrently (default: the available number of cores on the machine). :param measure_time: if the total simulation time should be printed out. :param sequential: if True, runs the simulations sequentially instead of in parallel (default: False). :param same_seed: if True, all networks will use the same seed. If not, the seed will be randomly initialized with time(0) for each network (default). It has no influence when the ``networks`` argument is set (the seed has to be set individually for each network using ``net.set_seed()``), only when ``number`` is used. :param args: other named arguments you want to pass to the simulation method. :return: a list of the values returned by ``method``. \"\"\" # Check inputs if not networks and number < 1 : Global . _error ( 'parallel_run(): the networks or number arguments must be set.' , exit = True ) import types if not isinstance ( method , types . FunctionType ): Global . _error ( 'parallel_run(): the method argument must be a method.' , exit = True ) if not networks : # The magic network will run N times return _parallel_multi ( method , number , max_processes , measure_time , sequential , same_seed , args ) if not isinstance ( networks , list ): Global . _error ( 'parallel_run(): the networks argument must be a list.' , exit = True ) # Simulate the different networks return _parallel_networks ( method , networks , max_processes , measure_time , sequential , args )","title":"Network class"},{"location":"API/Network/#network-class","text":"A Network object holds copies of previously defined populations, projections or monitors in order to simulate them independently. The parallel_run() method can be used to simulate different networks in parallel.","title":"Network class"},{"location":"API/Network/#ANNarchy.core.Network.Network","text":"A network gathers already defined populations, projections and monitors in order to run them independently. This is particularly useful when varying single parameters of a network and comparing the results (see the parallel_run() method). Only objects declared before the creation of the network can be used. Global methods such as simulate() must be used on the network object. The objects must be accessed through the get() method, as the original ones will not be part of the network (a copy is made). Each network must be individually compiled, but it does not matter if the original objects were already compiled. When passing everything=True to the constructor, all populations/projections/monitors already defined at the global level will be added to the network. If not, you can select which object will be added to network with the add() method. Example with everything=True : pop = Population ( 100 , Izhikevich ) proj = Projection ( pop , pop , 'exc' ) proj . connect_all_to_all ( 1.0 ) m = Monitor ( pop , 'spike' ) compile () # Optional net = Network ( everything = True ) net . get ( pop ) . a = 0.02 net . compile () net . simulate ( 1000. ) net2 = Network ( everything = True ) net2 . get ( pop ) . a = 0.05 net2 . compile () net2 . simulate ( 1000. ) t , n = net . get ( m ) . raster_plot () t2 , n2 = net2 . get ( m ) . raster_plot () Example with everything=False (the default): pop = Population ( 100 , Izhikevich ) proj1 = Projection ( pop , pop , 'exc' ) proj1 . connect_all_to_all ( 1.0 ) proj2 = Projection ( pop , pop , 'exc' ) proj2 . connect_all_to_all ( 2.0 ) m = Monitor ( pop , 'spike' ) net = Network () net . add ([ pop , proj1 , m ]) net . compile () net . simulate ( 1000. ) net2 = Network () net2 . add ([ pop , proj2 , m ]) net2 . compile () net2 . simulate ( 1000. ) t , n = net . get ( m ) . raster_plot () t2 , n2 = net2 . get ( m ) . raster_plot ()","title":"Network"},{"location":"API/Network/#ANNarchy.core.Network.Network.__init__","text":"Parameters: Name Type Description Default everything defines if all existing populations and projections should be automatically added (default: False). False Source code in ANNarchy/core/Network.py def __init__ ( self , everything = False ): \"\"\" :param everything: defines if all existing populations and projections should be automatically added (default: False). \"\"\" self . id = Global . _network . add_network ( self ) self . everything = everything Simulate . _callbacks . append ([]) Simulate . _callbacks_enabled . append ( True ) self . populations = [] self . projections = [] self . monitors = [] self . extensions = [] if everything : self . add ( Global . _network [ 0 ][ 'populations' ]) self . add ( Global . _network [ 0 ][ 'projections' ]) self . add ( Global . _network [ 0 ][ 'monitors' ]) self . add ( Global . _network [ 0 ][ 'extensions' ])","title":"__init__()"},{"location":"API/Network/#ANNarchy.core.Network.Network.add","text":"Adds a Population, Projection or Monitor to the network. Parameters: Name Type Description Default objects A single object or a list to add to the network. required Source code in ANNarchy/core/Network.py def add ( self , objects ): \"\"\" Adds a Population, Projection or Monitor to the network. :param objects: A single object or a list to add to the network. \"\"\" if isinstance ( objects , list ): for item in objects : self . _add_object ( item ) else : self . _add_object ( objects )","title":"add()"},{"location":"API/Network/#ANNarchy.core.Network.Network.compile","text":"Compiles the network. Parameters: Name Type Description Default directory name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\". 'annarchy' clean boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False). False compiler C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++]. 'default' compiler_flags platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended. 'default' cuda_config dictionary defining the CUDA configuration for each population and projection. {'device': 0} annarchy_json compiler flags etc are stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location. '' silent defines if the \"Compiling... OK\" should be printed. False Source code in ANNarchy/core/Network.py def compile ( self , directory = 'annarchy' , clean = False , compiler = \"default\" , compiler_flags = \"default\" , add_sources = \"\" , extra_libs = \"\" , cuda_config = { 'device' : 0 }, annarchy_json = \"\" , silent = False , debug_build = False , profile_enabled = False ): \"\"\" Compiles the network. :param directory: name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\". :param clean: boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False). :param compiler: C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++]. :param compiler_flags: platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended. :param cuda_config: dictionary defining the CUDA configuration for each population and projection. :param annarchy_json: compiler flags etc are stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location. :param silent: defines if the \"Compiling... OK\" should be printed. \"\"\" Compiler . compile ( directory = directory , clean = clean , silent = silent , debug_build = debug_build , add_sources = add_sources , extra_libs = extra_libs , compiler = compiler , compiler_flags = compiler_flags , cuda_config = cuda_config , annarchy_json = annarchy_json , profile_enabled = profile_enabled , net_id = self . id )","title":"compile()"},{"location":"API/Network/#ANNarchy.core.Network.Network.disable_learning","text":"Disables learning for all projections. Parameters: Name Type Description Default projections the projections whose learning should be disabled. By default, all the existing projections are disabled. None Source code in ANNarchy/core/Network.py def disable_learning ( self , projections = None ): \"\"\" Disables learning for all projections. :param projections: the projections whose learning should be disabled. By default, all the existing projections are disabled. \"\"\" if not projections : projections = self . projections for proj in projections : proj . disable_learning ()","title":"disable_learning()"},{"location":"API/Network/#ANNarchy.core.Network.Network.enable_learning","text":"Enables learning for all projections. Parameters: Name Type Description Default projections the projections whose learning should be enabled. By default, all the existing projections are disabled. None Source code in ANNarchy/core/Network.py def enable_learning ( self , projections = None ): \"\"\" Enables learning for all projections. :param projections: the projections whose learning should be enabled. By default, all the existing projections are disabled. \"\"\" if not projections : projections = self . projections for proj in projections : proj . enable_learning ()","title":"enable_learning()"},{"location":"API/Network/#ANNarchy.core.Network.Network.get","text":"Returns the local Population, Projection or Monitor identical to the provided argument. Example: pop = Population ( 100 , Izhikevich ) net = Network () net . add ( pop ) net . compile () net . simulate ( 100. ) print net . get ( pop ) . v Parameters: Name Type Description Default obj A single object or a list of objects. required Returns: Type Description The corresponding object or list of objects. Source code in ANNarchy/core/Network.py def get ( self , obj ): \"\"\" Returns the local Population, Projection or Monitor identical to the provided argument. Example: ```python pop = Population(100, Izhikevich) net = Network() net.add(pop) net.compile() net.simulate(100.) print net.get(pop).v ``` :param obj: A single object or a list of objects. :return: The corresponding object or list of objects. \"\"\" if isinstance ( obj , list ): return [ self . _get_object ( o ) for o in obj ] else : return self . _get_object ( obj )","title":"get()"},{"location":"API/Network/#ANNarchy.core.Network.Network.get_current_step","text":"Returns the current simulation step. Source code in ANNarchy/core/Network.py def get_current_step ( self ): \"Returns the current simulation step.\" return Global . get_current_step ( self . id )","title":"get_current_step()"},{"location":"API/Network/#ANNarchy.core.Network.Network.get_population","text":"Returns the population with the given name . Parameters: Name Type Description Default name name of the population required Returns: Type Description The requested Population object if existing, None otherwise. Source code in ANNarchy/core/Network.py def get_population ( self , name ): \"\"\" Returns the population with the given *name*. :param name: name of the population :return: The requested ``Population`` object if existing, ``None`` otherwise. \"\"\" for pop in self . populations : if pop . name == name : return pop Global . _print ( 'get_population(): the population' , name , 'does not exist in this network.' ) return None","title":"get_population()"},{"location":"API/Network/#ANNarchy.core.Network.Network.get_populations","text":"Returns a list of all declared populations in this network. Source code in ANNarchy/core/Network.py def get_populations ( self ): \"\"\" Returns a list of all declared populations in this network. \"\"\" if self . populations == []: Global . _warning ( \"Network.get_populations(): no populations attached to this network.\" ) return self . populations","title":"get_populations()"},{"location":"API/Network/#ANNarchy.core.Network.Network.get_projection","text":"Returns the projection with the given name . Parameters: Name Type Description Default name name of the projection required Returns: Type Description The requested Projection object if existing, None otherwise. Source code in ANNarchy/core/Network.py def get_projection ( self , name ): \"\"\" Returns the projection with the given *name*. :param name: name of the projection :return: The requested ``Projection`` object if existing, ``None`` otherwise. \"\"\" for proj in self . projections : if proj . name == name : return proj Global . _print ( 'get_projection(): the projection' , name , 'does not exist in this network.' ) return None","title":"get_projection()"},{"location":"API/Network/#ANNarchy.core.Network.Network.get_projections","text":"Get a list of declared projections for the current network. By default, the method returns all connections within the network. By setting the arguments, post, pre and target one can select a subset. Parameters: Name Type Description Default post all returned projections should have this population as post. None pre all returned projections should have this population as pre. None target all returned projections should have this target. None suppress_error by default, ANNarchy throws an error if the list of assigned projections is empty. If this flag is set to True, the error message is suppressed. False Returns: Type Description A list of all assigned projections in this network or a subset according to the arguments. Source code in ANNarchy/core/Network.py def get_projections ( self , post = None , pre = None , target = None , suppress_error = False ): \"\"\" Get a list of declared projections for the current network. By default, the method returns all connections within the network. By setting the arguments, post, pre and target one can select a subset. :param post: all returned projections should have this population as post. :param pre: all returned projections should have this population as pre. :param target: all returned projections should have this target. :param suppress_error: by default, ANNarchy throws an error if the list of assigned projections is empty. If this flag is set to True, the error message is suppressed. :return: A list of all assigned projections in this network or a subset according to the arguments. \"\"\" if self . projections == []: if not suppress_error : Global . _error ( \"Network.get_projections(): no projections attached to this network.\" ) if post is None and pre is None and target is None : return self . projections else : res = [] if isinstance ( post , str ): post = self . get_population ( post ) if isinstance ( pre , str ): pre = self . get_population ( pre ) for proj in self . projections : if post is not None : # post is exclusionary if proj . post == post : res . append ( proj ) if pre is not None : raise NotImplementedError if target is not None : raise NotImplementedError return res","title":"get_projections()"},{"location":"API/Network/#ANNarchy.core.Network.Network.get_time","text":"Returns the current time in ms. Source code in ANNarchy/core/Network.py def get_time ( self ): \"Returns the current time in ms.\" return Global . get_time ( self . id )","title":"get_time()"},{"location":"API/Network/#ANNarchy.core.Network.Network.load","text":"Loads a saved state of the current network by calling ANNarchy.core.IO.load(). Parameters: Name Type Description Default filename filename, may contain relative or absolute path. required populations if True, population data will be saved (by default True) True projections if True, projection data will be saved (by default True) True Source code in ANNarchy/core/Network.py def load ( self , filename , populations = True , projections = True ): \"\"\" Loads a saved state of the current network by calling ANNarchy.core.IO.load(). :param filename: filename, may contain relative or absolute path. :param populations: if True, population data will be saved (by default True) :param projections: if True, projection data will be saved (by default True) \"\"\" IO . load ( filename , populations , projections , self . id )","title":"load()"},{"location":"API/Network/#ANNarchy.core.Network.Network.reset","text":"Reinitialises the network to its state before the call to compile. Parameters: Name Type Description Default populations if True (default), the neural parameters and variables will be reset to their initial value. True projections if True, the synaptic parameters and variables (except the connections) will be reset (default=False). False synapses if True, the synaptic weights will be erased and recreated (default=False). False Source code in ANNarchy/core/Network.py def reset ( self , populations = True , projections = False , synapses = False ): \"\"\" Reinitialises the network to its state before the call to compile. :param populations: if True (default), the neural parameters and variables will be reset to their initial value. :param projections: if True, the synaptic parameters and variables (except the connections) will be reset (default=False). :param synapses: if True, the synaptic weights will be erased and recreated (default=False). \"\"\" Global . reset ( populations , projections , synapses , self . id )","title":"reset()"},{"location":"API/Network/#ANNarchy.core.Network.Network.save","text":"Saves the current network by calling ANNarchy.core.IO.save(). Parameters: Name Type Description Default filename filename, may contain relative or absolute path. required populations if True, population data will be saved (by default True) True projections if True, projection data will be saved (by default True) True Source code in ANNarchy/core/Network.py def save ( self , filename , populations = True , projections = True ): \"\"\" Saves the current network by calling ANNarchy.core.IO.save(). :param filename: filename, may contain relative or absolute path. :param populations: if True, population data will be saved (by default True) :param projections: if True, projection data will be saved (by default True) \"\"\" IO . save ( filename , populations , projections , self . id )","title":"save()"},{"location":"API/Network/#ANNarchy.core.Network.Network.set_current_step","text":"Sets the current simulation step. Warning: can be dangerous for some spiking models. Source code in ANNarchy/core/Network.py def set_current_step ( self , t ): \"\"\" Sets the current simulation step. **Warning:** can be dangerous for some spiking models. \"\"\" Global . set_current_step ( t , self . id )","title":"set_current_step()"},{"location":"API/Network/#ANNarchy.core.Network.Network.set_seed","text":"Sets the seed of the random number generators for this network. Source code in ANNarchy/core/Network.py def set_seed ( self , seed , use_seed_seq = True ): \"\"\" Sets the seed of the random number generators for this network. \"\"\" Global . set_seed ( seed , use_seed_seq , self . id )","title":"set_seed()"},{"location":"API/Network/#ANNarchy.core.Network.Network.set_time","text":"Sets the current time in ms. Warning: can be dangerous for some spiking models. Source code in ANNarchy/core/Network.py def set_time ( self , t , net_id = 0 ): \"\"\" Sets the current time in ms. **Warning:** can be dangerous for some spiking models. \"\"\" Global . set_time ( t , self . id )","title":"set_time()"},{"location":"API/Network/#ANNarchy.core.Network.Network.simulate","text":"Runs the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step dt declared in setup() (default: 1ms): simulate ( 1000.0 ) Parameters: Name Type Description Default duration the duration in milliseconds. required measure_time defines whether the simulation time should be printed (default=False). False Source code in ANNarchy/core/Network.py def simulate ( self , duration , measure_time = False ): \"\"\" Runs the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step ``dt`` declared in ``setup()`` (default: 1ms): ```python simulate(1000.0) ``` :param duration: the duration in milliseconds. :param measure_time: defines whether the simulation time should be printed (default=False). \"\"\" Simulate . simulate ( duration , measure_time , net_id = self . id )","title":"simulate()"},{"location":"API/Network/#ANNarchy.core.Network.Network.simulate_until","text":"Runs the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: pop1 = Population ( ... , stop_condition = \"r > 1.0 : any\" ) compile () simulate_until ( max_duration = 1000.0 . population = pop1 ) Parameters: Name Type Description Default max_duration the maximum duration of the simulation in milliseconds. required population the (list of) population whose stop_condition should be checked to stop the simulation. required operator operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). 'and' measure_time defines whether the simulation time should be printed (default=False). False Returns: Type Description the actual duration of the simulation in milliseconds. Source code in ANNarchy/core/Network.py def simulate_until ( self , max_duration , population , operator = 'and' , measure_time = False ): \"\"\" Runs the network for the maximal duration in milliseconds. If the ``stop_condition`` defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: ```python pop1 = Population( ..., stop_condition = \"r > 1.0 : any\") compile() simulate_until(max_duration=1000.0. population=pop1) ``` :param max_duration: the maximum duration of the simulation in milliseconds. :param population: the (list of) population whose ``stop_condition`` should be checked to stop the simulation. :param operator: operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). :param measure_time: defines whether the simulation time should be printed (default=False). :return: the actual duration of the simulation in milliseconds. \"\"\" return Simulate . simulate_until ( max_duration , population , operator , measure_time , net_id = self . id )","title":"simulate_until()"},{"location":"API/Network/#ANNarchy.core.Network.Network.step","text":"Performs a single simulation step (duration = dt ). Source code in ANNarchy/core/Network.py def step ( self ): \"\"\" Performs a single simulation step (duration = ``dt``). \"\"\" Simulate . step ( self . id )","title":"step()"},{"location":"API/Network/#ANNarchy.core.Network.parallel_run","text":"Allows to run multiple networks in parallel using multiprocessing. If the networks argument is provided as a list of Network objects, the given method will be executed for each of these networks. If number is given instead, the same number of networks will be created and the method is applied. If number is used, the created networks are not returned, you should return what you need to analyse. Example: pop1 = PoissonPopulation ( 100 , rates = 10.0 ) pop2 = Population ( 100 , Izhikevich ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_probability ( weights = 5.0 , probability = 0.2 ) m = Monitor ( pop2 , 'spike' ) compile () def simulation ( idx , net ): net . get ( pop1 ) . rates = 10. * idx net . simulate ( 1000. ) return net . get ( m ) . raster_plot () results = parallel_run ( method = simulation , number = 3 ) t1 , n1 = results [ 0 ] t2 , n2 = results [ 1 ] t3 , n3 = results [ 2 ] Parameters: Name Type Description Default method a Python method which will be executed for each network. This function must accept an integer as first argument (id of the simulation) and a Network object as second argument. required networks a list of networks to simulate in parallel. None number the number of odentical networks to run in parallel. 0 max_processes maximal number of processes to start concurrently (default: the available number of cores on the machine). -1 measure_time if the total simulation time should be printed out. False sequential if True, runs the simulations sequentially instead of in parallel (default: False). False same_seed if True, all networks will use the same seed. If not, the seed will be randomly initialized with time(0) for each network (default). It has no influence when the networks argument is set (the seed has to be set individually for each network using net.set_seed() ), only when number is used. False args other named arguments you want to pass to the simulation method. {} Returns: Type Description a list of the values returned by method . Source code in ANNarchy/core/Network.py def parallel_run ( method , networks = None , number = 0 , max_processes =- 1 , measure_time = False , sequential = False , same_seed = False , ** args ): \"\"\" Allows to run multiple networks in parallel using multiprocessing. If the ``networks`` argument is provided as a list of Network objects, the given method will be executed for each of these networks. If ``number`` is given instead, the same number of networks will be created and the method is applied. If ``number`` is used, the created networks are not returned, you should return what you need to analyse. Example: ```python pop1 = PoissonPopulation(100, rates=10.0) pop2 = Population(100, Izhikevich) proj = Projection(pop1, pop2, 'exc') proj.connect_fixed_probability(weights=5.0, probability=0.2) m = Monitor(pop2, 'spike') compile() def simulation(idx, net): net.get(pop1).rates = 10. * idx net.simulate(1000.) return net.get(m).raster_plot() results = parallel_run(method=simulation, number = 3) t1, n1 = results[0] t2, n2 = results[1] t3, n3 = results[2] ``` :param method: a Python method which will be executed for each network. This function must accept an integer as first argument (id of the simulation) and a Network object as second argument. :param networks: a list of networks to simulate in parallel. :param number: the number of odentical networks to run in parallel. :param max_processes: maximal number of processes to start concurrently (default: the available number of cores on the machine). :param measure_time: if the total simulation time should be printed out. :param sequential: if True, runs the simulations sequentially instead of in parallel (default: False). :param same_seed: if True, all networks will use the same seed. If not, the seed will be randomly initialized with time(0) for each network (default). It has no influence when the ``networks`` argument is set (the seed has to be set individually for each network using ``net.set_seed()``), only when ``number`` is used. :param args: other named arguments you want to pass to the simulation method. :return: a list of the values returned by ``method``. \"\"\" # Check inputs if not networks and number < 1 : Global . _error ( 'parallel_run(): the networks or number arguments must be set.' , exit = True ) import types if not isinstance ( method , types . FunctionType ): Global . _error ( 'parallel_run(): the method argument must be a method.' , exit = True ) if not networks : # The magic network will run N times return _parallel_multi ( method , number , max_processes , measure_time , sequential , same_seed , args ) if not isinstance ( networks , list ): Global . _error ( 'parallel_run(): the networks argument must be a list.' , exit = True ) # Simulate the different networks return _parallel_networks ( method , networks , max_processes , measure_time , sequential , args )","title":"parallel_run()"},{"location":"API/Neuron/","text":"Neuron class # Neurons are container objects for all information corresponding to a special neuron type. This encapsulation allows a higher readability of the code. Through derivation of ANNarchy.Neuron , the user can define the neuron types he needs in his model. The type of the neuron (rate-coded or spiking) depends on the presence of the spike argument. Neuron # Base class to define a neuron. __init__ ( self , parameters = '' , equations = '' , spike = None , axon_spike = None , reset = None , axon_reset = None , refractory = None , functions = None , name = '' , description = '' , extra_values = {}) special # Parameters: Name Type Description Default parameters parameters of the neuron and their initial value. '' equations equations defining the temporal evolution of variables. '' functions additional functions used in the variables' equations. None spike condition to emit a spike (only for spiking neurons). None axon_spike condition to emit an axonal spike (only for spiking neurons and optional). The axonal spike can appear additional to the spike and is independent from refractoriness of a neuron. None reset changes to the variables after a spike (only for spiking neurons). None axon_reset changes to the variables after an axonal spike (only for spiking neurons). None refractory refractory period of a neuron after a spike (only for spiking neurons). None name name of the neuron type (used for reporting only). '' description short description of the neuron type (used for reporting). '' Source code in ANNarchy/core/Neuron.py def __init__ ( self , parameters = \"\" , equations = \"\" , spike = None , axon_spike = None , reset = None , axon_reset = None , refractory = None , functions = None , name = \"\" , description = \"\" , extra_values = {} ): \"\"\" :param parameters: parameters of the neuron and their initial value. :param equations: equations defining the temporal evolution of variables. :param functions: additional functions used in the variables' equations. :param spike: condition to emit a spike (only for spiking neurons). :param axon_spike: condition to emit an axonal spike (only for spiking neurons and optional). The axonal spike can appear additional to the spike and is independent from refractoriness of a neuron. :param reset: changes to the variables after a spike (only for spiking neurons). :param axon_reset: changes to the variables after an axonal spike (only for spiking neurons). :param refractory: refractory period of a neuron after a spike (only for spiking neurons). :param name: name of the neuron type (used for reporting only). :param description: short description of the neuron type (used for reporting). \"\"\" # Store the parameters and equations self . parameters = parameters self . equations = equations self . functions = functions self . spike = spike self . axon_spike = axon_spike self . reset = reset self . axon_reset = axon_reset self . refractory = refractory self . extra_values = extra_values # Find the type of the neuron self . type = 'spike' if self . spike else 'rate' # Not available by now ... if axon_spike and config [ 'paradigm' ] != \"openmp\" : _error ( \"Axonal spike conditions are only available for openMP by now.\" ) # Reporting if not hasattr ( self , '_instantiated' ) : # User-defined _objects [ 'neurons' ] . append ( self ) elif len ( self . _instantiated ) == 0 : # First instantiated of the class _objects [ 'neurons' ] . append ( self ) self . _rk_neurons_type = len ( _objects [ 'neurons' ]) if name : self . name = name else : self . name = self . _default_names [ self . type ] if description : self . short_description = description else : self . short_description = \"User-defined model of a spiking neuron.\" if self . type == 'spike' else \"User-defined model of a rate-coded neuron.\" # Analyse the neuron type self . description = None","title":"Neuron class"},{"location":"API/Neuron/#neuron-class","text":"Neurons are container objects for all information corresponding to a special neuron type. This encapsulation allows a higher readability of the code. Through derivation of ANNarchy.Neuron , the user can define the neuron types he needs in his model. The type of the neuron (rate-coded or spiking) depends on the presence of the spike argument.","title":"Neuron class"},{"location":"API/Neuron/#ANNarchy.core.Neuron.Neuron","text":"Base class to define a neuron.","title":"Neuron"},{"location":"API/Neuron/#ANNarchy.core.Neuron.Neuron.__init__","text":"Parameters: Name Type Description Default parameters parameters of the neuron and their initial value. '' equations equations defining the temporal evolution of variables. '' functions additional functions used in the variables' equations. None spike condition to emit a spike (only for spiking neurons). None axon_spike condition to emit an axonal spike (only for spiking neurons and optional). The axonal spike can appear additional to the spike and is independent from refractoriness of a neuron. None reset changes to the variables after a spike (only for spiking neurons). None axon_reset changes to the variables after an axonal spike (only for spiking neurons). None refractory refractory period of a neuron after a spike (only for spiking neurons). None name name of the neuron type (used for reporting only). '' description short description of the neuron type (used for reporting). '' Source code in ANNarchy/core/Neuron.py def __init__ ( self , parameters = \"\" , equations = \"\" , spike = None , axon_spike = None , reset = None , axon_reset = None , refractory = None , functions = None , name = \"\" , description = \"\" , extra_values = {} ): \"\"\" :param parameters: parameters of the neuron and their initial value. :param equations: equations defining the temporal evolution of variables. :param functions: additional functions used in the variables' equations. :param spike: condition to emit a spike (only for spiking neurons). :param axon_spike: condition to emit an axonal spike (only for spiking neurons and optional). The axonal spike can appear additional to the spike and is independent from refractoriness of a neuron. :param reset: changes to the variables after a spike (only for spiking neurons). :param axon_reset: changes to the variables after an axonal spike (only for spiking neurons). :param refractory: refractory period of a neuron after a spike (only for spiking neurons). :param name: name of the neuron type (used for reporting only). :param description: short description of the neuron type (used for reporting). \"\"\" # Store the parameters and equations self . parameters = parameters self . equations = equations self . functions = functions self . spike = spike self . axon_spike = axon_spike self . reset = reset self . axon_reset = axon_reset self . refractory = refractory self . extra_values = extra_values # Find the type of the neuron self . type = 'spike' if self . spike else 'rate' # Not available by now ... if axon_spike and config [ 'paradigm' ] != \"openmp\" : _error ( \"Axonal spike conditions are only available for openMP by now.\" ) # Reporting if not hasattr ( self , '_instantiated' ) : # User-defined _objects [ 'neurons' ] . append ( self ) elif len ( self . _instantiated ) == 0 : # First instantiated of the class _objects [ 'neurons' ] . append ( self ) self . _rk_neurons_type = len ( _objects [ 'neurons' ]) if name : self . name = name else : self . name = self . _default_names [ self . type ] if description : self . short_description = description else : self . short_description = \"User-defined model of a spiking neuron.\" if self . type == 'spike' else \"User-defined model of a rate-coded neuron.\" # Analyse the neuron type self . description = None","title":"__init__()"},{"location":"API/Population/","text":"Population class # A Population object represents a group of identical neurons. It is associated with a geometry (defining the number of neurons and optionally its spatial structure), a neuron type and optionally a name. Population # Container for a population of homogeneous neurons. neurons property readonly # Returns iteratively each neuron in the population. For instance, if you want to iterate over all neurons of a population: for neuron in pop . neurons : neuron . r = 0.0 Alternatively, one could also benefit from the __iter__ special command. The following code is equivalent: for neuron in pop : neuron . r = 0.0 __getitem__ ( self , * args , ** kwds ) special # Returns neurons froms coordinates in the population. If only one argument is given, it is interpeted as a rank and returns a single neuron. If slices are given, it returns a PopulationView object. Source code in ANNarchy/core/Population.py def __getitem__ ( self , * args , ** kwds ): \"\"\" Returns neurons froms coordinates in the population. If only one argument is given, it is interpeted as a rank and returns a single neuron. If slices are given, it returns a PopulationView object. \"\"\" indices = args [ 0 ] try : if np . issubdtype ( indices , int ): indices = int ( indices ) except : pass if isinstance ( indices , int ): # a single neuron return PopulationView ( self , ranks = [ int ( indices )], geometry = ( 1 ,)) elif isinstance ( indices , ( list , np . ndarray )): if isinstance ( indices , ( np . ndarray )): if indices . ndim != 1 : Global . _error ( 'only one-dimensional lists/arrays are allowed to address a population.' ) indices = list ( indices . astype ( int )) return PopulationView ( self , list ( indices ), geometry = ( len ( indices ),)) elif isinstance ( indices , slice ): # a single slice of ranks start , stop , step = indices . start , indices . stop , indices . step if indices . start is None : start = 0 if indices . stop is None : stop = self . size if indices . step is None : step = 1 rk_range = list ( range ( start , stop , step )) return PopulationView ( self , rk_range , geometry = ( len ( rk_range ),)) elif isinstance ( indices , tuple ): # a tuple slices = False for idx in indices : # check if there are slices in the coordinates if isinstance ( idx , slice ): # there is at least one slices = True if not slices : # return one neuron return self . neuron ( indices ) else : # Compute a list of ranks from the slices coords = [] # Expand the slices for rank in range ( len ( indices )): idx = indices [ rank ] if isinstance ( idx , int ): # no slice coords . append ([ idx ]) elif isinstance ( idx , slice ): # slice start , stop , step = idx . start , idx . stop , idx . step if idx . start is None : start = 0 if idx . stop is None : stop = self . geometry [ rank ] if idx . step is None : step = 1 rk_range = list ( range ( start , stop , step )) coords . append ( rk_range ) # Generate all ranks from the indices if self . dimension == 2 : ranks = [ self . rank_from_coordinates (( x , y )) for x in coords [ 0 ] for y in coords [ 1 ]] geometry = ( len ( coords [ 0 ]), len ( coords [ 1 ])) elif self . dimension == 3 : ranks = [ self . rank_from_coordinates (( x , y , z )) for x in coords [ 0 ] for y in coords [ 1 ] for z in coords [ 2 ]] geometry = ( len ( coords [ 0 ]), len ( coords [ 1 ]), len ( coords [ 2 ])) elif self . dimension == 4 : ranks = [ self . rank_from_coordinates (( x , y , z , k )) for x in coords [ 0 ] for y in coords [ 1 ] for z in coords [ 2 ] for k in coords [ 3 ]] geometry = ( len ( coords [ 0 ]), len ( coords [ 1 ]), len ( coords [ 2 ]), len ( coords [ 3 ])) else : Global . _error ( \"Slicing is implemented only for population with 4 dimensions at maximum\" , self . geometry ) if not max ( ranks ) < self . size : Global . _error ( \"Indices do not match the geometry of the population\" , self . geometry ) return PopulationView ( self , ranks , geometry = geometry ) Global . _warning ( 'Population' + self . name + ': can not address the population with' , indices ) return None __init__ ( self , geometry , neuron , name = None , stop_condition = None , storage_order = 'post_to_pre' , copied = False ) special # Parameters: Name Type Description Default geometry population geometry as tuple. If an integer is given, it is the size of the population. required neuron instance of ANNarchy.Neuron . It can be user-defined or a built-in model. required name unique name of the population (optional, it defaults to pop0 , pop1 , etc). None stop_condition a single condition on a neural variable which can stop the simulation whenever it is true. Example: python pop = Population(100, neuron=Izhikevich, name=\"Excitatory population\") None Source code in ANNarchy/core/Population.py def __init__ ( self , geometry , neuron , name = None , stop_condition = None , storage_order = 'post_to_pre' , copied = False ): \"\"\" :param geometry: population geometry as tuple. If an integer is given, it is the size of the population. :param neuron: instance of ``ANNarchy.Neuron``. It can be user-defined or a built-in model. :param name: unique name of the population (optional, it defaults to ``pop0``, ``pop1``, etc). :param stop_condition: a single condition on a neural variable which can stop the simulation whenever it is true. Example: ```python pop = Population(100, neuron=Izhikevich, name=\"Excitatory population\") ``` \"\"\" # Check if the network has already been compiled if Global . _network [ 0 ][ 'compiled' ] and not copied : Global . _error ( 'You cannot add a population after the network has been compiled.' ) # Store the provided geometry # automatically defines w, h, d, size if isinstance ( geometry , ( int , float )): # 1D self . geometry = ( int ( geometry ), ) self . width = int ( geometry ) self . height = int ( 1 ) self . depth = int ( 1 ) self . dimension = int ( 1 ) elif isinstance ( geometry , tuple ): # a tuple is given, can be 1 .. N dimensional self . geometry = () for d in geometry : self . geometry += ( int ( d ),) self . width = int ( geometry [ 0 ]) if len ( geometry ) >= 2 : self . height = int ( geometry [ 1 ]) else : self . height = int ( 1 ) if len ( geometry ) >= 3 : self . depth = int ( geometry [ 2 ]) else : self . depth = int ( 1 ) self . dimension = len ( geometry ) else : Global . _error ( 'Population(): the geometry must be either an integer or a tuple.' ) # Compute the size size = int ( 1 ) for i in range ( len ( self . geometry )): size *= int ( self . geometry [ i ]) self . size = int ( size ) self . ranks = list ( range ( self . size )) # Store the neuron type if inspect . isclass ( neuron ): self . neuron_type = neuron () else : self . neuron_type = copy . deepcopy ( neuron ) self . neuron_type . _analyse () # Store the stop condition self . stop_condition = stop_condition # Attribute a name if not provided self . id = len ( Global . _network [ 0 ][ 'populations' ]) self . class_name = 'pop' + str ( self . id ) if name : self . name = name else : self . name = self . class_name # Add the population to the global variable Global . _network [ 0 ][ 'populations' ] . append ( self ) # Get a list of parameters and variables self . parameters = [] self . variables = [] for param in self . neuron_type . description [ 'parameters' ]: self . parameters . append ( param [ 'name' ]) for var in self . neuron_type . description [ 'variables' ]: self . variables . append ( var [ 'name' ]) self . attributes = self . parameters + self . variables # Get a list of user-defined functions self . functions = [ func [ 'name' ] for func in self . neuron_type . description [ 'functions' ]] # Store initial values self . init = {} for param in self . neuron_type . description [ 'parameters' ]: self . init [ param [ 'name' ]] = param [ 'init' ] for var in self . neuron_type . description [ 'variables' ]: self . init [ var [ 'name' ]] = var [ 'init' ] # List of targets actually connected self . targets = [] # List of global operations needed by connected projections self . global_operations = [] # Maximum delay of connected projections self . max_delay = 0 # Spiking neurons: do they have to compute an average? self . _compute_mean_fr = - 1. # Finalize initialization self . initialized = False self . cyInstance = None self . enabled = True # Rank <-> Coordinates methods # for the one till three dimensional case we use cython optimized functions. from ANNarchy.core.cython_ext import Coordinates if self . dimension == 1 : self . _rank_from_coord = Coordinates . get_rank_from_1d_coord self . _coord_from_rank = Coordinates . get_1d_coord elif self . dimension == 2 : self . _rank_from_coord = Coordinates . get_rank_from_2d_coord self . _coord_from_rank = Coordinates . get_2d_coord elif self . dimension == 3 : self . _rank_from_coord = Coordinates . get_rank_from_3d_coord self . _coord_from_rank = Coordinates . get_3d_coord else : self . _rank_from_coord = np . ravel_multi_index self . _coord_from_rank = np . unravel_index self . _norm_coord_dict = { 1 : Coordinates . get_normalized_1d_coord , 2 : Coordinates . get_normalized_2d_coord , 3 : Coordinates . get_normalized_3d_coord } # Recorded variables self . _monitor = None # Is overwritten by SpecificPopulations self . _specific_template = {} # Storage order. TODO: why? self . _storage_order = storage_order clear ( self ) # Clears all spiking events previously emitted (history of spikes, delayed spikes). Can be useful if you do not want to totally reset a population (i.e. all variables), only to clear the spiking history between two trials. Note: does nothing for rate-coded networks. Source code in ANNarchy/core/Population.py def clear ( self ): \"\"\" Clears all spiking events previously emitted (history of spikes, delayed spikes). Can be useful if you do not want to totally reset a population (i.e. all variables), only to clear the spiking history between two trials. Note: does nothing for rate-coded networks. \"\"\" self . cyInstance . reset () compute_firing_rate ( self , window ) # Tells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable r . This method has an effect on spiking neurons only. If this method is not called, r will always be 0.0. r can of course be accessed and recorded as any other variable. Parameters: Name Type Description Default window window in ms over which the spikes will be counted. required Source code in ANNarchy/core/Population.py def compute_firing_rate ( self , window ): \"\"\" Tells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable `r`. This method has an effect on spiking neurons only. If this method is not called, `r` will always be 0.0. `r` can of course be accessed and recorded as any other variable. :param window: window in ms over which the spikes will be counted. \"\"\" if Global . _check_paradigm ( 'cuda' ): Global . _error ( 'compute_firing_rate() is not supported on CUDA yet.' ) if self . neuron_type . type == 'rate' : Global . _error ( 'compute_firing_rate(): the neuron is already rate-coded...' ) self . _compute_mean_fr = float ( window ) if self . initialized : getattr ( self . cyInstance , 'compute_firing_rate' )( self . _compute_mean_fr ) coordinates_from_rank ( self , rank ) # Returns the coordinates of a neuron based on its rank. Parameters: Name Type Description Default rank rank of the neuron. required Source code in ANNarchy/core/Population.py def coordinates_from_rank ( self , rank ): \"\"\" Returns the coordinates of a neuron based on its rank. :param rank: rank of the neuron. \"\"\" # Check the rank if not rank < self . size : Global . _error ( 'The given rank' , str ( rank ), 'is larger than the size of the population' , str ( self . size ) + '.' ) try : coord = self . _coord_from_rank ( rank , self . geometry ) except : Global . _error ( 'The given rank' , str ( rank ), 'is larger than the size of the population' , str ( self . size ) + '.' ) else : return coord disable ( self ) # Temporarily disables computations in this population (including the projections leading to it). You can re-enable it with the enable() method. Source code in ANNarchy/core/Population.py def disable ( self ): \"\"\" Temporarily disables computations in this population (including the projections leading to it). You can re-enable it with the ``enable()`` method. \"\"\" if self . initialized : self . cyInstance . activate ( False ) self . enabled = False enable ( self ) # (Re)-enables computations in this population, after they were disabled by the disable() method. The status of the population is accessible through the enabled flag. Source code in ANNarchy/core/Population.py def enable ( self ): \"\"\" (Re)-enables computations in this population, after they were disabled by the ``disable()`` method. The status of the population is accessible through the ``enabled`` flag. \"\"\" if self . initialized : self . cyInstance . activate ( True ) self . enabled = True get ( self , name ) # Returns the value of neural variables and parameters. Parameters: Name Type Description Default name attribute name as a string. required Source code in ANNarchy/core/Population.py def get ( self , name ): \"\"\" Returns the value of neural variables and parameters. :param name: attribute name as a string. \"\"\" return self . __getattr__ ( name ) load ( self , filename ) # Load the saved state of the population by Population.save() . Warning: Matlab data can not be loaded. Example: pop . load ( 'pop1.npz' ) pop . load ( 'pop1.txt' ) pop . load ( 'pop1.txt.gz' ) Parameters: Name Type Description Default filename the filename with relative or absolute path. required Source code in ANNarchy/core/Population.py def load ( self , filename ): \"\"\" Load the saved state of the population by `Population.save()`. Warning: Matlab data can not be loaded. Example: ```python pop.load('pop1.npz') pop.load('pop1.txt') pop.load('pop1.txt.gz') ``` :param filename: the filename with relative or absolute path. \"\"\" from ANNarchy.core.IO import _load_data self . _load_pop_data ( _load_data ( filename )) neuron ( self , * coord ) # Returns an IndividualNeuron object wrapping the neuron with the provided rank or coordinates. Source code in ANNarchy/core/Population.py def neuron ( self , * coord ): \"\"\" Returns an ``IndividualNeuron`` object wrapping the neuron with the provided rank or coordinates. \"\"\" # Transform arguments if len ( coord ) == 1 : if isinstance ( coord [ 0 ], int ): rank = coord [ 0 ] if not rank < self . size : Global . _error ( ' when accessing neuron' , str ( rank ), ': the population' , self . name , 'has only' , self . size , 'neurons (geometry ' + str ( self . geometry ) + ').' ) else : rank = self . rank_from_coordinates ( coord [ 0 ] ) if rank is None : return None else : # a tuple rank = self . rank_from_coordinates ( coord ) if rank is None : return None # Return corresponding neuron return IndividualNeuron ( self , rank ) normalized_coordinates_from_rank ( self , rank , norm = 1.0 ) # Returns normalized coordinates of a neuron based on its rank. The geometry of the population is mapped to the hypercube \\([0, 1]^d\\) Parameters: Name Type Description Default rank rank of the neuron required norm norm of the cube (default = 1.0) 1.0 Source code in ANNarchy/core/Population.py def normalized_coordinates_from_rank ( self , rank , norm = 1. ): \"\"\" Returns normalized coordinates of a neuron based on its rank. The geometry of the population is mapped to the hypercube $[0, 1]^d$ :param rank: rank of the neuron :param norm: norm of the cube (default = 1.0) \"\"\" try : normal = self . _norm_coord_dict [ self . dimension ]( rank , self . geometry ) except KeyError : coord = self . coordinates_from_rank ( rank ) normal = tuple () for dim in range ( self . dimension ): if self . _geometry [ dim ] > 1 : normal += ( norm * float ( coord [ dim ]) / float ( self . geometry [ dim ] - 1 ), ) else : normal += ( float ( rank ) / ( float ( self . size ) - 1.0 ),) # default? return normal rank_from_coordinates ( self , coord ) # Returns the rank of a neuron based on coordinates. Parameters: Name Type Description Default coord coordinate tuple, can be multidimensional. required Source code in ANNarchy/core/Population.py def rank_from_coordinates ( self , coord ): \"\"\" Returns the rank of a neuron based on coordinates. :param coord: coordinate tuple, can be multidimensional. \"\"\" try : rank = self . _rank_from_coord ( coord , self . geometry ) except : Global . _error ( 'rank_from_coordinates(): There is no neuron of coordinates' , coord , 'in the population' , self . name , self . geometry ) if rank > self . size : Global . _error ( 'rank_from_coordinates(), neuron' , str ( coord ), ': the population' , self . name , 'has only' , self . size , 'neurons (geometry ' + str ( self . geometry ) + ').' ) else : return rank reset ( self , attributes =- 1 ) # Resets all parameters and variables of the population to the value they had before the call to compile(). Parameters: Name Type Description Default attributes list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. -1 Source code in ANNarchy/core/Population.py def reset ( self , attributes =- 1 ): \"\"\" Resets all parameters and variables of the population to the value they had before the call to compile(). :param attributes: list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. \"\"\" if attributes == - 1 : try : self . set ( self . init ) except Exception as e : Global . _print ( e ) Global . _error ( \"Population.reset(): something went wrong while resetting.\" ) else : # only some of them for var in attributes : # check it exists if not var in self . attributes : Global . _warning ( \"Population.reset():\" , var , \"is not an attribute of the population, skipping.\" ) continue try : self . __setattr__ ( var , self . init [ var ]) except Exception as e : Global . _print ( e ) Global . _warning ( \"Population.reset(): something went wrong while resetting\" , var ) self . cyInstance . activate ( self . enabled ) self . cyInstance . reset () save ( self , filename ) # Saves all information about the population (structure, current value of parameters and variables) into a file. If the file name is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. Otherwise, the data will be pickled into a simple binary text file using pickle. Warning: The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Parameters: Name Type Description Default filename filename, may contain relative or absolute path. Example: python pop.save('pop1.npz') pop.save('pop1.txt') pop.save('pop1.txt.gz') pop.save('pop1.mat') required Source code in ANNarchy/core/Population.py def save ( self , filename ): \"\"\" Saves all information about the population (structure, current value of parameters and variables) into a file. * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. **Warning:** The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. :param filename: filename, may contain relative or absolute path. Example: ```python pop.save('pop1.npz') pop.save('pop1.txt') pop.save('pop1.txt.gz') pop.save('pop1.mat') ``` \"\"\" from ANNarchy.core.IO import _save_data _save_data ( filename , self . _data ()) set ( self , values ) # Sets the value of neural variables and parameters. Example: pop . set ({ 'tau' : 20.0 , 'r' = np . random . rand (( 8 , 8 )) } ) Parameters: Name Type Description Default values dictionary of attributes to be updated. required Source code in ANNarchy/core/Population.py def set ( self , values ): \"\"\" Sets the value of neural variables and parameters. Example: ```python pop.set({ 'tau' : 20.0, 'r'= np.random.rand((8,8)) } ) ``` :param values: dictionary of attributes to be updated. \"\"\" for name , value in values . items (): self . __setattr__ ( name , value ) size_in_bytes ( self ) # Returns the size of allocated memory on the C++ side. Please note that this does not contain monitored data and works only if compile() was invoked. Source code in ANNarchy/core/Population.py def size_in_bytes ( self ): \"\"\" Returns the size of allocated memory on the C++ side. Please note that this does not contain monitored data and works only if compile() was invoked. \"\"\" if self . initialized : return self . cyInstance . size_in_bytes () else : return 0 sum ( self , target ) # Returns the array of weighted sums corresponding to the target: excitatory = pop . sum ( 'exc' ) For spiking networks, this is equivalent to accessing the conductances directly: excitatory = pop . g_exc If no incoming projection has the given target, the method returns zeros. Note: it is not possible to distinguish the original population when the same target is used. Parameters: Name Type Description Default target the desired projection target. required Source code in ANNarchy/core/Population.py def sum ( self , target ): \"\"\" Returns the array of weighted sums corresponding to the target: ```python excitatory = pop.sum('exc') ``` For spiking networks, this is equivalent to accessing the conductances directly: ```python excitatory = pop.g_exc ``` If no incoming projection has the given target, the method returns zeros. **Note:** it is not possible to distinguish the original population when the same target is used. :param target: the desired projection target. \"\"\" # Check if the network is initialized if not self . initialized : Global . _warning ( 'sum(): the population' , self . name , 'is not initialized yet.' ) return np . zeros ( self . geometry ) # Check if a projection has this type if not target in self . targets : Global . _warning ( 'sum(): the population' , self . name , 'receives no projection with the target' , target ) return np . zeros ( self . geometry ) # Spiking neurons already have conductances available if self . neuron_type . type == 'spike' : return getattr ( self , 'g_' + target ) # Otherwise, call the Cython method return getattr ( self . cyInstance , 'get_sum_' + target )()","title":"Population class"},{"location":"API/Population/#population-class","text":"A Population object represents a group of identical neurons. It is associated with a geometry (defining the number of neurons and optionally its spatial structure), a neuron type and optionally a name.","title":"Population class"},{"location":"API/Population/#ANNarchy.core.Population.Population","text":"Container for a population of homogeneous neurons.","title":"Population"},{"location":"API/Population/#ANNarchy.core.Population.Population.neurons","text":"Returns iteratively each neuron in the population. For instance, if you want to iterate over all neurons of a population: for neuron in pop . neurons : neuron . r = 0.0 Alternatively, one could also benefit from the __iter__ special command. The following code is equivalent: for neuron in pop : neuron . r = 0.0","title":"neurons"},{"location":"API/Population/#ANNarchy.core.Population.Population.__getitem__","text":"Returns neurons froms coordinates in the population. If only one argument is given, it is interpeted as a rank and returns a single neuron. If slices are given, it returns a PopulationView object. Source code in ANNarchy/core/Population.py def __getitem__ ( self , * args , ** kwds ): \"\"\" Returns neurons froms coordinates in the population. If only one argument is given, it is interpeted as a rank and returns a single neuron. If slices are given, it returns a PopulationView object. \"\"\" indices = args [ 0 ] try : if np . issubdtype ( indices , int ): indices = int ( indices ) except : pass if isinstance ( indices , int ): # a single neuron return PopulationView ( self , ranks = [ int ( indices )], geometry = ( 1 ,)) elif isinstance ( indices , ( list , np . ndarray )): if isinstance ( indices , ( np . ndarray )): if indices . ndim != 1 : Global . _error ( 'only one-dimensional lists/arrays are allowed to address a population.' ) indices = list ( indices . astype ( int )) return PopulationView ( self , list ( indices ), geometry = ( len ( indices ),)) elif isinstance ( indices , slice ): # a single slice of ranks start , stop , step = indices . start , indices . stop , indices . step if indices . start is None : start = 0 if indices . stop is None : stop = self . size if indices . step is None : step = 1 rk_range = list ( range ( start , stop , step )) return PopulationView ( self , rk_range , geometry = ( len ( rk_range ),)) elif isinstance ( indices , tuple ): # a tuple slices = False for idx in indices : # check if there are slices in the coordinates if isinstance ( idx , slice ): # there is at least one slices = True if not slices : # return one neuron return self . neuron ( indices ) else : # Compute a list of ranks from the slices coords = [] # Expand the slices for rank in range ( len ( indices )): idx = indices [ rank ] if isinstance ( idx , int ): # no slice coords . append ([ idx ]) elif isinstance ( idx , slice ): # slice start , stop , step = idx . start , idx . stop , idx . step if idx . start is None : start = 0 if idx . stop is None : stop = self . geometry [ rank ] if idx . step is None : step = 1 rk_range = list ( range ( start , stop , step )) coords . append ( rk_range ) # Generate all ranks from the indices if self . dimension == 2 : ranks = [ self . rank_from_coordinates (( x , y )) for x in coords [ 0 ] for y in coords [ 1 ]] geometry = ( len ( coords [ 0 ]), len ( coords [ 1 ])) elif self . dimension == 3 : ranks = [ self . rank_from_coordinates (( x , y , z )) for x in coords [ 0 ] for y in coords [ 1 ] for z in coords [ 2 ]] geometry = ( len ( coords [ 0 ]), len ( coords [ 1 ]), len ( coords [ 2 ])) elif self . dimension == 4 : ranks = [ self . rank_from_coordinates (( x , y , z , k )) for x in coords [ 0 ] for y in coords [ 1 ] for z in coords [ 2 ] for k in coords [ 3 ]] geometry = ( len ( coords [ 0 ]), len ( coords [ 1 ]), len ( coords [ 2 ]), len ( coords [ 3 ])) else : Global . _error ( \"Slicing is implemented only for population with 4 dimensions at maximum\" , self . geometry ) if not max ( ranks ) < self . size : Global . _error ( \"Indices do not match the geometry of the population\" , self . geometry ) return PopulationView ( self , ranks , geometry = geometry ) Global . _warning ( 'Population' + self . name + ': can not address the population with' , indices ) return None","title":"__getitem__()"},{"location":"API/Population/#ANNarchy.core.Population.Population.__init__","text":"Parameters: Name Type Description Default geometry population geometry as tuple. If an integer is given, it is the size of the population. required neuron instance of ANNarchy.Neuron . It can be user-defined or a built-in model. required name unique name of the population (optional, it defaults to pop0 , pop1 , etc). None stop_condition a single condition on a neural variable which can stop the simulation whenever it is true. Example: python pop = Population(100, neuron=Izhikevich, name=\"Excitatory population\") None Source code in ANNarchy/core/Population.py def __init__ ( self , geometry , neuron , name = None , stop_condition = None , storage_order = 'post_to_pre' , copied = False ): \"\"\" :param geometry: population geometry as tuple. If an integer is given, it is the size of the population. :param neuron: instance of ``ANNarchy.Neuron``. It can be user-defined or a built-in model. :param name: unique name of the population (optional, it defaults to ``pop0``, ``pop1``, etc). :param stop_condition: a single condition on a neural variable which can stop the simulation whenever it is true. Example: ```python pop = Population(100, neuron=Izhikevich, name=\"Excitatory population\") ``` \"\"\" # Check if the network has already been compiled if Global . _network [ 0 ][ 'compiled' ] and not copied : Global . _error ( 'You cannot add a population after the network has been compiled.' ) # Store the provided geometry # automatically defines w, h, d, size if isinstance ( geometry , ( int , float )): # 1D self . geometry = ( int ( geometry ), ) self . width = int ( geometry ) self . height = int ( 1 ) self . depth = int ( 1 ) self . dimension = int ( 1 ) elif isinstance ( geometry , tuple ): # a tuple is given, can be 1 .. N dimensional self . geometry = () for d in geometry : self . geometry += ( int ( d ),) self . width = int ( geometry [ 0 ]) if len ( geometry ) >= 2 : self . height = int ( geometry [ 1 ]) else : self . height = int ( 1 ) if len ( geometry ) >= 3 : self . depth = int ( geometry [ 2 ]) else : self . depth = int ( 1 ) self . dimension = len ( geometry ) else : Global . _error ( 'Population(): the geometry must be either an integer or a tuple.' ) # Compute the size size = int ( 1 ) for i in range ( len ( self . geometry )): size *= int ( self . geometry [ i ]) self . size = int ( size ) self . ranks = list ( range ( self . size )) # Store the neuron type if inspect . isclass ( neuron ): self . neuron_type = neuron () else : self . neuron_type = copy . deepcopy ( neuron ) self . neuron_type . _analyse () # Store the stop condition self . stop_condition = stop_condition # Attribute a name if not provided self . id = len ( Global . _network [ 0 ][ 'populations' ]) self . class_name = 'pop' + str ( self . id ) if name : self . name = name else : self . name = self . class_name # Add the population to the global variable Global . _network [ 0 ][ 'populations' ] . append ( self ) # Get a list of parameters and variables self . parameters = [] self . variables = [] for param in self . neuron_type . description [ 'parameters' ]: self . parameters . append ( param [ 'name' ]) for var in self . neuron_type . description [ 'variables' ]: self . variables . append ( var [ 'name' ]) self . attributes = self . parameters + self . variables # Get a list of user-defined functions self . functions = [ func [ 'name' ] for func in self . neuron_type . description [ 'functions' ]] # Store initial values self . init = {} for param in self . neuron_type . description [ 'parameters' ]: self . init [ param [ 'name' ]] = param [ 'init' ] for var in self . neuron_type . description [ 'variables' ]: self . init [ var [ 'name' ]] = var [ 'init' ] # List of targets actually connected self . targets = [] # List of global operations needed by connected projections self . global_operations = [] # Maximum delay of connected projections self . max_delay = 0 # Spiking neurons: do they have to compute an average? self . _compute_mean_fr = - 1. # Finalize initialization self . initialized = False self . cyInstance = None self . enabled = True # Rank <-> Coordinates methods # for the one till three dimensional case we use cython optimized functions. from ANNarchy.core.cython_ext import Coordinates if self . dimension == 1 : self . _rank_from_coord = Coordinates . get_rank_from_1d_coord self . _coord_from_rank = Coordinates . get_1d_coord elif self . dimension == 2 : self . _rank_from_coord = Coordinates . get_rank_from_2d_coord self . _coord_from_rank = Coordinates . get_2d_coord elif self . dimension == 3 : self . _rank_from_coord = Coordinates . get_rank_from_3d_coord self . _coord_from_rank = Coordinates . get_3d_coord else : self . _rank_from_coord = np . ravel_multi_index self . _coord_from_rank = np . unravel_index self . _norm_coord_dict = { 1 : Coordinates . get_normalized_1d_coord , 2 : Coordinates . get_normalized_2d_coord , 3 : Coordinates . get_normalized_3d_coord } # Recorded variables self . _monitor = None # Is overwritten by SpecificPopulations self . _specific_template = {} # Storage order. TODO: why? self . _storage_order = storage_order","title":"__init__()"},{"location":"API/Population/#ANNarchy.core.Population.Population.clear","text":"Clears all spiking events previously emitted (history of spikes, delayed spikes). Can be useful if you do not want to totally reset a population (i.e. all variables), only to clear the spiking history between two trials. Note: does nothing for rate-coded networks. Source code in ANNarchy/core/Population.py def clear ( self ): \"\"\" Clears all spiking events previously emitted (history of spikes, delayed spikes). Can be useful if you do not want to totally reset a population (i.e. all variables), only to clear the spiking history between two trials. Note: does nothing for rate-coded networks. \"\"\" self . cyInstance . reset ()","title":"clear()"},{"location":"API/Population/#ANNarchy.core.Population.Population.compute_firing_rate","text":"Tells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable r . This method has an effect on spiking neurons only. If this method is not called, r will always be 0.0. r can of course be accessed and recorded as any other variable. Parameters: Name Type Description Default window window in ms over which the spikes will be counted. required Source code in ANNarchy/core/Population.py def compute_firing_rate ( self , window ): \"\"\" Tells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable `r`. This method has an effect on spiking neurons only. If this method is not called, `r` will always be 0.0. `r` can of course be accessed and recorded as any other variable. :param window: window in ms over which the spikes will be counted. \"\"\" if Global . _check_paradigm ( 'cuda' ): Global . _error ( 'compute_firing_rate() is not supported on CUDA yet.' ) if self . neuron_type . type == 'rate' : Global . _error ( 'compute_firing_rate(): the neuron is already rate-coded...' ) self . _compute_mean_fr = float ( window ) if self . initialized : getattr ( self . cyInstance , 'compute_firing_rate' )( self . _compute_mean_fr )","title":"compute_firing_rate()"},{"location":"API/Population/#ANNarchy.core.Population.Population.coordinates_from_rank","text":"Returns the coordinates of a neuron based on its rank. Parameters: Name Type Description Default rank rank of the neuron. required Source code in ANNarchy/core/Population.py def coordinates_from_rank ( self , rank ): \"\"\" Returns the coordinates of a neuron based on its rank. :param rank: rank of the neuron. \"\"\" # Check the rank if not rank < self . size : Global . _error ( 'The given rank' , str ( rank ), 'is larger than the size of the population' , str ( self . size ) + '.' ) try : coord = self . _coord_from_rank ( rank , self . geometry ) except : Global . _error ( 'The given rank' , str ( rank ), 'is larger than the size of the population' , str ( self . size ) + '.' ) else : return coord","title":"coordinates_from_rank()"},{"location":"API/Population/#ANNarchy.core.Population.Population.disable","text":"Temporarily disables computations in this population (including the projections leading to it). You can re-enable it with the enable() method. Source code in ANNarchy/core/Population.py def disable ( self ): \"\"\" Temporarily disables computations in this population (including the projections leading to it). You can re-enable it with the ``enable()`` method. \"\"\" if self . initialized : self . cyInstance . activate ( False ) self . enabled = False","title":"disable()"},{"location":"API/Population/#ANNarchy.core.Population.Population.enable","text":"(Re)-enables computations in this population, after they were disabled by the disable() method. The status of the population is accessible through the enabled flag. Source code in ANNarchy/core/Population.py def enable ( self ): \"\"\" (Re)-enables computations in this population, after they were disabled by the ``disable()`` method. The status of the population is accessible through the ``enabled`` flag. \"\"\" if self . initialized : self . cyInstance . activate ( True ) self . enabled = True","title":"enable()"},{"location":"API/Population/#ANNarchy.core.Population.Population.get","text":"Returns the value of neural variables and parameters. Parameters: Name Type Description Default name attribute name as a string. required Source code in ANNarchy/core/Population.py def get ( self , name ): \"\"\" Returns the value of neural variables and parameters. :param name: attribute name as a string. \"\"\" return self . __getattr__ ( name )","title":"get()"},{"location":"API/Population/#ANNarchy.core.Population.Population.load","text":"Load the saved state of the population by Population.save() . Warning: Matlab data can not be loaded. Example: pop . load ( 'pop1.npz' ) pop . load ( 'pop1.txt' ) pop . load ( 'pop1.txt.gz' ) Parameters: Name Type Description Default filename the filename with relative or absolute path. required Source code in ANNarchy/core/Population.py def load ( self , filename ): \"\"\" Load the saved state of the population by `Population.save()`. Warning: Matlab data can not be loaded. Example: ```python pop.load('pop1.npz') pop.load('pop1.txt') pop.load('pop1.txt.gz') ``` :param filename: the filename with relative or absolute path. \"\"\" from ANNarchy.core.IO import _load_data self . _load_pop_data ( _load_data ( filename ))","title":"load()"},{"location":"API/Population/#ANNarchy.core.Population.Population.neuron","text":"Returns an IndividualNeuron object wrapping the neuron with the provided rank or coordinates. Source code in ANNarchy/core/Population.py def neuron ( self , * coord ): \"\"\" Returns an ``IndividualNeuron`` object wrapping the neuron with the provided rank or coordinates. \"\"\" # Transform arguments if len ( coord ) == 1 : if isinstance ( coord [ 0 ], int ): rank = coord [ 0 ] if not rank < self . size : Global . _error ( ' when accessing neuron' , str ( rank ), ': the population' , self . name , 'has only' , self . size , 'neurons (geometry ' + str ( self . geometry ) + ').' ) else : rank = self . rank_from_coordinates ( coord [ 0 ] ) if rank is None : return None else : # a tuple rank = self . rank_from_coordinates ( coord ) if rank is None : return None # Return corresponding neuron return IndividualNeuron ( self , rank )","title":"neuron()"},{"location":"API/Population/#ANNarchy.core.Population.Population.normalized_coordinates_from_rank","text":"Returns normalized coordinates of a neuron based on its rank. The geometry of the population is mapped to the hypercube \\([0, 1]^d\\) Parameters: Name Type Description Default rank rank of the neuron required norm norm of the cube (default = 1.0) 1.0 Source code in ANNarchy/core/Population.py def normalized_coordinates_from_rank ( self , rank , norm = 1. ): \"\"\" Returns normalized coordinates of a neuron based on its rank. The geometry of the population is mapped to the hypercube $[0, 1]^d$ :param rank: rank of the neuron :param norm: norm of the cube (default = 1.0) \"\"\" try : normal = self . _norm_coord_dict [ self . dimension ]( rank , self . geometry ) except KeyError : coord = self . coordinates_from_rank ( rank ) normal = tuple () for dim in range ( self . dimension ): if self . _geometry [ dim ] > 1 : normal += ( norm * float ( coord [ dim ]) / float ( self . geometry [ dim ] - 1 ), ) else : normal += ( float ( rank ) / ( float ( self . size ) - 1.0 ),) # default? return normal","title":"normalized_coordinates_from_rank()"},{"location":"API/Population/#ANNarchy.core.Population.Population.rank_from_coordinates","text":"Returns the rank of a neuron based on coordinates. Parameters: Name Type Description Default coord coordinate tuple, can be multidimensional. required Source code in ANNarchy/core/Population.py def rank_from_coordinates ( self , coord ): \"\"\" Returns the rank of a neuron based on coordinates. :param coord: coordinate tuple, can be multidimensional. \"\"\" try : rank = self . _rank_from_coord ( coord , self . geometry ) except : Global . _error ( 'rank_from_coordinates(): There is no neuron of coordinates' , coord , 'in the population' , self . name , self . geometry ) if rank > self . size : Global . _error ( 'rank_from_coordinates(), neuron' , str ( coord ), ': the population' , self . name , 'has only' , self . size , 'neurons (geometry ' + str ( self . geometry ) + ').' ) else : return rank","title":"rank_from_coordinates()"},{"location":"API/Population/#ANNarchy.core.Population.Population.reset","text":"Resets all parameters and variables of the population to the value they had before the call to compile(). Parameters: Name Type Description Default attributes list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. -1 Source code in ANNarchy/core/Population.py def reset ( self , attributes =- 1 ): \"\"\" Resets all parameters and variables of the population to the value they had before the call to compile(). :param attributes: list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. \"\"\" if attributes == - 1 : try : self . set ( self . init ) except Exception as e : Global . _print ( e ) Global . _error ( \"Population.reset(): something went wrong while resetting.\" ) else : # only some of them for var in attributes : # check it exists if not var in self . attributes : Global . _warning ( \"Population.reset():\" , var , \"is not an attribute of the population, skipping.\" ) continue try : self . __setattr__ ( var , self . init [ var ]) except Exception as e : Global . _print ( e ) Global . _warning ( \"Population.reset(): something went wrong while resetting\" , var ) self . cyInstance . activate ( self . enabled ) self . cyInstance . reset ()","title":"reset()"},{"location":"API/Population/#ANNarchy.core.Population.Population.save","text":"Saves all information about the population (structure, current value of parameters and variables) into a file. If the file name is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. Otherwise, the data will be pickled into a simple binary text file using pickle. Warning: The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Parameters: Name Type Description Default filename filename, may contain relative or absolute path. Example: python pop.save('pop1.npz') pop.save('pop1.txt') pop.save('pop1.txt.gz') pop.save('pop1.mat') required Source code in ANNarchy/core/Population.py def save ( self , filename ): \"\"\" Saves all information about the population (structure, current value of parameters and variables) into a file. * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. **Warning:** The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. :param filename: filename, may contain relative or absolute path. Example: ```python pop.save('pop1.npz') pop.save('pop1.txt') pop.save('pop1.txt.gz') pop.save('pop1.mat') ``` \"\"\" from ANNarchy.core.IO import _save_data _save_data ( filename , self . _data ())","title":"save()"},{"location":"API/Population/#ANNarchy.core.Population.Population.set","text":"Sets the value of neural variables and parameters. Example: pop . set ({ 'tau' : 20.0 , 'r' = np . random . rand (( 8 , 8 )) } ) Parameters: Name Type Description Default values dictionary of attributes to be updated. required Source code in ANNarchy/core/Population.py def set ( self , values ): \"\"\" Sets the value of neural variables and parameters. Example: ```python pop.set({ 'tau' : 20.0, 'r'= np.random.rand((8,8)) } ) ``` :param values: dictionary of attributes to be updated. \"\"\" for name , value in values . items (): self . __setattr__ ( name , value )","title":"set()"},{"location":"API/Population/#ANNarchy.core.Population.Population.size_in_bytes","text":"Returns the size of allocated memory on the C++ side. Please note that this does not contain monitored data and works only if compile() was invoked. Source code in ANNarchy/core/Population.py def size_in_bytes ( self ): \"\"\" Returns the size of allocated memory on the C++ side. Please note that this does not contain monitored data and works only if compile() was invoked. \"\"\" if self . initialized : return self . cyInstance . size_in_bytes () else : return 0","title":"size_in_bytes()"},{"location":"API/Population/#ANNarchy.core.Population.Population.sum","text":"Returns the array of weighted sums corresponding to the target: excitatory = pop . sum ( 'exc' ) For spiking networks, this is equivalent to accessing the conductances directly: excitatory = pop . g_exc If no incoming projection has the given target, the method returns zeros. Note: it is not possible to distinguish the original population when the same target is used. Parameters: Name Type Description Default target the desired projection target. required Source code in ANNarchy/core/Population.py def sum ( self , target ): \"\"\" Returns the array of weighted sums corresponding to the target: ```python excitatory = pop.sum('exc') ``` For spiking networks, this is equivalent to accessing the conductances directly: ```python excitatory = pop.g_exc ``` If no incoming projection has the given target, the method returns zeros. **Note:** it is not possible to distinguish the original population when the same target is used. :param target: the desired projection target. \"\"\" # Check if the network is initialized if not self . initialized : Global . _warning ( 'sum(): the population' , self . name , 'is not initialized yet.' ) return np . zeros ( self . geometry ) # Check if a projection has this type if not target in self . targets : Global . _warning ( 'sum(): the population' , self . name , 'receives no projection with the target' , target ) return np . zeros ( self . geometry ) # Spiking neurons already have conductances available if self . neuron_type . type == 'spike' : return getattr ( self , 'g_' + target ) # Otherwise, call the Cython method return getattr ( self . cyInstance , 'get_sum_' + target )()","title":"sum()"},{"location":"API/Projection/","text":"Projection class # The class ANNarchy.Projection defines projections at the population level. A projection is an ensemble of connections (or synapses) between a subset of a population (called the pre-synaptic population) and a subset of another population (the post-synaptic population), with a specific connection type. The pre- and post-synaptic populations may be the same. Projection # Container for all the synapses of the same type between two populations. dendrites property readonly # Iteratively returns the dendrites corresponding to this projection. nb_synapses property readonly # Total number of synapses in the projection. size property readonly # Number of post-synaptic neurons receiving synapses. __init__ ( self , pre , post , target , synapse = None , name = None , disable_omp = True , copied = False ) special # By default, the synapse only ensures linear synaptic transmission: For rate-coded populations: psp = w * pre.r For spiking populations: g_target += w to modify this behavior one need to provide a Synapse object. Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection. required synapse a Synapse instance. None name unique name of the projection (optional, it defaults to proj0 , proj1 , etc). None disable_omp especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to False . True Source code in ANNarchy/core/Projection.py def __init__ ( self , pre , post , target , synapse = None , name = None , disable_omp = True , copied = False ): \"\"\" By default, the synapse only ensures linear synaptic transmission: * For rate-coded populations: ``psp = w * pre.r`` * For spiking populations: ``g_target += w`` to modify this behavior one need to provide a Synapse object. :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection. :param synapse: a ``Synapse`` instance. :param name: unique name of the projection (optional, it defaults to ``proj0``, ``proj1``, etc). :param disable_omp: especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to `False`. \"\"\" # Check if the network has already been compiled if Global . _network [ 0 ][ 'compiled' ] and not copied : Global . _error ( 'you cannot add a projection after the network has been compiled.' ) # Store the pre and post synaptic populations # the user provide either a string or a population object # in case of string, we need to search for the corresponding object if isinstance ( pre , str ): for pop in Global . _network [ 0 ][ 'populations' ]: if pop . name == pre : self . pre = pop else : self . pre = pre if isinstance ( post , str ): for pop in Global . _network [ 0 ][ 'populations' ]: if pop . name == post : self . post = pop else : self . post = post # Store the arguments if isinstance ( target , list ) and len ( target ) == 1 : self . target = target [ 0 ] else : self . target = target # Add the target(s) to the postsynaptic population if isinstance ( self . target , list ): for _target in self . target : self . post . targets . append ( _target ) else : self . post . targets . append ( self . target ) # check if a synapse description is attached if not synapse : # No synapse attached assume default synapse based on # presynaptic population. if self . pre . neuron_type . type == 'rate' : from ANNarchy.models.Synapses import DefaultRateCodedSynapse self . synapse_type = DefaultRateCodedSynapse () self . synapse_type . type = 'rate' else : from ANNarchy.models.Synapses import DefaultSpikingSynapse self . synapse_type = DefaultSpikingSynapse () self . synapse_type . type = 'spike' elif inspect . isclass ( synapse ): self . synapse_type = synapse () self . synapse_type . type = self . pre . neuron_type . type else : self . synapse_type = copy . deepcopy ( synapse ) self . synapse_type . type = self . pre . neuron_type . type # Disable omp for spiking networks self . disable_omp = disable_omp # Analyse the parameters and variables self . synapse_type . _analyse () # Create a default name self . id = len ( Global . _network [ 0 ][ 'projections' ]) if name : self . name = name else : self . name = 'proj' + str ( self . id ) # Get a list of parameters and variables self . parameters = [] self . init = {} for param in self . synapse_type . description [ 'parameters' ]: self . parameters . append ( param [ 'name' ]) self . init [ param [ 'name' ]] = param [ 'init' ] self . variables = [] for var in self . synapse_type . description [ 'variables' ]: self . variables . append ( var [ 'name' ]) self . init [ var [ 'name' ]] = var [ 'init' ] self . attributes = self . parameters + self . variables # Get a list of user-defined functions self . functions = [ func [ 'name' ] for func in self . synapse_type . description [ 'functions' ]] # Add the population to the global network Global . _network [ 0 ][ 'projections' ] . append ( self ) # Finalize initialization self . initialized = False # Cython instance self . cyInstance = None # Connectivity self . _synapses = None self . _connection_method = None self . _connection_args = None self . _connection_delay = None self . _connector = None # Default configuration for connectivity self . _storage_format = \"lil\" self . _storage_order = \"post_to_pre\" # If a single weight value is used self . _single_constant_weight = False # If a dense matrix should be used instead of LIL self . _dense_matrix = False # Are random distribution used for weights/delays self . connector_weight_dist = None self . connector_delay_dist = None # Reporting self . connector_name = \"Specific\" self . connector_description = \"Specific\" # Overwritten by derived classes, to add # additional code self . _specific_template = {} # Set to False by derived classes to prevent saving of # data, e. g. in case of weight-sharing projections self . _saveable = True # To allow case-specific adjustment of parallelization # parameters, e. g. openMP schedule, we introduce a # dictionary read by the ProjectionGenerator. # # Will be overwritten either by inherited classes or # by an omp_config provided to the compile() method. self . _omp_config = { #'psp_schedule': 'schedule(dynamic)' } # If set to true, the code generator is not allowed to # split the matrix. This will be the case for many # SpecificProjections defined by the user or is disabled # globally. if self . synapse_type . type == \"rate\" : # Normally, the split should not be used for rate-coded models # but maybe there are cases where we want to enable it ... self . _no_split_matrix = Global . config [ \"disable_split_matrix\" ] # If the number of elements is too small, the split # might not be efficient. if self . post . size < Global . OMP_MIN_NB_NEURONS : self . _no_split_matrix = True else : # If the number of elements is too small, the split # might not be efficient. if self . post . size < Global . OMP_MIN_NB_NEURONS : self . _no_split_matrix = True else : self . _no_split_matrix = False connect_all_to_all ( self , weights , delays = 0.0 , allow_self_connections = False , force_multiple_weights = False , storage_format = 'lil' , storage_order = 'post_to_pre' ) # Builds an all-to-all connection pattern between the two populations. Parameters: Name Type Description Default weights synaptic values, either a single value or a random distribution object. required delays synaptic delays, either a single value or a random distribution object (default=dt). 0.0 allow_self_connections if True, self-connections between a neuron and itself are allowed (default = False if the pre- and post-populations are identical, True otherwise). False force_multiple_weights if a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used. False storage_format for some of the default connection patterns, ANNarchy provide different storage formats. For all-to-all we support list-of-list (\"lil\") or compressed sparse row (\"csr\"), by default lil is chosen. 'lil' storage_order for some of the available storage formats, ANNarchy provides different storage orderings. For all-to-all we support pre_to_post and post_to_pre, by default post_to_pre is chosen. Please note, the last two arguments should be changed carefully, as they can have large impact on the computational performance of ANNarchy. 'post_to_pre' Source code in ANNarchy/core/Projection.py def connect_all_to_all ( self , weights , delays = 0.0 , allow_self_connections = False , force_multiple_weights = False , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds an all-to-all connection pattern between the two populations. :param weights: synaptic values, either a single value or a random distribution object. :param delays: synaptic delays, either a single value or a random distribution object (default=dt). :param allow_self_connections: if True, self-connections between a neuron and itself are allowed (default = False if the pre- and post-populations are identical, True otherwise). :param force_multiple_weights: if a single value is provided for ``weights`` and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting ``force_multiple_weights`` to True ensures that a value per synapse will be used. :param storage_format: for some of the default connection patterns, ANNarchy provide different storage formats. For all-to-all we support list-of-list (\"lil\") or compressed sparse row (\"csr\"), by default lil is chosen. :param storage_order: for some of the available storage formats, ANNarchy provides different storage orderings. For all-to-all we support pre_to_post and post_to_pre, by default post_to_pre is chosen. Please note, the last two arguments should be changed carefully, as they can have large impact on the computational performance of ANNarchy. \"\"\" pre_pop = self . pre if not isinstance ( self . pre , PopulationView ) else self . pre . population post_pop = self . post if not isinstance ( self . post , PopulationView ) else self . post . population if pre_pop != post_pop : allow_self_connections = True self . connector_name = \"All-to-All\" self . connector_description = \"All-to-All, weights %(weight)s , delays %(delay)s \" % { 'weight' : _process_random ( weights ), 'delay' : _process_random ( delays )} # Does the projection define a single non-plastic weight? if isinstance ( weights , ( int , float )) and not force_multiple_weights : self . _single_constant_weight = True # Is it a dense connectivity matrix? if allow_self_connections and not isinstance ( self . pre , PopulationView ) and not isinstance ( self . post , PopulationView ): # TODO: for the moment disabled as it is not implemented # correctly (HD (15. Feb. 2019)) #self._dense_matrix = True self . _dense_matrix = False # if weights or delays are from random distribution I need to know this in code generator self . connector_weight_dist = weights if isinstance ( weights , RandomDistribution ) else None self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None # Store the connectivity self . _store_connectivity ( all_to_all , ( weights , delays , allow_self_connections , storage_format , storage_order ), delays , storage_format , storage_order ) return self connect_dog ( self , amp_pos , sigma_pos , amp_neg , sigma_neg , delays = 0.0 , limit = 0.01 , allow_self_connections = False , storage_format = 'lil' ) # Builds a Difference-Of-Gaussians connection pattern between the two populations. Each neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Difference-Of-Gaussians profile. Parameters: Name Type Description Default amp_pos amplitude of the positive Gaussian function required sigma_pos width of the positive Gaussian function required amp_neg amplitude of the negative Gaussian function required sigma_neg width of the negative Gaussian function required delays synaptic delay, either a single value or a random distribution object (default=dt). 0.0 limit proportion of amp below which synapses are not created (default: 0.01) 0.01 allow_self_connections allows connections between a neuron and itself. False storage_format for some of the default connection patterns, ANNarchy provide different storage formats. By default lil (list-in-list) is chosen. 'lil' Source code in ANNarchy/core/Projection.py def connect_dog ( self , amp_pos , sigma_pos , amp_neg , sigma_neg , delays = 0.0 , limit = 0.01 , allow_self_connections = False , storage_format = \"lil\" ): \"\"\" Builds a Difference-Of-Gaussians connection pattern between the two populations. Each neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Difference-Of-Gaussians profile. :param amp_pos: amplitude of the positive Gaussian function :param sigma_pos: width of the positive Gaussian function :param amp_neg: amplitude of the negative Gaussian function :param sigma_neg: width of the negative Gaussian function :param delays: synaptic delay, either a single value or a random distribution object (default=dt). :param limit: proportion of *amp* below which synapses are not created (default: 0.01) :param allow_self_connections: allows connections between a neuron and itself. :param storage_format: for some of the default connection patterns, ANNarchy provide different storage formats. By default *lil* (list-in-list) is chosen. \"\"\" if self . pre != self . post : allow_self_connections = True if isinstance ( self . pre , PopulationView ) or isinstance ( self . post , PopulationView ): Global . _error ( 'DoG connector is only possible on whole populations, not PopulationViews.' ) self . connector_name = \"Difference-of-Gaussian\" self . connector_description = \"Difference-of-Gaussian, $A^+ %(Aplus)s , $\\sigma^+$ %(sigmaplus)s , $A^- %(Aminus)s , $\\sigma^-$ %(sigmaminus)s , delays %(delay)s \" % { 'Aplus' : str ( amp_pos ), 'sigmaplus' : str ( sigma_pos ), 'Aminus' : str ( amp_neg ), 'sigmaminus' : str ( sigma_neg ), 'delay' : _process_random ( delays )} # delays are possibly drawn from distribution, weights not self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None self . _store_connectivity ( dog , ( amp_pos , sigma_pos , amp_neg , sigma_neg , delays , limit , allow_self_connections , storage_format , \"post_to_pre\" ), delays , storage_format , \"post_to_pre\" ) return self connect_fixed_number_post ( self , number , weights = 1.0 , delays = 0.0 , allow_self_connections = False , force_multiple_weights = False , storage_format = 'lil' , storage_order = 'post_to_pre' ) # Builds a connection pattern between the two populations with a fixed number of post-synaptic neurons. Each neuron in the pre-synaptic population sends connections to a fixed number of neurons of the post-synaptic population chosen randomly. Parameters: Name Type Description Default number number of synapses per pre-synaptic neuron. required weights either a single value for all synapses or a RandomDistribution object. 1.0 delays either a single value for all synapses or a RandomDistribution object (default = dt) 0.0 allow_self_connections defines if self-connections are allowed (default=False) False force_multiple_weights if a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used. False Source code in ANNarchy/core/Projection.py def connect_fixed_number_post ( self , number , weights = 1.0 , delays = 0.0 , allow_self_connections = False , force_multiple_weights = False , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds a connection pattern between the two populations with a fixed number of post-synaptic neurons. Each neuron in the pre-synaptic population sends connections to a fixed number of neurons of the post-synaptic population chosen randomly. :param number: number of synapses per pre-synaptic neuron. :param weights: either a single value for all synapses or a RandomDistribution object. :param delays: either a single value for all synapses or a RandomDistribution object (default = dt) :param allow_self_connections: defines if self-connections are allowed (default=False) :param force_multiple_weights: if a single value is provided for ``weights`` and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting ``force_multiple_weights`` to True ensures that a value per synapse will be used. \"\"\" if self . pre != self . post : allow_self_connections = True if number > self . post . size : Global . _error ( 'connect_fixed_number_post: the number of post-synaptic neurons exceeds the size of the population.' ) self . connector_name = \"Random Divergent\" self . connector_description = \"Random Divergent 1 $ \\\\ rightarrow$ %(number)s , weights %(weight)s , delays %(delay)s \" % { 'weight' : _process_random ( weights ), 'delay' : _process_random ( delays ), 'number' : number } if isinstance ( weights , ( int , float )) and not force_multiple_weights : self . _single_constant_weight = True # if weights or delays are from random distribution I need to know this in code generator self . connector_weight_dist = weights if isinstance ( weights , RandomDistribution ) else None self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None self . _store_connectivity ( fixed_number_post , ( number , weights , delays , allow_self_connections , storage_format , storage_order ), delays , storage_format , storage_order ) return self connect_fixed_number_pre ( self , number , weights , delays = 0.0 , allow_self_connections = False , force_multiple_weights = False , storage_format = 'lil' , storage_order = 'post_to_pre' ) # Builds a connection pattern between the two populations with a fixed number of pre-synaptic neurons. Each neuron in the postsynaptic population receives connections from a fixed number of neurons of the presynaptic population chosen randomly. Parameters: Name Type Description Default number number of synapses per postsynaptic neuron. required weights either a single value for all synapses or a RandomDistribution object. required delays either a single value for all synapses or a RandomDistribution object (default = dt) 0.0 allow_self_connections defines if self-connections are allowed (default=False). False force_multiple_weights if a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used. False Source code in ANNarchy/core/Projection.py def connect_fixed_number_pre ( self , number , weights , delays = 0.0 , allow_self_connections = False , force_multiple_weights = False , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds a connection pattern between the two populations with a fixed number of pre-synaptic neurons. Each neuron in the postsynaptic population receives connections from a fixed number of neurons of the presynaptic population chosen randomly. :param number: number of synapses per postsynaptic neuron. :param weights: either a single value for all synapses or a RandomDistribution object. :param delays: either a single value for all synapses or a RandomDistribution object (default = dt) :param allow_self_connections: defines if self-connections are allowed (default=False). :param force_multiple_weights: if a single value is provided for ``weights`` and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting ``force_multiple_weights`` to True ensures that a value per synapse will be used. \"\"\" if self . pre != self . post : allow_self_connections = True if number > self . pre . size : Global . _error ( 'connect_fixed_number_pre: the number of pre-synaptic neurons exceeds the size of the population.' ) self . connector_name = \"Random Convergent\" self . connector_description = \"Random Convergent %(number)s $ \\\\ rightarrow$ 1, weights %(weight)s , delays %(delay)s \" % { 'weight' : _process_random ( weights ), 'delay' : _process_random ( delays ), 'number' : number } if isinstance ( weights , ( int , float )) and not force_multiple_weights : self . _single_constant_weight = True # if weights or delays are from random distribution I need to know this in code generator self . connector_weight_dist = weights if isinstance ( weights , RandomDistribution ) else None self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None self . _store_connectivity ( fixed_number_pre , ( number , weights , delays , allow_self_connections , storage_format , storage_order ), delays , storage_format , storage_order ) return self connect_fixed_probability ( self , probability , weights , delays = 0.0 , allow_self_connections = False , force_multiple_weights = False , storage_format = 'lil' , storage_order = 'post_to_pre' ) # Builds a probabilistic connection pattern between the two populations. Each neuron in the postsynaptic population is connected to neurons of the presynaptic population with the given probability. Self-connections are avoided by default. Parameters: Name Type Description Default probability probability that a synapse is created. required weights either a single value for all synapses or a RandomDistribution object. required delays either a single value for all synapses or a RandomDistribution object (default = dt) 0.0 allow_self_connections defines if self-connections are allowed (default=False). False force_multiple_weights if a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used. False storage_format for some of the default connection patterns ANNarchy provide different storage formats. For all-to-all we support list-of-list (\"lil\") or compressed sparse row (\"csr\"), by default lil is chosen. 'lil' Source code in ANNarchy/core/Projection.py def connect_fixed_probability ( self , probability , weights , delays = 0.0 , allow_self_connections = False , force_multiple_weights = False , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds a probabilistic connection pattern between the two populations. Each neuron in the postsynaptic population is connected to neurons of the presynaptic population with the given probability. Self-connections are avoided by default. :param probability: probability that a synapse is created. :param weights: either a single value for all synapses or a RandomDistribution object. :param delays: either a single value for all synapses or a RandomDistribution object (default = dt) :param allow_self_connections: defines if self-connections are allowed (default=False). :param force_multiple_weights: if a single value is provided for ``weights`` and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting ``force_multiple_weights`` to True ensures that a value per synapse will be used. :param storage_format: for some of the default connection patterns ANNarchy provide different storage formats. For all-to-all we support list-of-list (\"lil\") or compressed sparse row (\"csr\"), by default lil is chosen. \"\"\" if self . pre != self . post : allow_self_connections = True self . connector_name = \"Random\" self . connector_description = \"Random, sparseness %(proba)s , weights %(weight)s , delays %(delay)s \" % { 'weight' : _process_random ( weights ), 'delay' : _process_random ( delays ), 'proba' : probability } if isinstance ( weights , ( int , float )) and not force_multiple_weights : self . _single_constant_weight = True # if weights or delays are from random distribution I need to know this in code generator self . connector_weight_dist = weights if isinstance ( weights , RandomDistribution ) else None self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None self . _store_connectivity ( fixed_probability , ( probability , weights , delays , allow_self_connections , storage_format , storage_order ), delays , storage_format , storage_order ) return self connect_from_file ( self , filename ) # Builds the connectivity matrix using data saved using the Projection.save_connectivity() method (not save()!). Admissible file formats are compressed Numpy files (.npz), gunzipped binary text files (.gz) or binary text files. Parameters: Name Type Description Default filename file where the connections were saved. .. note:: Only the ranks, weights and delays are loaded, not the other variables. required Source code in ANNarchy/core/Projection.py def connect_from_file ( self , filename ): \"\"\" Builds the connectivity matrix using data saved using the Projection.save_connectivity() method (not save()!). Admissible file formats are compressed Numpy files (.npz), gunzipped binary text files (.gz) or binary text files. :param filename: file where the connections were saved. .. note:: Only the ranks, weights and delays are loaded, not the other variables. \"\"\" # Create an empty LIL object lil = LILConnectivity () # Load the data from ANNarchy.core.IO import _load_connectivity_data try : data = _load_connectivity_data ( filename ) except Exception as e : Global . _print ( e ) Global . _error ( 'connect_from_file(): Unable to load the data' , filename , 'into the projection.' ) # Load the LIL object try : # Size lil . size = data [ 'size' ] lil . nb_synapses = data [ 'nb_synapses' ] # Ranks lil . post_rank = list ( data [ 'post_ranks' ]) lil . pre_rank = list ( data [ 'pre_ranks' ]) # Weights if isinstance ( data [ 'w' ], ( int , float )): self . _single_constant_weight = True lil . w = [[ float ( data [ 'w' ])]] elif isinstance ( data [ 'w' ], ( numpy . ndarray ,)) and data [ 'w' ] . size == 1 : self . _single_constant_weight = True lil . w = [[ float ( data [ 'w' ])]] else : lil . w = data [ 'w' ] # Delays if data [ 'delay' ]: lil . delay = data [ 'delay' ] lil . max_delay = data [ 'max_delay' ] lil . uniform_delay = data [ 'uniform_delay' ] except Exception as e : Global . _print ( e ) Global . _error ( 'Unable to load the data' , filename , 'into the projection.' ) # Store the synapses self . connector_name = \"From File\" self . connector_description = \"From File\" self . _store_connectivity ( self . _load_from_lil , ( lil ,), lil . max_delay if lil . uniform_delay > 0 else lil . delay ) return self connect_from_matrix ( self , weights , delays = 0.0 , pre_post = False , storage_format = 'lil' , storage_order = 'post_to_pre' ) # Builds a connection pattern according to a dense connectivity matrix. The matrix must be N*M, where N is the number of neurons in the post-synaptic population and M in the pre-synaptic one. Lists of lists must have the same size. If a synapse should not be created, the weight value should be None. Parameters: Name Type Description Default weights a matrix or list of lists representing the weights. If a value is None, the synapse will not be created. required delays a matrix or list of lists representing the delays. Must represent the same synapses as weights. If the argument is omitted, delays are 0. 0.0 pre_post states which index is first. By default, the first dimension is related to the post-synaptic population. If pre_post is True, the first dimension is the pre-synaptic population. False Source code in ANNarchy/core/Projection.py def connect_from_matrix ( self , weights , delays = 0.0 , pre_post = False , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds a connection pattern according to a dense connectivity matrix. The matrix must be N*M, where N is the number of neurons in the post-synaptic population and M in the pre-synaptic one. Lists of lists must have the same size. If a synapse should not be created, the weight value should be None. :param weights: a matrix or list of lists representing the weights. If a value is None, the synapse will not be created. :param delays: a matrix or list of lists representing the delays. Must represent the same synapses as weights. If the argument is omitted, delays are 0. :param pre_post: states which index is first. By default, the first dimension is related to the post-synaptic population. If ``pre_post`` is True, the first dimension is the pre-synaptic population. \"\"\" # Store the synapses self . connector_name = \"Connectivity matrix\" self . connector_description = \"Connectivity matrix\" if isinstance ( weights , list ): try : weights = numpy . array ( weights ) except : Global . _error ( 'connect_from_matrix(): You must provide a dense 2D matrix.' ) self . _store_connectivity ( self . _load_from_matrix , ( weights , delays , pre_post ), delays , storage_format , storage_order ) return self connect_from_matrix_market ( self , filename , storage_format = 'lil' ) # Read in a weight matrix encoded in the Matrix Market format. This connector is intended for benchmarking purposes. TODO: check if the routine works for empty rows! Source code in ANNarchy/core/Projection.py def connect_from_matrix_market ( self , filename , storage_format = \"lil\" ): \"\"\" Read in a weight matrix encoded in the Matrix Market format. This connector is intended for benchmarking purposes. TODO: check if the routine works for empty rows! \"\"\" from scipy.io import mmread from scipy.sparse import coo_matrix import tarfile from ANNarchy.core.cython_ext import LILConnectivity if not filename . endswith ( \".mtx\" ): raise ValueError ( \"connect_from_matrix_market(): expected .mtx file.\" ) # read with SciPy tmp = mmread ( filename ) # scipy should return a coo_matrix in case of sparse matrices if isinstance ( tmp , coo_matrix ): # transform into LIL (in place) tmp = tmp . tolil ( copy = True ) # build up ANNarchy LIL synapses = LILConnectivity () row_idx = 0 for col_idx , val in zip ( tmp . rows , tmp . data ): synapses . push_back ( row_idx , col_idx , val , [ 0 ]) row_idx += 1 # not needed anymore del tmp else : raise ValueError ( \"Error on read-out of matrix market file.\" ) delays = 0 self . _store_connectivity ( self . _load_from_lil , ( synapses , ), delays , storage_format = storage_format ) self . connector_name = \"MatrixMarket\" self . connector_description = \"A weight matrix load from .mtx file\" return self connect_from_sparse ( self , weights , delays = 0.0 , storage_format = 'lil' , storage_order = 'post_to_pre' ) # Builds a connectivity pattern using a Scipy sparse matrix for the weights and (optionally) delays. Warning: a sparse matrix has pre-synaptic ranks as first dimension. Parameters: Name Type Description Default weights a sparse lil_matrix object created from scipy. required delays the value of the constant delay (default: dt). 0.0 Source code in ANNarchy/core/Projection.py def connect_from_sparse ( self , weights , delays = 0.0 , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds a connectivity pattern using a Scipy sparse matrix for the weights and (optionally) delays. Warning: a sparse matrix has pre-synaptic ranks as first dimension. :param weights: a sparse lil_matrix object created from scipy. :param delays: the value of the constant delay (default: dt). \"\"\" try : from scipy.sparse import lil_matrix , csr_matrix , csc_matrix except : Global . _error ( \"connect_from_sparse(): scipy is not installed, sparse matrices can not be loaded.\" ) if not isinstance ( weights , ( lil_matrix , csr_matrix , csc_matrix )): Global . _error ( \"connect_from_sparse(): only lil, csr and csc matrices are allowed for now.\" ) if not isinstance ( delays , ( int , float )): Global . _error ( \"connect_from_sparse(): only constant delays are allowed for sparse matrices.\" ) weights = csc_matrix ( weights ) # if weights[weights.nonzero()].max() == weights[weights.nonzero()].min() : # self._single_constant_weight = True # Store the synapses self . connector_name = \"Sparse connectivity matrix\" self . connector_description = \"Sparse connectivity matrix\" self . _store_connectivity ( self . _load_from_sparse , ( weights , delays ), delays , storage_format , storage_order ) return self connect_gaussian ( self , amp , sigma , delays = 0.0 , limit = 0.01 , allow_self_connections = False , storage_format = 'lil' ) # Builds a Gaussian connection pattern between the two populations. Each neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Gaussian profile. Parameters: Name Type Description Default amp amplitude of the Gaussian function required sigma width of the Gaussian function required delays synaptic delay, either a single value or a random distribution object (default=dt). 0.0 limit proportion of amp below which synapses are not created (default: 0.01) 0.01 allow_self_connections allows connections between a neuron and itself. False storage_format for some of the default connection patterns, ANNarchy provide different storage formats. By default lil (list-in-list) is chosen. 'lil' Source code in ANNarchy/core/Projection.py def connect_gaussian ( self , amp , sigma , delays = 0.0 , limit = 0.01 , allow_self_connections = False , storage_format = \"lil\" ): \"\"\" Builds a Gaussian connection pattern between the two populations. Each neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Gaussian profile. :param amp: amplitude of the Gaussian function :param sigma: width of the Gaussian function :param delays: synaptic delay, either a single value or a random distribution object (default=dt). :param limit: proportion of *amp* below which synapses are not created (default: 0.01) :param allow_self_connections: allows connections between a neuron and itself. :param storage_format: for some of the default connection patterns, ANNarchy provide different storage formats. By default *lil* (list-in-list) is chosen. \"\"\" if self . pre != self . post : allow_self_connections = True if isinstance ( self . pre , PopulationView ) or isinstance ( self . post , PopulationView ): Global . _error ( 'Gaussian connector is only possible on whole populations, not PopulationViews.' ) self . connector_name = \"Gaussian\" self . connector_description = \"Gaussian, $A$ %(A)s , $\\sigma$ %(sigma)s , delays %(delay)s \" % { 'A' : str ( amp ), 'sigma' : str ( sigma ), 'delay' : _process_random ( delays )} # weights are not drawn, delays possibly self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None self . _store_connectivity ( gaussian , ( amp , sigma , delays , limit , allow_self_connections , storage_format , \"post_to_pre\" ), delays , storage_format , \"post_to_pre\" ) return self connect_one_to_one ( self , weights = 1.0 , delays = 0.0 , force_multiple_weights = False , storage_format = 'lil' , storage_order = 'post_to_pre' ) # Builds a one-to-one connection pattern between the two populations. Parameters: Name Type Description Default weights initial synaptic values, either a single value (float) or a random distribution object. 1.0 delays synaptic delays, either a single value or a random distribution object (default=dt). 0.0 force_multiple_weights if a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used. False storage_format for some of the default connection patterns, ANNarchy provide different storage formats. For one-to-one we support list-of-list (\"lil\") or compressed sparse row (\"csr\"), by default lil is chosen. 'lil' storage_order for some of the available storage formats, ANNarchy provides different storage orderings. For one-to-one we support pre_to_post and post_to_pre , by default post_to_pre is chosen. 'post_to_pre' Source code in ANNarchy/core/Projection.py def connect_one_to_one ( self , weights = 1.0 , delays = 0.0 , force_multiple_weights = False , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds a one-to-one connection pattern between the two populations. :param weights: initial synaptic values, either a single value (float) or a random distribution object. :param delays: synaptic delays, either a single value or a random distribution object (default=dt). :param force_multiple_weights: if a single value is provided for ``weights`` and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting ``force_multiple_weights`` to True ensures that a value per synapse will be used. :param storage_format: for some of the default connection patterns, ANNarchy provide different storage formats. For one-to-one we support list-of-list (\"lil\") or compressed sparse row (\"csr\"), by default lil is chosen. :param storage_order: for some of the available storage formats, ANNarchy provides different storage orderings. For one-to-one we support *pre_to_post* and *post_to_pre*, by default *post_to_pre* is chosen. \"\"\" if self . pre . size != self . post . size : Global . _warning ( \"connect_one_to_one() between\" , self . pre . name , 'and' , self . post . name , 'with target' , self . target ) Global . _print ( \" \\t the two populations have different sizes, please check the connection pattern is what you expect.\" ) self . connector_name = \"One-to-One\" self . connector_description = \"One-to-One, weights %(weight)s , delays %(delay)s \" % { 'weight' : _process_random ( weights ), 'delay' : _process_random ( delays )} if isinstance ( weights , ( int , float )) and not force_multiple_weights : self . _single_constant_weight = True # if weights or delays are from random distribution I need to know this in code generator self . connector_weight_dist = weights if isinstance ( weights , RandomDistribution ) else None self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None self . _store_connectivity ( one_to_one , ( weights , delays , storage_format , storage_order ), delays , storage_format , storage_order ) return self connect_with_func ( self , method , storage_format = 'lil' , ** args ) # Builds a connection pattern based on a user-defined method. Parameters: Name Type Description Default method method to call. The method must return a CSR object. required args list of arguments needed by the function {} Source code in ANNarchy/core/Projection.py def connect_with_func ( self , method , storage_format = \"lil\" , ** args ): \"\"\" Builds a connection pattern based on a user-defined method. :param method: method to call. The method **must** return a CSR object. :param args: list of arguments needed by the function \"\"\" # Invoke the method directly, we need the delays already.... synapses = method ( self . pre , self . post , ** args ) synapses . validate () # Treat delays if synapses . uniform_delay != - 1 : # uniform delay d = synapses . max_delay * Global . config [ 'dt' ] else : d = Uniform ( 0. , synapses . max_delay * Global . config [ 'dt' ]) # Just to trick _store_connectivity(), the real delays are in the CSR self . _store_connectivity ( self . _load_from_lil , ( synapses , ), d , storage_format = storage_format ) self . connector_name = \"User-defined\" self . connector_description = \"Created by the method \" + method . __name__ return self connectivity_matrix ( self , fill = 0.0 ) # Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations. The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones. If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default. Parameters: Name Type Description Default fill value to put in the matrix when there is no connection (default: 0.0). 0.0 Source code in ANNarchy/core/Projection.py def connectivity_matrix ( self , fill = 0.0 ): \"\"\" Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations. The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones. If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default. :param fill: value to put in the matrix when there is no connection (default: 0.0). \"\"\" if not self . initialized : Global . _error ( 'The connectivity matrix can only be accessed after compilation' ) # get correct dimensions for dense matrix if isinstance ( self . pre , PopulationView ): size_pre = self . pre . population . size else : size_pre = self . pre . size if isinstance ( self . post , PopulationView ): size_post = self . post . population . size else : size_post = self . post . size # create empty dense matrix with default values res = np . ones (( size_post , size_pre )) * fill # fill row-by-row with real values for rank in self . post_ranks : idx = self . post_ranks . index ( rank ) preranks = self . cyInstance . pre_rank ( idx ) if \"w\" in self . synapse_type . description [ 'local' ] and ( not self . _has_single_weight ()): w = self . cyInstance . get_local_attribute_row ( \"w\" , idx , Global . config [ \"precision\" ]) elif \"w\" in self . synapse_type . description [ 'semiglobal' ]: w = self . cyInstance . get_semiglobal_attribute ( \"w\" , idx ) * np . ones ( self . cyInstance . dendrite_size ( idx ), Global . config [ \"precision\" ]) else : w = self . cyInstance . get_global_attribute ( \"w\" ) * np . ones ( self . cyInstance . dendrite_size ( idx ), Global . config [ \"precision\" ]) res [ rank , preranks ] = w return res dendrite ( self , post ) # Returns the dendrite of a postsynaptic neuron according to its rank. Parameters: Name Type Description Default post can be either the rank or the coordinates of the post-synaptic neuron. required Source code in ANNarchy/core/Projection.py def dendrite ( self , post ): \"\"\" Returns the dendrite of a postsynaptic neuron according to its rank. :param post: can be either the rank or the coordinates of the post-synaptic neuron. \"\"\" if not self . initialized : Global . _error ( 'dendrites can only be accessed after compilation.' ) if isinstance ( post , int ): rank = post else : rank = self . post . rank_from_coordinates ( post ) if rank in self . post_ranks : return Dendrite ( self , rank , self . post_ranks . index ( rank )) else : Global . _error ( \" The neuron of rank \" + str ( rank ) + \" has no dendrite in this projection.\" , exit = True ) disable_learning ( self , update = None ) # Disables learning for all synapses of this projection. The effect depends on the rate-coded or spiking nature of the projection: Rate-coded : the updating of all synaptic variables is disabled (including the weights w ). This is equivalent to proj.update = False . Spiking : the updating of the weights w is disabled, but all other variables are updated. This is equivalent to proj.plasticity = False . This method is useful when performing some tests on a trained network without messing with the learned weights. Source code in ANNarchy/core/Projection.py def disable_learning ( self , update = None ): \"\"\" Disables learning for all synapses of this projection. The effect depends on the rate-coded or spiking nature of the projection: * **Rate-coded**: the updating of all synaptic variables is disabled (including the weights ``w``). This is equivalent to ``proj.update = False``. * **Spiking**: the updating of the weights ``w`` is disabled, but all other variables are updated. This is equivalent to ``proj.plasticity = False``. This method is useful when performing some tests on a trained network without messing with the learned weights. \"\"\" try : if self . synapse_type . type == 'rate' : self . cyInstance . _set_update ( False ) else : self . cyInstance . _set_plasticity ( False ) except : Global . _warning ( 'disabling learning is only possible after compile().' ) enable_learning ( self , period = None , offset = None ) # Enables learning for all the synapses of this projection. For example, providing the following parameters at time 10 ms: enable_learning ( period = 10. , offset = 5. ) would call the updating methods at times 15, 25, 35, etc... The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of dt . Parameters: Name Type Description Default period determines how often the synaptic variables will be updated. None offset determines the offset at which the synaptic variables will be updated relative to the current time. None Source code in ANNarchy/core/Projection.py def enable_learning ( self , period = None , offset = None ): \"\"\" Enables learning for all the synapses of this projection. For example, providing the following parameters at time 10 ms: ```python enable_learning(period=10., offset=5.) ``` would call the updating methods at times 15, 25, 35, etc... The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of ``dt``. :param period: determines how often the synaptic variables will be updated. :param offset: determines the offset at which the synaptic variables will be updated relative to the current time. \"\"\" # Check arguments if not period is None and not offset is None : if offset >= period : Global . _error ( 'enable_learning(): the offset must be smaller than the period.' ) if period is None and not offset is None : Global . _error ( 'enable_learning(): if you define an offset, you have to define a period.' ) try : self . cyInstance . _set_update ( True ) self . cyInstance . _set_plasticity ( True ) if period != None : self . cyInstance . _set_update_period ( int ( period / Global . config [ 'dt' ])) else : self . cyInstance . _set_update_period ( int ( 1 )) period = Global . config [ 'dt' ] if offset != None : relative_offset = Global . get_time () % period + offset self . cyInstance . _set_update_offset ( int ( int ( relative_offset % period ) / Global . config [ 'dt' ])) else : self . cyInstance . _set_update_offset ( int ( 0 )) except : Global . _warning ( 'Enable_learning() is only possible after compile()' ) get ( self , name ) # Returns a list of parameters/variables values for each dendrite in the projection. The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it. Parameters: Name Type Description Default name the name of the parameter or variable required Source code in ANNarchy/core/Projection.py def get ( self , name ): \"\"\" Returns a list of parameters/variables values for each dendrite in the projection. The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it. :param name: the name of the parameter or variable \"\"\" return self . __getattr__ ( name ) load ( self , filename ) # Loads the saved state of the projection by Projection.save() . Warning: Matlab data can not be loaded. Example: proj . load ( 'proj1.npz' ) proj . load ( 'proj1.txt' ) proj . load ( 'proj1.txt.gz' ) Parameters: Name Type Description Default filename the file name with relative or absolute path. required Source code in ANNarchy/core/Projection.py def load ( self , filename ): \"\"\" Loads the saved state of the projection by `Projection.save()`. Warning: Matlab data can not be loaded. Example: ```python proj.load('proj1.npz') proj.load('proj1.txt') proj.load('proj1.txt.gz') ``` :param filename: the file name with relative or absolute path. \"\"\" from ANNarchy.core.IO import _load_connectivity_data self . _load_proj_data ( _load_connectivity_data ( filename )) nb_efferent_synapses ( self ) # Number of efferent connections. Intended only for spiking models. Source code in ANNarchy/core/Projection.py def nb_efferent_synapses ( self ): \"Number of efferent connections. Intended only for spiking models.\" if self . synapse_type . type == \"rate\" : Global . _error ( \"Projection.nb_efferent_synapses() is not available for rate-coded projections.\" ) return self . cyInstance . nb_efferent_synapses () nb_synapses_per_dendrite ( self ) # Total number of synapses for each dendrite as a list. Source code in ANNarchy/core/Projection.py def nb_synapses_per_dendrite ( self ): \"Total number of synapses for each dendrite as a list.\" if self . cyInstance is None : Global . _warning ( \"Access 'nb_synapses_per_dendrite' attribute of a Projection is only valid after compile()\" ) return [] return [ self . cyInstance . dendrite_size ( n ) for n in range ( self . size )] receptive_fields ( self , variable = 'w' , in_post_geometry = True ) # Gathers all receptive fields within this projection. Parameters: Name Type Description Default variable name of the variable 'w' in_post_geometry if False, the data will be plotted as square grid. (default = True) True Source code in ANNarchy/core/Projection.py def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"\"\" Gathers all receptive fields within this projection. :param variable: name of the variable :param in_post_geometry: if False, the data will be plotted as square grid. (default = True) \"\"\" if in_post_geometry : x_size = self . post . geometry [ 1 ] y_size = self . post . geometry [ 0 ] else : x_size = int ( math . floor ( math . sqrt ( self . post . size )) ) y_size = int ( math . ceil ( math . sqrt ( self . post . size )) ) def get_rf ( rank ): # TODO: IMPROVE res = np . zeros ( self . pre . size ) for n in range ( len ( self . post_ranks )): if self . post_ranks [ n ] == n : pre_ranks = self . cyInstance . pre_rank ( n ) data = self . cyInstance . get_local_attribute_row ( variable , rank , Global . config [ \"precision\" ]) for j in range ( len ( pre_ranks )): res [ pre_ranks [ j ]] = data [ j ] return res . reshape ( self . pre . geometry ) res = np . zeros (( 1 , x_size * self . pre . geometry [ 1 ])) for y in range ( y_size ): row = np . concatenate ( [ get_rf ( self . post . rank_from_coordinates ( ( y , x ) ) ) for x in range ( x_size ) ], axis = 1 ) res = np . concatenate (( res , row )) return res reset ( self , attributes =- 1 , synapses = False ) # Resets all parameters and variables of the projection to the value they had before the call to compile. Note: Only parameters and variables are reinitialized, not the connectivity structure (including the weights and delays). The parameter synapses will be used in a future release to also reinitialize the connectivity structure. Parameters: Name Type Description Default attributes list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. -1 Source code in ANNarchy/core/Projection.py def reset ( self , attributes =- 1 , synapses = False ): \"\"\" Resets all parameters and variables of the projection to the value they had before the call to compile. **Note:** Only parameters and variables are reinitialized, not the connectivity structure (including the weights and delays). The parameter ``synapses`` will be used in a future release to also reinitialize the connectivity structure. :param attributes: list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. \"\"\" if attributes == - 1 : attributes = self . attributes if synapses : # destroy the previous C++ content self . _clear () # call the init connectivity again self . _connect ( None ) self . initialized = True for var in attributes : # Skip w if var == 'w' : continue # check it exists if not var in self . attributes : Global . _warning ( \"Projection.reset():\" , var , \"is not an attribute of the population, won't reset.\" ) continue # Set the value try : self . __setattr__ ( var , self . init [ var ]) except Exception as e : Global . _print ( e ) Global . _warning ( \"Projection.reset(): something went wrong while resetting\" , var ) #Global._warning('Projection.reset(): only parameters and variables are reinitialized, not the connectivity structure (including the weights)...') save ( self , filename ) # Saves all information about the projection (connectivity, current value of parameters and variables) into a file. If the file name is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. Otherwise, the data will be pickled into a simple binary text file using pickle. Parameters: Name Type Description Default filename file name, may contain relative or absolute path. Warning: the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: python proj.save('proj1.npz') proj.save('proj1.txt') proj.save('proj1.txt.gz') proj.save('proj1.mat') required Source code in ANNarchy/core/Projection.py def save ( self , filename ): \"\"\" Saves all information about the projection (connectivity, current value of parameters and variables) into a file. * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. :param filename: file name, may contain relative or absolute path. **Warning:** the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: ```python proj.save('proj1.npz') proj.save('proj1.txt') proj.save('proj1.txt.gz') proj.save('proj1.mat') ``` \"\"\" from ANNarchy.core.IO import _save_data _save_data ( filename , self . _data ()) save_connectivity ( self , filename ) # Saves the connectivity of the projection into a file. Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables. The generated data can be used to create a projection in another network: proj . connect_from_file ( filename ) If the file name is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. Otherwise, the data will be pickled into a simple binary text file using pickle. Parameters: Name Type Description Default filename file name, may contain relative or absolute path. required Source code in ANNarchy/core/Projection.py def save_connectivity ( self , filename ): \"\"\" Saves the connectivity of the projection into a file. Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables. The generated data can be used to create a projection in another network: ```python proj.connect_from_file(filename) ``` * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. :param filename: file name, may contain relative or absolute path. \"\"\" # Check that the network is compiled if not self . initialized : Global . _error ( 'save_connectivity(): the network has not been compiled yet.' ) return # Check if the repertory exist ( path , fname ) = os . path . split ( filename ) if not path == '' : if not os . path . isdir ( path ): Global . _print ( 'Creating folder' , path ) os . mkdir ( path ) extension = os . path . splitext ( fname )[ 1 ] # Gathering the data data = { 'name' : self . name , 'post_ranks' : self . post_ranks , 'pre_ranks' : np . array ( self . cyInstance . pre_rank_all (), dtype = object ), 'w' : np . array ( self . w , dtype = object ), 'delay' : np . array ( self . cyInstance . get_delay (), dtype = object ) if hasattr ( self . cyInstance , 'get_delay' ) else None , 'max_delay' : self . max_delay , 'uniform_delay' : self . uniform_delay , 'size' : self . size , 'nb_synapses' : self . cyInstance . nb_synapses () } # Save the data try : import cPickle as pickle # Python2 except : import pickle # Python3 if extension == '.gz' : Global . _print ( \"Saving connectivity in gunzipped binary format...\" ) try : import gzip except : Global . _error ( 'gzip is not installed.' ) return with gzip . open ( filename , mode = 'wb' ) as w_file : try : pickle . dump ( data , w_file , protocol = pickle . HIGHEST_PROTOCOL ) except Exception as e : Global . _print ( 'Error while saving in gzipped binary format.' ) Global . _print ( e ) return elif extension == '.npz' : Global . _print ( \"Saving connectivity in Numpy format...\" ) np . savez_compressed ( filename , ** data ) elif extension == '.mat' : Global . _print ( \"Saving connectivity in Matlab format...\" ) if data [ 'delay' ] is None : data [ 'delay' ] = 0 try : import scipy.io as sio sio . savemat ( filename , data ) except Exception as e : Global . _error ( 'Error while saving in Matlab format.' ) Global . _print ( e ) return else : Global . _print ( \"Saving connectivity in text format...\" ) # save in Pythons pickle format with open ( filename , mode = 'wb' ) as w_file : try : pickle . dump ( data , w_file , protocol = pickle . HIGHEST_PROTOCOL ) except Exception as e : Global . _print ( 'Error while saving in text format.' ) Global . _print ( e ) return return set ( self , value ) # Sets the parameters/variables values for each dendrite in the projection. For parameters, you can provide: a single value, which will be the same for all dendrites. a list or 1D numpy array of the same length as the number of actual dendrites (self.size). For variables, you can provide: a single value, which will be the same for all synapses of all dendrites. a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value. Warning: it is not possible to set different values to each synapse using this method. One should iterate over the dendrites: for dendrite in proj . dendrites : dendrite . w = np . ones ( dendrite . size ) Parameters: Name Type Description Default value a dictionary with the name of the parameter/variable as key. required Source code in ANNarchy/core/Projection.py def set ( self , value ): \"\"\" Sets the parameters/variables values for each dendrite in the projection. For parameters, you can provide: * a single value, which will be the same for all dendrites. * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). For variables, you can provide: * a single value, which will be the same for all synapses of all dendrites. * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value. **Warning:** it is not possible to set different values to each synapse using this method. One should iterate over the dendrites: ```python for dendrite in proj.dendrites: dendrite.w = np.ones(dendrite.size) ``` :param value: a dictionary with the name of the parameter/variable as key. \"\"\" for name , val in value . items (): self . __setattr__ ( name , val ) size_in_bytes ( self ) # Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked. Source code in ANNarchy/core/Projection.py def size_in_bytes ( self ): \"\"\" Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked. \"\"\" if self . initialized : return self . cyInstance . size_in_bytes () else : return 0 start_creating ( self , period = None ) # Starts creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). Parameters: Name Type Description Default period how often creating should be evaluated (default: dt, i.e. each step) None Source code in ANNarchy/core/Projection.py def start_creating ( self , period = None ): \"\"\" Starts creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). :param period: how often creating should be evaluated (default: dt, i.e. each step) \"\"\" if not period : period = Global . config [ 'dt' ] if not self . cyInstance : Global . _error ( 'Can not start creating if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . start_creating ( int ( period / Global . config [ 'dt' ]), Global . get_current_step ()) except : Global . _error ( \"The synapse does not define a 'creating' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start creating connections.\" ) start_pruning ( self , period = None ) # Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). Parameters: Name Type Description Default period how often pruning should be evaluated (default: dt, i.e. each step) None Source code in ANNarchy/core/Projection.py def start_pruning ( self , period = None ): \"\"\" Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). :param period: how often pruning should be evaluated (default: dt, i.e. each step) \"\"\" if not period : period = Global . config [ 'dt' ] if not self . cyInstance : Global . _error ( 'Can not start pruning if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . start_pruning ( int ( period / Global . config [ 'dt' ]), Global . get_current_step ()) except : Global . _error ( \"The synapse does not define a 'pruning' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start pruning connections.\" ) stop_creating ( self ) # Stops creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). Source code in ANNarchy/core/Projection.py def stop_creating ( self ): \"\"\" Stops creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). \"\"\" if not self . cyInstance : Global . _error ( 'Can not stop creating if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . stop_creating () except : Global . _error ( \"The synapse does not define a 'creating' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start creating connections.\" ) stop_pruning ( self ) # Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). Source code in ANNarchy/core/Projection.py def stop_pruning ( self ): \"\"\" Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). \"\"\" if not self . cyInstance : Global . _error ( 'Can not stop pruning if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . stop_pruning () except : Global . _error ( \"The synapse does not define a 'pruning' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start pruning connections.\" ) synapse ( self , pre , post ) # Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise. Parameters: Name Type Description Default pre rank of the pre-synaptic neuron. required post rank of the post-synaptic neuron. required Source code in ANNarchy/core/Projection.py def synapse ( self , pre , post ): \"\"\" Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise. :param pre: rank of the pre-synaptic neuron. :param post: rank of the post-synaptic neuron. \"\"\" if not isinstance ( pre , int ) or not isinstance ( post , int ): Global . _error ( 'Projection.synapse() only accepts ranks for the pre and post neurons.' ) return self . dendrite ( post ) . synapse ( pre )","title":"Projection class"},{"location":"API/Projection/#projection-class","text":"The class ANNarchy.Projection defines projections at the population level. A projection is an ensemble of connections (or synapses) between a subset of a population (called the pre-synaptic population) and a subset of another population (the post-synaptic population), with a specific connection type. The pre- and post-synaptic populations may be the same.","title":"Projection class"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection","text":"Container for all the synapses of the same type between two populations.","title":"Projection"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.dendrites","text":"Iteratively returns the dendrites corresponding to this projection.","title":"dendrites"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.nb_synapses","text":"Total number of synapses in the projection.","title":"nb_synapses"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.size","text":"Number of post-synaptic neurons receiving synapses.","title":"size"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.__init__","text":"By default, the synapse only ensures linear synaptic transmission: For rate-coded populations: psp = w * pre.r For spiking populations: g_target += w to modify this behavior one need to provide a Synapse object. Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection. required synapse a Synapse instance. None name unique name of the projection (optional, it defaults to proj0 , proj1 , etc). None disable_omp especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to False . True Source code in ANNarchy/core/Projection.py def __init__ ( self , pre , post , target , synapse = None , name = None , disable_omp = True , copied = False ): \"\"\" By default, the synapse only ensures linear synaptic transmission: * For rate-coded populations: ``psp = w * pre.r`` * For spiking populations: ``g_target += w`` to modify this behavior one need to provide a Synapse object. :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection. :param synapse: a ``Synapse`` instance. :param name: unique name of the projection (optional, it defaults to ``proj0``, ``proj1``, etc). :param disable_omp: especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to `False`. \"\"\" # Check if the network has already been compiled if Global . _network [ 0 ][ 'compiled' ] and not copied : Global . _error ( 'you cannot add a projection after the network has been compiled.' ) # Store the pre and post synaptic populations # the user provide either a string or a population object # in case of string, we need to search for the corresponding object if isinstance ( pre , str ): for pop in Global . _network [ 0 ][ 'populations' ]: if pop . name == pre : self . pre = pop else : self . pre = pre if isinstance ( post , str ): for pop in Global . _network [ 0 ][ 'populations' ]: if pop . name == post : self . post = pop else : self . post = post # Store the arguments if isinstance ( target , list ) and len ( target ) == 1 : self . target = target [ 0 ] else : self . target = target # Add the target(s) to the postsynaptic population if isinstance ( self . target , list ): for _target in self . target : self . post . targets . append ( _target ) else : self . post . targets . append ( self . target ) # check if a synapse description is attached if not synapse : # No synapse attached assume default synapse based on # presynaptic population. if self . pre . neuron_type . type == 'rate' : from ANNarchy.models.Synapses import DefaultRateCodedSynapse self . synapse_type = DefaultRateCodedSynapse () self . synapse_type . type = 'rate' else : from ANNarchy.models.Synapses import DefaultSpikingSynapse self . synapse_type = DefaultSpikingSynapse () self . synapse_type . type = 'spike' elif inspect . isclass ( synapse ): self . synapse_type = synapse () self . synapse_type . type = self . pre . neuron_type . type else : self . synapse_type = copy . deepcopy ( synapse ) self . synapse_type . type = self . pre . neuron_type . type # Disable omp for spiking networks self . disable_omp = disable_omp # Analyse the parameters and variables self . synapse_type . _analyse () # Create a default name self . id = len ( Global . _network [ 0 ][ 'projections' ]) if name : self . name = name else : self . name = 'proj' + str ( self . id ) # Get a list of parameters and variables self . parameters = [] self . init = {} for param in self . synapse_type . description [ 'parameters' ]: self . parameters . append ( param [ 'name' ]) self . init [ param [ 'name' ]] = param [ 'init' ] self . variables = [] for var in self . synapse_type . description [ 'variables' ]: self . variables . append ( var [ 'name' ]) self . init [ var [ 'name' ]] = var [ 'init' ] self . attributes = self . parameters + self . variables # Get a list of user-defined functions self . functions = [ func [ 'name' ] for func in self . synapse_type . description [ 'functions' ]] # Add the population to the global network Global . _network [ 0 ][ 'projections' ] . append ( self ) # Finalize initialization self . initialized = False # Cython instance self . cyInstance = None # Connectivity self . _synapses = None self . _connection_method = None self . _connection_args = None self . _connection_delay = None self . _connector = None # Default configuration for connectivity self . _storage_format = \"lil\" self . _storage_order = \"post_to_pre\" # If a single weight value is used self . _single_constant_weight = False # If a dense matrix should be used instead of LIL self . _dense_matrix = False # Are random distribution used for weights/delays self . connector_weight_dist = None self . connector_delay_dist = None # Reporting self . connector_name = \"Specific\" self . connector_description = \"Specific\" # Overwritten by derived classes, to add # additional code self . _specific_template = {} # Set to False by derived classes to prevent saving of # data, e. g. in case of weight-sharing projections self . _saveable = True # To allow case-specific adjustment of parallelization # parameters, e. g. openMP schedule, we introduce a # dictionary read by the ProjectionGenerator. # # Will be overwritten either by inherited classes or # by an omp_config provided to the compile() method. self . _omp_config = { #'psp_schedule': 'schedule(dynamic)' } # If set to true, the code generator is not allowed to # split the matrix. This will be the case for many # SpecificProjections defined by the user or is disabled # globally. if self . synapse_type . type == \"rate\" : # Normally, the split should not be used for rate-coded models # but maybe there are cases where we want to enable it ... self . _no_split_matrix = Global . config [ \"disable_split_matrix\" ] # If the number of elements is too small, the split # might not be efficient. if self . post . size < Global . OMP_MIN_NB_NEURONS : self . _no_split_matrix = True else : # If the number of elements is too small, the split # might not be efficient. if self . post . size < Global . OMP_MIN_NB_NEURONS : self . _no_split_matrix = True else : self . _no_split_matrix = False","title":"__init__()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connect_all_to_all","text":"Builds an all-to-all connection pattern between the two populations. Parameters: Name Type Description Default weights synaptic values, either a single value or a random distribution object. required delays synaptic delays, either a single value or a random distribution object (default=dt). 0.0 allow_self_connections if True, self-connections between a neuron and itself are allowed (default = False if the pre- and post-populations are identical, True otherwise). False force_multiple_weights if a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used. False storage_format for some of the default connection patterns, ANNarchy provide different storage formats. For all-to-all we support list-of-list (\"lil\") or compressed sparse row (\"csr\"), by default lil is chosen. 'lil' storage_order for some of the available storage formats, ANNarchy provides different storage orderings. For all-to-all we support pre_to_post and post_to_pre, by default post_to_pre is chosen. Please note, the last two arguments should be changed carefully, as they can have large impact on the computational performance of ANNarchy. 'post_to_pre' Source code in ANNarchy/core/Projection.py def connect_all_to_all ( self , weights , delays = 0.0 , allow_self_connections = False , force_multiple_weights = False , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds an all-to-all connection pattern between the two populations. :param weights: synaptic values, either a single value or a random distribution object. :param delays: synaptic delays, either a single value or a random distribution object (default=dt). :param allow_self_connections: if True, self-connections between a neuron and itself are allowed (default = False if the pre- and post-populations are identical, True otherwise). :param force_multiple_weights: if a single value is provided for ``weights`` and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting ``force_multiple_weights`` to True ensures that a value per synapse will be used. :param storage_format: for some of the default connection patterns, ANNarchy provide different storage formats. For all-to-all we support list-of-list (\"lil\") or compressed sparse row (\"csr\"), by default lil is chosen. :param storage_order: for some of the available storage formats, ANNarchy provides different storage orderings. For all-to-all we support pre_to_post and post_to_pre, by default post_to_pre is chosen. Please note, the last two arguments should be changed carefully, as they can have large impact on the computational performance of ANNarchy. \"\"\" pre_pop = self . pre if not isinstance ( self . pre , PopulationView ) else self . pre . population post_pop = self . post if not isinstance ( self . post , PopulationView ) else self . post . population if pre_pop != post_pop : allow_self_connections = True self . connector_name = \"All-to-All\" self . connector_description = \"All-to-All, weights %(weight)s , delays %(delay)s \" % { 'weight' : _process_random ( weights ), 'delay' : _process_random ( delays )} # Does the projection define a single non-plastic weight? if isinstance ( weights , ( int , float )) and not force_multiple_weights : self . _single_constant_weight = True # Is it a dense connectivity matrix? if allow_self_connections and not isinstance ( self . pre , PopulationView ) and not isinstance ( self . post , PopulationView ): # TODO: for the moment disabled as it is not implemented # correctly (HD (15. Feb. 2019)) #self._dense_matrix = True self . _dense_matrix = False # if weights or delays are from random distribution I need to know this in code generator self . connector_weight_dist = weights if isinstance ( weights , RandomDistribution ) else None self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None # Store the connectivity self . _store_connectivity ( all_to_all , ( weights , delays , allow_self_connections , storage_format , storage_order ), delays , storage_format , storage_order ) return self","title":"connect_all_to_all()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connect_dog","text":"Builds a Difference-Of-Gaussians connection pattern between the two populations. Each neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Difference-Of-Gaussians profile. Parameters: Name Type Description Default amp_pos amplitude of the positive Gaussian function required sigma_pos width of the positive Gaussian function required amp_neg amplitude of the negative Gaussian function required sigma_neg width of the negative Gaussian function required delays synaptic delay, either a single value or a random distribution object (default=dt). 0.0 limit proportion of amp below which synapses are not created (default: 0.01) 0.01 allow_self_connections allows connections between a neuron and itself. False storage_format for some of the default connection patterns, ANNarchy provide different storage formats. By default lil (list-in-list) is chosen. 'lil' Source code in ANNarchy/core/Projection.py def connect_dog ( self , amp_pos , sigma_pos , amp_neg , sigma_neg , delays = 0.0 , limit = 0.01 , allow_self_connections = False , storage_format = \"lil\" ): \"\"\" Builds a Difference-Of-Gaussians connection pattern between the two populations. Each neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Difference-Of-Gaussians profile. :param amp_pos: amplitude of the positive Gaussian function :param sigma_pos: width of the positive Gaussian function :param amp_neg: amplitude of the negative Gaussian function :param sigma_neg: width of the negative Gaussian function :param delays: synaptic delay, either a single value or a random distribution object (default=dt). :param limit: proportion of *amp* below which synapses are not created (default: 0.01) :param allow_self_connections: allows connections between a neuron and itself. :param storage_format: for some of the default connection patterns, ANNarchy provide different storage formats. By default *lil* (list-in-list) is chosen. \"\"\" if self . pre != self . post : allow_self_connections = True if isinstance ( self . pre , PopulationView ) or isinstance ( self . post , PopulationView ): Global . _error ( 'DoG connector is only possible on whole populations, not PopulationViews.' ) self . connector_name = \"Difference-of-Gaussian\" self . connector_description = \"Difference-of-Gaussian, $A^+ %(Aplus)s , $\\sigma^+$ %(sigmaplus)s , $A^- %(Aminus)s , $\\sigma^-$ %(sigmaminus)s , delays %(delay)s \" % { 'Aplus' : str ( amp_pos ), 'sigmaplus' : str ( sigma_pos ), 'Aminus' : str ( amp_neg ), 'sigmaminus' : str ( sigma_neg ), 'delay' : _process_random ( delays )} # delays are possibly drawn from distribution, weights not self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None self . _store_connectivity ( dog , ( amp_pos , sigma_pos , amp_neg , sigma_neg , delays , limit , allow_self_connections , storage_format , \"post_to_pre\" ), delays , storage_format , \"post_to_pre\" ) return self","title":"connect_dog()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connect_fixed_number_post","text":"Builds a connection pattern between the two populations with a fixed number of post-synaptic neurons. Each neuron in the pre-synaptic population sends connections to a fixed number of neurons of the post-synaptic population chosen randomly. Parameters: Name Type Description Default number number of synapses per pre-synaptic neuron. required weights either a single value for all synapses or a RandomDistribution object. 1.0 delays either a single value for all synapses or a RandomDistribution object (default = dt) 0.0 allow_self_connections defines if self-connections are allowed (default=False) False force_multiple_weights if a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used. False Source code in ANNarchy/core/Projection.py def connect_fixed_number_post ( self , number , weights = 1.0 , delays = 0.0 , allow_self_connections = False , force_multiple_weights = False , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds a connection pattern between the two populations with a fixed number of post-synaptic neurons. Each neuron in the pre-synaptic population sends connections to a fixed number of neurons of the post-synaptic population chosen randomly. :param number: number of synapses per pre-synaptic neuron. :param weights: either a single value for all synapses or a RandomDistribution object. :param delays: either a single value for all synapses or a RandomDistribution object (default = dt) :param allow_self_connections: defines if self-connections are allowed (default=False) :param force_multiple_weights: if a single value is provided for ``weights`` and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting ``force_multiple_weights`` to True ensures that a value per synapse will be used. \"\"\" if self . pre != self . post : allow_self_connections = True if number > self . post . size : Global . _error ( 'connect_fixed_number_post: the number of post-synaptic neurons exceeds the size of the population.' ) self . connector_name = \"Random Divergent\" self . connector_description = \"Random Divergent 1 $ \\\\ rightarrow$ %(number)s , weights %(weight)s , delays %(delay)s \" % { 'weight' : _process_random ( weights ), 'delay' : _process_random ( delays ), 'number' : number } if isinstance ( weights , ( int , float )) and not force_multiple_weights : self . _single_constant_weight = True # if weights or delays are from random distribution I need to know this in code generator self . connector_weight_dist = weights if isinstance ( weights , RandomDistribution ) else None self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None self . _store_connectivity ( fixed_number_post , ( number , weights , delays , allow_self_connections , storage_format , storage_order ), delays , storage_format , storage_order ) return self","title":"connect_fixed_number_post()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connect_fixed_number_pre","text":"Builds a connection pattern between the two populations with a fixed number of pre-synaptic neurons. Each neuron in the postsynaptic population receives connections from a fixed number of neurons of the presynaptic population chosen randomly. Parameters: Name Type Description Default number number of synapses per postsynaptic neuron. required weights either a single value for all synapses or a RandomDistribution object. required delays either a single value for all synapses or a RandomDistribution object (default = dt) 0.0 allow_self_connections defines if self-connections are allowed (default=False). False force_multiple_weights if a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used. False Source code in ANNarchy/core/Projection.py def connect_fixed_number_pre ( self , number , weights , delays = 0.0 , allow_self_connections = False , force_multiple_weights = False , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds a connection pattern between the two populations with a fixed number of pre-synaptic neurons. Each neuron in the postsynaptic population receives connections from a fixed number of neurons of the presynaptic population chosen randomly. :param number: number of synapses per postsynaptic neuron. :param weights: either a single value for all synapses or a RandomDistribution object. :param delays: either a single value for all synapses or a RandomDistribution object (default = dt) :param allow_self_connections: defines if self-connections are allowed (default=False). :param force_multiple_weights: if a single value is provided for ``weights`` and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting ``force_multiple_weights`` to True ensures that a value per synapse will be used. \"\"\" if self . pre != self . post : allow_self_connections = True if number > self . pre . size : Global . _error ( 'connect_fixed_number_pre: the number of pre-synaptic neurons exceeds the size of the population.' ) self . connector_name = \"Random Convergent\" self . connector_description = \"Random Convergent %(number)s $ \\\\ rightarrow$ 1, weights %(weight)s , delays %(delay)s \" % { 'weight' : _process_random ( weights ), 'delay' : _process_random ( delays ), 'number' : number } if isinstance ( weights , ( int , float )) and not force_multiple_weights : self . _single_constant_weight = True # if weights or delays are from random distribution I need to know this in code generator self . connector_weight_dist = weights if isinstance ( weights , RandomDistribution ) else None self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None self . _store_connectivity ( fixed_number_pre , ( number , weights , delays , allow_self_connections , storage_format , storage_order ), delays , storage_format , storage_order ) return self","title":"connect_fixed_number_pre()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connect_fixed_probability","text":"Builds a probabilistic connection pattern between the two populations. Each neuron in the postsynaptic population is connected to neurons of the presynaptic population with the given probability. Self-connections are avoided by default. Parameters: Name Type Description Default probability probability that a synapse is created. required weights either a single value for all synapses or a RandomDistribution object. required delays either a single value for all synapses or a RandomDistribution object (default = dt) 0.0 allow_self_connections defines if self-connections are allowed (default=False). False force_multiple_weights if a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used. False storage_format for some of the default connection patterns ANNarchy provide different storage formats. For all-to-all we support list-of-list (\"lil\") or compressed sparse row (\"csr\"), by default lil is chosen. 'lil' Source code in ANNarchy/core/Projection.py def connect_fixed_probability ( self , probability , weights , delays = 0.0 , allow_self_connections = False , force_multiple_weights = False , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds a probabilistic connection pattern between the two populations. Each neuron in the postsynaptic population is connected to neurons of the presynaptic population with the given probability. Self-connections are avoided by default. :param probability: probability that a synapse is created. :param weights: either a single value for all synapses or a RandomDistribution object. :param delays: either a single value for all synapses or a RandomDistribution object (default = dt) :param allow_self_connections: defines if self-connections are allowed (default=False). :param force_multiple_weights: if a single value is provided for ``weights`` and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting ``force_multiple_weights`` to True ensures that a value per synapse will be used. :param storage_format: for some of the default connection patterns ANNarchy provide different storage formats. For all-to-all we support list-of-list (\"lil\") or compressed sparse row (\"csr\"), by default lil is chosen. \"\"\" if self . pre != self . post : allow_self_connections = True self . connector_name = \"Random\" self . connector_description = \"Random, sparseness %(proba)s , weights %(weight)s , delays %(delay)s \" % { 'weight' : _process_random ( weights ), 'delay' : _process_random ( delays ), 'proba' : probability } if isinstance ( weights , ( int , float )) and not force_multiple_weights : self . _single_constant_weight = True # if weights or delays are from random distribution I need to know this in code generator self . connector_weight_dist = weights if isinstance ( weights , RandomDistribution ) else None self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None self . _store_connectivity ( fixed_probability , ( probability , weights , delays , allow_self_connections , storage_format , storage_order ), delays , storage_format , storage_order ) return self","title":"connect_fixed_probability()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connect_from_file","text":"Builds the connectivity matrix using data saved using the Projection.save_connectivity() method (not save()!). Admissible file formats are compressed Numpy files (.npz), gunzipped binary text files (.gz) or binary text files. Parameters: Name Type Description Default filename file where the connections were saved. .. note:: Only the ranks, weights and delays are loaded, not the other variables. required Source code in ANNarchy/core/Projection.py def connect_from_file ( self , filename ): \"\"\" Builds the connectivity matrix using data saved using the Projection.save_connectivity() method (not save()!). Admissible file formats are compressed Numpy files (.npz), gunzipped binary text files (.gz) or binary text files. :param filename: file where the connections were saved. .. note:: Only the ranks, weights and delays are loaded, not the other variables. \"\"\" # Create an empty LIL object lil = LILConnectivity () # Load the data from ANNarchy.core.IO import _load_connectivity_data try : data = _load_connectivity_data ( filename ) except Exception as e : Global . _print ( e ) Global . _error ( 'connect_from_file(): Unable to load the data' , filename , 'into the projection.' ) # Load the LIL object try : # Size lil . size = data [ 'size' ] lil . nb_synapses = data [ 'nb_synapses' ] # Ranks lil . post_rank = list ( data [ 'post_ranks' ]) lil . pre_rank = list ( data [ 'pre_ranks' ]) # Weights if isinstance ( data [ 'w' ], ( int , float )): self . _single_constant_weight = True lil . w = [[ float ( data [ 'w' ])]] elif isinstance ( data [ 'w' ], ( numpy . ndarray ,)) and data [ 'w' ] . size == 1 : self . _single_constant_weight = True lil . w = [[ float ( data [ 'w' ])]] else : lil . w = data [ 'w' ] # Delays if data [ 'delay' ]: lil . delay = data [ 'delay' ] lil . max_delay = data [ 'max_delay' ] lil . uniform_delay = data [ 'uniform_delay' ] except Exception as e : Global . _print ( e ) Global . _error ( 'Unable to load the data' , filename , 'into the projection.' ) # Store the synapses self . connector_name = \"From File\" self . connector_description = \"From File\" self . _store_connectivity ( self . _load_from_lil , ( lil ,), lil . max_delay if lil . uniform_delay > 0 else lil . delay ) return self","title":"connect_from_file()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connect_from_matrix","text":"Builds a connection pattern according to a dense connectivity matrix. The matrix must be N*M, where N is the number of neurons in the post-synaptic population and M in the pre-synaptic one. Lists of lists must have the same size. If a synapse should not be created, the weight value should be None. Parameters: Name Type Description Default weights a matrix or list of lists representing the weights. If a value is None, the synapse will not be created. required delays a matrix or list of lists representing the delays. Must represent the same synapses as weights. If the argument is omitted, delays are 0. 0.0 pre_post states which index is first. By default, the first dimension is related to the post-synaptic population. If pre_post is True, the first dimension is the pre-synaptic population. False Source code in ANNarchy/core/Projection.py def connect_from_matrix ( self , weights , delays = 0.0 , pre_post = False , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds a connection pattern according to a dense connectivity matrix. The matrix must be N*M, where N is the number of neurons in the post-synaptic population and M in the pre-synaptic one. Lists of lists must have the same size. If a synapse should not be created, the weight value should be None. :param weights: a matrix or list of lists representing the weights. If a value is None, the synapse will not be created. :param delays: a matrix or list of lists representing the delays. Must represent the same synapses as weights. If the argument is omitted, delays are 0. :param pre_post: states which index is first. By default, the first dimension is related to the post-synaptic population. If ``pre_post`` is True, the first dimension is the pre-synaptic population. \"\"\" # Store the synapses self . connector_name = \"Connectivity matrix\" self . connector_description = \"Connectivity matrix\" if isinstance ( weights , list ): try : weights = numpy . array ( weights ) except : Global . _error ( 'connect_from_matrix(): You must provide a dense 2D matrix.' ) self . _store_connectivity ( self . _load_from_matrix , ( weights , delays , pre_post ), delays , storage_format , storage_order ) return self","title":"connect_from_matrix()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connect_from_matrix_market","text":"Read in a weight matrix encoded in the Matrix Market format. This connector is intended for benchmarking purposes. TODO: check if the routine works for empty rows! Source code in ANNarchy/core/Projection.py def connect_from_matrix_market ( self , filename , storage_format = \"lil\" ): \"\"\" Read in a weight matrix encoded in the Matrix Market format. This connector is intended for benchmarking purposes. TODO: check if the routine works for empty rows! \"\"\" from scipy.io import mmread from scipy.sparse import coo_matrix import tarfile from ANNarchy.core.cython_ext import LILConnectivity if not filename . endswith ( \".mtx\" ): raise ValueError ( \"connect_from_matrix_market(): expected .mtx file.\" ) # read with SciPy tmp = mmread ( filename ) # scipy should return a coo_matrix in case of sparse matrices if isinstance ( tmp , coo_matrix ): # transform into LIL (in place) tmp = tmp . tolil ( copy = True ) # build up ANNarchy LIL synapses = LILConnectivity () row_idx = 0 for col_idx , val in zip ( tmp . rows , tmp . data ): synapses . push_back ( row_idx , col_idx , val , [ 0 ]) row_idx += 1 # not needed anymore del tmp else : raise ValueError ( \"Error on read-out of matrix market file.\" ) delays = 0 self . _store_connectivity ( self . _load_from_lil , ( synapses , ), delays , storage_format = storage_format ) self . connector_name = \"MatrixMarket\" self . connector_description = \"A weight matrix load from .mtx file\" return self","title":"connect_from_matrix_market()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connect_from_sparse","text":"Builds a connectivity pattern using a Scipy sparse matrix for the weights and (optionally) delays. Warning: a sparse matrix has pre-synaptic ranks as first dimension. Parameters: Name Type Description Default weights a sparse lil_matrix object created from scipy. required delays the value of the constant delay (default: dt). 0.0 Source code in ANNarchy/core/Projection.py def connect_from_sparse ( self , weights , delays = 0.0 , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds a connectivity pattern using a Scipy sparse matrix for the weights and (optionally) delays. Warning: a sparse matrix has pre-synaptic ranks as first dimension. :param weights: a sparse lil_matrix object created from scipy. :param delays: the value of the constant delay (default: dt). \"\"\" try : from scipy.sparse import lil_matrix , csr_matrix , csc_matrix except : Global . _error ( \"connect_from_sparse(): scipy is not installed, sparse matrices can not be loaded.\" ) if not isinstance ( weights , ( lil_matrix , csr_matrix , csc_matrix )): Global . _error ( \"connect_from_sparse(): only lil, csr and csc matrices are allowed for now.\" ) if not isinstance ( delays , ( int , float )): Global . _error ( \"connect_from_sparse(): only constant delays are allowed for sparse matrices.\" ) weights = csc_matrix ( weights ) # if weights[weights.nonzero()].max() == weights[weights.nonzero()].min() : # self._single_constant_weight = True # Store the synapses self . connector_name = \"Sparse connectivity matrix\" self . connector_description = \"Sparse connectivity matrix\" self . _store_connectivity ( self . _load_from_sparse , ( weights , delays ), delays , storage_format , storage_order ) return self","title":"connect_from_sparse()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connect_gaussian","text":"Builds a Gaussian connection pattern between the two populations. Each neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Gaussian profile. Parameters: Name Type Description Default amp amplitude of the Gaussian function required sigma width of the Gaussian function required delays synaptic delay, either a single value or a random distribution object (default=dt). 0.0 limit proportion of amp below which synapses are not created (default: 0.01) 0.01 allow_self_connections allows connections between a neuron and itself. False storage_format for some of the default connection patterns, ANNarchy provide different storage formats. By default lil (list-in-list) is chosen. 'lil' Source code in ANNarchy/core/Projection.py def connect_gaussian ( self , amp , sigma , delays = 0.0 , limit = 0.01 , allow_self_connections = False , storage_format = \"lil\" ): \"\"\" Builds a Gaussian connection pattern between the two populations. Each neuron in the postsynaptic population is connected to a region of the presynaptic population centered around the neuron with the same normalized coordinates using a Gaussian profile. :param amp: amplitude of the Gaussian function :param sigma: width of the Gaussian function :param delays: synaptic delay, either a single value or a random distribution object (default=dt). :param limit: proportion of *amp* below which synapses are not created (default: 0.01) :param allow_self_connections: allows connections between a neuron and itself. :param storage_format: for some of the default connection patterns, ANNarchy provide different storage formats. By default *lil* (list-in-list) is chosen. \"\"\" if self . pre != self . post : allow_self_connections = True if isinstance ( self . pre , PopulationView ) or isinstance ( self . post , PopulationView ): Global . _error ( 'Gaussian connector is only possible on whole populations, not PopulationViews.' ) self . connector_name = \"Gaussian\" self . connector_description = \"Gaussian, $A$ %(A)s , $\\sigma$ %(sigma)s , delays %(delay)s \" % { 'A' : str ( amp ), 'sigma' : str ( sigma ), 'delay' : _process_random ( delays )} # weights are not drawn, delays possibly self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None self . _store_connectivity ( gaussian , ( amp , sigma , delays , limit , allow_self_connections , storage_format , \"post_to_pre\" ), delays , storage_format , \"post_to_pre\" ) return self","title":"connect_gaussian()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connect_one_to_one","text":"Builds a one-to-one connection pattern between the two populations. Parameters: Name Type Description Default weights initial synaptic values, either a single value (float) or a random distribution object. 1.0 delays synaptic delays, either a single value or a random distribution object (default=dt). 0.0 force_multiple_weights if a single value is provided for weights and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting force_multiple_weights to True ensures that a value per synapse will be used. False storage_format for some of the default connection patterns, ANNarchy provide different storage formats. For one-to-one we support list-of-list (\"lil\") or compressed sparse row (\"csr\"), by default lil is chosen. 'lil' storage_order for some of the available storage formats, ANNarchy provides different storage orderings. For one-to-one we support pre_to_post and post_to_pre , by default post_to_pre is chosen. 'post_to_pre' Source code in ANNarchy/core/Projection.py def connect_one_to_one ( self , weights = 1.0 , delays = 0.0 , force_multiple_weights = False , storage_format = \"lil\" , storage_order = \"post_to_pre\" ): \"\"\" Builds a one-to-one connection pattern between the two populations. :param weights: initial synaptic values, either a single value (float) or a random distribution object. :param delays: synaptic delays, either a single value or a random distribution object (default=dt). :param force_multiple_weights: if a single value is provided for ``weights`` and there is no learning, a single weight value will be used for the whole projection instead of one per synapse. Setting ``force_multiple_weights`` to True ensures that a value per synapse will be used. :param storage_format: for some of the default connection patterns, ANNarchy provide different storage formats. For one-to-one we support list-of-list (\"lil\") or compressed sparse row (\"csr\"), by default lil is chosen. :param storage_order: for some of the available storage formats, ANNarchy provides different storage orderings. For one-to-one we support *pre_to_post* and *post_to_pre*, by default *post_to_pre* is chosen. \"\"\" if self . pre . size != self . post . size : Global . _warning ( \"connect_one_to_one() between\" , self . pre . name , 'and' , self . post . name , 'with target' , self . target ) Global . _print ( \" \\t the two populations have different sizes, please check the connection pattern is what you expect.\" ) self . connector_name = \"One-to-One\" self . connector_description = \"One-to-One, weights %(weight)s , delays %(delay)s \" % { 'weight' : _process_random ( weights ), 'delay' : _process_random ( delays )} if isinstance ( weights , ( int , float )) and not force_multiple_weights : self . _single_constant_weight = True # if weights or delays are from random distribution I need to know this in code generator self . connector_weight_dist = weights if isinstance ( weights , RandomDistribution ) else None self . connector_delay_dist = delays if isinstance ( delays , RandomDistribution ) else None self . _store_connectivity ( one_to_one , ( weights , delays , storage_format , storage_order ), delays , storage_format , storage_order ) return self","title":"connect_one_to_one()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connect_with_func","text":"Builds a connection pattern based on a user-defined method. Parameters: Name Type Description Default method method to call. The method must return a CSR object. required args list of arguments needed by the function {} Source code in ANNarchy/core/Projection.py def connect_with_func ( self , method , storage_format = \"lil\" , ** args ): \"\"\" Builds a connection pattern based on a user-defined method. :param method: method to call. The method **must** return a CSR object. :param args: list of arguments needed by the function \"\"\" # Invoke the method directly, we need the delays already.... synapses = method ( self . pre , self . post , ** args ) synapses . validate () # Treat delays if synapses . uniform_delay != - 1 : # uniform delay d = synapses . max_delay * Global . config [ 'dt' ] else : d = Uniform ( 0. , synapses . max_delay * Global . config [ 'dt' ]) # Just to trick _store_connectivity(), the real delays are in the CSR self . _store_connectivity ( self . _load_from_lil , ( synapses , ), d , storage_format = storage_format ) self . connector_name = \"User-defined\" self . connector_description = \"Created by the method \" + method . __name__ return self","title":"connect_with_func()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.connectivity_matrix","text":"Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations. The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones. If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default. Parameters: Name Type Description Default fill value to put in the matrix when there is no connection (default: 0.0). 0.0 Source code in ANNarchy/core/Projection.py def connectivity_matrix ( self , fill = 0.0 ): \"\"\" Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations. The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones. If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default. :param fill: value to put in the matrix when there is no connection (default: 0.0). \"\"\" if not self . initialized : Global . _error ( 'The connectivity matrix can only be accessed after compilation' ) # get correct dimensions for dense matrix if isinstance ( self . pre , PopulationView ): size_pre = self . pre . population . size else : size_pre = self . pre . size if isinstance ( self . post , PopulationView ): size_post = self . post . population . size else : size_post = self . post . size # create empty dense matrix with default values res = np . ones (( size_post , size_pre )) * fill # fill row-by-row with real values for rank in self . post_ranks : idx = self . post_ranks . index ( rank ) preranks = self . cyInstance . pre_rank ( idx ) if \"w\" in self . synapse_type . description [ 'local' ] and ( not self . _has_single_weight ()): w = self . cyInstance . get_local_attribute_row ( \"w\" , idx , Global . config [ \"precision\" ]) elif \"w\" in self . synapse_type . description [ 'semiglobal' ]: w = self . cyInstance . get_semiglobal_attribute ( \"w\" , idx ) * np . ones ( self . cyInstance . dendrite_size ( idx ), Global . config [ \"precision\" ]) else : w = self . cyInstance . get_global_attribute ( \"w\" ) * np . ones ( self . cyInstance . dendrite_size ( idx ), Global . config [ \"precision\" ]) res [ rank , preranks ] = w return res","title":"connectivity_matrix()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.dendrite","text":"Returns the dendrite of a postsynaptic neuron according to its rank. Parameters: Name Type Description Default post can be either the rank or the coordinates of the post-synaptic neuron. required Source code in ANNarchy/core/Projection.py def dendrite ( self , post ): \"\"\" Returns the dendrite of a postsynaptic neuron according to its rank. :param post: can be either the rank or the coordinates of the post-synaptic neuron. \"\"\" if not self . initialized : Global . _error ( 'dendrites can only be accessed after compilation.' ) if isinstance ( post , int ): rank = post else : rank = self . post . rank_from_coordinates ( post ) if rank in self . post_ranks : return Dendrite ( self , rank , self . post_ranks . index ( rank )) else : Global . _error ( \" The neuron of rank \" + str ( rank ) + \" has no dendrite in this projection.\" , exit = True )","title":"dendrite()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.disable_learning","text":"Disables learning for all synapses of this projection. The effect depends on the rate-coded or spiking nature of the projection: Rate-coded : the updating of all synaptic variables is disabled (including the weights w ). This is equivalent to proj.update = False . Spiking : the updating of the weights w is disabled, but all other variables are updated. This is equivalent to proj.plasticity = False . This method is useful when performing some tests on a trained network without messing with the learned weights. Source code in ANNarchy/core/Projection.py def disable_learning ( self , update = None ): \"\"\" Disables learning for all synapses of this projection. The effect depends on the rate-coded or spiking nature of the projection: * **Rate-coded**: the updating of all synaptic variables is disabled (including the weights ``w``). This is equivalent to ``proj.update = False``. * **Spiking**: the updating of the weights ``w`` is disabled, but all other variables are updated. This is equivalent to ``proj.plasticity = False``. This method is useful when performing some tests on a trained network without messing with the learned weights. \"\"\" try : if self . synapse_type . type == 'rate' : self . cyInstance . _set_update ( False ) else : self . cyInstance . _set_plasticity ( False ) except : Global . _warning ( 'disabling learning is only possible after compile().' )","title":"disable_learning()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.enable_learning","text":"Enables learning for all the synapses of this projection. For example, providing the following parameters at time 10 ms: enable_learning ( period = 10. , offset = 5. ) would call the updating methods at times 15, 25, 35, etc... The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of dt . Parameters: Name Type Description Default period determines how often the synaptic variables will be updated. None offset determines the offset at which the synaptic variables will be updated relative to the current time. None Source code in ANNarchy/core/Projection.py def enable_learning ( self , period = None , offset = None ): \"\"\" Enables learning for all the synapses of this projection. For example, providing the following parameters at time 10 ms: ```python enable_learning(period=10., offset=5.) ``` would call the updating methods at times 15, 25, 35, etc... The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of ``dt``. :param period: determines how often the synaptic variables will be updated. :param offset: determines the offset at which the synaptic variables will be updated relative to the current time. \"\"\" # Check arguments if not period is None and not offset is None : if offset >= period : Global . _error ( 'enable_learning(): the offset must be smaller than the period.' ) if period is None and not offset is None : Global . _error ( 'enable_learning(): if you define an offset, you have to define a period.' ) try : self . cyInstance . _set_update ( True ) self . cyInstance . _set_plasticity ( True ) if period != None : self . cyInstance . _set_update_period ( int ( period / Global . config [ 'dt' ])) else : self . cyInstance . _set_update_period ( int ( 1 )) period = Global . config [ 'dt' ] if offset != None : relative_offset = Global . get_time () % period + offset self . cyInstance . _set_update_offset ( int ( int ( relative_offset % period ) / Global . config [ 'dt' ])) else : self . cyInstance . _set_update_offset ( int ( 0 )) except : Global . _warning ( 'Enable_learning() is only possible after compile()' )","title":"enable_learning()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.get","text":"Returns a list of parameters/variables values for each dendrite in the projection. The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it. Parameters: Name Type Description Default name the name of the parameter or variable required Source code in ANNarchy/core/Projection.py def get ( self , name ): \"\"\" Returns a list of parameters/variables values for each dendrite in the projection. The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it. :param name: the name of the parameter or variable \"\"\" return self . __getattr__ ( name )","title":"get()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.load","text":"Loads the saved state of the projection by Projection.save() . Warning: Matlab data can not be loaded. Example: proj . load ( 'proj1.npz' ) proj . load ( 'proj1.txt' ) proj . load ( 'proj1.txt.gz' ) Parameters: Name Type Description Default filename the file name with relative or absolute path. required Source code in ANNarchy/core/Projection.py def load ( self , filename ): \"\"\" Loads the saved state of the projection by `Projection.save()`. Warning: Matlab data can not be loaded. Example: ```python proj.load('proj1.npz') proj.load('proj1.txt') proj.load('proj1.txt.gz') ``` :param filename: the file name with relative or absolute path. \"\"\" from ANNarchy.core.IO import _load_connectivity_data self . _load_proj_data ( _load_connectivity_data ( filename ))","title":"load()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.nb_efferent_synapses","text":"Number of efferent connections. Intended only for spiking models. Source code in ANNarchy/core/Projection.py def nb_efferent_synapses ( self ): \"Number of efferent connections. Intended only for spiking models.\" if self . synapse_type . type == \"rate\" : Global . _error ( \"Projection.nb_efferent_synapses() is not available for rate-coded projections.\" ) return self . cyInstance . nb_efferent_synapses ()","title":"nb_efferent_synapses()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.nb_synapses_per_dendrite","text":"Total number of synapses for each dendrite as a list. Source code in ANNarchy/core/Projection.py def nb_synapses_per_dendrite ( self ): \"Total number of synapses for each dendrite as a list.\" if self . cyInstance is None : Global . _warning ( \"Access 'nb_synapses_per_dendrite' attribute of a Projection is only valid after compile()\" ) return [] return [ self . cyInstance . dendrite_size ( n ) for n in range ( self . size )]","title":"nb_synapses_per_dendrite()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.receptive_fields","text":"Gathers all receptive fields within this projection. Parameters: Name Type Description Default variable name of the variable 'w' in_post_geometry if False, the data will be plotted as square grid. (default = True) True Source code in ANNarchy/core/Projection.py def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"\"\" Gathers all receptive fields within this projection. :param variable: name of the variable :param in_post_geometry: if False, the data will be plotted as square grid. (default = True) \"\"\" if in_post_geometry : x_size = self . post . geometry [ 1 ] y_size = self . post . geometry [ 0 ] else : x_size = int ( math . floor ( math . sqrt ( self . post . size )) ) y_size = int ( math . ceil ( math . sqrt ( self . post . size )) ) def get_rf ( rank ): # TODO: IMPROVE res = np . zeros ( self . pre . size ) for n in range ( len ( self . post_ranks )): if self . post_ranks [ n ] == n : pre_ranks = self . cyInstance . pre_rank ( n ) data = self . cyInstance . get_local_attribute_row ( variable , rank , Global . config [ \"precision\" ]) for j in range ( len ( pre_ranks )): res [ pre_ranks [ j ]] = data [ j ] return res . reshape ( self . pre . geometry ) res = np . zeros (( 1 , x_size * self . pre . geometry [ 1 ])) for y in range ( y_size ): row = np . concatenate ( [ get_rf ( self . post . rank_from_coordinates ( ( y , x ) ) ) for x in range ( x_size ) ], axis = 1 ) res = np . concatenate (( res , row )) return res","title":"receptive_fields()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.reset","text":"Resets all parameters and variables of the projection to the value they had before the call to compile. Note: Only parameters and variables are reinitialized, not the connectivity structure (including the weights and delays). The parameter synapses will be used in a future release to also reinitialize the connectivity structure. Parameters: Name Type Description Default attributes list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. -1 Source code in ANNarchy/core/Projection.py def reset ( self , attributes =- 1 , synapses = False ): \"\"\" Resets all parameters and variables of the projection to the value they had before the call to compile. **Note:** Only parameters and variables are reinitialized, not the connectivity structure (including the weights and delays). The parameter ``synapses`` will be used in a future release to also reinitialize the connectivity structure. :param attributes: list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. \"\"\" if attributes == - 1 : attributes = self . attributes if synapses : # destroy the previous C++ content self . _clear () # call the init connectivity again self . _connect ( None ) self . initialized = True for var in attributes : # Skip w if var == 'w' : continue # check it exists if not var in self . attributes : Global . _warning ( \"Projection.reset():\" , var , \"is not an attribute of the population, won't reset.\" ) continue # Set the value try : self . __setattr__ ( var , self . init [ var ]) except Exception as e : Global . _print ( e ) Global . _warning ( \"Projection.reset(): something went wrong while resetting\" , var ) #Global._warning('Projection.reset(): only parameters and variables are reinitialized, not the connectivity structure (including the weights)...')","title":"reset()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.save","text":"Saves all information about the projection (connectivity, current value of parameters and variables) into a file. If the file name is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. Otherwise, the data will be pickled into a simple binary text file using pickle. Parameters: Name Type Description Default filename file name, may contain relative or absolute path. Warning: the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: python proj.save('proj1.npz') proj.save('proj1.txt') proj.save('proj1.txt.gz') proj.save('proj1.mat') required Source code in ANNarchy/core/Projection.py def save ( self , filename ): \"\"\" Saves all information about the projection (connectivity, current value of parameters and variables) into a file. * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. :param filename: file name, may contain relative or absolute path. **Warning:** the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: ```python proj.save('proj1.npz') proj.save('proj1.txt') proj.save('proj1.txt.gz') proj.save('proj1.mat') ``` \"\"\" from ANNarchy.core.IO import _save_data _save_data ( filename , self . _data ())","title":"save()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.save_connectivity","text":"Saves the connectivity of the projection into a file. Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables. The generated data can be used to create a projection in another network: proj . connect_from_file ( filename ) If the file name is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. Otherwise, the data will be pickled into a simple binary text file using pickle. Parameters: Name Type Description Default filename file name, may contain relative or absolute path. required Source code in ANNarchy/core/Projection.py def save_connectivity ( self , filename ): \"\"\" Saves the connectivity of the projection into a file. Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables. The generated data can be used to create a projection in another network: ```python proj.connect_from_file(filename) ``` * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. :param filename: file name, may contain relative or absolute path. \"\"\" # Check that the network is compiled if not self . initialized : Global . _error ( 'save_connectivity(): the network has not been compiled yet.' ) return # Check if the repertory exist ( path , fname ) = os . path . split ( filename ) if not path == '' : if not os . path . isdir ( path ): Global . _print ( 'Creating folder' , path ) os . mkdir ( path ) extension = os . path . splitext ( fname )[ 1 ] # Gathering the data data = { 'name' : self . name , 'post_ranks' : self . post_ranks , 'pre_ranks' : np . array ( self . cyInstance . pre_rank_all (), dtype = object ), 'w' : np . array ( self . w , dtype = object ), 'delay' : np . array ( self . cyInstance . get_delay (), dtype = object ) if hasattr ( self . cyInstance , 'get_delay' ) else None , 'max_delay' : self . max_delay , 'uniform_delay' : self . uniform_delay , 'size' : self . size , 'nb_synapses' : self . cyInstance . nb_synapses () } # Save the data try : import cPickle as pickle # Python2 except : import pickle # Python3 if extension == '.gz' : Global . _print ( \"Saving connectivity in gunzipped binary format...\" ) try : import gzip except : Global . _error ( 'gzip is not installed.' ) return with gzip . open ( filename , mode = 'wb' ) as w_file : try : pickle . dump ( data , w_file , protocol = pickle . HIGHEST_PROTOCOL ) except Exception as e : Global . _print ( 'Error while saving in gzipped binary format.' ) Global . _print ( e ) return elif extension == '.npz' : Global . _print ( \"Saving connectivity in Numpy format...\" ) np . savez_compressed ( filename , ** data ) elif extension == '.mat' : Global . _print ( \"Saving connectivity in Matlab format...\" ) if data [ 'delay' ] is None : data [ 'delay' ] = 0 try : import scipy.io as sio sio . savemat ( filename , data ) except Exception as e : Global . _error ( 'Error while saving in Matlab format.' ) Global . _print ( e ) return else : Global . _print ( \"Saving connectivity in text format...\" ) # save in Pythons pickle format with open ( filename , mode = 'wb' ) as w_file : try : pickle . dump ( data , w_file , protocol = pickle . HIGHEST_PROTOCOL ) except Exception as e : Global . _print ( 'Error while saving in text format.' ) Global . _print ( e ) return return","title":"save_connectivity()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.set","text":"Sets the parameters/variables values for each dendrite in the projection. For parameters, you can provide: a single value, which will be the same for all dendrites. a list or 1D numpy array of the same length as the number of actual dendrites (self.size). For variables, you can provide: a single value, which will be the same for all synapses of all dendrites. a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value. Warning: it is not possible to set different values to each synapse using this method. One should iterate over the dendrites: for dendrite in proj . dendrites : dendrite . w = np . ones ( dendrite . size ) Parameters: Name Type Description Default value a dictionary with the name of the parameter/variable as key. required Source code in ANNarchy/core/Projection.py def set ( self , value ): \"\"\" Sets the parameters/variables values for each dendrite in the projection. For parameters, you can provide: * a single value, which will be the same for all dendrites. * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). For variables, you can provide: * a single value, which will be the same for all synapses of all dendrites. * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value. **Warning:** it is not possible to set different values to each synapse using this method. One should iterate over the dendrites: ```python for dendrite in proj.dendrites: dendrite.w = np.ones(dendrite.size) ``` :param value: a dictionary with the name of the parameter/variable as key. \"\"\" for name , val in value . items (): self . __setattr__ ( name , val )","title":"set()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.size_in_bytes","text":"Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked. Source code in ANNarchy/core/Projection.py def size_in_bytes ( self ): \"\"\" Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked. \"\"\" if self . initialized : return self . cyInstance . size_in_bytes () else : return 0","title":"size_in_bytes()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.start_creating","text":"Starts creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). Parameters: Name Type Description Default period how often creating should be evaluated (default: dt, i.e. each step) None Source code in ANNarchy/core/Projection.py def start_creating ( self , period = None ): \"\"\" Starts creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). :param period: how often creating should be evaluated (default: dt, i.e. each step) \"\"\" if not period : period = Global . config [ 'dt' ] if not self . cyInstance : Global . _error ( 'Can not start creating if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . start_creating ( int ( period / Global . config [ 'dt' ]), Global . get_current_step ()) except : Global . _error ( \"The synapse does not define a 'creating' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start creating connections.\" )","title":"start_creating()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.start_pruning","text":"Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). Parameters: Name Type Description Default period how often pruning should be evaluated (default: dt, i.e. each step) None Source code in ANNarchy/core/Projection.py def start_pruning ( self , period = None ): \"\"\" Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). :param period: how often pruning should be evaluated (default: dt, i.e. each step) \"\"\" if not period : period = Global . config [ 'dt' ] if not self . cyInstance : Global . _error ( 'Can not start pruning if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . start_pruning ( int ( period / Global . config [ 'dt' ]), Global . get_current_step ()) except : Global . _error ( \"The synapse does not define a 'pruning' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start pruning connections.\" )","title":"start_pruning()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.stop_creating","text":"Stops creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). Source code in ANNarchy/core/Projection.py def stop_creating ( self ): \"\"\" Stops creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). \"\"\" if not self . cyInstance : Global . _error ( 'Can not stop creating if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . stop_creating () except : Global . _error ( \"The synapse does not define a 'creating' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start creating connections.\" )","title":"stop_creating()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.stop_pruning","text":"Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). Source code in ANNarchy/core/Projection.py def stop_pruning ( self ): \"\"\" Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). \"\"\" if not self . cyInstance : Global . _error ( 'Can not stop pruning if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . stop_pruning () except : Global . _error ( \"The synapse does not define a 'pruning' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start pruning connections.\" )","title":"stop_pruning()"},{"location":"API/Projection/#ANNarchy.core.Projection.Projection.synapse","text":"Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise. Parameters: Name Type Description Default pre rank of the pre-synaptic neuron. required post rank of the post-synaptic neuron. required Source code in ANNarchy/core/Projection.py def synapse ( self , pre , post ): \"\"\" Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise. :param pre: rank of the pre-synaptic neuron. :param post: rank of the post-synaptic neuron. \"\"\" if not isinstance ( pre , int ) or not isinstance ( post , int ): Global . _error ( 'Projection.synapse() only accepts ranks for the pre and post neurons.' ) return self . dendrite ( post ) . synapse ( pre )","title":"synapse()"},{"location":"API/RandomDistribution/","text":"Random Distributions # Introduction # Random number generators can be used at several places: while initializing parameters or variables, while creating connection patterns, when injecting noise into a neural or synaptic variable. ANNarchy provides several random distribution objects, implementing the following distributions: Uniform DiscreteUniform Normal LogNormal Gamma Exponential Warning DiscreteUniform, Gamma and Exponential distributions are not available if the CUDA paradigm is used. They can be used in the Python code, as a normal object: dist = Uniform ( - 1.0 , 1.0 ) values = dist . get_values ( 100 ) or inside mathematical expressions: tau * dv/dt + v = g_exc + Normal(0.0, 20.0) The Python objects rely on the numpy.random library, while the C++ values are based on the standard library of C++11. The seed of the underlying random number generator (Mersenne twister, mt19937 in C++11) can be set globally, by defining its value in setup() : setup(seed=187348768237) All random distribution objects (Python or C++) will use this seed. By default, the global seed is taken to be time(NULL) . The seed can also be set individually for each RandomDistribution object as a last argument: dist = Uniform(-1.0, 1.0, 36875937346) as well as in a mathematical expression: tau * dv/dt + v = g_exc + Normal(0.0, 20.0, 497536526) Implementation details # ANNarchy uses default implementations for random number generation: STL methods of C++11 for OpenMP and the device API of the curand library for CUDA. As engines we use mt19937 on openMP side and XORWOW on CUDA. The latter is subject to changes in future releases. It may be important to know that the drawing mechanisms differ between openMP and CUDA slightly: openMP: all distribution objects draw the numbers from one source in a single threaded way. CUDA: each distribution object has it own source, the random numbers are drawn in a parallel way. For further details on random numbers on GPUs please refer to the curand documentation: http://docs.nvidia.com/cuda/curand/device-api-overview.html#device-api-overview Uniform # Random distribution object using the uniform distribution between min and max . The returned values are floats in the range [min, max]. __init__ ( self , min , max ) special # Parameters: Name Type Description Default min minimum value. required max maximum value. required Source code in ANNarchy/core/Random.py def __init__ ( self , min , max ): \"\"\" :param min: minimum value. :param max: maximum value. \"\"\" self . min = min self . max = max get_values ( self , shape ) # Returns a Numpy array with the given shape. Source code in ANNarchy/core/Random.py def get_values ( self , shape ): \"\"\" Returns a Numpy array with the given shape. \"\"\" return np . random . uniform ( self . min , self . max , shape ) DiscreteUniform # Random distribution object using the discrete uniform distribution between min and max . The returned values are integers in the range [min, max]. __init__ ( self , min , max ) special # Parameters: Name Type Description Default min minimum value. required max maximum value. required Source code in ANNarchy/core/Random.py def __init__ ( self , min , max ): \"\"\" :param min: minimum value. :param max: maximum value. \"\"\" self . min = min self . max = max get_values ( self , shape ) # Returns a np.ndarray with the given shape Source code in ANNarchy/core/Random.py def get_values ( self , shape ): \"\"\" Returns a np.ndarray with the given shape \"\"\" return np . random . random_integers ( self . min , self . max , shape ) Normal # Random distribution instance returning a random value based on a normal (Gaussian) distribution. __init__ ( self , mu , sigma , min = None , max = None ) special # Parameters: Name Type Description Default mu mean of the distribution. required sigma standard deviation of the distribution. required min minimum value (default: unlimited). None max maximum value (default: unlimited). None Source code in ANNarchy/core/Random.py def __init__ ( self , mu , sigma , min = None , max = None ): \"\"\" :param mu: mean of the distribution. :param sigma: standard deviation of the distribution. :param min: minimum value (default: unlimited). :param max: maximum value (default: unlimited). \"\"\" self . mu = mu self . sigma = sigma self . min = min self . max = max get_values ( self , shape ) # Returns a np.ndarray with the given shape Source code in ANNarchy/core/Random.py def get_values ( self , shape ): \"\"\" Returns a np.ndarray with the given shape \"\"\" data = np . random . normal ( self . mu , self . sigma , shape ) if self . min != None : data [ data < self . min ] = self . min if self . max != None : data [ data > self . max ] = self . max return data LogNormal # Random distribution instance returning a random value based on lognormal distribution. __init__ ( self , mu , sigma , min = None , max = None ) special # Parameters: Name Type Description Default mu mean of the distribution. required sigma standard deviation of the distribution. required min minimum value (default: unlimited). None max maximum value (default: unlimited). None Source code in ANNarchy/core/Random.py def __init__ ( self , mu , sigma , min = None , max = None ): \"\"\" :param mu: mean of the distribution. :param sigma: standard deviation of the distribution. :param min: minimum value (default: unlimited). :param max: maximum value (default: unlimited). \"\"\" self . mu = mu self . sigma = sigma self . min = min self . max = max get_values ( self , shape ) # Returns a np.ndarray with the given shape Source code in ANNarchy/core/Random.py def get_values ( self , shape ): \"\"\" Returns a np.ndarray with the given shape \"\"\" data = np . random . lognormal ( self . mu , self . sigma , shape ) if self . min != None : data [ data < self . min ] = self . min if self . max != None : data [ data > self . max ] = self . max return data Gamma # Random distribution instance returning a random value based on gamma distribution. __init__ ( self , alpha , beta = 1.0 , seed =- 1 , min = None , max = None ) special # Parameters: Name Type Description Default alpha shape of the gamma distribution required beta scale of the gamma distribution 1.0 min minimum value returned (default: unlimited). None max maximum value returned (default: unlimited). None Source code in ANNarchy/core/Random.py def __init__ ( self , alpha , beta = 1.0 , seed =- 1 , min = None , max = None ): \"\"\" :param alpha: shape of the gamma distribution :param beta: scale of the gamma distribution :param min: minimum value returned (default: unlimited). :param max: maximum value returned (default: unlimited). \"\"\" self . alpha = alpha self . beta = beta self . min = min self . max = max get_values ( self , shape ) # Returns a np.ndarray with the given shape Source code in ANNarchy/core/Random.py def get_values ( self , shape ): \"\"\" Returns a np.ndarray with the given shape \"\"\" data = np . random . gamma ( self . alpha , self . beta , shape ) if self . min != None : data [ data < self . min ] = self . min if self . max != None : data [ data > self . max ] = self . max return data Exponential # Random distribution instance returning a random value based on exponential distribution, according the density function: \\[P(x | \\lambda) = \\lambda e^{(-\\lambda x )}\\] __init__ ( self , Lambda , min = None , max = None ) special # Note: Lambda is capitalized, otherwise it would be a reserved Python keyword. Parameters: Name Type Description Default Lambda rate parameter. required min minimum value (default: unlimited). None max maximum value (default: unlimited). None Source code in ANNarchy/core/Random.py def __init__ ( self , Lambda , min = None , max = None ): \"\"\" **Note:** ``Lambda`` is capitalized, otherwise it would be a reserved Python keyword. :param Lambda: rate parameter. :param min: minimum value (default: unlimited). :param max: maximum value (default: unlimited). \"\"\" self . Lambda = Lambda self . min = min self . max = max get_values ( self , shape ) # Returns a np.ndarray with the given shape. Source code in ANNarchy/core/Random.py def get_values ( self , shape ): \"\"\" Returns a np.ndarray with the given shape. \"\"\" data = np . random . exponential ( self . Lambda , shape ) if self . min != None : data [ data < self . min ] = self . min if self . max != None : data [ data > self . max ] = self . max return data","title":"Random Distributions"},{"location":"API/RandomDistribution/#random-distributions","text":"","title":"Random Distributions"},{"location":"API/RandomDistribution/#introduction","text":"Random number generators can be used at several places: while initializing parameters or variables, while creating connection patterns, when injecting noise into a neural or synaptic variable. ANNarchy provides several random distribution objects, implementing the following distributions: Uniform DiscreteUniform Normal LogNormal Gamma Exponential Warning DiscreteUniform, Gamma and Exponential distributions are not available if the CUDA paradigm is used. They can be used in the Python code, as a normal object: dist = Uniform ( - 1.0 , 1.0 ) values = dist . get_values ( 100 ) or inside mathematical expressions: tau * dv/dt + v = g_exc + Normal(0.0, 20.0) The Python objects rely on the numpy.random library, while the C++ values are based on the standard library of C++11. The seed of the underlying random number generator (Mersenne twister, mt19937 in C++11) can be set globally, by defining its value in setup() : setup(seed=187348768237) All random distribution objects (Python or C++) will use this seed. By default, the global seed is taken to be time(NULL) . The seed can also be set individually for each RandomDistribution object as a last argument: dist = Uniform(-1.0, 1.0, 36875937346) as well as in a mathematical expression: tau * dv/dt + v = g_exc + Normal(0.0, 20.0, 497536526)","title":"Introduction"},{"location":"API/RandomDistribution/#implementation-details","text":"ANNarchy uses default implementations for random number generation: STL methods of C++11 for OpenMP and the device API of the curand library for CUDA. As engines we use mt19937 on openMP side and XORWOW on CUDA. The latter is subject to changes in future releases. It may be important to know that the drawing mechanisms differ between openMP and CUDA slightly: openMP: all distribution objects draw the numbers from one source in a single threaded way. CUDA: each distribution object has it own source, the random numbers are drawn in a parallel way. For further details on random numbers on GPUs please refer to the curand documentation: http://docs.nvidia.com/cuda/curand/device-api-overview.html#device-api-overview","title":"Implementation details"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.Uniform","text":"Random distribution object using the uniform distribution between min and max . The returned values are floats in the range [min, max].","title":"Uniform"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.Uniform.__init__","text":"Parameters: Name Type Description Default min minimum value. required max maximum value. required Source code in ANNarchy/core/Random.py def __init__ ( self , min , max ): \"\"\" :param min: minimum value. :param max: maximum value. \"\"\" self . min = min self . max = max","title":"__init__()"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.Uniform.get_values","text":"Returns a Numpy array with the given shape. Source code in ANNarchy/core/Random.py def get_values ( self , shape ): \"\"\" Returns a Numpy array with the given shape. \"\"\" return np . random . uniform ( self . min , self . max , shape )","title":"get_values()"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.DiscreteUniform","text":"Random distribution object using the discrete uniform distribution between min and max . The returned values are integers in the range [min, max].","title":"DiscreteUniform"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.DiscreteUniform.__init__","text":"Parameters: Name Type Description Default min minimum value. required max maximum value. required Source code in ANNarchy/core/Random.py def __init__ ( self , min , max ): \"\"\" :param min: minimum value. :param max: maximum value. \"\"\" self . min = min self . max = max","title":"__init__()"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.DiscreteUniform.get_values","text":"Returns a np.ndarray with the given shape Source code in ANNarchy/core/Random.py def get_values ( self , shape ): \"\"\" Returns a np.ndarray with the given shape \"\"\" return np . random . random_integers ( self . min , self . max , shape )","title":"get_values()"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.Normal","text":"Random distribution instance returning a random value based on a normal (Gaussian) distribution.","title":"Normal"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.Normal.__init__","text":"Parameters: Name Type Description Default mu mean of the distribution. required sigma standard deviation of the distribution. required min minimum value (default: unlimited). None max maximum value (default: unlimited). None Source code in ANNarchy/core/Random.py def __init__ ( self , mu , sigma , min = None , max = None ): \"\"\" :param mu: mean of the distribution. :param sigma: standard deviation of the distribution. :param min: minimum value (default: unlimited). :param max: maximum value (default: unlimited). \"\"\" self . mu = mu self . sigma = sigma self . min = min self . max = max","title":"__init__()"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.Normal.get_values","text":"Returns a np.ndarray with the given shape Source code in ANNarchy/core/Random.py def get_values ( self , shape ): \"\"\" Returns a np.ndarray with the given shape \"\"\" data = np . random . normal ( self . mu , self . sigma , shape ) if self . min != None : data [ data < self . min ] = self . min if self . max != None : data [ data > self . max ] = self . max return data","title":"get_values()"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.LogNormal","text":"Random distribution instance returning a random value based on lognormal distribution.","title":"LogNormal"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.LogNormal.__init__","text":"Parameters: Name Type Description Default mu mean of the distribution. required sigma standard deviation of the distribution. required min minimum value (default: unlimited). None max maximum value (default: unlimited). None Source code in ANNarchy/core/Random.py def __init__ ( self , mu , sigma , min = None , max = None ): \"\"\" :param mu: mean of the distribution. :param sigma: standard deviation of the distribution. :param min: minimum value (default: unlimited). :param max: maximum value (default: unlimited). \"\"\" self . mu = mu self . sigma = sigma self . min = min self . max = max","title":"__init__()"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.LogNormal.get_values","text":"Returns a np.ndarray with the given shape Source code in ANNarchy/core/Random.py def get_values ( self , shape ): \"\"\" Returns a np.ndarray with the given shape \"\"\" data = np . random . lognormal ( self . mu , self . sigma , shape ) if self . min != None : data [ data < self . min ] = self . min if self . max != None : data [ data > self . max ] = self . max return data","title":"get_values()"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.Gamma","text":"Random distribution instance returning a random value based on gamma distribution.","title":"Gamma"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.Gamma.__init__","text":"Parameters: Name Type Description Default alpha shape of the gamma distribution required beta scale of the gamma distribution 1.0 min minimum value returned (default: unlimited). None max maximum value returned (default: unlimited). None Source code in ANNarchy/core/Random.py def __init__ ( self , alpha , beta = 1.0 , seed =- 1 , min = None , max = None ): \"\"\" :param alpha: shape of the gamma distribution :param beta: scale of the gamma distribution :param min: minimum value returned (default: unlimited). :param max: maximum value returned (default: unlimited). \"\"\" self . alpha = alpha self . beta = beta self . min = min self . max = max","title":"__init__()"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.Gamma.get_values","text":"Returns a np.ndarray with the given shape Source code in ANNarchy/core/Random.py def get_values ( self , shape ): \"\"\" Returns a np.ndarray with the given shape \"\"\" data = np . random . gamma ( self . alpha , self . beta , shape ) if self . min != None : data [ data < self . min ] = self . min if self . max != None : data [ data > self . max ] = self . max return data","title":"get_values()"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.Exponential","text":"Random distribution instance returning a random value based on exponential distribution, according the density function: \\[P(x | \\lambda) = \\lambda e^{(-\\lambda x )}\\]","title":"Exponential"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.Exponential.__init__","text":"Note: Lambda is capitalized, otherwise it would be a reserved Python keyword. Parameters: Name Type Description Default Lambda rate parameter. required min minimum value (default: unlimited). None max maximum value (default: unlimited). None Source code in ANNarchy/core/Random.py def __init__ ( self , Lambda , min = None , max = None ): \"\"\" **Note:** ``Lambda`` is capitalized, otherwise it would be a reserved Python keyword. :param Lambda: rate parameter. :param min: minimum value (default: unlimited). :param max: maximum value (default: unlimited). \"\"\" self . Lambda = Lambda self . min = min self . max = max","title":"__init__()"},{"location":"API/RandomDistribution/#ANNarchy.core.Random.Exponential.get_values","text":"Returns a np.ndarray with the given shape. Source code in ANNarchy/core/Random.py def get_values ( self , shape ): \"\"\" Returns a np.ndarray with the given shape. \"\"\" data = np . random . exponential ( self . Lambda , shape ) if self . min != None : data [ data < self . min ] = self . min if self . max != None : data [ data > self . max ] = self . max return data","title":"get_values()"},{"location":"API/SpecificNeuron/","text":"Built-in neuron types # ANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN ( http://neuralensemble.org/docs/PyNN/reference/neuronmodels.html ). LeakyIntegrator # Leaky-integrator rate-coded neuron, optionally noisy. This simple rate-coded neuron defines an internal variable \\(v(t)\\) which integrates the inputs \\(I(t)\\) with a time constant \\(\\tau\\) and a baseline \\(B\\) . An additive noise \\(N(t)\\) can be optionally defined: \\[\\tau \\cdot \\frac{dv(t)}{dt} + v(t) = I(t) + B + N(t)\\] The transfer function is the positive (or rectified linear ReLU) function with a threshold \\(T\\) : \\[r(t) = (v(t) - T)^+\\] By default, the input \\(I(t)\\) to this neuron is \"sum(exc) - sum(inh)\", but this can be changed by setting the sum argument: neuron = LeakyIntegrator ( sum = \"sum('exc')\" ) By default, there is no additive noise, but the noise argument can be passed with a specific distribution: neuron = LeakyIntegrator ( noise = \"Normal(0.0, 1.0)\" ) Parameters: tau = 10.0 : Time constant in ms of the neuron. B = 0.0 : Baseline value for v. T = 0.0 : Threshold for the positive transfer function. Variables: v : internal variable (init = 0.0): tau * dv/dt + v = sum(exc) - sum(inh) + B + N r : firing rate (init = 0.0): r = pos(v - T) The ODE is solved using the exponential Euler method. Equivalent code: LeakyIntegrator = Neuron ( parameters = ''' tau = 10.0 : population B = 0.0 T = 0.0 : population ''' , equations = ''' tau * dv/dt + v = sum(exc) - sum(inh) + B : exponential r = pos(v - T) ''' ) Izhikevich # Izhikevich neuron as proposed in: Izhikevich, E.M. (2003). Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks , 14:6. http://dx.doi.org/10.1109/TNN.2003.820440 The equations are: \\[\\frac{dv}{dt} = 0.04 * v^2 + 5.0 * v + 140.0 - u + I\\] \\[\\frac{du}{dt} = a * (b * v - u)\\] By default, the conductance is \"g_exc - g_inh\", but this can be changed by setting the conductance argument: neuron = Izhikevich ( conductance = 'g_ampa * (1 + g_nmda) - g_gaba' ) The synapses are instantaneous, i.e the corresponding conductance is increased from the synaptic efficiency w at the time step when a spike is received. Parameters: a = 0.02 : Speed of the recovery variable b = 0.2: Scaling of the recovery variable c = -65.0 : Reset potential. d = 8.0 : Increment of the recovery variable after a spike. v_thresh = 30.0 : Spike threshold (mV). i_offset = 0.0 : external current (nA). noise = 0.0 : Amplitude of the normal additive noise. tau_refrac = 0.0 : Duration of refractory period (ms). Variables: I : input current (user-defined conductance/current + external current + normal noise): I = conductance + i_offset + noise * Normal(0.0, 1.0) v : membrane potential in mV (init = c): dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I u : recovery variable (init= b * c): du/dt = a * (b * v - u) Spike emission: v > v_thresh Reset: v = c u += d The ODEs are solved using the explicit Euler method. Equivalent code: Izhikevich = Neuron ( parameters = \"\"\" noise = 0.0 a = 0.02 b = 0.2 c = -65.0 d = 8.0 v_thresh = 30.0 i_offset = 0.0 \"\"\" , equations = \"\"\" I = g_exc - g_inh + noise * Normal(0.0, 1.0) + i_offset dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init = -65.0 du/dt = a * (b*v - u) : init= -13.0 \"\"\" , spike = \"v > v_thresh\" , reset = \"v = c; u += d\" , refractory = 0.0 ) The default parameters are for a regular spiking (RS) neuron derived from the above mentioned article. IF_curr_exp # IF_curr_exp neuron. Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current. (Separate synaptic currents for excitatory and inhibitory synapses). Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_curr_exp = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 ) IF_cond_exp # IF_cond_exp neuron. Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance. Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -70.0 : Reversal potential for inhibitory input (mv) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_cond_exp = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -70.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 ) IF_curr_alpha # IF_curr_alpha neuron. Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents. (Separate synaptic currents for excitatory and inhibitory synapses). The alpha currents are calculated through a system of two linears ODEs. After a spike is received at t_spike, it peaks at t_spike + tau_syn_X, with a maximum equal to the synaptic efficiency. Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc alpha_exc : alpha function of excitatory current (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh alpha_inh : alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_curr_alpha = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 ) IF_cond_alpha # IF_cond_exp neuron. Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance. Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -70.0 : Reversal potential for inhibitory input (mv) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset g_exc : excitatory conductance (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc alpha_exc : alpha function of excitatory conductance (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc g_inh : inhibitory conductance (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh alpha_inh : alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_cond_alpha = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -70.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 ) HH_cond_exp # HH_cond_exp neuron. Single-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub. Parameters: gbar_Na = 20.0 : Maximal conductance of the Sodium current. gbar_K = 6.0 : Maximal conductance of the Potassium current. gleak = 0.01 : Conductance of the leak current (nF) cm = 0.2 : Capacity of the membrane (nF) v_offset = -63.0 : Threshold for the rate constants (mV) e_rev_Na = 50.0 : Reversal potential for the Sodium current (mV) e_rev_K = -90.0 : Reversal potential for the Potassium current (mV) e_rev_leak = -65.0 : Reversal potential for the leak current (mV) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -80.0 : Reversal potential for inhibitory input (mV) tau_syn_E = 0.2 : Decay time of excitatory synaptic current (ms) tau_syn_I = 2.0 : Decay time of inhibitory synaptic current (ms) i_offset = 0.0 : Offset current (nA) v_thresh = 0.0 : Threshold for spike emission Variables: Voltage-dependent rate constants an, bn, am, bm, ah, bh: an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) Activation variables n, m, h (h is initialized to 1.0, n and m to 0.0): dn/dt = an * (1.0 - n) - bn * n dm/dt = am * (1.0 - m) - bm * m dh/dt = ah * (1.0 - h) - bh * h v : membrane potential in mV (init=-65.0): cm * dv/dt = gleak (e_rev_leak -v) + gbar_K * n 4 * (e_rev_K - v) + gbar_Na * m *3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset g_exc : excitatory conductance (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory conductance (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission (the spike is emitted only once when v crosses the threshold from below): v > v_thresh and v(t-1) < v_thresh The ODEs for n, m, h and v are solved using the midpoint method, while the conductances g_exc and g_inh are solved using the exponential Euler method. Equivalent code: HH_cond_exp = Neuron ( parameters = \"\"\" gbar_Na = 20.0 gbar_K = 6.0 gleak = 0.01 cm = 0.2 v_offset = -63.0 e_rev_Na = 50.0 e_rev_K = -90.0 e_rev_leak = -65.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_syn_E = 0.2 tau_syn_I = 2.0 i_offset = 0.0 v_thresh = 0.0 \"\"\" , equations = \"\"\" # Previous membrane potential prev_v = v # Voltage-dependent rate constants an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) # Activation variables dn/dt = an * (1.0 - n) - bn * n : init = 0.0, exponential dm/dt = am * (1.0 - m) - bm * m : init = 0.0, exponential dh/dt = ah * (1.0 - h) - bh * h : init = 1.0, exponential # Membrane equation cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset: exponential, init=-65.0 # Exponentially-decaying conductances tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"(v > v_thresh) and (prev_v <= v_thresh)\" , reset = \"\" ) EIF_cond_exp_isfa_ista # EIF_cond_exp neuron. Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.) according to: Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642 Parameters: v_rest = -70.6 : Resting membrane potential (mV) cm = 0.281 : Capacity of the membrane (nF) tau_m = 9.3667 : Membrane time constant (ms) tau_refrac = 0.1 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -80.0 : Reversal potential for inhibitory input (mv) tau_w = 144.0 : Time constant of the adaptation variable (ms) a = 4.0 : Scaling of the adaptation variable b = 0.0805 : Increment on the adaptation variable after a spike i_offset = 0.0 : Offset current (nA) delta_T = 2.0 : Speed of the exponential (mV) v_thresh = -50.4 : Spike threshold for the exponential (mV) v_reset = -70.6 : Reset potential after a spike (mV) v_spike = -40.0 : Spike threshold (mV) Variables: I : input current (nA): I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset v : membrane potential in mV (init=-70.6): tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) w : adaptation variable (init=0.0): tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset u += b The ODEs are solved using the explicit Euler method. Equivalent code: EIF_cond_exp_isfa_ista = Neuron ( parameters = \"\"\" v_rest = -70.6 cm = 0.281 tau_m = 9.3667 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_w = 144.0 a = 4.0 b = 0.0805 i_offset = 0.0 delta_T = 2.0 v_thresh = -50.4 v_reset = -70.6 v_spike = -40.0 \"\"\" , equations = \"\"\" I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6 tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"v > v_spike\" , reset = \"\"\" v = v_reset w += b \"\"\" , refractory = 0.1 ) EIF_cond_alpha_isfa_ista # EIF_cond_alpha neuron. Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.) according to: Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642 Parameters: v_rest = -70.6 : Resting membrane potential (mV) cm = 0.281 : Capacity of the membrane (nF) tau_m = 9.3667 : Membrane time constant (ms) tau_refrac = 0.1 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -80.0 : Reversal potential for inhibitory input (mv) tau_w = 144.0 : Time constant of the adaptation variable (ms) a = 4.0 : Scaling of the adaptation variable b = 0.0805 : Increment on the adaptation variable after a spike i_offset = 0.0 : Offset current (nA) delta_T = 2.0 : Speed of the exponential (mV) v_thresh = -50.4 : Spike threshold for the exponential (mV) v_reset = -70.6 : Reset potential after a spike (mV) v_spike = -40.0 : Spike threshold (mV) Variables: I : input current (nA): I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset v : membrane potential in mV (init=-70.6): tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) w : adaptation variable (init=0.0): tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh alpha_exc : alpha function of excitatory current (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc alpha_inh: alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_spike Reset: v = v_reset u += b The ODEs are solved using the explicit Euler method. Equivalent code: EIF_cond_alpha_isfa_ista = Neuron ( parameters = \"\"\" v_rest = -70.6 cm = 0.281 tau_m = 9.3667 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_w = 144.0 a = 4.0 b = 0.0805 i_offset = 0.0 delta_T = 2.0 v_thresh = -50.4 v_reset = -70.6 v_spike = -40.0 \"\"\" , equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6 tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" , spike = \"v > v_spike\" , reset = \"\"\" v = v_reset w += b \"\"\" , refractory = 0.1 )","title":"Built-in neuron types"},{"location":"API/SpecificNeuron/#built-in-neuron-types","text":"ANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN ( http://neuralensemble.org/docs/PyNN/reference/neuronmodels.html ).","title":"Built-in neuron types"},{"location":"API/SpecificNeuron/#ANNarchy.models.Neurons.LeakyIntegrator","text":"Leaky-integrator rate-coded neuron, optionally noisy. This simple rate-coded neuron defines an internal variable \\(v(t)\\) which integrates the inputs \\(I(t)\\) with a time constant \\(\\tau\\) and a baseline \\(B\\) . An additive noise \\(N(t)\\) can be optionally defined: \\[\\tau \\cdot \\frac{dv(t)}{dt} + v(t) = I(t) + B + N(t)\\] The transfer function is the positive (or rectified linear ReLU) function with a threshold \\(T\\) : \\[r(t) = (v(t) - T)^+\\] By default, the input \\(I(t)\\) to this neuron is \"sum(exc) - sum(inh)\", but this can be changed by setting the sum argument: neuron = LeakyIntegrator ( sum = \"sum('exc')\" ) By default, there is no additive noise, but the noise argument can be passed with a specific distribution: neuron = LeakyIntegrator ( noise = \"Normal(0.0, 1.0)\" ) Parameters: tau = 10.0 : Time constant in ms of the neuron. B = 0.0 : Baseline value for v. T = 0.0 : Threshold for the positive transfer function. Variables: v : internal variable (init = 0.0): tau * dv/dt + v = sum(exc) - sum(inh) + B + N r : firing rate (init = 0.0): r = pos(v - T) The ODE is solved using the exponential Euler method. Equivalent code: LeakyIntegrator = Neuron ( parameters = ''' tau = 10.0 : population B = 0.0 T = 0.0 : population ''' , equations = ''' tau * dv/dt + v = sum(exc) - sum(inh) + B : exponential r = pos(v - T) ''' )","title":"LeakyIntegrator"},{"location":"API/SpecificNeuron/#ANNarchy.models.Neurons.Izhikevich","text":"Izhikevich neuron as proposed in: Izhikevich, E.M. (2003). Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks , 14:6. http://dx.doi.org/10.1109/TNN.2003.820440 The equations are: \\[\\frac{dv}{dt} = 0.04 * v^2 + 5.0 * v + 140.0 - u + I\\] \\[\\frac{du}{dt} = a * (b * v - u)\\] By default, the conductance is \"g_exc - g_inh\", but this can be changed by setting the conductance argument: neuron = Izhikevich ( conductance = 'g_ampa * (1 + g_nmda) - g_gaba' ) The synapses are instantaneous, i.e the corresponding conductance is increased from the synaptic efficiency w at the time step when a spike is received. Parameters: a = 0.02 : Speed of the recovery variable b = 0.2: Scaling of the recovery variable c = -65.0 : Reset potential. d = 8.0 : Increment of the recovery variable after a spike. v_thresh = 30.0 : Spike threshold (mV). i_offset = 0.0 : external current (nA). noise = 0.0 : Amplitude of the normal additive noise. tau_refrac = 0.0 : Duration of refractory period (ms). Variables: I : input current (user-defined conductance/current + external current + normal noise): I = conductance + i_offset + noise * Normal(0.0, 1.0) v : membrane potential in mV (init = c): dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I u : recovery variable (init= b * c): du/dt = a * (b * v - u) Spike emission: v > v_thresh Reset: v = c u += d The ODEs are solved using the explicit Euler method. Equivalent code: Izhikevich = Neuron ( parameters = \"\"\" noise = 0.0 a = 0.02 b = 0.2 c = -65.0 d = 8.0 v_thresh = 30.0 i_offset = 0.0 \"\"\" , equations = \"\"\" I = g_exc - g_inh + noise * Normal(0.0, 1.0) + i_offset dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init = -65.0 du/dt = a * (b*v - u) : init= -13.0 \"\"\" , spike = \"v > v_thresh\" , reset = \"v = c; u += d\" , refractory = 0.0 ) The default parameters are for a regular spiking (RS) neuron derived from the above mentioned article.","title":"Izhikevich"},{"location":"API/SpecificNeuron/#ANNarchy.models.Neurons.IF_curr_exp","text":"IF_curr_exp neuron. Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current. (Separate synaptic currents for excitatory and inhibitory synapses). Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_curr_exp = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 )","title":"IF_curr_exp"},{"location":"API/SpecificNeuron/#ANNarchy.models.Neurons.IF_cond_exp","text":"IF_cond_exp neuron. Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance. Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -70.0 : Reversal potential for inhibitory input (mv) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_cond_exp = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -70.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 )","title":"IF_cond_exp"},{"location":"API/SpecificNeuron/#ANNarchy.models.Neurons.IF_curr_alpha","text":"IF_curr_alpha neuron. Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents. (Separate synaptic currents for excitatory and inhibitory synapses). The alpha currents are calculated through a system of two linears ODEs. After a spike is received at t_spike, it peaks at t_spike + tau_syn_X, with a maximum equal to the synaptic efficiency. Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc alpha_exc : alpha function of excitatory current (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh alpha_inh : alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_curr_alpha = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 )","title":"IF_curr_alpha"},{"location":"API/SpecificNeuron/#ANNarchy.models.Neurons.IF_cond_alpha","text":"IF_cond_exp neuron. Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance. Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -70.0 : Reversal potential for inhibitory input (mv) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset g_exc : excitatory conductance (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc alpha_exc : alpha function of excitatory conductance (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc g_inh : inhibitory conductance (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh alpha_inh : alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_cond_alpha = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -70.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 )","title":"IF_cond_alpha"},{"location":"API/SpecificNeuron/#ANNarchy.models.Neurons.HH_cond_exp","text":"HH_cond_exp neuron. Single-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub. Parameters: gbar_Na = 20.0 : Maximal conductance of the Sodium current. gbar_K = 6.0 : Maximal conductance of the Potassium current. gleak = 0.01 : Conductance of the leak current (nF) cm = 0.2 : Capacity of the membrane (nF) v_offset = -63.0 : Threshold for the rate constants (mV) e_rev_Na = 50.0 : Reversal potential for the Sodium current (mV) e_rev_K = -90.0 : Reversal potential for the Potassium current (mV) e_rev_leak = -65.0 : Reversal potential for the leak current (mV) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -80.0 : Reversal potential for inhibitory input (mV) tau_syn_E = 0.2 : Decay time of excitatory synaptic current (ms) tau_syn_I = 2.0 : Decay time of inhibitory synaptic current (ms) i_offset = 0.0 : Offset current (nA) v_thresh = 0.0 : Threshold for spike emission Variables: Voltage-dependent rate constants an, bn, am, bm, ah, bh: an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) Activation variables n, m, h (h is initialized to 1.0, n and m to 0.0): dn/dt = an * (1.0 - n) - bn * n dm/dt = am * (1.0 - m) - bm * m dh/dt = ah * (1.0 - h) - bh * h v : membrane potential in mV (init=-65.0): cm * dv/dt = gleak (e_rev_leak -v) + gbar_K * n 4 * (e_rev_K - v) + gbar_Na * m *3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset g_exc : excitatory conductance (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory conductance (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission (the spike is emitted only once when v crosses the threshold from below): v > v_thresh and v(t-1) < v_thresh The ODEs for n, m, h and v are solved using the midpoint method, while the conductances g_exc and g_inh are solved using the exponential Euler method. Equivalent code: HH_cond_exp = Neuron ( parameters = \"\"\" gbar_Na = 20.0 gbar_K = 6.0 gleak = 0.01 cm = 0.2 v_offset = -63.0 e_rev_Na = 50.0 e_rev_K = -90.0 e_rev_leak = -65.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_syn_E = 0.2 tau_syn_I = 2.0 i_offset = 0.0 v_thresh = 0.0 \"\"\" , equations = \"\"\" # Previous membrane potential prev_v = v # Voltage-dependent rate constants an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) # Activation variables dn/dt = an * (1.0 - n) - bn * n : init = 0.0, exponential dm/dt = am * (1.0 - m) - bm * m : init = 0.0, exponential dh/dt = ah * (1.0 - h) - bh * h : init = 1.0, exponential # Membrane equation cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset: exponential, init=-65.0 # Exponentially-decaying conductances tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"(v > v_thresh) and (prev_v <= v_thresh)\" , reset = \"\" )","title":"HH_cond_exp"},{"location":"API/SpecificNeuron/#ANNarchy.models.Neurons.EIF_cond_exp_isfa_ista","text":"EIF_cond_exp neuron. Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.) according to: Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642 Parameters: v_rest = -70.6 : Resting membrane potential (mV) cm = 0.281 : Capacity of the membrane (nF) tau_m = 9.3667 : Membrane time constant (ms) tau_refrac = 0.1 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -80.0 : Reversal potential for inhibitory input (mv) tau_w = 144.0 : Time constant of the adaptation variable (ms) a = 4.0 : Scaling of the adaptation variable b = 0.0805 : Increment on the adaptation variable after a spike i_offset = 0.0 : Offset current (nA) delta_T = 2.0 : Speed of the exponential (mV) v_thresh = -50.4 : Spike threshold for the exponential (mV) v_reset = -70.6 : Reset potential after a spike (mV) v_spike = -40.0 : Spike threshold (mV) Variables: I : input current (nA): I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset v : membrane potential in mV (init=-70.6): tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) w : adaptation variable (init=0.0): tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset u += b The ODEs are solved using the explicit Euler method. Equivalent code: EIF_cond_exp_isfa_ista = Neuron ( parameters = \"\"\" v_rest = -70.6 cm = 0.281 tau_m = 9.3667 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_w = 144.0 a = 4.0 b = 0.0805 i_offset = 0.0 delta_T = 2.0 v_thresh = -50.4 v_reset = -70.6 v_spike = -40.0 \"\"\" , equations = \"\"\" I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6 tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"v > v_spike\" , reset = \"\"\" v = v_reset w += b \"\"\" , refractory = 0.1 )","title":"EIF_cond_exp_isfa_ista"},{"location":"API/SpecificNeuron/#ANNarchy.models.Neurons.EIF_cond_alpha_isfa_ista","text":"EIF_cond_alpha neuron. Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.) according to: Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642 Parameters: v_rest = -70.6 : Resting membrane potential (mV) cm = 0.281 : Capacity of the membrane (nF) tau_m = 9.3667 : Membrane time constant (ms) tau_refrac = 0.1 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -80.0 : Reversal potential for inhibitory input (mv) tau_w = 144.0 : Time constant of the adaptation variable (ms) a = 4.0 : Scaling of the adaptation variable b = 0.0805 : Increment on the adaptation variable after a spike i_offset = 0.0 : Offset current (nA) delta_T = 2.0 : Speed of the exponential (mV) v_thresh = -50.4 : Spike threshold for the exponential (mV) v_reset = -70.6 : Reset potential after a spike (mV) v_spike = -40.0 : Spike threshold (mV) Variables: I : input current (nA): I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset v : membrane potential in mV (init=-70.6): tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) w : adaptation variable (init=0.0): tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh alpha_exc : alpha function of excitatory current (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc alpha_inh: alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_spike Reset: v = v_reset u += b The ODEs are solved using the explicit Euler method. Equivalent code: EIF_cond_alpha_isfa_ista = Neuron ( parameters = \"\"\" v_rest = -70.6 cm = 0.281 tau_m = 9.3667 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_w = 144.0 a = 4.0 b = 0.0805 i_offset = 0.0 delta_T = 2.0 v_thresh = -50.4 v_reset = -70.6 v_spike = -40.0 \"\"\" , equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6 tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" , spike = \"v > v_spike\" , reset = \"\"\" v = v_reset w += b \"\"\" , refractory = 0.1 )","title":"EIF_cond_alpha_isfa_ista"},{"location":"API/SpecificPopulation/","text":"Specific Populations # ANNarchy provides a set of predefined Population objects to ease the definition of standard networks. PoissonPopulation # Population of spiking neurons following a Poisson distribution. Case 1: Input population Each neuron of the population will randomly emit spikes, with a mean firing rate defined by the rates argument. The mean firing rate in Hz can be a fixed value for all neurons: pop = PoissonPopulation ( geometry = 100 , rates = 100.0 ) but it can be modified later as a normal parameter: pop . rates = np . linspace ( 10 , 150 , 100 ) It is also possible to define a temporal equation for the rates, by passing a string to the argument: pop = PoissonPopulation ( geometry = 100 , rates = \"100.0 * (1.0 + sin(2*pi*t/1000.0) )/2.0\" ) The syntax of this equation follows the same structure as neural variables. It is also possible to add parameters to the population which can be used in the equation of rates : pop = PoissonPopulation ( geometry = 100 , parameters = ''' amp = 100.0 frequency = 1.0 ''' , rates = \"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\" ) Note: The preceding definition is fully equivalent to the definition of this neuron: poisson = Neuron ( parameters = ''' amp = 100.0 frequency = 1.0 ''' , equations = ''' rates = amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0 p = Uniform(0.0, 1.0) * 1000.0 / dt ''' , spike = ''' p < rates ''' ) The refractory period can also be set, so that a neuron can not emit two spikes too close from each other. Case 2: Hybrid population If the rates argument is not set, the population can be used as an interface from a rate-coded population. The target argument specifies which incoming projections will be summed to determine the instantaneous firing rate of each neuron. See the example in examples/hybrid/Hybrid.py for a usage. __init__ ( self , geometry , name = None , rates = None , target = None , parameters = None , refractory = None , copied = False ) special # Parameters: Name Type Description Default geometry population geometry as tuple. required name unique name of the population (optional). None rates mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string). None target the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\"). None parameters additional parameters which can be used in the rates equation. None refractory refractory period in ms. None Source code in ANNarchy/core/SpecificPopulation.py def __init__ ( self , geometry , name = None , rates = None , target = None , parameters = None , refractory = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. :param name: unique name of the population (optional). :param rates: mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string). :param target: the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\"). :param parameters: additional parameters which can be used in the *rates* equation. :param refractory: refractory period in ms. \"\"\" if rates is None and target is None : Global . _error ( 'A PoissonPopulation must define either rates or target.' ) self . target = target self . parameters = parameters self . refractory_init = refractory self . rates_init = rates if target is not None : # hybrid population # Create the neuron poisson_neuron = Neuron ( parameters = \"\"\" %(params)s \"\"\" % { 'params' : parameters if parameters else '' }, equations = \"\"\" rates = sum( %(target)s ) p = Uniform(0.0, 1.0) * 1000.0 / dt _sum_ %(target)s = 0.0 \"\"\" % { 'target' : target }, spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Hybrid\" , description = \"Hybrid spiking neuron emitting spikes according to a Poisson distribution at a frequency determined by the weighted sum of inputs.\" ) elif isinstance ( rates , str ): # Create the neuron poisson_neuron = Neuron ( parameters = \"\"\" %(params)s \"\"\" % { 'params' : parameters if parameters else '' }, equations = \"\"\" rates = %(rates)s p = Uniform(0.0, 1.0) * 1000.0 / dt _sum_exc = 0.0 \"\"\" % { 'rates' : rates }, spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) elif isinstance ( rates , np . ndarray ): poisson_neuron = Neuron ( parameters = \"\"\" rates = 10.0 \"\"\" , equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) else : poisson_neuron = Neuron ( parameters = \"\"\" rates = %(rates)s \"\"\" % { 'rates' : rates }, equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = poisson_neuron , name = name , copied = copied ) if isinstance ( rates , np . ndarray ): self . rates = rates SpikeSourceArray # Spike source generating spikes at the times given in the spike_times array. Depending on the initial array provided, the population will have one or several neurons, but the geometry can only be one-dimensional. You can later modify the spike_times attribute of the population, but it must have the same number of neurons as the initial one. The spike times are by default relative to the start of a simulation ( ANNarchy.get_time() is 0.0). If you call the reset() method of a SpikeSourceArray , this will set the spike times relative to the current time. You can then repeat a stimulation many times. # 2 neurons firing at 100Hz with a 1 ms delay times = [ [ 10 , 20 , 30 , 40 ], [ 11 , 21 , 31 , 41 ] ] inp = SpikeSourceArray ( spike_times = times ) compile () # Spikes at 10/11, 20/21, etc simulate ( 50 ) # Reset the internal time of the SpikeSourceArray inp . reset () # Spikes at 60/61, 70/71, etc simulate ( 50 ) __init__ ( self , spike_times , name = None , copied = False ) special # Parameters: Name Type Description Default spike_times a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt. required name optional name for the population. None Source code in ANNarchy/core/SpecificPopulation.py def __init__ ( self , spike_times , name = None , copied = False ): \"\"\" :param spike_times: a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt. :param name: optional name for the population. \"\"\" if not isinstance ( spike_times , list ): Global . _error ( 'In a SpikeSourceArray, spike_times must be a Python list.' ) if isinstance ( spike_times [ 0 ], list ): # several neurons nb_neurons = len ( spike_times ) else : # a single Neuron nb_neurons = 1 spike_times = [ spike_times ] # Create a fake neuron just to be sure the description has the correct parameters neuron = Neuron ( parameters = \"\" , equations = \"\" , spike = \" t == 0\" , reset = \"\" , name = \"Spike source\" , description = \"Spike source array.\" ) SpecificPopulation . __init__ ( self , geometry = nb_neurons , neuron = neuron , name = name , copied = copied ) self . init [ 'spike_times' ] = spike_times TimedArray # Data structure holding sequential inputs for a rate-coded network. The input values are stored in the (recordable) attribute r , without any further processing. You will need to connect this population to another one using the connect_one_to_one() method. By default, the firing rate of this population will iterate over the different values step by step: inputs = np . array ( [ [ 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 ] ] ) inp = TimedArray ( rates = inputs ) pop = Population ( 10 , ... ) proj = Projection ( inp , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) compile () simulate ( 10. ) This creates a population of 10 neurons whose activity will change during the first 10*dt milliseconds of the simulation. After that delay, the last input will be kept (i.e. 1 for the last neuron). If you want the TimedArray to \"loop\" over the different input vectors, you can specify a period for the inputs: inp = TimedArray ( rates = inputs , period = 10. ) If the period is smaller than the length of the rates, the last inputs will not be set. If you do not want the inputs to be set at every step, but every 10 ms for example, youcan use the schedule argument: inp = TimedArray ( rates = inputs , schedule = 10. ) The input [1, 0, 0,...] will stay for 10 ms, then[0, 1, 0, ...] for the next 10 ms, etc... If you need a less regular schedule, you can specify it as a list of times: inp = TimedArray ( rates = inputs , schedule = [ 10. , 20. , 50. , 60. , 100. , 110. ]) The first input is set at t = 10 ms (r = 0.0 in the first 10 ms), the second at t = 20 ms, the third at t = 50 ms, etc. If you specify less times than in the array of rates, the last ones will be ignored. Scheduling can be combined with periodic cycling. Note that you can use the reset() method to manually reinitialize the TimedArray, times becoming relative to that call: simulate ( 100. ) # ten inputs are shown with a schedule of 10 ms inp . reset () simulate ( 100. ) # the same ten inputs are presented again. __init__ ( self , rates , schedule = 0.0 , period =- 1.0 , name = None , copied = False ) special # Parameters: Name Type Description Default rates array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population. required schedule either a single value or a list of time points where inputs should be set. Default: every timestep. 0.0 period time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.). -1.0 Source code in ANNarchy/core/SpecificPopulation.py def __init__ ( self , rates , schedule = 0. , period = - 1. , name = None , copied = False ): \"\"\" :param rates: array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population. :param schedule: either a single value or a list of time points where inputs should be set. Default: every timestep. :param period: time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.). \"\"\" neuron = Neuron ( parameters = \"\" , equations = \" r = 0.0\" , name = \"Timed Array\" , description = \"Timed array source.\" ) # Geometry of the population geometry = rates . shape [ 1 :] # Check the schedule if isinstance ( schedule , ( int , float )): if float ( schedule ) <= 0.0 : schedule = Global . config [ 'dt' ] schedule = [ float ( schedule * i ) for i in range ( rates . shape [ 0 ])] if len ( schedule ) > rates . shape [ 0 ]: Global . _error ( 'TimedArray: the length of the schedule parameter cannot exceed the first dimension of the rates parameter.' ) if len ( schedule ) < rates . shape [ 0 ]: Global . _warning ( 'TimedArray: the length of the schedule parameter is smaller than the first dimension of the rates parameter (more data than time points). Make sure it is what you expect.' ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = neuron , name = name , copied = copied ) self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'period' ] = period HomogeneousCorrelatedSpikeTrains # Population of spiking neurons following a homogeneous distribution with correlated spike trains. The method describing the generation of homogeneous correlated spike trains is described in: Brette, R. (2009). Generation of correlated spike trains. Neural Computation 21(1). http://romainbrette.fr/WordPress3/wp-content/uploads/2014/06/Brette2008NC.pdf The implementation is based on the one provided by Brian http://briansimulator.org . To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation: \\[\\frac{dx}{dt} = \\frac{(\\mu - x)}{\\tau} + \\sigma \\, \\frac{\\xi}{\\sqrt{\\tau}}\\] where \\(\\xi\\) is a random variable. In short, \\(x\\) will randomly vary around mu over time, with an amplitude determined by sigma and a speed determined by tau. This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process. To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates , the desired correlation strength corr and the time constant tau . See Brette's paper for details. In short, you should only define the parameters rates , corr and tau , and let the class compute mu and sigma for you. Changing rates , corr or tau after initialization automatically recomputes mu and sigma. Example: from ANNarchy import * setup ( dt = 0.1 ) pop_corr = HomogeneousCorrelatedSpikeTrains ( 200 , rates = 10. , corr = 0.3 , tau = 10. ) compile () simulate ( 1000. ) pop_corr . rates = 30. simulate ( 1000. ) Alternatively, a schedule can be provided to change automatically the value of rates and corr (but not tau ) at the required times (as in TimedArray or TimedPoissonPopulation): from ANNarchy import * setup ( dt = 0.1 ) pop_corr = HomogeneousCorrelatedSpikeTrains ( geometry = 200 , rates = [ 10. , 30. ], corr = [ 0.3 , 0.5 ], tau = 10. , schedule = [ 0. , 1000. ] ) compile () simulate ( 2000. ) Even when using a schedule, corr accepts a single constant value. The first value of schedule must be 0. period specifies when the schedule \"loops\" back to its initial value. __init__ ( self , geometry , rates , corr , tau , schedule = None , period =- 1.0 , name = None , refractory = None , copied = False ) special # Parameters: Name Type Description Default geometry population geometry as tuple. required rates rate in Hz of the population (must be a positive float or a list) required corr total correlation strength (float in [0, 1], or a list) required tau correlation time constant in ms. required schedule list of times where new values of rates and corr will be used to computre mu and sigma. None period time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.) -1.0 name unique name of the population (optional). None refractory refractory period in ms (careful: may break the correlation) None Source code in ANNarchy/core/SpecificPopulation.py def __init__ ( self , geometry , rates , corr , tau , schedule = None , period =- 1. , name = None , refractory = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. :param rates: rate in Hz of the population (must be a positive float or a list) :param corr: total correlation strength (float in [0, 1], or a list) :param tau: correlation time constant in ms. :param schedule: list of times where new values of ``rates``and ``corr``will be used to computre mu and sigma. :param period: time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.) :param name: unique name of the population (optional). :param refractory: refractory period in ms (careful: may break the correlation) \"\"\" if schedule is not None : self . _has_schedule = True # Rates if not isinstance ( rates , ( list , np . ndarray )): Global . _error ( \"TimedHomogeneousCorrelatedSpikeTrains: the rates argument must be a list or a numpy array.\" ) rates = np . array ( rates ) # Schedule schedule = np . array ( schedule ) nb_schedules = rates . shape [ 0 ] if nb_schedules != schedule . size : Global . _error ( \"TimedHomogeneousCorrelatedSpikeTrains: the length of rates must be the same length as for schedule.\" ) # corr corr = np . array ( corr ) if corr . size == 1 : corr = np . full ( nb_schedules , corr ) else : self . _has_schedule = False rates = np . array ([ float ( rates )]) schedule = np . array ([ 0.0 ]) corr = np . array ([ corr ]) # Store refractory self . refractory_init = refractory # Correction of mu and sigma mu_list , sigma_list = self . _correction ( rates , corr , tau ) self . rates = rates self . corr = corr self . tau = tau # Create the neuron corr_neuron = Neuron ( parameters = \"\"\" tau = %(tau)s : population mu = %(mu)s : population sigma = %(sigma)s : population \"\"\" % { 'tau' : tau , 'mu' : mu_list [ 0 ], 'sigma' : sigma_list [ 0 ]}, equations = \"\"\" x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) : population, init= %(mu)s p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" % { 'mu' : mu_list [ 0 ]}, spike = \"p < x\" , refractory = refractory , name = \"HomogeneousCorrelated\" , description = \"Homogeneous correlated spike trains.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = corr_neuron , name = name , copied = copied ) # Initial values self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'corr' ] = corr self . init [ 'tau' ] = tau self . init [ 'period' ] = period if self . _has_schedule : self . init [ 'mu' ] = mu_list self . init [ 'sigma' ] = sigma_list else : self . init [ 'mu' ] = mu_list [ 0 ] self . init [ 'sigma' ] = sigma_list [ 0 ] TimedPoissonPopulation # Poisson population whose rate vary with the provided schedule. Example: inp = TimedPoissonPopulation ( geometry = 100 , rates = [ 10. , 20. , 100. , 20. , 5. ], schedule = [ 0. , 100. , 200. , 500. , 600. ], ) This creates a population of 100 Poisson neurons whose rate will be: 10 Hz during the first 100 ms. 20 HZ during the next 100 ms. 100 Hz during the next 300 ms. 20 Hz during the next 100 ms. 5 Hz until the end of the simulation. If you want the TimedPoissonPopulation to \"loop\" over the schedule, you can specify a period: inp = TimedPoissonPopulation ( geometry = 100 , rates = [ 10. , 20. , 100. , 20. , 5. ], schedule = [ 0. , 100. , 200. , 500. , 600. ], period = 1000. , ) Here the rate will become 10Hz again every 1 second of simulation. If the period is smaller than the schedule, the remaining rates will not be set. Note that you can use the reset() method to manually reinitialize the schedule, times becoming relative to that call: simulate ( 1200. ) # Should switch to 100 Hz due to the period of 1000. inp . reset () simulate ( 1000. ) # Starts at 10 Hz again. The rates were here global to the population. If you want each neuron to have a different rate, rates must have additional dimensions corresponding to the geometry of the population. inp = TimedPoissonPopulation ( geometry = 100 , rates = [ [ 10. + 0.05 * i for i in range ( 100 )], [ 20. + 0.05 * i for i in range ( 100 )], ], schedule = [ 0. , 100. ], period = 1000. , ) __init__ ( self , geometry , rates , schedule , period =- 1.0 , name = None , copied = False ) special # Parameters: Name Type Description Default rates array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match with the geometr of the population. required schedule list of times (in ms) where the firing rate should change. required period time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.). -1.0 Source code in ANNarchy/core/SpecificPopulation.py def __init__ ( self , geometry , rates , schedule , period = - 1. , name = None , copied = False ): \"\"\" :param rates: array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match with the geometr of the population. :param schedule: list of times (in ms) where the firing rate should change. :param period: time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.). \"\"\" neuron = Neuron ( parameters = \"\"\" proba = 1.0 \"\"\" , equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < proba \"\"\" , name = \"TimedPoisson\" , description = \"Spiking neuron following a Poisson distribution.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = neuron , name = name , copied = copied ) # Check arguments try : rates = np . array ( rates ) except : Global . _error ( \"TimedPoissonPopulation: the rates argument must be a numpy array.\" ) schedule = np . array ( schedule ) nb_schedules = rates . shape [ 0 ] if nb_schedules != schedule . size : Global . _error ( \"TimedPoissonPopulation: the first axis of the rates argument must be the same length as schedule.\" ) if rates . ndim == 1 : # One rate for the whole population rates = np . array ([ np . full ( self . size , rates [ i ]) for i in range ( nb_schedules )]) # Initial values self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'period' ] = period ImagePopulation # Specific rate-coded Population allowing to represent images (png, jpg...) as the firing rate of a population (each neuron represents one pixel). This extension requires the Python Image Library (pip install Pillow). Usage: from ANNarchy import * from ANNarchy.extensions.image import ImagePopulation pop = ImagePopulation ( geometry = ( 480 , 640 )) pop . set_image ( 'image.jpg' ) About the geometry: If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image). If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color). If the third dimension is 3, each will correspond to the RGB values of the pixels. Warning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population. __init__ ( self , geometry , name = None , copied = False ) special # Parameters: Name Type Description Default geometry population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation. required name unique name of the population (optional). None Source code in ANNarchy/extensions/image/ImagePopulation.py def __init__ ( self , geometry , name = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation. :param name: unique name of the population (optional). \"\"\" # Check geometry if isinstance ( geometry , int ) or len ( geometry ) == 1 : Global . _error ( 'The geometry of an ImagePopulation should be 2D (grayscale) or 3D (color).' ) if len ( geometry ) == 3 and ( geometry [ 2 ] != 3 and geometry [ 2 ] != 1 ): Global . _error ( 'The third dimension of an ImagePopulation should be either 1 (grayscale) or 3 (color).' ) if len ( geometry ) == 3 and geometry [ 2 ] == 1 : geometry = ( int ( geometry [ 0 ]), int ( geometry [ 1 ])) # Create the population Population . __init__ ( self , geometry = geometry , name = name , neuron = Neuron ( parameters = \"r = 0.0\" ), copied = copied ) set_image ( self , image_name ) # Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population. If the image has a different size from the population, it will be resized. Source code in ANNarchy/extensions/image/ImagePopulation.py def set_image ( self , image_name ): \"\"\" Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population. If the image has a different size from the population, it will be resized. \"\"\" try : im = Image . open ( image_name ) except : # image does not exist Global . _error ( 'The image ' + image_name + ' does not exist.' ) # Resize the image if needed ( width , height ) = ( self . geometry [ 1 ], self . geometry [ 0 ]) if im . size != ( width , height ): Global . _warning ( 'The image ' + image_name + ' does not have the same size ' + str ( im . size ) + ' as the population ' + str (( width , height )) + '. It will be resized.' ) im = im . resize (( width , height )) # Check if only the luminance should be extracted if self . dimension == 2 or self . geometry [ 2 ] == 1 : im = im . convert ( \"L\" ) # Set the rate of the population self . r = np . array ( im ) . reshape ( self . size ) / 255. VideoPopulation # Specific rate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel). This extension requires the C++ library OpenCV >= 4.0 (apt-get/yum install opencv). pkg-config opencv4 --cflags --libs should not return an error. vtk might additionally have to be installed. Usage: from ANNarchy import * from ANNarchy.extensions.image import VideoPopulation pop = VideoPopulation ( geometry = ( 480 , 640 )) compile () pop . start_camera ( 0 ) while ( True ): pop . grab_image () simulate ( 10.0 ) About the geometry: If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image). If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color). If the third dimension is 3, each will correspond to the RGB values of the pixels. Warning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population. __init__ ( self , geometry , opencv_version = '4' , name = None , copied = False ) special # Parameters: Name Type Description Default geometry population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized. required opencv_version OpenCV version (default=4). '4' name unique name of the population (optional). None Source code in ANNarchy/extensions/image/ImagePopulation.py def __init__ ( self , geometry , opencv_version = \"4\" , name = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized. :param opencv_version: OpenCV version (default=4). :param name: unique name of the population (optional). \"\"\" # Create the population ImagePopulation . __init__ ( self , geometry = geometry , name = name , copied = copied ) self . opencv_version = opencv_version grab_image ( self ) # Grabs one image from the camera and feeds it into the population. The camera must be first started with: pop.start_camera(0) Source code in ANNarchy/extensions/image/ImagePopulation.py def grab_image ( self ): \"\"\" Grabs one image from the camera and feeds it into the population. The camera must be first started with: pop.start_camera(0) \"\"\" self . cyInstance . grab_image () release ( self ) # Releases the camera: pop.release() Source code in ANNarchy/extensions/image/ImagePopulation.py def release ( self ): \"\"\" Releases the camera: pop.release() \"\"\" self . cyInstance . release_camera () start_camera ( self , camera_port = 0 ) # Starts the webcam with the corresponding device (default = 0). On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc. Source code in ANNarchy/extensions/image/ImagePopulation.py def start_camera ( self , camera_port = 0 ): \"\"\" Starts the webcam with the corresponding device (default = 0). On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc. \"\"\" self . cyInstance . start_camera ( camera_port , self . geometry [ 1 ], self . geometry [ 0 ], 3 if self . dimension == 3 else 1 )","title":"Specific Populations"},{"location":"API/SpecificPopulation/#specific-populations","text":"ANNarchy provides a set of predefined Population objects to ease the definition of standard networks.","title":"Specific Populations"},{"location":"API/SpecificPopulation/#ANNarchy.core.SpecificPopulation.PoissonPopulation","text":"Population of spiking neurons following a Poisson distribution. Case 1: Input population Each neuron of the population will randomly emit spikes, with a mean firing rate defined by the rates argument. The mean firing rate in Hz can be a fixed value for all neurons: pop = PoissonPopulation ( geometry = 100 , rates = 100.0 ) but it can be modified later as a normal parameter: pop . rates = np . linspace ( 10 , 150 , 100 ) It is also possible to define a temporal equation for the rates, by passing a string to the argument: pop = PoissonPopulation ( geometry = 100 , rates = \"100.0 * (1.0 + sin(2*pi*t/1000.0) )/2.0\" ) The syntax of this equation follows the same structure as neural variables. It is also possible to add parameters to the population which can be used in the equation of rates : pop = PoissonPopulation ( geometry = 100 , parameters = ''' amp = 100.0 frequency = 1.0 ''' , rates = \"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\" ) Note: The preceding definition is fully equivalent to the definition of this neuron: poisson = Neuron ( parameters = ''' amp = 100.0 frequency = 1.0 ''' , equations = ''' rates = amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0 p = Uniform(0.0, 1.0) * 1000.0 / dt ''' , spike = ''' p < rates ''' ) The refractory period can also be set, so that a neuron can not emit two spikes too close from each other. Case 2: Hybrid population If the rates argument is not set, the population can be used as an interface from a rate-coded population. The target argument specifies which incoming projections will be summed to determine the instantaneous firing rate of each neuron. See the example in examples/hybrid/Hybrid.py for a usage.","title":"PoissonPopulation"},{"location":"API/SpecificPopulation/#ANNarchy.core.SpecificPopulation.PoissonPopulation.__init__","text":"Parameters: Name Type Description Default geometry population geometry as tuple. required name unique name of the population (optional). None rates mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string). None target the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\"). None parameters additional parameters which can be used in the rates equation. None refractory refractory period in ms. None Source code in ANNarchy/core/SpecificPopulation.py def __init__ ( self , geometry , name = None , rates = None , target = None , parameters = None , refractory = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. :param name: unique name of the population (optional). :param rates: mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string). :param target: the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\"). :param parameters: additional parameters which can be used in the *rates* equation. :param refractory: refractory period in ms. \"\"\" if rates is None and target is None : Global . _error ( 'A PoissonPopulation must define either rates or target.' ) self . target = target self . parameters = parameters self . refractory_init = refractory self . rates_init = rates if target is not None : # hybrid population # Create the neuron poisson_neuron = Neuron ( parameters = \"\"\" %(params)s \"\"\" % { 'params' : parameters if parameters else '' }, equations = \"\"\" rates = sum( %(target)s ) p = Uniform(0.0, 1.0) * 1000.0 / dt _sum_ %(target)s = 0.0 \"\"\" % { 'target' : target }, spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Hybrid\" , description = \"Hybrid spiking neuron emitting spikes according to a Poisson distribution at a frequency determined by the weighted sum of inputs.\" ) elif isinstance ( rates , str ): # Create the neuron poisson_neuron = Neuron ( parameters = \"\"\" %(params)s \"\"\" % { 'params' : parameters if parameters else '' }, equations = \"\"\" rates = %(rates)s p = Uniform(0.0, 1.0) * 1000.0 / dt _sum_exc = 0.0 \"\"\" % { 'rates' : rates }, spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) elif isinstance ( rates , np . ndarray ): poisson_neuron = Neuron ( parameters = \"\"\" rates = 10.0 \"\"\" , equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) else : poisson_neuron = Neuron ( parameters = \"\"\" rates = %(rates)s \"\"\" % { 'rates' : rates }, equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = poisson_neuron , name = name , copied = copied ) if isinstance ( rates , np . ndarray ): self . rates = rates","title":"__init__()"},{"location":"API/SpecificPopulation/#ANNarchy.core.SpecificPopulation.SpikeSourceArray","text":"Spike source generating spikes at the times given in the spike_times array. Depending on the initial array provided, the population will have one or several neurons, but the geometry can only be one-dimensional. You can later modify the spike_times attribute of the population, but it must have the same number of neurons as the initial one. The spike times are by default relative to the start of a simulation ( ANNarchy.get_time() is 0.0). If you call the reset() method of a SpikeSourceArray , this will set the spike times relative to the current time. You can then repeat a stimulation many times. # 2 neurons firing at 100Hz with a 1 ms delay times = [ [ 10 , 20 , 30 , 40 ], [ 11 , 21 , 31 , 41 ] ] inp = SpikeSourceArray ( spike_times = times ) compile () # Spikes at 10/11, 20/21, etc simulate ( 50 ) # Reset the internal time of the SpikeSourceArray inp . reset () # Spikes at 60/61, 70/71, etc simulate ( 50 )","title":"SpikeSourceArray"},{"location":"API/SpecificPopulation/#ANNarchy.core.SpecificPopulation.SpikeSourceArray.__init__","text":"Parameters: Name Type Description Default spike_times a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt. required name optional name for the population. None Source code in ANNarchy/core/SpecificPopulation.py def __init__ ( self , spike_times , name = None , copied = False ): \"\"\" :param spike_times: a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt. :param name: optional name for the population. \"\"\" if not isinstance ( spike_times , list ): Global . _error ( 'In a SpikeSourceArray, spike_times must be a Python list.' ) if isinstance ( spike_times [ 0 ], list ): # several neurons nb_neurons = len ( spike_times ) else : # a single Neuron nb_neurons = 1 spike_times = [ spike_times ] # Create a fake neuron just to be sure the description has the correct parameters neuron = Neuron ( parameters = \"\" , equations = \"\" , spike = \" t == 0\" , reset = \"\" , name = \"Spike source\" , description = \"Spike source array.\" ) SpecificPopulation . __init__ ( self , geometry = nb_neurons , neuron = neuron , name = name , copied = copied ) self . init [ 'spike_times' ] = spike_times","title":"__init__()"},{"location":"API/SpecificPopulation/#ANNarchy.core.SpecificPopulation.TimedArray","text":"Data structure holding sequential inputs for a rate-coded network. The input values are stored in the (recordable) attribute r , without any further processing. You will need to connect this population to another one using the connect_one_to_one() method. By default, the firing rate of this population will iterate over the different values step by step: inputs = np . array ( [ [ 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 ] ] ) inp = TimedArray ( rates = inputs ) pop = Population ( 10 , ... ) proj = Projection ( inp , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) compile () simulate ( 10. ) This creates a population of 10 neurons whose activity will change during the first 10*dt milliseconds of the simulation. After that delay, the last input will be kept (i.e. 1 for the last neuron). If you want the TimedArray to \"loop\" over the different input vectors, you can specify a period for the inputs: inp = TimedArray ( rates = inputs , period = 10. ) If the period is smaller than the length of the rates, the last inputs will not be set. If you do not want the inputs to be set at every step, but every 10 ms for example, youcan use the schedule argument: inp = TimedArray ( rates = inputs , schedule = 10. ) The input [1, 0, 0,...] will stay for 10 ms, then[0, 1, 0, ...] for the next 10 ms, etc... If you need a less regular schedule, you can specify it as a list of times: inp = TimedArray ( rates = inputs , schedule = [ 10. , 20. , 50. , 60. , 100. , 110. ]) The first input is set at t = 10 ms (r = 0.0 in the first 10 ms), the second at t = 20 ms, the third at t = 50 ms, etc. If you specify less times than in the array of rates, the last ones will be ignored. Scheduling can be combined with periodic cycling. Note that you can use the reset() method to manually reinitialize the TimedArray, times becoming relative to that call: simulate ( 100. ) # ten inputs are shown with a schedule of 10 ms inp . reset () simulate ( 100. ) # the same ten inputs are presented again.","title":"TimedArray"},{"location":"API/SpecificPopulation/#ANNarchy.core.SpecificPopulation.TimedArray.__init__","text":"Parameters: Name Type Description Default rates array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population. required schedule either a single value or a list of time points where inputs should be set. Default: every timestep. 0.0 period time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.). -1.0 Source code in ANNarchy/core/SpecificPopulation.py def __init__ ( self , rates , schedule = 0. , period = - 1. , name = None , copied = False ): \"\"\" :param rates: array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population. :param schedule: either a single value or a list of time points where inputs should be set. Default: every timestep. :param period: time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.). \"\"\" neuron = Neuron ( parameters = \"\" , equations = \" r = 0.0\" , name = \"Timed Array\" , description = \"Timed array source.\" ) # Geometry of the population geometry = rates . shape [ 1 :] # Check the schedule if isinstance ( schedule , ( int , float )): if float ( schedule ) <= 0.0 : schedule = Global . config [ 'dt' ] schedule = [ float ( schedule * i ) for i in range ( rates . shape [ 0 ])] if len ( schedule ) > rates . shape [ 0 ]: Global . _error ( 'TimedArray: the length of the schedule parameter cannot exceed the first dimension of the rates parameter.' ) if len ( schedule ) < rates . shape [ 0 ]: Global . _warning ( 'TimedArray: the length of the schedule parameter is smaller than the first dimension of the rates parameter (more data than time points). Make sure it is what you expect.' ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = neuron , name = name , copied = copied ) self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'period' ] = period","title":"__init__()"},{"location":"API/SpecificPopulation/#ANNarchy.core.SpecificPopulation.HomogeneousCorrelatedSpikeTrains","text":"Population of spiking neurons following a homogeneous distribution with correlated spike trains. The method describing the generation of homogeneous correlated spike trains is described in: Brette, R. (2009). Generation of correlated spike trains. Neural Computation 21(1). http://romainbrette.fr/WordPress3/wp-content/uploads/2014/06/Brette2008NC.pdf The implementation is based on the one provided by Brian http://briansimulator.org . To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation: \\[\\frac{dx}{dt} = \\frac{(\\mu - x)}{\\tau} + \\sigma \\, \\frac{\\xi}{\\sqrt{\\tau}}\\] where \\(\\xi\\) is a random variable. In short, \\(x\\) will randomly vary around mu over time, with an amplitude determined by sigma and a speed determined by tau. This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process. To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates , the desired correlation strength corr and the time constant tau . See Brette's paper for details. In short, you should only define the parameters rates , corr and tau , and let the class compute mu and sigma for you. Changing rates , corr or tau after initialization automatically recomputes mu and sigma. Example: from ANNarchy import * setup ( dt = 0.1 ) pop_corr = HomogeneousCorrelatedSpikeTrains ( 200 , rates = 10. , corr = 0.3 , tau = 10. ) compile () simulate ( 1000. ) pop_corr . rates = 30. simulate ( 1000. ) Alternatively, a schedule can be provided to change automatically the value of rates and corr (but not tau ) at the required times (as in TimedArray or TimedPoissonPopulation): from ANNarchy import * setup ( dt = 0.1 ) pop_corr = HomogeneousCorrelatedSpikeTrains ( geometry = 200 , rates = [ 10. , 30. ], corr = [ 0.3 , 0.5 ], tau = 10. , schedule = [ 0. , 1000. ] ) compile () simulate ( 2000. ) Even when using a schedule, corr accepts a single constant value. The first value of schedule must be 0. period specifies when the schedule \"loops\" back to its initial value.","title":"HomogeneousCorrelatedSpikeTrains"},{"location":"API/SpecificPopulation/#ANNarchy.core.SpecificPopulation.HomogeneousCorrelatedSpikeTrains.__init__","text":"Parameters: Name Type Description Default geometry population geometry as tuple. required rates rate in Hz of the population (must be a positive float or a list) required corr total correlation strength (float in [0, 1], or a list) required tau correlation time constant in ms. required schedule list of times where new values of rates and corr will be used to computre mu and sigma. None period time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.) -1.0 name unique name of the population (optional). None refractory refractory period in ms (careful: may break the correlation) None Source code in ANNarchy/core/SpecificPopulation.py def __init__ ( self , geometry , rates , corr , tau , schedule = None , period =- 1. , name = None , refractory = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. :param rates: rate in Hz of the population (must be a positive float or a list) :param corr: total correlation strength (float in [0, 1], or a list) :param tau: correlation time constant in ms. :param schedule: list of times where new values of ``rates``and ``corr``will be used to computre mu and sigma. :param period: time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.) :param name: unique name of the population (optional). :param refractory: refractory period in ms (careful: may break the correlation) \"\"\" if schedule is not None : self . _has_schedule = True # Rates if not isinstance ( rates , ( list , np . ndarray )): Global . _error ( \"TimedHomogeneousCorrelatedSpikeTrains: the rates argument must be a list or a numpy array.\" ) rates = np . array ( rates ) # Schedule schedule = np . array ( schedule ) nb_schedules = rates . shape [ 0 ] if nb_schedules != schedule . size : Global . _error ( \"TimedHomogeneousCorrelatedSpikeTrains: the length of rates must be the same length as for schedule.\" ) # corr corr = np . array ( corr ) if corr . size == 1 : corr = np . full ( nb_schedules , corr ) else : self . _has_schedule = False rates = np . array ([ float ( rates )]) schedule = np . array ([ 0.0 ]) corr = np . array ([ corr ]) # Store refractory self . refractory_init = refractory # Correction of mu and sigma mu_list , sigma_list = self . _correction ( rates , corr , tau ) self . rates = rates self . corr = corr self . tau = tau # Create the neuron corr_neuron = Neuron ( parameters = \"\"\" tau = %(tau)s : population mu = %(mu)s : population sigma = %(sigma)s : population \"\"\" % { 'tau' : tau , 'mu' : mu_list [ 0 ], 'sigma' : sigma_list [ 0 ]}, equations = \"\"\" x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) : population, init= %(mu)s p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" % { 'mu' : mu_list [ 0 ]}, spike = \"p < x\" , refractory = refractory , name = \"HomogeneousCorrelated\" , description = \"Homogeneous correlated spike trains.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = corr_neuron , name = name , copied = copied ) # Initial values self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'corr' ] = corr self . init [ 'tau' ] = tau self . init [ 'period' ] = period if self . _has_schedule : self . init [ 'mu' ] = mu_list self . init [ 'sigma' ] = sigma_list else : self . init [ 'mu' ] = mu_list [ 0 ] self . init [ 'sigma' ] = sigma_list [ 0 ]","title":"__init__()"},{"location":"API/SpecificPopulation/#ANNarchy.core.SpecificPopulation.TimedPoissonPopulation","text":"Poisson population whose rate vary with the provided schedule. Example: inp = TimedPoissonPopulation ( geometry = 100 , rates = [ 10. , 20. , 100. , 20. , 5. ], schedule = [ 0. , 100. , 200. , 500. , 600. ], ) This creates a population of 100 Poisson neurons whose rate will be: 10 Hz during the first 100 ms. 20 HZ during the next 100 ms. 100 Hz during the next 300 ms. 20 Hz during the next 100 ms. 5 Hz until the end of the simulation. If you want the TimedPoissonPopulation to \"loop\" over the schedule, you can specify a period: inp = TimedPoissonPopulation ( geometry = 100 , rates = [ 10. , 20. , 100. , 20. , 5. ], schedule = [ 0. , 100. , 200. , 500. , 600. ], period = 1000. , ) Here the rate will become 10Hz again every 1 second of simulation. If the period is smaller than the schedule, the remaining rates will not be set. Note that you can use the reset() method to manually reinitialize the schedule, times becoming relative to that call: simulate ( 1200. ) # Should switch to 100 Hz due to the period of 1000. inp . reset () simulate ( 1000. ) # Starts at 10 Hz again. The rates were here global to the population. If you want each neuron to have a different rate, rates must have additional dimensions corresponding to the geometry of the population. inp = TimedPoissonPopulation ( geometry = 100 , rates = [ [ 10. + 0.05 * i for i in range ( 100 )], [ 20. + 0.05 * i for i in range ( 100 )], ], schedule = [ 0. , 100. ], period = 1000. , )","title":"TimedPoissonPopulation"},{"location":"API/SpecificPopulation/#ANNarchy.core.SpecificPopulation.TimedPoissonPopulation.__init__","text":"Parameters: Name Type Description Default rates array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match with the geometr of the population. required schedule list of times (in ms) where the firing rate should change. required period time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.). -1.0 Source code in ANNarchy/core/SpecificPopulation.py def __init__ ( self , geometry , rates , schedule , period = - 1. , name = None , copied = False ): \"\"\" :param rates: array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match with the geometr of the population. :param schedule: list of times (in ms) where the firing rate should change. :param period: time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.). \"\"\" neuron = Neuron ( parameters = \"\"\" proba = 1.0 \"\"\" , equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < proba \"\"\" , name = \"TimedPoisson\" , description = \"Spiking neuron following a Poisson distribution.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = neuron , name = name , copied = copied ) # Check arguments try : rates = np . array ( rates ) except : Global . _error ( \"TimedPoissonPopulation: the rates argument must be a numpy array.\" ) schedule = np . array ( schedule ) nb_schedules = rates . shape [ 0 ] if nb_schedules != schedule . size : Global . _error ( \"TimedPoissonPopulation: the first axis of the rates argument must be the same length as schedule.\" ) if rates . ndim == 1 : # One rate for the whole population rates = np . array ([ np . full ( self . size , rates [ i ]) for i in range ( nb_schedules )]) # Initial values self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'period' ] = period","title":"__init__()"},{"location":"API/SpecificPopulation/#ANNarchy.extensions.image.ImagePopulation.ImagePopulation","text":"Specific rate-coded Population allowing to represent images (png, jpg...) as the firing rate of a population (each neuron represents one pixel). This extension requires the Python Image Library (pip install Pillow). Usage: from ANNarchy import * from ANNarchy.extensions.image import ImagePopulation pop = ImagePopulation ( geometry = ( 480 , 640 )) pop . set_image ( 'image.jpg' ) About the geometry: If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image). If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color). If the third dimension is 3, each will correspond to the RGB values of the pixels. Warning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population.","title":"ImagePopulation"},{"location":"API/SpecificPopulation/#ANNarchy.extensions.image.ImagePopulation.ImagePopulation.__init__","text":"Parameters: Name Type Description Default geometry population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation. required name unique name of the population (optional). None Source code in ANNarchy/extensions/image/ImagePopulation.py def __init__ ( self , geometry , name = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation. :param name: unique name of the population (optional). \"\"\" # Check geometry if isinstance ( geometry , int ) or len ( geometry ) == 1 : Global . _error ( 'The geometry of an ImagePopulation should be 2D (grayscale) or 3D (color).' ) if len ( geometry ) == 3 and ( geometry [ 2 ] != 3 and geometry [ 2 ] != 1 ): Global . _error ( 'The third dimension of an ImagePopulation should be either 1 (grayscale) or 3 (color).' ) if len ( geometry ) == 3 and geometry [ 2 ] == 1 : geometry = ( int ( geometry [ 0 ]), int ( geometry [ 1 ])) # Create the population Population . __init__ ( self , geometry = geometry , name = name , neuron = Neuron ( parameters = \"r = 0.0\" ), copied = copied )","title":"__init__()"},{"location":"API/SpecificPopulation/#ANNarchy.extensions.image.ImagePopulation.ImagePopulation.set_image","text":"Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population. If the image has a different size from the population, it will be resized. Source code in ANNarchy/extensions/image/ImagePopulation.py def set_image ( self , image_name ): \"\"\" Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population. If the image has a different size from the population, it will be resized. \"\"\" try : im = Image . open ( image_name ) except : # image does not exist Global . _error ( 'The image ' + image_name + ' does not exist.' ) # Resize the image if needed ( width , height ) = ( self . geometry [ 1 ], self . geometry [ 0 ]) if im . size != ( width , height ): Global . _warning ( 'The image ' + image_name + ' does not have the same size ' + str ( im . size ) + ' as the population ' + str (( width , height )) + '. It will be resized.' ) im = im . resize (( width , height )) # Check if only the luminance should be extracted if self . dimension == 2 or self . geometry [ 2 ] == 1 : im = im . convert ( \"L\" ) # Set the rate of the population self . r = np . array ( im ) . reshape ( self . size ) / 255.","title":"set_image()"},{"location":"API/SpecificPopulation/#ANNarchy.extensions.image.ImagePopulation.VideoPopulation","text":"Specific rate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel). This extension requires the C++ library OpenCV >= 4.0 (apt-get/yum install opencv). pkg-config opencv4 --cflags --libs should not return an error. vtk might additionally have to be installed. Usage: from ANNarchy import * from ANNarchy.extensions.image import VideoPopulation pop = VideoPopulation ( geometry = ( 480 , 640 )) compile () pop . start_camera ( 0 ) while ( True ): pop . grab_image () simulate ( 10.0 ) About the geometry: If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image). If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color). If the third dimension is 3, each will correspond to the RGB values of the pixels. Warning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population.","title":"VideoPopulation"},{"location":"API/SpecificPopulation/#ANNarchy.extensions.image.ImagePopulation.VideoPopulation.__init__","text":"Parameters: Name Type Description Default geometry population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized. required opencv_version OpenCV version (default=4). '4' name unique name of the population (optional). None Source code in ANNarchy/extensions/image/ImagePopulation.py def __init__ ( self , geometry , opencv_version = \"4\" , name = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized. :param opencv_version: OpenCV version (default=4). :param name: unique name of the population (optional). \"\"\" # Create the population ImagePopulation . __init__ ( self , geometry = geometry , name = name , copied = copied ) self . opencv_version = opencv_version","title":"__init__()"},{"location":"API/SpecificPopulation/#ANNarchy.extensions.image.ImagePopulation.VideoPopulation.grab_image","text":"Grabs one image from the camera and feeds it into the population. The camera must be first started with: pop.start_camera(0) Source code in ANNarchy/extensions/image/ImagePopulation.py def grab_image ( self ): \"\"\" Grabs one image from the camera and feeds it into the population. The camera must be first started with: pop.start_camera(0) \"\"\" self . cyInstance . grab_image ()","title":"grab_image()"},{"location":"API/SpecificPopulation/#ANNarchy.extensions.image.ImagePopulation.VideoPopulation.release","text":"Releases the camera: pop.release() Source code in ANNarchy/extensions/image/ImagePopulation.py def release ( self ): \"\"\" Releases the camera: pop.release() \"\"\" self . cyInstance . release_camera ()","title":"release()"},{"location":"API/SpecificPopulation/#ANNarchy.extensions.image.ImagePopulation.VideoPopulation.start_camera","text":"Starts the webcam with the corresponding device (default = 0). On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc. Source code in ANNarchy/extensions/image/ImagePopulation.py def start_camera ( self , camera_port = 0 ): \"\"\" Starts the webcam with the corresponding device (default = 0). On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc. \"\"\" self . cyInstance . start_camera ( camera_port , self . geometry [ 1 ], self . geometry [ 0 ], 3 if self . dimension == 3 else 1 )","title":"start_camera()"},{"location":"API/SpecificProjection/","text":"Specific Projections # ANNarchy provides a set of predefined Projection objects to ease the definition of standard networks. CurrentInjection # Inject current from a rate-coded population into a spiking population. The pre-synaptic population must be be rate-coded, the post-synaptic one must be spiking, both must have the same size and no plasticity is allowed. For each post-synaptic neuron, the current g_target will be set at each time step to the firing rate r of the pre-synaptic neuron with the same rank. The projection must be connected with connect_current() , which takes no parameter and does not accept delays. It is equivalent to connect_one_to_one(weights=1) . Example: inp = Population ( 100 , Neuron ( equations = \"r = sin(t)\" )) pop = Population ( 100 , Izhikevich ) proj = CurrentInjection ( inp , pop , 'exc' ) proj . connect_current () __init__ ( self , pre , post , target , name = None , copied = False ) special # Parameters: Name Type Description Default pre pre-synaptic population. required post post-synaptic population. required target type of the connection. required Source code in ANNarchy/core/SpecificProjection.py def __init__ ( self , pre , post , target , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population. :param post: post-synaptic population. :param target: type of the connection. \"\"\" # Instantiate the projection SpecificProjection . __init__ ( self , pre , post , target , None , name , copied ) # Check populations if not self . pre . neuron_type . type == 'rate' : Global . _error ( 'The pre-synaptic population of a CurrentInjection must be rate-coded.' ) if not self . post . neuron_type . type == 'spike' : Global . _error ( 'The post-synaptic population of a CurrentInjection must be spiking.' ) if not self . post . size == self . pre . size : Global . _error ( 'CurrentInjection: The pre- and post-synaptic populations must have the same size.' ) if Global . _check_paradigm ( \"cuda\" ) and ( isinstance ( pre , PopulationView ) or isinstance ( post , PopulationView )): Global . _error ( \"CurrentInjection on GPUs is not allowed for PopulationViews\" ) # Prevent automatic split of matrices self . _no_split_matrix = True","title":"Specific Projections"},{"location":"API/SpecificProjection/#specific-projections","text":"ANNarchy provides a set of predefined Projection objects to ease the definition of standard networks.","title":"Specific Projections"},{"location":"API/SpecificProjection/#ANNarchy.core.SpecificProjection.CurrentInjection","text":"Inject current from a rate-coded population into a spiking population. The pre-synaptic population must be be rate-coded, the post-synaptic one must be spiking, both must have the same size and no plasticity is allowed. For each post-synaptic neuron, the current g_target will be set at each time step to the firing rate r of the pre-synaptic neuron with the same rank. The projection must be connected with connect_current() , which takes no parameter and does not accept delays. It is equivalent to connect_one_to_one(weights=1) . Example: inp = Population ( 100 , Neuron ( equations = \"r = sin(t)\" )) pop = Population ( 100 , Izhikevich ) proj = CurrentInjection ( inp , pop , 'exc' ) proj . connect_current ()","title":"CurrentInjection"},{"location":"API/SpecificProjection/#ANNarchy.core.SpecificProjection.CurrentInjection.__init__","text":"Parameters: Name Type Description Default pre pre-synaptic population. required post post-synaptic population. required target type of the connection. required Source code in ANNarchy/core/SpecificProjection.py def __init__ ( self , pre , post , target , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population. :param post: post-synaptic population. :param target: type of the connection. \"\"\" # Instantiate the projection SpecificProjection . __init__ ( self , pre , post , target , None , name , copied ) # Check populations if not self . pre . neuron_type . type == 'rate' : Global . _error ( 'The pre-synaptic population of a CurrentInjection must be rate-coded.' ) if not self . post . neuron_type . type == 'spike' : Global . _error ( 'The post-synaptic population of a CurrentInjection must be spiking.' ) if not self . post . size == self . pre . size : Global . _error ( 'CurrentInjection: The pre- and post-synaptic populations must have the same size.' ) if Global . _check_paradigm ( \"cuda\" ) and ( isinstance ( pre , PopulationView ) or isinstance ( post , PopulationView )): Global . _error ( \"CurrentInjection on GPUs is not allowed for PopulationViews\" ) # Prevent automatic split of matrices self . _no_split_matrix = True","title":"__init__()"},{"location":"API/SpecificSynapse/","text":"Built-in synapse types # ANNarchy provides standard spiking synapse models, similar to the ones defined in PyNN ( http://neuralensemble.org/docs/PyNN/reference/plasticitymodels.html ). Hebb # Rate-coded synapse with Hebbian plasticity. Parameters (global) : eta = 0.01 : learning rate. Learning rule : w : weight. dw/dt = eta * pre.r * post.r Equivalent code: Hebb = Synapse ( parameters = \"\"\" eta = 0.01 : projection \"\"\" , equations = \"\"\" dw/dt = eta * pre.r * post.r : min=0.0 \"\"\" ) Oja # Rate-coded synapse with regularized Hebbian plasticity (Oja). Parameters (global) : eta = 0.01 : learning rate. alpha = 1.0 : regularization constant. Learning rule : w : weight: dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) Equivalent code: Oja = Synapse ( parameters = \"\"\" eta = 0.01 : projection alpha = 1.0 : projection \"\"\" , equations = \"\"\" dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) : min=0.0 \"\"\" ) IBCM # Rate-coded synapse with Intrator & Cooper (1992) plasticity. Parameters (global) : eta = 0.01 : learning rate. tau = 2000.0 : time constant of the post-synaptic threshold. Learning rule : theta : post-synaptic threshold: tau * dtheta/dt + theta = post.r^2 w : weight: dw/dt = eta * post.r * (post.r - theta) * pre.r Equivalent code: IBCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 2000.0 : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit \"\"\" ) STP # Synapse exhibiting short-term facilitation and depression, implemented using the model of Tsodyks, Markram et al.: Tsodyks, Uziel and Markram (2000) Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses. Journal of Neuroscience 20:RC50 Note that the time constant of the post-synaptic current is set in the neuron model, not here. Parameters (global) : tau_rec = 100.0 : depression time constant (ms). tau_facil = 0.01 : facilitation time constant (ms). U = 0.5 : use parameter. Variables : x : recovery variable:: dx/dt = (1 - x)/tau_rec u : facilitation variable:: du/dt = (U - u)/tau_facil Both variables are integrated event-driven. Pre-spike events : g_target += w * u * x x *= (1 - u) u += U * (1 - u) Equivalent code: STP = Synapse ( parameters = \"\"\" tau_rec = 100.0 : projection tau_facil = 0.01 : projection U = 0.5 \"\"\" , equations = \"\"\" dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven du/dt = (U - u)/tau_facil : init = 0.5, event-driven \"\"\" , pre_spike = \"\"\" g_target += w * u * x x *= (1 - u) u += U * (1 - u) \"\"\" ) STDP # Spike-timing dependent plasticity. This is the online version of the STDP rule. Song, S., and Abbott, L.F. (2001). Cortical development and remapping through spike timing-dependent plasticity. Neuron 32, 339-350. Parameters (global) : tau_plus = 20.0 : time constant of the pre-synaptic trace (ms) tau_minus = 20.0 : time constant of the pre-synaptic trace (ms) A_plus = 0.01 : increase of the pre-synaptic trace after a spike. A_minus = 0.01 : decrease of the post-synaptic trace after a spike. w_min = 0.0 : minimal value of the weight w. w_max = 1.0 : maximal value of the weight w. Variables : x : pre-synaptic trace: tau_plus * dx/dt = -x y: post-synaptic trace: tau_minus * dy/dt = -y Both variables are evaluated event-driven. Pre-spike events : g_target += w x += A_plus * w_max w = clip(w + y, w_min , w_max) Post-spike events :: y -= A_minus * w_max w = clip(w + x, w_min , w_max) Equivalent code: STDP = Synapse ( parameters = \"\"\" tau_plus = 20.0 : projection tau_minus = 20.0 : projection A_plus = 0.01 : projection A_minus = 0.01 : projection w_min = 0.0 : projection w_max = 1.0 : projection \"\"\" , equations = \"\"\" tau_plus * dx/dt = -x : event-driven tau_minus * dy/dt = -y : event-driven \"\"\" , pre_spike = \"\"\" g_target += w x += A_plus * w_max w = clip(w + y, w_min , w_max) \"\"\" , post_spike = \"\"\" y -= A_minus * w_max w = clip(w + x, w_min , w_max) \"\"\" )","title":"Built-in synapse types"},{"location":"API/SpecificSynapse/#built-in-synapse-types","text":"ANNarchy provides standard spiking synapse models, similar to the ones defined in PyNN ( http://neuralensemble.org/docs/PyNN/reference/plasticitymodels.html ).","title":"Built-in synapse types"},{"location":"API/SpecificSynapse/#ANNarchy.models.Synapses.Hebb","text":"Rate-coded synapse with Hebbian plasticity. Parameters (global) : eta = 0.01 : learning rate. Learning rule : w : weight. dw/dt = eta * pre.r * post.r Equivalent code: Hebb = Synapse ( parameters = \"\"\" eta = 0.01 : projection \"\"\" , equations = \"\"\" dw/dt = eta * pre.r * post.r : min=0.0 \"\"\" )","title":"Hebb"},{"location":"API/SpecificSynapse/#ANNarchy.models.Synapses.Oja","text":"Rate-coded synapse with regularized Hebbian plasticity (Oja). Parameters (global) : eta = 0.01 : learning rate. alpha = 1.0 : regularization constant. Learning rule : w : weight: dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) Equivalent code: Oja = Synapse ( parameters = \"\"\" eta = 0.01 : projection alpha = 1.0 : projection \"\"\" , equations = \"\"\" dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) : min=0.0 \"\"\" )","title":"Oja"},{"location":"API/SpecificSynapse/#ANNarchy.models.Synapses.IBCM","text":"Rate-coded synapse with Intrator & Cooper (1992) plasticity. Parameters (global) : eta = 0.01 : learning rate. tau = 2000.0 : time constant of the post-synaptic threshold. Learning rule : theta : post-synaptic threshold: tau * dtheta/dt + theta = post.r^2 w : weight: dw/dt = eta * post.r * (post.r - theta) * pre.r Equivalent code: IBCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 2000.0 : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit \"\"\" )","title":"IBCM"},{"location":"API/SpecificSynapse/#ANNarchy.models.Synapses.STP","text":"Synapse exhibiting short-term facilitation and depression, implemented using the model of Tsodyks, Markram et al.: Tsodyks, Uziel and Markram (2000) Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses. Journal of Neuroscience 20:RC50 Note that the time constant of the post-synaptic current is set in the neuron model, not here. Parameters (global) : tau_rec = 100.0 : depression time constant (ms). tau_facil = 0.01 : facilitation time constant (ms). U = 0.5 : use parameter. Variables : x : recovery variable:: dx/dt = (1 - x)/tau_rec u : facilitation variable:: du/dt = (U - u)/tau_facil Both variables are integrated event-driven. Pre-spike events : g_target += w * u * x x *= (1 - u) u += U * (1 - u) Equivalent code: STP = Synapse ( parameters = \"\"\" tau_rec = 100.0 : projection tau_facil = 0.01 : projection U = 0.5 \"\"\" , equations = \"\"\" dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven du/dt = (U - u)/tau_facil : init = 0.5, event-driven \"\"\" , pre_spike = \"\"\" g_target += w * u * x x *= (1 - u) u += U * (1 - u) \"\"\" )","title":"STP"},{"location":"API/SpecificSynapse/#ANNarchy.models.Synapses.STDP","text":"Spike-timing dependent plasticity. This is the online version of the STDP rule. Song, S., and Abbott, L.F. (2001). Cortical development and remapping through spike timing-dependent plasticity. Neuron 32, 339-350. Parameters (global) : tau_plus = 20.0 : time constant of the pre-synaptic trace (ms) tau_minus = 20.0 : time constant of the pre-synaptic trace (ms) A_plus = 0.01 : increase of the pre-synaptic trace after a spike. A_minus = 0.01 : decrease of the post-synaptic trace after a spike. w_min = 0.0 : minimal value of the weight w. w_max = 1.0 : maximal value of the weight w. Variables : x : pre-synaptic trace: tau_plus * dx/dt = -x y: post-synaptic trace: tau_minus * dy/dt = -y Both variables are evaluated event-driven. Pre-spike events : g_target += w x += A_plus * w_max w = clip(w + y, w_min , w_max) Post-spike events :: y -= A_minus * w_max w = clip(w + x, w_min , w_max) Equivalent code: STDP = Synapse ( parameters = \"\"\" tau_plus = 20.0 : projection tau_minus = 20.0 : projection A_plus = 0.01 : projection A_minus = 0.01 : projection w_min = 0.0 : projection w_max = 1.0 : projection \"\"\" , equations = \"\"\" tau_plus * dx/dt = -x : event-driven tau_minus * dy/dt = -y : event-driven \"\"\" , pre_spike = \"\"\" g_target += w x += A_plus * w_max w = clip(w + y, w_min , w_max) \"\"\" , post_spike = \"\"\" y -= A_minus * w_max w = clip(w + x, w_min , w_max) \"\"\" )","title":"STDP"},{"location":"API/Synapse/","text":"Synapse class # The class Synapse is used to describe the behavior of a synapse (parameters, equations...). Synapse # Base class to define a synapse. __init__ ( self , parameters = '' , equations = '' , psp = None , operation = 'sum' , pre_spike = None , post_spike = None , pre_axon_spike = None , functions = None , pruning = None , creating = None , name = None , description = None , extra_values = {}) special # Parameters: Name Type Description Default parameters parameters of the neuron and their initial value. '' equations equations defining the temporal evolution of variables. '' psp continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r ). Synaptic transmission in spiking synapses occurs in pre_spike . None operation operation (sum, max, min, mean) performed by the post-synaptic neuron on the individual psp (rate-coded only, default=sum). 'sum' pre_spike updating of variables when a pre-synaptic spike is received (spiking only). None post_spike updating of variables when a post-synaptic spike is emitted (spiking only). None pre_axon_spike updating of variables when an axonal spike was emitted (spiking only, default None). The usage of this arguments prevents the application of learning rules. None functions additional functions used in the equations. None name name of the synapse type (used for reporting only). None description short description of the synapse type (used for reporting). None Source code in ANNarchy/core/Synapse.py def __init__ ( self , parameters = \"\" , equations = \"\" , psp = None , operation = 'sum' , pre_spike = None , post_spike = None , pre_axon_spike = None , functions = None , pruning = None , creating = None , name = None , description = None , extra_values = {} ): \"\"\" :param parameters: parameters of the neuron and their initial value. :param equations: equations defining the temporal evolution of variables. :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). Synaptic transmission in spiking synapses occurs in ``pre_spike``. :param operation: operation (sum, max, min, mean) performed by the post-synaptic neuron on the individual psp (rate-coded only, default=sum). :param pre_spike: updating of variables when a pre-synaptic spike is received (spiking only). :param post_spike: updating of variables when a post-synaptic spike is emitted (spiking only). :param pre_axon_spike: updating of variables when an axonal spike was emitted (spiking only, default None). The usage of this arguments prevents the application of learning rules. :param functions: additional functions used in the equations. :param name: name of the synapse type (used for reporting only). :param description: short description of the synapse type (used for reporting). \"\"\" # Store the parameters and equations self . parameters = parameters self . equations = equations self . functions = functions self . pre_spike = pre_spike self . post_spike = post_spike self . psp = psp self . pre_axon_spike = pre_axon_spike self . operation = operation self . extra_values = extra_values self . pruning = pruning self . creating = creating # Type of the synapse TODO: smarter self . type = 'spike' if pre_spike else 'rate' # Check the operation if self . type == 'spike' and self . operation != 'sum' : Global . _error ( 'Spiking synapses can only perform a sum of presynaptic potentials.' ) if not self . operation in [ 'sum' , 'min' , 'max' , 'mean' ]: Global . _error ( 'The only operations permitted are: sum (default), min, max, mean.' ) # Sanity check if self . pre_axon_spike and self . post_spike : Global . _error ( \"The usage of axonal spike events is currently not allowed for plastic connections.\" ) if ( self . pruning or self . creating ) and not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), pruning or creating statements in Synapse() would be without effect.' ) # Description self . description = None # Reporting if not hasattr ( self , '_instantiated' ) : # User-defined Global . _objects [ 'synapses' ] . append ( self ) elif len ( self . _instantiated ) == 0 : # First instantiation of the class Global . _objects [ 'synapses' ] . append ( self ) self . _rk_synapses_type = len ( Global . _objects [ 'synapses' ]) if name : self . name = name else : self . name = self . _default_names [ self . type ] if description : self . short_description = description else : if self . type == 'spike' : self . short_description = \"User-defined spiking synapse.\" else : self . short_description = \"User-defined rate-coded synapse.\"","title":"Synapse class"},{"location":"API/Synapse/#synapse-class","text":"The class Synapse is used to describe the behavior of a synapse (parameters, equations...).","title":"Synapse class"},{"location":"API/Synapse/#ANNarchy.core.Synapse.Synapse","text":"Base class to define a synapse.","title":"Synapse"},{"location":"API/Synapse/#ANNarchy.core.Synapse.Synapse.__init__","text":"Parameters: Name Type Description Default parameters parameters of the neuron and their initial value. '' equations equations defining the temporal evolution of variables. '' psp continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r ). Synaptic transmission in spiking synapses occurs in pre_spike . None operation operation (sum, max, min, mean) performed by the post-synaptic neuron on the individual psp (rate-coded only, default=sum). 'sum' pre_spike updating of variables when a pre-synaptic spike is received (spiking only). None post_spike updating of variables when a post-synaptic spike is emitted (spiking only). None pre_axon_spike updating of variables when an axonal spike was emitted (spiking only, default None). The usage of this arguments prevents the application of learning rules. None functions additional functions used in the equations. None name name of the synapse type (used for reporting only). None description short description of the synapse type (used for reporting). None Source code in ANNarchy/core/Synapse.py def __init__ ( self , parameters = \"\" , equations = \"\" , psp = None , operation = 'sum' , pre_spike = None , post_spike = None , pre_axon_spike = None , functions = None , pruning = None , creating = None , name = None , description = None , extra_values = {} ): \"\"\" :param parameters: parameters of the neuron and their initial value. :param equations: equations defining the temporal evolution of variables. :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). Synaptic transmission in spiking synapses occurs in ``pre_spike``. :param operation: operation (sum, max, min, mean) performed by the post-synaptic neuron on the individual psp (rate-coded only, default=sum). :param pre_spike: updating of variables when a pre-synaptic spike is received (spiking only). :param post_spike: updating of variables when a post-synaptic spike is emitted (spiking only). :param pre_axon_spike: updating of variables when an axonal spike was emitted (spiking only, default None). The usage of this arguments prevents the application of learning rules. :param functions: additional functions used in the equations. :param name: name of the synapse type (used for reporting only). :param description: short description of the synapse type (used for reporting). \"\"\" # Store the parameters and equations self . parameters = parameters self . equations = equations self . functions = functions self . pre_spike = pre_spike self . post_spike = post_spike self . psp = psp self . pre_axon_spike = pre_axon_spike self . operation = operation self . extra_values = extra_values self . pruning = pruning self . creating = creating # Type of the synapse TODO: smarter self . type = 'spike' if pre_spike else 'rate' # Check the operation if self . type == 'spike' and self . operation != 'sum' : Global . _error ( 'Spiking synapses can only perform a sum of presynaptic potentials.' ) if not self . operation in [ 'sum' , 'min' , 'max' , 'mean' ]: Global . _error ( 'The only operations permitted are: sum (default), min, max, mean.' ) # Sanity check if self . pre_axon_spike and self . post_spike : Global . _error ( \"The usage of axonal spike events is currently not allowed for plastic connections.\" ) if ( self . pruning or self . creating ) and not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), pruning or creating statements in Synapse() would be without effect.' ) # Description self . description = None # Reporting if not hasattr ( self , '_instantiated' ) : # User-defined Global . _objects [ 'synapses' ] . append ( self ) elif len ( self . _instantiated ) == 0 : # First instantiation of the class Global . _objects [ 'synapses' ] . append ( self ) self . _rk_synapses_type = len ( Global . _objects [ 'synapses' ]) if name : self . name = name else : self . name = self . _default_names [ self . type ] if description : self . short_description = description else : if self . type == 'spike' : self . short_description = \"User-defined spiking synapse.\" else : self . short_description = \"User-defined rate-coded synapse.\"","title":"__init__()"},{"location":"API/Utilities/","text":"Reporting # report ( filename = './report.tex' , standalone = True , gather_subprojections = False , title = None , author = None , date = None , net_id = 0 ) # Generates a report describing the network. If the filename ends with .tex , the TeX file is generated according to: Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8): e1000456. If the filename ends with .md , a (more complete) Markdown file is generated, which can be converted to pdf or html by pandoc :: pandoc report.md -sSN -V geometry:margin=1in -o report.pdf pandoc report.md -sSN -o report.html Parameters: Name Type Description Default filename name of the file where the report will be written (default: \"./report.tex\") './report.tex' standalone tells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown. True gather_subprojections if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False). False title title of the document (Markdown only) None author author of the document (Markdown only) None date date of the document (Markdown only) None net_id id of the network to be used for reporting (default: 0, everything that was declared) 0 Source code in ANNarchy/parser/report/Report.py def report ( filename = \"./report.tex\" , standalone = True , gather_subprojections = False , title = None , author = None , date = None , net_id = 0 ): \"\"\" Generates a report describing the network. If the filename ends with ``.tex``, the TeX file is generated according to: Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8): e1000456. If the filename ends with ``.md``, a (more complete) Markdown file is generated, which can be converted to pdf or html by ``pandoc``:: pandoc report.md -sSN -V geometry:margin=1in -o report.pdf pandoc report.md -sSN -o report.html :param filename: name of the file where the report will be written (default: \"./report.tex\") :param standalone: tells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown. :param gather_subprojections: if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False). :param title: title of the document (Markdown only) :param author: author of the document (Markdown only) :param date: date of the document (Markdown only) :param net_id: id of the network to be used for reporting (default: 0, everything that was declared) \"\"\" if filename . endswith ( '.tex' ): from .LatexReport import report_latex report_latex ( filename , standalone , gather_subprojections , net_id ) elif filename . endswith ( '.md' ): from .MarkdownReport import report_markdown report_markdown ( filename , standalone , gather_subprojections , title , author , date , net_id ) else : _error ( 'report(): the filename must end with .tex or .md.' )","title":"Reporting"},{"location":"API/Utilities/#reporting","text":"","title":"Reporting"},{"location":"API/Utilities/#ANNarchy.parser.report.Report.report","text":"Generates a report describing the network. If the filename ends with .tex , the TeX file is generated according to: Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8): e1000456. If the filename ends with .md , a (more complete) Markdown file is generated, which can be converted to pdf or html by pandoc :: pandoc report.md -sSN -V geometry:margin=1in -o report.pdf pandoc report.md -sSN -o report.html Parameters: Name Type Description Default filename name of the file where the report will be written (default: \"./report.tex\") './report.tex' standalone tells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown. True gather_subprojections if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False). False title title of the document (Markdown only) None author author of the document (Markdown only) None date date of the document (Markdown only) None net_id id of the network to be used for reporting (default: 0, everything that was declared) 0 Source code in ANNarchy/parser/report/Report.py def report ( filename = \"./report.tex\" , standalone = True , gather_subprojections = False , title = None , author = None , date = None , net_id = 0 ): \"\"\" Generates a report describing the network. If the filename ends with ``.tex``, the TeX file is generated according to: Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8): e1000456. If the filename ends with ``.md``, a (more complete) Markdown file is generated, which can be converted to pdf or html by ``pandoc``:: pandoc report.md -sSN -V geometry:margin=1in -o report.pdf pandoc report.md -sSN -o report.html :param filename: name of the file where the report will be written (default: \"./report.tex\") :param standalone: tells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown. :param gather_subprojections: if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False). :param title: title of the document (Markdown only) :param author: author of the document (Markdown only) :param date: date of the document (Markdown only) :param net_id: id of the network to be used for reporting (default: 0, everything that was declared) \"\"\" if filename . endswith ( '.tex' ): from .LatexReport import report_latex report_latex ( filename , standalone , gather_subprojections , net_id ) elif filename . endswith ( '.md' ): from .MarkdownReport import report_markdown report_markdown ( filename , standalone , gather_subprojections , title , author , date , net_id ) else : _error ( 'report(): the filename must end with .tex or .md.' )","title":"report()"},{"location":"example/BarLearning/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Bar Learning problem # The implementation of the bar learning problem is located in the examples/bar_learning folder. The bar learning problem describes the process of learning receptive fields on an artificial input pattern. Images consisting of independent bars are used. Those images are generated as following: an 8*8 image can filled randomly by eight horizontal or vertical bars, with a probability of 1/8 for each. These input images are fed into a neural population, whose neurons should learn to extract the independent components of the input distribution, namely single horizontal or vertical bars. If you have pyqtgraph installed, you can simply try the network by typing: python BarLearning.py Model overview # The model consists of two populations Input and Feature . The size of Input should be chosen to fit the input image size (here 8*8). The number of neurons in the Feature population should be higher than the total number of independent bars (16, we choose here 32 neurons). The Feature population gets excitory connections from Input through an all-to-all connection pattern. The same pattern is used for the inhibitory connections within Feature . Defining the neurons and populations # from ANNarchy import * clear () ANNarchy 4.7 (4.7.0) on linux (posix). Input population: The input pattern will be clamped into this population by the main loop for every trial, so we need just an empty neuron at this point: InputNeuron = Neuron ( parameters = \"r = 0.0\" ) The trick here is to declare r as a parameter, not a variable: its value will not be computed by the simulator, but only set by external inputs. The Input population can then be created: Input = Population ( geometry = ( 8 , 8 ), neuron = InputNeuron ) Feature population: The neuron type composing this population sums up all the excitory inputs gain from Input and the lateral inhibition within Feature . \\[\\tau \\frac {dr_{j}^{\\text{Feature}}}{dt} + r_{j}^{Feature} = \\sum_{i} w_{ij} \\cdot r_{i}^{\\text{Input}} - \\sum_{k, k \\ne j} w_{kj} * r_{k}^{Feature}\\] could be implemented as the following: LeakyNeuron = Neuron ( parameters = \"\"\" tau = 10.0 : population \"\"\" , equations = \"\"\" tau * dr/dt + r = sum(exc) - sum(inh) : min=0.0 \"\"\" ) The firing rate is restricted to positive values with the min=0.0 flag. The population is created in the following way: Feature = Population ( geometry = ( 8 , 4 ), neuron = LeakyNeuron ) We give it a (8, 4) geometry for visualization only, it does not influence computations at all. Defining the synapse and projections # Both feedforward ( Input \\(\\rightarrow\\) Feature ) and lateral ( Feature \\(\\rightarrow\\) Feature ) projections are learned using the Oja learning rule (a regularized Hebbian learning rule ensuring the sum of all weights coming to a neuron is constant). Only some parameters will differ between the projections. \\[\\tau \\frac{dw_{ij}}{dt} = r_{i} * r_{j} - \\alpha * r_{j}^{2} * w_{ij}\\] where \\(\\alpha\\) is a parameter defining the strength of the regularization, \\(r_i\\) is the pre-synaptic firing rate and \\(r_j\\) the post-synaptic one. The implementation of this synapse type is straightforward: Oja = Synapse ( parameters = \"\"\" tau = 2000.0 : postsynaptic alpha = 8.0 : postsynaptic min_w = 0.0 : postsynaptic \"\"\" , equations = \"\"\" tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=min_w \"\"\" ) For this network we need to create two projections, one excitory between the populations Input and Feature and one inhibitory within the Feature population itself: ff = Projection ( pre = Input , post = Feature , target = 'exc' , synapse = Oja ) ff . connect_all_to_all ( weights = Uniform ( - 0.5 , 0.5 )) lat = Projection ( pre = Feature , post = Feature , target = 'inh' , synapse = Oja ) lat . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) <ANNarchy.core.Projection.Projection at 0x7fe444220a00> The two projections are all-to-all and use the Oja synapse type. They only differ by the parameter alpha (lower in lat ) and the fact that the weights of ff are allowed to be negative (so we set the minimum value to -10.0): ff . min_w = - 10.0 lat . alpha = 0.3 Setting inputs # Once the network is defined, one has to specify how inputs are fed into the Input population. A simple solution is to define a method that sets the firing rate of Input according to the specified probabilities every time it is called, and runs the simulation for 50 ms: def trial (): # Reset the firing rate for all neurons Input . r = 0.0 # Clamp horizontal bars randomly for h in range ( Input . geometry [ 0 ]): if np . random . random () < 1.0 / float ( Input . geometry [ 0 ]): Input [ h , :] . r = 1.0 # Clamp vertical bars randomly for w in range ( Input . geometry [ 1 ]): if np . random . random () < 1.0 / float ( Input . geometry [ 1 ]): Input [:, w ] . r = 1.0 # Simulate for 50ms simulate ( 50. ) # Return firing rates and receptive fields for visualization return Input . r , Feature . r , ff . receptive_fields () One can use here a single value or a Numpy array (e.g. np.zeros(Input.geometry)) ) to reset activity in Input, it does not matter. For all possible horizontal bars, a decision is then made whether the bar should appear or not, in which case the firing rate of the correspondng neurons is set to 1.0: for h in range ( Input . geometry [ 0 ]): if np . random . random () < 1.0 / float ( Input . geometry [ 0 ]): Input [ h , :] . r = 1.0 Input[h, :] is a PopulationView, i.e. a group of neurons defined by the sub-indices (here the row of index h ). Their attributes, such as r , can be accessed as if it were a regular population. The same is done for vertical bars. Running the simulation # Once the method for setting inputs is defined, the simulation can be started. A basic approach would be to define a for loop where the trial() method is called repetitively: compile () for t in range ( 1000 ): input_r , feature_r , weights = trial () Compiling ... OK import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 131 ) plt . imshow ( input_r . T , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . title ( 'Input' ) plt . subplot ( 132 ) plt . imshow ( feature_r . T , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . title ( 'Feature' ) plt . subplot ( 133 ) plt . imshow ( weights . T , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . title ( 'Receptive fields' ) plt . show () In the file BarLearning.py , a visualization class using pyqtgraph is imported from Viz.py , but the user is free to use whatever method he prefers to visualize the result of learning. from Viz import Viewer view = Viewer ( func = trial ) view . run ()","title":"Bar Learning"},{"location":"example/BarLearning/#bar-learning-problem","text":"The implementation of the bar learning problem is located in the examples/bar_learning folder. The bar learning problem describes the process of learning receptive fields on an artificial input pattern. Images consisting of independent bars are used. Those images are generated as following: an 8*8 image can filled randomly by eight horizontal or vertical bars, with a probability of 1/8 for each. These input images are fed into a neural population, whose neurons should learn to extract the independent components of the input distribution, namely single horizontal or vertical bars. If you have pyqtgraph installed, you can simply try the network by typing: python BarLearning.py","title":"Bar Learning problem"},{"location":"example/BarLearning/#model-overview","text":"The model consists of two populations Input and Feature . The size of Input should be chosen to fit the input image size (here 8*8). The number of neurons in the Feature population should be higher than the total number of independent bars (16, we choose here 32 neurons). The Feature population gets excitory connections from Input through an all-to-all connection pattern. The same pattern is used for the inhibitory connections within Feature .","title":"Model overview"},{"location":"example/BarLearning/#defining-the-neurons-and-populations","text":"from ANNarchy import * clear () ANNarchy 4.7 (4.7.0) on linux (posix). Input population: The input pattern will be clamped into this population by the main loop for every trial, so we need just an empty neuron at this point: InputNeuron = Neuron ( parameters = \"r = 0.0\" ) The trick here is to declare r as a parameter, not a variable: its value will not be computed by the simulator, but only set by external inputs. The Input population can then be created: Input = Population ( geometry = ( 8 , 8 ), neuron = InputNeuron ) Feature population: The neuron type composing this population sums up all the excitory inputs gain from Input and the lateral inhibition within Feature . \\[\\tau \\frac {dr_{j}^{\\text{Feature}}}{dt} + r_{j}^{Feature} = \\sum_{i} w_{ij} \\cdot r_{i}^{\\text{Input}} - \\sum_{k, k \\ne j} w_{kj} * r_{k}^{Feature}\\] could be implemented as the following: LeakyNeuron = Neuron ( parameters = \"\"\" tau = 10.0 : population \"\"\" , equations = \"\"\" tau * dr/dt + r = sum(exc) - sum(inh) : min=0.0 \"\"\" ) The firing rate is restricted to positive values with the min=0.0 flag. The population is created in the following way: Feature = Population ( geometry = ( 8 , 4 ), neuron = LeakyNeuron ) We give it a (8, 4) geometry for visualization only, it does not influence computations at all.","title":"Defining the neurons and populations"},{"location":"example/BarLearning/#defining-the-synapse-and-projections","text":"Both feedforward ( Input \\(\\rightarrow\\) Feature ) and lateral ( Feature \\(\\rightarrow\\) Feature ) projections are learned using the Oja learning rule (a regularized Hebbian learning rule ensuring the sum of all weights coming to a neuron is constant). Only some parameters will differ between the projections. \\[\\tau \\frac{dw_{ij}}{dt} = r_{i} * r_{j} - \\alpha * r_{j}^{2} * w_{ij}\\] where \\(\\alpha\\) is a parameter defining the strength of the regularization, \\(r_i\\) is the pre-synaptic firing rate and \\(r_j\\) the post-synaptic one. The implementation of this synapse type is straightforward: Oja = Synapse ( parameters = \"\"\" tau = 2000.0 : postsynaptic alpha = 8.0 : postsynaptic min_w = 0.0 : postsynaptic \"\"\" , equations = \"\"\" tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=min_w \"\"\" ) For this network we need to create two projections, one excitory between the populations Input and Feature and one inhibitory within the Feature population itself: ff = Projection ( pre = Input , post = Feature , target = 'exc' , synapse = Oja ) ff . connect_all_to_all ( weights = Uniform ( - 0.5 , 0.5 )) lat = Projection ( pre = Feature , post = Feature , target = 'inh' , synapse = Oja ) lat . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) <ANNarchy.core.Projection.Projection at 0x7fe444220a00> The two projections are all-to-all and use the Oja synapse type. They only differ by the parameter alpha (lower in lat ) and the fact that the weights of ff are allowed to be negative (so we set the minimum value to -10.0): ff . min_w = - 10.0 lat . alpha = 0.3","title":"Defining the synapse and projections"},{"location":"example/BarLearning/#setting-inputs","text":"Once the network is defined, one has to specify how inputs are fed into the Input population. A simple solution is to define a method that sets the firing rate of Input according to the specified probabilities every time it is called, and runs the simulation for 50 ms: def trial (): # Reset the firing rate for all neurons Input . r = 0.0 # Clamp horizontal bars randomly for h in range ( Input . geometry [ 0 ]): if np . random . random () < 1.0 / float ( Input . geometry [ 0 ]): Input [ h , :] . r = 1.0 # Clamp vertical bars randomly for w in range ( Input . geometry [ 1 ]): if np . random . random () < 1.0 / float ( Input . geometry [ 1 ]): Input [:, w ] . r = 1.0 # Simulate for 50ms simulate ( 50. ) # Return firing rates and receptive fields for visualization return Input . r , Feature . r , ff . receptive_fields () One can use here a single value or a Numpy array (e.g. np.zeros(Input.geometry)) ) to reset activity in Input, it does not matter. For all possible horizontal bars, a decision is then made whether the bar should appear or not, in which case the firing rate of the correspondng neurons is set to 1.0: for h in range ( Input . geometry [ 0 ]): if np . random . random () < 1.0 / float ( Input . geometry [ 0 ]): Input [ h , :] . r = 1.0 Input[h, :] is a PopulationView, i.e. a group of neurons defined by the sub-indices (here the row of index h ). Their attributes, such as r , can be accessed as if it were a regular population. The same is done for vertical bars.","title":"Setting inputs"},{"location":"example/BarLearning/#running-the-simulation","text":"Once the method for setting inputs is defined, the simulation can be started. A basic approach would be to define a for loop where the trial() method is called repetitively: compile () for t in range ( 1000 ): input_r , feature_r , weights = trial () Compiling ... OK import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 131 ) plt . imshow ( input_r . T , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . title ( 'Input' ) plt . subplot ( 132 ) plt . imshow ( feature_r . T , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . title ( 'Feature' ) plt . subplot ( 133 ) plt . imshow ( weights . T , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . title ( 'Receptive fields' ) plt . show () In the file BarLearning.py , a visualization class using pyqtgraph is imported from Viz.py , but the user is free to use whatever method he prefers to visualize the result of learning. from Viz import Viewer view = Viewer ( func = trial ) view . run ()","title":"Running the simulation"},{"location":"example/BasalGanglia/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Logging with tensorboard # The tensorboard extension allows to log various information (scalars, images, etc) during training for visualization using tensorboard . It has to be explicitly imported: from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger import matplotlib.pyplot as plt ANNarchy 4.7 (4.7.0) on linux (posix). As it is just for demonstration purposes, we will be an extremely simplified model of the basal ganglia learning to solve through reinforcement learning a stimulus-response task with 4 stimuli and 2 responses (left and right). The two first stimuli should be responded with left, the two others with right. stimuli = [ ([ 1 , 0 , 0 , 0 ], 0 ), # A : left ([ 0 , 1 , 0 , 0 ], 0 ), # B : left ([ 0 , 0 , 1 , 0 ], 1 ), # C : right ([ 0 , 0 , 0 , 1 ], 1 ), # D : right ] We keep here the model as simple as possible. It is inspired from the rate-coded model described here: Vitay J, Hamker FH. 2010. A computational model of Basal Ganglia and its role in memory retrieval in rewarded visual memory tasks. Frontiers in computational neuroscience 4. doi:10.3389/fncom.2010.00013 The input population is composed of 4 static neurons to represent the inputs: cortex = Population ( 4 , Neuron ( parameters = \"r=0.0\" )) The cortex projects on the striatum, which is composed of 10 neurons integrating excitatory and inhibitory inputs: msn = Neuron ( parameters = \"tau = 10.0 : population; noise = 0.1 : population\" , equations = \"\"\" tau*dv/dt + v = sum(exc) - sum(inh) + noise * Uniform(-1, 1) r = clip(v, 0.0, 1.0) \"\"\" ) striatum = Population ( 10 , msn ) The striatum projects inhibitorily on GPi, whose neurons are tonically active (high baseline). Normally, GPi would project on the thalamus and back to the cortex, but here we read the output of the network directly in GPi: if the first neuron (corresponding to the left action) is less active than the second neuron, the selected action is left. gp_neuron = Neuron ( parameters = \"tau = 10.0 : population; B = 1.0\" , equations = \"tau*dv/dt + v = B - sum(inh); r= pos(v)\" ) gpi = Population ( 2 , gp_neuron ) Learning occurs at the cortico-striatal synapses, using a reward-modulated Hebbian learning rule, with Oja regularization: corticostriatal = Synapse ( parameters = \"\"\" eta = 0.1 : projection alpha = 0.5 : projection dopamine = 0.0 : projection\"\"\" , equations = \"w += eta*(dopamine * pre.r * post.r - alpha*w*post.r*post.r) : min=0.0\" ) cx_str = Projection ( cortex , striatum , \"exc\" , corticostriatal ) cx_str . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) <ANNarchy.core.Projection.Projection at 0x7f2bcff37880> Some lateral competition between the striatal neurons: str_str = Projection ( striatum , striatum , \"inh\" ) str_str . connect_all_to_all ( weights = 0.6 ) <ANNarchy.core.Projection.Projection at 0x7f2bcff37c70> One half of the striatal population is connected to the left GPi neuron, the other half to the right neuron: str_gpi1 = Projection ( striatum [: int ( striatum . size / 2 )], gpi [ 0 ], 'inh' ) . connect_all_to_all ( 1.0 ) str_gpi2 = Projection ( striatum [ int ( striatum . size / 2 ):], gpi [ 1 ], 'inh' ) . connect_all_to_all ( 1.0 ) We add a monitor on GPi and compile: m = Monitor ( gpi , 'r' ) compile () Compiling ... OK Each trial is very simple: we get a stimulus x from the stimuli array and a correct response t , reset the network for 40 ms, set the input and simulate for 50 ms, observe the activity in GPi to decide what the answer of the network is, provide reward accordingly to the corticostriatal projection and let learn for 10 ms. Here the \"dopamine\" signal is directly the reward (+1 for success, -1 for failure), not the reward prediction error, but it is just for demonstration. def training_trial ( x , t ): # Delay period cortex . r = 0.0 cx_str . dopamine = 0.0 simulate ( 40.0 ) # Set inputs cortex . r = np . array ( x ) simulate ( 50.0 ) # Read output output = gpi . r answer = np . argmin ( output ) # Provide reward reward = 1.0 if answer == t else - 1.0 cx_str . dopamine = reward simulate ( 10.0 ) # Get recordings data = m . get ( 'r' ) return reward , data The whole training procedure will simply iterate over the four stimuli for 100 trials: for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) We use the Logger class of the tensorboard extension to keep track of various data: with Logger () as logger : for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) # Log data... Note that it would be equivalent to manually close the Logger after training: logger = Logger () for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) # Log data... logger . close () We log here different quantities, just to demonstrate the different methods of the Logger class: The reward received after each trial: logger . add_scalar ( \"Reward\" , reward , trial ) The tag \"Reward\" will be the name of the plot in tensorboard. reward is the value that will be displayed, while trial is the index of the current trial (x-axis). The activity of the two GPi cells at the end of the trial, in separate plots depending on the stimulus: if trial % len ( stimuli ) == 0 : label = \"GPi activity/A\" elif trial % len ( stimuli ) == 1 : label = \"GPi activity/B\" elif trial % len ( stimuli ) == 2 : label = \"GPi activity/C\" elif trial % len ( stimuli ) == 3 : label = \"GPi activity/D\" logger . add_scalars ( label , { \"Left neuron\" : gpi . r [ 0 ], \"Right neuron\" : gpi . r [ 1 ]}, trial ) The four plots will be grouped under the label \"GPi activity\", with a title A, B, C or D. Note that add_scalars() requires a dictionary of values that will plot together. The activity in the striatum as a 2*5 image: logger . add_image ( \"Activity/Striatum\" , striatum . r . reshape (( 2 , 5 )), trial ) The activity should be reshaped to the correct dimensions. Note that activity in the striatum is bounded between 0 and 1, so there is no need for equalization. An histogram of the preference for the stimuli A and B of striatal cells: w = np . array ( cx_str . w ) logger . add_histogram ( \"Cortico-striatal weights/Left - AB/CD\" , np . mean ( w [: 5 , : 2 ] - w [: 5 , 2 :], axis = 1 ), trial ) logger . add_histogram ( \"Cortico-striatal weights/Right - AB/CD\" , np . mean ( w [ 5 :, : 2 ] - w [ 5 :, 2 :], axis = 1 ), trial ) We make here two plots, one for the first 5 striatal cells, the other for the rest. We plot the difference between the mean weights of each cell for the stimuli A and B, and the mean weights for the stimuli C and D. If learning goes well, the first five striatal cells should have stronger weights for A and B than for C and D, as they project to the left GPi cell. A matplotlib figure showing the time course of the two GPi cells (as recorded by the monitor): fig = plt . figure ( figsize = ( 10 , 8 )) plt . plot ( data [:, 0 ], label = \"left\" ) plt . plot ( data [:, 1 ], label = \"right\" ) plt . legend () logger . add_figure ( \"Activity/GPi\" , fig , trial ) Note that the figure will be automatically closed by the logger, no need to call show() . Logging figures is extremely slow, use that feature wisely. By default, the logs are saved in the subfolder runs/ , but this can be changed when creating the Logger: with Logger ( \"/tmp/experiment\" ) as logger : Each run of the network will be saved in this folder. You may want to delete the folder before each run, in order to only visualize the last run: % rm - rf runs with Logger () as logger : for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) # Log received rewards logger . add_scalar ( \"Reward\" , reward , trial ) # Log outputs depending on the task if trial % len ( stimuli ) == 0 : label = \"GPi activity/A\" elif trial % len ( stimuli ) == 1 : label = \"GPi activity/B\" elif trial % len ( stimuli ) == 2 : label = \"GPi activity/C\" elif trial % len ( stimuli ) == 3 : label = \"GPi activity/D\" logger . add_scalars ( label , { \"Left neuron\" : gpi . r [ 0 ], \"Right neuron\" : gpi . r [ 1 ]}, trial ) # Log striatal activity as a 2*5 image logger . add_image ( \"Activity/Striatum\" , striatum . r . reshape (( 2 , 5 )), trial ) # Log histogram of cortico-striatal weights w = np . array ( cx_str . w ) logger . add_histogram ( \"Cortico-striatal weights/Left - AB/CD\" , np . mean ( w [: 5 , : 2 ] - w [: 5 , 2 :], axis = 1 ), trial ) logger . add_histogram ( \"Cortico-striatal weights/Right - AB/CD\" , np . mean ( w [ 5 :, : 2 ] - w [ 5 :, 2 :], axis = 1 ), trial ) # Log matplotlib figure of GPi activity fig = plt . figure ( figsize = ( 10 , 8 )) plt . plot ( data [:, 0 ], label = \"left\" ) plt . plot ( data [:, 1 ], label = \"right\" ) plt . legend () logger . add_figure ( \"Activity/GPi\" , fig , trial ) Logging in runs/Sep29_10-58-54_machine You can now visualize the logged information by running tensorboard in a separate terminal and opening the corresponding page: tensorboard --logdir runs or directly in the notebook if you have the tensorboard extension installed: % load_ext tensorboard % tensorboard -- logdir runs -- samples_per_plugin images = 100 You should see a tensorboard page with four tabs Scalars, Images, Distributions and Histograms: The Reward plot shows that the network successfully learns to solve the task, as it consistently gets rewards of +1 (note that this may vary from run to run, depending on weight initialization): The GPi activity tab shows that the two GPi cells quickly learn to be inhibited for the right stimuli. In the Images tab, the plot for the striatum allows to visualize activity at the end of each rtial, showing that only one cell in the correct subpopulation is active: The matplotlib figure for the GPi activity shows what happens during a trial, especially at the end of the reset period: In the histograms tab, we can see that the left striatal population has acquired a preference (stronger weights) for the stimuli A and B, as the values are positive. The right population has negative values, so the neurons have stronger weights to the stimuli C and D. Note that some neurons in the right population still have stronger weights from A and B, but they are probably inhibited by the left population, so they do not impair performance.","title":"Basal Ganglia"},{"location":"example/BasalGanglia/#logging-with-tensorboard","text":"The tensorboard extension allows to log various information (scalars, images, etc) during training for visualization using tensorboard . It has to be explicitly imported: from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger import matplotlib.pyplot as plt ANNarchy 4.7 (4.7.0) on linux (posix). As it is just for demonstration purposes, we will be an extremely simplified model of the basal ganglia learning to solve through reinforcement learning a stimulus-response task with 4 stimuli and 2 responses (left and right). The two first stimuli should be responded with left, the two others with right. stimuli = [ ([ 1 , 0 , 0 , 0 ], 0 ), # A : left ([ 0 , 1 , 0 , 0 ], 0 ), # B : left ([ 0 , 0 , 1 , 0 ], 1 ), # C : right ([ 0 , 0 , 0 , 1 ], 1 ), # D : right ] We keep here the model as simple as possible. It is inspired from the rate-coded model described here: Vitay J, Hamker FH. 2010. A computational model of Basal Ganglia and its role in memory retrieval in rewarded visual memory tasks. Frontiers in computational neuroscience 4. doi:10.3389/fncom.2010.00013 The input population is composed of 4 static neurons to represent the inputs: cortex = Population ( 4 , Neuron ( parameters = \"r=0.0\" )) The cortex projects on the striatum, which is composed of 10 neurons integrating excitatory and inhibitory inputs: msn = Neuron ( parameters = \"tau = 10.0 : population; noise = 0.1 : population\" , equations = \"\"\" tau*dv/dt + v = sum(exc) - sum(inh) + noise * Uniform(-1, 1) r = clip(v, 0.0, 1.0) \"\"\" ) striatum = Population ( 10 , msn ) The striatum projects inhibitorily on GPi, whose neurons are tonically active (high baseline). Normally, GPi would project on the thalamus and back to the cortex, but here we read the output of the network directly in GPi: if the first neuron (corresponding to the left action) is less active than the second neuron, the selected action is left. gp_neuron = Neuron ( parameters = \"tau = 10.0 : population; B = 1.0\" , equations = \"tau*dv/dt + v = B - sum(inh); r= pos(v)\" ) gpi = Population ( 2 , gp_neuron ) Learning occurs at the cortico-striatal synapses, using a reward-modulated Hebbian learning rule, with Oja regularization: corticostriatal = Synapse ( parameters = \"\"\" eta = 0.1 : projection alpha = 0.5 : projection dopamine = 0.0 : projection\"\"\" , equations = \"w += eta*(dopamine * pre.r * post.r - alpha*w*post.r*post.r) : min=0.0\" ) cx_str = Projection ( cortex , striatum , \"exc\" , corticostriatal ) cx_str . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) <ANNarchy.core.Projection.Projection at 0x7f2bcff37880> Some lateral competition between the striatal neurons: str_str = Projection ( striatum , striatum , \"inh\" ) str_str . connect_all_to_all ( weights = 0.6 ) <ANNarchy.core.Projection.Projection at 0x7f2bcff37c70> One half of the striatal population is connected to the left GPi neuron, the other half to the right neuron: str_gpi1 = Projection ( striatum [: int ( striatum . size / 2 )], gpi [ 0 ], 'inh' ) . connect_all_to_all ( 1.0 ) str_gpi2 = Projection ( striatum [ int ( striatum . size / 2 ):], gpi [ 1 ], 'inh' ) . connect_all_to_all ( 1.0 ) We add a monitor on GPi and compile: m = Monitor ( gpi , 'r' ) compile () Compiling ... OK Each trial is very simple: we get a stimulus x from the stimuli array and a correct response t , reset the network for 40 ms, set the input and simulate for 50 ms, observe the activity in GPi to decide what the answer of the network is, provide reward accordingly to the corticostriatal projection and let learn for 10 ms. Here the \"dopamine\" signal is directly the reward (+1 for success, -1 for failure), not the reward prediction error, but it is just for demonstration. def training_trial ( x , t ): # Delay period cortex . r = 0.0 cx_str . dopamine = 0.0 simulate ( 40.0 ) # Set inputs cortex . r = np . array ( x ) simulate ( 50.0 ) # Read output output = gpi . r answer = np . argmin ( output ) # Provide reward reward = 1.0 if answer == t else - 1.0 cx_str . dopamine = reward simulate ( 10.0 ) # Get recordings data = m . get ( 'r' ) return reward , data The whole training procedure will simply iterate over the four stimuli for 100 trials: for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) We use the Logger class of the tensorboard extension to keep track of various data: with Logger () as logger : for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) # Log data... Note that it would be equivalent to manually close the Logger after training: logger = Logger () for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) # Log data... logger . close () We log here different quantities, just to demonstrate the different methods of the Logger class: The reward received after each trial: logger . add_scalar ( \"Reward\" , reward , trial ) The tag \"Reward\" will be the name of the plot in tensorboard. reward is the value that will be displayed, while trial is the index of the current trial (x-axis). The activity of the two GPi cells at the end of the trial, in separate plots depending on the stimulus: if trial % len ( stimuli ) == 0 : label = \"GPi activity/A\" elif trial % len ( stimuli ) == 1 : label = \"GPi activity/B\" elif trial % len ( stimuli ) == 2 : label = \"GPi activity/C\" elif trial % len ( stimuli ) == 3 : label = \"GPi activity/D\" logger . add_scalars ( label , { \"Left neuron\" : gpi . r [ 0 ], \"Right neuron\" : gpi . r [ 1 ]}, trial ) The four plots will be grouped under the label \"GPi activity\", with a title A, B, C or D. Note that add_scalars() requires a dictionary of values that will plot together. The activity in the striatum as a 2*5 image: logger . add_image ( \"Activity/Striatum\" , striatum . r . reshape (( 2 , 5 )), trial ) The activity should be reshaped to the correct dimensions. Note that activity in the striatum is bounded between 0 and 1, so there is no need for equalization. An histogram of the preference for the stimuli A and B of striatal cells: w = np . array ( cx_str . w ) logger . add_histogram ( \"Cortico-striatal weights/Left - AB/CD\" , np . mean ( w [: 5 , : 2 ] - w [: 5 , 2 :], axis = 1 ), trial ) logger . add_histogram ( \"Cortico-striatal weights/Right - AB/CD\" , np . mean ( w [ 5 :, : 2 ] - w [ 5 :, 2 :], axis = 1 ), trial ) We make here two plots, one for the first 5 striatal cells, the other for the rest. We plot the difference between the mean weights of each cell for the stimuli A and B, and the mean weights for the stimuli C and D. If learning goes well, the first five striatal cells should have stronger weights for A and B than for C and D, as they project to the left GPi cell. A matplotlib figure showing the time course of the two GPi cells (as recorded by the monitor): fig = plt . figure ( figsize = ( 10 , 8 )) plt . plot ( data [:, 0 ], label = \"left\" ) plt . plot ( data [:, 1 ], label = \"right\" ) plt . legend () logger . add_figure ( \"Activity/GPi\" , fig , trial ) Note that the figure will be automatically closed by the logger, no need to call show() . Logging figures is extremely slow, use that feature wisely. By default, the logs are saved in the subfolder runs/ , but this can be changed when creating the Logger: with Logger ( \"/tmp/experiment\" ) as logger : Each run of the network will be saved in this folder. You may want to delete the folder before each run, in order to only visualize the last run: % rm - rf runs with Logger () as logger : for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) # Log received rewards logger . add_scalar ( \"Reward\" , reward , trial ) # Log outputs depending on the task if trial % len ( stimuli ) == 0 : label = \"GPi activity/A\" elif trial % len ( stimuli ) == 1 : label = \"GPi activity/B\" elif trial % len ( stimuli ) == 2 : label = \"GPi activity/C\" elif trial % len ( stimuli ) == 3 : label = \"GPi activity/D\" logger . add_scalars ( label , { \"Left neuron\" : gpi . r [ 0 ], \"Right neuron\" : gpi . r [ 1 ]}, trial ) # Log striatal activity as a 2*5 image logger . add_image ( \"Activity/Striatum\" , striatum . r . reshape (( 2 , 5 )), trial ) # Log histogram of cortico-striatal weights w = np . array ( cx_str . w ) logger . add_histogram ( \"Cortico-striatal weights/Left - AB/CD\" , np . mean ( w [: 5 , : 2 ] - w [: 5 , 2 :], axis = 1 ), trial ) logger . add_histogram ( \"Cortico-striatal weights/Right - AB/CD\" , np . mean ( w [ 5 :, : 2 ] - w [ 5 :, 2 :], axis = 1 ), trial ) # Log matplotlib figure of GPi activity fig = plt . figure ( figsize = ( 10 , 8 )) plt . plot ( data [:, 0 ], label = \"left\" ) plt . plot ( data [:, 1 ], label = \"right\" ) plt . legend () logger . add_figure ( \"Activity/GPi\" , fig , trial ) Logging in runs/Sep29_10-58-54_machine You can now visualize the logged information by running tensorboard in a separate terminal and opening the corresponding page: tensorboard --logdir runs or directly in the notebook if you have the tensorboard extension installed: % load_ext tensorboard % tensorboard -- logdir runs -- samples_per_plugin images = 100 You should see a tensorboard page with four tabs Scalars, Images, Distributions and Histograms: The Reward plot shows that the network successfully learns to solve the task, as it consistently gets rewards of +1 (note that this may vary from run to run, depending on weight initialization): The GPi activity tab shows that the two GPi cells quickly learn to be inhibited for the right stimuli. In the Images tab, the plot for the striatum allows to visualize activity at the end of each rtial, showing that only one cell in the correct subpopulation is active: The matplotlib figure for the GPi activity shows what happens during a trial, especially at the end of the reset period: In the histograms tab, we can see that the left striatal population has acquired a preference (stronger weights) for the stimuli A and B, as the values are positive. The right population has negative values, so the neurons have stronger weights to the stimuli C and D. Note that some neurons in the right population still have stronger weights from A and B, but they are probably inhibited by the left population, so they do not impair performance.","title":"Logging with tensorboard"},{"location":"example/BayesianOptimization/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Hyperparameter optimization # Most of the work in computational neuroscience is to guess the values of parameters which are not constrained by the biology. The most basic approach is to simply try out different values, run the simulation, reason about why the results are not what you want, change some parameters, run again, etc. It is very easy to get lost in this process and it requires a great deal of intuition about how the model works. If you are able to define an objective function for your model (a single number that tells how well your model performs), you can use search algorithms to find this hyperparameters automatically, at the cost of running your model multiple times. Let's take the example of a rate-coded model depending on two hyperparameters a and b , where is the objective is to have a minimal activity after 1 s of simulation (dummy example): from ANNarchy import * pop = Population ( ... ) ... compile () def run ( a , b ): pop . a = a pop . b = b simulate ( 1000. ) return ( pop . r ) ** 2 Grid search would iterate over all possible values of the parameters to perform the search: min_loss = 1000. for a in np . linspace ( 0.0 , 1.0 , 100 ): for b in np . linspace ( 0.0 , 1.0 , 100 ): loss = run ( a , b ) if loss < min_loss : min_loss = loss a_best = a ; b_best = b If you try 100 values for each parameters, you need 10000 simulations to find your parameters. The number of simulations explodes with the number of free parameters. Moreover, you cannot stop the search before the end, as you could miss the interesting region. Random search samples blindly values for the hyperparameters: min_loss = 1000. for _ in range ( 1000 ): a = np . random . uniform ( 0.0 , 1.0 ) b = np . random . uniform ( 0.0 , 1.0 ) loss = run ( a , b ) if loss < min_loss : min_loss = loss a_best = a ; b_best = b If you are lucky, you may find a good solution quite early in the search, so you can stop it when the loss is below a desired threshold. The main drawback is that the search may spend a lot of time in uninteresting regions: it does not learn anything between two samples. An often much more efficient search method is Bayesian optimization (also called sequential model-based optimization - SMBO). It is a form of random search that updates beliefs on the hyperparameters. In short, if some parameter values do not lead to good values of the objective function in early samples, they will not be used in later samples. The search becomes more and more focused on the interesting regions of the hyperparameter space. As always with Python, there are many libraries for that, including: hyperopt https://github.com/hyperopt/hyperopt optuna https://github.com/pfnet/optuna talos (for keras models) https://github.com/autonomio/talos This notebook demonstrates how to use hyperopt to find some hyperparameters of the COBA models already included in the ANNarchy examples: https://annarchy.readthedocs.io/en/stable/example/COBA.html Additionally, we will use the tensorboard extension to visualize the dependency between the parameters and the objective function. from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger clear () setup ( dt = 0.1 ) ANNarchy 4.7 (4.7.0) on linux (posix). COBA = Neuron ( parameters = \"\"\" El = -60.0 : population Vr = -60.0 : population Erev_exc = 0.0 : population Erev_inh = -80.0 : population Vt = -50.0 : population tau = 20.0 : population tau_exc = 5.0 : population tau_inh = 10.0 : population I = 20.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I tau_exc * dg_exc/dt = - g_exc tau_inh * dg_inh/dt = - g_inh \"\"\" , spike = \"v > Vt\" , reset = \"v = Vr\" , refractory = 5.0 ) P = Population ( geometry = 4000 , neuron = COBA ) Pe = P [: 3200 ] Pi = P [ 3200 :] P . v = Normal ( - 55.0 , 5.0 ) P . g_exc = Normal ( 4.0 , 1.5 ) P . g_inh = Normal ( 20.0 , 12.0 ) Ce = Projection ( pre = Pe , post = P , target = 'exc' ) Ce . connect_fixed_probability ( weights = 0.6 , probability = 0.02 ) Ci = Projection ( pre = Pi , post = P , target = 'inh' ) Ci . connect_fixed_probability ( weights = 6.7 , probability = 0.02 ) compile () m = Monitor ( P , [ 'spike' ]) Compiling ... OK With the default parameters, the COBA network fires at around 20 Hz: simulate ( 1000.0 ) data = m . get ( 'spike' ) fr = m . mean_fr ( data ) print ( fr ) 20.399 Let's suppose we now want the network to fire at 30 Hz. Which parameters should we change to obtain that value? Many parameters might influence the firing rate of the network (if not all). Here, we make the assumption that the weight values for the excitatory connections (0.6) and inhibitory ones (6.7) are the most critical ones. Let's start by importing hyperopt (after installing it with pip install hyperopt ): from hyperopt import fmin , tpe , hp , STATUS_OK We define a trial() method taking values for the two hyperparameters as inputs. It starts by resetting the network, sets the excitatory and inhibitory weights to the desired value, simulates for one second, computes the mean firing rate of the population, logs the parameters and finally returns the objective function: the squared error between the recorded firing rate and 30 Hz. logger = Logger () def trial ( args ): # Retrieve the parameters w_exc = args [ 0 ] w_inh = args [ 1 ] # Reset the network reset () # Set the hyperparameters Ce . w = w_exc Ci . w = w_inh # Simulate 1 second simulate ( 1000.0 ) # Retrieve the spike recordings and the membrane potential spikes = m . get ( 'spike' ) # Compute the population firing rate fr = m . mean_fr ( spikes ) # Compute a qudratic loss around 30 Hz loss = 0.001 * ( fr - 30.0 ) ** 2 # Log the parameters logger . add_parameters ({ 'w_exc' : w_exc , 'w_inh' : w_inh }, { 'loss' : loss , 'firing_rate' : fr }) return { 'loss' : loss , 'status' : STATUS_OK , # -- store other results like this 'fr' : fr , } Logging in runs/Sep29_11-00-29_machine We can check that the default parameters indeed lead to a firing rate of 20 Hz: trial ([ 0.6 , 6.7 ]) {'loss': 0.09217920099999997, 'status': 'ok', 'fr': 20.399} We can now use hyperopt to find the hyperparameters making the network fire at 30 Hz. The fmin() function takes: fn : the objective function for a set of parameters. space : the search space for the hyperparameters (the prior). algo : which algorithm to use, either tpe.suggest or random.suggest max_evals : number of samples (simulations) to make. Here, we will sample the excitatory weights between 0.1 and 1, the inhibitory ones between 1 and 10. Of course, the smaller the range, the better. Refer to the doc of hyperopt for other sampling priors. best = fmin ( fn = trial , space = [ hp . uniform ( 'w_exc' , 0.1 , 1.0 ), hp . uniform ( 'w_inh' , 1.0 , 10.0 ) ], algo = tpe . suggest , max_evals = 100 ) print ( best ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:38<00:00, 2.57trial/s, best loss: 6.630624999999283e-07] {'w_exc': 0.5352251591809416, 'w_inh': 3.533247914589427} After 100 simulations, hyperopt returns a set of hyperparameter values that make the network fire at 30Hz. We can check that it is true with: trial ([ best [ 'w_exc' ], best [ 'w_inh' ]]) {'loss': 6.630624999999283e-07, 'status': 'ok', 'fr': 30.02575} There are plenty of options to hyperopt (check Trials or the parallel search using MongoDB), but this simple example should get you started. If we start tensorboard in the default directory runs/ , we can additionally visualize how the firing rate depends on w_exc and w_inh in the HPARAMS tab. logger . close () % load_ext tensorboard % tensorboard -- logdir runs","title":"Bayesian optimization"},{"location":"example/BayesianOptimization/#hyperparameter-optimization","text":"Most of the work in computational neuroscience is to guess the values of parameters which are not constrained by the biology. The most basic approach is to simply try out different values, run the simulation, reason about why the results are not what you want, change some parameters, run again, etc. It is very easy to get lost in this process and it requires a great deal of intuition about how the model works. If you are able to define an objective function for your model (a single number that tells how well your model performs), you can use search algorithms to find this hyperparameters automatically, at the cost of running your model multiple times. Let's take the example of a rate-coded model depending on two hyperparameters a and b , where is the objective is to have a minimal activity after 1 s of simulation (dummy example): from ANNarchy import * pop = Population ( ... ) ... compile () def run ( a , b ): pop . a = a pop . b = b simulate ( 1000. ) return ( pop . r ) ** 2 Grid search would iterate over all possible values of the parameters to perform the search: min_loss = 1000. for a in np . linspace ( 0.0 , 1.0 , 100 ): for b in np . linspace ( 0.0 , 1.0 , 100 ): loss = run ( a , b ) if loss < min_loss : min_loss = loss a_best = a ; b_best = b If you try 100 values for each parameters, you need 10000 simulations to find your parameters. The number of simulations explodes with the number of free parameters. Moreover, you cannot stop the search before the end, as you could miss the interesting region. Random search samples blindly values for the hyperparameters: min_loss = 1000. for _ in range ( 1000 ): a = np . random . uniform ( 0.0 , 1.0 ) b = np . random . uniform ( 0.0 , 1.0 ) loss = run ( a , b ) if loss < min_loss : min_loss = loss a_best = a ; b_best = b If you are lucky, you may find a good solution quite early in the search, so you can stop it when the loss is below a desired threshold. The main drawback is that the search may spend a lot of time in uninteresting regions: it does not learn anything between two samples. An often much more efficient search method is Bayesian optimization (also called sequential model-based optimization - SMBO). It is a form of random search that updates beliefs on the hyperparameters. In short, if some parameter values do not lead to good values of the objective function in early samples, they will not be used in later samples. The search becomes more and more focused on the interesting regions of the hyperparameter space. As always with Python, there are many libraries for that, including: hyperopt https://github.com/hyperopt/hyperopt optuna https://github.com/pfnet/optuna talos (for keras models) https://github.com/autonomio/talos This notebook demonstrates how to use hyperopt to find some hyperparameters of the COBA models already included in the ANNarchy examples: https://annarchy.readthedocs.io/en/stable/example/COBA.html Additionally, we will use the tensorboard extension to visualize the dependency between the parameters and the objective function. from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger clear () setup ( dt = 0.1 ) ANNarchy 4.7 (4.7.0) on linux (posix). COBA = Neuron ( parameters = \"\"\" El = -60.0 : population Vr = -60.0 : population Erev_exc = 0.0 : population Erev_inh = -80.0 : population Vt = -50.0 : population tau = 20.0 : population tau_exc = 5.0 : population tau_inh = 10.0 : population I = 20.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I tau_exc * dg_exc/dt = - g_exc tau_inh * dg_inh/dt = - g_inh \"\"\" , spike = \"v > Vt\" , reset = \"v = Vr\" , refractory = 5.0 ) P = Population ( geometry = 4000 , neuron = COBA ) Pe = P [: 3200 ] Pi = P [ 3200 :] P . v = Normal ( - 55.0 , 5.0 ) P . g_exc = Normal ( 4.0 , 1.5 ) P . g_inh = Normal ( 20.0 , 12.0 ) Ce = Projection ( pre = Pe , post = P , target = 'exc' ) Ce . connect_fixed_probability ( weights = 0.6 , probability = 0.02 ) Ci = Projection ( pre = Pi , post = P , target = 'inh' ) Ci . connect_fixed_probability ( weights = 6.7 , probability = 0.02 ) compile () m = Monitor ( P , [ 'spike' ]) Compiling ... OK With the default parameters, the COBA network fires at around 20 Hz: simulate ( 1000.0 ) data = m . get ( 'spike' ) fr = m . mean_fr ( data ) print ( fr ) 20.399 Let's suppose we now want the network to fire at 30 Hz. Which parameters should we change to obtain that value? Many parameters might influence the firing rate of the network (if not all). Here, we make the assumption that the weight values for the excitatory connections (0.6) and inhibitory ones (6.7) are the most critical ones. Let's start by importing hyperopt (after installing it with pip install hyperopt ): from hyperopt import fmin , tpe , hp , STATUS_OK We define a trial() method taking values for the two hyperparameters as inputs. It starts by resetting the network, sets the excitatory and inhibitory weights to the desired value, simulates for one second, computes the mean firing rate of the population, logs the parameters and finally returns the objective function: the squared error between the recorded firing rate and 30 Hz. logger = Logger () def trial ( args ): # Retrieve the parameters w_exc = args [ 0 ] w_inh = args [ 1 ] # Reset the network reset () # Set the hyperparameters Ce . w = w_exc Ci . w = w_inh # Simulate 1 second simulate ( 1000.0 ) # Retrieve the spike recordings and the membrane potential spikes = m . get ( 'spike' ) # Compute the population firing rate fr = m . mean_fr ( spikes ) # Compute a qudratic loss around 30 Hz loss = 0.001 * ( fr - 30.0 ) ** 2 # Log the parameters logger . add_parameters ({ 'w_exc' : w_exc , 'w_inh' : w_inh }, { 'loss' : loss , 'firing_rate' : fr }) return { 'loss' : loss , 'status' : STATUS_OK , # -- store other results like this 'fr' : fr , } Logging in runs/Sep29_11-00-29_machine We can check that the default parameters indeed lead to a firing rate of 20 Hz: trial ([ 0.6 , 6.7 ]) {'loss': 0.09217920099999997, 'status': 'ok', 'fr': 20.399} We can now use hyperopt to find the hyperparameters making the network fire at 30 Hz. The fmin() function takes: fn : the objective function for a set of parameters. space : the search space for the hyperparameters (the prior). algo : which algorithm to use, either tpe.suggest or random.suggest max_evals : number of samples (simulations) to make. Here, we will sample the excitatory weights between 0.1 and 1, the inhibitory ones between 1 and 10. Of course, the smaller the range, the better. Refer to the doc of hyperopt for other sampling priors. best = fmin ( fn = trial , space = [ hp . uniform ( 'w_exc' , 0.1 , 1.0 ), hp . uniform ( 'w_inh' , 1.0 , 10.0 ) ], algo = tpe . suggest , max_evals = 100 ) print ( best ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:38<00:00, 2.57trial/s, best loss: 6.630624999999283e-07] {'w_exc': 0.5352251591809416, 'w_inh': 3.533247914589427} After 100 simulations, hyperopt returns a set of hyperparameter values that make the network fire at 30Hz. We can check that it is true with: trial ([ best [ 'w_exc' ], best [ 'w_inh' ]]) {'loss': 6.630624999999283e-07, 'status': 'ok', 'fr': 30.02575} There are plenty of options to hyperopt (check Trials or the parallel search using MongoDB), but this simple example should get you started. If we start tensorboard in the default directory runs/ , we can additionally visualize how the firing rate depends on w_exc and w_inh in the HPARAMS tab. logger . close () % load_ext tensorboard % tensorboard -- logdir runs","title":"Hyperparameter optimization"},{"location":"example/BoldMonitoring/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Recording BOLD signals # This notebook demonstrates the usage of the BOLD monitoring extension. Background # ANNarchy pre-implements some model variants of the BOLD models presented in Stephan et al. (2007) which are variations of the Balloon Model originally designed by Buxton et al. (1998). The four balloon variants model pre-implemented in ANNarchy follow the naming scheme of Stephan et al. (2007). The model name starts with balloon_ followed by either C (i. e. classical coefficient) or R (i. e. revised coefficient) and then followed by either N which means non-linear BOLD equation or L which means linear bold equation. We only provide here the equations without much explanations, for more details please refer to the literature: Buxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855\u2013864. doi:10.1002/mrm.1910390602 Friston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466\u2013477 Buxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220\u2013S233. doi:10.1016/j.neuroimage.2004.07.013 Stephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387\u2013401. doi:10.1016/j.neuroimage.2007.07.040 Maith et al. (2021) A computational model-based analysis of basal ganglia pathway changes in Parkinson\u2019s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278\u2013 2295. doi:10.1111/ejn.14868 Single input Balloon model # This script shows a simple example for the application of one of the default models (the balloon_RN model) on a simple network. The balloon_RN model is described by the following equations: \\[ \\tau \\cdot \\frac{ds}{dt} = \\phi \\cdot I_\\text{CBF} - \\kappa \\cdot s - \\gamma \\cdot (f_{in} - 1) \\] \\[ \\frac{df_{in}}{dt} = s \\] \\[ E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} } \\] \\[ \\tau \\cdot \\frac{dv}{dt} = \\frac{1}{\\tau_0} \\cdot (f_{in} - f_{out}) \\] \\[ f_{out} = v^{\\frac{1}{\\alpha}} \\] \\[ \\tau \\cdot \\frac{dq}{dt} = \\frac{1}{\\tau_0} \\cdot ( f_{in} \\cdot \\frac{E}{E_0} - \\frac{q}{v} \\cdot f_{out} ) \\] with revised coefficients and non-linear bold equation: \\[k_1 = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2 = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\\] \\[k_3 = 1 - \\epsilon\\] \\[ BOLD = v_0 \\cdot ( k_1 \\cdot (1-q) + k_2 \\cdot (1 - \\dfrac{q}{v}) + k_3 \\cdot (1 - v) ) \\] There are two important variables in that model: BOLD which is the output of the model and I_CBF which is the input signal, reflecting the mean firing rate of the input populations. As the BOLD model reflects the change of activity levels in the recorded area, we will implement the following experiment. We record from two populations with 100 Izhikevich neurons each. After a short period of time we raise the activity level of one population for a fixed time window. For simplicity, we do not use connections between the neurons but simply increase the noise term in the Izhikevich model. This should lead to a noticeable BOLD signal. Populations # For this example we want to create two groups of Izhikevich neurons: from ANNarchy import * from ANNarchy.extensions.bold import * clear () # A population of 100 izhikevich neurons pop0 = Population ( 100 , neuron = Izhikevich ) pop1 = Population ( 100 , neuron = Izhikevich ) ANNarchy 4.7 (4.7.0) on linux (posix). As we will not have any connections between the neurons, we need to increase the noise to create some baseline activity: # Set noise to create some baseline activity pop0 . noise = 5.0 ; pop1 . noise = 5.0 To evaluate the result of the simulation we want to investigate the mean firing rate of the neurons. This need to be activated explicitely as the computation of this value is quite expensive. In our example, the mean firing rate should be computed across a time window of 100 ms. The resulting value will be stored in the r attribute of the populations and can be easily recorded. # Compute mean firing rate in Hz on 100ms window pop0 . compute_firing_rate ( window = 100.0 ) pop1 . compute_firing_rate ( window = 100.0 ) # Record the mean firing rate mon_pop0 = Monitor ( pop0 , [ \"r\" ], start = False ) mon_pop1 = Monitor ( pop1 , [ \"r\" ], start = False ) BOLD Monitor definition # The BOLD monitor expects a list of populations which we want to record (in our case pop0 and pop1 ). A BOLD model should be specified, here we atke balloon_RN which is the default. We then specify the mapping from the population variable that should influence the BOLD signal, in our case the mean firing rate r , to the input variable of the BOLD model I_CBF . The mean firing rate is normalized over a time window of 2000 ms and we record both input and output signals: m_bold = BoldMonitor ( populations = [ pop0 , pop1 ], # recorded populations bold_model = balloon_RN (), # BOLD model to use (default is balloon_RN) mapping = { 'I_CBF' : 'r' }, normalize_input = 2000 , # time window to compute baseline. It should be a multiple of the window used for the mean firing rate. recorded_variables = [ \"I_CBF\" , \"BOLD\" ] # we want to analyze the BOLD input ) Now we can compile and initialize the network: compile () Compiling ... OK Simulation # We first simulate 1 second biological time to ensure that the network reaches a stable firing rate: # Ramp up time simulate ( 1000 ) We then enable the recording of all monitors: # Start recording mon_pop0 . start () mon_pop1 . start () m_bold . start () We simulate for 5 seconds with lower noise and we increase the noise in pop0 for 5 seconds before decreasing it again: # we manipulate the noise for the half of the neurons simulate ( 5000 ) # 5s with low noise pop0 . noise = 7.5 simulate ( 5000 ) # 5s with higher noise (one population) pop0 . noise = 5 simulate ( 10000 ) # 10s with low noise # Retrieve the recordings mean_fr1 = np . mean ( mon_pop0 . get ( \"r\" ), axis = 1 ) mean_fr2 = np . mean ( mon_pop1 . get ( \"r\" ), axis = 1 ) input_data = m_bold . get ( \"I_CBF\" ) bold_data = m_bold . get ( \"BOLD\" ) Evaluation # We can now plot: the mean firing rate in the input populations. the recorded activity I which serves as an input to the BOLD model. the resulting BOLD signal. import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 6 )) grid = plt . GridSpec ( 1 , 3 , left = 0.05 , right = 0.95 ) # mean firing rate ax1 = plt . subplot ( grid [ 0 , 0 ]) ax1 . plot ( mean_fr1 , label = \"pop0\" ) ax1 . plot ( mean_fr2 , label = \"pop1\" ) plt . legend () ax1 . set_ylabel ( \"Average mean firing rate [Hz]\" , fontweight = \"bold\" , fontsize = 18 ) # BOLD input signal ax2 = plt . subplot ( grid [ 0 , 1 ]) ax2 . plot ( input_data , color = \"k\" ) ax2 . set_ylabel ( \"BOLD input I_CBF\" , fontweight = \"bold\" , fontsize = 18 ) # BOLD output signal ax3 = plt . subplot ( grid [ 0 , 2 ]) ax3 . plot ( bold_data * 100.0 , color = \"k\" ) ax3 . set_ylabel ( \"BOLD [%]\" , fontweight = \"bold\" , fontsize = 18 ) # x-axis labels as seconds for ax in [ ax1 , ax2 , ax3 ]: ax . set_xticks ( np . arange ( 0 , 21 , 2 ) * 1000 ) ax . set_xticklabels ( np . arange ( 0 , 21 , 2 )) ax . set_xlabel ( \"time [s]\" , fontweight = \"bold\" , fontsize = 18 ) plt . show () Two inputs Balloon model # Let's now see the effect of a two-inputs model using both the firing rate and the membrane potential of the populations as inputs. We first need to recreate the network, as the BOLD monitors are compiled and cannot be created afterwards: clear () # Two populations of 100 izhikevich neurons pop0 = Population ( 100 , neuron = Izhikevich ) pop1 = Population ( 100 , neuron = Izhikevich ) # Set noise to create some baseline activity pop0 . noise = 5.0 ; pop1 . noise = 5.0 # Compute mean firing rate in Hz on 100ms window pop0 . compute_firing_rate ( window = 100.0 ) pop1 . compute_firing_rate ( window = 100.0 ) # Create required monitors mon_pop0 = Monitor ( pop0 , [ \"r\" ], start = False ) mon_pop1 = Monitor ( pop1 , [ \"r\" ], start = False ) m_bold = BoldMonitor ( populations = [ pop0 , pop1 ], # recorded populations bold_model = balloon_two_inputs (), # BOLD model to use # mean firing rate as source variable coupled to the input variable I_CBF # membrane potential as source variable coupled to the input variable I_CMRO2 mapping = { 'I_CBF' : 'r' , 'I_CMRO2' : 'v' }, normalize_input = 2000 , # time window to compute the baseline recorded_variables = [ \"I_CBF\" , \"I_CMRO2\" , \"BOLD\" ] # we want to analyze the BOLD model input ) compile () Compiling ... OK # Ramp up time simulate ( 1000 ) # Start recording mon_pop0 . start () mon_pop1 . start () m_bold . start () # we manipulate the noise for the half of the neurons simulate ( 5000 ) # 5s with low noise pop0 . noise = 7.5 simulate ( 5000 ) # 5s with higher noise (one population) pop0 . noise = 5 simulate ( 10000 ) # 10s with low noise # retrieve the recordings mean_fr1 = np . mean ( mon_pop0 . get ( \"r\" ), axis = 1 ) mean_fr2 = np . mean ( mon_pop1 . get ( \"r\" ), axis = 1 ) If_data = m_bold . get ( \"I_CBF\" ) Ir_data = m_bold . get ( \"I_CMRO2\" ) bold_data = m_bold . get ( \"BOLD\" ) plt . figure ( figsize = ( 20 , 6 )) grid = plt . GridSpec ( 1 , 3 , left = 0.05 , right = 0.95 ) # mean firing rate ax1 = plt . subplot ( grid [ 0 , 0 ]) ax1 . plot ( mean_fr1 , label = \"pop0\" ) ax1 . plot ( mean_fr2 , label = \"pop1\" ) plt . legend () ax1 . set_ylabel ( \"average mean firing rate [Hz]\" , fontweight = \"bold\" , fontsize = 18 ) # BOLD input signal ax2 = plt . subplot ( grid [ 0 , 1 ]) ax2 . plot ( If_data , color = \"k\" , label = 'I_CBF' ) ax2 . plot ( Ir_data , color = \"g\" , label = 'I_CMRO2' ) ax2 . set_ylabel ( \"BOLD input variables\" , fontweight = \"bold\" , fontsize = 18 ) ax2 . legend () # BOLD input signal as percent ax3 = plt . subplot ( grid [ 0 , 2 ]) ax3 . plot ( bold_data * 100.0 , color = \"k\" ) ax3 . set_ylabel ( \"BOLD [%]\" , fontweight = \"bold\" , fontsize = 18 ) # x-axis labels as seconds for ax in [ ax1 , ax2 , ax3 ]: ax . set_xticks ( np . arange ( 0 , 21 , 2 ) * 1000 ) ax . set_xticklabels ( np . arange ( 0 , 21 , 2 )) ax . set_xlabel ( \"time [s]\" , fontweight = \"bold\" , fontsize = 18 ) plt . show ()","title":"BOLD monitoring"},{"location":"example/BoldMonitoring/#recording-bold-signals","text":"This notebook demonstrates the usage of the BOLD monitoring extension.","title":"Recording BOLD signals"},{"location":"example/BoldMonitoring/#background","text":"ANNarchy pre-implements some model variants of the BOLD models presented in Stephan et al. (2007) which are variations of the Balloon Model originally designed by Buxton et al. (1998). The four balloon variants model pre-implemented in ANNarchy follow the naming scheme of Stephan et al. (2007). The model name starts with balloon_ followed by either C (i. e. classical coefficient) or R (i. e. revised coefficient) and then followed by either N which means non-linear BOLD equation or L which means linear bold equation. We only provide here the equations without much explanations, for more details please refer to the literature: Buxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855\u2013864. doi:10.1002/mrm.1910390602 Friston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466\u2013477 Buxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220\u2013S233. doi:10.1016/j.neuroimage.2004.07.013 Stephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387\u2013401. doi:10.1016/j.neuroimage.2007.07.040 Maith et al. (2021) A computational model-based analysis of basal ganglia pathway changes in Parkinson\u2019s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278\u2013 2295. doi:10.1111/ejn.14868","title":"Background"},{"location":"example/BoldMonitoring/#single-input-balloon-model","text":"This script shows a simple example for the application of one of the default models (the balloon_RN model) on a simple network. The balloon_RN model is described by the following equations: \\[ \\tau \\cdot \\frac{ds}{dt} = \\phi \\cdot I_\\text{CBF} - \\kappa \\cdot s - \\gamma \\cdot (f_{in} - 1) \\] \\[ \\frac{df_{in}}{dt} = s \\] \\[ E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} } \\] \\[ \\tau \\cdot \\frac{dv}{dt} = \\frac{1}{\\tau_0} \\cdot (f_{in} - f_{out}) \\] \\[ f_{out} = v^{\\frac{1}{\\alpha}} \\] \\[ \\tau \\cdot \\frac{dq}{dt} = \\frac{1}{\\tau_0} \\cdot ( f_{in} \\cdot \\frac{E}{E_0} - \\frac{q}{v} \\cdot f_{out} ) \\] with revised coefficients and non-linear bold equation: \\[k_1 = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2 = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\\] \\[k_3 = 1 - \\epsilon\\] \\[ BOLD = v_0 \\cdot ( k_1 \\cdot (1-q) + k_2 \\cdot (1 - \\dfrac{q}{v}) + k_3 \\cdot (1 - v) ) \\] There are two important variables in that model: BOLD which is the output of the model and I_CBF which is the input signal, reflecting the mean firing rate of the input populations. As the BOLD model reflects the change of activity levels in the recorded area, we will implement the following experiment. We record from two populations with 100 Izhikevich neurons each. After a short period of time we raise the activity level of one population for a fixed time window. For simplicity, we do not use connections between the neurons but simply increase the noise term in the Izhikevich model. This should lead to a noticeable BOLD signal.","title":"Single input Balloon model"},{"location":"example/BoldMonitoring/#populations","text":"For this example we want to create two groups of Izhikevich neurons: from ANNarchy import * from ANNarchy.extensions.bold import * clear () # A population of 100 izhikevich neurons pop0 = Population ( 100 , neuron = Izhikevich ) pop1 = Population ( 100 , neuron = Izhikevich ) ANNarchy 4.7 (4.7.0) on linux (posix). As we will not have any connections between the neurons, we need to increase the noise to create some baseline activity: # Set noise to create some baseline activity pop0 . noise = 5.0 ; pop1 . noise = 5.0 To evaluate the result of the simulation we want to investigate the mean firing rate of the neurons. This need to be activated explicitely as the computation of this value is quite expensive. In our example, the mean firing rate should be computed across a time window of 100 ms. The resulting value will be stored in the r attribute of the populations and can be easily recorded. # Compute mean firing rate in Hz on 100ms window pop0 . compute_firing_rate ( window = 100.0 ) pop1 . compute_firing_rate ( window = 100.0 ) # Record the mean firing rate mon_pop0 = Monitor ( pop0 , [ \"r\" ], start = False ) mon_pop1 = Monitor ( pop1 , [ \"r\" ], start = False )","title":"Populations"},{"location":"example/BoldMonitoring/#bold-monitor-definition","text":"The BOLD monitor expects a list of populations which we want to record (in our case pop0 and pop1 ). A BOLD model should be specified, here we atke balloon_RN which is the default. We then specify the mapping from the population variable that should influence the BOLD signal, in our case the mean firing rate r , to the input variable of the BOLD model I_CBF . The mean firing rate is normalized over a time window of 2000 ms and we record both input and output signals: m_bold = BoldMonitor ( populations = [ pop0 , pop1 ], # recorded populations bold_model = balloon_RN (), # BOLD model to use (default is balloon_RN) mapping = { 'I_CBF' : 'r' }, normalize_input = 2000 , # time window to compute baseline. It should be a multiple of the window used for the mean firing rate. recorded_variables = [ \"I_CBF\" , \"BOLD\" ] # we want to analyze the BOLD input ) Now we can compile and initialize the network: compile () Compiling ... OK","title":"BOLD Monitor definition"},{"location":"example/BoldMonitoring/#simulation","text":"We first simulate 1 second biological time to ensure that the network reaches a stable firing rate: # Ramp up time simulate ( 1000 ) We then enable the recording of all monitors: # Start recording mon_pop0 . start () mon_pop1 . start () m_bold . start () We simulate for 5 seconds with lower noise and we increase the noise in pop0 for 5 seconds before decreasing it again: # we manipulate the noise for the half of the neurons simulate ( 5000 ) # 5s with low noise pop0 . noise = 7.5 simulate ( 5000 ) # 5s with higher noise (one population) pop0 . noise = 5 simulate ( 10000 ) # 10s with low noise # Retrieve the recordings mean_fr1 = np . mean ( mon_pop0 . get ( \"r\" ), axis = 1 ) mean_fr2 = np . mean ( mon_pop1 . get ( \"r\" ), axis = 1 ) input_data = m_bold . get ( \"I_CBF\" ) bold_data = m_bold . get ( \"BOLD\" )","title":"Simulation"},{"location":"example/BoldMonitoring/#evaluation","text":"We can now plot: the mean firing rate in the input populations. the recorded activity I which serves as an input to the BOLD model. the resulting BOLD signal. import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 6 )) grid = plt . GridSpec ( 1 , 3 , left = 0.05 , right = 0.95 ) # mean firing rate ax1 = plt . subplot ( grid [ 0 , 0 ]) ax1 . plot ( mean_fr1 , label = \"pop0\" ) ax1 . plot ( mean_fr2 , label = \"pop1\" ) plt . legend () ax1 . set_ylabel ( \"Average mean firing rate [Hz]\" , fontweight = \"bold\" , fontsize = 18 ) # BOLD input signal ax2 = plt . subplot ( grid [ 0 , 1 ]) ax2 . plot ( input_data , color = \"k\" ) ax2 . set_ylabel ( \"BOLD input I_CBF\" , fontweight = \"bold\" , fontsize = 18 ) # BOLD output signal ax3 = plt . subplot ( grid [ 0 , 2 ]) ax3 . plot ( bold_data * 100.0 , color = \"k\" ) ax3 . set_ylabel ( \"BOLD [%]\" , fontweight = \"bold\" , fontsize = 18 ) # x-axis labels as seconds for ax in [ ax1 , ax2 , ax3 ]: ax . set_xticks ( np . arange ( 0 , 21 , 2 ) * 1000 ) ax . set_xticklabels ( np . arange ( 0 , 21 , 2 )) ax . set_xlabel ( \"time [s]\" , fontweight = \"bold\" , fontsize = 18 ) plt . show ()","title":"Evaluation"},{"location":"example/BoldMonitoring/#two-inputs-balloon-model","text":"Let's now see the effect of a two-inputs model using both the firing rate and the membrane potential of the populations as inputs. We first need to recreate the network, as the BOLD monitors are compiled and cannot be created afterwards: clear () # Two populations of 100 izhikevich neurons pop0 = Population ( 100 , neuron = Izhikevich ) pop1 = Population ( 100 , neuron = Izhikevich ) # Set noise to create some baseline activity pop0 . noise = 5.0 ; pop1 . noise = 5.0 # Compute mean firing rate in Hz on 100ms window pop0 . compute_firing_rate ( window = 100.0 ) pop1 . compute_firing_rate ( window = 100.0 ) # Create required monitors mon_pop0 = Monitor ( pop0 , [ \"r\" ], start = False ) mon_pop1 = Monitor ( pop1 , [ \"r\" ], start = False ) m_bold = BoldMonitor ( populations = [ pop0 , pop1 ], # recorded populations bold_model = balloon_two_inputs (), # BOLD model to use # mean firing rate as source variable coupled to the input variable I_CBF # membrane potential as source variable coupled to the input variable I_CMRO2 mapping = { 'I_CBF' : 'r' , 'I_CMRO2' : 'v' }, normalize_input = 2000 , # time window to compute the baseline recorded_variables = [ \"I_CBF\" , \"I_CMRO2\" , \"BOLD\" ] # we want to analyze the BOLD model input ) compile () Compiling ... OK # Ramp up time simulate ( 1000 ) # Start recording mon_pop0 . start () mon_pop1 . start () m_bold . start () # we manipulate the noise for the half of the neurons simulate ( 5000 ) # 5s with low noise pop0 . noise = 7.5 simulate ( 5000 ) # 5s with higher noise (one population) pop0 . noise = 5 simulate ( 10000 ) # 10s with low noise # retrieve the recordings mean_fr1 = np . mean ( mon_pop0 . get ( \"r\" ), axis = 1 ) mean_fr2 = np . mean ( mon_pop1 . get ( \"r\" ), axis = 1 ) If_data = m_bold . get ( \"I_CBF\" ) Ir_data = m_bold . get ( \"I_CMRO2\" ) bold_data = m_bold . get ( \"BOLD\" ) plt . figure ( figsize = ( 20 , 6 )) grid = plt . GridSpec ( 1 , 3 , left = 0.05 , right = 0.95 ) # mean firing rate ax1 = plt . subplot ( grid [ 0 , 0 ]) ax1 . plot ( mean_fr1 , label = \"pop0\" ) ax1 . plot ( mean_fr2 , label = \"pop1\" ) plt . legend () ax1 . set_ylabel ( \"average mean firing rate [Hz]\" , fontweight = \"bold\" , fontsize = 18 ) # BOLD input signal ax2 = plt . subplot ( grid [ 0 , 1 ]) ax2 . plot ( If_data , color = \"k\" , label = 'I_CBF' ) ax2 . plot ( Ir_data , color = \"g\" , label = 'I_CMRO2' ) ax2 . set_ylabel ( \"BOLD input variables\" , fontweight = \"bold\" , fontsize = 18 ) ax2 . legend () # BOLD input signal as percent ax3 = plt . subplot ( grid [ 0 , 2 ]) ax3 . plot ( bold_data * 100.0 , color = \"k\" ) ax3 . set_ylabel ( \"BOLD [%]\" , fontweight = \"bold\" , fontsize = 18 ) # x-axis labels as seconds for ax in [ ax1 , ax2 , ax3 ]: ax . set_xticks ( np . arange ( 0 , 21 , 2 ) * 1000 ) ax . set_xticklabels ( np . arange ( 0 , 21 , 2 )) ax . set_xlabel ( \"time [s]\" , fontweight = \"bold\" , fontsize = 18 ) plt . show ()","title":"Two inputs Balloon model"},{"location":"example/COBA/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); COBA and CUBA networks # The scripts COBA.py and CUBA.py in examples/vogels_abbott reproduce the two first benchmarks used in: Brette, R., Rudolph, M., Carnevale, T., Hines, M., Beeman, D., Bower, J. M., et al. (2007), Simulation of networks of spiking neurons: a review of tools and strategies., J. Comput. Neurosci., 23, 3, 349\u201398 Both are based on the balanced network proposed by: Vogels, T. P. and Abbott, L. F. (2005), Signal propagation and logic gating in networks of integrate-and-fire neurons., J. Neurosci., 25, 46, 10786\u201395 The network is composed of 4000 neurons (3200 excitatory and 800 inhibitory), reciprocally connected with a probability of 0.02 (sparse connection). The CUBA network uses a current-based integrate-and-fire neuron model: \\[\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) - g_\\text{inh} (t)\\] while the COBA model uses conductance-based IF neurons: \\[\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) * (E_\\text{exc}) - v(t)) + g_\\text{inh} (t) * (E_\\text{inh}) - v(t)) + I(t)\\] Apart from the neuron model and synaptic weights, both networks are equal, so we'll focus on the COBA network here. The discretization step has to be set to 0.1 ms: from ANNarchy import * setup ( dt = 0.1 ) ANNarchy 4.7 (4.7.0) on linux (posix). Neuron definition # COBA = Neuron ( parameters = \"\"\" El = -60.0 : population Vr = -60.0 : population Erev_exc = 0.0 : population Erev_inh = -80.0 : population Vt = -50.0 : population tau = 20.0 : population tau_exc = 5.0 : population tau_inh = 10.0 : population I = 20.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I tau_exc * dg_exc/dt = - g_exc tau_inh * dg_inh/dt = - g_inh \"\"\" , spike = \"v > Vt\" , reset = \"v = Vr\" , refractory = 5.0 ) CUBA = Neuron ( parameters = \"\"\" El = -49.0 : population Vr = -60.0 : population Vt = -50.0 : population tau_m = 20.0 : population tau_exc = 5.0 : population tau_inh = 10.0 : population \"\"\" , equations = \"\"\" tau_m * dv/dt = (El - v) + g_exc + g_inh tau_exc * dg_exc/dt = - g_exc tau_inh * dg_inh/dt = - g_inh \"\"\" , spike = \"v > Vt\" , reset = \"v = Vr\" , refractory = 5.0 ) The neurons define exponentially-decreasing conductance g_exc and g_inh for the excitatory and inhibitory conductances/currents, respectively. They also define a refractory period of 5 ms. Population # P = Population ( geometry = 4000 , neuron = COBA ) Pe = P [: 3200 ] Pi = P [ 3200 :] We create a population of 4000 COBA neurons, and assign the 3200 first ones to the excitatory population and the 800 last ones to the inhibitory population. It would have been equivalent to declare two separate populations as: Pe = Population ( geometry = 3200 , neuron = COBA ) Pi = Population ( geometry = 800 , neuron = COBA ) but splitting a global population allows to apply methods to all neurons, for example when recording all spikes with a single monitor, or when initializing populations parameters uniformly: P . v = Normal ( - 55.0 , 5.0 ) P . g_exc = Normal ( 4.0 , 1.5 ) P . g_inh = Normal ( 20.0 , 12.0 ) Connections # The neurons are randomly connected with a probability of 0.02. Excitatory neurons project on all other neurons with the target \"exc\" and a weight of 0.6, while the inhibitory neurons have the target \"inh\" and a weight of 6.7. Ce = Projection ( pre = Pe , post = P , target = 'exc' ) Ce . connect_fixed_probability ( weights = 0.6 , probability = 0.02 ) Ci = Projection ( pre = Pi , post = P , target = 'inh' ) Ci . connect_fixed_probability ( weights = 6.7 , probability = 0.02 ) <ANNarchy.core.Projection.Projection at 0x7f14e3b0bd90> compile () Simulation # We first define a monitor to record the spikes emitted in the whole population: m = Monitor ( P , [ 'spike' ]) We can then simulate for 1 second: simulate ( 1000. ) We retrieve the recorded spikes from the monitor: data = m . get ( 'spike' ) and compute a raster plot from the data: t , n = m . raster_plot ( data ) t and n are lists representing for each spike emitted during the simulation the time at which it was emitted and the index the neuron which fired. The length of this list represents the total number of spikes in the popultion, so we can compute the population mean firing rate: print ( 'Mean firing rate in the population: ' + str ( len ( t ) / 4000. ) + 'Hz' ) Mean firing rate in the population: 22.295Hz Finally, we can show the raster plot with pylab: import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 12 )) plt . plot ( t , n , '.' , markersize = 0.5 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show ()","title":"COBA"},{"location":"example/COBA/#coba-and-cuba-networks","text":"The scripts COBA.py and CUBA.py in examples/vogels_abbott reproduce the two first benchmarks used in: Brette, R., Rudolph, M., Carnevale, T., Hines, M., Beeman, D., Bower, J. M., et al. (2007), Simulation of networks of spiking neurons: a review of tools and strategies., J. Comput. Neurosci., 23, 3, 349\u201398 Both are based on the balanced network proposed by: Vogels, T. P. and Abbott, L. F. (2005), Signal propagation and logic gating in networks of integrate-and-fire neurons., J. Neurosci., 25, 46, 10786\u201395 The network is composed of 4000 neurons (3200 excitatory and 800 inhibitory), reciprocally connected with a probability of 0.02 (sparse connection). The CUBA network uses a current-based integrate-and-fire neuron model: \\[\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) - g_\\text{inh} (t)\\] while the COBA model uses conductance-based IF neurons: \\[\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) * (E_\\text{exc}) - v(t)) + g_\\text{inh} (t) * (E_\\text{inh}) - v(t)) + I(t)\\] Apart from the neuron model and synaptic weights, both networks are equal, so we'll focus on the COBA network here. The discretization step has to be set to 0.1 ms: from ANNarchy import * setup ( dt = 0.1 ) ANNarchy 4.7 (4.7.0) on linux (posix).","title":"COBA and CUBA networks"},{"location":"example/COBA/#neuron-definition","text":"COBA = Neuron ( parameters = \"\"\" El = -60.0 : population Vr = -60.0 : population Erev_exc = 0.0 : population Erev_inh = -80.0 : population Vt = -50.0 : population tau = 20.0 : population tau_exc = 5.0 : population tau_inh = 10.0 : population I = 20.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I tau_exc * dg_exc/dt = - g_exc tau_inh * dg_inh/dt = - g_inh \"\"\" , spike = \"v > Vt\" , reset = \"v = Vr\" , refractory = 5.0 ) CUBA = Neuron ( parameters = \"\"\" El = -49.0 : population Vr = -60.0 : population Vt = -50.0 : population tau_m = 20.0 : population tau_exc = 5.0 : population tau_inh = 10.0 : population \"\"\" , equations = \"\"\" tau_m * dv/dt = (El - v) + g_exc + g_inh tau_exc * dg_exc/dt = - g_exc tau_inh * dg_inh/dt = - g_inh \"\"\" , spike = \"v > Vt\" , reset = \"v = Vr\" , refractory = 5.0 ) The neurons define exponentially-decreasing conductance g_exc and g_inh for the excitatory and inhibitory conductances/currents, respectively. They also define a refractory period of 5 ms.","title":"Neuron definition"},{"location":"example/COBA/#population","text":"P = Population ( geometry = 4000 , neuron = COBA ) Pe = P [: 3200 ] Pi = P [ 3200 :] We create a population of 4000 COBA neurons, and assign the 3200 first ones to the excitatory population and the 800 last ones to the inhibitory population. It would have been equivalent to declare two separate populations as: Pe = Population ( geometry = 3200 , neuron = COBA ) Pi = Population ( geometry = 800 , neuron = COBA ) but splitting a global population allows to apply methods to all neurons, for example when recording all spikes with a single monitor, or when initializing populations parameters uniformly: P . v = Normal ( - 55.0 , 5.0 ) P . g_exc = Normal ( 4.0 , 1.5 ) P . g_inh = Normal ( 20.0 , 12.0 )","title":"Population"},{"location":"example/COBA/#connections","text":"The neurons are randomly connected with a probability of 0.02. Excitatory neurons project on all other neurons with the target \"exc\" and a weight of 0.6, while the inhibitory neurons have the target \"inh\" and a weight of 6.7. Ce = Projection ( pre = Pe , post = P , target = 'exc' ) Ce . connect_fixed_probability ( weights = 0.6 , probability = 0.02 ) Ci = Projection ( pre = Pi , post = P , target = 'inh' ) Ci . connect_fixed_probability ( weights = 6.7 , probability = 0.02 ) <ANNarchy.core.Projection.Projection at 0x7f14e3b0bd90> compile ()","title":"Connections"},{"location":"example/COBA/#simulation","text":"We first define a monitor to record the spikes emitted in the whole population: m = Monitor ( P , [ 'spike' ]) We can then simulate for 1 second: simulate ( 1000. ) We retrieve the recorded spikes from the monitor: data = m . get ( 'spike' ) and compute a raster plot from the data: t , n = m . raster_plot ( data ) t and n are lists representing for each spike emitted during the simulation the time at which it was emitted and the index the neuron which fired. The length of this list represents the total number of spikes in the popultion, so we can compute the population mean firing rate: print ( 'Mean firing rate in the population: ' + str ( len ( t ) / 4000. ) + 'Hz' ) Mean firing rate in the population: 22.295Hz Finally, we can show the raster plot with pylab: import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 12 )) plt . plot ( t , n , '.' , markersize = 0.5 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show ()","title":"Simulation"},{"location":"example/GapJunctions/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Gap Junctions # A simple network with gap junctions. This is a reimplementation of the Brian example: http://brian2.readthedocs.org/en/2.0b3/examples/synapses.gapjunctions.html from ANNarchy import * clear () setup ( dt = 0.1 ) neuron = Neuron ( parameters = \"v0 = 1.05: population; tau = 10.0: population\" , equations = \"tau*dv/dt = v0 - v + g_gap\" , spike = \"v > 1.\" , reset = \"v = 0.\" ) gap_junction = Synapse ( psp = \"w * (pre.v - post.v)\" ) pop = Population ( 10 , neuron ) pop . v = np . linspace ( 0. , 1. , 10 ) proj = Projection ( pop , pop , 'gap' , gap_junction ) proj . connect_all_to_all ( 0.02 ) trace = Monitor ( pop [ 0 ] + pop [ 5 ], 'v' ) compile () simulate ( 500. ) data = trace . get ( 'v' ) import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . plot ( data [:, 0 ]) plt . plot ( data [:, 1 ]) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( 'v' ) plt . show () ANNarchy 4.7 (4.7.0) on linux (posix). Compiling ... OK","title":"Gap junctions"},{"location":"example/GapJunctions/#gap-junctions","text":"A simple network with gap junctions. This is a reimplementation of the Brian example: http://brian2.readthedocs.org/en/2.0b3/examples/synapses.gapjunctions.html from ANNarchy import * clear () setup ( dt = 0.1 ) neuron = Neuron ( parameters = \"v0 = 1.05: population; tau = 10.0: population\" , equations = \"tau*dv/dt = v0 - v + g_gap\" , spike = \"v > 1.\" , reset = \"v = 0.\" ) gap_junction = Synapse ( psp = \"w * (pre.v - post.v)\" ) pop = Population ( 10 , neuron ) pop . v = np . linspace ( 0. , 1. , 10 ) proj = Projection ( pop , pop , 'gap' , gap_junction ) proj . connect_all_to_all ( 0.02 ) trace = Monitor ( pop [ 0 ] + pop [ 5 ], 'v' ) compile () simulate ( 500. ) data = trace . get ( 'v' ) import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . plot ( data [:, 0 ]) plt . plot ( data [:, 1 ]) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( 'v' ) plt . show () ANNarchy 4.7 (4.7.0) on linux (posix). Compiling ... OK","title":"Gap Junctions"},{"location":"example/HodgkinHuxley/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Hodgkin Huxley neuron # Simple Hodgkin-Huxley neuron. from ANNarchy import * clear () dt = 0.01 setup ( dt = dt ) HH = Neuron ( parameters = \"\"\" C = 1.0 # Capacitance VL = -59.387 # Leak voltage VK = -82.0 # Potassium reversal voltage VNa = 45.0 # Sodium reveral voltage gK = 36.0 # Maximal Potassium conductance gNa = 120.0 # Maximal Sodium conductance gL = 0.3 # Leak conductance vt = 30.0 # Threshold for spike emission I = 0.0 # External current \"\"\" , equations = \"\"\" # Previous membrane potential prev_V = V # Voltage-dependency parameters an = 0.01 * (V + 60.0) / (1.0 - exp(-0.1* (V + 60.0) ) ) am = 0.1 * (V + 45.0) / (1.0 - exp (- 0.1 * ( V + 45.0 ))) ah = 0.07 * exp(- 0.05 * ( V + 70.0 )) bn = 0.125 * exp (- 0.0125 * (V + 70.0)) bm = 4.0 * exp (- (V + 70.0) / 80.0) bh = 1.0/(1.0 + exp (- 0.1 * ( V + 40.0 )) ) # Alpha/Beta functions dn/dt = an * (1.0 - n) - bn * n : init = 0.3, midpoint dm/dt = am * (1.0 - m) - bm * m : init = 0.0, midpoint dh/dt = ah * (1.0 - h) - bh * h : init = 0.6, midpoint # Membrane equation C * dV/dt = gL * (VL - V ) + gK * n**4 * (VK - V) + gNa * m**3 * h * (VNa - V) + I : midpoint \"\"\" , spike = \"\"\" # Spike is emitted when the membrane potential crosses the threshold from below (V > vt) and (prev_V <= vt) \"\"\" , reset = \"\"\" # Nothing to do, it is built-in... \"\"\" ) pop = Population ( neuron = HH , geometry = 1 ) pop . V = - 50.0 compile () m = Monitor ( pop , [ 'spike' , 'V' , 'n' , 'm' , 'h' ]) # Preparation simulate ( 100.0 ) # Current impulse for 1 ms pop . I = 200.0 simulate ( 1.0 ) # Reset pop . I = 0.0 simulate ( 100.0 ) data = m . get () tstart = int ( 90.0 / dt ) tstop = int ( 120.0 / dt ) import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 2 , 2 , 1 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'V' ][ tstart : tstop , 0 ]) plt . title ( 'V' ) plt . subplot ( 2 , 2 , 2 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'n' ][ tstart : tstop , 0 ]) plt . title ( 'n' ) plt . ylim (( 0.0 , 1.0 )) plt . subplot ( 2 , 2 , 3 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'm' ][ tstart : tstop , 0 ]) plt . title ( 'm' ) plt . ylim (( 0.0 , 1.0 )) plt . subplot ( 2 , 2 , 4 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'h' ][ tstart : tstop , 0 ]) plt . title ( 'h' ) plt . ylim (( 0.0 , 1.0 )) plt . show () ANNarchy 4.7 (4.7.0) on linux (posix). Compiling ... OK","title":"Hodgkin-Huxley"},{"location":"example/HodgkinHuxley/#hodgkin-huxley-neuron","text":"Simple Hodgkin-Huxley neuron. from ANNarchy import * clear () dt = 0.01 setup ( dt = dt ) HH = Neuron ( parameters = \"\"\" C = 1.0 # Capacitance VL = -59.387 # Leak voltage VK = -82.0 # Potassium reversal voltage VNa = 45.0 # Sodium reveral voltage gK = 36.0 # Maximal Potassium conductance gNa = 120.0 # Maximal Sodium conductance gL = 0.3 # Leak conductance vt = 30.0 # Threshold for spike emission I = 0.0 # External current \"\"\" , equations = \"\"\" # Previous membrane potential prev_V = V # Voltage-dependency parameters an = 0.01 * (V + 60.0) / (1.0 - exp(-0.1* (V + 60.0) ) ) am = 0.1 * (V + 45.0) / (1.0 - exp (- 0.1 * ( V + 45.0 ))) ah = 0.07 * exp(- 0.05 * ( V + 70.0 )) bn = 0.125 * exp (- 0.0125 * (V + 70.0)) bm = 4.0 * exp (- (V + 70.0) / 80.0) bh = 1.0/(1.0 + exp (- 0.1 * ( V + 40.0 )) ) # Alpha/Beta functions dn/dt = an * (1.0 - n) - bn * n : init = 0.3, midpoint dm/dt = am * (1.0 - m) - bm * m : init = 0.0, midpoint dh/dt = ah * (1.0 - h) - bh * h : init = 0.6, midpoint # Membrane equation C * dV/dt = gL * (VL - V ) + gK * n**4 * (VK - V) + gNa * m**3 * h * (VNa - V) + I : midpoint \"\"\" , spike = \"\"\" # Spike is emitted when the membrane potential crosses the threshold from below (V > vt) and (prev_V <= vt) \"\"\" , reset = \"\"\" # Nothing to do, it is built-in... \"\"\" ) pop = Population ( neuron = HH , geometry = 1 ) pop . V = - 50.0 compile () m = Monitor ( pop , [ 'spike' , 'V' , 'n' , 'm' , 'h' ]) # Preparation simulate ( 100.0 ) # Current impulse for 1 ms pop . I = 200.0 simulate ( 1.0 ) # Reset pop . I = 0.0 simulate ( 100.0 ) data = m . get () tstart = int ( 90.0 / dt ) tstop = int ( 120.0 / dt ) import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 2 , 2 , 1 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'V' ][ tstart : tstop , 0 ]) plt . title ( 'V' ) plt . subplot ( 2 , 2 , 2 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'n' ][ tstart : tstop , 0 ]) plt . title ( 'n' ) plt . ylim (( 0.0 , 1.0 )) plt . subplot ( 2 , 2 , 3 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'm' ][ tstart : tstop , 0 ]) plt . title ( 'm' ) plt . ylim (( 0.0 , 1.0 )) plt . subplot ( 2 , 2 , 4 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'h' ][ tstart : tstop , 0 ]) plt . title ( 'h' ) plt . ylim (( 0.0 , 1.0 )) plt . show () ANNarchy 4.7 (4.7.0) on linux (posix). Compiling ... OK","title":"Hodgkin Huxley neuron"},{"location":"example/Hybrid/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Hybrid network # Simple example showing hybrid spike/rate-coded networks. Reproduces Fig.4 of (Vitay, Dinkelbach and Hamker, 2015) from ANNarchy import * clear () setup ( dt = 0.1 ) # Rate-coded input neuron input_neuron = Neuron ( parameters = \"baseline = 0.0\" , equations = \"r = baseline\" ) # Rate-coded output neuron simple_neuron = Neuron ( equations = \"r = sum(exc)\" ) # Rate-coded population for input pop1 = Population ( geometry = 1 , neuron = input_neuron ) # Poisson Population to encode pop2 = PoissonPopulation ( geometry = 1000 , target = \"exc\" ) proj = Projection ( pop1 , pop2 , 'exc' ) . connect_all_to_all ( weights = 1. ) # Rate-coded population to decode pop3 = Population ( geometry = 1000 , neuron = simple_neuron ) proj = DecodingProjection ( pop2 , pop3 , 'exc' , window = 10.0 ) def diagonal ( pre , post , weights ): \"\"\" Simple connector pattern to progressively connect each post-synaptic neuron to a growing number of pre-synaptic neurons. \"\"\" lil = CSR () for rk_post in range ( post . size ): lil . add ( rk_post , range (( rk_post + 1 )), [ weights ], [ 0 ] ) return lil proj . connect_with_func ( method = diagonal , weights = 1. ) compile () # Monitors m1 = Monitor ( pop1 , 'r' ) m2 = Monitor ( pop2 , 'spike' ) m3 = Monitor ( pop3 , 'r' ) # Simulate duration = 250. # 0 Hz pop1 . baseline = 0.0 simulate ( duration ) # 10 Hz pop1 . baseline = 10.0 simulate ( duration ) # 50 Hz pop1 . baseline = 50.0 simulate ( duration ) # 100 Hz pop1 . baseline = 100.0 simulate ( duration ) # Get recordings data1 = m1 . get () data2 = m2 . get () data3 = m3 . get () # Raster plot of the spiking population t , n = m2 . raster_plot ( data2 [ 'spike' ]) # Variance of the the decoded firing rate data_10 = data3 [ 'r' ][ int ( 1.0 * duration / dt ()): int ( 2 * duration / dt ()), :] data_50 = data3 [ 'r' ][ int ( 2.0 * duration / dt ()): int ( 3 * duration / dt ()), :] data_100 = data3 [ 'r' ][ int ( 3.0 * duration / dt ()): int ( 4 * duration / dt ()), :] var_10 = np . mean ( np . abs (( data_10 - 10. ) / 10. ), axis = 0 ) var_50 = np . mean ( np . abs (( data_50 - 50. ) / 50. ), axis = 0 ) var_100 = np . mean ( np . abs (( data_100 - 100. ) / 100. ), axis = 0 ) ### Plot the results import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 3 , 1 , 1 ) plt . plot ( t , n , '.' , markersize = 0.5 ) plt . title ( 'a) Raster plot' ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neurons' ) plt . xlim (( 0 , 4 * duration )) plt . subplot ( 3 , 1 , 2 ) plt . plot ( np . arange ( 0 , 4 * duration , 0.1 ), data1 [ 'r' ][:, 0 ], label = 'Original firing rate' ) plt . plot ( np . arange ( 0 , 4 * duration , 0.1 ), data3 [ 'r' ][:, 999 ], label = 'Decoded firing rate' ) plt . legend ( frameon = False , loc = 2 ) plt . title ( 'b) Decoded firing rate' ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( 'Activity (Hz)' ) plt . subplot ( 3 , 1 , 3 ) plt . plot ( var_10 , label = '10 Hz' ) plt . plot ( var_50 , label = '50 Hz' ) plt . plot ( var_100 , label = '100 Hz' ) plt . legend ( frameon = False ) plt . title ( 'c) Precision' ) plt . xlabel ( '# neurons used for decoding' ) plt . ylabel ( 'Normalized error' ) plt . ylim (( 0 , 1 )) plt . show () ANNarchy 4.7 (4.7.0) on linux (posix). Compiling ... OK","title":"Hybrid"},{"location":"example/Hybrid/#hybrid-network","text":"Simple example showing hybrid spike/rate-coded networks. Reproduces Fig.4 of (Vitay, Dinkelbach and Hamker, 2015) from ANNarchy import * clear () setup ( dt = 0.1 ) # Rate-coded input neuron input_neuron = Neuron ( parameters = \"baseline = 0.0\" , equations = \"r = baseline\" ) # Rate-coded output neuron simple_neuron = Neuron ( equations = \"r = sum(exc)\" ) # Rate-coded population for input pop1 = Population ( geometry = 1 , neuron = input_neuron ) # Poisson Population to encode pop2 = PoissonPopulation ( geometry = 1000 , target = \"exc\" ) proj = Projection ( pop1 , pop2 , 'exc' ) . connect_all_to_all ( weights = 1. ) # Rate-coded population to decode pop3 = Population ( geometry = 1000 , neuron = simple_neuron ) proj = DecodingProjection ( pop2 , pop3 , 'exc' , window = 10.0 ) def diagonal ( pre , post , weights ): \"\"\" Simple connector pattern to progressively connect each post-synaptic neuron to a growing number of pre-synaptic neurons. \"\"\" lil = CSR () for rk_post in range ( post . size ): lil . add ( rk_post , range (( rk_post + 1 )), [ weights ], [ 0 ] ) return lil proj . connect_with_func ( method = diagonal , weights = 1. ) compile () # Monitors m1 = Monitor ( pop1 , 'r' ) m2 = Monitor ( pop2 , 'spike' ) m3 = Monitor ( pop3 , 'r' ) # Simulate duration = 250. # 0 Hz pop1 . baseline = 0.0 simulate ( duration ) # 10 Hz pop1 . baseline = 10.0 simulate ( duration ) # 50 Hz pop1 . baseline = 50.0 simulate ( duration ) # 100 Hz pop1 . baseline = 100.0 simulate ( duration ) # Get recordings data1 = m1 . get () data2 = m2 . get () data3 = m3 . get () # Raster plot of the spiking population t , n = m2 . raster_plot ( data2 [ 'spike' ]) # Variance of the the decoded firing rate data_10 = data3 [ 'r' ][ int ( 1.0 * duration / dt ()): int ( 2 * duration / dt ()), :] data_50 = data3 [ 'r' ][ int ( 2.0 * duration / dt ()): int ( 3 * duration / dt ()), :] data_100 = data3 [ 'r' ][ int ( 3.0 * duration / dt ()): int ( 4 * duration / dt ()), :] var_10 = np . mean ( np . abs (( data_10 - 10. ) / 10. ), axis = 0 ) var_50 = np . mean ( np . abs (( data_50 - 50. ) / 50. ), axis = 0 ) var_100 = np . mean ( np . abs (( data_100 - 100. ) / 100. ), axis = 0 ) ### Plot the results import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 3 , 1 , 1 ) plt . plot ( t , n , '.' , markersize = 0.5 ) plt . title ( 'a) Raster plot' ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neurons' ) plt . xlim (( 0 , 4 * duration )) plt . subplot ( 3 , 1 , 2 ) plt . plot ( np . arange ( 0 , 4 * duration , 0.1 ), data1 [ 'r' ][:, 0 ], label = 'Original firing rate' ) plt . plot ( np . arange ( 0 , 4 * duration , 0.1 ), data3 [ 'r' ][:, 999 ], label = 'Decoded firing rate' ) plt . legend ( frameon = False , loc = 2 ) plt . title ( 'b) Decoded firing rate' ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( 'Activity (Hz)' ) plt . subplot ( 3 , 1 , 3 ) plt . plot ( var_10 , label = '10 Hz' ) plt . plot ( var_50 , label = '50 Hz' ) plt . plot ( var_100 , label = '100 Hz' ) plt . legend ( frameon = False ) plt . title ( 'c) Precision' ) plt . xlabel ( '# neurons used for decoding' ) plt . ylabel ( 'Normalized error' ) plt . ylim (( 0 , 1 )) plt . show () ANNarchy 4.7 (4.7.0) on linux (posix). Compiling ... OK","title":"Hybrid network"},{"location":"example/Image/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Image Processing # This simple example in examples/image demonstrates how to load images directly into the firing rates of a population and apply basic linear filters on it. It relies on the ANNarchy extensions image and convolution which must be explicitly imported: from ANNarchy import * from ANNarchy.extensions.image import * from ANNarchy.extensions.convolution import Convolution , Pooling clear () ANNarchy 4.7 (4.7.0) on linux (posix). ANNarchy.extensions.image depends on the Python bindings of OpenCV, they must be installed before running the script. We first create an ImagePopulation that will load images: image = ImagePopulation ( geometry = ( 480 , 640 , 3 )) Its geometry specifies the size of the images that can be loaded, here 640x480 RGB images. Note the geometry must be of the form (height, width, channels), where channels is 1 for grayscale images and 3 for color images. The next step is to reduce the size of the image, what can be done by using the Pooling class of the convolution extension. We define a dummy artificial neuron, whose firing rate r will simply be the sum of excitatory connections /ensured to be positive, but this should always be the case). We then create a smaller population pooled with this neuron type, and connect it to the ImagePopulation using mean-pooling: # Simple ANN LinearNeuron = Neuron ( equations = \"r=sum(exc): min=0.0\" ) # Subsampling population pooled = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Mean-pooling projection pool_proj = Pooling ( pre = image , post = pooled , target = 'exc' , operation = 'mean' ) pool_proj . connect_pooling () <ANNarchy.extensions.convolution.Pooling.Pooling at 0x7f0383d44430> The pooled population reduces the size of the image by a factor ten (defined by the size of the population) by averaging the pixels values over 10x10 regions ( operation is set to 'mean' , but one could use 'max' or 'min' ). The connect_pooling() connector creates the \"fake\" connection pattern (as no weights are involved). Let's apply now a 3x3 box filter on each channel of the pooled population: # Smoothing population smoothed = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Box filter projection box_filter = np . ones (( 3 , 3 , 1 )) / 9. smooth_proj = Convolution ( pre = pooled , post = smoothed , target = 'exc' ) smooth_proj . connect_filter ( weights = box_filter ) <ANNarchy.extensions.convolution.Convolve.Convolution at 0x7f03ca2afbe0> To perform a convolution operation on the population (or more precisely a cross-correlation), we call the connect_filter() connector method of the Convolution projection. It requires to define a kernel ( weights ) that will be convolved over the input population. Here we use a simple box filter, but any filter can be used. As the pooled population has three dimensions and we want to smooth the activities per color channel, we need to define a (3, 3, 1) kernel. If we wanted to smooth also over the color channels, we could have used a (3, 3) filter: the resulting population would have the shape (48, 64). We now apply a bank of three filters, each selective to a particular color (red/green/blue). This filters do not have a spatial extent (1x1 convolution), but sum over the third dimension (the color channels): # Convolution population filtered = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Red/Green/Blue filter bank filter_bank = np . array ([ [[ [ 2.0 , - 1.0 , - 1.0 ] ]] , # Red filter [[ [ - 1.0 , 2.0 , - 1.0 ] ]] , # Blue filter [[ [ - 1.0 , - 1.0 , 2.0 ] ]] # Green filter ]) filter_proj = Convolution ( pre = smoothed , post = filtered , target = 'exc' ) filter_proj . connect_filters ( weights = filter_bank ) <ANNarchy.extensions.convolution.Convolve.Convolution at 0x7f0383c6ccd0> Each of the three filter has the shape (1, 1, 3). The result of each convolution would then be (48, 64), but as there are three filters, the output population is (48, 64, 3). The last dimension does not correspond to the number of color channels, but to the number of filters in the bank: if you add a filter, the population will have to be (48, 64, 4). Banks of filters require to use connect_filters() instead of connect_filter() . compile () Compiling ... OK After compilation, we can load an image into the input population: image . set_image ( 'test.jpg' ) To see the result, we need to simulate for four time steps (4 milliseconds, as dt=1.0 ). Step 1: The image population loads the image. Step 2: The pooled population subsamples the image. Step 3: The smoothed population filters the pooled image. Step 4: The bank of filters are applied by filtered . simulate ( 4.0 ) import matplotlib.pyplot as plt fig = plt . figure ( figsize = ( 20.0 , 20.0 )) plt . subplot ( 532 ) plt . imshow ( image . r ) plt . title ( 'Original' ) plt . subplot ( 534 ) plt . imshow ( image . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'image R' ) plt . subplot ( 535 ) plt . imshow ( image . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'image G' ) plt . subplot ( 536 ) plt . imshow ( image . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'image B' ) plt . subplot ( 537 ) plt . imshow ( pooled . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'pooled R' ) plt . subplot ( 538 ) plt . imshow ( pooled . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'pooled G' ) plt . subplot ( 539 ) plt . imshow ( pooled . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'pooled B' ) plt . subplot ( 5 , 3 , 10 ) plt . imshow ( smoothed . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'smoothed R' ) plt . subplot ( 5 , 3 , 11 ) plt . imshow ( smoothed . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'smoothed G' ) plt . subplot ( 5 , 3 , 12 ) plt . imshow ( smoothed . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'smoothed B' ) plt . subplot ( 5 , 3 , 13 ) plt . imshow ( filtered . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'filtered R' ) plt . subplot ( 5 , 3 , 14 ) plt . imshow ( filtered . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'filtered G' ) plt . subplot ( 5 , 3 , 15 ) plt . imshow ( filtered . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'filtered B' ) plt . show ()","title":"Image"},{"location":"example/Image/#image-processing","text":"This simple example in examples/image demonstrates how to load images directly into the firing rates of a population and apply basic linear filters on it. It relies on the ANNarchy extensions image and convolution which must be explicitly imported: from ANNarchy import * from ANNarchy.extensions.image import * from ANNarchy.extensions.convolution import Convolution , Pooling clear () ANNarchy 4.7 (4.7.0) on linux (posix). ANNarchy.extensions.image depends on the Python bindings of OpenCV, they must be installed before running the script. We first create an ImagePopulation that will load images: image = ImagePopulation ( geometry = ( 480 , 640 , 3 )) Its geometry specifies the size of the images that can be loaded, here 640x480 RGB images. Note the geometry must be of the form (height, width, channels), where channels is 1 for grayscale images and 3 for color images. The next step is to reduce the size of the image, what can be done by using the Pooling class of the convolution extension. We define a dummy artificial neuron, whose firing rate r will simply be the sum of excitatory connections /ensured to be positive, but this should always be the case). We then create a smaller population pooled with this neuron type, and connect it to the ImagePopulation using mean-pooling: # Simple ANN LinearNeuron = Neuron ( equations = \"r=sum(exc): min=0.0\" ) # Subsampling population pooled = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Mean-pooling projection pool_proj = Pooling ( pre = image , post = pooled , target = 'exc' , operation = 'mean' ) pool_proj . connect_pooling () <ANNarchy.extensions.convolution.Pooling.Pooling at 0x7f0383d44430> The pooled population reduces the size of the image by a factor ten (defined by the size of the population) by averaging the pixels values over 10x10 regions ( operation is set to 'mean' , but one could use 'max' or 'min' ). The connect_pooling() connector creates the \"fake\" connection pattern (as no weights are involved). Let's apply now a 3x3 box filter on each channel of the pooled population: # Smoothing population smoothed = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Box filter projection box_filter = np . ones (( 3 , 3 , 1 )) / 9. smooth_proj = Convolution ( pre = pooled , post = smoothed , target = 'exc' ) smooth_proj . connect_filter ( weights = box_filter ) <ANNarchy.extensions.convolution.Convolve.Convolution at 0x7f03ca2afbe0> To perform a convolution operation on the population (or more precisely a cross-correlation), we call the connect_filter() connector method of the Convolution projection. It requires to define a kernel ( weights ) that will be convolved over the input population. Here we use a simple box filter, but any filter can be used. As the pooled population has three dimensions and we want to smooth the activities per color channel, we need to define a (3, 3, 1) kernel. If we wanted to smooth also over the color channels, we could have used a (3, 3) filter: the resulting population would have the shape (48, 64). We now apply a bank of three filters, each selective to a particular color (red/green/blue). This filters do not have a spatial extent (1x1 convolution), but sum over the third dimension (the color channels): # Convolution population filtered = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Red/Green/Blue filter bank filter_bank = np . array ([ [[ [ 2.0 , - 1.0 , - 1.0 ] ]] , # Red filter [[ [ - 1.0 , 2.0 , - 1.0 ] ]] , # Blue filter [[ [ - 1.0 , - 1.0 , 2.0 ] ]] # Green filter ]) filter_proj = Convolution ( pre = smoothed , post = filtered , target = 'exc' ) filter_proj . connect_filters ( weights = filter_bank ) <ANNarchy.extensions.convolution.Convolve.Convolution at 0x7f0383c6ccd0> Each of the three filter has the shape (1, 1, 3). The result of each convolution would then be (48, 64), but as there are three filters, the output population is (48, 64, 3). The last dimension does not correspond to the number of color channels, but to the number of filters in the bank: if you add a filter, the population will have to be (48, 64, 4). Banks of filters require to use connect_filters() instead of connect_filter() . compile () Compiling ... OK After compilation, we can load an image into the input population: image . set_image ( 'test.jpg' ) To see the result, we need to simulate for four time steps (4 milliseconds, as dt=1.0 ). Step 1: The image population loads the image. Step 2: The pooled population subsamples the image. Step 3: The smoothed population filters the pooled image. Step 4: The bank of filters are applied by filtered . simulate ( 4.0 ) import matplotlib.pyplot as plt fig = plt . figure ( figsize = ( 20.0 , 20.0 )) plt . subplot ( 532 ) plt . imshow ( image . r ) plt . title ( 'Original' ) plt . subplot ( 534 ) plt . imshow ( image . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'image R' ) plt . subplot ( 535 ) plt . imshow ( image . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'image G' ) plt . subplot ( 536 ) plt . imshow ( image . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'image B' ) plt . subplot ( 537 ) plt . imshow ( pooled . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'pooled R' ) plt . subplot ( 538 ) plt . imshow ( pooled . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'pooled G' ) plt . subplot ( 539 ) plt . imshow ( pooled . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'pooled B' ) plt . subplot ( 5 , 3 , 10 ) plt . imshow ( smoothed . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'smoothed R' ) plt . subplot ( 5 , 3 , 11 ) plt . imshow ( smoothed . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'smoothed G' ) plt . subplot ( 5 , 3 , 12 ) plt . imshow ( smoothed . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'smoothed B' ) plt . subplot ( 5 , 3 , 13 ) plt . imshow ( filtered . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'filtered R' ) plt . subplot ( 5 , 3 , 14 ) plt . imshow ( filtered . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'filtered G' ) plt . subplot ( 5 , 3 , 15 ) plt . imshow ( filtered . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'filtered B' ) plt . show ()","title":"Image Processing"},{"location":"example/Izhikevich/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Izhikevich's pulse-coupled network # This script reproduces the simple pulse-coupled network proposed by Eugene Izhikevich in the article: Izhikevich, E.M. (2003). Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks, 14:6 . The original Matlab code is provided below: % Created by Eugene M. Izhikevich, February 25, 2003 % Excitatory neurons Inhibitory neurons Ne = 800 ; Ni = 200 ; re = rand ( Ne , 1 ); ri = rand ( Ni , 1 ); a = [ 0.02 * ones ( Ne , 1 ); 0.02 + 0.08 * ri ]; b = [ 0.2 * ones ( Ne , 1 ); 0.25 - 0.05 * ri ]; c = [ - 65 + 15 * re .^ 2 ; - 65 * ones ( Ni , 1 )]; d = [ 8 - 6 * re .^ 2 ; 2 * ones ( Ni , 1 )]; S = [ 0.5 * rand ( Ne + Ni , Ne ), - rand ( Ne + Ni , Ni )]; v = - 65 * ones ( Ne + Ni , 1 ); % Initial values of v u = b .* v ; % Initial values of u firings = []; % spike timings for t = 1 : 1000 % simulation of 1000 ms I = [ 5 * randn ( Ne , 1 ); 2 * randn ( Ni , 1 )]; % thalamic input fired = find ( v >= 30 ); % indices of spikes firings = [ firings ; t + 0 * fired , fired ]; v ( fired ) = c ( fired ); u ( fired ) = u ( fired ) + d ( fired ); I = I + sum ( S (:, fired ), 2 ); v = v + 0.5 * ( 0.04 * v .^ 2 + 5 * v + 140 - u + I ); % step 0.5 ms v = v + 0.5 * ( 0.04 * v .^ 2 + 5 * v + 140 - u + I ); % for numerical u = u + a .* ( b .* v - u ); % stability end ; plot ( firings (:, 1 ), firings (:, 2 ), \u2019 . \u2019 ) Neuron type # The network is composed of parameterized quadratic integrate-and-fire neurons, known as Izhikevich neurons. They are simply defined by the following equations: \\[ \\frac{dv}{dt} = 0.04 \\, v^2 + 5 \\, v + 140 - u + I \\] \\[ \\frac{du}{dt} = a \\, (b \\, v - u) \\] The spiking mechanism is defined by: if v > 30.0: emit_spike() v = c u = u + d v is the membrane potential, u is the membrane recovery variable and a , b , c , d are parameters allowing to reproduce many types of neural firing. I is the input voltage to a neuron at each time t . For the desired network, it is the sum of a random value taken from a normal distribution with mean 0.0 and variance 1.0 (multiplied by a scaling factor) and the net effect of incoming spikes (excitatory and inhibitory). Implementing such a neuron in ANNarchy is straightforward: Izhikevich = Neuron ( parameters = \"\"\" noise = 5.0 a = 0.02 b = 0.2 c = -65.0 d = 2.0 v_thresh = 30.0 \"\"\" , equations = \"\"\" I = g_exc - g_inh + noise * Normal(0.0, 1.0) dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I du/dt = a * (b*v - u) \"\"\" , spike = \"\"\" v >= v_thresh \"\"\" , reset = \"\"\" v = c u += d \"\"\" ) The parameters a , b , c , d as well as the noise amplitude noise are declared in the parameters argument, as their value is constant during the simulation. noise is declared as the same throughout the population with the population flag. The equations for v and u are direct translations of their mathematical counterparts. Note the use of dx/dt for the time derivative and ^2 for the square function. The input voltage I is defined as the sum of: the total conductance of excitatory synapses g_exc , the total conductance of inhibitory synapses -g_inh (in this example, we consider all weights to be positive, so we need to invert g_inh in order to model inhibitory synapses), a random number taken from the normal distribution \\(N(0,1)\\) and multiplied by the noise scale noise . In the pulse-coupled network, synapses are considered as instantaneous, i.e. a pre-synaptic spikes increases immediately the post-synaptic conductance proportionally to the weight of the synapse, but does not leave further trace. As this is the default behavior in ANNarchy, nothing has to be specified in the neuron's equations. The spike argument specifies the condition for when a spike should be emitted (here the membrane potential v should be greater than v_thresh ). The reset argument specifies the changes to neural variables that should occur after a spike is emitted: here, the membrane potential is reset to the resting potential c and the membrane recovery variable u is increased from d . The Izhikevich neuron is already defined in ANNarchy, so we will use it directly. Defining the populations # We start by defining a population of 1000 Izhikevich neurons and split it into 800 excitatory neurons and 200 inhibitory ones: from ANNarchy import * clear () pop = Population ( geometry = 1000 , neuron = Izhikevich ) Exc = pop [: 800 ] Inh = pop [ 800 :] ANNarchy 4.7 (4.7.0) on linux (posix). Exc and Inh are subsets of pop , which have the same properties as a population. We can then set parameters differently for each population: re = np . random . random ( 800 ) ; ri = np . random . random ( 200 ) Exc . noise = 5.0 ; Inh . noise = 2.0 Exc . a = 0.02 ; Inh . a = 0.02 + 0.08 * ri Exc . b = 0.2 ; Inh . b = 0.25 - 0.05 * ri Exc . c = - 65.0 + 15.0 * re ** 2 ; Inh . c = - 65.0 Exc . d = 8.0 - 6.0 * re ** 2 ; Inh . d = 2.0 Exc . v = - 65.0 ; Inh . v = - 65.0 Exc . u = Exc . v * Exc . b ; Inh . u = Inh . v * Inh . b Defining the projections # We can now define the connections within the network: The excitatory neurons are connected to all neurons with a weight randomly chosen in [0, 0.5] The inhibitory neurons are connected to all neurons with a weight randomly chosen in [0, 1] exc_proj = Projection ( pre = Exc , post = pop , target = 'exc' ) exc_proj . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) inh_proj = Projection ( pre = Inh , post = pop , target = 'inh' ) inh_proj . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) <ANNarchy.core.Projection.Projection at 0x7fb2896aa2e0> The network is now ready, we can compile: compile () Compiling ... OK Running the simulation # We start by monitoring the spikes and membrane potential in the whole population: M = Monitor ( pop , [ 'spike' , 'v' ]) We run the simulation for 1000 milliseconds: simulate ( 1000.0 , measure_time = True ) Simulating 1.0 seconds of the network took 0.21297645568847656 seconds. We retrieve the recordings, generate a raster plot and the population firing rate: spikes = M . get ( 'spike' ) v = M . get ( 'v' ) t , n = M . raster_plot ( spikes ) fr = M . histogram ( spikes ) We plot: The raster plot of population The evolution of the membrane potential of a single excitatory neuron The population firing rate import matplotlib.pyplot as plt fig = plt . figure ( figsize = ( 12 , 12 )) # First plot: raster plot plt . subplot ( 311 ) plt . plot ( t , n , 'b.' ) plt . title ( 'Raster plot' ) # Second plot: membrane potential of a single excitatory cell plt . subplot ( 312 ) plt . plot ( v [:, 15 ]) # for example plt . title ( 'Membrane potential' ) # Third plot: number of spikes per step in the population. plt . subplot ( 313 ) plt . plot ( fr ) plt . title ( 'Number of spikes' ) plt . xlabel ( 'Time (ms)' ) plt . tight_layout () plt . show ()","title":"Izhikevich"},{"location":"example/Izhikevich/#izhikevichs-pulse-coupled-network","text":"This script reproduces the simple pulse-coupled network proposed by Eugene Izhikevich in the article: Izhikevich, E.M. (2003). Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks, 14:6 . The original Matlab code is provided below: % Created by Eugene M. Izhikevich, February 25, 2003 % Excitatory neurons Inhibitory neurons Ne = 800 ; Ni = 200 ; re = rand ( Ne , 1 ); ri = rand ( Ni , 1 ); a = [ 0.02 * ones ( Ne , 1 ); 0.02 + 0.08 * ri ]; b = [ 0.2 * ones ( Ne , 1 ); 0.25 - 0.05 * ri ]; c = [ - 65 + 15 * re .^ 2 ; - 65 * ones ( Ni , 1 )]; d = [ 8 - 6 * re .^ 2 ; 2 * ones ( Ni , 1 )]; S = [ 0.5 * rand ( Ne + Ni , Ne ), - rand ( Ne + Ni , Ni )]; v = - 65 * ones ( Ne + Ni , 1 ); % Initial values of v u = b .* v ; % Initial values of u firings = []; % spike timings for t = 1 : 1000 % simulation of 1000 ms I = [ 5 * randn ( Ne , 1 ); 2 * randn ( Ni , 1 )]; % thalamic input fired = find ( v >= 30 ); % indices of spikes firings = [ firings ; t + 0 * fired , fired ]; v ( fired ) = c ( fired ); u ( fired ) = u ( fired ) + d ( fired ); I = I + sum ( S (:, fired ), 2 ); v = v + 0.5 * ( 0.04 * v .^ 2 + 5 * v + 140 - u + I ); % step 0.5 ms v = v + 0.5 * ( 0.04 * v .^ 2 + 5 * v + 140 - u + I ); % for numerical u = u + a .* ( b .* v - u ); % stability end ; plot ( firings (:, 1 ), firings (:, 2 ), \u2019 . \u2019 )","title":"Izhikevich's pulse-coupled network"},{"location":"example/Izhikevich/#neuron-type","text":"The network is composed of parameterized quadratic integrate-and-fire neurons, known as Izhikevich neurons. They are simply defined by the following equations: \\[ \\frac{dv}{dt} = 0.04 \\, v^2 + 5 \\, v + 140 - u + I \\] \\[ \\frac{du}{dt} = a \\, (b \\, v - u) \\] The spiking mechanism is defined by: if v > 30.0: emit_spike() v = c u = u + d v is the membrane potential, u is the membrane recovery variable and a , b , c , d are parameters allowing to reproduce many types of neural firing. I is the input voltage to a neuron at each time t . For the desired network, it is the sum of a random value taken from a normal distribution with mean 0.0 and variance 1.0 (multiplied by a scaling factor) and the net effect of incoming spikes (excitatory and inhibitory). Implementing such a neuron in ANNarchy is straightforward: Izhikevich = Neuron ( parameters = \"\"\" noise = 5.0 a = 0.02 b = 0.2 c = -65.0 d = 2.0 v_thresh = 30.0 \"\"\" , equations = \"\"\" I = g_exc - g_inh + noise * Normal(0.0, 1.0) dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I du/dt = a * (b*v - u) \"\"\" , spike = \"\"\" v >= v_thresh \"\"\" , reset = \"\"\" v = c u += d \"\"\" ) The parameters a , b , c , d as well as the noise amplitude noise are declared in the parameters argument, as their value is constant during the simulation. noise is declared as the same throughout the population with the population flag. The equations for v and u are direct translations of their mathematical counterparts. Note the use of dx/dt for the time derivative and ^2 for the square function. The input voltage I is defined as the sum of: the total conductance of excitatory synapses g_exc , the total conductance of inhibitory synapses -g_inh (in this example, we consider all weights to be positive, so we need to invert g_inh in order to model inhibitory synapses), a random number taken from the normal distribution \\(N(0,1)\\) and multiplied by the noise scale noise . In the pulse-coupled network, synapses are considered as instantaneous, i.e. a pre-synaptic spikes increases immediately the post-synaptic conductance proportionally to the weight of the synapse, but does not leave further trace. As this is the default behavior in ANNarchy, nothing has to be specified in the neuron's equations. The spike argument specifies the condition for when a spike should be emitted (here the membrane potential v should be greater than v_thresh ). The reset argument specifies the changes to neural variables that should occur after a spike is emitted: here, the membrane potential is reset to the resting potential c and the membrane recovery variable u is increased from d . The Izhikevich neuron is already defined in ANNarchy, so we will use it directly.","title":"Neuron type"},{"location":"example/Izhikevich/#defining-the-populations","text":"We start by defining a population of 1000 Izhikevich neurons and split it into 800 excitatory neurons and 200 inhibitory ones: from ANNarchy import * clear () pop = Population ( geometry = 1000 , neuron = Izhikevich ) Exc = pop [: 800 ] Inh = pop [ 800 :] ANNarchy 4.7 (4.7.0) on linux (posix). Exc and Inh are subsets of pop , which have the same properties as a population. We can then set parameters differently for each population: re = np . random . random ( 800 ) ; ri = np . random . random ( 200 ) Exc . noise = 5.0 ; Inh . noise = 2.0 Exc . a = 0.02 ; Inh . a = 0.02 + 0.08 * ri Exc . b = 0.2 ; Inh . b = 0.25 - 0.05 * ri Exc . c = - 65.0 + 15.0 * re ** 2 ; Inh . c = - 65.0 Exc . d = 8.0 - 6.0 * re ** 2 ; Inh . d = 2.0 Exc . v = - 65.0 ; Inh . v = - 65.0 Exc . u = Exc . v * Exc . b ; Inh . u = Inh . v * Inh . b","title":"Defining the populations"},{"location":"example/Izhikevich/#defining-the-projections","text":"We can now define the connections within the network: The excitatory neurons are connected to all neurons with a weight randomly chosen in [0, 0.5] The inhibitory neurons are connected to all neurons with a weight randomly chosen in [0, 1] exc_proj = Projection ( pre = Exc , post = pop , target = 'exc' ) exc_proj . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) inh_proj = Projection ( pre = Inh , post = pop , target = 'inh' ) inh_proj . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) <ANNarchy.core.Projection.Projection at 0x7fb2896aa2e0> The network is now ready, we can compile: compile () Compiling ... OK","title":"Defining the projections"},{"location":"example/Izhikevich/#running-the-simulation","text":"We start by monitoring the spikes and membrane potential in the whole population: M = Monitor ( pop , [ 'spike' , 'v' ]) We run the simulation for 1000 milliseconds: simulate ( 1000.0 , measure_time = True ) Simulating 1.0 seconds of the network took 0.21297645568847656 seconds. We retrieve the recordings, generate a raster plot and the population firing rate: spikes = M . get ( 'spike' ) v = M . get ( 'v' ) t , n = M . raster_plot ( spikes ) fr = M . histogram ( spikes ) We plot: The raster plot of population The evolution of the membrane potential of a single excitatory neuron The population firing rate import matplotlib.pyplot as plt fig = plt . figure ( figsize = ( 12 , 12 )) # First plot: raster plot plt . subplot ( 311 ) plt . plot ( t , n , 'b.' ) plt . title ( 'Raster plot' ) # Second plot: membrane potential of a single excitatory cell plt . subplot ( 312 ) plt . plot ( v [:, 15 ]) # for example plt . title ( 'Membrane potential' ) # Third plot: number of spikes per step in the population. plt . subplot ( 313 ) plt . plot ( fr ) plt . title ( 'Number of spikes' ) plt . xlabel ( 'Time (ms)' ) plt . tight_layout () plt . show ()","title":"Running the simulation"},{"location":"example/List/","text":"List of notebooks # This section provides a list of the sample models provided in the examples/ directory of the source code. Rate-coded networks # Neural Field : a simple model using neural field recurrent networks. This is a very simple rate-coded model without learning. Bar learning : an implementation of the bar learning problem, illustrating synaptic plasticity in rate-coded networks. Image and Webcam : shows how to use the ImagePopulation and VideoPopulation classes of the image extension to clamp directly images and video streams into a rate-coded network. Also demonstrate the convolution extension. Structural Plasticity : a dummy example using structural plasticity. Multiple Networks : shows how to use multiple networks and call parallel_run to run several networks in parallel. Spiking networks # Simple networks Izhikevich : an implementation of the simple pulse-coupled network described in (Izhikevich, 2003). It shows how to build a simple spiking network without synaptic plasticity. Gap Junctions : an example using gap junctions. example/HodgkinHuxley {.interpreted-text role=\"doc\"}: a single Hodgkin-Huxley neuron. A collection of Brain/PyNN/NEST model reproductions in the folder examples/pyNN . Complex networks COBA : an implementation of the balanced network described in (Vogels and Abbott, 2005). It shows how to build a simple spiking network using integrate-and-fire neurons and sparse connectivity. STP : an example of short-term plasticity based on the model of Tsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses . The Journal of Neuroscience. With synaptic plasticity STDP : a simple example using spike-timing dependent plasticity (STDP). Ramp : an example of homeostatic STDP based on the model of Carlson, Richert, Dutt and Krichmar (2013). Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks . IJCNN. Hybrid networks # Hybrid networks : a simple hybrid network with both rate-coded and spiking parts. General # Bayesian optimization : a demo showing how to use hyperopt to search for hyperparameters of a model. Basal Ganglia : a simple basal ganglia model to show how to use the tensorboard extension. BOLD monitoring : a showcase of the bold extension allowing to record BOLD signals fron a network.","title":"List of notebooks"},{"location":"example/List/#list-of-notebooks","text":"This section provides a list of the sample models provided in the examples/ directory of the source code.","title":"List of notebooks"},{"location":"example/List/#rate-coded-networks","text":"Neural Field : a simple model using neural field recurrent networks. This is a very simple rate-coded model without learning. Bar learning : an implementation of the bar learning problem, illustrating synaptic plasticity in rate-coded networks. Image and Webcam : shows how to use the ImagePopulation and VideoPopulation classes of the image extension to clamp directly images and video streams into a rate-coded network. Also demonstrate the convolution extension. Structural Plasticity : a dummy example using structural plasticity. Multiple Networks : shows how to use multiple networks and call parallel_run to run several networks in parallel.","title":"Rate-coded networks"},{"location":"example/List/#spiking-networks","text":"Simple networks Izhikevich : an implementation of the simple pulse-coupled network described in (Izhikevich, 2003). It shows how to build a simple spiking network without synaptic plasticity. Gap Junctions : an example using gap junctions. example/HodgkinHuxley {.interpreted-text role=\"doc\"}: a single Hodgkin-Huxley neuron. A collection of Brain/PyNN/NEST model reproductions in the folder examples/pyNN . Complex networks COBA : an implementation of the balanced network described in (Vogels and Abbott, 2005). It shows how to build a simple spiking network using integrate-and-fire neurons and sparse connectivity. STP : an example of short-term plasticity based on the model of Tsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses . The Journal of Neuroscience. With synaptic plasticity STDP : a simple example using spike-timing dependent plasticity (STDP). Ramp : an example of homeostatic STDP based on the model of Carlson, Richert, Dutt and Krichmar (2013). Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks . IJCNN.","title":"Spiking networks"},{"location":"example/List/#hybrid-networks","text":"Hybrid networks : a simple hybrid network with both rate-coded and spiking parts.","title":"Hybrid networks"},{"location":"example/List/#general","text":"Bayesian optimization : a demo showing how to use hyperopt to search for hyperparameters of a model. Basal Ganglia : a simple basal ganglia model to show how to use the tensorboard extension. BOLD monitoring : a showcase of the bold extension allowing to record BOLD signals fron a network.","title":"General"},{"location":"example/MultipleNetworks/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Multiple networks # This example demonstrates the use of multiple networks in a single script and the use of parallel_run() : from ANNarchy import * clear () # Create the whole population P = Population ( geometry = 1000 , neuron = Izhikevich ) # Create the excitatory population Exc = P [: 800 ] re = np . random . random ( 800 ) Exc . noise = 5.0 Exc . a = 0.02 Exc . b = 0.2 Exc . c = - 65.0 + 15.0 * re ** 2 Exc . d = 8.0 - 6.0 * re ** 2 Exc . v = - 65.0 Exc . u = Exc . v * Exc . b # Create the Inh population Inh = P [ 800 :] ri = np . random . random ( 200 ) Inh . noise = 2.0 Inh . a = 0.02 + 0.08 * ri Inh . b = 0.25 - 0.05 * ri Inh . c = - 65.0 Inh . d = 2.0 Inh . v = - 65.0 Inh . u = Inh . v * Inh . b # Create the projections proj_exc = Projection ( Exc , P , 'exc' ) proj_inh = Projection ( Inh , P , 'inh' ) proj_exc . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) proj_inh . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) # Create a spike monitor M = Monitor ( P , 'spike' ) compile () # Create a network with specified populations and projections net = Network () net . add ( P ) net . add ([ proj_exc , proj_inh ]) net . add ( M ) net . compile () # Create a network based on everything created until now (equivalent) net2 = Network ( everything = True ) net2 . compile () # Method to be applied on each network def run_network ( idx , net ): # Retrieve subpopulations P_local = net . get ( P ) Exc = P_local [: 800 ] Inh = P_local [ 800 :] # Randomize initialization re = np . random . random ( 800 ) Exc . c = - 65.0 + 15.0 * re ** 2 Exc . d = 8.0 - 6.0 * re ** 2 ri = np . random . random ( 200 ) Inh . noise = 2.0 Inh . a = 0.02 + 0.08 * ri Inh . b = 0.25 - 0.05 * ri Inh . u = Inh . v * Inh . b # Simulate net . simulate ( 1000. ) # Recordings t , n = net . get ( M ) . raster_plot () return t , n # Simulating using the created networks vals = parallel_run ( method = run_network , networks = [ net , net2 ], measure_time = True , sequential = True ) vals = parallel_run ( method = run_network , networks = [ net , net2 ], measure_time = True ) # Using just a number of networks to create vals = parallel_run ( method = run_network , number = 4 , measure_time = True ) # Data analysis t , n = vals [ 0 ] t2 , n2 = vals [ 1 ] import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 121 ) plt . plot ( t , n , '.' ) plt . subplot ( 122 ) plt . plot ( t2 , n2 , '.' ) plt . show () ANNarchy 4.7 (4.7.0) on linux (posix). Compiling network 1... OK Compiling network 2... OK Running 2 networks sequentially took: 0.3665199279785156 Running 2 networks in parallel took: 0.3787410259246826 Running 4 networks in parallel took: 1.0636274814605713","title":"Multiple networks"},{"location":"example/MultipleNetworks/#multiple-networks","text":"This example demonstrates the use of multiple networks in a single script and the use of parallel_run() : from ANNarchy import * clear () # Create the whole population P = Population ( geometry = 1000 , neuron = Izhikevich ) # Create the excitatory population Exc = P [: 800 ] re = np . random . random ( 800 ) Exc . noise = 5.0 Exc . a = 0.02 Exc . b = 0.2 Exc . c = - 65.0 + 15.0 * re ** 2 Exc . d = 8.0 - 6.0 * re ** 2 Exc . v = - 65.0 Exc . u = Exc . v * Exc . b # Create the Inh population Inh = P [ 800 :] ri = np . random . random ( 200 ) Inh . noise = 2.0 Inh . a = 0.02 + 0.08 * ri Inh . b = 0.25 - 0.05 * ri Inh . c = - 65.0 Inh . d = 2.0 Inh . v = - 65.0 Inh . u = Inh . v * Inh . b # Create the projections proj_exc = Projection ( Exc , P , 'exc' ) proj_inh = Projection ( Inh , P , 'inh' ) proj_exc . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) proj_inh . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) # Create a spike monitor M = Monitor ( P , 'spike' ) compile () # Create a network with specified populations and projections net = Network () net . add ( P ) net . add ([ proj_exc , proj_inh ]) net . add ( M ) net . compile () # Create a network based on everything created until now (equivalent) net2 = Network ( everything = True ) net2 . compile () # Method to be applied on each network def run_network ( idx , net ): # Retrieve subpopulations P_local = net . get ( P ) Exc = P_local [: 800 ] Inh = P_local [ 800 :] # Randomize initialization re = np . random . random ( 800 ) Exc . c = - 65.0 + 15.0 * re ** 2 Exc . d = 8.0 - 6.0 * re ** 2 ri = np . random . random ( 200 ) Inh . noise = 2.0 Inh . a = 0.02 + 0.08 * ri Inh . b = 0.25 - 0.05 * ri Inh . u = Inh . v * Inh . b # Simulate net . simulate ( 1000. ) # Recordings t , n = net . get ( M ) . raster_plot () return t , n # Simulating using the created networks vals = parallel_run ( method = run_network , networks = [ net , net2 ], measure_time = True , sequential = True ) vals = parallel_run ( method = run_network , networks = [ net , net2 ], measure_time = True ) # Using just a number of networks to create vals = parallel_run ( method = run_network , number = 4 , measure_time = True ) # Data analysis t , n = vals [ 0 ] t2 , n2 = vals [ 1 ] import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 121 ) plt . plot ( t , n , '.' ) plt . subplot ( 122 ) plt . plot ( t2 , n2 , '.' ) plt . show () ANNarchy 4.7 (4.7.0) on linux (posix). Compiling network 1... OK Compiling network 2... OK Running 2 networks sequentially took: 0.3665199279785156 Running 2 networks in parallel took: 0.3787410259246826 Running 4 networks in parallel took: 1.0636274814605713","title":"Multiple networks"},{"location":"example/NeuralField/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Neural Field # The folder examples/neural_field contains a simple rate-coded model using Neural Fields . It consists of two 2D populations inp and focus , with one-to-one connections between inp and focus , and Difference-of-Gaussians (DoG) lateral connections within focus . If you have PyQtGraph and python-openGL installed, you can simply try the network by typing: python NeuralField.py Model overview # Each population consists of N*N neurons, with N=20. The inp population is solely used to represent inputs for focus . The firing rate of each neuron is defined by a simple equation: \\[r_i(t) = (\\text{baseline}_i(t) + \\eta(t))^+\\] where \\(r_i(t)\\) is the instantaneous firing rate, \\(\\text{baseline}_i(t)\\) its baseline activity, \\(\\eta(t)\\) an additive noise uniformly taken in \\([-0.5, 0.5]\\) and \\(()^+\\) the positive function. The focus population implements a discretized neural field, with neurons following the ODE: \\[\\tau \\frac{d r_i(t)}{dt} + r_i(t) = r^\\text{input}_i(t) + \\sum_{j=1}^{N} w_{j, i} \\cdot r_j(t) + \\eta(t)\\] where \\(r_i(t)\\) is the neuron's firing rate, \\(\\tau\\) a time constant and \\(w_{j, i}\\) the weight value (synaptic efficiency) of the synapse between the neurons j and i. \\(f()\\) is a semi-linear function, ensuring the firing rate is bounded between 0 and 1. Each neuron in focus takes inputs from the neuron of inp which has the same position, leading to a one_to_one connection pattern. The lateral connections within focus follow a difference-of-Gaussians ( dog ) connection pattern, with the connection weights \\(w_{i,j}\\) depending on the normalized euclidian distance between the neurons in the N*N population: \\[w_{j, i} = A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_+^2}) - A^- \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_-^2})\\] If i and j have coordinates \\((x_i, y_i)\\) and \\((x_j, y_j)\\) in the N*N space, the distance between them is computed as: \\[d(i, j)^2 = (\\frac{x_i - x_j}{N})^2 + (\\frac{y_i - y_j}{N})^2\\] Inputs are given to the network by changing the baseline of inp neurons. This example clamps one or several gaussian profiles (called \"bubbles\") with an additive noise, moving along a circular path at a certain speed (launch the example to understand this sentence...). Importing ANNarchy # The beginning of the script solely consists of importing the ANNarchy library: from ANNarchy import * clear () ANNarchy 4.7 (4.7.0) on linux (posix). If you want to run the simulation on your graphic card instead of CPU, simply uncomment the following line: #setup(paradigm=\"cuda\") The setup() method allows to configure ANNarchy to run in different modes, such as chosing the parallel framework (omp or cuda), setting the simulation step dt , the numerical method method or the seed of the random number generators. Defining the neurons # Input neuron # InputNeuron = Neuron ( parameters = \"\"\" baseline = 0.0 \"\"\" , equations = \"\"\" r = pos(baseline + Uniform(-0.5, 0.5)) \"\"\" ) Defining the input neuron is straightforward. InputNeuron is here an instance of Neuron , whose only parameter is baseline (initialized to 0.0, but it does not matter here as it will be set externally). The firing rate of each neuron, r , is updated at every time step as the positive part ( pos() ) of the sum of the baseline and a random number taken from a uniform distribution between -0.5 and 0.5. Neural Field neuron # NeuralFieldNeuron = Neuron ( parameters = \"\"\" tau = 10.0 : population \"\"\" , equations = \"\"\" tau * dr/dt + r = sum(exc) + sum(inh) + Uniform(-0.5, 0.5) : min=0.0, max=1.0 \"\"\" ) The second neuron we need is a bit more complex, as it is governed by an ODE and considers inputs from other neurons. It also has a non-linear activation function, which is linear when the firing rate is between 0.0 and 1.0, and constant otherwise. tau is a population-wise parameter, whose value will be the same for all neuron of the population. r is the firing rate of he neuron, whose dynamics are governed by a first-order linear ODE, integrating the sums of excitatory and inhibitory inputs with noise. As explained in the manual for rate-coded neurons, sum(exc) retrieves the weighted sum of pre-synaptic firing rates for the synapses having the connection type exc , here the one_to_one connections between inp and focus . sum(inh) does the same for inh type connections, here the lateral connections within focus . The firing rate is restricted to the range [0, 1] by setting the min and max accordingly in the flags section (everything after the : ). This means that after evaluating the ODE and getting a new value for r , its value will be clamped if it outside these values. One can define both min and max , only one, or none. Creating the populations # The two populations have a geometry of (20, 20), therefore 400 neurons each. They are created simply by instantiating the Population class: N = 20 inp = Population ( geometry = ( N , N ), neuron = InputNeuron , name = 'Input' ) focus = Population ( geometry = ( N , N ), neuron = NeuralFieldNeuron , name = 'Focus' ) The populations can be assigned a unique name (here 'Input' and 'Focus') in order to be be able to retrieve them if the references inp and focus are lost. They are given a 2D geometry and associated to the corresponding Neuron instance. Creating the projections # The first projection is a one-to-one projection from Input to Focus with the type 'exc'. This connection pattern pattern is possible because the two populations have the same geometry. The weights are initialized to 1.0, and this value will not change with time (no learning), so it is not necessary to define a synapse type: ff = Projection ( pre = inp , post = focus , target = 'exc' ) ff . connect_one_to_one ( weights = 1.0 , delays = 20.0 ) <ANNarchy.core.Projection.Projection at 0x7f106254c3a0> The references to the pre- and post-synaptic population (or their names), as well as the target type, are passed to the constructor of Projection . The connector method connect_one_to_one() is immediately applied to the Projection, defining how many synapses will be created. The weights are initialized uniformly to 1.0. The second projection is a difference of gaussians (DoG) for the lateral connections within 'focus'. The connector method is already provided by ANNarchy, so there is nothing more to do than to call it with the right parameters: lat = Projection ( pre = focus , post = focus , target = 'inh' ) lat . connect_dog ( amp_pos = 0.2 , sigma_pos = 0.1 , amp_neg = 0.1 , sigma_neg = 0.7 ) <ANNarchy.core.Projection.Projection at 0x7f1062543fa0> Compiling the network and simulating # Once the populations and projections are created, the network is ready to be generated, compiled and simulated. Compilation is simply done by calling compile() : compile () Compiling ... OK This generates optimized C++ code from the neurons' definition and network structure, compiles it with gcc/clang and instantiates all objects, particularly the synapses. If some errors were made in the neuron definition, they will be signaled at this point. Hint: The call to compile() is mandatory in any script. After it is called, populations and projections can not be added anymore. Once the compilation is successful, the network can be simulated by calling simulate() : simulate ( 1000.0 ) # simulate for 1 second As no input has been fed into the network, calling simulate() now won't lead to anything interesting. The next step is to clamp inputs into the input population's baseline. Setting inputs # Pure Python approach # In this example, we consider as input a moving bubble of activity rotating along a circle in the input space in 5 seconds. A naive way of setting such inputs would be to access population attributes (namely inp.baseline ) in a tight loop in Python: angle = 0.0 x , y = np . meshgrid ( np . linspace ( 0 , 19 , 20 ), np . linspace ( 0 , 19 , 20 )) # Main loop while True : # Update the angle angle += 1.0 / 5000.0 # Compute the center of the bubble cx = 10.0 * ( 1.0 + 0.5 * np . cos ( 2.0 * np . pi * angle ) ) cy = 10.0 * ( 1.0 + 0.5 * np . sin ( 2.0 * np . pi * angle ) ) # Clamp the bubble into pop.baseline inp . baseline = ( np . exp ( - (( x - cx ) ** 2 + ( y - cy ) ** 2 ) / 8.0 )) # Simulate for 1 ms step () angle represents the angle made by the bubble with respect to the center of the input population. x and y are Numpy arrays representing the X- and Y- coordinates of neurons in the input population. At each iteration of the simulation (i.e. every millisecond of simulation, the bubble is slightly rotated ( angle is incremented) so as to make a complete revolution in 5 seconds (5000 steps). cx and cy represent the coordinates of the center of the bubble in neural coordinates according to the new value of the angle. A Gaussian profile (in the form of a Numpy array) is then clamped into the baseline of inp using the distance between each neuron of the population ( x and y ) and the center of the bubble. Last, a single simulation step is performed using step() , before the whole process starts again until the user quits. step() is equivalent to simulate(1) , although a little bit faster as it does not check anything. Although this approach works, you would observe that it is very slow: the computation of the bubble and its feeding into InputPop takes much more time than the call to step() . The interest of using a parallel simulator disappears. This is due to the fact that Python is knowingly bad at performing tight loops because of its interpreted nature. If the while loop were compiled from C code, the computation would be much more efficient. This is what Cython brings you. Cython approach # Generalities on Cython The Cython approach requires to write Cython-specific code in a .pyx file, generate the corresponding C code with Python access methods, compile it and later import it into your Python code. Happily, the Cython syntax is very close to Python. In the most basic approach, it is simply Python code with a couple of type declarations. Instead of: bar = 1 foo = np . ones (( 10 , 10 )) you would write in Cython: cdef int bar = 1 cdef np . ndarray foo = np . ones (( 10 , 10 )) By specifing the type of a variable (which can not be changed later contrary to Python), you help Cython generate optimized C code, what can lead in some cases to speedups up to 100x. The rest of the syntax (indentation, for loops, if...) is the same as in Python. You can als import any Python module in your Cython code. Some modules (importantly Numpy) even provide a Cython interface where the equivalent Cython code can be directly imported (so it becomes very fast to use). The whole compilation procedure is very easy. One particularly simple approach is to use the pyximport module shipped with Cython. Let us suppose you wrote a dummy() method in a Cython file named TestModule.pyx . All you need to use this method in your python code is to write: import pyximport ; pyximport . install () from TestModule import dummy dummy () pyximport takes care of the compilation process (but emits quite a lot of warnings that can be ignored), and allows to import TestModule as if it were a regular Python module. Please refer to the Cython documentation to know more. Moving bubbles in Cython The file BubbleWorld.pyx defines a World class able to rotate the bubble for a specified duration. import numpy as np cimport numpy as np At the beginning of the file, numpy is imported once as a normal Python module with import , and once as a Cython module with cimport . This allows our Cython module to access directly the internal representations of Numpy without going through the Python interpreter. We can then define a World class taking as parameters: the population which will be used as input (here Input ), several arguments such as radius , sigma and period which allow to parameterize the behavior of the rotating bubble, func which is the Python method that will be called at each time step, i.e.e the step() method of ANNarchy. cdef class World : \" Environment class allowing to clamp a rotating bubble into the baseline of a population.\" cdef pop # Input population cdef func # Function to call cdef float angle # Current angle cdef float radius # Radius of the circle cdef float sigma # Width of the bubble cdef float period # Number of steps needed to make one revolution cdef np . ndarray xx , yy # indices cdef float cx , cy , midw , midh cdef np . ndarray data def __cinit__ ( self , population , radius , sigma , period , func ): \" Constructor\" self . pop = population self . func = func self . angle = 0.0 self . radius = radius self . sigma = sigma self . period = period cdef np . ndarray x = np . linspace ( 0 , self . pop . geometry [ 0 ] - 1 , self . pop . geometry [ 0 ]) cdef np . ndarray y = np . linspace ( 0 , self . pop . geometry [ 1 ] - 1 , self . pop . geometry [ 1 ]) self . xx , self . yy = np . meshgrid ( x , y ) self . midw = self . pop . geometry [ 0 ] / 2 self . midh = self . pop . geometry [ 1 ] / 2 def rotate ( self , int duration ): \" Rotates the bubble for the given duration\" cdef int t for t in xrange ( duration ): # Update the angle self . angle += 1.0 / self . period # Compute the center of the bubble self . cx = self . midw * ( 1.0 + self . radius * np . cos ( 2.0 * np . pi * self . angle ) ) self . cy = self . midh * ( 1.0 + self . radius * np . sin ( 2.0 * np . pi * self . angle ) ) # Create the bubble self . data = ( np . exp ( - (( self . xx - self . cx ) ** 2 + ( self . yy - self . cy ) ** 2 ) / 2.0 / self . sigma ** 2 )) # Clamp the bubble into pop.baseline self . pop . baseline = self . data # Simulate for 1 step self . func () Although this tutorial won't go into much detail, you can note the following: The data given to or initialized in the constructor are previously declared (with their type) as attributes of the class. This way, Cython knows at the compilation time which operations are possible on them, which amount of memory to allocate and so on, resulting in a more efficient implementation. The input population ( self.pop ) can be accessed as a normal Python object. In particular, self.pop.geometry is used in the constructor to initialize the meshgrid. The method rotate() performs the simulation for the given duration (in steps, not milliseconds). Its content is relatively similar to the Python version. Running the simulation Once the environment has been defined, the simulation can be executed. The following code, to be placed after the network definition, performs a simulation of the network, taking inputs from BubbleWorld.pyx , during 2 seconds: # Create the environment import pyximport ; pyximport . install ( setup_args = { 'include_dirs' : np . get_include ()}) from BubbleWorld import World world = World ( population = inp , radius = 0.5 , sigma = 2.0 , period = 5000.0 , func = step ) # Simulate for 2 seconds with inputs world . rotate ( 2000 ) Visualizing the network # The preceding code performs correctly the intended simulation, but nothing is visualized. The user has all freedom to visualize his network the way he prefers (for example through animated Matplotlib figures): import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 121 ) plt . imshow ( inp . r , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . subplot ( 122 ) plt . imshow ( focus . r , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . show () However, Matplotlib animations are rather slow, and visualizing the network at each time step would take more time than running the simulation. The provided example takes advantage of the PyQtGraph library (www.pyqtgraph.org) to visualize efficiently activity in the network using OpenGL. The following class and method is defined in Viz.py : # Visualizer using PyQtGraph try : from pyqtgraph.Qt import QtGui , QtCore import pyqtgraph as pg except : print ( 'PyQtGraph is not installed on your system, can not visualize the network.' ) exit ( 0 ) try : import pyqtgraph.opengl as gl except : print ( 'OpenGL is not installed on your system, can not visualize the network.' ) exit ( 0 ) import numpy as np class GLViewer ( object ): \" Class to visualize the network activity using PyQtGraph and openGL.\" def __init__ ( self , populations , func , update_rate ): # Parameters self . populations = populations self . func = func self . update_rate = update_rate # Window self . win = gl . GLViewWidget () self . win . show () self . win . setCameraPosition ( distance = 40 ) # Prepare the plots self . plots = [] shift = 0 for pop in self . populations : p = gl . GLSurfacePlotItem ( x = np . linspace ( 0 , pop . geometry [ 0 ] - 1 , pop . geometry [ 0 ]), y = np . linspace ( 0 , pop . geometry [ 1 ] - 1 , pop . geometry [ 1 ]), shader = 'heightColor' , computeNormals = False , smooth = False ) p . translate ( shift , - 10 , - 1 ) self . win . addItem ( p ) self . plots . append ( p ) shift -= 25 def scale ( self , data ): \" Colors are shown in the range [-1, 1] per default.\" return 1.8 * data - 0.9 def update ( self ): \"Callback\" # Simulate for 200ms self . func ( self . update_rate ) # Refresh the GUI for i in range ( len ( self . populations )): self . plots [ i ] . setData ( z = self . scale ( self . populations [ i ] . r )) # Listen to mouse/keyboard events QtGui . QApplication . processEvents () def run ( self ): \"Inifinite loop\" timer = QtCore . QTimer () timer . timeout . connect ( self . update ) timer . start ( 0 ) QtGui . QApplication . instance () . exec_ () def loop_bubbles ( populations , func , update_rate ): \"Launches the GL GUI and rotates the bubble infinitely.\" # Create the GUI using PyQtGraph app = QtGui . QApplication ([]) viewer = GLViewer ( populations , func , update_rate ) # Start the simulation forever viewer . run () We leave out again the details about this class (please look at the examples and tutorials on the PyQtGraph website to understand it). It allows to open a PyQtGraph window and display the firing rate of both Input and Focus populations using OpenGL. The run() method is an endless loop calling regularly the update() method. The update() method calls first World.rotate(200) and waits for its completion before reactualizing the display. The reason is that refreshing the display can only be done sequentially with the simulation, and calling it too often would impair the simulation time. Once this class has been defined, the simulation can be run endlessly by importing the Viz module: # Launch the GUI and run the simulation from Viz import loop_bubbles loop_bubbles ( populations = [ inp , focus ], func = world . rotate , update_rate = 200 )","title":"Neural Field"},{"location":"example/NeuralField/#neural-field","text":"The folder examples/neural_field contains a simple rate-coded model using Neural Fields . It consists of two 2D populations inp and focus , with one-to-one connections between inp and focus , and Difference-of-Gaussians (DoG) lateral connections within focus . If you have PyQtGraph and python-openGL installed, you can simply try the network by typing: python NeuralField.py","title":"Neural Field"},{"location":"example/NeuralField/#model-overview","text":"Each population consists of N*N neurons, with N=20. The inp population is solely used to represent inputs for focus . The firing rate of each neuron is defined by a simple equation: \\[r_i(t) = (\\text{baseline}_i(t) + \\eta(t))^+\\] where \\(r_i(t)\\) is the instantaneous firing rate, \\(\\text{baseline}_i(t)\\) its baseline activity, \\(\\eta(t)\\) an additive noise uniformly taken in \\([-0.5, 0.5]\\) and \\(()^+\\) the positive function. The focus population implements a discretized neural field, with neurons following the ODE: \\[\\tau \\frac{d r_i(t)}{dt} + r_i(t) = r^\\text{input}_i(t) + \\sum_{j=1}^{N} w_{j, i} \\cdot r_j(t) + \\eta(t)\\] where \\(r_i(t)\\) is the neuron's firing rate, \\(\\tau\\) a time constant and \\(w_{j, i}\\) the weight value (synaptic efficiency) of the synapse between the neurons j and i. \\(f()\\) is a semi-linear function, ensuring the firing rate is bounded between 0 and 1. Each neuron in focus takes inputs from the neuron of inp which has the same position, leading to a one_to_one connection pattern. The lateral connections within focus follow a difference-of-Gaussians ( dog ) connection pattern, with the connection weights \\(w_{i,j}\\) depending on the normalized euclidian distance between the neurons in the N*N population: \\[w_{j, i} = A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_+^2}) - A^- \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_-^2})\\] If i and j have coordinates \\((x_i, y_i)\\) and \\((x_j, y_j)\\) in the N*N space, the distance between them is computed as: \\[d(i, j)^2 = (\\frac{x_i - x_j}{N})^2 + (\\frac{y_i - y_j}{N})^2\\] Inputs are given to the network by changing the baseline of inp neurons. This example clamps one or several gaussian profiles (called \"bubbles\") with an additive noise, moving along a circular path at a certain speed (launch the example to understand this sentence...).","title":"Model overview"},{"location":"example/NeuralField/#importing-annarchy","text":"The beginning of the script solely consists of importing the ANNarchy library: from ANNarchy import * clear () ANNarchy 4.7 (4.7.0) on linux (posix). If you want to run the simulation on your graphic card instead of CPU, simply uncomment the following line: #setup(paradigm=\"cuda\") The setup() method allows to configure ANNarchy to run in different modes, such as chosing the parallel framework (omp or cuda), setting the simulation step dt , the numerical method method or the seed of the random number generators.","title":"Importing ANNarchy"},{"location":"example/NeuralField/#defining-the-neurons","text":"","title":"Defining the neurons"},{"location":"example/NeuralField/#input-neuron","text":"InputNeuron = Neuron ( parameters = \"\"\" baseline = 0.0 \"\"\" , equations = \"\"\" r = pos(baseline + Uniform(-0.5, 0.5)) \"\"\" ) Defining the input neuron is straightforward. InputNeuron is here an instance of Neuron , whose only parameter is baseline (initialized to 0.0, but it does not matter here as it will be set externally). The firing rate of each neuron, r , is updated at every time step as the positive part ( pos() ) of the sum of the baseline and a random number taken from a uniform distribution between -0.5 and 0.5.","title":"Input neuron"},{"location":"example/NeuralField/#neural-field-neuron","text":"NeuralFieldNeuron = Neuron ( parameters = \"\"\" tau = 10.0 : population \"\"\" , equations = \"\"\" tau * dr/dt + r = sum(exc) + sum(inh) + Uniform(-0.5, 0.5) : min=0.0, max=1.0 \"\"\" ) The second neuron we need is a bit more complex, as it is governed by an ODE and considers inputs from other neurons. It also has a non-linear activation function, which is linear when the firing rate is between 0.0 and 1.0, and constant otherwise. tau is a population-wise parameter, whose value will be the same for all neuron of the population. r is the firing rate of he neuron, whose dynamics are governed by a first-order linear ODE, integrating the sums of excitatory and inhibitory inputs with noise. As explained in the manual for rate-coded neurons, sum(exc) retrieves the weighted sum of pre-synaptic firing rates for the synapses having the connection type exc , here the one_to_one connections between inp and focus . sum(inh) does the same for inh type connections, here the lateral connections within focus . The firing rate is restricted to the range [0, 1] by setting the min and max accordingly in the flags section (everything after the : ). This means that after evaluating the ODE and getting a new value for r , its value will be clamped if it outside these values. One can define both min and max , only one, or none.","title":"Neural Field neuron"},{"location":"example/NeuralField/#creating-the-populations","text":"The two populations have a geometry of (20, 20), therefore 400 neurons each. They are created simply by instantiating the Population class: N = 20 inp = Population ( geometry = ( N , N ), neuron = InputNeuron , name = 'Input' ) focus = Population ( geometry = ( N , N ), neuron = NeuralFieldNeuron , name = 'Focus' ) The populations can be assigned a unique name (here 'Input' and 'Focus') in order to be be able to retrieve them if the references inp and focus are lost. They are given a 2D geometry and associated to the corresponding Neuron instance.","title":"Creating the populations"},{"location":"example/NeuralField/#creating-the-projections","text":"The first projection is a one-to-one projection from Input to Focus with the type 'exc'. This connection pattern pattern is possible because the two populations have the same geometry. The weights are initialized to 1.0, and this value will not change with time (no learning), so it is not necessary to define a synapse type: ff = Projection ( pre = inp , post = focus , target = 'exc' ) ff . connect_one_to_one ( weights = 1.0 , delays = 20.0 ) <ANNarchy.core.Projection.Projection at 0x7f106254c3a0> The references to the pre- and post-synaptic population (or their names), as well as the target type, are passed to the constructor of Projection . The connector method connect_one_to_one() is immediately applied to the Projection, defining how many synapses will be created. The weights are initialized uniformly to 1.0. The second projection is a difference of gaussians (DoG) for the lateral connections within 'focus'. The connector method is already provided by ANNarchy, so there is nothing more to do than to call it with the right parameters: lat = Projection ( pre = focus , post = focus , target = 'inh' ) lat . connect_dog ( amp_pos = 0.2 , sigma_pos = 0.1 , amp_neg = 0.1 , sigma_neg = 0.7 ) <ANNarchy.core.Projection.Projection at 0x7f1062543fa0>","title":"Creating the projections"},{"location":"example/NeuralField/#compiling-the-network-and-simulating","text":"Once the populations and projections are created, the network is ready to be generated, compiled and simulated. Compilation is simply done by calling compile() : compile () Compiling ... OK This generates optimized C++ code from the neurons' definition and network structure, compiles it with gcc/clang and instantiates all objects, particularly the synapses. If some errors were made in the neuron definition, they will be signaled at this point. Hint: The call to compile() is mandatory in any script. After it is called, populations and projections can not be added anymore. Once the compilation is successful, the network can be simulated by calling simulate() : simulate ( 1000.0 ) # simulate for 1 second As no input has been fed into the network, calling simulate() now won't lead to anything interesting. The next step is to clamp inputs into the input population's baseline.","title":"Compiling the network and simulating"},{"location":"example/NeuralField/#setting-inputs","text":"","title":"Setting inputs"},{"location":"example/NeuralField/#pure-python-approach","text":"In this example, we consider as input a moving bubble of activity rotating along a circle in the input space in 5 seconds. A naive way of setting such inputs would be to access population attributes (namely inp.baseline ) in a tight loop in Python: angle = 0.0 x , y = np . meshgrid ( np . linspace ( 0 , 19 , 20 ), np . linspace ( 0 , 19 , 20 )) # Main loop while True : # Update the angle angle += 1.0 / 5000.0 # Compute the center of the bubble cx = 10.0 * ( 1.0 + 0.5 * np . cos ( 2.0 * np . pi * angle ) ) cy = 10.0 * ( 1.0 + 0.5 * np . sin ( 2.0 * np . pi * angle ) ) # Clamp the bubble into pop.baseline inp . baseline = ( np . exp ( - (( x - cx ) ** 2 + ( y - cy ) ** 2 ) / 8.0 )) # Simulate for 1 ms step () angle represents the angle made by the bubble with respect to the center of the input population. x and y are Numpy arrays representing the X- and Y- coordinates of neurons in the input population. At each iteration of the simulation (i.e. every millisecond of simulation, the bubble is slightly rotated ( angle is incremented) so as to make a complete revolution in 5 seconds (5000 steps). cx and cy represent the coordinates of the center of the bubble in neural coordinates according to the new value of the angle. A Gaussian profile (in the form of a Numpy array) is then clamped into the baseline of inp using the distance between each neuron of the population ( x and y ) and the center of the bubble. Last, a single simulation step is performed using step() , before the whole process starts again until the user quits. step() is equivalent to simulate(1) , although a little bit faster as it does not check anything. Although this approach works, you would observe that it is very slow: the computation of the bubble and its feeding into InputPop takes much more time than the call to step() . The interest of using a parallel simulator disappears. This is due to the fact that Python is knowingly bad at performing tight loops because of its interpreted nature. If the while loop were compiled from C code, the computation would be much more efficient. This is what Cython brings you.","title":"Pure Python approach"},{"location":"example/NeuralField/#cython-approach","text":"Generalities on Cython The Cython approach requires to write Cython-specific code in a .pyx file, generate the corresponding C code with Python access methods, compile it and later import it into your Python code. Happily, the Cython syntax is very close to Python. In the most basic approach, it is simply Python code with a couple of type declarations. Instead of: bar = 1 foo = np . ones (( 10 , 10 )) you would write in Cython: cdef int bar = 1 cdef np . ndarray foo = np . ones (( 10 , 10 )) By specifing the type of a variable (which can not be changed later contrary to Python), you help Cython generate optimized C code, what can lead in some cases to speedups up to 100x. The rest of the syntax (indentation, for loops, if...) is the same as in Python. You can als import any Python module in your Cython code. Some modules (importantly Numpy) even provide a Cython interface where the equivalent Cython code can be directly imported (so it becomes very fast to use). The whole compilation procedure is very easy. One particularly simple approach is to use the pyximport module shipped with Cython. Let us suppose you wrote a dummy() method in a Cython file named TestModule.pyx . All you need to use this method in your python code is to write: import pyximport ; pyximport . install () from TestModule import dummy dummy () pyximport takes care of the compilation process (but emits quite a lot of warnings that can be ignored), and allows to import TestModule as if it were a regular Python module. Please refer to the Cython documentation to know more. Moving bubbles in Cython The file BubbleWorld.pyx defines a World class able to rotate the bubble for a specified duration. import numpy as np cimport numpy as np At the beginning of the file, numpy is imported once as a normal Python module with import , and once as a Cython module with cimport . This allows our Cython module to access directly the internal representations of Numpy without going through the Python interpreter. We can then define a World class taking as parameters: the population which will be used as input (here Input ), several arguments such as radius , sigma and period which allow to parameterize the behavior of the rotating bubble, func which is the Python method that will be called at each time step, i.e.e the step() method of ANNarchy. cdef class World : \" Environment class allowing to clamp a rotating bubble into the baseline of a population.\" cdef pop # Input population cdef func # Function to call cdef float angle # Current angle cdef float radius # Radius of the circle cdef float sigma # Width of the bubble cdef float period # Number of steps needed to make one revolution cdef np . ndarray xx , yy # indices cdef float cx , cy , midw , midh cdef np . ndarray data def __cinit__ ( self , population , radius , sigma , period , func ): \" Constructor\" self . pop = population self . func = func self . angle = 0.0 self . radius = radius self . sigma = sigma self . period = period cdef np . ndarray x = np . linspace ( 0 , self . pop . geometry [ 0 ] - 1 , self . pop . geometry [ 0 ]) cdef np . ndarray y = np . linspace ( 0 , self . pop . geometry [ 1 ] - 1 , self . pop . geometry [ 1 ]) self . xx , self . yy = np . meshgrid ( x , y ) self . midw = self . pop . geometry [ 0 ] / 2 self . midh = self . pop . geometry [ 1 ] / 2 def rotate ( self , int duration ): \" Rotates the bubble for the given duration\" cdef int t for t in xrange ( duration ): # Update the angle self . angle += 1.0 / self . period # Compute the center of the bubble self . cx = self . midw * ( 1.0 + self . radius * np . cos ( 2.0 * np . pi * self . angle ) ) self . cy = self . midh * ( 1.0 + self . radius * np . sin ( 2.0 * np . pi * self . angle ) ) # Create the bubble self . data = ( np . exp ( - (( self . xx - self . cx ) ** 2 + ( self . yy - self . cy ) ** 2 ) / 2.0 / self . sigma ** 2 )) # Clamp the bubble into pop.baseline self . pop . baseline = self . data # Simulate for 1 step self . func () Although this tutorial won't go into much detail, you can note the following: The data given to or initialized in the constructor are previously declared (with their type) as attributes of the class. This way, Cython knows at the compilation time which operations are possible on them, which amount of memory to allocate and so on, resulting in a more efficient implementation. The input population ( self.pop ) can be accessed as a normal Python object. In particular, self.pop.geometry is used in the constructor to initialize the meshgrid. The method rotate() performs the simulation for the given duration (in steps, not milliseconds). Its content is relatively similar to the Python version. Running the simulation Once the environment has been defined, the simulation can be executed. The following code, to be placed after the network definition, performs a simulation of the network, taking inputs from BubbleWorld.pyx , during 2 seconds: # Create the environment import pyximport ; pyximport . install ( setup_args = { 'include_dirs' : np . get_include ()}) from BubbleWorld import World world = World ( population = inp , radius = 0.5 , sigma = 2.0 , period = 5000.0 , func = step ) # Simulate for 2 seconds with inputs world . rotate ( 2000 )","title":"Cython approach"},{"location":"example/NeuralField/#visualizing-the-network","text":"The preceding code performs correctly the intended simulation, but nothing is visualized. The user has all freedom to visualize his network the way he prefers (for example through animated Matplotlib figures): import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 121 ) plt . imshow ( inp . r , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . subplot ( 122 ) plt . imshow ( focus . r , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . show () However, Matplotlib animations are rather slow, and visualizing the network at each time step would take more time than running the simulation. The provided example takes advantage of the PyQtGraph library (www.pyqtgraph.org) to visualize efficiently activity in the network using OpenGL. The following class and method is defined in Viz.py : # Visualizer using PyQtGraph try : from pyqtgraph.Qt import QtGui , QtCore import pyqtgraph as pg except : print ( 'PyQtGraph is not installed on your system, can not visualize the network.' ) exit ( 0 ) try : import pyqtgraph.opengl as gl except : print ( 'OpenGL is not installed on your system, can not visualize the network.' ) exit ( 0 ) import numpy as np class GLViewer ( object ): \" Class to visualize the network activity using PyQtGraph and openGL.\" def __init__ ( self , populations , func , update_rate ): # Parameters self . populations = populations self . func = func self . update_rate = update_rate # Window self . win = gl . GLViewWidget () self . win . show () self . win . setCameraPosition ( distance = 40 ) # Prepare the plots self . plots = [] shift = 0 for pop in self . populations : p = gl . GLSurfacePlotItem ( x = np . linspace ( 0 , pop . geometry [ 0 ] - 1 , pop . geometry [ 0 ]), y = np . linspace ( 0 , pop . geometry [ 1 ] - 1 , pop . geometry [ 1 ]), shader = 'heightColor' , computeNormals = False , smooth = False ) p . translate ( shift , - 10 , - 1 ) self . win . addItem ( p ) self . plots . append ( p ) shift -= 25 def scale ( self , data ): \" Colors are shown in the range [-1, 1] per default.\" return 1.8 * data - 0.9 def update ( self ): \"Callback\" # Simulate for 200ms self . func ( self . update_rate ) # Refresh the GUI for i in range ( len ( self . populations )): self . plots [ i ] . setData ( z = self . scale ( self . populations [ i ] . r )) # Listen to mouse/keyboard events QtGui . QApplication . processEvents () def run ( self ): \"Inifinite loop\" timer = QtCore . QTimer () timer . timeout . connect ( self . update ) timer . start ( 0 ) QtGui . QApplication . instance () . exec_ () def loop_bubbles ( populations , func , update_rate ): \"Launches the GL GUI and rotates the bubble infinitely.\" # Create the GUI using PyQtGraph app = QtGui . QApplication ([]) viewer = GLViewer ( populations , func , update_rate ) # Start the simulation forever viewer . run () We leave out again the details about this class (please look at the examples and tutorials on the PyQtGraph website to understand it). It allows to open a PyQtGraph window and display the firing rate of both Input and Focus populations using OpenGL. The run() method is an endless loop calling regularly the update() method. The update() method calls first World.rotate(200) and waits for its completion before reactualizing the display. The reason is that refreshing the display can only be done sequentially with the simulation, and calling it too often would impair the simulation time. Once this class has been defined, the simulation can be run endlessly by importing the Viz module: # Launch the GUI and run the simulation from Viz import loop_bubbles loop_bubbles ( populations = [ inp , focus ], func = world . rotate , update_rate = 200 )","title":"Visualizing the network"},{"location":"example/Ramp/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Homeostatic STDP # This example in examples/homeostatic_stdp is a reimplementation of the mechanism described in: Carlson, K.D.; Richert, M.; Dutt, N.; Krichmar, J.L., \"Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks,\" in Neural Networks (IJCNN), The 2013 International Joint Conference on , vol., no., pp.1-8, 4-9 Aug. 2013. doi: 10.1109/IJCNN.2013.6706961 It is based on the corresponding Carlsim tutorial: http://www.socsci.uci.edu/~jkrichma/CARLsim/doc/tut3_plasticity.html This noteboob focuses on the simple \"Ramp\" experiment ( Ramp.py ), but the principle is similar for the self-organizing receptive fileds (SORF) one ( SORF.py ). from ANNarchy import * clear () ANNarchy 4.7 (4.7.0) on linux (posix). The network uses regular-spiking Izhikevich neurons (see the Izhikevich notebook), but using exponentially-decaying conductances and NMDA synapses: RSNeuron = Neuron ( parameters = \"\"\" a = 0.02 : population b = 0.2 : population c = -65. : population d = 8. : population tau_ampa = 5. : population tau_nmda = 150. : population vrev = 0.0 : population \"\"\" , equations = \"\"\" # Inputs I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v) # Membrane potential and recovery variable are solved using the midpoint method for stability dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint du/dt = a * (b*v - u) : init=-13., midpoint # AMPA and NMDA conductances tau_ampa * dg_ampa/dt = -g_ampa : exponential tau_nmda * dg_nmda/dt = -g_nmda : exponential \"\"\" , spike = \"\"\" v >= 30. \"\"\" , reset = \"\"\" v = c u += d \"\"\" , functions = \"\"\" nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2) \"\"\" ) The main particularity about NMDA synaptic models is that a single synaptic connection influences two conductances: 1) The AMPA conductance, which primarily drives the post-synaptic neuron: \\[ I_\\text{AMPA} = g_\\text{AMPA} \\times (V_\\text{rev} - V) \\] 2) The NMDA conductance, which is non-linearly dependent on the membrane potential: \\[ I_\\text{NMDA} = g_\\text{NMDA} \\times \\frac{(\\frac{V - V_\\text{NMDA}}{\\sigma})^2}{1 + (\\frac{V - V_\\text{NMDA}}{\\sigma})^2} \\times (V_\\text{rev} - V) \\] In short, the NMDA conductance only increases if the post-synaptic neuron is already depolarized. The nmda function is defined in the functions argument for readability. The parameters \\(V_\\text{NMDA} =-80 \\text{mV}\\) and \\(\\sigma = 60 \\text{mV}\\) are here hardcoded in the equation, but they could be defined as global parameters. The AMPA and NMDA conductances are exponentially decreasing with different time constants: \\[ \\tau_\\text{AMPA} \\frac{dg_\\text{AMPA}(t)}{dt} + g_\\text{AMPA}(t) = 0 $$ $$ \\tau_\\text{NMDA} \\frac{dg_\\text{NMDA}(t)}{dt} + g_\\text{NMDA}(t) = 0 \\] Another thing to notice in this neuron model is that the differential equations for the membrane potential and recovery variable are solved concurrently using the midpoint numerical method for stability: the semi-implicit method initially proposed by Izhikevich would fail. The input of the network is a population of 100 Poisson neurons, whose firing rate vary linearly from 0.2 to 20 Hz: # Input population inp = PoissonPopulation ( 100 , rates = np . linspace ( 0.2 , 20. , 100 )) We will consider two RS neurons, one learning inputs from the Poisson population using the regular STDP, the other learning using the proposed homeostatic STDP: # RS neuron without homeostatic mechanism pop1 = Population ( 1 , RSNeuron ) # RS neuron with homeostatic mechanism pop2 = Population ( 1 , RSNeuron ) The regular STDP used in the article is a nearest-neighbour variant, which integrates LTP and LTD traces triggered after each pre- or post-synaptic spikes, respectively. Contrary to the STDP synapse provided by ANNarchy, weight changes occur at each each time step: In a post-pre interval, weight changes follow the LTP trace, In a pre-post interval, weight changes follow the LTD trace. The weights are clipped between 0 and \\(w_\\text{max}\\) . nearest_neighbour_stdp = Synapse ( parameters = \"\"\" tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_max = 0.03 : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Nearest-neighbour w += if t_post >= t_pre: ltp else: - ltd : min=0.0, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" ) The homeostatic STDP rule proposed by Carlson et al. is more complex. It has a regular STDP part (the nearest-neighbour variant above) and a homeostatic regularization part, ensuring that the post-synaptic firing rate \\(R\\) does not exceed a target firing rate \\(R_\\text{target}\\) = 35 Hz. The firing rate of a spiking neuron can be automatically computed by ANNarchy (see later). It is then accessible as the variable r of the neuron (as if it were a regular rate-coded neuron). The homeostatic STDP rule is defined by: \\[ \\Delta w = K \\, (\\alpha \\, (1 - \\frac{R}{R_\\text{target}}) \\, w + \\beta \\, \\text{stdp} ) \\] where stdp is the regular STDP weight change, and \\(K\\) is a firing rate-dependent learning rate: \\[ K = \\frac{R}{ T \\, (1 + |1 - \\gamma \\, \\frac{R}{R_\\text{target}}|}) \\] with \\(T\\) being the window over which the mean firing rate is computed (5 seconds) and \\(\\alpha\\) , \\(\\beta\\) , \\(\\gamma\\) are parameters. homeo_stdp = Synapse ( parameters = \"\"\" # STDP tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_min = 0.0 : projection w_max = 0.03 : projection # Homeostatic regulation alpha = 0.1 : projection beta = 1.0 : projection gamma = 50. : projection Rtarget = 35. : projection T = 5000. : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Homeostatic values R = post.r : postsynaptic K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic # Nearest-neighbour stdp = if t_post >= t_pre: ltp else: - ltd w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" ) This rule necessitates that the post-synaptic neurons compute their average firing rate over a 5 seconds window. This has to be explicitely enabled, as it would be computationally too expensive to allow it by default: pop1 . compute_firing_rate ( 5000. ) pop2 . compute_firing_rate ( 5000. ) We can now fully connect the input population to the two neurons with random weights: # Projection without homeostatic mechanism proj1 = Projection ( inp , pop1 , [ 'ampa' , 'nmda' ], synapse = nearest_neighbour_stdp ) proj1 . connect_all_to_all ( Uniform ( 0.01 , 0.03 )) # Projection with homeostatic mechanism proj2 = Projection ( inp , pop2 , [ 'ampa' , 'nmda' ], synapse = homeo_stdp ) proj2 . connect_all_to_all ( weights = Uniform ( 0.01 , 0.03 )) <ANNarchy.core.Projection.Projection at 0x7f31101fa6d0> Note that the same weights will target both AMPA and NMDA conductances in the post-synaptic neurons. By default, the argument target of Projection should be a string, but you can also pass a list of strings to reach several conductances with the same weights. We can now compileand simulate for 1000 seconds while recording the relevat information: compile () # Record m1 = Monitor ( pop1 , 'r' ) m2 = Monitor ( pop2 , 'r' ) m3 = Monitor ( proj1 [ 0 ], 'w' , period = 1000. ) m4 = Monitor ( proj2 [ 0 ], 'w' , period = 1000. ) # Simulate T = 1000 # 1000s simulate ( T * 1000. , True ) # Get the data data1 = m1 . get ( 'r' ) data2 = m2 . get ( 'r' ) data3 = m3 . get ( 'w' ) data4 = m4 . get ( 'w' ) print ( 'Mean Firing Rate without homeostasis:' , np . mean ( data1 [:, 0 ])) print ( 'Mean Firing Rate with homeostasis:' , np . mean ( data2 [:, 0 ])) Compiling ... OK Simulating 1000.0 seconds of the network took 4.251189470291138 seconds. Mean Firing Rate without homeostasis: 55.672524999999986 Mean Firing Rate with homeostasis: 35.2491844 import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 311 ) plt . plot ( np . linspace ( 0 , T , len ( data1 [:, 0 ])), data1 [:, 0 ], 'r-' , label = \"Without homeostasis\" ) plt . plot ( np . linspace ( 0 , T , len ( data2 [:, 0 ])), data2 [:, 0 ], 'b-' , label = \"With homeostasis\" ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'Firing rate (Hz)' ) plt . subplot ( 312 ) plt . plot ( data3 [ - 1 , :], 'r-' ) plt . plot ( data4 [ - 1 , :], 'bx' ) axes = plt . gca () axes . set_ylim ([ 0. , 0.035 ]) plt . xlabel ( '# neuron' ) plt . ylabel ( 'Weights after 1000s' ) plt . subplot ( 313 ) plt . imshow ( data4 . T , aspect = 'auto' , cmap = 'hot' ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( '# neuron' ) plt . show () We see that without homeostasis, the post-synaptic neuron reaches quickly a firing of 55 Hz, with all weights saturating at their maximum value 0.03. This is true even for inputs as low as 0.2Hz. Meanwhile, with homeostasis, the post-synaptic neuron gets a firing rate of 35 Hz (its desired value), and the weights from the input population are proportional to the underlying activity.","title":"Ramp"},{"location":"example/Ramp/#homeostatic-stdp","text":"This example in examples/homeostatic_stdp is a reimplementation of the mechanism described in: Carlson, K.D.; Richert, M.; Dutt, N.; Krichmar, J.L., \"Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks,\" in Neural Networks (IJCNN), The 2013 International Joint Conference on , vol., no., pp.1-8, 4-9 Aug. 2013. doi: 10.1109/IJCNN.2013.6706961 It is based on the corresponding Carlsim tutorial: http://www.socsci.uci.edu/~jkrichma/CARLsim/doc/tut3_plasticity.html This noteboob focuses on the simple \"Ramp\" experiment ( Ramp.py ), but the principle is similar for the self-organizing receptive fileds (SORF) one ( SORF.py ). from ANNarchy import * clear () ANNarchy 4.7 (4.7.0) on linux (posix). The network uses regular-spiking Izhikevich neurons (see the Izhikevich notebook), but using exponentially-decaying conductances and NMDA synapses: RSNeuron = Neuron ( parameters = \"\"\" a = 0.02 : population b = 0.2 : population c = -65. : population d = 8. : population tau_ampa = 5. : population tau_nmda = 150. : population vrev = 0.0 : population \"\"\" , equations = \"\"\" # Inputs I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v) # Membrane potential and recovery variable are solved using the midpoint method for stability dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint du/dt = a * (b*v - u) : init=-13., midpoint # AMPA and NMDA conductances tau_ampa * dg_ampa/dt = -g_ampa : exponential tau_nmda * dg_nmda/dt = -g_nmda : exponential \"\"\" , spike = \"\"\" v >= 30. \"\"\" , reset = \"\"\" v = c u += d \"\"\" , functions = \"\"\" nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2) \"\"\" ) The main particularity about NMDA synaptic models is that a single synaptic connection influences two conductances: 1) The AMPA conductance, which primarily drives the post-synaptic neuron: \\[ I_\\text{AMPA} = g_\\text{AMPA} \\times (V_\\text{rev} - V) \\] 2) The NMDA conductance, which is non-linearly dependent on the membrane potential: \\[ I_\\text{NMDA} = g_\\text{NMDA} \\times \\frac{(\\frac{V - V_\\text{NMDA}}{\\sigma})^2}{1 + (\\frac{V - V_\\text{NMDA}}{\\sigma})^2} \\times (V_\\text{rev} - V) \\] In short, the NMDA conductance only increases if the post-synaptic neuron is already depolarized. The nmda function is defined in the functions argument for readability. The parameters \\(V_\\text{NMDA} =-80 \\text{mV}\\) and \\(\\sigma = 60 \\text{mV}\\) are here hardcoded in the equation, but they could be defined as global parameters. The AMPA and NMDA conductances are exponentially decreasing with different time constants: \\[ \\tau_\\text{AMPA} \\frac{dg_\\text{AMPA}(t)}{dt} + g_\\text{AMPA}(t) = 0 $$ $$ \\tau_\\text{NMDA} \\frac{dg_\\text{NMDA}(t)}{dt} + g_\\text{NMDA}(t) = 0 \\] Another thing to notice in this neuron model is that the differential equations for the membrane potential and recovery variable are solved concurrently using the midpoint numerical method for stability: the semi-implicit method initially proposed by Izhikevich would fail. The input of the network is a population of 100 Poisson neurons, whose firing rate vary linearly from 0.2 to 20 Hz: # Input population inp = PoissonPopulation ( 100 , rates = np . linspace ( 0.2 , 20. , 100 )) We will consider two RS neurons, one learning inputs from the Poisson population using the regular STDP, the other learning using the proposed homeostatic STDP: # RS neuron without homeostatic mechanism pop1 = Population ( 1 , RSNeuron ) # RS neuron with homeostatic mechanism pop2 = Population ( 1 , RSNeuron ) The regular STDP used in the article is a nearest-neighbour variant, which integrates LTP and LTD traces triggered after each pre- or post-synaptic spikes, respectively. Contrary to the STDP synapse provided by ANNarchy, weight changes occur at each each time step: In a post-pre interval, weight changes follow the LTP trace, In a pre-post interval, weight changes follow the LTD trace. The weights are clipped between 0 and \\(w_\\text{max}\\) . nearest_neighbour_stdp = Synapse ( parameters = \"\"\" tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_max = 0.03 : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Nearest-neighbour w += if t_post >= t_pre: ltp else: - ltd : min=0.0, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" ) The homeostatic STDP rule proposed by Carlson et al. is more complex. It has a regular STDP part (the nearest-neighbour variant above) and a homeostatic regularization part, ensuring that the post-synaptic firing rate \\(R\\) does not exceed a target firing rate \\(R_\\text{target}\\) = 35 Hz. The firing rate of a spiking neuron can be automatically computed by ANNarchy (see later). It is then accessible as the variable r of the neuron (as if it were a regular rate-coded neuron). The homeostatic STDP rule is defined by: \\[ \\Delta w = K \\, (\\alpha \\, (1 - \\frac{R}{R_\\text{target}}) \\, w + \\beta \\, \\text{stdp} ) \\] where stdp is the regular STDP weight change, and \\(K\\) is a firing rate-dependent learning rate: \\[ K = \\frac{R}{ T \\, (1 + |1 - \\gamma \\, \\frac{R}{R_\\text{target}}|}) \\] with \\(T\\) being the window over which the mean firing rate is computed (5 seconds) and \\(\\alpha\\) , \\(\\beta\\) , \\(\\gamma\\) are parameters. homeo_stdp = Synapse ( parameters = \"\"\" # STDP tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_min = 0.0 : projection w_max = 0.03 : projection # Homeostatic regulation alpha = 0.1 : projection beta = 1.0 : projection gamma = 50. : projection Rtarget = 35. : projection T = 5000. : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Homeostatic values R = post.r : postsynaptic K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic # Nearest-neighbour stdp = if t_post >= t_pre: ltp else: - ltd w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" ) This rule necessitates that the post-synaptic neurons compute their average firing rate over a 5 seconds window. This has to be explicitely enabled, as it would be computationally too expensive to allow it by default: pop1 . compute_firing_rate ( 5000. ) pop2 . compute_firing_rate ( 5000. ) We can now fully connect the input population to the two neurons with random weights: # Projection without homeostatic mechanism proj1 = Projection ( inp , pop1 , [ 'ampa' , 'nmda' ], synapse = nearest_neighbour_stdp ) proj1 . connect_all_to_all ( Uniform ( 0.01 , 0.03 )) # Projection with homeostatic mechanism proj2 = Projection ( inp , pop2 , [ 'ampa' , 'nmda' ], synapse = homeo_stdp ) proj2 . connect_all_to_all ( weights = Uniform ( 0.01 , 0.03 )) <ANNarchy.core.Projection.Projection at 0x7f31101fa6d0> Note that the same weights will target both AMPA and NMDA conductances in the post-synaptic neurons. By default, the argument target of Projection should be a string, but you can also pass a list of strings to reach several conductances with the same weights. We can now compileand simulate for 1000 seconds while recording the relevat information: compile () # Record m1 = Monitor ( pop1 , 'r' ) m2 = Monitor ( pop2 , 'r' ) m3 = Monitor ( proj1 [ 0 ], 'w' , period = 1000. ) m4 = Monitor ( proj2 [ 0 ], 'w' , period = 1000. ) # Simulate T = 1000 # 1000s simulate ( T * 1000. , True ) # Get the data data1 = m1 . get ( 'r' ) data2 = m2 . get ( 'r' ) data3 = m3 . get ( 'w' ) data4 = m4 . get ( 'w' ) print ( 'Mean Firing Rate without homeostasis:' , np . mean ( data1 [:, 0 ])) print ( 'Mean Firing Rate with homeostasis:' , np . mean ( data2 [:, 0 ])) Compiling ... OK Simulating 1000.0 seconds of the network took 4.251189470291138 seconds. Mean Firing Rate without homeostasis: 55.672524999999986 Mean Firing Rate with homeostasis: 35.2491844 import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 311 ) plt . plot ( np . linspace ( 0 , T , len ( data1 [:, 0 ])), data1 [:, 0 ], 'r-' , label = \"Without homeostasis\" ) plt . plot ( np . linspace ( 0 , T , len ( data2 [:, 0 ])), data2 [:, 0 ], 'b-' , label = \"With homeostasis\" ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'Firing rate (Hz)' ) plt . subplot ( 312 ) plt . plot ( data3 [ - 1 , :], 'r-' ) plt . plot ( data4 [ - 1 , :], 'bx' ) axes = plt . gca () axes . set_ylim ([ 0. , 0.035 ]) plt . xlabel ( '# neuron' ) plt . ylabel ( 'Weights after 1000s' ) plt . subplot ( 313 ) plt . imshow ( data4 . T , aspect = 'auto' , cmap = 'hot' ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( '# neuron' ) plt . show () We see that without homeostasis, the post-synaptic neuron reaches quickly a firing of 55 Hz, with all weights saturating at their maximum value 0.03. This is true even for inputs as low as 0.2Hz. Meanwhile, with homeostasis, the post-synaptic neuron gets a firing rate of 35 Hz (its desired value), and the weights from the input population are proportional to the underlying activity.","title":"Homeostatic STDP"},{"location":"example/STP/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Short-Term Plasticity and Synchrony # Implementation of the recurrent network proposed in: Tsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses, The Journal of Neuroscience, 20(50). This example in examples/tsodyks_markram shows how to define short-term plasticity (STP). from ANNarchy import * clear () dt = 0.25 setup ( dt = dt ) ANNarchy 4.7 (4.7.0) on linux (posix). This network uses simple leaky integrate-and-fire (LIF) neurons: LIF = Neuron ( parameters = \"\"\" tau = 30.0 : population I = 15.0 tau_I = 3.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = -v + g_exc - g_inh + I : init=13.5 tau_I * dg_exc/dt = -g_exc tau_I * dg_inh/dt = -g_inh \"\"\" , spike = \"v > 15.0\" , reset = \"v = 13.5\" , refractory = 3.0 ) P = Population ( geometry = 500 , neuron = LIF ) P . I = np . sort ( Uniform ( 14.625 , 15.375 ) . get_values ( 500 )) P . v = Uniform ( 0.0 , 15.0 ) Exc = P [: 400 ] Inh = P [ 400 :] Short-term plasticity can be defined by dynamical changes of synaptic efficiency, based on pre- or post-synaptic activity. We define a STP synapse, whose post-pynaptic potential (psp, define by g_target ) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u : STP = Synapse ( parameters = \"\"\" w=0.0 tau_rec = 1.0 tau_facil = 1.0 U = 0.1 \"\"\" , equations = \"\"\" dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven du/dt = (U - u)/tau_facil : init = 0.1, event-driven \"\"\" , pre_spike = \"\"\" g_target += w * u * x x *= (1 - u) u += U * (1 - u) \"\"\" ) Creating the projection between the excitatory and inhibitory is straightforward when the right parameters are chosen: # Parameters for the synapses Aee = 1.8 Aei = 5.4 Aie = 7.2 Aii = 7.2 Uee = 0.5 Uei = 0.5 Uie = 0.04 Uii = 0.04 tau_rec_ee = 800.0 tau_rec_ei = 800.0 tau_rec_ie = 100.0 tau_rec_ii = 100.0 tau_facil_ie = 1000.0 tau_facil_ii = 1000.0 # Create projections proj_ee = Projection ( pre = Exc , post = Exc , target = 'exc' , synapse = STP ) proj_ee . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aee , ( Aee / 2.0 ), min = 0.2 * Aee , max = 2.0 * Aee )) proj_ee . U = Normal ( Uee , ( Uee / 2.0 ), min = 0.1 , max = 0.9 ) proj_ee . tau_rec = Normal ( tau_rec_ee , ( tau_rec_ee / 2.0 ), min = 5.0 ) proj_ee . tau_facil = dt # Cannot be 0! proj_ei = Projection ( pre = Inh , post = Exc , target = 'inh' , synapse = STP ) proj_ei . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aei , ( Aei / 2.0 ), min = 0.2 * Aei , max = 2.0 * Aei )) proj_ei . U = Normal ( Uei , ( Uei / 2.0 ), min = 0.1 , max = 0.9 ) proj_ei . tau_rec = Normal ( tau_rec_ei , ( tau_rec_ei / 2.0 ), min = 5.0 ) proj_ei . tau_facil = dt # Cannot be 0! proj_ie = Projection ( pre = Exc , post = Inh , target = 'exc' , synapse = STP ) proj_ie . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aie , ( Aie / 2.0 ), min = 0.2 * Aie , max = 2.0 * Aie )) proj_ie . U = Normal ( Uie , ( Uie / 2.0 ), min = 0.001 , max = 0.07 ) proj_ie . tau_rec = Normal ( tau_rec_ie , ( tau_rec_ie / 2.0 ), min = 5.0 ) proj_ie . tau_facil = Normal ( tau_facil_ie , ( tau_facil_ie / 2.0 ), min = 5.0 ) proj_ii = Projection ( pre = Inh , post = Inh , target = 'inh' , synapse = STP ) proj_ii . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aii , ( Aii / 2.0 ), min = 0.2 * Aii , max = 2.0 * Aii )) proj_ii . U = Normal ( Uii , ( Uii / 2.0 ), min = 0.001 , max = 0.07 ) proj_ii . tau_rec = Normal ( tau_rec_ii , ( tau_rec_ii / 2.0 ), min = 5.0 ) proj_ii . tau_facil = Normal ( tau_facil_ii , ( tau_facil_ii / 2.0 ), min = 5.0 ) We compile and simulate for 10 seconds: compile () # Record Me = Monitor ( Exc , 'spike' ) Mi = Monitor ( Inh , 'spike' ) # Simulate duration = 10000.0 simulate ( duration , measure_time = True ) Compiling ... OK Simulating 10.0 seconds of the network took 0.22113656997680664 seconds. We retrieve the recordings and plot them: # Retrieve recordings data_exc = Me . get () data_inh = Mi . get () te , ne = Me . raster_plot ( data_exc [ 'spike' ]) ti , ni = Mi . raster_plot ( data_inh [ 'spike' ]) # Histogram of the exc population h = Me . histogram ( data_exc [ 'spike' ], bins = 1.0 ) # Mean firing rate of each excitatory neuron rates = [] for neur in data_exc [ 'spike' ] . keys (): rates . append ( len ( data_exc [ 'spike' ][ neur ]) / duration * 1000.0 ) # Plot import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 3 , 1 , 1 ) plt . plot ( te , ne , 'b.' , markersize = 1.0 ) plt . plot ( ti , ni , 'b.' , markersize = 1.0 ) plt . xlim (( 0 , duration )); plt . ylim (( 0 , 500 )) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . subplot ( 3 , 1 , 2 ) plt . plot ( h / 400. ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( 'Net activity' ) plt . subplot ( 3 , 1 , 3 ) plt . plot ( sorted ( rates )) plt . ylabel ( 'Spikes / sec' ) plt . xlabel ( '# neuron' ) plt . show ()","title":"STP"},{"location":"example/STP/#short-term-plasticity-and-synchrony","text":"Implementation of the recurrent network proposed in: Tsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses, The Journal of Neuroscience, 20(50). This example in examples/tsodyks_markram shows how to define short-term plasticity (STP). from ANNarchy import * clear () dt = 0.25 setup ( dt = dt ) ANNarchy 4.7 (4.7.0) on linux (posix). This network uses simple leaky integrate-and-fire (LIF) neurons: LIF = Neuron ( parameters = \"\"\" tau = 30.0 : population I = 15.0 tau_I = 3.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = -v + g_exc - g_inh + I : init=13.5 tau_I * dg_exc/dt = -g_exc tau_I * dg_inh/dt = -g_inh \"\"\" , spike = \"v > 15.0\" , reset = \"v = 13.5\" , refractory = 3.0 ) P = Population ( geometry = 500 , neuron = LIF ) P . I = np . sort ( Uniform ( 14.625 , 15.375 ) . get_values ( 500 )) P . v = Uniform ( 0.0 , 15.0 ) Exc = P [: 400 ] Inh = P [ 400 :] Short-term plasticity can be defined by dynamical changes of synaptic efficiency, based on pre- or post-synaptic activity. We define a STP synapse, whose post-pynaptic potential (psp, define by g_target ) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u : STP = Synapse ( parameters = \"\"\" w=0.0 tau_rec = 1.0 tau_facil = 1.0 U = 0.1 \"\"\" , equations = \"\"\" dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven du/dt = (U - u)/tau_facil : init = 0.1, event-driven \"\"\" , pre_spike = \"\"\" g_target += w * u * x x *= (1 - u) u += U * (1 - u) \"\"\" ) Creating the projection between the excitatory and inhibitory is straightforward when the right parameters are chosen: # Parameters for the synapses Aee = 1.8 Aei = 5.4 Aie = 7.2 Aii = 7.2 Uee = 0.5 Uei = 0.5 Uie = 0.04 Uii = 0.04 tau_rec_ee = 800.0 tau_rec_ei = 800.0 tau_rec_ie = 100.0 tau_rec_ii = 100.0 tau_facil_ie = 1000.0 tau_facil_ii = 1000.0 # Create projections proj_ee = Projection ( pre = Exc , post = Exc , target = 'exc' , synapse = STP ) proj_ee . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aee , ( Aee / 2.0 ), min = 0.2 * Aee , max = 2.0 * Aee )) proj_ee . U = Normal ( Uee , ( Uee / 2.0 ), min = 0.1 , max = 0.9 ) proj_ee . tau_rec = Normal ( tau_rec_ee , ( tau_rec_ee / 2.0 ), min = 5.0 ) proj_ee . tau_facil = dt # Cannot be 0! proj_ei = Projection ( pre = Inh , post = Exc , target = 'inh' , synapse = STP ) proj_ei . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aei , ( Aei / 2.0 ), min = 0.2 * Aei , max = 2.0 * Aei )) proj_ei . U = Normal ( Uei , ( Uei / 2.0 ), min = 0.1 , max = 0.9 ) proj_ei . tau_rec = Normal ( tau_rec_ei , ( tau_rec_ei / 2.0 ), min = 5.0 ) proj_ei . tau_facil = dt # Cannot be 0! proj_ie = Projection ( pre = Exc , post = Inh , target = 'exc' , synapse = STP ) proj_ie . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aie , ( Aie / 2.0 ), min = 0.2 * Aie , max = 2.0 * Aie )) proj_ie . U = Normal ( Uie , ( Uie / 2.0 ), min = 0.001 , max = 0.07 ) proj_ie . tau_rec = Normal ( tau_rec_ie , ( tau_rec_ie / 2.0 ), min = 5.0 ) proj_ie . tau_facil = Normal ( tau_facil_ie , ( tau_facil_ie / 2.0 ), min = 5.0 ) proj_ii = Projection ( pre = Inh , post = Inh , target = 'inh' , synapse = STP ) proj_ii . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aii , ( Aii / 2.0 ), min = 0.2 * Aii , max = 2.0 * Aii )) proj_ii . U = Normal ( Uii , ( Uii / 2.0 ), min = 0.001 , max = 0.07 ) proj_ii . tau_rec = Normal ( tau_rec_ii , ( tau_rec_ii / 2.0 ), min = 5.0 ) proj_ii . tau_facil = Normal ( tau_facil_ii , ( tau_facil_ii / 2.0 ), min = 5.0 ) We compile and simulate for 10 seconds: compile () # Record Me = Monitor ( Exc , 'spike' ) Mi = Monitor ( Inh , 'spike' ) # Simulate duration = 10000.0 simulate ( duration , measure_time = True ) Compiling ... OK Simulating 10.0 seconds of the network took 0.22113656997680664 seconds. We retrieve the recordings and plot them: # Retrieve recordings data_exc = Me . get () data_inh = Mi . get () te , ne = Me . raster_plot ( data_exc [ 'spike' ]) ti , ni = Mi . raster_plot ( data_inh [ 'spike' ]) # Histogram of the exc population h = Me . histogram ( data_exc [ 'spike' ], bins = 1.0 ) # Mean firing rate of each excitatory neuron rates = [] for neur in data_exc [ 'spike' ] . keys (): rates . append ( len ( data_exc [ 'spike' ][ neur ]) / duration * 1000.0 ) # Plot import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 3 , 1 , 1 ) plt . plot ( te , ne , 'b.' , markersize = 1.0 ) plt . plot ( ti , ni , 'b.' , markersize = 1.0 ) plt . xlim (( 0 , duration )); plt . ylim (( 0 , 500 )) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . subplot ( 3 , 1 , 2 ) plt . plot ( h / 400. ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( 'Net activity' ) plt . subplot ( 3 , 1 , 3 ) plt . plot ( sorted ( rates )) plt . ylabel ( 'Spikes / sec' ) plt . xlabel ( '# neuron' ) plt . show ()","title":"Short-Term Plasticity and Synchrony"},{"location":"example/SimpleSTDP/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Simple STDP # A simple model showing the STDP learning on a single neuron. Model adapted from Song, Miller and Abbott (2000) and Song and Abbott (2001) Code adapted from the Brian example: http://brian.readthedocs.org/en/1.4.1/examples-plasticity_STDP1.html from ANNarchy import * # Parameters F = 15.0 # Poisson distribution at 15 Hz N = 1000 # 1000 Poisson inputs gmax = 0.01 # Maximum weight duration = 100000.0 # Simulation for 100 seconds # Definition of the neuron IF = Neuron ( parameters = \"\"\" tau_m = 10.0 tau_e = 5.0 vt = -54.0 vr = -60.0 El = -74.0 Ee = 0.0 \"\"\" , equations = \"\"\" tau_m * dv/dt = El - v + g_exc * (Ee - vr) : init = -60.0 tau_e * dg_exc/dt = - g_exc \"\"\" , spike = \"\"\" v > vt \"\"\" , reset = \"\"\" v = vr \"\"\" ) # Input population Input = PoissonPopulation ( name = 'Input' , geometry = N , rates = F ) # Output neuron Output = Population ( name = 'Output' , geometry = 1 , neuron = IF ) # Projection learned using STDP proj = Projection ( pre = Input , post = Output , target = 'exc' , synapse = STDP ( tau_plus = 20.0 , tau_minus = 20.0 , A_plus = 0.01 , A_minus = 0.0105 , w_max = 0.01 ) ) proj . connect_all_to_all ( weights = Uniform ( 0.0 , gmax )) # Compile the network compile () # Start recording Mi = Monitor ( Input , 'spike' ) Mo = Monitor ( Output , 'spike' ) # Start the simulation print ( 'Start the simulation' ) simulate ( duration , measure_time = True ) # Retrieve the recordings input_spikes = Mi . get ( 'spike' ) output_spikes = Mo . get ( 'spike' ) # Compute the mean firing rates during the simulation print ( 'Mean firing rate in the input population: ' + str ( Mi . mean_fr ( input_spikes )) ) print ( 'Mean firing rate of the output neuron: ' + str ( Mo . mean_fr ( output_spikes )) ) # Compute the instantaneous firing rate of the output neuron output_rate = Mo . smoothed_rate ( output_spikes , 100.0 ) # Receptive field after simulation weights = proj . w [ 0 ] import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 3 , 1 , 1 ) plt . plot ( output_rate [ 0 , :]) plt . subplot ( 3 , 1 , 2 ) plt . plot ( weights , '.' ) plt . subplot ( 3 , 1 , 3 ) plt . hist ( weights , bins = 20 ) plt . show () ANNarchy 4.7 (4.7.0) on linux (posix). Compiling ... OK Start the simulation Simulating 100.0 seconds of the network took 2.8736979961395264 seconds. Mean firing rate in the input population: 15.00488 Mean firing rate of the output neuron: 28.07","title":"STDP"},{"location":"example/SimpleSTDP/#simple-stdp","text":"A simple model showing the STDP learning on a single neuron. Model adapted from Song, Miller and Abbott (2000) and Song and Abbott (2001) Code adapted from the Brian example: http://brian.readthedocs.org/en/1.4.1/examples-plasticity_STDP1.html from ANNarchy import * # Parameters F = 15.0 # Poisson distribution at 15 Hz N = 1000 # 1000 Poisson inputs gmax = 0.01 # Maximum weight duration = 100000.0 # Simulation for 100 seconds # Definition of the neuron IF = Neuron ( parameters = \"\"\" tau_m = 10.0 tau_e = 5.0 vt = -54.0 vr = -60.0 El = -74.0 Ee = 0.0 \"\"\" , equations = \"\"\" tau_m * dv/dt = El - v + g_exc * (Ee - vr) : init = -60.0 tau_e * dg_exc/dt = - g_exc \"\"\" , spike = \"\"\" v > vt \"\"\" , reset = \"\"\" v = vr \"\"\" ) # Input population Input = PoissonPopulation ( name = 'Input' , geometry = N , rates = F ) # Output neuron Output = Population ( name = 'Output' , geometry = 1 , neuron = IF ) # Projection learned using STDP proj = Projection ( pre = Input , post = Output , target = 'exc' , synapse = STDP ( tau_plus = 20.0 , tau_minus = 20.0 , A_plus = 0.01 , A_minus = 0.0105 , w_max = 0.01 ) ) proj . connect_all_to_all ( weights = Uniform ( 0.0 , gmax )) # Compile the network compile () # Start recording Mi = Monitor ( Input , 'spike' ) Mo = Monitor ( Output , 'spike' ) # Start the simulation print ( 'Start the simulation' ) simulate ( duration , measure_time = True ) # Retrieve the recordings input_spikes = Mi . get ( 'spike' ) output_spikes = Mo . get ( 'spike' ) # Compute the mean firing rates during the simulation print ( 'Mean firing rate in the input population: ' + str ( Mi . mean_fr ( input_spikes )) ) print ( 'Mean firing rate of the output neuron: ' + str ( Mo . mean_fr ( output_spikes )) ) # Compute the instantaneous firing rate of the output neuron output_rate = Mo . smoothed_rate ( output_spikes , 100.0 ) # Receptive field after simulation weights = proj . w [ 0 ] import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 3 , 1 , 1 ) plt . plot ( output_rate [ 0 , :]) plt . subplot ( 3 , 1 , 2 ) plt . plot ( weights , '.' ) plt . subplot ( 3 , 1 , 3 ) plt . hist ( weights , bins = 20 ) plt . show () ANNarchy 4.7 (4.7.0) on linux (posix). Compiling ... OK Start the simulation Simulating 100.0 seconds of the network took 2.8736979961395264 seconds. Mean firing rate in the input population: 15.00488 Mean firing rate of the output neuron: 28.07","title":"Simple STDP"},{"location":"example/StructuralPlasticity/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Structural plasticity # As simple example showing how to use structural plasticity (creation/pruning of synapses) in a rate-coded network (spiking networks work similarly). First, the structural plasticity mechanisms must be allowed in setup() : from ANNarchy import * clear () # Compulsory to allow structural plasticity setup ( structural_plasticity = True ) ANNarchy 4.7 (4.7.0) on linux (posix). We define a leaky integrator rate-coded neuron and a small population: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 : population baseline = 0.0 \"\"\" , equations = \"\"\" tau * dr/dt + r = baseline + sum(exc) : min=0.0 \"\"\" ) pop = Population ( 100 , LeakyIntegratorNeuron ) Structural plasticity has two components: creation of synapses and pruning (removal) under certain conditions. These conditions are defined in the synapse type itself in the pruning and creating arguments: StructuralPlasticSynapse = Synapse ( parameters = \" T = 10000 : int, projection \" , equations = \"\"\" age = if pre.r * post.r > 1.0 : 0 else : age + 1 : init = 0, int\"\"\" , pruning = \"age > T : proba = 0.2\" , creating = \"pre.r * post.r > 1.0 : proba = 0.1, w = 0.01\" , ) proj = Projection ( pop , pop , 'exc' , StructuralPlasticSynapse ) proj . connect_fixed_probability ( weights = 0.01 , probability = 0.1 ) <ANNarchy.core.Projection.Projection at 0x7fe1fa111880> These conditions must be boolean values, which when True may trigger the creation/pruning of a synapse. The flag proba gives the probability by which the synapse will actually be created/pruned. When creating is True , a synapse that did not exist will be created with the provided probability. Its weight will take the value provided by the flag w (0.01), the other variables take their default value. When pruning is True , a synapse that exists will be deleted with the given probability. The pruning condition can depend on any pre-synaptic, post-synaptic or synaptic variable. The creating condition can only depend on pre- or post-synaptic conditions, as the synapse does not exist yet. Apart from these two fields, the synapse is a regular synapse, one could also define synaptic plasticity mechanisms and so on. We finally create a sparse projection within the population, with 10% connectivity. compile () Compiling ... OK The creation and pruning have to be explicitly started before a simulation, as they are very expensive computationally. The period argument states how often the conditions will be checked (avoid using dt ): proj . start_creating ( period = 100.0 ) proj . start_pruning ( period = 100.0 ) To see the effect of structural plasticity, one alternatively activates one half of the population by setting a high baseline (mimicking corrrelated inputs). As neurons in one half will be activated at the same time, they will create synapses between each other. Between the two halves, the neurons are never co-activated, so the existing synapses will slowly die out. # Save the initial connectivity matrix initial_weights = proj . connectivity_matrix () # Let structural plasticity over several trials num_trials = 100 for trial in range ( num_trials ): # Activate the first subpopulation pop [: 50 ] . baseline = 1.0 # Simulate for 1s simulate ( 1000. ) # Reset the population pop . baseline = 0.0 simulate ( 100. ) # Activate the second subpopulation pop [ 50 :] . baseline = 1.0 # Simulate for 1s simulate ( 1000. ) # Reset the population pop . baseline = 0.0 simulate ( 100. ) # Inspect the final connectivity matrix final_weights = proj . connectivity_matrix () We can check the effect of structural plasticity by looking at the connectivity matrix before and after the stimulation: import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 121 ) plt . imshow ( initial_weights ) plt . title ( 'Connectivity matrix before' ) plt . subplot ( 122 ) plt . imshow ( final_weights ) plt . title ( 'Connectivity matrix after' ) plt . show ()","title":"Structural plasticity"},{"location":"example/StructuralPlasticity/#structural-plasticity","text":"As simple example showing how to use structural plasticity (creation/pruning of synapses) in a rate-coded network (spiking networks work similarly). First, the structural plasticity mechanisms must be allowed in setup() : from ANNarchy import * clear () # Compulsory to allow structural plasticity setup ( structural_plasticity = True ) ANNarchy 4.7 (4.7.0) on linux (posix). We define a leaky integrator rate-coded neuron and a small population: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 : population baseline = 0.0 \"\"\" , equations = \"\"\" tau * dr/dt + r = baseline + sum(exc) : min=0.0 \"\"\" ) pop = Population ( 100 , LeakyIntegratorNeuron ) Structural plasticity has two components: creation of synapses and pruning (removal) under certain conditions. These conditions are defined in the synapse type itself in the pruning and creating arguments: StructuralPlasticSynapse = Synapse ( parameters = \" T = 10000 : int, projection \" , equations = \"\"\" age = if pre.r * post.r > 1.0 : 0 else : age + 1 : init = 0, int\"\"\" , pruning = \"age > T : proba = 0.2\" , creating = \"pre.r * post.r > 1.0 : proba = 0.1, w = 0.01\" , ) proj = Projection ( pop , pop , 'exc' , StructuralPlasticSynapse ) proj . connect_fixed_probability ( weights = 0.01 , probability = 0.1 ) <ANNarchy.core.Projection.Projection at 0x7fe1fa111880> These conditions must be boolean values, which when True may trigger the creation/pruning of a synapse. The flag proba gives the probability by which the synapse will actually be created/pruned. When creating is True , a synapse that did not exist will be created with the provided probability. Its weight will take the value provided by the flag w (0.01), the other variables take their default value. When pruning is True , a synapse that exists will be deleted with the given probability. The pruning condition can depend on any pre-synaptic, post-synaptic or synaptic variable. The creating condition can only depend on pre- or post-synaptic conditions, as the synapse does not exist yet. Apart from these two fields, the synapse is a regular synapse, one could also define synaptic plasticity mechanisms and so on. We finally create a sparse projection within the population, with 10% connectivity. compile () Compiling ... OK The creation and pruning have to be explicitly started before a simulation, as they are very expensive computationally. The period argument states how often the conditions will be checked (avoid using dt ): proj . start_creating ( period = 100.0 ) proj . start_pruning ( period = 100.0 ) To see the effect of structural plasticity, one alternatively activates one half of the population by setting a high baseline (mimicking corrrelated inputs). As neurons in one half will be activated at the same time, they will create synapses between each other. Between the two halves, the neurons are never co-activated, so the existing synapses will slowly die out. # Save the initial connectivity matrix initial_weights = proj . connectivity_matrix () # Let structural plasticity over several trials num_trials = 100 for trial in range ( num_trials ): # Activate the first subpopulation pop [: 50 ] . baseline = 1.0 # Simulate for 1s simulate ( 1000. ) # Reset the population pop . baseline = 0.0 simulate ( 100. ) # Activate the second subpopulation pop [ 50 :] . baseline = 1.0 # Simulate for 1s simulate ( 1000. ) # Reset the population pop . baseline = 0.0 simulate ( 100. ) # Inspect the final connectivity matrix final_weights = proj . connectivity_matrix () We can check the effect of structural plasticity by looking at the connectivity matrix before and after the stimulation: import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 121 ) plt . imshow ( initial_weights ) plt . title ( 'Connectivity matrix before' ) plt . subplot ( 122 ) plt . imshow ( final_weights ) plt . title ( 'Connectivity matrix after' ) plt . show ()","title":"Structural plasticity"},{"location":"example/Webcam/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Webcam # The script examples/image/Webcam.py applies a red filter on the input from the webcam, and isolates one mode using a dynamical neural field. Most of the concepts are similar to the Image Processing example. The VideoPopulation object also requires the Python bindings to OpenCV. from ANNarchy import * from ANNarchy.extensions.image import * from ANNarchy.extensions.convolution import Convolution , Pooling clear () ANNarchy 4.7 (4.7.0) on linux (posix). # Definition of the neurons LinearNeuron = Neuron ( equations = \"r=sum(exc): min=0.0\" ) DNF = Neuron ( parameters = \"tau=10.0\" , equations = \"tau*dr/dt + r = sum(exc) + sum(inh): min=0.0, max=1.0\" ) # Population getting the video stream width = 640 height = 480 video = VideoPopulation ( geometry = ( height , width , 3 )) # Subsampling population pooled = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Mean-pooling projection pool_proj = Pooling ( pre = video , post = pooled , target = 'exc' , operation = 'mean' ) pool_proj . connect_pooling () # Define a red filter with no spatial extent red_filter = [[ [ 2.0 , - 1.0 , - 1.0 ] ]] # Create a population of DNF neurons downscaling the image with a factor 10 dnf = Population ( geometry = ( 48 , 64 ), neuron = DNF ) # Create the convolution using the red filter ff = Convolution ( pre = pooled , post = dnf , target = 'exc' ) ff . connect_filter ( weights = red_filter ) # Create difference of Gaussians lateral connections for denoising/competition lat = Projection ( pre = dnf , post = dnf , target = 'inh' ) lat . connect_dog ( amp_pos = 0.2 , sigma_pos = 0.1 , amp_neg = 0.1 , sigma_neg = 0.7 ) <ANNarchy.core.Projection.Projection at 0x7f261f28bf10> The VideoPopulation acquires images from the webcam: here the webcam should be able to deliver 640x480 colored images. The corresponding population is then subsampled with a factor 10, and a red filter is applied on it. This feeds a DNF (see the Neural Field\" example) which selects the region with the highest density. compile () Compiling ... OK We can now start the camera 0 ( /dev/video0 , adapt it to your machine): video . start_camera ( 0 ) [ WARN:0] global /build/opencv/src/opencv-4.5.3/modules/videoio/src/cap_gstreamer.cpp (1081) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1 A simple GUI based on PyQtGraph allows to display the input and output of the network: try : from pyqtgraph.Qt import QtGui , QtCore import pyqtgraph as pg except : print ( 'PyQtGraph is not installed, can not visualize the network.' ) exit ( 0 ) # Wrapping class class Viewer ( object ): \" Class to visualize the network activity using PyQtGraph.\" def __init__ ( self , video , result ): self . video = video self . result = result app = pg . mkQApp () self . win = pg . GraphicsWindow ( title = \"Live webcam\" ) self . win . resize ( 640 , 480 ) box = self . win . addViewBox ( lockAspect = True ) box . invertY () self . vis = pg . ImageItem () box . addItem ( self . vis ) box = self . win . addViewBox ( lockAspect = True ) box . invertY () self . res = pg . ImageItem () box . addItem ( self . res ) self . win . show () self . lastUpdate = pg . ptime . time () self . avgFps = 0.0 def update ( self ): # Set the input self . video . grab_image () # Simulate for 10 ms with a new input simulate ( 5.0 ) # Refresh the GUI self . vis . setImage ( np . swapaxes ( self . video . r , 0 , 1 )) self . res . setImage ( np . swapaxes ( self . result . r , 0 , 1 )) # Listen to mouse/keyboard events QtGui . QApplication . processEvents () # FPS now = pg . ptime . time () fps = 1.0 / ( now - self . lastUpdate ) self . lastUpdate = now self . avgFps = self . avgFps * 0.8 + fps * 0.2 # print(self.avgFps) def run ( self ): timer = QtCore . QTimer () timer . timeout . connect ( self . update ) timer . start ( 0 ) QtGui . QApplication . instance () . exec_ () timer . stop () # Start the GUI view = Viewer ( video , dnf ) view . run () video . release ()","title":"Webcam"},{"location":"example/Webcam/#webcam","text":"The script examples/image/Webcam.py applies a red filter on the input from the webcam, and isolates one mode using a dynamical neural field. Most of the concepts are similar to the Image Processing example. The VideoPopulation object also requires the Python bindings to OpenCV. from ANNarchy import * from ANNarchy.extensions.image import * from ANNarchy.extensions.convolution import Convolution , Pooling clear () ANNarchy 4.7 (4.7.0) on linux (posix). # Definition of the neurons LinearNeuron = Neuron ( equations = \"r=sum(exc): min=0.0\" ) DNF = Neuron ( parameters = \"tau=10.0\" , equations = \"tau*dr/dt + r = sum(exc) + sum(inh): min=0.0, max=1.0\" ) # Population getting the video stream width = 640 height = 480 video = VideoPopulation ( geometry = ( height , width , 3 )) # Subsampling population pooled = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Mean-pooling projection pool_proj = Pooling ( pre = video , post = pooled , target = 'exc' , operation = 'mean' ) pool_proj . connect_pooling () # Define a red filter with no spatial extent red_filter = [[ [ 2.0 , - 1.0 , - 1.0 ] ]] # Create a population of DNF neurons downscaling the image with a factor 10 dnf = Population ( geometry = ( 48 , 64 ), neuron = DNF ) # Create the convolution using the red filter ff = Convolution ( pre = pooled , post = dnf , target = 'exc' ) ff . connect_filter ( weights = red_filter ) # Create difference of Gaussians lateral connections for denoising/competition lat = Projection ( pre = dnf , post = dnf , target = 'inh' ) lat . connect_dog ( amp_pos = 0.2 , sigma_pos = 0.1 , amp_neg = 0.1 , sigma_neg = 0.7 ) <ANNarchy.core.Projection.Projection at 0x7f261f28bf10> The VideoPopulation acquires images from the webcam: here the webcam should be able to deliver 640x480 colored images. The corresponding population is then subsampled with a factor 10, and a red filter is applied on it. This feeds a DNF (see the Neural Field\" example) which selects the region with the highest density. compile () Compiling ... OK We can now start the camera 0 ( /dev/video0 , adapt it to your machine): video . start_camera ( 0 ) [ WARN:0] global /build/opencv/src/opencv-4.5.3/modules/videoio/src/cap_gstreamer.cpp (1081) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1 A simple GUI based on PyQtGraph allows to display the input and output of the network: try : from pyqtgraph.Qt import QtGui , QtCore import pyqtgraph as pg except : print ( 'PyQtGraph is not installed, can not visualize the network.' ) exit ( 0 ) # Wrapping class class Viewer ( object ): \" Class to visualize the network activity using PyQtGraph.\" def __init__ ( self , video , result ): self . video = video self . result = result app = pg . mkQApp () self . win = pg . GraphicsWindow ( title = \"Live webcam\" ) self . win . resize ( 640 , 480 ) box = self . win . addViewBox ( lockAspect = True ) box . invertY () self . vis = pg . ImageItem () box . addItem ( self . vis ) box = self . win . addViewBox ( lockAspect = True ) box . invertY () self . res = pg . ImageItem () box . addItem ( self . res ) self . win . show () self . lastUpdate = pg . ptime . time () self . avgFps = 0.0 def update ( self ): # Set the input self . video . grab_image () # Simulate for 10 ms with a new input simulate ( 5.0 ) # Refresh the GUI self . vis . setImage ( np . swapaxes ( self . video . r , 0 , 1 )) self . res . setImage ( np . swapaxes ( self . result . r , 0 , 1 )) # Listen to mouse/keyboard events QtGui . QApplication . processEvents () # FPS now = pg . ptime . time () fps = 1.0 / ( now - self . lastUpdate ) self . lastUpdate = now self . avgFps = self . avgFps * 0.8 + fps * 0.2 # print(self.avgFps) def run ( self ): timer = QtCore . QTimer () timer . timeout . connect ( self . update ) timer . start ( 0 ) QtGui . QApplication . instance () . exec_ () timer . stop () # Start the GUI view = Viewer ( video , dnf ) view . run () video . release ()","title":"Webcam"},{"location":"manual/Configuration/","text":"Configuration # Setting the discretization step # An important value for the simulation is the discretization step dt . Its default value is 1 ms, which is usually fine for rate-coded networks, but may be too high for spiking networks, as the equations are stiffer. Taken too high, it can lead to high numerical errors. Too low, and the simulation will take an unnecessary amount of time. To set the discretization step, just pass the desired value to setup() at the beginning of the script, or at any rate before the call to compile() : setup(dt=0.1) Changing its value after calling compile() will not have any effect. Setting the seed of the random number generators # By default, the random number generators are seeded with time(NULL) , so each simulation will be different. If you want to have deterministic simulations, you simply need to provide a fixed seed to setup() : setup(seed=62756) Note that this also sets the seed of Numpy, so you can also reproduce random initialization values prduced by numpy.random . Note Using the same seed with the OpenMP and CUDA backends will not lead to the same sequences of numbers! Cleaning the compilation directory # When calling compile() for the first time, a subfolder annarchy/ will be created in the current directory, where the generated code will be compiled. The first compilation may last a couple of seconds, but further modifications to the script are much faster. If no modification to the network has been made except for parameter values, it will not be recompiled, sparing us this overhead. ANNarchy tracks the changes in the script and re-generates the corresponding code. In some cases (a new version of ANNarchy has been installed, bugs), it may be necessary to perform a fresh compilation of the network. You can either delete the annarchy/ subfolder and restart the script: $ rm -rf annarchy/ $ python MyNetwork.py or pass the --clean flag to Python: $ python MyNetwork.py --clean Selecting the compiler # ANNarchy requires a C++ compiler. On GNU/Linux, the default choice is g++ , while on MacOS it is clang++ . You can change the compiler (and its flags) to use either during the call to compile() in your script: compile(compiler=\"clang++\", compiler_flags=\"-march=native -O2\") or globally by modifying the configuration file located at ~/.config/ANNarchy/annarchy.json : { \"openmp\" : { \"compiler\" : \"clang++\" , \"flags\" : \"-march=native -O2\" } } Be careful with the flags: for example, the optimization level -O3 does not obligatorily produce faster code. Parallel computing with OpenMP # The default paradigm for an ANNarchy simulation is through openMP, which distributes automatically the computations over the available CPU cores. By default, OpenMP would use all the available cores for your simulation, even if it is not optimal: small networks in particular tend to run faster with a small amount of cores (for the provided example with Neural Fields, it is for example 2). For this reason, the OMP_NUM_THREADS environment variable has no effect in ANNarchy. You can control the number of cores by passing the -j flag to the Python command: user@machine:~$ python NeuralField.py -j2 It is the responsability of the user to find out which number of cores is optimal for his network, by comparing simulation times. When this optimal number is found, it can be hard-coded in the script by setting the num_threads argument to ANNarchy.setup() : from ANNarchy import * setup ( num_threads = 2 ) Parallel computing with CUDA # To run your network on GPUs, you need to declare to ANNarchy that you want to use CUDA. One way to do so is to pass the --gpu flag to the command line: user@machine:~$ python NeuralField.py --gpu You can also set the paradigm argument of ANNarchy.setup() to make it permanent: from ANNarchy import * setup ( paradigm = \"cuda\" ) If there are multiple GPUs on your machine, you can select the ID of the device by specifying it to the --gpu flag on the command line: user@machine:~$ python NeuralField.py --gpu=2 You can also pass the cuda_config dictionary argument to compile() : compile ( cuda_config = { 'device' : 2 }) The default GPU is defined in the configuration file ~/.config/ANNarchy/annarchy.json (0 unless you modify it). { \"cuda\" : { \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } Hint As the current implementation is a development version, some of the features provided by ANNarchy are not supported yet with CUDA: weight sharing, non-uniform synaptic delays, structural plasticity, spiking neurons: a) with mean firing rate and b) continous integration of inputs, SpikeSourceArray .","title":"Configuration"},{"location":"manual/Configuration/#configuration","text":"","title":"Configuration"},{"location":"manual/Configuration/#setting-the-discretization-step","text":"An important value for the simulation is the discretization step dt . Its default value is 1 ms, which is usually fine for rate-coded networks, but may be too high for spiking networks, as the equations are stiffer. Taken too high, it can lead to high numerical errors. Too low, and the simulation will take an unnecessary amount of time. To set the discretization step, just pass the desired value to setup() at the beginning of the script, or at any rate before the call to compile() : setup(dt=0.1) Changing its value after calling compile() will not have any effect.","title":"Setting the discretization step"},{"location":"manual/Configuration/#setting-the-seed-of-the-random-number-generators","text":"By default, the random number generators are seeded with time(NULL) , so each simulation will be different. If you want to have deterministic simulations, you simply need to provide a fixed seed to setup() : setup(seed=62756) Note that this also sets the seed of Numpy, so you can also reproduce random initialization values prduced by numpy.random . Note Using the same seed with the OpenMP and CUDA backends will not lead to the same sequences of numbers!","title":"Setting the seed of the random number generators"},{"location":"manual/Configuration/#cleaning-the-compilation-directory","text":"When calling compile() for the first time, a subfolder annarchy/ will be created in the current directory, where the generated code will be compiled. The first compilation may last a couple of seconds, but further modifications to the script are much faster. If no modification to the network has been made except for parameter values, it will not be recompiled, sparing us this overhead. ANNarchy tracks the changes in the script and re-generates the corresponding code. In some cases (a new version of ANNarchy has been installed, bugs), it may be necessary to perform a fresh compilation of the network. You can either delete the annarchy/ subfolder and restart the script: $ rm -rf annarchy/ $ python MyNetwork.py or pass the --clean flag to Python: $ python MyNetwork.py --clean","title":"Cleaning the compilation directory"},{"location":"manual/Configuration/#selecting-the-compiler","text":"ANNarchy requires a C++ compiler. On GNU/Linux, the default choice is g++ , while on MacOS it is clang++ . You can change the compiler (and its flags) to use either during the call to compile() in your script: compile(compiler=\"clang++\", compiler_flags=\"-march=native -O2\") or globally by modifying the configuration file located at ~/.config/ANNarchy/annarchy.json : { \"openmp\" : { \"compiler\" : \"clang++\" , \"flags\" : \"-march=native -O2\" } } Be careful with the flags: for example, the optimization level -O3 does not obligatorily produce faster code.","title":"Selecting the compiler"},{"location":"manual/Configuration/#parallel-computing-with-openmp","text":"The default paradigm for an ANNarchy simulation is through openMP, which distributes automatically the computations over the available CPU cores. By default, OpenMP would use all the available cores for your simulation, even if it is not optimal: small networks in particular tend to run faster with a small amount of cores (for the provided example with Neural Fields, it is for example 2). For this reason, the OMP_NUM_THREADS environment variable has no effect in ANNarchy. You can control the number of cores by passing the -j flag to the Python command: user@machine:~$ python NeuralField.py -j2 It is the responsability of the user to find out which number of cores is optimal for his network, by comparing simulation times. When this optimal number is found, it can be hard-coded in the script by setting the num_threads argument to ANNarchy.setup() : from ANNarchy import * setup ( num_threads = 2 )","title":"Parallel computing with OpenMP"},{"location":"manual/Configuration/#parallel-computing-with-cuda","text":"To run your network on GPUs, you need to declare to ANNarchy that you want to use CUDA. One way to do so is to pass the --gpu flag to the command line: user@machine:~$ python NeuralField.py --gpu You can also set the paradigm argument of ANNarchy.setup() to make it permanent: from ANNarchy import * setup ( paradigm = \"cuda\" ) If there are multiple GPUs on your machine, you can select the ID of the device by specifying it to the --gpu flag on the command line: user@machine:~$ python NeuralField.py --gpu=2 You can also pass the cuda_config dictionary argument to compile() : compile ( cuda_config = { 'device' : 2 }) The default GPU is defined in the configuration file ~/.config/ANNarchy/annarchy.json (0 unless you modify it). { \"cuda\" : { \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } Hint As the current implementation is a development version, some of the features provided by ANNarchy are not supported yet with CUDA: weight sharing, non-uniform synaptic delays, structural plasticity, spiking neurons: a) with mean firing rate and b) continous integration of inputs, SpikeSourceArray .","title":"Parallel computing with CUDA"},{"location":"manual/Connector/","text":"Connectivity # There are basically four methods to instantiate projections: By using a built-in connector method. By using a saved projection. By loading dense or sparse matrices. By defining a custom connector method. Available connector methods # For further detailed information about these connectors, please refer to the library reference Projections . connect_all_to_all # All neurons of the post-synaptic population form connections with all neurons of the pre-synaptic population (dense connectivity). Self-connections are avoided by default, but the parameter allow_self_connections can be set to True : proj . connect_all_to_all ( weights = 1.0 , delays = 2.0 , allow_self_connections = False ) The weights and delays arguments accept both single float values (all synapses will take this initial value), as well as random objects allowing to randomly select the initial values for different synapses: proj . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) connect_one_to_one # A neuron of the post-synaptic population forms a connection with only one neuron of the pre-synaptic population. The order depends on the ranks: neuron 0 is connected with neuron 0 and so on. It is advised that the pre- and post-populations have the same size/geometry, especially when using population views. pop1 = Population (( 20 , 20 ), Neuron ( parameters = \"r=0.0\" )) pop2 = Population (( 10 , 10 ), Neuron ( equations = \"r=sum(exc)\" )) proj = Projection ( pop1 [ 5 : 15 , 5 : 15 ], pop2 , 'exc' ) proj . connect_one_to_one ( weights = 1.0 ) Weights and delays also accept random distributions. Below is a graphical representation of the difference between all_to_all and one_to_one : connect_gaussian # A neuron of the post-synaptic population forms a connection with a limited region of the pre-synaptic population, centered around the neuron with the same normalized position. Weight values are initialized using a Gaussian function, with a maximal value amp for the neuron of same position and decreasing with distance (standard deviation sigma ): \\[w(x, y) = A \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma^2})\\] where \\((x, y)\\) is the position of the pre-synaptic neuron (normalized to \\([0, 1]^d\\) ) and \\((x_c, y_c)\\) is the position of the post-synaptic neuron (normalized to \\([0, 1]^d\\) ). A = amp, sigma = \\(\\sigma\\) . In order to void creating useless synapses, the parameter limit can be set to restrict the creation of synapses to the cases where the value of the weight would be superior to limit*abs(amp) . Default is 0.01 (1%). Self-connections are avoided by default (parameter allow_self_connections ). The two populations must have the same number of dimensions, but the number of neurons can vary as the positions of each neuron are normalized in \\([0, 1]^d\\) : proj . connect_gaussian ( amp = 1.0 , sigma = 0.2 , limit = 0.001 ) connect_dog # The same as connect_gaussian , except weight values are computed using a Difference-of-Gaussians (DoG), usually positive in the center, negative a bit further away and small at long distances. \\[\\begin{aligned} w(x, y) &= A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_+^2}) \\\\ &- A^- \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_-^2}) \\\\ \\end{aligned}\\] Weights smaller than limit * abs(amp_pos - amp_neg) are not created and self-connections are avoided by default (parameter allow_self_connections ): proj . connect_dog ( amp_pos = 1.0 , sigma_pos = 0.2 , amp_neg = 0.3 , sigma_neg = 0.7 , limit = 0.001 ) The following figure shows the example of a neuron of coordinates (10, 10) in the post-synaptic population, which is connected through the gaussian (left) and dog (right) projections to a population of geometry 30*30. The X and Y axis denote the coordinates of the pre-synaptic neurons, while the Z axis is the weight value. connect_fixed_number_pre # Each neuron in the post-synaptic population receives connections from a fixed number of neurons of the pre-synaptic population chosen randomly. It may happen that two post-synaptic neurons are connected to the same pre-synaptic neuron and that some pre-synaptic neurons are connected to nothing: proj . connect_fixed_number_pre ( number = 20 , weights = 1.0 ) weights and delays can also take a random object. connect_fixed_number_post # Each neuron in the pre-synaptic population sends a connection to a fixed number of neurons of the post-synaptic population chosen randomly. It may happen that two pre-synaptic neurons are connected to the same post-synaptic neuron and that some post-synaptic neurons receive no connection at all: proj . connect_fixed_number_post ( number = 20 , weights = 1.0 ) The following figure shows the fixed_number_pre (left) and fixed_number_post projections between two populations of 4 neurons, with number=2 . In fixed_number_pre , each post-synaptic neuron receives exactly 2 connections, while in fixed_number_post , each pre-synaptic neuron send exactly two connections: connect_fixed_probability # For each post-synaptic neuron, there is a fixed probability that it forms a connection with a neuron of the pre-synaptic population. It is basically a all_to_all projection, except some synapses are not created, making the projection sparser: proj . connect_fixed_probability ( probability = 0.2 , weights = 1.0 ) Important If a single value is used for the weights argument of connect_all_to_all , connect_one_to_one , connect_fixed_probability , connect_fixed_number_pre and connect_fixed_number_post , and the default synapse is used (no synaptic plasticity), ANNarchy will generate a single weight value for all the synapses of the projection, not one per synapse. This allows to save a lot of memory and improve performance. However, if you wish to manually change the weights of some of the synapses after the creation, you need to force the creation of one value per synapse by setting force_multiple_weights=True in the call to the connector. Saved connectivity # It is also possible to build a connection pattern using data saved during a precedent simulation. This is useful when: pre-learning is done in another context; a connector method for static synapses is particularly slow (e.g. DoG), but loading the result from a file is faster. The connectivity of a projection can be saved (after compile() ) using: proj . save_connectivity ( filename = 'proj.npz' ) The filename can used relative or absolute paths. The data is saved in a binary format: Compressed Numpy format when the filename ends with .npz . Compressed binary file format when the filename ends with .gz . Binary file format otherwise. It can then be used to instantiate another projection: proj . connect_from_file ( filename = 'proj.npz' ) Only the connectivity (which neurons are connected), the weights and delays are loaded. Other synaptic variables are left untouched. The pre- and post-synaptic population must have the same size during saving and loading. From connectivity matrices # One can also create connections using Numpy dense matrices or Scipy sparse matrices. connect_from_matrix # This method accepts a Numpy array to define the weights of the projection (and optionally the delays). By default, the matrix should have the size (post.size, pre.size) , so that the first index represents a post-synaptic neuron and the second the pre-synaptic neurons. If your matrix is defined in the reversed order, you can either transpose it or set the pre_post argument to True . This method is useful for dense connectivity matrices (all-to-all). If you do not want to create some synapses, the weight value should be set to None . The following code creates a synfire chain inside a population of 100 neurons: N = 100 proj = Projection ( pop , pop , 'exc' ) # Initialize an empty connectivity matrix w = np . array ([[ None ] * N ] * N ) # Connect each post-synaptic neuron to its predecessor for i in range ( N ): w [ i , ( i - 1 ) % N ] = 1.0 # Create the connections proj . connect_from_matrix ( w ) Connectivity matrices can not work with multi-dimensional coordinates, only ranks are used. Population views can be used in the projection, but the connection matrix must have the corresponding size: proj = Projection ( pop [ 10 : 20 ], pop [ 50 : 60 ], 'exc' ) # Create the connectivity matrix w = np . ones (( 10 , 10 )) # Create the connections proj . connect_from_matrix ( w ) connect_from_sparse # For sparse connection matrices, the Numpy array format may have a huge memory overhead if most of its values are None. It is possible to use Scipy sparse matrices in that case. The previous synfire chain example becomes: from scipy.sparse import lil_matrix proj = Projection ( pop , pop , 'exc' ) w = lil_matrix (( N , N )) for i in range ( N ): w [ i , ( i + 1 ) % N ] = 1.0 proj . connect_from_sparse ( w ) Note Contrary to connect_from_matrix() , the first index of the sparse matrix represents the pre-synaptic neurons, not the post-synaptic ones. This is for compatibility with other neural simulators. connect_from_sparse() accepts lil_matrix , csr_matrix and csc_matrix objects, although lil_matrix should be preferred for its simplicity of element access. User-defined patterns # This section describes the creation of user-specific connection patterns in ANNarchy, if the available patterns are not enough. A connection pattern is simply implemented as a method returning a CSR (compressed sparse-row) object containing all the necessary information to create the synapses. A connector method must take on the first position the pre-synaptic population (or a subset of it) and on the second one the post-synaptic population. Other arguments are free, but should be passed when creating the projection. probabilistic_pattern ( pre , post , < other arguments > ) As an example, we will recreate the fixed_probability connector method, building synapses with a given probability. For this new pattern we need a weight value (common for all synapses) and a probability value as additional arguments. We consider that no delay is introduced in the synaptic transmission.. from ANNarchy import * def probabilistic_pattern ( pre , post , weight , probability ): synapses = CSR () ... pattern code comes here ... return synapses fixed_probability in Python # The connector method needs to return a CSR object storing the connectivity. For each post-synaptic neuron receiving synapses, a list of pre-synaptic ranks, weight values and delays must be added to the structure. If you use 2D or 3D populations you need to transform the coordinates into ranks with the rank_from_coordinates function. import random from ANNarchy import * def probabilistic_pattern ( pre , post , weight , probability ): # Create a compressed sparse row (CSR) structure for the connectivity matrix synapses = CSR () # For all neurons in the post-synaptic population for post_rank in xrange ( post . size ): # Decide which pre-synaptic neurons should form synapses ranks = [] for pre_rank in xrange ( pre . size ): if random . random () < probability : ranks . append ( pre_rank ) # Create weights and delays arrays of the same size values = [ weight for i in xrange ( len ( ranks )) ] delays = [ 0 for i in xrange ( len ( ranks )) ] # Add this information to the CSR matrix synapses . add ( post_rank , ranks , values , delays ) return synapses The first for - loop iterates over all post-synaptic neurons in the projection. The inner for - loop decides for each of these neurons if a synapse with a pre-synaptic neuron should be created, based on the value probability provided as argument to the function. The lists values and delays are then created with the same size as ranks (important!), and filled with the desired value. All this information is then fed into the CSR matrix using the add(post_rank, ranks, values, delays) method. Note Building such connectivity matrices in Python can be extremely slow, as Python is not made for tight nested loops. If the construction of your network lasts too long, you should definitely write this function in Cython . Warning The add() should be only called once per post-synaptic neuron! If not, ANNarchy will have to reorder its internal representations and this will be really slow. Usage of the pattern To use the pattern within a projection you provide the pattern method to the connect_with_func method of Projection proj = Projection ( pre = pop1 , post = pop2 , target = 'inh' ) proj . connect_with_func ( method = probabilistic_pattern , weight = 1.0 , probability = 0.3 ) method is the method you just wrote. Extra arguments (other than pre and post ) should be passed with the same name. fixed_probability in Cython # For this example, we will create a Cython file CustomPatterns.pyx in the same directory as the script. Its content should be relatively similar to the Python version, except some type definitions: # distutils: language = c++ import random import ANNarchy cimport ANNarchy . core . cython_ext . Connector as Connector def probabilistic_pattern ( pre , post , weight , probability ): # Typedefs cdef Connector . LILConnectivity synapses cdef int post_rank , pre_rank cdef list ranks , values , delays # Create a LILConnectivity structure for the connectivity matrix synapses = Connector . LILConnectivity () # For all neurons in the post-synaptic population for post_rank in xrange ( post . size ): # Decide which pre-synaptic neurons should form synapses ranks = [] for pre_rank in xrange ( pre . size ): if random . random () < probability : ranks . append ( pre_rank ) # Create weights and delays arrays of the same size values = [ weight for i in xrange ( len ( ranks )) ] delays = [ 0 for i in xrange ( len ( ranks )) ] # Add this information to the LILConnectivity matrix synapses . add ( post_rank , ranks , values , delays ) return synapses The only differences with the Python code are: The module Connector where the LILConnectivity connection matrix class is defined should be cimported with: cimport ANNarchy . core . cython_ext . Connector as Connector Data structures should be declared with cdef at the beginning of the method: # Typedefs cdef Connector . LILConnectivity synapses cdef int post_rank , pre_rank cdef list ranks , values , delays To allow Cython to compile this file, we also need to provide with a kind of \\\"Makefile\\\" specifying that the code should be generated in C++, not C. This file should have the same name as the Cython file but end with .pyxbld , here : CustomPatterns.pyxbld . from distutils.extension import Extension import ANNarchy def make_ext ( modname , pyxfilename ): return Extension ( name = modname , sources = [ pyxfilename ], include_dirs = ANNarchy . include_path (), extra_compile_args = [ '-std=c++11' ], language = \"c++\" ) Note This .pyxbld is generic, you don't need to modify anything, except its name. Now you can import the method probabilistic_pattern() into your Python code using the pyximport module of Cython and build the Projection normally: import pyximport ; pyximport . install () from CustomPatterns import probabilistic_pattern proj . connect_with_func ( method = probabilistic_pattern , weight = 1.0 , probability = 0.3 ) Writing the connector in Cython can bring speedups up to 100x compared to Python if the projection has a lot of synapses.","title":"Connectivity"},{"location":"manual/Connector/#connectivity","text":"There are basically four methods to instantiate projections: By using a built-in connector method. By using a saved projection. By loading dense or sparse matrices. By defining a custom connector method.","title":"Connectivity"},{"location":"manual/Connector/#available-connector-methods","text":"For further detailed information about these connectors, please refer to the library reference Projections .","title":"Available connector methods"},{"location":"manual/Connector/#connect_all_to_all","text":"All neurons of the post-synaptic population form connections with all neurons of the pre-synaptic population (dense connectivity). Self-connections are avoided by default, but the parameter allow_self_connections can be set to True : proj . connect_all_to_all ( weights = 1.0 , delays = 2.0 , allow_self_connections = False ) The weights and delays arguments accept both single float values (all synapses will take this initial value), as well as random objects allowing to randomly select the initial values for different synapses: proj . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 ))","title":"connect_all_to_all"},{"location":"manual/Connector/#connect_one_to_one","text":"A neuron of the post-synaptic population forms a connection with only one neuron of the pre-synaptic population. The order depends on the ranks: neuron 0 is connected with neuron 0 and so on. It is advised that the pre- and post-populations have the same size/geometry, especially when using population views. pop1 = Population (( 20 , 20 ), Neuron ( parameters = \"r=0.0\" )) pop2 = Population (( 10 , 10 ), Neuron ( equations = \"r=sum(exc)\" )) proj = Projection ( pop1 [ 5 : 15 , 5 : 15 ], pop2 , 'exc' ) proj . connect_one_to_one ( weights = 1.0 ) Weights and delays also accept random distributions. Below is a graphical representation of the difference between all_to_all and one_to_one :","title":"connect_one_to_one"},{"location":"manual/Connector/#connect_gaussian","text":"A neuron of the post-synaptic population forms a connection with a limited region of the pre-synaptic population, centered around the neuron with the same normalized position. Weight values are initialized using a Gaussian function, with a maximal value amp for the neuron of same position and decreasing with distance (standard deviation sigma ): \\[w(x, y) = A \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma^2})\\] where \\((x, y)\\) is the position of the pre-synaptic neuron (normalized to \\([0, 1]^d\\) ) and \\((x_c, y_c)\\) is the position of the post-synaptic neuron (normalized to \\([0, 1]^d\\) ). A = amp, sigma = \\(\\sigma\\) . In order to void creating useless synapses, the parameter limit can be set to restrict the creation of synapses to the cases where the value of the weight would be superior to limit*abs(amp) . Default is 0.01 (1%). Self-connections are avoided by default (parameter allow_self_connections ). The two populations must have the same number of dimensions, but the number of neurons can vary as the positions of each neuron are normalized in \\([0, 1]^d\\) : proj . connect_gaussian ( amp = 1.0 , sigma = 0.2 , limit = 0.001 )","title":"connect_gaussian"},{"location":"manual/Connector/#connect_dog","text":"The same as connect_gaussian , except weight values are computed using a Difference-of-Gaussians (DoG), usually positive in the center, negative a bit further away and small at long distances. \\[\\begin{aligned} w(x, y) &= A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_+^2}) \\\\ &- A^- \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_-^2}) \\\\ \\end{aligned}\\] Weights smaller than limit * abs(amp_pos - amp_neg) are not created and self-connections are avoided by default (parameter allow_self_connections ): proj . connect_dog ( amp_pos = 1.0 , sigma_pos = 0.2 , amp_neg = 0.3 , sigma_neg = 0.7 , limit = 0.001 ) The following figure shows the example of a neuron of coordinates (10, 10) in the post-synaptic population, which is connected through the gaussian (left) and dog (right) projections to a population of geometry 30*30. The X and Y axis denote the coordinates of the pre-synaptic neurons, while the Z axis is the weight value.","title":"connect_dog"},{"location":"manual/Connector/#connect_fixed_number_pre","text":"Each neuron in the post-synaptic population receives connections from a fixed number of neurons of the pre-synaptic population chosen randomly. It may happen that two post-synaptic neurons are connected to the same pre-synaptic neuron and that some pre-synaptic neurons are connected to nothing: proj . connect_fixed_number_pre ( number = 20 , weights = 1.0 ) weights and delays can also take a random object.","title":"connect_fixed_number_pre"},{"location":"manual/Connector/#connect_fixed_number_post","text":"Each neuron in the pre-synaptic population sends a connection to a fixed number of neurons of the post-synaptic population chosen randomly. It may happen that two pre-synaptic neurons are connected to the same post-synaptic neuron and that some post-synaptic neurons receive no connection at all: proj . connect_fixed_number_post ( number = 20 , weights = 1.0 ) The following figure shows the fixed_number_pre (left) and fixed_number_post projections between two populations of 4 neurons, with number=2 . In fixed_number_pre , each post-synaptic neuron receives exactly 2 connections, while in fixed_number_post , each pre-synaptic neuron send exactly two connections:","title":"connect_fixed_number_post"},{"location":"manual/Connector/#connect_fixed_probability","text":"For each post-synaptic neuron, there is a fixed probability that it forms a connection with a neuron of the pre-synaptic population. It is basically a all_to_all projection, except some synapses are not created, making the projection sparser: proj . connect_fixed_probability ( probability = 0.2 , weights = 1.0 ) Important If a single value is used for the weights argument of connect_all_to_all , connect_one_to_one , connect_fixed_probability , connect_fixed_number_pre and connect_fixed_number_post , and the default synapse is used (no synaptic plasticity), ANNarchy will generate a single weight value for all the synapses of the projection, not one per synapse. This allows to save a lot of memory and improve performance. However, if you wish to manually change the weights of some of the synapses after the creation, you need to force the creation of one value per synapse by setting force_multiple_weights=True in the call to the connector.","title":"connect_fixed_probability"},{"location":"manual/Connector/#saved-connectivity","text":"It is also possible to build a connection pattern using data saved during a precedent simulation. This is useful when: pre-learning is done in another context; a connector method for static synapses is particularly slow (e.g. DoG), but loading the result from a file is faster. The connectivity of a projection can be saved (after compile() ) using: proj . save_connectivity ( filename = 'proj.npz' ) The filename can used relative or absolute paths. The data is saved in a binary format: Compressed Numpy format when the filename ends with .npz . Compressed binary file format when the filename ends with .gz . Binary file format otherwise. It can then be used to instantiate another projection: proj . connect_from_file ( filename = 'proj.npz' ) Only the connectivity (which neurons are connected), the weights and delays are loaded. Other synaptic variables are left untouched. The pre- and post-synaptic population must have the same size during saving and loading.","title":"Saved connectivity"},{"location":"manual/Connector/#from-connectivity-matrices","text":"One can also create connections using Numpy dense matrices or Scipy sparse matrices.","title":"From connectivity matrices"},{"location":"manual/Connector/#connect_from_matrix","text":"This method accepts a Numpy array to define the weights of the projection (and optionally the delays). By default, the matrix should have the size (post.size, pre.size) , so that the first index represents a post-synaptic neuron and the second the pre-synaptic neurons. If your matrix is defined in the reversed order, you can either transpose it or set the pre_post argument to True . This method is useful for dense connectivity matrices (all-to-all). If you do not want to create some synapses, the weight value should be set to None . The following code creates a synfire chain inside a population of 100 neurons: N = 100 proj = Projection ( pop , pop , 'exc' ) # Initialize an empty connectivity matrix w = np . array ([[ None ] * N ] * N ) # Connect each post-synaptic neuron to its predecessor for i in range ( N ): w [ i , ( i - 1 ) % N ] = 1.0 # Create the connections proj . connect_from_matrix ( w ) Connectivity matrices can not work with multi-dimensional coordinates, only ranks are used. Population views can be used in the projection, but the connection matrix must have the corresponding size: proj = Projection ( pop [ 10 : 20 ], pop [ 50 : 60 ], 'exc' ) # Create the connectivity matrix w = np . ones (( 10 , 10 )) # Create the connections proj . connect_from_matrix ( w )","title":"connect_from_matrix"},{"location":"manual/Connector/#connect_from_sparse","text":"For sparse connection matrices, the Numpy array format may have a huge memory overhead if most of its values are None. It is possible to use Scipy sparse matrices in that case. The previous synfire chain example becomes: from scipy.sparse import lil_matrix proj = Projection ( pop , pop , 'exc' ) w = lil_matrix (( N , N )) for i in range ( N ): w [ i , ( i + 1 ) % N ] = 1.0 proj . connect_from_sparse ( w ) Note Contrary to connect_from_matrix() , the first index of the sparse matrix represents the pre-synaptic neurons, not the post-synaptic ones. This is for compatibility with other neural simulators. connect_from_sparse() accepts lil_matrix , csr_matrix and csc_matrix objects, although lil_matrix should be preferred for its simplicity of element access.","title":"connect_from_sparse"},{"location":"manual/Connector/#user-defined-patterns","text":"This section describes the creation of user-specific connection patterns in ANNarchy, if the available patterns are not enough. A connection pattern is simply implemented as a method returning a CSR (compressed sparse-row) object containing all the necessary information to create the synapses. A connector method must take on the first position the pre-synaptic population (or a subset of it) and on the second one the post-synaptic population. Other arguments are free, but should be passed when creating the projection. probabilistic_pattern ( pre , post , < other arguments > ) As an example, we will recreate the fixed_probability connector method, building synapses with a given probability. For this new pattern we need a weight value (common for all synapses) and a probability value as additional arguments. We consider that no delay is introduced in the synaptic transmission.. from ANNarchy import * def probabilistic_pattern ( pre , post , weight , probability ): synapses = CSR () ... pattern code comes here ... return synapses","title":"User-defined patterns"},{"location":"manual/Connector/#fixed_probability-in-python","text":"The connector method needs to return a CSR object storing the connectivity. For each post-synaptic neuron receiving synapses, a list of pre-synaptic ranks, weight values and delays must be added to the structure. If you use 2D or 3D populations you need to transform the coordinates into ranks with the rank_from_coordinates function. import random from ANNarchy import * def probabilistic_pattern ( pre , post , weight , probability ): # Create a compressed sparse row (CSR) structure for the connectivity matrix synapses = CSR () # For all neurons in the post-synaptic population for post_rank in xrange ( post . size ): # Decide which pre-synaptic neurons should form synapses ranks = [] for pre_rank in xrange ( pre . size ): if random . random () < probability : ranks . append ( pre_rank ) # Create weights and delays arrays of the same size values = [ weight for i in xrange ( len ( ranks )) ] delays = [ 0 for i in xrange ( len ( ranks )) ] # Add this information to the CSR matrix synapses . add ( post_rank , ranks , values , delays ) return synapses The first for - loop iterates over all post-synaptic neurons in the projection. The inner for - loop decides for each of these neurons if a synapse with a pre-synaptic neuron should be created, based on the value probability provided as argument to the function. The lists values and delays are then created with the same size as ranks (important!), and filled with the desired value. All this information is then fed into the CSR matrix using the add(post_rank, ranks, values, delays) method. Note Building such connectivity matrices in Python can be extremely slow, as Python is not made for tight nested loops. If the construction of your network lasts too long, you should definitely write this function in Cython . Warning The add() should be only called once per post-synaptic neuron! If not, ANNarchy will have to reorder its internal representations and this will be really slow. Usage of the pattern To use the pattern within a projection you provide the pattern method to the connect_with_func method of Projection proj = Projection ( pre = pop1 , post = pop2 , target = 'inh' ) proj . connect_with_func ( method = probabilistic_pattern , weight = 1.0 , probability = 0.3 ) method is the method you just wrote. Extra arguments (other than pre and post ) should be passed with the same name.","title":"fixed_probability in Python"},{"location":"manual/Connector/#fixed_probability-in-cython","text":"For this example, we will create a Cython file CustomPatterns.pyx in the same directory as the script. Its content should be relatively similar to the Python version, except some type definitions: # distutils: language = c++ import random import ANNarchy cimport ANNarchy . core . cython_ext . Connector as Connector def probabilistic_pattern ( pre , post , weight , probability ): # Typedefs cdef Connector . LILConnectivity synapses cdef int post_rank , pre_rank cdef list ranks , values , delays # Create a LILConnectivity structure for the connectivity matrix synapses = Connector . LILConnectivity () # For all neurons in the post-synaptic population for post_rank in xrange ( post . size ): # Decide which pre-synaptic neurons should form synapses ranks = [] for pre_rank in xrange ( pre . size ): if random . random () < probability : ranks . append ( pre_rank ) # Create weights and delays arrays of the same size values = [ weight for i in xrange ( len ( ranks )) ] delays = [ 0 for i in xrange ( len ( ranks )) ] # Add this information to the LILConnectivity matrix synapses . add ( post_rank , ranks , values , delays ) return synapses The only differences with the Python code are: The module Connector where the LILConnectivity connection matrix class is defined should be cimported with: cimport ANNarchy . core . cython_ext . Connector as Connector Data structures should be declared with cdef at the beginning of the method: # Typedefs cdef Connector . LILConnectivity synapses cdef int post_rank , pre_rank cdef list ranks , values , delays To allow Cython to compile this file, we also need to provide with a kind of \\\"Makefile\\\" specifying that the code should be generated in C++, not C. This file should have the same name as the Cython file but end with .pyxbld , here : CustomPatterns.pyxbld . from distutils.extension import Extension import ANNarchy def make_ext ( modname , pyxfilename ): return Extension ( name = modname , sources = [ pyxfilename ], include_dirs = ANNarchy . include_path (), extra_compile_args = [ '-std=c++11' ], language = \"c++\" ) Note This .pyxbld is generic, you don't need to modify anything, except its name. Now you can import the method probabilistic_pattern() into your Python code using the pyximport module of Cython and build the Projection normally: import pyximport ; pyximport . install () from CustomPatterns import probabilistic_pattern proj . connect_with_func ( method = probabilistic_pattern , weight = 1.0 , probability = 0.3 ) Writing the connector in Cython can bring speedups up to 100x compared to Python if the projection has a lot of synapses.","title":"fixed_probability in Cython"},{"location":"manual/ConvolutionalNetworks/","text":"Convolution and pooling # Projections use by default a set of weights per post-synaptic neuron. Some networks, including convolutional networks, define a single operation (convolution or pooling) to be applied systematically on all pre-synaptic neurons. In such cases, it would be a waste of resources to allocate weights for each post-synaptic neuron. The extension convolution (see its API ) allows to implement such specific projections. It has to be imported explicitly at the beginning of the script: from ANNarchy import * from ANNarchy.extensions.convolution import * Warning Shared weights are only implemented for rate-coded networks. The only possible backend is currently OpenMP, CUDA will be implemented later. Simple convolutions # The simplest case of convolution is when the pre- and post-synaptic population have the same number of dimensions, for example: pre = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) Contrary to normal projections, the geometry of the populations (number of dimensions and neurons in each dimension) has a great influence on the operation to be performed. In particular the number of dimensions will define how the convolution will be applied. If for example the pre-synaptic population represents an 2D image, you may want to apply a vertical edge detector to it and get the result in the post-synaptic population. Such a filter can be defined by the following Numpy array: vertical_filter = np . array ( [ [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ] ] ) With 2 dimensions, the convolution operation (or more exactly, cross-correlation ) with a 3*3 filter is defined for all neurons in the post-synaptic population by: \\[\\text{post}[i, j] = \\sum_{c_i=-1}^1 \\sum_{c_j=-1}^1 \\text{filter}[c_i][c_j] \\cdot \\text{pre}[i + c_i, j + c_j]\\] Such a convolution is achieved by creating a Convolution object and using the connect_filter() method to create the connection pattern: proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = vertical_filter ) Each neuron of the post-synaptic population will then receive in sum('exc') (or whatever target name is used) the convolution between the kernel and a sub-region of the pre-syanptic population. ANNarchy defines the convolution operation for populations having 1, 2, 3, or 4 dimensions. Several options can be passed to the convolve() method: padding defines the value of the pre-synaptic firing rates which will be used when the coordinates are out-of-bounds. By default zero-padding is used, but you can specify another value with this argument. You can also use the 'border' value to repeat the firing rate of the neurons on the border (for example, if the filter tries to reach a neuron of coordinates (-1, -1), the firing rate of the neuron (0, 0) will be used instead). subsampling . In convolutional networks, the convolution operation is often coupled with a reduction in the number of neurons in each dimension. In the example above, the post-synaptic population could be defined with a geometry (50, 50). For each post-synaptic neuron, the coordinates of the center of the applied kernel would be automatically shifted from two pre-synaptic neurons compared to the previous one. However, if the number of neurons in one dimension of the pre-synaptic population is not exactly a multiple of the number of post-synaptic neurons in the same dimension, ANNarchy can not guess what the correct correspondance should be. In this case, you have to specify this mapping by providing to the subsampling argument a list of pre-synaptic coordinates defining the position of the center of the kernel for each post-synaptic neuron. The list is indexed by the rank of the post-synaptic neurons (use the rank_from_coordinates() method) and must have the same size as the population. Each element should be a list of coordinates in the pre-synaptic population's geometry (with as many elements as dimensions). It is possible to provide a Numpy array instead of a list of lists. One can access the coordinates in the pre-synaptic geometry of the center of the filter corresponding to a particular post-synaptic neuron by calling the center() method of Convolution with the rank or coordinates of the post neuron: pre = Population ( geometry = ( 100 , 100 ), neuron = Whatever ) post = Population ( geometry = ( 50 , 50 ), neuron = Whatever ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = vertical_filter ) pre_coordinates = proj . center ( 10 , 10 ) # returns (20, 20) In some cases, the post-synaptic population can have less dimensions than the pre-synaptic one. An example would be when the pre-synaptic population has three dimensions (e.g. (100, 100, 3)), the last representing the R, G and B components of an image. A 3D filter, with 3 components in the last dimension, would result in a (100, 100, 1) post-synaptic population (or any subsampling of it). ANNarchy accepts in this case the use of a 2D population (100, 100), but it will be checked that the number of elements in the last dimension of the filter equals the number of pre-synaptic neurons in the last dimension: pre = Population ( geometry = ( 100 , 100 , 3 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) red_filter = np . array ( [ [ [ 2.0 , - 1.0 , - 1.0 ] ] ] ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = red_filter ) Non-linear convolutions # Convolution uses by default a regular cross-correlation, summing w * pre.r over the extent of the kernel. As for regular synapses, you can change this behavior when creating the projection: the psp argument defines what will be summed. It is w*pre.r by default but can be changed to any combination of w and pre.r , such as w * log(1+pre.r) : proj = Convolution ( pre = pre , post = post , target = 'exc' , psp = 'w*log(1+pre.r)' ) the operation argument allows to change the summation operation. You can set it to 'max' (the maximum value of w*pre.r over the extent of the filter will be returned), 'min' (minimum) or 'mean' (same as 'sum', but normalized by the number of elements in the filter). The default is 'sum': proj = Convolution ( pre = pre , post = post , target = 'exc' , operation = 'max' ) Layer-wise convolutions # It is possible to define kernels with less dimensions than the pre-synaptic population. A 2D filter can for example be applied on each color component independently: pre = Population ( geometry = ( 100 , 100 , 3 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 , 3 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) vertical_filter = np . array ( [ [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ] ] ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = vertical_filter , keep_last_dimension = True ) The important parameter in this case is keep_last_dimension which tells the code generator that the last dimension of the input should not be used for convolution. The important constraint is that the post-synaptic population must have the same number of neurons in the last dimension than the pre-synaptic one (no subsampling is possible by definition). Bank of filters # Convolutional networks often use banks of filters to perform different operations (such as edge detection with various orientations). It is possible to specify this mode of functioning by using the connect_filters() method: pre = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 , 4 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) bank_filters = np . array ( [ [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ] ], [ [ - 1.0 , 0.0 , 1.0 ], [ - 1.0 , 0.0 , 1.0 ], [ - 1.0 , 0.0 , 1.0 ] ], [ [ - 1.0 , - 1.0 , - 1.0 ], [ 0.0 , 0.0 , 0.0 ], [ 1.0 , 1.0 , 1.0 ] ], [ [ 1.0 , 1.0 , 1.0 ], [ 0.0 , 0.0 , 0.0 ], [ - 1.0 , - 1.0 , - 1.0 ] ] ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filters ( weights = bank_filters ) Here the filter has three dimensions. The first one must correspond to each filter. The last dimension of the post-synaptic population must correspond to the total number of filters. It cannot be combined with keep_last_dimension . Note Current limitation : Each filter must have the same size, it is not possible yet to convolve over multiple scales. Pooling # Another form of atypical projection for a neural network is the pooling operation. In max-pooling, each post-synaptic neuron is associated to a region of the pre-synaptic population and responds like the maximum firing rate in this region. This is already possible by defining the operation argument of the synapse type, but it would use instantiated synapses, what would be a waste of memory. The Pooling class allows to define such an operation without defining any synapse: pre = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Pooling ( pre = pre , post = post , target = 'exc' , operation = 'max' ) proj . connect_pooling () The pooling region of a post-synaptic region is automatically determined by comparing the dimensions of the two populations: here each post-synaptic neuron will cover an area of 2*2 neurons. If the number of dimensions do not match, you have to specify the extent argument to pooling() . For example, you can pool completely over one dimension of the pre-synaptic population: pre = Population ( geometry = ( 100 , 100 , 10 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Pooling ( pre = pre , post = post , target = 'exc' , operation = 'max' ) proj . connect_pooling ( extent = ( 2 , 2 , 10 )) Sharing weights with another projection # A different possibility to share weights is between two projections. If your network is composed of populations of the same size, and the projection patterns are identical, it could save some memory to \"share\" the weights of one projection with another, so they are created only once. To this end, you can use the Copy class and pass it an existing projection: pop1 = Population ( geometry = ( 30 , 30 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) pop2 = Population ( geometry = ( 20 , 20 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) pop3 = Population ( geometry = ( 20 , 20 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj1 = Projection ( pop1 , pop2 , 'exc' ) proj1 . connect_gaussian ( amp = 1.0 , sigma = 0.3 , delays = 2.0 ) proj2 = Copy ( pop1 , pop3 , 'exc' ) proj2 . connect_copy ( proj1 ) This only works when the pre- and post-populations of each projection have the same geometry, but they can be different, of course. If the original projection is learnable, the copied projection will see the changes. However, it is not possible for the shared projection to learn on its own. Copy only accepts psp and operation as parameters, which can be different from the original projection. It is only possible to copy regular projections, not other shared projections. The transmission delays will be identical between the two projections.","title":"Convolution and pooling"},{"location":"manual/ConvolutionalNetworks/#convolution-and-pooling","text":"Projections use by default a set of weights per post-synaptic neuron. Some networks, including convolutional networks, define a single operation (convolution or pooling) to be applied systematically on all pre-synaptic neurons. In such cases, it would be a waste of resources to allocate weights for each post-synaptic neuron. The extension convolution (see its API ) allows to implement such specific projections. It has to be imported explicitly at the beginning of the script: from ANNarchy import * from ANNarchy.extensions.convolution import * Warning Shared weights are only implemented for rate-coded networks. The only possible backend is currently OpenMP, CUDA will be implemented later.","title":"Convolution and pooling"},{"location":"manual/ConvolutionalNetworks/#simple-convolutions","text":"The simplest case of convolution is when the pre- and post-synaptic population have the same number of dimensions, for example: pre = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) Contrary to normal projections, the geometry of the populations (number of dimensions and neurons in each dimension) has a great influence on the operation to be performed. In particular the number of dimensions will define how the convolution will be applied. If for example the pre-synaptic population represents an 2D image, you may want to apply a vertical edge detector to it and get the result in the post-synaptic population. Such a filter can be defined by the following Numpy array: vertical_filter = np . array ( [ [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ] ] ) With 2 dimensions, the convolution operation (or more exactly, cross-correlation ) with a 3*3 filter is defined for all neurons in the post-synaptic population by: \\[\\text{post}[i, j] = \\sum_{c_i=-1}^1 \\sum_{c_j=-1}^1 \\text{filter}[c_i][c_j] \\cdot \\text{pre}[i + c_i, j + c_j]\\] Such a convolution is achieved by creating a Convolution object and using the connect_filter() method to create the connection pattern: proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = vertical_filter ) Each neuron of the post-synaptic population will then receive in sum('exc') (or whatever target name is used) the convolution between the kernel and a sub-region of the pre-syanptic population. ANNarchy defines the convolution operation for populations having 1, 2, 3, or 4 dimensions. Several options can be passed to the convolve() method: padding defines the value of the pre-synaptic firing rates which will be used when the coordinates are out-of-bounds. By default zero-padding is used, but you can specify another value with this argument. You can also use the 'border' value to repeat the firing rate of the neurons on the border (for example, if the filter tries to reach a neuron of coordinates (-1, -1), the firing rate of the neuron (0, 0) will be used instead). subsampling . In convolutional networks, the convolution operation is often coupled with a reduction in the number of neurons in each dimension. In the example above, the post-synaptic population could be defined with a geometry (50, 50). For each post-synaptic neuron, the coordinates of the center of the applied kernel would be automatically shifted from two pre-synaptic neurons compared to the previous one. However, if the number of neurons in one dimension of the pre-synaptic population is not exactly a multiple of the number of post-synaptic neurons in the same dimension, ANNarchy can not guess what the correct correspondance should be. In this case, you have to specify this mapping by providing to the subsampling argument a list of pre-synaptic coordinates defining the position of the center of the kernel for each post-synaptic neuron. The list is indexed by the rank of the post-synaptic neurons (use the rank_from_coordinates() method) and must have the same size as the population. Each element should be a list of coordinates in the pre-synaptic population's geometry (with as many elements as dimensions). It is possible to provide a Numpy array instead of a list of lists. One can access the coordinates in the pre-synaptic geometry of the center of the filter corresponding to a particular post-synaptic neuron by calling the center() method of Convolution with the rank or coordinates of the post neuron: pre = Population ( geometry = ( 100 , 100 ), neuron = Whatever ) post = Population ( geometry = ( 50 , 50 ), neuron = Whatever ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = vertical_filter ) pre_coordinates = proj . center ( 10 , 10 ) # returns (20, 20) In some cases, the post-synaptic population can have less dimensions than the pre-synaptic one. An example would be when the pre-synaptic population has three dimensions (e.g. (100, 100, 3)), the last representing the R, G and B components of an image. A 3D filter, with 3 components in the last dimension, would result in a (100, 100, 1) post-synaptic population (or any subsampling of it). ANNarchy accepts in this case the use of a 2D population (100, 100), but it will be checked that the number of elements in the last dimension of the filter equals the number of pre-synaptic neurons in the last dimension: pre = Population ( geometry = ( 100 , 100 , 3 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) red_filter = np . array ( [ [ [ 2.0 , - 1.0 , - 1.0 ] ] ] ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = red_filter )","title":"Simple convolutions"},{"location":"manual/ConvolutionalNetworks/#non-linear-convolutions","text":"Convolution uses by default a regular cross-correlation, summing w * pre.r over the extent of the kernel. As for regular synapses, you can change this behavior when creating the projection: the psp argument defines what will be summed. It is w*pre.r by default but can be changed to any combination of w and pre.r , such as w * log(1+pre.r) : proj = Convolution ( pre = pre , post = post , target = 'exc' , psp = 'w*log(1+pre.r)' ) the operation argument allows to change the summation operation. You can set it to 'max' (the maximum value of w*pre.r over the extent of the filter will be returned), 'min' (minimum) or 'mean' (same as 'sum', but normalized by the number of elements in the filter). The default is 'sum': proj = Convolution ( pre = pre , post = post , target = 'exc' , operation = 'max' )","title":"Non-linear convolutions"},{"location":"manual/ConvolutionalNetworks/#layer-wise-convolutions","text":"It is possible to define kernels with less dimensions than the pre-synaptic population. A 2D filter can for example be applied on each color component independently: pre = Population ( geometry = ( 100 , 100 , 3 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 , 3 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) vertical_filter = np . array ( [ [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ] ] ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = vertical_filter , keep_last_dimension = True ) The important parameter in this case is keep_last_dimension which tells the code generator that the last dimension of the input should not be used for convolution. The important constraint is that the post-synaptic population must have the same number of neurons in the last dimension than the pre-synaptic one (no subsampling is possible by definition).","title":"Layer-wise convolutions"},{"location":"manual/ConvolutionalNetworks/#bank-of-filters","text":"Convolutional networks often use banks of filters to perform different operations (such as edge detection with various orientations). It is possible to specify this mode of functioning by using the connect_filters() method: pre = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 , 4 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) bank_filters = np . array ( [ [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ] ], [ [ - 1.0 , 0.0 , 1.0 ], [ - 1.0 , 0.0 , 1.0 ], [ - 1.0 , 0.0 , 1.0 ] ], [ [ - 1.0 , - 1.0 , - 1.0 ], [ 0.0 , 0.0 , 0.0 ], [ 1.0 , 1.0 , 1.0 ] ], [ [ 1.0 , 1.0 , 1.0 ], [ 0.0 , 0.0 , 0.0 ], [ - 1.0 , - 1.0 , - 1.0 ] ] ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filters ( weights = bank_filters ) Here the filter has three dimensions. The first one must correspond to each filter. The last dimension of the post-synaptic population must correspond to the total number of filters. It cannot be combined with keep_last_dimension . Note Current limitation : Each filter must have the same size, it is not possible yet to convolve over multiple scales.","title":"Bank of filters"},{"location":"manual/ConvolutionalNetworks/#pooling","text":"Another form of atypical projection for a neural network is the pooling operation. In max-pooling, each post-synaptic neuron is associated to a region of the pre-synaptic population and responds like the maximum firing rate in this region. This is already possible by defining the operation argument of the synapse type, but it would use instantiated synapses, what would be a waste of memory. The Pooling class allows to define such an operation without defining any synapse: pre = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Pooling ( pre = pre , post = post , target = 'exc' , operation = 'max' ) proj . connect_pooling () The pooling region of a post-synaptic region is automatically determined by comparing the dimensions of the two populations: here each post-synaptic neuron will cover an area of 2*2 neurons. If the number of dimensions do not match, you have to specify the extent argument to pooling() . For example, you can pool completely over one dimension of the pre-synaptic population: pre = Population ( geometry = ( 100 , 100 , 10 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Pooling ( pre = pre , post = post , target = 'exc' , operation = 'max' ) proj . connect_pooling ( extent = ( 2 , 2 , 10 ))","title":"Pooling"},{"location":"manual/ConvolutionalNetworks/#sharing-weights-with-another-projection","text":"A different possibility to share weights is between two projections. If your network is composed of populations of the same size, and the projection patterns are identical, it could save some memory to \"share\" the weights of one projection with another, so they are created only once. To this end, you can use the Copy class and pass it an existing projection: pop1 = Population ( geometry = ( 30 , 30 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) pop2 = Population ( geometry = ( 20 , 20 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) pop3 = Population ( geometry = ( 20 , 20 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj1 = Projection ( pop1 , pop2 , 'exc' ) proj1 . connect_gaussian ( amp = 1.0 , sigma = 0.3 , delays = 2.0 ) proj2 = Copy ( pop1 , pop3 , 'exc' ) proj2 . connect_copy ( proj1 ) This only works when the pre- and post-populations of each projection have the same geometry, but they can be different, of course. If the original projection is learnable, the copied projection will see the changes. However, it is not possible for the shared projection to learn on its own. Copy only accepts psp and operation as parameters, which can be different from the original projection. It is only possible to copy regular projections, not other shared projections. The transmission delays will be identical between the two projections.","title":"Sharing weights with another projection"},{"location":"manual/Hybrid/","text":"Hybrid networks # ANNarchy has the possibility to simulate either rate-coded or spiking networks. It is therefore possible to define hybrid networks mixing rate-coded and spiking populations. A typical application would be to define a rate-coded network to process visual inputs, which is used to feed a spiking network for action selection. A dummy example is provided in examples/hybrid . Rate-coded to Spike # Converting a rate-coded population to a spiking network is straightforward. The PoissonPopulation (see its API ) defines a population of spiking neurons emitting spikes following a Poisson distribution: pop = PoissonPopulation ( 1000 , rates = 50. ) In this case, the 1000 neurons emit spikes at a rate of 50 Hz (the rate of individual neurons can be later modified). It is possible to use a weighted sum of rate-coded synapses in order to determine the firing rate of each Poisson neuron. It requires to connect a rate-coded population to the PoissonPopulation with a given target: pop1 = Population ( 4 , Neuron ( parameters = \"r=0.0\" )) pop2 = PoissonPopulation ( 1000 , target = 'exc' ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_number_pre ( weights = 10.0 , number = 1 ) In this example, each of the 4 pre-synaptic neurons \"controls\" the firing rate of one fourth (on average) of the post-synaptic ones. If target is used in the Poisson population, rates will be ignored. The weights determine the scaling of the transmission: a presynaptic rate r of 1.0 generates a firing rate of w Hz in the post-synaptic neurons. Here setting pop1.r = 1.0 will make the post-synaptic neurons fire at 10 Hz. Spike to Rate-coded # Decoding a spiking population is a harder process, because of the stochastic nature of spike trains. One can take advantage of the fact here that a rate-coded neuron usually represents an ensemble of spiking neurons, so the average firing rate in that ensemble can be more precisely decoded. In order to do so, one needs to connect the spiking population to a rate-coded one with a many-to-one pattern using a DecodingProjection . A DecodingProjection heritates all methods of Projection (including the connection methods) but performs the necessary conversion from spike trains to a instantaneous rate: pop1 = PoissonPopulation ( 1000 , rates = 50.0 ) pop2 = Population ( 1 , Neuron ( equations = \"r=sum(exc)\" )) proj = DecodingProjection ( pop1 , pop2 , 'exc' , window = 10.0 ) proj . connect_all_to_all ( weights = 1.0 ) In this example, the spiking population fires at 50 Hz. The single rate-coded neuron decoding that population will count how many spikes arrived in the last \\(T\\) milliseconds and divide it by the total number of synapses in order to estimate the population firing rate in pop1 . This would be accessed in sum(exc) (or whatever target is used in the projection). Because of its simple definition, it will therefore have its rate r at 50.0 (with some variance due to the stochastic nature of spike trains). The window argument defines the duration in milliseconds of the sliding temporal window used to estimate the firing rate. By default, it is equal to dt , which means spikes are counted in a very narrow period of time, what could lead to very big variations of the decoded firing rate. If the window is too big, it would introduce a noticeable lag for the decoded firing rate if the input varies too quickly. window = 10.0 is usually a good compromise, but this depends on the input firing rate. The weights of the projection define the scaling of the decoded firing rate. If one wants a firing rate of 100 Hz to be represented by r=1.0 , the weights should be set to 0.01. No Synapse model can be used in a DecodingProjection . Warning DecodingProjection is not implemented on CUDA yet.","title":"Hybrid networks"},{"location":"manual/Hybrid/#hybrid-networks","text":"ANNarchy has the possibility to simulate either rate-coded or spiking networks. It is therefore possible to define hybrid networks mixing rate-coded and spiking populations. A typical application would be to define a rate-coded network to process visual inputs, which is used to feed a spiking network for action selection. A dummy example is provided in examples/hybrid .","title":"Hybrid networks"},{"location":"manual/Hybrid/#rate-coded-to-spike","text":"Converting a rate-coded population to a spiking network is straightforward. The PoissonPopulation (see its API ) defines a population of spiking neurons emitting spikes following a Poisson distribution: pop = PoissonPopulation ( 1000 , rates = 50. ) In this case, the 1000 neurons emit spikes at a rate of 50 Hz (the rate of individual neurons can be later modified). It is possible to use a weighted sum of rate-coded synapses in order to determine the firing rate of each Poisson neuron. It requires to connect a rate-coded population to the PoissonPopulation with a given target: pop1 = Population ( 4 , Neuron ( parameters = \"r=0.0\" )) pop2 = PoissonPopulation ( 1000 , target = 'exc' ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_number_pre ( weights = 10.0 , number = 1 ) In this example, each of the 4 pre-synaptic neurons \"controls\" the firing rate of one fourth (on average) of the post-synaptic ones. If target is used in the Poisson population, rates will be ignored. The weights determine the scaling of the transmission: a presynaptic rate r of 1.0 generates a firing rate of w Hz in the post-synaptic neurons. Here setting pop1.r = 1.0 will make the post-synaptic neurons fire at 10 Hz.","title":"Rate-coded to Spike"},{"location":"manual/Hybrid/#spike-to-rate-coded","text":"Decoding a spiking population is a harder process, because of the stochastic nature of spike trains. One can take advantage of the fact here that a rate-coded neuron usually represents an ensemble of spiking neurons, so the average firing rate in that ensemble can be more precisely decoded. In order to do so, one needs to connect the spiking population to a rate-coded one with a many-to-one pattern using a DecodingProjection . A DecodingProjection heritates all methods of Projection (including the connection methods) but performs the necessary conversion from spike trains to a instantaneous rate: pop1 = PoissonPopulation ( 1000 , rates = 50.0 ) pop2 = Population ( 1 , Neuron ( equations = \"r=sum(exc)\" )) proj = DecodingProjection ( pop1 , pop2 , 'exc' , window = 10.0 ) proj . connect_all_to_all ( weights = 1.0 ) In this example, the spiking population fires at 50 Hz. The single rate-coded neuron decoding that population will count how many spikes arrived in the last \\(T\\) milliseconds and divide it by the total number of synapses in order to estimate the population firing rate in pop1 . This would be accessed in sum(exc) (or whatever target is used in the projection). Because of its simple definition, it will therefore have its rate r at 50.0 (with some variance due to the stochastic nature of spike trains). The window argument defines the duration in milliseconds of the sliding temporal window used to estimate the firing rate. By default, it is equal to dt , which means spikes are counted in a very narrow period of time, what could lead to very big variations of the decoded firing rate. If the window is too big, it would introduce a noticeable lag for the decoded firing rate if the input varies too quickly. window = 10.0 is usually a good compromise, but this depends on the input firing rate. The weights of the projection define the scaling of the decoded firing rate. If one wants a firing rate of 100 Hz to be represented by r=1.0 , the weights should be set to 0.01. No Synapse model can be used in a DecodingProjection . Warning DecodingProjection is not implemented on CUDA yet.","title":"Spike to Rate-coded"},{"location":"manual/Inputs/","text":"Setting inputs # The methods to set inputs to a network depends on whether it is rate-coded or spiking. Specific populations to set inputs are described in the API . Inputs to a rate-coded network # Standard method # The simplest way to define an input population is to use a dummy neuron which simply holds a firing rate r as parameter, and connect it to another population: input_pop = Population ( 10 , Neuron ( parameters = \"r=0.0\" )) pop = Population ( 10 , LeakyIntegrator ) proj = Projection ( input_pop , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) compile () simulate ( 100. ) input_pop . r = 1.0 simulate ( 100. ) The only thing you need to do is to manipulate the numpy array r holded by input_pop , and it will influence the \\\"real\\\" population pop It is important to define r as a parameter of the neuron, not a variable in equations . A variable sees its value updated at each step, so the value you set would be immediately forgotten. Note Using this method necessitates to interact with the input population in the Python script everytime you want to change the inputs. If the inputs change every time step, your simulation will alternate between Python and C++ executions and potentially become very slow. Timed Arrays # If the inputs change frequently, it may be more efficient to store all these values in a TimedArray (doc in the API ). Let's suppose you have a population of 10 neurons which should be activated sequentially over time. You can store the inputs to these neurons in a Numpy array, where the first axis corresponds to time and the second (or more) to the geometry of the population: inputs = np . array ( [ [ 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 ] ] ) inp = TimedArray ( rates = inputs ) pop = Population ( 10 , Neuron ( equations = \"r=sum(exc)\" )) proj = Projection ( inp , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) compile () simulate ( 10. ) With this code, each neuron will be activated in sequence at each time step ( dt=1.0 by default). If you simulate longer than 10 ms, the last input [0, 0, .., 1] will be kept forever. If the rates array has two dimensions, the corresponding population will be 1D. You can pass a multidimensional array to obtain a 2D or 3D population. Presenting a input for only one time step is very short, especially if the population pop uses ODEs to integrate the inputs. You can provide a schedule parameter to the TimedArray to define how long (in ms) an input should be presented: inp = TimedArray ( rates = inputs , schedule = 10. ) Here each input will be kept constant for 10 ms, so the 10 inputs will need 100 ms of simulation to be presented. If you do not want a regular schedule, you can also provide a list of times where inputs should be set: inp = TimedArray ( rates = inputs , schedule = [ 0. , 10. , 30. , 60. , 100. , 150. , 210. , 280. , 360. , 450. ]) The length of the schedule list should be equal or smaller to the number of inputs defined in rates . If this length is smaller (e.g. 7), only the 7 first inputs will be used as inputs. If the length is bigger, it will lead to an error. A TimedArray can be reset to iterate again over the inputs: inp = TimedArray ( rates = inputs , schedule = 10. ) ... compile () simulate ( 100. ) # The ten inputs are shown with a schedule of 10 ms inp . reset () simulate ( 100. ) # The same ten inputs are presented again. The times declared in schedule are therefore relative to the last call to reset() (or to t=0.0 at the beginning). If you want to systematically iterate over the inputs without iterating over simulate() and reset() , you can provide the period argument to the TimedArray to define how often the inputs will be reset: inp = TimedArray ( rates = inputs , schedule = 10. . period = 100. ) ... simulate ( 100000. ) If the period is smaller than the total durations of the inputs, the last inputs will be skipped. The rates , schedule and period can be modified after compilation. The only constraint is that the size of the population (defined in the rates array) must stay the same. Images and Videos # Images A simple utility to directly load an image into the firing rates r of a Population is provided by the ImagePopulation class. This class is not automatically imported with ANNarchy, you need to explicitly import it: from ANNarchy import * from ANNarchy.extensions.image import ImagePopulation inp = ImagePopulation ( geometry = ( 480 , 640 )) inp . set_image ( 'image.jpg' ) Using this class requires that you have the Python Image Library installed ( pip install Pillow ). Any image with a format supported by Pillow can be loaded, see the documentation . The ImagePopulation must be initialized with a geometry corresponding to the desired size of the population. If it differs from the resolution of the image (set with set_image ), the image will be first resized to match the geometry of the population. Note The size of an image is defined as (height, width), so a 640x480 image should be loaded in a (480, 640 population). If the geometry has only two dimensions (480, 640), each neuron will represent the luminance (or brightness) of the corresponding pixel. If the geometry has three dimensions (480, 640, 3), the color channels will additionally be represented (RGB). Any other value than 3 for the third dimension will generate an error. Note The firing rate r of a neuron is 1.0 when the corresponding pixel is white (value 255 as an unsigned integer on 8 bits). Note that the following code is functionally equivalent: from ANNarchy import * from PIL import Image inp = Population ( geometry = ( 480 , 640 ), Neuron ( parameters = \"r=0.0\" )) img = Image . open ( 'image.jpg' ) img = img . convert ( 'L' ) img = img . resize (( 480 , 640 )) / 255. inp . r = np . array ( img ) An example is provided in examples/image/Image.py . Videos The VideoPopulation class allows to retrieve images from a Webcam, using the OpenCV computer vision library, version 4.0 or later. pkg-config opencv4 --cflags --libs should not return an error. vtk might have to be additionally installed. from ANNarchy import * from ANNarchy.extensions.image import VideoPopulation inp = VideoPopulation ( geometry = ( 480 , 640 )) compile () inp . start_camera ( 0 ) while ( True ): inp . grab_image () simulate ( 10.0 ) A geometry must be provided as for ImagePopulations . The camera must be explicitly started after compile() with inp.start_camera(0) . 0 corresponds to the index of your camera, change it if you have multiple cameras. The VideoPopulation can then acquire frames from the camera with inp.grab_image() and store the correponding image in its firing rate r (also scaled between 0.0 and 1.0). An example is provided in examples/image/Webcam.py . Warning VideoPopulation is not available with the CUDA backend. Inputs to a spiking network # Standard method # To control the spiking patterns of a spiking population, the simplest way is to inject current into the corresponding membrane potentials. The built-in neuron types defined by ANNarchy have a i_offset variable that can be used for this purpose: from ANNarchy import * setup ( dt = 0.1 ) pop = Population ( 100 , Izhikevich ) pop . i_offset = np . linspace ( 0.0 , 30.0 , 100 ) m = Monitor ( pop , 'spike' ) compile () simulate ( 100. ) data = m . get ( 'spike' ) t , n = m . raster_plot ( data ) import matplotlib.pyplot as plt plt . plot ( t , n , '.' ) plt . ylim ( 0 , 100 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show () Current injection # If you want the injected current to be time-varying, you can design a rate-coded population of the same size as the spiking population and create a CurrentInjection projection between them: inp = Population(100, Neuron(equations=\"r = sin(t)\")) pop = Population(100, Izhikevich) proj = CurrentInjection(inp, pop, 'exc') proj.connect_current() The current g_exc of a neuron in pop will be set at each time step to the firing rate r of the corresponding neuron in inp (i.e. with the same rank). inp can also be defined as a TimedArray . The connector method should be connect_current() , which accepts no weight value and no delay. SpikeSourceArray # If you want to control precisely the spiking patterns used as inputs, you can provide a list of spike times to a SpikeSourceArray object: from ANNarchy import * setup ( dt = 0.1 ) spike_times = [ [ 10 + i / 10 , 20 + i / 10 , 30 + i / 10 , 40 + i / 10 , 50 + i / 10 , 60 + i / 10 , 70 + i / 10 , 80 + i / 10 , 90 + i / 10 ] for i in range ( 100 ) ] pop = SpikeSourceArray ( spike_times = spike_times ) m = Monitor ( pop , 'spike' ) compile () simulate ( 100. ) data = m . get ( 'spike' ) t , n = m . raster_plot ( data ) import matplotlib.pyplot as plt plt . plot ( t , n , '.' ) plt . ylim ( 0 , 100 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show () The spike_times argument must be a list of lists containing the spike times in ms. Its length defines the number of neurons in the population. It is not possible to define a geometry. If one neuron should not spike at all, just provide an empty list. The different neurons can have a different number of spikes. If you want to repeat the same stimulation, you can reset the SpikeSourceArray, what will set its internal time back to 0.0: simulate ( 100. ) pop . reset () simulate ( 100. ) The spikes times can be changed after compilation, bit it must have the same number of neurons: pop.spike_times = new_spike_times_array An example is provided in examples/pyNN/IF_curr_alpha.py . Warning SpikeSourceArray is not available with the CUDA backend. Poisson population # The PoissonPopulation class allows to create a population of identical spiking neurons, whose spiking patterns vary according to a Poisson distribution: from ANNarchy import * setup ( dt = 0.1 ) pop = PoissonPopulation ( 100 , rates = 30. ) m = Monitor ( pop , 'spike' ) compile () simulate ( 100. ) data = m . get ( 'spike' ) t , n = m . raster_plot ( data ) import matplotlib.pyplot as plt plt . plot ( t , n , '.' ) plt . ylim ( 0 , 100 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show () In this example, each of the 100 neurons fires randomly, with a mean firing rate of 30 Hz (next figure, top-left). It is also possible to specify the mean firing rate individually for each neuron (next figure, top-right): pop = PoissonPopulation ( 100 , rates = np . linspace ( 0.0 , 100.0 , 100 )) The rates attribute can be modified at any time during the simulation, as long as it has the same size as the population. Another possibility is to define a rule for the evolution of the mean firing rate in the population (next figure, bottom-left): pop = PoissonPopulation ( geometry = 100 , parameters = \"\"\" amp = 100.0 frequency = 50.0 \"\"\" , rates = \"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\" ) The rule can only depend on the time t : the corresponding mean firing rate is the same for all neurons in the population. Finally, the rates argument can be replaced by a target, so it can be computed by another rate-coded population (next figure, bottom-right): rates = 10. * np . ones (( 2 , 100 )) rates [ 0 , : 50 ] = 100. rates [ 1 , 50 :] = 100. inp = TimedArray ( rates = rates , schedule = 50. ) pop = PoissonPopulation ( 100 , target = \"exc\" ) proj = Projection ( inp , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) In the code above, we define a TimedArray for 100 neurons, so that half of the neurons fire at 100 Hz, while the others fire at 10 Hz. Every 50 ms, the two halves are swapped. We just need to create a projection with the target \\\"exc\\\" between the TimedArray and the PoissonPopulation (with a one-to-one pattern and weights 1.0 to preserve scaling), and the Poisson population will reflect the firing rates defined by the TimedArray. Homogeneous correlated inputs # HomogeneousCorrelatedSpikeTrains defines spiking neurons following a homogeneous distribution with correlated spike trains. The method describing the generation of homogeneous correlated spike trains is described in: Brette, R. (2009). Generation of correlated spike trains. http://audition.ens.fr/brette/papers/Brette2008NC.html The implementation is based on the one provided by Brian . To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation: \\[\\dfrac{dx}{dt} = \\dfrac{\\mu - x}{\\tau} + \\sigma \\, \\dfrac{\\xi }{\\sqrt{\\tau}}\\] where \\(\\xi\\) is a random variable. Basically, \\(x\\) will randomly vary around $\\mu\u00a7 over time, with an amplitude determined by \\(\\sigma\\) and a speed determined by \\(\\tau\\) . This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process. To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates , the desired correlation strength corr and the time constant tau . See Brette's paper for details. In short, you should only define the parameters rates , corr and tau , and let the class compute mu and sigma for you. Changing rates , corr or tau after initialization automatically recomputes mu and sigma. Example: from ANNarchy import * setup ( dt = 0.1 ) pop_poisson = PoissonPopulation ( 200 , rates = 10. ) pop_corr = HomogeneousCorrelatedSpikeTrains ( 200 , rates = 10. , corr = 0.3 , tau = 10. ) compile () simulate ( 1000. ) pop_poisson . rates = 30. pop_corr . rates = 30. simulate ( 1000. )","title":"Setting inputs"},{"location":"manual/Inputs/#setting-inputs","text":"The methods to set inputs to a network depends on whether it is rate-coded or spiking. Specific populations to set inputs are described in the API .","title":"Setting inputs"},{"location":"manual/Inputs/#inputs-to-a-rate-coded-network","text":"","title":"Inputs to a rate-coded network"},{"location":"manual/Inputs/#standard-method","text":"The simplest way to define an input population is to use a dummy neuron which simply holds a firing rate r as parameter, and connect it to another population: input_pop = Population ( 10 , Neuron ( parameters = \"r=0.0\" )) pop = Population ( 10 , LeakyIntegrator ) proj = Projection ( input_pop , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) compile () simulate ( 100. ) input_pop . r = 1.0 simulate ( 100. ) The only thing you need to do is to manipulate the numpy array r holded by input_pop , and it will influence the \\\"real\\\" population pop It is important to define r as a parameter of the neuron, not a variable in equations . A variable sees its value updated at each step, so the value you set would be immediately forgotten. Note Using this method necessitates to interact with the input population in the Python script everytime you want to change the inputs. If the inputs change every time step, your simulation will alternate between Python and C++ executions and potentially become very slow.","title":"Standard method"},{"location":"manual/Inputs/#timed-arrays","text":"If the inputs change frequently, it may be more efficient to store all these values in a TimedArray (doc in the API ). Let's suppose you have a population of 10 neurons which should be activated sequentially over time. You can store the inputs to these neurons in a Numpy array, where the first axis corresponds to time and the second (or more) to the geometry of the population: inputs = np . array ( [ [ 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 ] ] ) inp = TimedArray ( rates = inputs ) pop = Population ( 10 , Neuron ( equations = \"r=sum(exc)\" )) proj = Projection ( inp , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) compile () simulate ( 10. ) With this code, each neuron will be activated in sequence at each time step ( dt=1.0 by default). If you simulate longer than 10 ms, the last input [0, 0, .., 1] will be kept forever. If the rates array has two dimensions, the corresponding population will be 1D. You can pass a multidimensional array to obtain a 2D or 3D population. Presenting a input for only one time step is very short, especially if the population pop uses ODEs to integrate the inputs. You can provide a schedule parameter to the TimedArray to define how long (in ms) an input should be presented: inp = TimedArray ( rates = inputs , schedule = 10. ) Here each input will be kept constant for 10 ms, so the 10 inputs will need 100 ms of simulation to be presented. If you do not want a regular schedule, you can also provide a list of times where inputs should be set: inp = TimedArray ( rates = inputs , schedule = [ 0. , 10. , 30. , 60. , 100. , 150. , 210. , 280. , 360. , 450. ]) The length of the schedule list should be equal or smaller to the number of inputs defined in rates . If this length is smaller (e.g. 7), only the 7 first inputs will be used as inputs. If the length is bigger, it will lead to an error. A TimedArray can be reset to iterate again over the inputs: inp = TimedArray ( rates = inputs , schedule = 10. ) ... compile () simulate ( 100. ) # The ten inputs are shown with a schedule of 10 ms inp . reset () simulate ( 100. ) # The same ten inputs are presented again. The times declared in schedule are therefore relative to the last call to reset() (or to t=0.0 at the beginning). If you want to systematically iterate over the inputs without iterating over simulate() and reset() , you can provide the period argument to the TimedArray to define how often the inputs will be reset: inp = TimedArray ( rates = inputs , schedule = 10. . period = 100. ) ... simulate ( 100000. ) If the period is smaller than the total durations of the inputs, the last inputs will be skipped. The rates , schedule and period can be modified after compilation. The only constraint is that the size of the population (defined in the rates array) must stay the same.","title":"Timed Arrays"},{"location":"manual/Inputs/#images-and-videos","text":"Images A simple utility to directly load an image into the firing rates r of a Population is provided by the ImagePopulation class. This class is not automatically imported with ANNarchy, you need to explicitly import it: from ANNarchy import * from ANNarchy.extensions.image import ImagePopulation inp = ImagePopulation ( geometry = ( 480 , 640 )) inp . set_image ( 'image.jpg' ) Using this class requires that you have the Python Image Library installed ( pip install Pillow ). Any image with a format supported by Pillow can be loaded, see the documentation . The ImagePopulation must be initialized with a geometry corresponding to the desired size of the population. If it differs from the resolution of the image (set with set_image ), the image will be first resized to match the geometry of the population. Note The size of an image is defined as (height, width), so a 640x480 image should be loaded in a (480, 640 population). If the geometry has only two dimensions (480, 640), each neuron will represent the luminance (or brightness) of the corresponding pixel. If the geometry has three dimensions (480, 640, 3), the color channels will additionally be represented (RGB). Any other value than 3 for the third dimension will generate an error. Note The firing rate r of a neuron is 1.0 when the corresponding pixel is white (value 255 as an unsigned integer on 8 bits). Note that the following code is functionally equivalent: from ANNarchy import * from PIL import Image inp = Population ( geometry = ( 480 , 640 ), Neuron ( parameters = \"r=0.0\" )) img = Image . open ( 'image.jpg' ) img = img . convert ( 'L' ) img = img . resize (( 480 , 640 )) / 255. inp . r = np . array ( img ) An example is provided in examples/image/Image.py . Videos The VideoPopulation class allows to retrieve images from a Webcam, using the OpenCV computer vision library, version 4.0 or later. pkg-config opencv4 --cflags --libs should not return an error. vtk might have to be additionally installed. from ANNarchy import * from ANNarchy.extensions.image import VideoPopulation inp = VideoPopulation ( geometry = ( 480 , 640 )) compile () inp . start_camera ( 0 ) while ( True ): inp . grab_image () simulate ( 10.0 ) A geometry must be provided as for ImagePopulations . The camera must be explicitly started after compile() with inp.start_camera(0) . 0 corresponds to the index of your camera, change it if you have multiple cameras. The VideoPopulation can then acquire frames from the camera with inp.grab_image() and store the correponding image in its firing rate r (also scaled between 0.0 and 1.0). An example is provided in examples/image/Webcam.py . Warning VideoPopulation is not available with the CUDA backend.","title":"Images and Videos"},{"location":"manual/Inputs/#inputs-to-a-spiking-network","text":"","title":"Inputs to a spiking network"},{"location":"manual/Inputs/#standard-method_1","text":"To control the spiking patterns of a spiking population, the simplest way is to inject current into the corresponding membrane potentials. The built-in neuron types defined by ANNarchy have a i_offset variable that can be used for this purpose: from ANNarchy import * setup ( dt = 0.1 ) pop = Population ( 100 , Izhikevich ) pop . i_offset = np . linspace ( 0.0 , 30.0 , 100 ) m = Monitor ( pop , 'spike' ) compile () simulate ( 100. ) data = m . get ( 'spike' ) t , n = m . raster_plot ( data ) import matplotlib.pyplot as plt plt . plot ( t , n , '.' ) plt . ylim ( 0 , 100 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show ()","title":"Standard method"},{"location":"manual/Inputs/#current-injection","text":"If you want the injected current to be time-varying, you can design a rate-coded population of the same size as the spiking population and create a CurrentInjection projection between them: inp = Population(100, Neuron(equations=\"r = sin(t)\")) pop = Population(100, Izhikevich) proj = CurrentInjection(inp, pop, 'exc') proj.connect_current() The current g_exc of a neuron in pop will be set at each time step to the firing rate r of the corresponding neuron in inp (i.e. with the same rank). inp can also be defined as a TimedArray . The connector method should be connect_current() , which accepts no weight value and no delay.","title":"Current injection"},{"location":"manual/Inputs/#spikesourcearray","text":"If you want to control precisely the spiking patterns used as inputs, you can provide a list of spike times to a SpikeSourceArray object: from ANNarchy import * setup ( dt = 0.1 ) spike_times = [ [ 10 + i / 10 , 20 + i / 10 , 30 + i / 10 , 40 + i / 10 , 50 + i / 10 , 60 + i / 10 , 70 + i / 10 , 80 + i / 10 , 90 + i / 10 ] for i in range ( 100 ) ] pop = SpikeSourceArray ( spike_times = spike_times ) m = Monitor ( pop , 'spike' ) compile () simulate ( 100. ) data = m . get ( 'spike' ) t , n = m . raster_plot ( data ) import matplotlib.pyplot as plt plt . plot ( t , n , '.' ) plt . ylim ( 0 , 100 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show () The spike_times argument must be a list of lists containing the spike times in ms. Its length defines the number of neurons in the population. It is not possible to define a geometry. If one neuron should not spike at all, just provide an empty list. The different neurons can have a different number of spikes. If you want to repeat the same stimulation, you can reset the SpikeSourceArray, what will set its internal time back to 0.0: simulate ( 100. ) pop . reset () simulate ( 100. ) The spikes times can be changed after compilation, bit it must have the same number of neurons: pop.spike_times = new_spike_times_array An example is provided in examples/pyNN/IF_curr_alpha.py . Warning SpikeSourceArray is not available with the CUDA backend.","title":"SpikeSourceArray"},{"location":"manual/Inputs/#poisson-population","text":"The PoissonPopulation class allows to create a population of identical spiking neurons, whose spiking patterns vary according to a Poisson distribution: from ANNarchy import * setup ( dt = 0.1 ) pop = PoissonPopulation ( 100 , rates = 30. ) m = Monitor ( pop , 'spike' ) compile () simulate ( 100. ) data = m . get ( 'spike' ) t , n = m . raster_plot ( data ) import matplotlib.pyplot as plt plt . plot ( t , n , '.' ) plt . ylim ( 0 , 100 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show () In this example, each of the 100 neurons fires randomly, with a mean firing rate of 30 Hz (next figure, top-left). It is also possible to specify the mean firing rate individually for each neuron (next figure, top-right): pop = PoissonPopulation ( 100 , rates = np . linspace ( 0.0 , 100.0 , 100 )) The rates attribute can be modified at any time during the simulation, as long as it has the same size as the population. Another possibility is to define a rule for the evolution of the mean firing rate in the population (next figure, bottom-left): pop = PoissonPopulation ( geometry = 100 , parameters = \"\"\" amp = 100.0 frequency = 50.0 \"\"\" , rates = \"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\" ) The rule can only depend on the time t : the corresponding mean firing rate is the same for all neurons in the population. Finally, the rates argument can be replaced by a target, so it can be computed by another rate-coded population (next figure, bottom-right): rates = 10. * np . ones (( 2 , 100 )) rates [ 0 , : 50 ] = 100. rates [ 1 , 50 :] = 100. inp = TimedArray ( rates = rates , schedule = 50. ) pop = PoissonPopulation ( 100 , target = \"exc\" ) proj = Projection ( inp , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) In the code above, we define a TimedArray for 100 neurons, so that half of the neurons fire at 100 Hz, while the others fire at 10 Hz. Every 50 ms, the two halves are swapped. We just need to create a projection with the target \\\"exc\\\" between the TimedArray and the PoissonPopulation (with a one-to-one pattern and weights 1.0 to preserve scaling), and the Poisson population will reflect the firing rates defined by the TimedArray.","title":"Poisson population"},{"location":"manual/Inputs/#homogeneous-correlated-inputs","text":"HomogeneousCorrelatedSpikeTrains defines spiking neurons following a homogeneous distribution with correlated spike trains. The method describing the generation of homogeneous correlated spike trains is described in: Brette, R. (2009). Generation of correlated spike trains. http://audition.ens.fr/brette/papers/Brette2008NC.html The implementation is based on the one provided by Brian . To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation: \\[\\dfrac{dx}{dt} = \\dfrac{\\mu - x}{\\tau} + \\sigma \\, \\dfrac{\\xi }{\\sqrt{\\tau}}\\] where \\(\\xi\\) is a random variable. Basically, \\(x\\) will randomly vary around $\\mu\u00a7 over time, with an amplitude determined by \\(\\sigma\\) and a speed determined by \\(\\tau\\) . This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process. To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates , the desired correlation strength corr and the time constant tau . See Brette's paper for details. In short, you should only define the parameters rates , corr and tau , and let the class compute mu and sigma for you. Changing rates , corr or tau after initialization automatically recomputes mu and sigma. Example: from ANNarchy import * setup ( dt = 0.1 ) pop_poisson = PoissonPopulation ( 200 , rates = 10. ) pop_corr = HomogeneousCorrelatedSpikeTrains ( 200 , rates = 10. , corr = 0.3 , tau = 10. ) compile () simulate ( 1000. ) pop_poisson . rates = 30. pop_corr . rates = 30. simulate ( 1000. )","title":"Homogeneous correlated inputs"},{"location":"manual/Logging/","text":"Logging with tensorboard # The tensorboard extension allows to visualize ANNarchy simulations using tensorboard. It requires the tensorboardX package: pip install tensorboardX as well as tensorboard, of course: pip install tensorboard The Logger class is a thin wrapper around tensorboardX.SummaryWriter , which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io . Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch. The extension has to be imported explicitly: from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger For detailed examples of how to use the extension, refer to the examples Basal Ganglia and Bayesian optimization , which are available as notebooks in the folder examples/tensorboard . Creating the logger # The Logger class has to be closed properly at the end of the script, so it is advised to use a context: with Logger () as logger : logger . add_scalar ( \"Accuracy\" , acc , trial ) You can also make sure to close it: logger = Logger () logger . add_scalar ( \"Accuracy\" , acc , trial ) logger . close () By default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine . You can control these two elements by passing arguments to Logger() : with Logger ( logdir = \"/tmp/annarchy\" , experiment = \"trial1\" ): # logs in /tmp/annarchy/trial1 Launching tensorboard # After (or while) logging data within your simulation, run tensorboard in the terminal by specifying the path to the log directory: tensorboard --logdir runs You will then be asked to open localhost:6006 in your browser and will see a page similar to this: The information logged will be available in the different tabs (scalars, images...). You can also visualize and compare several experiments at the same time. Logging scalars # The add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc. The simplest information to log is a scalar, for example the accuracy at the end of a trial: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) accuracy = ... logger . add_scalar ( \"Accuracy\" , accuracy , trial ) A tag should be given for each plot as the first argument. In the example above, the figure with the accuracy will be labelled \\\"Accuracy\\\" in tensorboard. You can also group plots together with 2-levels tags such as: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) train_accuracy = ... test_accuracy = ... logger . add_scalar ( \"Accuracy/Train\" , train_accuracy , trial ) logger . add_scalar ( \"Accuracy/Test\" , test_accuracy , trial ) The second argument is the scalar, obviously. The third is the index of x-axis of the plot. It can be the index of the trial, the current time or whatever you prefer. If you want to display several scalars on the same plot, you can use the method add_scalars() and provide a dictionary: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) train_accuracy = ... test_accuracy = ... logger . add_scalars ( \"Accuracy\" , { 'train' : train_accuracy , 'test' : test_accuracy }, trial ) Logging images # It is also possible to log images, for example an input image or the firing rate of a 2D population, with the add_image() method: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) img = pop . r . reshape (( 10 , 10 )) logger . add_image ( \"Population/Firing rate\" , img , trial ) The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values must be floats between 0 and 1 or integers between 0 and 255 in order to be displayed correctly. You can either do it yourself, or pass equalize=True to the add_image() : logger . add_image ( \"Population/Firing rate\" , img , trial , equalize = True ) The min/max values in the array are internally used to rescale the image: img = ( img - img . min ()) / ( img . max () - img . min ()) To display several images together, for example the receptive fields of a population, an array of size (number, height, width) or (number, height, width, 3) can be passed to add_images() , where number is the number of images to display: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . reshape ( 100 , 10 , 10 ) # 100 post neurons, 10*10 pre neurons logger . add_images ( \"Projection/Receptive fields\" , weights , trial , equalize = True ) equalize=True applies the same scaling to all images, but you additionally pass equalize_per_image=True to have indepent scalings per image. Logging histograms # Histograms can also be logged, for example to visualize the statistics of weights in a projection: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . flatten () logger . add_histogram ( \"Weight distribution\" , weights , trial ) Logging figures # Matplotlib figures can also be logged: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) fig = plt . figure () plt . plot ( pop . r ) logger . add_figure ( \"Activity\" , fig , trial ) add_figure() will automatically close the figure, no need to call show() . Beware that this is very slow and requires a lot of space. Logging parameters # The previous methods can be called multiple times during a simulation, in order to visualize the changes during learning. add_parameters() is more useful in the context of hyperparameter optimization, where the same network with different parameters is run multiple times. Only once per simulation, typically at the end, you can log the value of some important parameters together with some metrics such as accuracy, error rate or so. This will allow tensorboard to display over multiple runs the relation between the parameters and the metrics in the tab \"HPARAMS\": with Logger () as logger : # ... logger . add_parameters ({ 'learning_rate' : lr , 'tau' : tau }, { 'accuracy' : accuracy }) Refer to Bayesian optimization for an example using the hyperopt library.","title":"Logging with tensorboard"},{"location":"manual/Logging/#logging-with-tensorboard","text":"The tensorboard extension allows to visualize ANNarchy simulations using tensorboard. It requires the tensorboardX package: pip install tensorboardX as well as tensorboard, of course: pip install tensorboard The Logger class is a thin wrapper around tensorboardX.SummaryWriter , which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io . Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch. The extension has to be imported explicitly: from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger For detailed examples of how to use the extension, refer to the examples Basal Ganglia and Bayesian optimization , which are available as notebooks in the folder examples/tensorboard .","title":"Logging with tensorboard"},{"location":"manual/Logging/#creating-the-logger","text":"The Logger class has to be closed properly at the end of the script, so it is advised to use a context: with Logger () as logger : logger . add_scalar ( \"Accuracy\" , acc , trial ) You can also make sure to close it: logger = Logger () logger . add_scalar ( \"Accuracy\" , acc , trial ) logger . close () By default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine . You can control these two elements by passing arguments to Logger() : with Logger ( logdir = \"/tmp/annarchy\" , experiment = \"trial1\" ): # logs in /tmp/annarchy/trial1","title":"Creating the logger"},{"location":"manual/Logging/#launching-tensorboard","text":"After (or while) logging data within your simulation, run tensorboard in the terminal by specifying the path to the log directory: tensorboard --logdir runs You will then be asked to open localhost:6006 in your browser and will see a page similar to this: The information logged will be available in the different tabs (scalars, images...). You can also visualize and compare several experiments at the same time.","title":"Launching tensorboard"},{"location":"manual/Logging/#logging-scalars","text":"The add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc. The simplest information to log is a scalar, for example the accuracy at the end of a trial: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) accuracy = ... logger . add_scalar ( \"Accuracy\" , accuracy , trial ) A tag should be given for each plot as the first argument. In the example above, the figure with the accuracy will be labelled \\\"Accuracy\\\" in tensorboard. You can also group plots together with 2-levels tags such as: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) train_accuracy = ... test_accuracy = ... logger . add_scalar ( \"Accuracy/Train\" , train_accuracy , trial ) logger . add_scalar ( \"Accuracy/Test\" , test_accuracy , trial ) The second argument is the scalar, obviously. The third is the index of x-axis of the plot. It can be the index of the trial, the current time or whatever you prefer. If you want to display several scalars on the same plot, you can use the method add_scalars() and provide a dictionary: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) train_accuracy = ... test_accuracy = ... logger . add_scalars ( \"Accuracy\" , { 'train' : train_accuracy , 'test' : test_accuracy }, trial )","title":"Logging scalars"},{"location":"manual/Logging/#logging-images","text":"It is also possible to log images, for example an input image or the firing rate of a 2D population, with the add_image() method: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) img = pop . r . reshape (( 10 , 10 )) logger . add_image ( \"Population/Firing rate\" , img , trial ) The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values must be floats between 0 and 1 or integers between 0 and 255 in order to be displayed correctly. You can either do it yourself, or pass equalize=True to the add_image() : logger . add_image ( \"Population/Firing rate\" , img , trial , equalize = True ) The min/max values in the array are internally used to rescale the image: img = ( img - img . min ()) / ( img . max () - img . min ()) To display several images together, for example the receptive fields of a population, an array of size (number, height, width) or (number, height, width, 3) can be passed to add_images() , where number is the number of images to display: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . reshape ( 100 , 10 , 10 ) # 100 post neurons, 10*10 pre neurons logger . add_images ( \"Projection/Receptive fields\" , weights , trial , equalize = True ) equalize=True applies the same scaling to all images, but you additionally pass equalize_per_image=True to have indepent scalings per image.","title":"Logging images"},{"location":"manual/Logging/#logging-histograms","text":"Histograms can also be logged, for example to visualize the statistics of weights in a projection: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . flatten () logger . add_histogram ( \"Weight distribution\" , weights , trial )","title":"Logging histograms"},{"location":"manual/Logging/#logging-figures","text":"Matplotlib figures can also be logged: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) fig = plt . figure () plt . plot ( pop . r ) logger . add_figure ( \"Activity\" , fig , trial ) add_figure() will automatically close the figure, no need to call show() . Beware that this is very slow and requires a lot of space.","title":"Logging figures"},{"location":"manual/Logging/#logging-parameters","text":"The previous methods can be called multiple times during a simulation, in order to visualize the changes during learning. add_parameters() is more useful in the context of hyperparameter optimization, where the same network with different parameters is run multiple times. Only once per simulation, typically at the end, you can log the value of some important parameters together with some metrics such as accuracy, error rate or so. This will allow tensorboard to display over multiple runs the relation between the parameters and the metrics in the tab \"HPARAMS\": with Logger () as logger : # ... logger . add_parameters ({ 'learning_rate' : lr , 'tau' : tau }, { 'accuracy' : accuracy }) Refer to Bayesian optimization for an example using the hyperopt library.","title":"Logging parameters"},{"location":"manual/Network/","text":"Parallel simulations and networks # A typical ANNarchy script represents a single network of populations and projections. Most of the work in computational neuroscience consists in running the same network again and again, varying some free parameters each time, until the fit to the data is publishable. The reset() allows to return the network to its state before compilation, but this is particularly tedious to implement. In order to run different networks using the same script, the Network object can be used to make copies of existing objects (populations, projections and monitors) and simulate them in parallel. Let's suppose the following dummy network is defined: pop1 = PoissonPopulation ( 100 , rates = 10.0 ) pop2 = Population ( 100 , Izhikevich ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_probability ( weights = 5.0 , probability = 0.2 ) m = Monitor ( pop2 , 'spike' ) compile () One would like to compare the firing patterns in pop2 when: There is no input to pop2 . The Poisson input is at 10 Hz. The Poisson input is at 20 Hz. Warning Running multiple networks in parallel is not supported on CUDA. Parallel simulation # parallel_run # The most simple method is to create a single network and to iterate over some parameters values to run identical simulations multiple times using parallel_run() : pop1 = PoissonPopulation ( 100 , rates = 10.0 ) pop2 = Population ( 100 , Izhikevich ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_probability ( weights = 5.0 , probability = 0.2 ) m = Monitor ( pop2 , 'spike' ) compile () def simulation ( idx , net ): net . get ( pop1 ) . rates = 10. * idx net . simulate ( 1000. ) return net . get ( m ) . raster_plot () results = parallel_run ( method = simulation , number = 3 ) t1 , n1 = results [ 0 ] t2 , n2 = results [ 1 ] t3 , n3 = results [ 2 ] The simulation() method will be called over three copies of the network (in different processes). The first argument to this method must be an integer corresponding to the index of a network (here idx = [0, 1, 2] ), the second must be a Network object (class Network ../API/Network.md)). Populations, projections and monitors of a network must be accessed with: net . get ( pop1 ) net . get ( pop2 ) net . get ( proj ) net . get ( m ) Networks only work on copies of the corresponding objects at the time they are added to the network. It is no use to modify the rates parameter of pop1 after the network are created: the network is now independent. Only net.get(pop1).rates allows to change rates for the current simulation. Similarly, it is useless to read variables from the original objects when the networks are simulated: they would still have their original values. You do not have access on the internally-created networks after the simulation (they are in a separate memory space). The method must the data you want to analyze (here the raster plot) or write them to disk (in separate files). parallel_run() returns a list of the values returned by the passed method: results = parallel_run ( method = simulation , networks = [ net1 , net2 , net3 ]) t1 , n1 = results [ 0 ] t2 , n2 = results [ 1 ] t3 , n3 = results [ 2 ] If you initialize some variables randomly in the main network, for example: pop2 . v = - 60. + 10. * np . random . random ( 100 ) they will have the same value in all networks, they are not drawn again. You need to perform random initialization on each network inside the simulation method: def simulation ( idx , net ): net . get ( pop1 ) . rates = 10. * idx net . get ( pop2 ) . v = - 60. + 10. * np . random . random ( 100 ) net . simulate ( 1000. ) return net . get ( m ) . raster_plot () Oppositely, connection methods having a random components (e.g. connect_fixed_probability() or using weights=Uniform(0.0, 1.0) ) will be redrawn for each network. Global simulation methods should not be called directly, even with the net_id parameter. The Network class overrides them: net . step () net . simulate () net . simulate_until () net . reset () net . get_time () net . set_time ( t ) net . get_current_step () net . set_current_step ( t ) net . set_seed ( seed ) net . enable_learning () net . disable_learning () net . get_population ( name ) Passing additional arguments # The two first obligatory arguments of the simulation callback are idx , the index of the network in the simulation, and net , the network object. You can of course use other names, but these two arguments will be passed. idx can be used for example to access arrays of parameter values: rates = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 ] def simulation ( idx , net ): net . get ( pop1 ) . rates = rates [ idx ] ... results = parallel_run ( method = simulation , number = len ( rates )) Another option is to provide additional arguments to the simulation callback during the parallel_run() call: def simulation ( idx , net , rates ): net . get ( pop1 ) . rates = rates ... rates = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 ] results = parallel_run ( method = simulation , number = len ( rates ), rates = rates ) These additional arguments must be lists of the same size as the number of networks ( number or len(networks) ). You can use as many additional arguments as you want: def simulation ( idx , net , a , b , c , d ): ... results = parallel_run ( method = simulation , number = 10 , a =... , b =... , c =... , d =... ) In parallel_run() , the arguments can be passed in any order, but they must be named (e.g. , a=list(range(0)), , not , list(range(10)), ). Multiple network instances # One can also create three different Network objects to implement the three conditions: net1 = Network () net1 . add ([ pop2 , m ]) net1 . compile () The network is created empty, and the population pop2 as well as the attached monitor are added to it through the add() method. This method takes a list of objects (populations, projections and monitors). The network has then to be compiled by calling the compile() method specifically on the network. The network can be simulated independently by calling simulate() or simulate_until() on the network. The basic network, with inputs at 10 Hz, can be simulated directly using the normal methods, or copied into a new network: net2 = Network () net2 . add ([ pop1 , pop2 , proj , m ]) net2 . compile () Here, all defined objects are added to the network. It would be easier to pass the everything argument of the Network constructor as True , which has the same effect. We can use this for the third network: net3 = Network ( everything = True ) net3 . get ( pop1 ) . rates = 20.0 net3 . compile () Here, the population pop1 of the third network has to be accessed though the get() method. The data corresponding to pop1 will not be the same as for net3.get(pop1) , only the geometry and neuron models are the same. Once a network is compiled, it can be simulated (but it does not matter if the other networks are also compiled, including the \\\"original\\\" network): net1 . simulate ( 1000. ) net2 . simulate ( 1000. ) net3 . simulate ( 1000. ) Spike recordings have to be accessed per network, through the copies of the monitor m : t1 , n1 = net1 . get ( m ) . raster_plot () t2 , n2 = net2 . get ( m ) . raster_plot () t3 , n3 = net3 . get ( m ) . raster_plot () One can also call the parallel_run() method and pass it a list of networks instead of number : parallel_run ( method = simulation , networks = [ net1 , net2 , net3 ]) This will apply simulation() in parallel on the 3 networks, reducing the total computation time.","title":"Parallel simulations and networks"},{"location":"manual/Network/#parallel-simulations-and-networks","text":"A typical ANNarchy script represents a single network of populations and projections. Most of the work in computational neuroscience consists in running the same network again and again, varying some free parameters each time, until the fit to the data is publishable. The reset() allows to return the network to its state before compilation, but this is particularly tedious to implement. In order to run different networks using the same script, the Network object can be used to make copies of existing objects (populations, projections and monitors) and simulate them in parallel. Let's suppose the following dummy network is defined: pop1 = PoissonPopulation ( 100 , rates = 10.0 ) pop2 = Population ( 100 , Izhikevich ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_probability ( weights = 5.0 , probability = 0.2 ) m = Monitor ( pop2 , 'spike' ) compile () One would like to compare the firing patterns in pop2 when: There is no input to pop2 . The Poisson input is at 10 Hz. The Poisson input is at 20 Hz. Warning Running multiple networks in parallel is not supported on CUDA.","title":"Parallel simulations and networks"},{"location":"manual/Network/#parallel-simulation","text":"","title":"Parallel simulation"},{"location":"manual/Network/#parallel_run","text":"The most simple method is to create a single network and to iterate over some parameters values to run identical simulations multiple times using parallel_run() : pop1 = PoissonPopulation ( 100 , rates = 10.0 ) pop2 = Population ( 100 , Izhikevich ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_probability ( weights = 5.0 , probability = 0.2 ) m = Monitor ( pop2 , 'spike' ) compile () def simulation ( idx , net ): net . get ( pop1 ) . rates = 10. * idx net . simulate ( 1000. ) return net . get ( m ) . raster_plot () results = parallel_run ( method = simulation , number = 3 ) t1 , n1 = results [ 0 ] t2 , n2 = results [ 1 ] t3 , n3 = results [ 2 ] The simulation() method will be called over three copies of the network (in different processes). The first argument to this method must be an integer corresponding to the index of a network (here idx = [0, 1, 2] ), the second must be a Network object (class Network ../API/Network.md)). Populations, projections and monitors of a network must be accessed with: net . get ( pop1 ) net . get ( pop2 ) net . get ( proj ) net . get ( m ) Networks only work on copies of the corresponding objects at the time they are added to the network. It is no use to modify the rates parameter of pop1 after the network are created: the network is now independent. Only net.get(pop1).rates allows to change rates for the current simulation. Similarly, it is useless to read variables from the original objects when the networks are simulated: they would still have their original values. You do not have access on the internally-created networks after the simulation (they are in a separate memory space). The method must the data you want to analyze (here the raster plot) or write them to disk (in separate files). parallel_run() returns a list of the values returned by the passed method: results = parallel_run ( method = simulation , networks = [ net1 , net2 , net3 ]) t1 , n1 = results [ 0 ] t2 , n2 = results [ 1 ] t3 , n3 = results [ 2 ] If you initialize some variables randomly in the main network, for example: pop2 . v = - 60. + 10. * np . random . random ( 100 ) they will have the same value in all networks, they are not drawn again. You need to perform random initialization on each network inside the simulation method: def simulation ( idx , net ): net . get ( pop1 ) . rates = 10. * idx net . get ( pop2 ) . v = - 60. + 10. * np . random . random ( 100 ) net . simulate ( 1000. ) return net . get ( m ) . raster_plot () Oppositely, connection methods having a random components (e.g. connect_fixed_probability() or using weights=Uniform(0.0, 1.0) ) will be redrawn for each network. Global simulation methods should not be called directly, even with the net_id parameter. The Network class overrides them: net . step () net . simulate () net . simulate_until () net . reset () net . get_time () net . set_time ( t ) net . get_current_step () net . set_current_step ( t ) net . set_seed ( seed ) net . enable_learning () net . disable_learning () net . get_population ( name )","title":"parallel_run"},{"location":"manual/Network/#passing-additional-arguments","text":"The two first obligatory arguments of the simulation callback are idx , the index of the network in the simulation, and net , the network object. You can of course use other names, but these two arguments will be passed. idx can be used for example to access arrays of parameter values: rates = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 ] def simulation ( idx , net ): net . get ( pop1 ) . rates = rates [ idx ] ... results = parallel_run ( method = simulation , number = len ( rates )) Another option is to provide additional arguments to the simulation callback during the parallel_run() call: def simulation ( idx , net , rates ): net . get ( pop1 ) . rates = rates ... rates = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 ] results = parallel_run ( method = simulation , number = len ( rates ), rates = rates ) These additional arguments must be lists of the same size as the number of networks ( number or len(networks) ). You can use as many additional arguments as you want: def simulation ( idx , net , a , b , c , d ): ... results = parallel_run ( method = simulation , number = 10 , a =... , b =... , c =... , d =... ) In parallel_run() , the arguments can be passed in any order, but they must be named (e.g. , a=list(range(0)), , not , list(range(10)), ).","title":"Passing additional arguments"},{"location":"manual/Network/#multiple-network-instances","text":"One can also create three different Network objects to implement the three conditions: net1 = Network () net1 . add ([ pop2 , m ]) net1 . compile () The network is created empty, and the population pop2 as well as the attached monitor are added to it through the add() method. This method takes a list of objects (populations, projections and monitors). The network has then to be compiled by calling the compile() method specifically on the network. The network can be simulated independently by calling simulate() or simulate_until() on the network. The basic network, with inputs at 10 Hz, can be simulated directly using the normal methods, or copied into a new network: net2 = Network () net2 . add ([ pop1 , pop2 , proj , m ]) net2 . compile () Here, all defined objects are added to the network. It would be easier to pass the everything argument of the Network constructor as True , which has the same effect. We can use this for the third network: net3 = Network ( everything = True ) net3 . get ( pop1 ) . rates = 20.0 net3 . compile () Here, the population pop1 of the third network has to be accessed though the get() method. The data corresponding to pop1 will not be the same as for net3.get(pop1) , only the geometry and neuron models are the same. Once a network is compiled, it can be simulated (but it does not matter if the other networks are also compiled, including the \\\"original\\\" network): net1 . simulate ( 1000. ) net2 . simulate ( 1000. ) net3 . simulate ( 1000. ) Spike recordings have to be accessed per network, through the copies of the monitor m : t1 , n1 = net1 . get ( m ) . raster_plot () t2 , n2 = net2 . get ( m ) . raster_plot () t3 , n3 = net3 . get ( m ) . raster_plot () One can also call the parallel_run() method and pass it a list of networks instead of number : parallel_run ( method = simulation , networks = [ net1 , net2 , net3 ]) This will apply simulation() in parallel on the 3 networks, reducing the total computation time.","title":"Multiple network instances"},{"location":"manual/Notebooks/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Jupyter / IPython Notebooks # It is possible to use ANNarchy in Jupyter / IPython notebooks. Several examples are provided in the examples directory of the source distribution. There are nevertheless two things to take into account when re-running cells: ANNarchy uses global variables to store the populations and projections # Internally, there is a network manager that stores which population and projection has been declared. It is empty when starting a script, but it can know anything about the Jupyter cells. Here are the main consequences: If you re-run the line from ANNarchy import * , Python will not clear the stored populations and projections (by design, as ANNarchy is already cached) If you re-run a cell creating a population or projection, it will create a new population, not replace the previous one. If you create a new population / projection after a call to compile() in the current kernel, this will lead to an error, as the network is already compiled and new objects cannot (yet) be added on the fly. The solution to these problems is to call the clear() command right after importing ANNarchy. This deletes all created objects and puts ANNarchy in a \" clean \" state: from ANNarchy import * clear () ANNarchy 4.6 (4.6.9b) on linux (posix). When you change something to the parameters of your network, you can re-run this cell, as well as all cells defining your network. You will then be able to call compile() again. This is equivalent to a reset of your network. However, if the structure of your network changes (new populations, changed equations), this will have no effect because of the second issue. Python cannot dynamically reload modules # If you change something in the definition of the neurons or synapses (anything that usually provoke a recompilation), this will have NO effect in the notebook, even if the network is successfully recompiled. Python is unable to reload libraries dynamically at runtime ( ). The C++ core actually running the simulation is imported as a dynamic library, so the result of the recompilation will not be reloaded. The only solution is to restart the kernel.","title":"Notebooks"},{"location":"manual/Notebooks/#jupyter-ipython-notebooks","text":"It is possible to use ANNarchy in Jupyter / IPython notebooks. Several examples are provided in the examples directory of the source distribution. There are nevertheless two things to take into account when re-running cells:","title":"Jupyter / IPython Notebooks"},{"location":"manual/Notebooks/#annarchy-uses-global-variables-to-store-the-populations-and-projections","text":"Internally, there is a network manager that stores which population and projection has been declared. It is empty when starting a script, but it can know anything about the Jupyter cells. Here are the main consequences: If you re-run the line from ANNarchy import * , Python will not clear the stored populations and projections (by design, as ANNarchy is already cached) If you re-run a cell creating a population or projection, it will create a new population, not replace the previous one. If you create a new population / projection after a call to compile() in the current kernel, this will lead to an error, as the network is already compiled and new objects cannot (yet) be added on the fly. The solution to these problems is to call the clear() command right after importing ANNarchy. This deletes all created objects and puts ANNarchy in a \" clean \" state: from ANNarchy import * clear () ANNarchy 4.6 (4.6.9b) on linux (posix). When you change something to the parameters of your network, you can re-run this cell, as well as all cells defining your network. You will then be able to call compile() again. This is equivalent to a reset of your network. However, if the structure of your network changes (new populations, changed equations), this will have no effect because of the second issue.","title":"ANNarchy uses global variables to store the populations and projections"},{"location":"manual/Notebooks/#python-cannot-dynamically-reload-modules","text":"If you change something in the definition of the neurons or synapses (anything that usually provoke a recompilation), this will have NO effect in the notebook, even if the network is successfully recompiled. Python is unable to reload libraries dynamically at runtime ( ). The C++ core actually running the simulation is imported as a dynamic library, so the result of the recompilation will not be reloaded. The only solution is to restart the kernel.","title":"Python cannot dynamically reload modules"},{"location":"manual/NumericalMethods/","text":"Equations and numerical methods # Numerical methods # First-order ordinary differential equations (ODE) can be solved using different numerical methods. The method can be declared globally in the setup() call and used in all ODEs of the network: from ANNarchy import * setup ( method = 'exponential' ) or specified explicitely for each ODE by specifying a flag: equations = \"\"\" tau * dV/dt + V = A : init = 0.0, exponential \"\"\" If nothing is specified, the explicit Euler method will be used. Different numerical methods are available: Explicit Euler 'explicit' Implicit Euler 'implicit' Exponential Euler 'exponential' Midpoint 'midpoint' Event-driven 'event-driven' Each method has advantages/drawbacks in term of numerical error, stability and computational cost. To describe these methods, we will take the example of a system of two linear first-order ODEs: \\[\\frac{dx(t)}{dt} = f(x(t), y(t)) = a_x \\cdot x(t) + b_x \\cdot y(t) + c_x\\] \\[\\frac{dy(t)}{dt} = g(x(t), y(t)) = a_y \\cdot x(t) + b_y \\cdot y(t) + c_y\\] The objective of a numerical method is to approximate the value of \\(x\\) and \\(y\\) at time \\(t+h\\) based on its value at time \\(t\\) , where \\(h\\) is the discretization time step (noted dt in ANNarchy): \\[x(t + h) = F(x(t), y(t))\\] \\[y(t + h) = G(x(t), y(t))\\] At each step of the simulation, the new values for the variables are computed using this update rule and will be used for the following step. The derivative of each variable is usually approximated by: \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h}\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h}\\] The different numerical methods mostly differ in the time at which the functions \\(f\\) and \\(g\\) are evaluated. Explicit Euler method # The explicit (forward) Euler method computes the next value for the variables by estimating their derivative at time \\(t\\) : \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t), y(t))\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t), y(t))\\] so the solution is straightforward to obtain: \\[x(t+h) = x(t) + h \\cdot f(x(t), y(t))\\] \\[y(t+h) = y(t) + h \\cdot g(x(t), y(t))\\] Implicit Euler method # The implicit (backward) Euler method computes the next value for the variables by estimating their derivative at time \\(t + h\\) : \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t+h), y(t+h))\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t+h), y(t+h))\\] This leads to a system of equations which must be solved in order to find the update rule. With the linear equations defined above, we need to solve: \\[\\frac{x(t+h) - x(t)}{h} = a_x \\cdot x(t + h) + b_x \\cdot y(t + h) + c_x\\] \\[\\frac{y(t+h) - y(t)}{h} = a_y \\cdot x(t + h) + b_y \\cdot y(t + h) + c_y\\] what gives something like: \\[x(t+h) = x(t) - h \\cdot \\frac{ \\left(a_{x} x(t) + b_{x} y(t) + c_{x} + h \\left(- a_{x} b_{y} x(t) + a_{y} b_{x} x(t) + b_{x} c_{y} - b_{y} c_{x}\\right)\\right)}{h^{2} \\left(- a_{x} b_{y} + a_{y} b_{x}\\right) + h \\left(a_{x} + b_{y}\\right) - 1}\\] \\[y(t+h) = y(t) -h \\cdot \\frac{ a_{y} \\left(c_{x} h + x(t)\\right) + y(t) \\left(- a_{y} b_{x} h^{2} + \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)\\right) + \\left(a_{x} h - 1\\right) \\left(c_{y} h + y(t)\\right)}{a_{y} b_{x} h^{2} - \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)}\\] ANNarchy relies on Sympy to solve and simplify this system of equations and generate the update rule. Note This method is obviously much more computationally expensive than the explicit Euler method, although more stable. The midpoint method is a better trade-off between complexity and stability than the implicit Euler method. Exponential Euler # The exponential Euler method is particularly stable for single first-order linear equations, of the type: \\[\\tau(t) \\cdot \\frac{dx(t)}{dt} + x(t) = A(t)\\] The update rule is then given by: \\[x(t+h) = x(t) + (1 - \\exp(- \\frac{h}{\\tau(t)}) ) \\cdot (A(t) - x(t))\\] The difference with the explicit Euler method is the step size, which is an exponential function of the ratio \\(\\frac{\\tau}{h}\\) . The accurary of the exponential Euler method on linear first-order ODEs is close to perfect, compared to the other Euler methods. As it is an explicit method, systems of equations are solved very easily with the same rule. When the exponential method is used, ANNarchy first tries to reduce the ODE to its canonical form above (with the time constant being possibly dependent on time or inputs) and then generates the update rule accordingly. For example, the description: tau * dv/dt = (E - v) + g_exc * (Ee - v) + g_inh * (v - Ei) would be first transformed in: (1 + g_exc - g_inh) * dv/dt + v = (E + g_exc * Ee - g_inh * Ei) / (1 + g_exc - g_inh) before being transformed into an update rule, with \\(\\tau(t) = 1 + g_\\text{exc} - g_\\text{inh}\\) : \\[v(t+h) = v(t) + (1 - \\exp(- \\frac{h}{1 + g_\\text{exc} - g_\\text{inh}}) ) \\cdot (\\frac{E + g_\\text{exc} \\cdot E_e - g_\\text{inh} \\cdot E_i}{1 + g_\\text{exc} - g_\\text{inh}} - v(t))\\] The exponential method can only be applied to first-order linear ODEs. Any other form of ODE will be rejected by the parser. Important note: The step size \\(1 - \\exp(- \\frac{h}{\\tau(t)})\\) is computationally expensive because of the exponential function. If the time constant \\(\\tau\\) is a global parameter of the population or projection, ANNarchy can pre-compute the step size outside of the for loop over all neurons/synapses, which leads to huge increases in performance. The exponential method should therefore be reserved to first-order linear ODEs with the same time constant for all neurons/synapses: neuron = Neuron ( parameters = \"tau = 10. : population\" , equations = \"tau * dr/dt + r = sum(exc) : min=0.0, exponential\" ) Midpoint # The midpoint method is a Runge-Kutta method of order 2. It estimates the derivative in the middle of the interval \\(t + \\frac{h}{2}\\) . \\[k_x = f(x(t), y(t))\\] \\[k_y = g(x(t), y(t))\\] \\[x(t+h) = x(t) + h \\cdot f(x(t) + k_x \\cdot \\frac{h}{2}, y(t) + k_y \\cdot \\frac{h}{2})\\] \\[y(t+h) = y(t) + h \\cdot g(x(t) + k_x \\cdot \\frac{h}{2}, y(t) + k_y \\cdot \\frac{h}{2})\\] Event-driven # Event-driven integration is only available for spiking synapses with variables following linear first-order dynamics. Let's consider the following STDP synapse (see Spiking Synapse for explanations): STDP = Synapse ( parameters = \"\"\" tau_pre = 10.0 : postsynaptic tau_post = 10.0 : postsynaptic \"\"\" , equations = \"\"\" tau_pre * dApre/dt = - Apre : event-driven tau_post * dApost/dt = - Apost : event-driven \"\"\" , pre_spike = \"\"\" g_target += w Apre += cApre w = clip(w + Apost, 0.0 , 1.0) \"\"\" , post_spike = \"\"\" Apost += cApost w = clip(w + Apre, 0.0 , 1.0) \"\"\" ) The value of Apost and Apre is only needed when a pre- or post-synaptic spike occurs at the synapse, so there is no need to integrate the corresponding equations between two such events. First-order linear ODEs have the nice property that their analytical solution is easy to obtain. Let's consider an equation of the form: \\[\\tau \\frac{dv}{dt} = E - v\\] If \\(v\\) has the value \\(V_0\\) at time \\(t\\) , its value at time \\(t + \\Delta t\\) is given by: \\[v(t + \\Delta t) = V_0 \\cdot \\exp(-\\frac{\\Delta t}{\\tau})\\] Note If the synapse defines a psp argument (synaptic transmission is continuous), or if another continuous variable depends on the value of an event-driven one, it is not possible to use event-driven integration. Order of evaluation # The values of variables are stored in a single array in order to save some memory. Special care therefore has to be taken on whether the update of a variable depends on the value of another variable at the previous time step or in the same step. Systems of ODEs # Systems of ODEs are integrated concurrently, which means that the following system: tau*dv/dt = I - v - u tau*du/dt = v - u would be numerized using the explicit Euler method as: v[t+1] = v[t] + dt*(I - v[t] - u[t])/tau u[t+1] = u[t] + dt*(v[t] - u[t])/tau As we use a single array, the generated code is similar to: new_v = v + dt*(I - v - u)/tau new_u = u + dt*(v - u)/tau v = new_v u = new_u This way, we ensure that the interdependent ODEs use the correct value for the other variables. Assignments # When assignments ( = , += ...) are used in an equations field, the order of valuation is different: Assigments occurring before or after a system of ODEs are updated sequentially. Systems of ODEs are updated concurrently. Let's consider the following dummy equations: # Process the inputs Exc = some_function ( sum ( exc )) Inh = another_function ( sum ( inh )) I = Exc - Inh # ODE for the membrane potential, with a recovery variable tau * dv / dt = I - v - u tau * du / dt = v - u # Firing rate is the positive part of v r = pos ( v ) Here, Exc and Inh represent the inputs to the neuron at the current time t . The new values should be immediately available for updating I , whose value should similarly be immediately used in the ODE of v . Similarly, the value of r should be the positive part of the value of v that was just calculated, not at the previous time step. Doing otherwise would introduce a lag in the neuron: changes in sum(exc) at t would be reflected in Exc at t+1 , in I at t+2 , in v at t+3 and finally in r at t+4 . This is generally unwanted. The generated code is therefore equivalent to: # Process the inputs Exc = some_function ( sum ( exc )) Inh = another_function ( sum ( inh )) I = Exc - Inh # ODE for the membrane potential, with a recovery variable new_v = v + dt * ( I - v - u ) / tau new_u = u + dt * ( v - u ) / tau v = new_v u = new_u # Firing rate is the positive part of v r = pos ( v ) One can even define multiple groups of assignments and systems of ODEs: systems of ODEs separated by at least one assignment will be evaluated sequentially (but concurrently inside each system). For example, in: tau*du/dt = v - u I = g_exc - g_inh tau*dk/dt = v - k tau*dv/dt = I - v - u + k u and k are updated using the previous value of v , while v uses the new values of both I and u , but the previous one of k .","title":"Equations and numerical methods"},{"location":"manual/NumericalMethods/#equations-and-numerical-methods","text":"","title":"Equations and numerical methods"},{"location":"manual/NumericalMethods/#numerical-methods","text":"First-order ordinary differential equations (ODE) can be solved using different numerical methods. The method can be declared globally in the setup() call and used in all ODEs of the network: from ANNarchy import * setup ( method = 'exponential' ) or specified explicitely for each ODE by specifying a flag: equations = \"\"\" tau * dV/dt + V = A : init = 0.0, exponential \"\"\" If nothing is specified, the explicit Euler method will be used. Different numerical methods are available: Explicit Euler 'explicit' Implicit Euler 'implicit' Exponential Euler 'exponential' Midpoint 'midpoint' Event-driven 'event-driven' Each method has advantages/drawbacks in term of numerical error, stability and computational cost. To describe these methods, we will take the example of a system of two linear first-order ODEs: \\[\\frac{dx(t)}{dt} = f(x(t), y(t)) = a_x \\cdot x(t) + b_x \\cdot y(t) + c_x\\] \\[\\frac{dy(t)}{dt} = g(x(t), y(t)) = a_y \\cdot x(t) + b_y \\cdot y(t) + c_y\\] The objective of a numerical method is to approximate the value of \\(x\\) and \\(y\\) at time \\(t+h\\) based on its value at time \\(t\\) , where \\(h\\) is the discretization time step (noted dt in ANNarchy): \\[x(t + h) = F(x(t), y(t))\\] \\[y(t + h) = G(x(t), y(t))\\] At each step of the simulation, the new values for the variables are computed using this update rule and will be used for the following step. The derivative of each variable is usually approximated by: \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h}\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h}\\] The different numerical methods mostly differ in the time at which the functions \\(f\\) and \\(g\\) are evaluated.","title":"Numerical methods"},{"location":"manual/NumericalMethods/#explicit-euler-method","text":"The explicit (forward) Euler method computes the next value for the variables by estimating their derivative at time \\(t\\) : \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t), y(t))\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t), y(t))\\] so the solution is straightforward to obtain: \\[x(t+h) = x(t) + h \\cdot f(x(t), y(t))\\] \\[y(t+h) = y(t) + h \\cdot g(x(t), y(t))\\]","title":"Explicit Euler method"},{"location":"manual/NumericalMethods/#implicit-euler-method","text":"The implicit (backward) Euler method computes the next value for the variables by estimating their derivative at time \\(t + h\\) : \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t+h), y(t+h))\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t+h), y(t+h))\\] This leads to a system of equations which must be solved in order to find the update rule. With the linear equations defined above, we need to solve: \\[\\frac{x(t+h) - x(t)}{h} = a_x \\cdot x(t + h) + b_x \\cdot y(t + h) + c_x\\] \\[\\frac{y(t+h) - y(t)}{h} = a_y \\cdot x(t + h) + b_y \\cdot y(t + h) + c_y\\] what gives something like: \\[x(t+h) = x(t) - h \\cdot \\frac{ \\left(a_{x} x(t) + b_{x} y(t) + c_{x} + h \\left(- a_{x} b_{y} x(t) + a_{y} b_{x} x(t) + b_{x} c_{y} - b_{y} c_{x}\\right)\\right)}{h^{2} \\left(- a_{x} b_{y} + a_{y} b_{x}\\right) + h \\left(a_{x} + b_{y}\\right) - 1}\\] \\[y(t+h) = y(t) -h \\cdot \\frac{ a_{y} \\left(c_{x} h + x(t)\\right) + y(t) \\left(- a_{y} b_{x} h^{2} + \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)\\right) + \\left(a_{x} h - 1\\right) \\left(c_{y} h + y(t)\\right)}{a_{y} b_{x} h^{2} - \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)}\\] ANNarchy relies on Sympy to solve and simplify this system of equations and generate the update rule. Note This method is obviously much more computationally expensive than the explicit Euler method, although more stable. The midpoint method is a better trade-off between complexity and stability than the implicit Euler method.","title":"Implicit Euler method"},{"location":"manual/NumericalMethods/#exponential-euler","text":"The exponential Euler method is particularly stable for single first-order linear equations, of the type: \\[\\tau(t) \\cdot \\frac{dx(t)}{dt} + x(t) = A(t)\\] The update rule is then given by: \\[x(t+h) = x(t) + (1 - \\exp(- \\frac{h}{\\tau(t)}) ) \\cdot (A(t) - x(t))\\] The difference with the explicit Euler method is the step size, which is an exponential function of the ratio \\(\\frac{\\tau}{h}\\) . The accurary of the exponential Euler method on linear first-order ODEs is close to perfect, compared to the other Euler methods. As it is an explicit method, systems of equations are solved very easily with the same rule. When the exponential method is used, ANNarchy first tries to reduce the ODE to its canonical form above (with the time constant being possibly dependent on time or inputs) and then generates the update rule accordingly. For example, the description: tau * dv/dt = (E - v) + g_exc * (Ee - v) + g_inh * (v - Ei) would be first transformed in: (1 + g_exc - g_inh) * dv/dt + v = (E + g_exc * Ee - g_inh * Ei) / (1 + g_exc - g_inh) before being transformed into an update rule, with \\(\\tau(t) = 1 + g_\\text{exc} - g_\\text{inh}\\) : \\[v(t+h) = v(t) + (1 - \\exp(- \\frac{h}{1 + g_\\text{exc} - g_\\text{inh}}) ) \\cdot (\\frac{E + g_\\text{exc} \\cdot E_e - g_\\text{inh} \\cdot E_i}{1 + g_\\text{exc} - g_\\text{inh}} - v(t))\\] The exponential method can only be applied to first-order linear ODEs. Any other form of ODE will be rejected by the parser. Important note: The step size \\(1 - \\exp(- \\frac{h}{\\tau(t)})\\) is computationally expensive because of the exponential function. If the time constant \\(\\tau\\) is a global parameter of the population or projection, ANNarchy can pre-compute the step size outside of the for loop over all neurons/synapses, which leads to huge increases in performance. The exponential method should therefore be reserved to first-order linear ODEs with the same time constant for all neurons/synapses: neuron = Neuron ( parameters = \"tau = 10. : population\" , equations = \"tau * dr/dt + r = sum(exc) : min=0.0, exponential\" )","title":"Exponential Euler"},{"location":"manual/NumericalMethods/#midpoint","text":"The midpoint method is a Runge-Kutta method of order 2. It estimates the derivative in the middle of the interval \\(t + \\frac{h}{2}\\) . \\[k_x = f(x(t), y(t))\\] \\[k_y = g(x(t), y(t))\\] \\[x(t+h) = x(t) + h \\cdot f(x(t) + k_x \\cdot \\frac{h}{2}, y(t) + k_y \\cdot \\frac{h}{2})\\] \\[y(t+h) = y(t) + h \\cdot g(x(t) + k_x \\cdot \\frac{h}{2}, y(t) + k_y \\cdot \\frac{h}{2})\\]","title":"Midpoint"},{"location":"manual/NumericalMethods/#event-driven","text":"Event-driven integration is only available for spiking synapses with variables following linear first-order dynamics. Let's consider the following STDP synapse (see Spiking Synapse for explanations): STDP = Synapse ( parameters = \"\"\" tau_pre = 10.0 : postsynaptic tau_post = 10.0 : postsynaptic \"\"\" , equations = \"\"\" tau_pre * dApre/dt = - Apre : event-driven tau_post * dApost/dt = - Apost : event-driven \"\"\" , pre_spike = \"\"\" g_target += w Apre += cApre w = clip(w + Apost, 0.0 , 1.0) \"\"\" , post_spike = \"\"\" Apost += cApost w = clip(w + Apre, 0.0 , 1.0) \"\"\" ) The value of Apost and Apre is only needed when a pre- or post-synaptic spike occurs at the synapse, so there is no need to integrate the corresponding equations between two such events. First-order linear ODEs have the nice property that their analytical solution is easy to obtain. Let's consider an equation of the form: \\[\\tau \\frac{dv}{dt} = E - v\\] If \\(v\\) has the value \\(V_0\\) at time \\(t\\) , its value at time \\(t + \\Delta t\\) is given by: \\[v(t + \\Delta t) = V_0 \\cdot \\exp(-\\frac{\\Delta t}{\\tau})\\] Note If the synapse defines a psp argument (synaptic transmission is continuous), or if another continuous variable depends on the value of an event-driven one, it is not possible to use event-driven integration.","title":"Event-driven"},{"location":"manual/NumericalMethods/#order-of-evaluation","text":"The values of variables are stored in a single array in order to save some memory. Special care therefore has to be taken on whether the update of a variable depends on the value of another variable at the previous time step or in the same step.","title":"Order of evaluation"},{"location":"manual/NumericalMethods/#systems-of-odes","text":"Systems of ODEs are integrated concurrently, which means that the following system: tau*dv/dt = I - v - u tau*du/dt = v - u would be numerized using the explicit Euler method as: v[t+1] = v[t] + dt*(I - v[t] - u[t])/tau u[t+1] = u[t] + dt*(v[t] - u[t])/tau As we use a single array, the generated code is similar to: new_v = v + dt*(I - v - u)/tau new_u = u + dt*(v - u)/tau v = new_v u = new_u This way, we ensure that the interdependent ODEs use the correct value for the other variables.","title":"Systems of ODEs"},{"location":"manual/NumericalMethods/#assignments","text":"When assignments ( = , += ...) are used in an equations field, the order of valuation is different: Assigments occurring before or after a system of ODEs are updated sequentially. Systems of ODEs are updated concurrently. Let's consider the following dummy equations: # Process the inputs Exc = some_function ( sum ( exc )) Inh = another_function ( sum ( inh )) I = Exc - Inh # ODE for the membrane potential, with a recovery variable tau * dv / dt = I - v - u tau * du / dt = v - u # Firing rate is the positive part of v r = pos ( v ) Here, Exc and Inh represent the inputs to the neuron at the current time t . The new values should be immediately available for updating I , whose value should similarly be immediately used in the ODE of v . Similarly, the value of r should be the positive part of the value of v that was just calculated, not at the previous time step. Doing otherwise would introduce a lag in the neuron: changes in sum(exc) at t would be reflected in Exc at t+1 , in I at t+2 , in v at t+3 and finally in r at t+4 . This is generally unwanted. The generated code is therefore equivalent to: # Process the inputs Exc = some_function ( sum ( exc )) Inh = another_function ( sum ( inh )) I = Exc - Inh # ODE for the membrane potential, with a recovery variable new_v = v + dt * ( I - v - u ) / tau new_u = u + dt * ( v - u ) / tau v = new_v u = new_u # Firing rate is the positive part of v r = pos ( v ) One can even define multiple groups of assignments and systems of ODEs: systems of ODEs separated by at least one assignment will be evaluated sequentially (but concurrently inside each system). For example, in: tau*du/dt = v - u I = g_exc - g_inh tau*dk/dt = v - k tau*dv/dt = I - v - u + k u and k are updated using the previous value of v , while v uses the new values of both I and u , but the previous one of k .","title":"Assignments"},{"location":"manual/Parser/","text":"Parser # A Neuron or Synapse type is primarily defined by two sets of values which must be specified in its constructor: Parameters are values such as time constants which are constant during the simulation. They can be the same throughout the population/projection, or take different values. Variables are neuronal variables (for example the membrane potential or firing rate) or synaptic variables (the synaptic efficiency) whose value evolve with time during the simulation. The equation (whether it is an ordinary differential equation or not) ruling their evolution can be described using a specific meta-language. Parameters # Parameters are defined by a multi-string consisting of one or more parameter definitions: parameters = \"\"\" tau = 10.0 eta = 0.5 \"\"\" Each parameter should be defined on a single line, with its name on the left side of the equal sign, and its value on the right side. The given value corresponds to the initial value of the parameter (but it can be changed at any further point of the simulation). As a neuron/synapse type is likely to be reused in different populations/projections, it is good practice to set reasonable initial values in the neuron/synapse type, and eventually adapt them to the corresponding populations/projections later on. Local vs. global parameters By default, a neural parameter will be unique to each neuron (i.e. each neuron instance will hold a copy of the parameter) or synapse. In order to save memory space, one can force ANNarchy to store only one parameter value for a whole population by specifying the population flag after a : symbol following the parameter definition: parameters = \"\"\" tau = 10.0 eta = 0.5 : population \"\"\" In this case, there will be only only one instance of the eta parameter for the whole population. eta is called a global parameter, in opposition to local parameters which are the default. The same is true for synapses, whose parameters are by default unique to each synapse in a given projection. If the post-synaptic flag is passed, the parameter will be common to all synapses of a post-synaptic neuron, but can differ from one post-synaptic neuron to another. If the projection flag is passed, the parameter will be common to all synapses of a projection (e.g. the learning rate). Type of the variable Parameters have floating-point precision by default. If you want to force the parameter to be an integer or boolean, you can also pass the int and bool flags, separated by commas: parameters = \"\"\" tau = 10.0 eta = 1 : population, int \"\"\" Constants Alternatively, it is possible to use constants in the parameter definition (see later): tau_exc = Constant ( 'tau_exc' , 10.0 ) neuron = Neuron ( parameters = \"\"\" tau = tau_exc \"\"\" , ) The advantage of this method is that if a parameter value is \\\"shared\\\" across several neuron/synapse types, you only need to change the value once, instead of in each neuron/synapse definition. Variables # Time-varying variables are also defined using a multi-line description: equations = \"\"\" noise = Uniform(0.0, 0.2) tau * dmp/dt + mp = baseline + sum(exc) + noise r = pos(mp) \"\"\" The evolution of each variable with time can be described through a simple equation or an ordinary differential equation (ODE). ANNarchy provides a simple parser for mathematical expressions, whose role is to translate a high-level description of the equation into an optimized C++ code snippet. The equation for one variable can depend on parameters, other variables (even when declared later) or constants. Variables are updated in the same order as their declaration in the multistring (see Numerical methods , as it influences how ODEs are solved). The declaration of a single variable can extend on multiple lines: equations = \"\"\" noise = Uniform(0.0, 0.2) tau * dmp/dt = baseline - mp + sum(exc) + noise : max = 1.0 rate = pos(mp) \"\"\" As it is only a parser and not a solver, some limitations exist: Simple equations must hold only the name of the variable on the left sign of the equation. Variable definitions such as rate + mp = noise are forbidden, as it would be impossible to guess which variable should be updated. ODEs are more free regarding the left side, but only one variable should hold the gradient: the one which will be updated. The following definitions are equivalent and will lead to the same C++ code: tau * dmp / dt = baseline - mp tau * dmp / dt + mp = baseline tau * dmp / dt + mp - baseline = 0 dmp / dt = ( baseline - mp ) / tau In practice, ODEs are transformed using Sympy into the last form (only the gradient stays on the left) and numerized using the chosen numerical method (see Numerical methods ). Flags # Locality and type Like the parameters, variables also accept the population , postsynaptic and projection flags to define the local/global character of the variable, as well as the int or bool flags for their type. Initial value The initial value of the variable (before the first simulation starts) can also be specified using the init keyword followed by the desired value: equations = \"\"\" tau * dmp/dt + mp = baseline : init = 0.2 \"\"\" It must be a single value (the same for all neurons in the population or all synapses in the projection) and should not depend on other parameters and variables. This initial value can be specifically changed after the Population or Projection objects are created (see Populations ). It is also possible to use constants for the initial value: init_mp = Constant ( 'init_mp' , 0.2 ) neuron = Neuron ( equations = \"\"\" tau * dmp/dt + mp = baseline : init = init_mp \"\"\" , ) Min and Max values of a variable Upper- and lower-bounds can be set using the min and max keywords: equations = \"\"\" tau * dmp/dt + mp = baseline : min = -0.2, max = 1.0 \"\"\" At each step of the simulation, after the update rule is calculated for mp , the new value will be compared to the min and max value, and clamped if necessary. min and max can be single values, constants, parameters, variables or functions of all these: parameters = \"\"\" tau = 10.0 min_mp = -1.0 : population max_mp = 1.0 \"\"\" , equations = \"\"\" variance = Uniform(0.0, 1.0) tau * dmp/dt + mp = sum(exc) : min = min_mp, max = max_mp + variance r = mp : min = 0.0 # Equivalent to r = pos(mp) \"\"\" Numerical method The numerization method for a single ODEs can be explicitely set by specifying a flag: tau * dmp/dt + mp = sum(exc) : exponential The available numerical methods are described in Numerical methods . Summary of allowed flags for variables: init : defines the initialization value at begin of simulation and after a network reset (default: 0.0) min : minimum allowed value (unset by default) max : maximum allowed value (unset by default) population : the attribute is shared by all neurons of a population. postsynaptic : the attribute is shared by all synapses of a post-synaptic neuron. projection : the attribute is shared by all synapses of a projection. explicit , implicit , exponential , midpoint , event-driven : the numerical method to be used. Constants # Global constants can be created by the user and used inside any equation. They must define an unique name and a floating point value: tau = Constant ( 'tau' , 10.0 ) neuron = Neuron ( equations = \"tau * dr/dt + r = sum(exc)\" ) In this example, a Neuron or Synapse does not have to define the parameter tau to use it: it is available everywhere. If the Neuron/Synapse redefines a parameter called tau , the constant is not visible anymore to that object. Constants can be manipulated as normal floats to define complex values: tau = Constant ( 'tau' , 20 ) factor = Constant ( 'factor' , 0.1 ) real_tau = Constant ( 'real_tau' , tau * factor ) neuron = Neuron ( equations = ''' real_tau*dr/dt + r =1.0 ''' ) Note that constants are only global, changing their value impacts all objects using them. Changing the value of a constant can only be done through the set() method (before or after compile() ): tau = Constant ( 'tau' , 20 ) tau . set ( 10.0 ) Allowed vocabulary # The mathematical parser relies heavily on the one provided by SymPy . Numerical values # All parameters and variables use implicitly the floating-point double precision, except when stated otherwise with the int or bool keywords. You can use numerical constants within the equation, noting that they will be automatically converted to this precision: tau * dmp / dt = 1 / pos ( mp ) + 1 The constant \\(\\pi\\) is available under the literal form pi . Operators # Additions (+), substractions (-), multiplications (*), divisions (/) and power functions (\\^) are of course allowed. Gradients are allowed only for the variable currently described. They take the form: dmp / dt = A with a d preceding the variable's name and terminated by /dt (with or without spaces). Gradients must be on the left side of the equation. To update the value of a variable at each time step, the operators = , += , -= , *= , and /= are allowed. Parameters and Variables # Any parameter or variable defined in the same Neuron/Synapse can be used inside an equation. User-defined constants can also be used. Additionally, the following variables are pre-defined: dt : the discretization time step for the simulation. Using this variable, you can define the numerical method by yourself. For example: tau * dmp / dt + mp = baseline with backward Euler would be equivalent to: mp += dt / tau * ( baseline - mp ) t : the time in milliseconds elapsed since the creation of the network. This allows to generate oscillating variables: f = 10.0 # Frequency of 10 Hz phi = pi / 4 # Phase ts = t / 1000.0 # ts is in seconds r = 10.0 * ( sin ( 2 * pi * f * ts + phi ) + 1.0 ) Random number generators # Several random generators are available and can be used within an equation. In the current version are for example available: Uniform(min, max) generates random numbers from a uniform distribution in the range \\([\\text{min}, \\text{max}]\\) . Normal(mu, sigma) generates random numbers from a normal distribution with min mu and standard deviation sigma. See Random DIstributions for more distributions. For example: noise = Uniform ( - 0.5 , 0.5 ) The arguments to the random distributions can be either fixed values or (functions of) global parameters. min_val = - 0.5 : population max_val = 0.5 : population noise = Uniform ( min_val , max_val ) It is not allowed to use local parameters (with different values per neuron) or variables, as the random number generators are initialized only once at network creation (doing otherwise would impair performance too much). If a global parameter is used, changing its value will not affect the generator after compilation. It is therefore better practice to use normalized random generators and scale their outputs: min_val = - 0.5 : population max_val = 0.5 : population noise = min_val + ( max_val - min_val ) * Uniform ( 0.0 , 1.0 ) Mathematical functions # Most mathematical functions of the cmath library are understood by the parser, for example: cos , sin , tan , acos , asin , atan , exp , abs , fabs , sqrt , log , ln The positive and negative parts of a term are also defined, with short and long versions: r = pos ( mp ) r = positive ( mp ) r = neg ( mp ) r = negative ( mp ) A piecewise linear function is also provided (linear when x is between a and b, saturated at a or b otherwise): r = clip ( x , a , b ) For integer variables, the modulo operator is defined: x += 1 : int y = modulo ( x , 10 ) When using the power function ( r = x^2 or r = pow(x, 2) ), the cmath pow(double, int) method is used. For small exponents (quadratic or cubic functions), it can be extremely slow, compared to r = x*x or r = x*x*x . Unfortunately, Sympy transforms automatically r = x*x into r = pow(x, 2) . We therefore advise to use the built-in power(double, int) function instead: r = power ( x , 3 ) These functions must be followed by a set of matching brackets: tau * dmp / dt + mp = exp ( - cos ( 2 * pi * f * t + pi / 4 ) + 1 ) Conditional statements # Python-style It is possible to use Python-style conditional statements as the right term of an equation or ODE. They follow the form: if condition : statement1 else : statement2 For example, to define a piecewise linear function, you can nest different conditionals: r = if mp < 1. : if mp > 0. : mp else : 0. else : 1. which is equivalent to: r = clip ( mp , 0.0 , 1.0 ) The condition can use the following vocabulary: True , False , and , or , not , is , is not , == , != , > , < , >= , <= Note The and , or and not logical operators must be used with parentheses around their terms. Example: var = if ( mp > 0 ) and ( ( noise < 0.1 ) or ( not ( condition )) ): 1.0 else : 0.0 is is equivalent to == , is not is equivalent to != . When a conditional statement is split over multiple lines, the flags must be set after the last line: rate = if mp < 1.0 : if mp < 0.0 : 0.0 else : mp else : 1.0 : init = 0.6 An if a: b else:c statement must be exactly the right term of an equation. It is for example NOT possible to write: r = 1.0 + (if mp> 0.0: mp else: 0.0) + b Ternary operator The ternary operator ite(cond, then, else) (ite stands for if-then-else) is available to ease the combination of conditionals with other terms: r = ite ( mp > 0.0 , mp , 0.0 ) # is exactly the same as: r = if mp > 0.0 : mp else : 0.0 The advantage is that the conditional term is not restricted to the right term of the equation, and can be used multiple times: r = ite ( mp > 0.0 , ite ( mp < 1.0 , mp , 1.0 ), 0.0 ) + ite ( stimulated , 1.0 , 0.0 ) Custom functions # To simplify the writing of equations, custom functions can be defined either globally (usable by all neurons and synapses) or locally (only for the particular type of neuron/synapse) using the same mathematical parser. Global functions can be defined using the add_function() method: add_function ( 'sigmoid(x) = 1.0 / (1.0 + exp(-x))' ) With this declaration, sigmoid() can be used in the declaration of any variable, for example: neuron = Neuron ( equations = \"\"\" r = sigmoid(sum(exc)) \"\"\" ) Functions must be one-liners, i.e. they should have only one return value. They can use as many arguments as needed, but are totally unaware of the context: all the needed information should be passed as an argument (except constants which are visible to the function). The types of the arguments (including the return value) are by default floating-point. If other types should be used, they should be specified at the end of the definition, after the : sign, with the type of the return value first, followed by the type of all arguments separated by commas: add_function ( 'conditional_increment(c, v, t) = if v > t : c + 1 else: c : int, int, float, float' ) After compilation , the function can be called using arbitrary list of values for the arguments using the functions() method and the name of the function: add_function ( 'sigmoid(x) = 1.0 / (1.0 + exp(-x))' ) compile () x = np . linspace ( - 10. , 10. , 1000 ) y = functions ( 'sigmoid' )( x ) You can pass a list or a 1D Numpy array as argument, but not a single value or a multidimensional array. When several arguemnts are passed, they must have the same size. Local functions are specific to a Neuron or Synapse class and can only be used within this context (if they have the same name as global variables, they will override them). They can be passed as a multi-line argument to the constructor of a neuron or synapse (see later): functions == \"\"\" sigmoid(x) = 1.0 / (1.0 + exp(-x)) conditional_increment(c, v, t) = if v > t : c + 1 else: c : int, int, float, float \"\"\"","title":"Parser"},{"location":"manual/Parser/#parser","text":"A Neuron or Synapse type is primarily defined by two sets of values which must be specified in its constructor: Parameters are values such as time constants which are constant during the simulation. They can be the same throughout the population/projection, or take different values. Variables are neuronal variables (for example the membrane potential or firing rate) or synaptic variables (the synaptic efficiency) whose value evolve with time during the simulation. The equation (whether it is an ordinary differential equation or not) ruling their evolution can be described using a specific meta-language.","title":"Parser"},{"location":"manual/Parser/#parameters","text":"Parameters are defined by a multi-string consisting of one or more parameter definitions: parameters = \"\"\" tau = 10.0 eta = 0.5 \"\"\" Each parameter should be defined on a single line, with its name on the left side of the equal sign, and its value on the right side. The given value corresponds to the initial value of the parameter (but it can be changed at any further point of the simulation). As a neuron/synapse type is likely to be reused in different populations/projections, it is good practice to set reasonable initial values in the neuron/synapse type, and eventually adapt them to the corresponding populations/projections later on. Local vs. global parameters By default, a neural parameter will be unique to each neuron (i.e. each neuron instance will hold a copy of the parameter) or synapse. In order to save memory space, one can force ANNarchy to store only one parameter value for a whole population by specifying the population flag after a : symbol following the parameter definition: parameters = \"\"\" tau = 10.0 eta = 0.5 : population \"\"\" In this case, there will be only only one instance of the eta parameter for the whole population. eta is called a global parameter, in opposition to local parameters which are the default. The same is true for synapses, whose parameters are by default unique to each synapse in a given projection. If the post-synaptic flag is passed, the parameter will be common to all synapses of a post-synaptic neuron, but can differ from one post-synaptic neuron to another. If the projection flag is passed, the parameter will be common to all synapses of a projection (e.g. the learning rate). Type of the variable Parameters have floating-point precision by default. If you want to force the parameter to be an integer or boolean, you can also pass the int and bool flags, separated by commas: parameters = \"\"\" tau = 10.0 eta = 1 : population, int \"\"\" Constants Alternatively, it is possible to use constants in the parameter definition (see later): tau_exc = Constant ( 'tau_exc' , 10.0 ) neuron = Neuron ( parameters = \"\"\" tau = tau_exc \"\"\" , ) The advantage of this method is that if a parameter value is \\\"shared\\\" across several neuron/synapse types, you only need to change the value once, instead of in each neuron/synapse definition.","title":"Parameters"},{"location":"manual/Parser/#variables","text":"Time-varying variables are also defined using a multi-line description: equations = \"\"\" noise = Uniform(0.0, 0.2) tau * dmp/dt + mp = baseline + sum(exc) + noise r = pos(mp) \"\"\" The evolution of each variable with time can be described through a simple equation or an ordinary differential equation (ODE). ANNarchy provides a simple parser for mathematical expressions, whose role is to translate a high-level description of the equation into an optimized C++ code snippet. The equation for one variable can depend on parameters, other variables (even when declared later) or constants. Variables are updated in the same order as their declaration in the multistring (see Numerical methods , as it influences how ODEs are solved). The declaration of a single variable can extend on multiple lines: equations = \"\"\" noise = Uniform(0.0, 0.2) tau * dmp/dt = baseline - mp + sum(exc) + noise : max = 1.0 rate = pos(mp) \"\"\" As it is only a parser and not a solver, some limitations exist: Simple equations must hold only the name of the variable on the left sign of the equation. Variable definitions such as rate + mp = noise are forbidden, as it would be impossible to guess which variable should be updated. ODEs are more free regarding the left side, but only one variable should hold the gradient: the one which will be updated. The following definitions are equivalent and will lead to the same C++ code: tau * dmp / dt = baseline - mp tau * dmp / dt + mp = baseline tau * dmp / dt + mp - baseline = 0 dmp / dt = ( baseline - mp ) / tau In practice, ODEs are transformed using Sympy into the last form (only the gradient stays on the left) and numerized using the chosen numerical method (see Numerical methods ).","title":"Variables"},{"location":"manual/Parser/#flags","text":"Locality and type Like the parameters, variables also accept the population , postsynaptic and projection flags to define the local/global character of the variable, as well as the int or bool flags for their type. Initial value The initial value of the variable (before the first simulation starts) can also be specified using the init keyword followed by the desired value: equations = \"\"\" tau * dmp/dt + mp = baseline : init = 0.2 \"\"\" It must be a single value (the same for all neurons in the population or all synapses in the projection) and should not depend on other parameters and variables. This initial value can be specifically changed after the Population or Projection objects are created (see Populations ). It is also possible to use constants for the initial value: init_mp = Constant ( 'init_mp' , 0.2 ) neuron = Neuron ( equations = \"\"\" tau * dmp/dt + mp = baseline : init = init_mp \"\"\" , ) Min and Max values of a variable Upper- and lower-bounds can be set using the min and max keywords: equations = \"\"\" tau * dmp/dt + mp = baseline : min = -0.2, max = 1.0 \"\"\" At each step of the simulation, after the update rule is calculated for mp , the new value will be compared to the min and max value, and clamped if necessary. min and max can be single values, constants, parameters, variables or functions of all these: parameters = \"\"\" tau = 10.0 min_mp = -1.0 : population max_mp = 1.0 \"\"\" , equations = \"\"\" variance = Uniform(0.0, 1.0) tau * dmp/dt + mp = sum(exc) : min = min_mp, max = max_mp + variance r = mp : min = 0.0 # Equivalent to r = pos(mp) \"\"\" Numerical method The numerization method for a single ODEs can be explicitely set by specifying a flag: tau * dmp/dt + mp = sum(exc) : exponential The available numerical methods are described in Numerical methods . Summary of allowed flags for variables: init : defines the initialization value at begin of simulation and after a network reset (default: 0.0) min : minimum allowed value (unset by default) max : maximum allowed value (unset by default) population : the attribute is shared by all neurons of a population. postsynaptic : the attribute is shared by all synapses of a post-synaptic neuron. projection : the attribute is shared by all synapses of a projection. explicit , implicit , exponential , midpoint , event-driven : the numerical method to be used.","title":"Flags"},{"location":"manual/Parser/#constants","text":"Global constants can be created by the user and used inside any equation. They must define an unique name and a floating point value: tau = Constant ( 'tau' , 10.0 ) neuron = Neuron ( equations = \"tau * dr/dt + r = sum(exc)\" ) In this example, a Neuron or Synapse does not have to define the parameter tau to use it: it is available everywhere. If the Neuron/Synapse redefines a parameter called tau , the constant is not visible anymore to that object. Constants can be manipulated as normal floats to define complex values: tau = Constant ( 'tau' , 20 ) factor = Constant ( 'factor' , 0.1 ) real_tau = Constant ( 'real_tau' , tau * factor ) neuron = Neuron ( equations = ''' real_tau*dr/dt + r =1.0 ''' ) Note that constants are only global, changing their value impacts all objects using them. Changing the value of a constant can only be done through the set() method (before or after compile() ): tau = Constant ( 'tau' , 20 ) tau . set ( 10.0 )","title":"Constants"},{"location":"manual/Parser/#allowed-vocabulary","text":"The mathematical parser relies heavily on the one provided by SymPy .","title":"Allowed vocabulary"},{"location":"manual/Parser/#numerical-values","text":"All parameters and variables use implicitly the floating-point double precision, except when stated otherwise with the int or bool keywords. You can use numerical constants within the equation, noting that they will be automatically converted to this precision: tau * dmp / dt = 1 / pos ( mp ) + 1 The constant \\(\\pi\\) is available under the literal form pi .","title":"Numerical values"},{"location":"manual/Parser/#operators","text":"Additions (+), substractions (-), multiplications (*), divisions (/) and power functions (\\^) are of course allowed. Gradients are allowed only for the variable currently described. They take the form: dmp / dt = A with a d preceding the variable's name and terminated by /dt (with or without spaces). Gradients must be on the left side of the equation. To update the value of a variable at each time step, the operators = , += , -= , *= , and /= are allowed.","title":"Operators"},{"location":"manual/Parser/#parameters-and-variables","text":"Any parameter or variable defined in the same Neuron/Synapse can be used inside an equation. User-defined constants can also be used. Additionally, the following variables are pre-defined: dt : the discretization time step for the simulation. Using this variable, you can define the numerical method by yourself. For example: tau * dmp / dt + mp = baseline with backward Euler would be equivalent to: mp += dt / tau * ( baseline - mp ) t : the time in milliseconds elapsed since the creation of the network. This allows to generate oscillating variables: f = 10.0 # Frequency of 10 Hz phi = pi / 4 # Phase ts = t / 1000.0 # ts is in seconds r = 10.0 * ( sin ( 2 * pi * f * ts + phi ) + 1.0 )","title":"Parameters and Variables"},{"location":"manual/Parser/#random-number-generators","text":"Several random generators are available and can be used within an equation. In the current version are for example available: Uniform(min, max) generates random numbers from a uniform distribution in the range \\([\\text{min}, \\text{max}]\\) . Normal(mu, sigma) generates random numbers from a normal distribution with min mu and standard deviation sigma. See Random DIstributions for more distributions. For example: noise = Uniform ( - 0.5 , 0.5 ) The arguments to the random distributions can be either fixed values or (functions of) global parameters. min_val = - 0.5 : population max_val = 0.5 : population noise = Uniform ( min_val , max_val ) It is not allowed to use local parameters (with different values per neuron) or variables, as the random number generators are initialized only once at network creation (doing otherwise would impair performance too much). If a global parameter is used, changing its value will not affect the generator after compilation. It is therefore better practice to use normalized random generators and scale their outputs: min_val = - 0.5 : population max_val = 0.5 : population noise = min_val + ( max_val - min_val ) * Uniform ( 0.0 , 1.0 )","title":"Random number generators"},{"location":"manual/Parser/#mathematical-functions","text":"Most mathematical functions of the cmath library are understood by the parser, for example: cos , sin , tan , acos , asin , atan , exp , abs , fabs , sqrt , log , ln The positive and negative parts of a term are also defined, with short and long versions: r = pos ( mp ) r = positive ( mp ) r = neg ( mp ) r = negative ( mp ) A piecewise linear function is also provided (linear when x is between a and b, saturated at a or b otherwise): r = clip ( x , a , b ) For integer variables, the modulo operator is defined: x += 1 : int y = modulo ( x , 10 ) When using the power function ( r = x^2 or r = pow(x, 2) ), the cmath pow(double, int) method is used. For small exponents (quadratic or cubic functions), it can be extremely slow, compared to r = x*x or r = x*x*x . Unfortunately, Sympy transforms automatically r = x*x into r = pow(x, 2) . We therefore advise to use the built-in power(double, int) function instead: r = power ( x , 3 ) These functions must be followed by a set of matching brackets: tau * dmp / dt + mp = exp ( - cos ( 2 * pi * f * t + pi / 4 ) + 1 )","title":"Mathematical functions"},{"location":"manual/Parser/#conditional-statements","text":"Python-style It is possible to use Python-style conditional statements as the right term of an equation or ODE. They follow the form: if condition : statement1 else : statement2 For example, to define a piecewise linear function, you can nest different conditionals: r = if mp < 1. : if mp > 0. : mp else : 0. else : 1. which is equivalent to: r = clip ( mp , 0.0 , 1.0 ) The condition can use the following vocabulary: True , False , and , or , not , is , is not , == , != , > , < , >= , <= Note The and , or and not logical operators must be used with parentheses around their terms. Example: var = if ( mp > 0 ) and ( ( noise < 0.1 ) or ( not ( condition )) ): 1.0 else : 0.0 is is equivalent to == , is not is equivalent to != . When a conditional statement is split over multiple lines, the flags must be set after the last line: rate = if mp < 1.0 : if mp < 0.0 : 0.0 else : mp else : 1.0 : init = 0.6 An if a: b else:c statement must be exactly the right term of an equation. It is for example NOT possible to write: r = 1.0 + (if mp> 0.0: mp else: 0.0) + b Ternary operator The ternary operator ite(cond, then, else) (ite stands for if-then-else) is available to ease the combination of conditionals with other terms: r = ite ( mp > 0.0 , mp , 0.0 ) # is exactly the same as: r = if mp > 0.0 : mp else : 0.0 The advantage is that the conditional term is not restricted to the right term of the equation, and can be used multiple times: r = ite ( mp > 0.0 , ite ( mp < 1.0 , mp , 1.0 ), 0.0 ) + ite ( stimulated , 1.0 , 0.0 )","title":"Conditional statements"},{"location":"manual/Parser/#custom-functions","text":"To simplify the writing of equations, custom functions can be defined either globally (usable by all neurons and synapses) or locally (only for the particular type of neuron/synapse) using the same mathematical parser. Global functions can be defined using the add_function() method: add_function ( 'sigmoid(x) = 1.0 / (1.0 + exp(-x))' ) With this declaration, sigmoid() can be used in the declaration of any variable, for example: neuron = Neuron ( equations = \"\"\" r = sigmoid(sum(exc)) \"\"\" ) Functions must be one-liners, i.e. they should have only one return value. They can use as many arguments as needed, but are totally unaware of the context: all the needed information should be passed as an argument (except constants which are visible to the function). The types of the arguments (including the return value) are by default floating-point. If other types should be used, they should be specified at the end of the definition, after the : sign, with the type of the return value first, followed by the type of all arguments separated by commas: add_function ( 'conditional_increment(c, v, t) = if v > t : c + 1 else: c : int, int, float, float' ) After compilation , the function can be called using arbitrary list of values for the arguments using the functions() method and the name of the function: add_function ( 'sigmoid(x) = 1.0 / (1.0 + exp(-x))' ) compile () x = np . linspace ( - 10. , 10. , 1000 ) y = functions ( 'sigmoid' )( x ) You can pass a list or a 1D Numpy array as argument, but not a single value or a multidimensional array. When several arguemnts are passed, they must have the same size. Local functions are specific to a Neuron or Synapse class and can only be used within this context (if they have the same name as global variables, they will override them). They can be passed as a multi-line argument to the constructor of a neuron or synapse (see later): functions == \"\"\" sigmoid(x) = 1.0 / (1.0 + exp(-x)) conditional_increment(c, v, t) = if v > t : c + 1 else: c : int, int, float, float \"\"\"","title":"Custom functions"},{"location":"manual/Populations/","text":"Populations # Once the Neuron objects have been defined, the populations can be created. Let's suppose we have defined the following rate-coded neuron: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) \"\"\" ) Creating populations # Populations of neurons are created using the Population class: pop1 = Population ( geometry = 100 , neuron = LeakyIntegratorNeuron ) pop2 = Population ( geometry = ( 8 , 8 ), neuron = LeakyIntegratorNeuron , name = \"pop2\" ) The rate-coded or spiking nature of the Neuron instance is irrelevant when creating the Population object. It takes different parameters: geometry defines the number of neurons in the population, as well as its spatial structure (1D/2D/3D or more). For example, a two-dimensional population with 15*10 neurons takes the argument (15, 10) , while a one-dimensional array of 100 neurons would take (100,) or simply 100 . neuron indicates the neuron type to use for this population (which must have been defined before). It requires a Neuron instance. name is an unique string for each population in the network. If name is omitted, an internal name such as pop0 will be given (the number is incremented every time a new population is defined). Although this argument is optional, it is strongly recommended to give an understandable name to each population: if you somehow \\\"lose\\\" the reference to the Population object in some portion of your code, you can always retrieve it using the get_population(name) method. After creation, each population has several attributes defined (corresponding to the parameters and variables of the Neuron type) and is assigned a fixed size ( pop.size corresponding to the total number of neurons, here 100 for pop1 and 64 for pop2 ) and geometry ( pop1.geometry , here (100, ) and (8, 8) ). Geometry and ranks # Each neuron in the population has therefore a set of coordinates (expressed relative to pop1.geometry ) and a rank (from 0 to pop1.size -1 ). The reason is that spatial coordinates are useful for visualization, or when defining a distance-dependent connection pattern, but that ANNarchy internally uses flat arrays for performance reasons. The coordinates use the matrix notation for multi-dimensional arrays, which is also used by Numpy (for a 2D matrix, the first index represents the row, the second the column). You can therefore use safely the reshape() method of Numpy to switch between coordinates-based and rank-based representations of an array. To convert the rank of a neuron to its coordinates (and vice-versa), you can use the ravel_multi_index and unravel_index methods of Numpy, but they can be quite slow. The Population class provides two more efficient methods to do this conversion: coordinates_from_rank returns a tuple representing the coordinates of neuron based on its rank (between 0 and size -1 , otherwise an error is thrown). rank_from_coordinates returns the rank corresponding to the coordinates. For example, with pop2 having a geometry (8, 8) : >>> pop2 . coordinates_from_rank ( 15 ) ( 1 , 7 ) >>> pop2 . rank_from_coordinates (( 4 , 6 )) 38 Population attributes # The value of the parameters and variables of all neurons in a population can be accessed and modified through population attributes. With the previously defined populations, you can list all their parameters and variables with: >>> pop2 . attributes [ 'tau' , 'baseline' , 'mp' , 'r' ] >>> pop2 . parameters [ 'tau' , 'baseline' ] >>> pop2 . variables [ 'r' , 'mp' ] Reading their value is straightforward: >>> pop2 . tau 10.0 >>> pop2 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) Population-wise parameters/variables have a single value for the population, while neuron-specific ones return a NumPy array with the same geometry has the population. Setting their value is also simple: >>> pop2 . tau = 20.0 >>> pop2 . tau 20.0 >>> pop2 . r = 1.0 >>> pop2 . r array ([[ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ]]) >>> pop2 . mp = 0.5 * np . ones ( pop2 . geometry ) array ([[ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ]]) >>> pop2 . r = Uniform ( 0.0 , 1.0 ) array ([[ 0.97931939 , 0.64865327 , 0.29740417 , 0.49352664 , 0.36511704 , 0.59879869 , 0.10835491 , 0.38481751 ], [ 0.07664157 , 0.77532887 , 0.04773084 , 0.75395453 , 0.56072342 , 0.54139054 , 0.28553319 , 0.96159595 ], [ 0.01811468 , 0.30214921 , 0.45321071 , 0.56728733 , 0.24577655 , 0.32798484 , 0.84929103 , 0.63025331 ], [ 0.34168482 , 0.07411291 , 0.6510492 , 0.89025337 , 0.31192464 , 0.59834719 , 0.77102494 , 0.88537967 ], [ 0.41813573 , 0.47395247 , 0.46603402 , 0.45863676 , 0.76628989 , 0.42256749 , 0.18527079 , 0.8322103 ], [ 0.70616692 , 0.73210377 , 0.05255718 , 0.01939817 , 0.24659769 , 0.50349528 , 0.79201573 , 0.19159611 ], [ 0.21246111 , 0.93570727 , 0.68544108 , 0.61158741 , 0.17954022 , 0.90084004 , 0.41286698 , 0.45550662 ], [ 0.14720568 , 0.51426136 , 0.36225438 , 0.06096426 , 0.77209455 , 0.07348683 , 0.43178591 , 0.32451531 ]]) For population-wide attributes, you can only specify a single value (float, int or bool depending on the type of the parameter/variable). For neuron-specific attributes, you can provide either: a single value which will be applied to all neurons of the population. a list or a one-dimensional Numpy array of the same length as the number of neurons in the population. This information is provided by pop1.size . a Numpy array of the same shape as the geometry of the population. This information is provided by pop1.geometry . a random number generator object (Uniform, Normal...). Note If you do not want to use the attributes of Python (for example when doing a loop over unknown attributes), you can also use the get(name) and set(values) methods of Population : pop1 . get ( 'tau' ) pop1 . set ({ 'mp' : 1.0 , 'r' : Uniform ( 0.0 , 1.0 )}) Accessing individual neurons # There exists a purely semantic access to individual neurons of a population. The IndividualNeuron class wraps population data for a specific neuron. It can be accessed through the Population.neuron() method using either the rank of the neuron (from 0 to pop1.size - 1 ) or its coordinates in the population's geometry: >>> print pop2 . neuron ( 2 , 2 ) Neuron of the population pop2 with rank 18 ( coordinates ( 2 , 2 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 The individual neurons can be manipulated individually: >>> my_neuron = pop2 . neuron ( 2 , 2 ) >>> my_neuron . rate = 1.0 >>> print my_neuron Neuron of the population pop2 with rank 18 ( coordinates ( 2 , 2 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 1.0 Warning IndividualNeuron is only a wrapper for ease of use, the real data is stored in arrays for the whole population, so accessing individual neurons is much slower and should be reserved to specific cases (i.e. only from time to time and for a limited set of neurons). Accessing groups of neurons # Individual neurons can be grouped into PopulationView objects, which hold references to different neurons of the same population. One can create population views by \\\"adding\\\" several neurons together: >>> popview = pop2 . neuron ( 2 , 2 ) + pop2 . neuron ( 3 , 3 ) + pop2 . neuron ( 4 , 4 ) >>> popview PopulationView of pop2 Ranks : [ 18 , 27 , 36 ] * Neuron of the population pop2 with rank 18 ( coordinates ( 2 , 2 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 * Neuron of the population pop2 with rank 27 ( coordinates ( 3 , 3 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 * Neuron of the population pop2 with rank 36 ( coordinates ( 4 , 4 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 >>> popview . r = 1.0 >>> pop2 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) One can also use the slice operators to create PopulationViews: >>> popview = pop2 [ 3 , :] >>> popview . r = 1.0 >>> pop2 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) or: >>> popview = pop2 [ 2 : 5 , 4 ] >>> popview . r = 1.0 >>> pop1 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) PopulationView objects can be used to create projections. Warning Contrary to the equivalent in PyNN, PopulationViews in ANNarchy can only group neurons from the same population. Functions # If you have defined a function inside a Neuron definition: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 slope = 1.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = sigmoid(mp, slope) \"\"\" , functions == \"\"\" sigmoid(x, k) = 1.0 / (1.0 + exp(-x*k)) \"\"\" ) you can use this function in Python as if it were a method of the corresponding object: pop = Population ( 1000 , LeakyIntegratorNeuron ) x = np . linspace ( - 1. , 1. , 100 ) k = np . ones ( 100 ) r = pop . sigmoid ( x , k ) You can pass either a list or a 1D Numpy array to each argument ( not a single value, nor a multidimensional array! ). The size of the arrays passed for each argument is arbitrary (it must not match the population's size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.","title":"Populations"},{"location":"manual/Populations/#populations","text":"Once the Neuron objects have been defined, the populations can be created. Let's suppose we have defined the following rate-coded neuron: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) \"\"\" )","title":"Populations"},{"location":"manual/Populations/#creating-populations","text":"Populations of neurons are created using the Population class: pop1 = Population ( geometry = 100 , neuron = LeakyIntegratorNeuron ) pop2 = Population ( geometry = ( 8 , 8 ), neuron = LeakyIntegratorNeuron , name = \"pop2\" ) The rate-coded or spiking nature of the Neuron instance is irrelevant when creating the Population object. It takes different parameters: geometry defines the number of neurons in the population, as well as its spatial structure (1D/2D/3D or more). For example, a two-dimensional population with 15*10 neurons takes the argument (15, 10) , while a one-dimensional array of 100 neurons would take (100,) or simply 100 . neuron indicates the neuron type to use for this population (which must have been defined before). It requires a Neuron instance. name is an unique string for each population in the network. If name is omitted, an internal name such as pop0 will be given (the number is incremented every time a new population is defined). Although this argument is optional, it is strongly recommended to give an understandable name to each population: if you somehow \\\"lose\\\" the reference to the Population object in some portion of your code, you can always retrieve it using the get_population(name) method. After creation, each population has several attributes defined (corresponding to the parameters and variables of the Neuron type) and is assigned a fixed size ( pop.size corresponding to the total number of neurons, here 100 for pop1 and 64 for pop2 ) and geometry ( pop1.geometry , here (100, ) and (8, 8) ).","title":"Creating populations"},{"location":"manual/Populations/#geometry-and-ranks","text":"Each neuron in the population has therefore a set of coordinates (expressed relative to pop1.geometry ) and a rank (from 0 to pop1.size -1 ). The reason is that spatial coordinates are useful for visualization, or when defining a distance-dependent connection pattern, but that ANNarchy internally uses flat arrays for performance reasons. The coordinates use the matrix notation for multi-dimensional arrays, which is also used by Numpy (for a 2D matrix, the first index represents the row, the second the column). You can therefore use safely the reshape() method of Numpy to switch between coordinates-based and rank-based representations of an array. To convert the rank of a neuron to its coordinates (and vice-versa), you can use the ravel_multi_index and unravel_index methods of Numpy, but they can be quite slow. The Population class provides two more efficient methods to do this conversion: coordinates_from_rank returns a tuple representing the coordinates of neuron based on its rank (between 0 and size -1 , otherwise an error is thrown). rank_from_coordinates returns the rank corresponding to the coordinates. For example, with pop2 having a geometry (8, 8) : >>> pop2 . coordinates_from_rank ( 15 ) ( 1 , 7 ) >>> pop2 . rank_from_coordinates (( 4 , 6 )) 38","title":"Geometry and ranks"},{"location":"manual/Populations/#population-attributes","text":"The value of the parameters and variables of all neurons in a population can be accessed and modified through population attributes. With the previously defined populations, you can list all their parameters and variables with: >>> pop2 . attributes [ 'tau' , 'baseline' , 'mp' , 'r' ] >>> pop2 . parameters [ 'tau' , 'baseline' ] >>> pop2 . variables [ 'r' , 'mp' ] Reading their value is straightforward: >>> pop2 . tau 10.0 >>> pop2 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) Population-wise parameters/variables have a single value for the population, while neuron-specific ones return a NumPy array with the same geometry has the population. Setting their value is also simple: >>> pop2 . tau = 20.0 >>> pop2 . tau 20.0 >>> pop2 . r = 1.0 >>> pop2 . r array ([[ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ]]) >>> pop2 . mp = 0.5 * np . ones ( pop2 . geometry ) array ([[ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ]]) >>> pop2 . r = Uniform ( 0.0 , 1.0 ) array ([[ 0.97931939 , 0.64865327 , 0.29740417 , 0.49352664 , 0.36511704 , 0.59879869 , 0.10835491 , 0.38481751 ], [ 0.07664157 , 0.77532887 , 0.04773084 , 0.75395453 , 0.56072342 , 0.54139054 , 0.28553319 , 0.96159595 ], [ 0.01811468 , 0.30214921 , 0.45321071 , 0.56728733 , 0.24577655 , 0.32798484 , 0.84929103 , 0.63025331 ], [ 0.34168482 , 0.07411291 , 0.6510492 , 0.89025337 , 0.31192464 , 0.59834719 , 0.77102494 , 0.88537967 ], [ 0.41813573 , 0.47395247 , 0.46603402 , 0.45863676 , 0.76628989 , 0.42256749 , 0.18527079 , 0.8322103 ], [ 0.70616692 , 0.73210377 , 0.05255718 , 0.01939817 , 0.24659769 , 0.50349528 , 0.79201573 , 0.19159611 ], [ 0.21246111 , 0.93570727 , 0.68544108 , 0.61158741 , 0.17954022 , 0.90084004 , 0.41286698 , 0.45550662 ], [ 0.14720568 , 0.51426136 , 0.36225438 , 0.06096426 , 0.77209455 , 0.07348683 , 0.43178591 , 0.32451531 ]]) For population-wide attributes, you can only specify a single value (float, int or bool depending on the type of the parameter/variable). For neuron-specific attributes, you can provide either: a single value which will be applied to all neurons of the population. a list or a one-dimensional Numpy array of the same length as the number of neurons in the population. This information is provided by pop1.size . a Numpy array of the same shape as the geometry of the population. This information is provided by pop1.geometry . a random number generator object (Uniform, Normal...). Note If you do not want to use the attributes of Python (for example when doing a loop over unknown attributes), you can also use the get(name) and set(values) methods of Population : pop1 . get ( 'tau' ) pop1 . set ({ 'mp' : 1.0 , 'r' : Uniform ( 0.0 , 1.0 )})","title":"Population attributes"},{"location":"manual/Populations/#accessing-individual-neurons","text":"There exists a purely semantic access to individual neurons of a population. The IndividualNeuron class wraps population data for a specific neuron. It can be accessed through the Population.neuron() method using either the rank of the neuron (from 0 to pop1.size - 1 ) or its coordinates in the population's geometry: >>> print pop2 . neuron ( 2 , 2 ) Neuron of the population pop2 with rank 18 ( coordinates ( 2 , 2 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 The individual neurons can be manipulated individually: >>> my_neuron = pop2 . neuron ( 2 , 2 ) >>> my_neuron . rate = 1.0 >>> print my_neuron Neuron of the population pop2 with rank 18 ( coordinates ( 2 , 2 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 1.0 Warning IndividualNeuron is only a wrapper for ease of use, the real data is stored in arrays for the whole population, so accessing individual neurons is much slower and should be reserved to specific cases (i.e. only from time to time and for a limited set of neurons).","title":"Accessing individual neurons"},{"location":"manual/Populations/#accessing-groups-of-neurons","text":"Individual neurons can be grouped into PopulationView objects, which hold references to different neurons of the same population. One can create population views by \\\"adding\\\" several neurons together: >>> popview = pop2 . neuron ( 2 , 2 ) + pop2 . neuron ( 3 , 3 ) + pop2 . neuron ( 4 , 4 ) >>> popview PopulationView of pop2 Ranks : [ 18 , 27 , 36 ] * Neuron of the population pop2 with rank 18 ( coordinates ( 2 , 2 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 * Neuron of the population pop2 with rank 27 ( coordinates ( 3 , 3 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 * Neuron of the population pop2 with rank 36 ( coordinates ( 4 , 4 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 >>> popview . r = 1.0 >>> pop2 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) One can also use the slice operators to create PopulationViews: >>> popview = pop2 [ 3 , :] >>> popview . r = 1.0 >>> pop2 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) or: >>> popview = pop2 [ 2 : 5 , 4 ] >>> popview . r = 1.0 >>> pop1 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) PopulationView objects can be used to create projections. Warning Contrary to the equivalent in PyNN, PopulationViews in ANNarchy can only group neurons from the same population.","title":"Accessing groups of neurons"},{"location":"manual/Populations/#functions","text":"If you have defined a function inside a Neuron definition: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 slope = 1.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = sigmoid(mp, slope) \"\"\" , functions == \"\"\" sigmoid(x, k) = 1.0 / (1.0 + exp(-x*k)) \"\"\" ) you can use this function in Python as if it were a method of the corresponding object: pop = Population ( 1000 , LeakyIntegratorNeuron ) x = np . linspace ( - 1. , 1. , 100 ) k = np . ones ( 100 ) r = pop . sigmoid ( x , k ) You can pass either a list or a 1D Numpy array to each argument ( not a single value, nor a multidimensional array! ). The size of the arrays passed for each argument is arbitrary (it must not match the population's size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.","title":"Functions"},{"location":"manual/Projections/","text":"Projections # Declaring the projections # Once the populations are created, one can connect them by creating Projection instances: proj = Projection ( pre = pop1 , post = pop2 , target = \"exc\" , synapse = BCM ) pre is either the name of the pre-synaptic population or the corresponding Population object. post is either the name of the post-synaptic population or the corresponding Population object. target is the type of the connection. synapse is an optional argument requiring a Synapse instance. The post-synaptic neuron type must use sum(exc) in the rate-coded case respectively g_exc in the spiking case, otherwise the projection will be useless. If the synapse argument is omitted, the default synapse will be used: the default rate-coded synapse defines psp = w * pre.r , the default spiking synapse defines g_target += w . Building the projections # Creating the Projection objects only defines the information that two populations are connected. The synapses must be explicitely created by applying a connector method on the Projection object. To this end, ANNarchy already provides a set of predefined connector methods, but the user has also the possibility to define his own (see Connector ). The pattern can be applied either directly at the creation of the Projection: proj = Projection ( pre = pop1 , post = pop2 , target = \"exc\" , synapse = BCM ) . connect_all_to_all ( weights = 1.0 ) or afterwards: proj = Projection ( pre = pop1 , post = pop2 , target = \"exc\" , synapse = BCM ) proj . connect_all_to_all ( weights = 1.0 ) The connector method must be called before the network is compiled. Projection attributes # Let's suppose the BCM synapse is used to create the Projection proj (spiking synapses are accessed similarly): BCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 100. : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0 \"\"\" ) Global attributes # The global parameters and variables of a projection (i.e. defined with the postsynaptic or projection flags) can be accessed directly through attributes. Attributes defined with projection have a single value for the whole population: >>> proj . tau 100 Attributes defined with postsynaptic have one value per post-synaptic neuron, so the result is a vector: >>> proj . theta [ 3.575 , 15.987 , ... , 4.620 ] Post-synaptic variables can be modified by passing: a single value, which will be the same for all post-synaptic neurons. a list of values, with the same size as the number of neurons receiving synapses (for some sparse connectivity patterns, it may not be the same as the size of the population, so no multidimensional array is accepted). After compilation (and therefore creation of the synapses), you can access how many post-synaptic neurons receive actual synapses with: >>> proj . size 4 The list of ranks of the post-synaptic neurons receiving synapses is obtained with: >>> proj . post_ranks [ 0 , 1 , 2 , 3 ] Local attributes # At the projection level Local attributes can also be accessed globally through attributes. It will return a list of lists containing the synapse-specific values. The first index represents the post-synaptic neurons. It has the same length as proj.post_ranks . Beware that if some post-synaptic neurons do not receive any connection, this index will not correspond to the ranksof the post-synaptic population. The second index addresses the pre-synaptic neurons. If the connection is sparse, it also is unrelated to the ranks of the pre-synaptic neurons in their populations. Warning Modifying these lists of lists is error-prone, so this method should be avoided if possible. At the post-synaptic level The local parameters and variables of a projection (synapse-specific) should better be accessed through the Dendrite object, which gathers for a single post-synaptic neuron all synapses belonging to the projection. Beware: As projections are only instantiated after the call to compile() , local attributes of a Projection are only available then. Trying to access them before compilation will lead to an error! Each dendrite stores the parameters and variables of the corresponding synapses as attributes, as populations do for neurons. You can loop over all post-synaptic neurons receiving synapses with the dendrites iterator: for dendrite in proj . dendrites : print dendrite . pre_ranks print dendrite . size print dendrite . tau print dendrite . alpha print dendrite . w dendrite.pre_ranks returns a list of pre-synaptic neuron ranks. dendrite.size returns the number of synapses for the considered post-synaptic neuron. Global parameters/variables return a single value ( dendrite.tau ) and local ones return a list ( dendrite.w ). You can even omit the .dendrites part of the iterator: for dendrite in proj : print dendrite . pre_ranks print dendrite . size print dendrite . tau print dendrite . alpha print dendrite . w You can also access the dendrites individually, either by specifying the rank of the post-synaptic neuron: dendrite = proj . dendrite ( 13 ) print dendrite . w or its coordinates: dendrite = proj . dendrite ( 5 , 5 ) print dendrite . w When using ranks, you can also directly address the projection as an array: dendrite = proj [ 13 ] print dendrite . w Warning You should make sure that the dendrite actually exists before accessing it through its rank, because it is otherwise a None object. Functions # If you have defined a function inside a Synapse definition: BCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 100. : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic dw/dt = eta * BCMRule(pre.r, post.r, theta) : min=0.0 \"\"\" , functions = \"\"\" BCMRule(pre, post, theta) = post * (post - theta) * pre \"\"\" ) you can use this function in Python as if it were a method of the corresponding object: proj = Projection ( pop1 , pop2 , 'exc' , BCM ) . connect_xxx () pre = np . linspace ( 0. , 1. , 100 ) post = np . linspace ( 0. , 1. , 100 ) theta = 0.01 * np . ones ( 100 ) weight_change = proj . BCMRule ( pre , post , theta ) You can pass either a list or a 1D Numpy array to each argument ( not a single value, nor a multidimensional array! ). The size of the arrays passed for each argument is arbitrary (it must not match the projection's size) but you have to make sure that they all have the same size. Errors are not catched, so be careful. Connecting population views # Projections are usually understood as a connectivity pattern between two populations. Complex connectivity patterns have to specifically designed (see Connector ). In some cases, it can be much simpler to connect subsets of neurons directly, using built-in connector methods. To this end, the Projection object also accepts PopulationView objects for the pre and post arguments. Let's suppose we want to connect the (8,8) populations pop1 and pop2 in a all-to-all manner, but only for the (4,4) neurons in the center of these populations. The first step is to create the PopulationView objects using the slice operator: pop1_center = pop1 [ 2 : 7 , 2 : 7 ] pop2_center = pop2 [ 2 : 7 , 2 : 7 ] They can then be simply used to create a projection: proj = Projection ( pre = pop1_center , post = pop2_center , target = \"exc\" , synapse = BCM ) . connect_all_to_all ( weights = 1.0 ) Each neuron of pop2_center will receive synapses from all neurons of pop1_center , and only them. Neurons of pop2 which are not in pop2_center will not receive any synapse. Warning If you define your own connector method and want to use PopulationViews, you will need to iterate over the ranks attribute of the PopulationView object. Specifying delays in synaptic transmission # By default, synaptic transmission is considered to be instantaneous (or more precisely, it takes one simulation step ( dt ) for a newly computed firing rate to be taken into account by post-synaptic neurons). In order to take longer propagation times into account in the transmission of information between two populations, one has the possibility to define synaptic delays for a projection. All the built-in connector methods take an argument delays (default= dt ), which can be a float (in milliseconds) or a random number generator. proj . connect_all_to_all ( weights = 1.0 , delays = 10.0 ) proj . connect_all_to_all ( weights = 1.0 , delays = Uniform ( 1.0 , 10.0 )) If the delay is not a multiple of the simulation time step ( dt = 1.0 by default), it will be rounded to the closest multiple. The same is true for the values returned by a random number generator. Note: Per design, the minimal possible delay is equal to dt : values smaller than dt will be replaced by dt . Negative values do not make any sense and are ignored. Warning Non-uniform delays are not available on CUDA. Controlling projections # Synaptic transmission, update and plasticity It is possible to selectively control synaptic transmission and plasticity at the projection level. The boolean flags transmission , update and plasticity can be set for that purpose: proj.transmission = False proj.update = False proj.plasticity = False If transmission is False , the projection is totally shut down: it does not transmit any information to the post-synaptic population (the corresponding weighted sums or conductances are constantly 0) and all synaptic variables are frozen to their current value (including the synaptic weights w ). If update is False , synaptic transmission occurs normally, but the synaptic variables are not updated. For spiking synapses, this includes traces when they are computed at each step, but not when they are integrated in an event-driven manner (flag event-driven ). Beware: continous synaptic transmission as in NMDA synapses will not work in this mode, as internal variables are not updated. If only plasticity is False , synaptic transmission and synaptic variable updates occur normally, but changes to the synaptic weight w are ignored. Disabling learning Alternatively, one can use the enable_learning() and disable_learning() methods of Projection . The effect of disable_learning() depends on the type of the projection: for rate-coded projections, disable_learning() is equivalent to update=False : no synaptic variables is updated. for spiking projections, it is equivalent to plasticity=False : only the weights are blocked. The reason of this difference is to allow continuous synaptic transmission and computation of traces. Calling enable_learning() without arguments resumes the default learning behaviour. Periodic learning enable_learning() also accepts two arguments period and offset . period defines the interval in ms between two evaluations of the synaptic variables. This can be useful when learning should only occur once at the end of a trial. It is recommended not to use ODEs in the equations in this case, as they are numerized according to a fixed time step. offset defines the time inside the period at which the evaluation should occur. By default, it is 0, so the variable updates will occur at the next step, then after period ms, and so on. Setting it to -1 will shift the update at the end of the period. Note that spiking synapses using online evaluation will not be affected by these parameters, as they are event-driven. Multiple targets # For spiking neurons, it may be desirable that a single synapses activates different currents (or conductances) in the post-synaptic neuron. One example are AMPA/NMDA synapses, where a single spike generates a \\\"classical\\\" AMPA current, plus a voltage-gated slower NMDA current. The following conductance-based Izhikevich is an example: RSNeuron = Neuron ( parameters = \"\"\" a = 0.02 b = 0.2 c = -65. d = 8. tau_ampa = 5. tau_nmda = 150. vrev = 0.0 \"\"\" , equations = \"\"\" I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v) dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init=-65., midpoint du/dt = a * (b*v - u) : init=-13. tau_ampa * dg_ampa/dt = -g_ampa tau_nmda * dg_nmda/dt = -g_nmda \"\"\" , spike = \"\"\" v >= 30. \"\"\" , reset = \"\"\" v = c u += d \"\"\" , functions = \"\"\" nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2) \"\"\" ) However, g_ampa and g_nmda collect by default spikes from different projections, so the weights will not be shared between the \\\"ampa\\\" projection and the \\\"nmda\\\" one. It is therefore possible to specify a list of targets when building a projection, meaning that a single pre-synaptic spike will increase both g_ampa and g_nmda from the same weight: proj = Projection(pop1, pop2, ['ampa', 'nmda'], STDP) An example is provided in examples/homeostatic_stdp/Ramp.py . Warning Multiple targets are not available on CUDA yet.","title":"Projections"},{"location":"manual/Projections/#projections","text":"","title":"Projections"},{"location":"manual/Projections/#declaring-the-projections","text":"Once the populations are created, one can connect them by creating Projection instances: proj = Projection ( pre = pop1 , post = pop2 , target = \"exc\" , synapse = BCM ) pre is either the name of the pre-synaptic population or the corresponding Population object. post is either the name of the post-synaptic population or the corresponding Population object. target is the type of the connection. synapse is an optional argument requiring a Synapse instance. The post-synaptic neuron type must use sum(exc) in the rate-coded case respectively g_exc in the spiking case, otherwise the projection will be useless. If the synapse argument is omitted, the default synapse will be used: the default rate-coded synapse defines psp = w * pre.r , the default spiking synapse defines g_target += w .","title":"Declaring the projections"},{"location":"manual/Projections/#building-the-projections","text":"Creating the Projection objects only defines the information that two populations are connected. The synapses must be explicitely created by applying a connector method on the Projection object. To this end, ANNarchy already provides a set of predefined connector methods, but the user has also the possibility to define his own (see Connector ). The pattern can be applied either directly at the creation of the Projection: proj = Projection ( pre = pop1 , post = pop2 , target = \"exc\" , synapse = BCM ) . connect_all_to_all ( weights = 1.0 ) or afterwards: proj = Projection ( pre = pop1 , post = pop2 , target = \"exc\" , synapse = BCM ) proj . connect_all_to_all ( weights = 1.0 ) The connector method must be called before the network is compiled.","title":"Building the projections"},{"location":"manual/Projections/#projection-attributes","text":"Let's suppose the BCM synapse is used to create the Projection proj (spiking synapses are accessed similarly): BCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 100. : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0 \"\"\" )","title":"Projection attributes"},{"location":"manual/Projections/#global-attributes","text":"The global parameters and variables of a projection (i.e. defined with the postsynaptic or projection flags) can be accessed directly through attributes. Attributes defined with projection have a single value for the whole population: >>> proj . tau 100 Attributes defined with postsynaptic have one value per post-synaptic neuron, so the result is a vector: >>> proj . theta [ 3.575 , 15.987 , ... , 4.620 ] Post-synaptic variables can be modified by passing: a single value, which will be the same for all post-synaptic neurons. a list of values, with the same size as the number of neurons receiving synapses (for some sparse connectivity patterns, it may not be the same as the size of the population, so no multidimensional array is accepted). After compilation (and therefore creation of the synapses), you can access how many post-synaptic neurons receive actual synapses with: >>> proj . size 4 The list of ranks of the post-synaptic neurons receiving synapses is obtained with: >>> proj . post_ranks [ 0 , 1 , 2 , 3 ]","title":"Global attributes"},{"location":"manual/Projections/#local-attributes","text":"At the projection level Local attributes can also be accessed globally through attributes. It will return a list of lists containing the synapse-specific values. The first index represents the post-synaptic neurons. It has the same length as proj.post_ranks . Beware that if some post-synaptic neurons do not receive any connection, this index will not correspond to the ranksof the post-synaptic population. The second index addresses the pre-synaptic neurons. If the connection is sparse, it also is unrelated to the ranks of the pre-synaptic neurons in their populations. Warning Modifying these lists of lists is error-prone, so this method should be avoided if possible. At the post-synaptic level The local parameters and variables of a projection (synapse-specific) should better be accessed through the Dendrite object, which gathers for a single post-synaptic neuron all synapses belonging to the projection. Beware: As projections are only instantiated after the call to compile() , local attributes of a Projection are only available then. Trying to access them before compilation will lead to an error! Each dendrite stores the parameters and variables of the corresponding synapses as attributes, as populations do for neurons. You can loop over all post-synaptic neurons receiving synapses with the dendrites iterator: for dendrite in proj . dendrites : print dendrite . pre_ranks print dendrite . size print dendrite . tau print dendrite . alpha print dendrite . w dendrite.pre_ranks returns a list of pre-synaptic neuron ranks. dendrite.size returns the number of synapses for the considered post-synaptic neuron. Global parameters/variables return a single value ( dendrite.tau ) and local ones return a list ( dendrite.w ). You can even omit the .dendrites part of the iterator: for dendrite in proj : print dendrite . pre_ranks print dendrite . size print dendrite . tau print dendrite . alpha print dendrite . w You can also access the dendrites individually, either by specifying the rank of the post-synaptic neuron: dendrite = proj . dendrite ( 13 ) print dendrite . w or its coordinates: dendrite = proj . dendrite ( 5 , 5 ) print dendrite . w When using ranks, you can also directly address the projection as an array: dendrite = proj [ 13 ] print dendrite . w Warning You should make sure that the dendrite actually exists before accessing it through its rank, because it is otherwise a None object.","title":"Local attributes"},{"location":"manual/Projections/#functions","text":"If you have defined a function inside a Synapse definition: BCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 100. : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic dw/dt = eta * BCMRule(pre.r, post.r, theta) : min=0.0 \"\"\" , functions = \"\"\" BCMRule(pre, post, theta) = post * (post - theta) * pre \"\"\" ) you can use this function in Python as if it were a method of the corresponding object: proj = Projection ( pop1 , pop2 , 'exc' , BCM ) . connect_xxx () pre = np . linspace ( 0. , 1. , 100 ) post = np . linspace ( 0. , 1. , 100 ) theta = 0.01 * np . ones ( 100 ) weight_change = proj . BCMRule ( pre , post , theta ) You can pass either a list or a 1D Numpy array to each argument ( not a single value, nor a multidimensional array! ). The size of the arrays passed for each argument is arbitrary (it must not match the projection's size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.","title":"Functions"},{"location":"manual/Projections/#connecting-population-views","text":"Projections are usually understood as a connectivity pattern between two populations. Complex connectivity patterns have to specifically designed (see Connector ). In some cases, it can be much simpler to connect subsets of neurons directly, using built-in connector methods. To this end, the Projection object also accepts PopulationView objects for the pre and post arguments. Let's suppose we want to connect the (8,8) populations pop1 and pop2 in a all-to-all manner, but only for the (4,4) neurons in the center of these populations. The first step is to create the PopulationView objects using the slice operator: pop1_center = pop1 [ 2 : 7 , 2 : 7 ] pop2_center = pop2 [ 2 : 7 , 2 : 7 ] They can then be simply used to create a projection: proj = Projection ( pre = pop1_center , post = pop2_center , target = \"exc\" , synapse = BCM ) . connect_all_to_all ( weights = 1.0 ) Each neuron of pop2_center will receive synapses from all neurons of pop1_center , and only them. Neurons of pop2 which are not in pop2_center will not receive any synapse. Warning If you define your own connector method and want to use PopulationViews, you will need to iterate over the ranks attribute of the PopulationView object.","title":"Connecting population views"},{"location":"manual/Projections/#specifying-delays-in-synaptic-transmission","text":"By default, synaptic transmission is considered to be instantaneous (or more precisely, it takes one simulation step ( dt ) for a newly computed firing rate to be taken into account by post-synaptic neurons). In order to take longer propagation times into account in the transmission of information between two populations, one has the possibility to define synaptic delays for a projection. All the built-in connector methods take an argument delays (default= dt ), which can be a float (in milliseconds) or a random number generator. proj . connect_all_to_all ( weights = 1.0 , delays = 10.0 ) proj . connect_all_to_all ( weights = 1.0 , delays = Uniform ( 1.0 , 10.0 )) If the delay is not a multiple of the simulation time step ( dt = 1.0 by default), it will be rounded to the closest multiple. The same is true for the values returned by a random number generator. Note: Per design, the minimal possible delay is equal to dt : values smaller than dt will be replaced by dt . Negative values do not make any sense and are ignored. Warning Non-uniform delays are not available on CUDA.","title":"Specifying delays in synaptic transmission"},{"location":"manual/Projections/#controlling-projections","text":"Synaptic transmission, update and plasticity It is possible to selectively control synaptic transmission and plasticity at the projection level. The boolean flags transmission , update and plasticity can be set for that purpose: proj.transmission = False proj.update = False proj.plasticity = False If transmission is False , the projection is totally shut down: it does not transmit any information to the post-synaptic population (the corresponding weighted sums or conductances are constantly 0) and all synaptic variables are frozen to their current value (including the synaptic weights w ). If update is False , synaptic transmission occurs normally, but the synaptic variables are not updated. For spiking synapses, this includes traces when they are computed at each step, but not when they are integrated in an event-driven manner (flag event-driven ). Beware: continous synaptic transmission as in NMDA synapses will not work in this mode, as internal variables are not updated. If only plasticity is False , synaptic transmission and synaptic variable updates occur normally, but changes to the synaptic weight w are ignored. Disabling learning Alternatively, one can use the enable_learning() and disable_learning() methods of Projection . The effect of disable_learning() depends on the type of the projection: for rate-coded projections, disable_learning() is equivalent to update=False : no synaptic variables is updated. for spiking projections, it is equivalent to plasticity=False : only the weights are blocked. The reason of this difference is to allow continuous synaptic transmission and computation of traces. Calling enable_learning() without arguments resumes the default learning behaviour. Periodic learning enable_learning() also accepts two arguments period and offset . period defines the interval in ms between two evaluations of the synaptic variables. This can be useful when learning should only occur once at the end of a trial. It is recommended not to use ODEs in the equations in this case, as they are numerized according to a fixed time step. offset defines the time inside the period at which the evaluation should occur. By default, it is 0, so the variable updates will occur at the next step, then after period ms, and so on. Setting it to -1 will shift the update at the end of the period. Note that spiking synapses using online evaluation will not be affected by these parameters, as they are event-driven.","title":"Controlling projections"},{"location":"manual/Projections/#multiple-targets","text":"For spiking neurons, it may be desirable that a single synapses activates different currents (or conductances) in the post-synaptic neuron. One example are AMPA/NMDA synapses, where a single spike generates a \\\"classical\\\" AMPA current, plus a voltage-gated slower NMDA current. The following conductance-based Izhikevich is an example: RSNeuron = Neuron ( parameters = \"\"\" a = 0.02 b = 0.2 c = -65. d = 8. tau_ampa = 5. tau_nmda = 150. vrev = 0.0 \"\"\" , equations = \"\"\" I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v) dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init=-65., midpoint du/dt = a * (b*v - u) : init=-13. tau_ampa * dg_ampa/dt = -g_ampa tau_nmda * dg_nmda/dt = -g_nmda \"\"\" , spike = \"\"\" v >= 30. \"\"\" , reset = \"\"\" v = c u += d \"\"\" , functions = \"\"\" nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2) \"\"\" ) However, g_ampa and g_nmda collect by default spikes from different projections, so the weights will not be shared between the \\\"ampa\\\" projection and the \\\"nmda\\\" one. It is therefore possible to specify a list of targets when building a projection, meaning that a single pre-synaptic spike will increase both g_ampa and g_nmda from the same weight: proj = Projection(pop1, pop2, ['ampa', 'nmda'], STDP) An example is provided in examples/homeostatic_stdp/Ramp.py . Warning Multiple targets are not available on CUDA yet.","title":"Multiple targets"},{"location":"manual/RateNeuron/","text":"Rate-coded neurons # Defining parameters and variables # Let's consider first a simple rate-coded neuron of the leaky-integrator type, which simply integrates the weighted sum of its excitatory inputs: \\[ \\tau \\frac{d \\text{mp}(t)}{dt} = ( B - \\text{mp}(t) ) + \\sum_{i}^{\\text{exc}} \\text{r}_{i} * w_{i} \\] \\[r(t) = ( \\text{mp}(t) )^+\\] where \\(mp(t)\\) represents the membrane potential of the neuron, \\(\\tau\\) the time constant of the neuron, \\(B\\) its baseline firing rate, \\(\\text{r}(t)\\) its instantaneous firing rate, \\(i\\) an index over all excitatory synapses of this neuron, \\(w_i\\) the efficiency of the synapse with the pre-synaptic neuron of firing rate \\(\\text{r}_{i}\\) . It can be implemented in the ANNarchy framework with: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) \"\"\" ) The only required variable is r , which represents the instantaneous firing rate and will be used to propagate activity in the network. All other parameters and variables are freely decided by the user. Custom functions # Custom functions can also be defined when creating the Neuron type and used inside the equations field: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = sigmoid(mp) \"\"\" , functions = \"\"\" sigmoid(x) = 1.0 / (1.0 + exp(-x)) \"\"\" ) Make sure that the name of the function does not overlap with existing mathematical functions (cos, exp), existing variables (tau, r) or built-in functions (pos, t, dt). Predefined attributes # The ODE can depend on other parameters of the neuron (e.g. r depends on mp ), but not on unknown names. ANNarchy already defines the following variables and parameters for a neuron: variable t : time in milliseconds elapsed since the creation of the network. parameter dt : the discretization step, default is 1 ms. Weighted sum of inputs # The sum(target) term gives a direct access to the weighted sum of all inputs to the neuron having the corresponding target (see Projections to see how the target is defined). These inputs are organized in a data structure called Dendrite . It is possible to modify how weighted sums are computed when creating a rate-coded synapse . Note The connection type, e.g. exc or inh , needs to match with the names used as a target parameter when creating a Projection . If such a projection does not exist when the network is compiled, the weighted sum will be set to 0.0 for all neurons. Using only sum() in the equations sums over all defined targets. For example, if two projections with targets \"exc\" and \"inh\" reach a neuron, sum() is equivalent to sum(exc) + sum(inh) . Inhibitory weights must then be defined as negative. Global operations # One has the possibility to use global operations on the population inside the neuron definition, such as the maximal activity in the population. One only needs to use one of the following operations: min(v) for the minimum: \\(\\min_i v_i\\) , max(v) for the maximum: \\(\\max_i v_i\\) , mean(v) for the mean: \\(\\frac{1}{N} \\sum_i v_i\\) , norm1(v) for the L1-norm: \\(\\frac{1}{N} \\sum_i |v_i|\\) , norm2(v) for the L2-norm: \\(\\frac{1}{N} \\sum_i v_i^2\\) Example where neurons react to their inputs only where they exceed the mean over the population: WTANeuron = Neuron( parameters=\"\"\" tau = 10.0 \"\"\", equations = \"\"\" input = sum(exc) tau * dr/dt + r = pos(input - mean(input)) \"\"\" ) Note The global operations are computed using values at the previous time step (like weighted sums), not in the step currently evaluated. There is therefore implicitely a delay of dt , but it cannot be changed.","title":"Rate-coded neurons"},{"location":"manual/RateNeuron/#rate-coded-neurons","text":"","title":"Rate-coded neurons"},{"location":"manual/RateNeuron/#defining-parameters-and-variables","text":"Let's consider first a simple rate-coded neuron of the leaky-integrator type, which simply integrates the weighted sum of its excitatory inputs: \\[ \\tau \\frac{d \\text{mp}(t)}{dt} = ( B - \\text{mp}(t) ) + \\sum_{i}^{\\text{exc}} \\text{r}_{i} * w_{i} \\] \\[r(t) = ( \\text{mp}(t) )^+\\] where \\(mp(t)\\) represents the membrane potential of the neuron, \\(\\tau\\) the time constant of the neuron, \\(B\\) its baseline firing rate, \\(\\text{r}(t)\\) its instantaneous firing rate, \\(i\\) an index over all excitatory synapses of this neuron, \\(w_i\\) the efficiency of the synapse with the pre-synaptic neuron of firing rate \\(\\text{r}_{i}\\) . It can be implemented in the ANNarchy framework with: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) \"\"\" ) The only required variable is r , which represents the instantaneous firing rate and will be used to propagate activity in the network. All other parameters and variables are freely decided by the user.","title":"Defining parameters and variables"},{"location":"manual/RateNeuron/#custom-functions","text":"Custom functions can also be defined when creating the Neuron type and used inside the equations field: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = sigmoid(mp) \"\"\" , functions = \"\"\" sigmoid(x) = 1.0 / (1.0 + exp(-x)) \"\"\" ) Make sure that the name of the function does not overlap with existing mathematical functions (cos, exp), existing variables (tau, r) or built-in functions (pos, t, dt).","title":"Custom functions"},{"location":"manual/RateNeuron/#predefined-attributes","text":"The ODE can depend on other parameters of the neuron (e.g. r depends on mp ), but not on unknown names. ANNarchy already defines the following variables and parameters for a neuron: variable t : time in milliseconds elapsed since the creation of the network. parameter dt : the discretization step, default is 1 ms.","title":"Predefined attributes"},{"location":"manual/RateNeuron/#weighted-sum-of-inputs","text":"The sum(target) term gives a direct access to the weighted sum of all inputs to the neuron having the corresponding target (see Projections to see how the target is defined). These inputs are organized in a data structure called Dendrite . It is possible to modify how weighted sums are computed when creating a rate-coded synapse . Note The connection type, e.g. exc or inh , needs to match with the names used as a target parameter when creating a Projection . If such a projection does not exist when the network is compiled, the weighted sum will be set to 0.0 for all neurons. Using only sum() in the equations sums over all defined targets. For example, if two projections with targets \"exc\" and \"inh\" reach a neuron, sum() is equivalent to sum(exc) + sum(inh) . Inhibitory weights must then be defined as negative.","title":"Weighted sum of inputs"},{"location":"manual/RateNeuron/#global-operations","text":"One has the possibility to use global operations on the population inside the neuron definition, such as the maximal activity in the population. One only needs to use one of the following operations: min(v) for the minimum: \\(\\min_i v_i\\) , max(v) for the maximum: \\(\\max_i v_i\\) , mean(v) for the mean: \\(\\frac{1}{N} \\sum_i v_i\\) , norm1(v) for the L1-norm: \\(\\frac{1}{N} \\sum_i |v_i|\\) , norm2(v) for the L2-norm: \\(\\frac{1}{N} \\sum_i v_i^2\\) Example where neurons react to their inputs only where they exceed the mean over the population: WTANeuron = Neuron( parameters=\"\"\" tau = 10.0 \"\"\", equations = \"\"\" input = sum(exc) tau * dr/dt + r = pos(input - mean(input)) \"\"\" ) Note The global operations are computed using values at the previous time step (like weighted sums), not in the step currently evaluated. There is therefore implicitely a delay of dt , but it cannot be changed.","title":"Global operations"},{"location":"manual/RateSynapse/","text":"Rate-coded synapses # As for neurons, you can define the synaptic behavior using a Synapse object. Although the description is local to a synapse, the same ODE will be applied to all synapses of a given Projection from one population to another. The same vocabulary as for neurons is accessible (constants, functions, conditional statements), except that the synapse must distinguish pre-synaptic and post-synaptic parameters/variables. Like r for a rate-coded neuron, one variable is special for a rate-coded synapse: w represents the synaptic efficiency (or the weight of the connection). If an ODE is defined for this variable, this will implement a learning rule. If none is provided, the synapse is non-plastic. The ODEs for synaptic variables follow the same syntax as for neurons. As for neurons, the following variables are already defined: t : time in milliseconds elapsed since the creation of the network. dt : the discretization step is 1.0ms by default. Synaptic plasticity # Learning is possible by modifying the variable w of a single synapse during the simulation. For example, the Oja learning rule (see the example Bar learning ): \\[\\tau \\frac{d w(t)}{dt} = r_\\text{pre} * r_\\text{post} - \\alpha * r_\\text{post}^2 * w(t)\\] could be implemented this way: Oja = Synapse ( parameters = \"\"\" tau = 5000 alpha = 8.0 \"\"\" , equations = \"\"\" tau * dw / dt = pre.r * post.r - alpha * post.r^2 * w \"\"\" ) Note that it is equivalent to define the increment directly if you want to apply the explicit Euler method: equations = \"\"\" w += dt / tau * ( pre.r * post.r - alpha * post.r^2 * w) \"\"\" The same vocabulary as for rate-coded neurons applies. Custom functions can also be defined: Oja = Synapse ( parameters = \"\"\" tau = 5000 alpha = 8.0 \"\"\" , equations = \"\"\" tau * dw / dt = product(pre.r, post.r) - alpha * post.r^2 * w \"\"\" , functions = \"\"\" product(x,y) = x * y \"\"\" , ) Neuron-specific variables # A synapse needs to access neural variables both at the pre- and post-synaptic levels. For the pre-synaptic neuron, biologically realistic synapses should only need its firing rate, but in some cases it may be useful to access other variables as well. In order to use neural variables in a synaptic variable, you have to prefix them with pre. or post. . For example: pre . r , post . baseline , post . mp ... ANNarchy will check before the compilation that the pre- or post-synaptic neuron types indeed define such variables. Note If the projection uses delays, all pre-synaptic variables used in the synapse model will be delayed. Locality # There are 3 levels of locality for a synaptic parameter or variable: synaptic : there is one value per synapse in the projection (default). postsynaptic : there is one value per post-synaptic neuron in the projection. projection : there is only one value for the whole projection. The following BCM learning rule makes use of the three levels of locality: BCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 100. : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0 \"\"\" ) eta and tau are global parameters to the projection: all synapses will use the same value. theta defines one value per post-synaptic neuron: it tracks the average of the post-synaptic firing rate. There is therefore no need to update one value per synapse, so we can use the flag \\\"postsynaptic\\\". Naturally, w is local to each synapse, so no locality flag should be passed. Global operations # Some learning rules require global information about the pre- or post-synaptic population, which is not local to the synapse, such as the mean or maximal activity in the pre-synaptic population. This information can be accessed at the synapse-level. The special functions: min(v) for the minimum: \\(\\min_i v_i\\) , max(v) for the maximum: \\(\\max_i v_i\\) , mean(v) for the mean: \\(\\frac{1}{N} \\sum_i v_i\\) , norm1(v) for the L1-norm: \\(\\frac{1}{N} \\sum_i |v_i|\\) , norm2(v) for the L2-norm: \\(\\frac{1}{N} \\sum_i v_i^2\\) are available for any pre- or post-synaptic variable. For example, some covariance-based learning rules depend on the mean firing in the pre- and post-synaptic populations: \\[\\tau \\frac{d w(t)}{dt} = (r_\\text{pre} - \\hat{r}_\\text{pre} ) * (r_\\text{post} - \\hat{r}_\\text{post} )\\] Using the global operations, such a learning rule is trivial to implement: Covariance = Synapse ( parameters = \"\"\" tau = 5000.0 \"\"\" , equations = \"\"\" tau * dw/dt = (pre.r - mean(pre.r) ) * (post.r - mean(post.r) ) \"\"\" ) Warning Such global operations can become expensive to compute if the populations are too big. The global operations are performed over the whole population, not only the synapses which actually reach the post-synaptic neuron. They can only be applied to a single variable, not a combination or function of them. Defining the post-synaptic potential (psp) # The argument psp of a Synapse object represents the post-synaptic potential evoked by the pre-synaptic neuron. This value is actually summed by the post-synaptic neuron over all other synapses of the same projection in sum(target) . If not defined, it will simply represent the product between the pre-synaptic firing rate ( pre.r ) and the weight value ( w ). The post-synaptic potential of a single synapse is by default: psp = w * pre . r where pre.r is the pre-synaptic firing rate, but you may want to override this behaviour in certain cases. For example, you may want to model a non-linear synapse with a logarithmic term: \\[r_{i} = \\sum_j log \\left( \\frac {( r_{j} * w_{ij} ) + 1 } { ( r_{j} * w_{ij} ) - 1 } \\right)\\] In this case, you can just modify the psp argument of the synapse: NonLinearSynapse = Synapse ( psp = \"\"\" log( (pre.r * w + 1 ) / (pre.r * w - 1) ) \"\"\" ) No further modification has to be done in the post-synaptic neuron, this value will be summed over all pre-synaptic neurons automatically when using sum(target) . Defining the post-synaptic operation # By default, a post-synaptic neuron calling sum(target) will compute the sum over all incoming synapses of their defined psp : \\[\\text{sum(exc)} = \\sum_{i \\in \\text{exc}} \\text{psp}(i) = \\sum_{i \\in \\text{exc}} w_i * \\text{pre}.r_i\\] It is possible to define a different operation performed on the connected synapses, using the operation argument of the synapse: MaxPooling = Synapse ( psp = \"w * pre.r\" , operation = \"max\" ) In this case, sum(target) will represent the maximum value of w * pre.r over all incoming synapses, not their sum. It can be useful when defining pooling operations in a convolutional network, for example. The available operations are: \"sum\" : (default): sum of all incoming psps. \"max\" : maximum of all incoming psps. \"min\" : minimum of all incoming psps. \"mean\" : mean of all incoming psps. Warning These operations are only possible for rate-coded synapses.","title":"Rate-coded synapses"},{"location":"manual/RateSynapse/#rate-coded-synapses","text":"As for neurons, you can define the synaptic behavior using a Synapse object. Although the description is local to a synapse, the same ODE will be applied to all synapses of a given Projection from one population to another. The same vocabulary as for neurons is accessible (constants, functions, conditional statements), except that the synapse must distinguish pre-synaptic and post-synaptic parameters/variables. Like r for a rate-coded neuron, one variable is special for a rate-coded synapse: w represents the synaptic efficiency (or the weight of the connection). If an ODE is defined for this variable, this will implement a learning rule. If none is provided, the synapse is non-plastic. The ODEs for synaptic variables follow the same syntax as for neurons. As for neurons, the following variables are already defined: t : time in milliseconds elapsed since the creation of the network. dt : the discretization step is 1.0ms by default.","title":"Rate-coded synapses"},{"location":"manual/RateSynapse/#synaptic-plasticity","text":"Learning is possible by modifying the variable w of a single synapse during the simulation. For example, the Oja learning rule (see the example Bar learning ): \\[\\tau \\frac{d w(t)}{dt} = r_\\text{pre} * r_\\text{post} - \\alpha * r_\\text{post}^2 * w(t)\\] could be implemented this way: Oja = Synapse ( parameters = \"\"\" tau = 5000 alpha = 8.0 \"\"\" , equations = \"\"\" tau * dw / dt = pre.r * post.r - alpha * post.r^2 * w \"\"\" ) Note that it is equivalent to define the increment directly if you want to apply the explicit Euler method: equations = \"\"\" w += dt / tau * ( pre.r * post.r - alpha * post.r^2 * w) \"\"\" The same vocabulary as for rate-coded neurons applies. Custom functions can also be defined: Oja = Synapse ( parameters = \"\"\" tau = 5000 alpha = 8.0 \"\"\" , equations = \"\"\" tau * dw / dt = product(pre.r, post.r) - alpha * post.r^2 * w \"\"\" , functions = \"\"\" product(x,y) = x * y \"\"\" , )","title":"Synaptic plasticity"},{"location":"manual/RateSynapse/#neuron-specific-variables","text":"A synapse needs to access neural variables both at the pre- and post-synaptic levels. For the pre-synaptic neuron, biologically realistic synapses should only need its firing rate, but in some cases it may be useful to access other variables as well. In order to use neural variables in a synaptic variable, you have to prefix them with pre. or post. . For example: pre . r , post . baseline , post . mp ... ANNarchy will check before the compilation that the pre- or post-synaptic neuron types indeed define such variables. Note If the projection uses delays, all pre-synaptic variables used in the synapse model will be delayed.","title":"Neuron-specific variables"},{"location":"manual/RateSynapse/#locality","text":"There are 3 levels of locality for a synaptic parameter or variable: synaptic : there is one value per synapse in the projection (default). postsynaptic : there is one value per post-synaptic neuron in the projection. projection : there is only one value for the whole projection. The following BCM learning rule makes use of the three levels of locality: BCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 100. : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0 \"\"\" ) eta and tau are global parameters to the projection: all synapses will use the same value. theta defines one value per post-synaptic neuron: it tracks the average of the post-synaptic firing rate. There is therefore no need to update one value per synapse, so we can use the flag \\\"postsynaptic\\\". Naturally, w is local to each synapse, so no locality flag should be passed.","title":"Locality"},{"location":"manual/RateSynapse/#global-operations","text":"Some learning rules require global information about the pre- or post-synaptic population, which is not local to the synapse, such as the mean or maximal activity in the pre-synaptic population. This information can be accessed at the synapse-level. The special functions: min(v) for the minimum: \\(\\min_i v_i\\) , max(v) for the maximum: \\(\\max_i v_i\\) , mean(v) for the mean: \\(\\frac{1}{N} \\sum_i v_i\\) , norm1(v) for the L1-norm: \\(\\frac{1}{N} \\sum_i |v_i|\\) , norm2(v) for the L2-norm: \\(\\frac{1}{N} \\sum_i v_i^2\\) are available for any pre- or post-synaptic variable. For example, some covariance-based learning rules depend on the mean firing in the pre- and post-synaptic populations: \\[\\tau \\frac{d w(t)}{dt} = (r_\\text{pre} - \\hat{r}_\\text{pre} ) * (r_\\text{post} - \\hat{r}_\\text{post} )\\] Using the global operations, such a learning rule is trivial to implement: Covariance = Synapse ( parameters = \"\"\" tau = 5000.0 \"\"\" , equations = \"\"\" tau * dw/dt = (pre.r - mean(pre.r) ) * (post.r - mean(post.r) ) \"\"\" ) Warning Such global operations can become expensive to compute if the populations are too big. The global operations are performed over the whole population, not only the synapses which actually reach the post-synaptic neuron. They can only be applied to a single variable, not a combination or function of them.","title":"Global operations"},{"location":"manual/RateSynapse/#defining-the-post-synaptic-potential-psp","text":"The argument psp of a Synapse object represents the post-synaptic potential evoked by the pre-synaptic neuron. This value is actually summed by the post-synaptic neuron over all other synapses of the same projection in sum(target) . If not defined, it will simply represent the product between the pre-synaptic firing rate ( pre.r ) and the weight value ( w ). The post-synaptic potential of a single synapse is by default: psp = w * pre . r where pre.r is the pre-synaptic firing rate, but you may want to override this behaviour in certain cases. For example, you may want to model a non-linear synapse with a logarithmic term: \\[r_{i} = \\sum_j log \\left( \\frac {( r_{j} * w_{ij} ) + 1 } { ( r_{j} * w_{ij} ) - 1 } \\right)\\] In this case, you can just modify the psp argument of the synapse: NonLinearSynapse = Synapse ( psp = \"\"\" log( (pre.r * w + 1 ) / (pre.r * w - 1) ) \"\"\" ) No further modification has to be done in the post-synaptic neuron, this value will be summed over all pre-synaptic neurons automatically when using sum(target) .","title":"Defining the post-synaptic potential (psp)"},{"location":"manual/RateSynapse/#defining-the-post-synaptic-operation","text":"By default, a post-synaptic neuron calling sum(target) will compute the sum over all incoming synapses of their defined psp : \\[\\text{sum(exc)} = \\sum_{i \\in \\text{exc}} \\text{psp}(i) = \\sum_{i \\in \\text{exc}} w_i * \\text{pre}.r_i\\] It is possible to define a different operation performed on the connected synapses, using the operation argument of the synapse: MaxPooling = Synapse ( psp = \"w * pre.r\" , operation = \"max\" ) In this case, sum(target) will represent the maximum value of w * pre.r over all incoming synapses, not their sum. It can be useful when defining pooling operations in a convolutional network, for example. The available operations are: \"sum\" : (default): sum of all incoming psps. \"max\" : maximum of all incoming psps. \"min\" : minimum of all incoming psps. \"mean\" : mean of all incoming psps. Warning These operations are only possible for rate-coded synapses.","title":"Defining the post-synaptic operation"},{"location":"manual/Recording/","text":"Recording with Monitors # Between two calls to simulate() , all neural and synaptic variables can be accessed through the generated attributes. The evolution of neural or synaptic variables during a simulation phase can be selectively recorded using Monitor objects. The Monitor object can be created at any time (before or after compile() ) to record any variable of a Population , PopulationView , Dendrite or Projection . Note The value of each variable is stored for every simulation step in the RAM. For huge networks and long simulations, this can very rapidly fill up the available memory and lead to cache defaults, thereby degrading strongly the performance. It is the user's responsability to record only the needed variables and to regularly save the values in a file. Neural variables # The Monitor object takes four arguments: obj : the object to monitor. It can be a population, a population view (a slice of a population or an individual neuron), a dendrite (the synapses of a projection which reach a single post-synaptic neuron) or a projection. variables : a (list of) variable name(s) which should be recorded. They should be variables of the neuron/synapse model of the corresponding object. Although it generally makes no sense, you can also record parameters of an object. By definition a parameter is constant throughout a simulation, but it maybe useful when tracking externally-set inputs, for example. You can know which attributes are recordable by checking the attributes attribute of the object ( pop.attributes or proj.attributes ). period : the period in ms at which recordings should be made. By default, recording is done after each simulation step ( dt ), but this may be overkill in long simulations. start : boolean value stating if the recordings should start immediately after the creation of the monitor (default), or if it should be started later. Some examples: m = Monitor ( pop , 'r' ) # record r in all neurons of pop m = Monitor ( pop , [ 'r' , 'v' ]) # record r and v of all neurons m = Monitor ( pop [: 100 ], 'r' , period = 10.0 ) # record r in the first 100 neurons of pop, every 10 ms m = Monitor ( pop , 'r' , start = False ) # record r in all neurons, but do not start recording Spiking networks additionally allow to record the spike events in a population (see later). You also can record conductances (e.g. g_exc ) and weighted sums of inputs in rate-coded networks ( sum(exc) ) the same way: m = Monitor ( pop , [ 'spike' , 'g_exc' , 'g_inh' ]) m = Monitor ( pop , [ 'r' , 'sum(exc)' , 'sum(inh)' ]) Starting the recordings # If start is set to False , recordings can be started later by calling the start() method: m = Monitor ( pop , 'r' , start = False ) simulate ( 100. ) m . start () simulate ( 100. ) In this case, only the last 100 ms of the simulation will be recorded. Otherwise, recording would start immediately after the creation of the object. Pausing/resuming the recordings # If you are interested in recording only specific periods of the simulation, you can ause and resume recordings: m = Monitor ( pop , 'r' ) simulate ( 100. ) m . pause () simulate ( 1000. ) m . resume () simulate ( 100. ) In this example, only the first and last 100 ms of the simulation are recorded. Retrieving the recordings # The recorded values are obtained through the get() method. If no argument is passed, a dictionary is returned with one element per recorded variable. If the name of a variable is passed (for example get('r') ), the recorded values for this variable are directly returned: m = Monitor ( pop , [ 'r' , 'v' ]) simulate ( 100. ) data = m . get () simulate ( 100. ) r = m . get ( 'r' ) v = m . get ( 'v' ) In the example above, data is a dictionary with two keys 'r' and 'v' , while r and v are directly the recorded arrays. The recorded values are Numpy arrays with two dimensions, the first one representing time , the second one representing the ranks of the recorded neurons. For example, the time course of the firing rate of the neuron of rank 15 is accessed through: data [ 'r' ][:, 15 ] The firing rates of the whole population after 50 ms of simulation are accessed with: data [ 'r' ][ 50 , :] Note Once you call get() , the internal data is erased, so calling it immediately afterwards will return an empty recording data. You need to simulate again in order to retrieve new values. Representation of time The time indices are in simulation steps (integers), not in real time (ms). If dt is different from 1.0, this indices must be multiplied by dt() in order to plot real times: setup ( dt = 0.1 ) # ... m = Monitor ( pop , 'r' ) simulate ( 100. ) r = m . get ( 'r' ) plt . plot ( dt () * np . arange ( 100 ), r [:, 15 ]) If recordings used the pause() and resume() methods, get() returns only one array with all recordings concatenated. You can access the steps at which the recording started or paused with the times() method: m = Monitor ( pop , 'r' ) simulate ( 100. ) m . pause () simulate ( 1000. ) m . resume () simulate ( 100. ) r = m . get ( 'r' ) # A (200, N) Numpy array print ( m . times ()) # {'start': [0, 1100], 'stop': [100, 1200]} Special case for spiking neurons # Any variable defined in the neuron type can be recorded. An exception for spiking neurons is the spike variable itself, which is never explicitely defined in the neuron type but can be recorded: m = Monitor ( pop , [ 'v' , 'spike' ]) Unlike other variables, the binary value of spike is not recorded at each time step, which would lead to very sparse matrices, but only the times (in steps, not milliseconds) at which spikes actually occur. As each neuron fires differently (so each neuron will have recorded spikes of different lengths), get() in this case does not return a Numpy array, but a dictionary associating to each recorded neuron a list of spike times: m = Monitor ( pop , [ 'v' , 'spike' ]) simulate ( 100.0 ) data = m . get ( 'spike' ) print ( data [ 0 ]) # [23, 76, 98] In the example above, the neuron of rank 0 has spiked 3 times (at t = 23, 76 and 98 ms if dt = 1.0 ) during the first 100 ms of the simulation. Raster plots In order to easily display raster plots, the method raster_plot() is provided to transform this data into an easily plottable format: spike_times , ranks = m . raster_plot ( data ) plt . plot ( spike_times , ranks , '.' ) raster_plot() returns two Numpy arrays, whose length is the total number of spikes emitted during the simulation. The first array contains the spike times (\u00edn ms) while the second contains the ranks of the neurons who fired. They can be directly used t produce the raster plot with Matplotlib. An example of the use of raster_plot() can be seen in the Izhikevich pulse network section. Mean firing rate The mean firing rate in the population can be easily calculated using the length of the arrays returned by raster_plot : N = 1000 # number of neurons duration = 500. # duration of the simulation data = m . get ( 'spike' ) spike_times , ranks = m . raster_plot ( data ) print ( 'Mean firing rate:' , len ( spike_times ) / float ( N ) / duration * 1000. , 'Hz.' ) For convenience, this value is returned by the mean_fr() method, which has access to the number of recorded neurons and the duration of the recordings: print ( 'Mean firing rate:' , m . mean_fr ( data ), 'Hz.' ) Firing rates Another useful method is smoothed_rate() . It allows to display the instantaneous firing rate of each neuron based on the spike recordings: rates = m . smoothed_rate ( data ) plt . imshow ( rates , aspect = 'auto' ) For each neuron, it returns an array with the instantaneous firing rate during the whole simulation. The instantaneous firing rate is computed by inverting the inter-spike interval (ISI) between two consecutive spikes, and assigning it to all simulation steps between the two spikes. As this value can be quite fluctuating, a smooth argument in milliseconds can be passed to smoothed_rate() to apply a low-pass filter on the firing rates: rates = m . smoothed_rate ( data , smooth = 200.0 ) plt . imshow ( rates , aspect = 'auto' ) A smoothed firing rate for the whole population is also accessible through population_rate() : fr = m . population_rate ( data , smooth = 200.0 ) Histogram histogram() allows to count the spikes emitted in the whole population during successive bins of the recording duration: histo = m . histogram ( data , bins = 1.0 ) plt . plot ( histo ) bins represents the size of each bin, here 1 ms. By default, the bin size is dt . Note : the methods to analyse the spike patterns are also available outside the monitors. For example if you save the spike recordings into a file using numpy: spikes = m . get ( 'spike' ) np . save ( 'spikes.npy' , spikes ) you can analyze them in a separate file like this: # Load the data spikes = np . load ( 'spikes.npy' ) . item () # Compute the raster plot t , n = raster_plot ( spikes ) # Compute the population firing rate fr = histogram ( spikes , bins = 1. ) # Smoothed firing rate sr = smoothed_rate ( spikes , smooth = 10.0 ) # Population firing rate pr = population_rate ( spikes , smooth = 10.0 ) # Global firing rate mfr = mean_fr ( spikes ) Synaptic variables # Recording of synaptic variables such as weights w during learning is also possible using the monitor object. However, it can very easily lead to important memory consumption. Let's suppose we have a network composed of two populations of 1000 neurons each, fully connected: each neuron of the second population receives 1000 synapses. This makes a total of 1 million synapses for the projection and, supposing the weights w use the double floating precision, requires 4 MB of memory. If you record w during a simulation of 1 second (1000 steps, with dt=1.0 ), the total added memory consumption would already be around 4GB. To avoid fast memory fills, you should either 1) record the projection variables infrequently (by setting the period argument of the Monitor), or 2) selectively record particular dendrites. The corresponding dendrite should be simply passed to the monitor: dendrite = proj . dendrite ( 12 ) # or simply proj[12] m = Monitor ( dendrite , 'w' ) simulate ( 1000.0 ) data = m . get ( 'w' ) The Monitor object has the same start() , pause() , resume() and get() methods as for populations. get() returns also 2D Numpy arrays, the first index being time, the second being the index of the synapse. To know to which pre-synaptic neuron it corresponds, use the pre_ranks attribute of the dendrite: dendrite . pre_ranks # [0, 3, 12, ..] To record a complete projection, simply pass it to the Monitor: m = Monitor ( proj , 'w' , period = 1000. ) simulate ( 10000.0 ) data = m . get ( 'w' ) One last time, do not record all weights of a projection at each time step! Warning Recording synaptic variables with CUDA is not available.","title":"Recording with Monitors"},{"location":"manual/Recording/#recording-with-monitors","text":"Between two calls to simulate() , all neural and synaptic variables can be accessed through the generated attributes. The evolution of neural or synaptic variables during a simulation phase can be selectively recorded using Monitor objects. The Monitor object can be created at any time (before or after compile() ) to record any variable of a Population , PopulationView , Dendrite or Projection . Note The value of each variable is stored for every simulation step in the RAM. For huge networks and long simulations, this can very rapidly fill up the available memory and lead to cache defaults, thereby degrading strongly the performance. It is the user's responsability to record only the needed variables and to regularly save the values in a file.","title":"Recording with Monitors"},{"location":"manual/Recording/#neural-variables","text":"The Monitor object takes four arguments: obj : the object to monitor. It can be a population, a population view (a slice of a population or an individual neuron), a dendrite (the synapses of a projection which reach a single post-synaptic neuron) or a projection. variables : a (list of) variable name(s) which should be recorded. They should be variables of the neuron/synapse model of the corresponding object. Although it generally makes no sense, you can also record parameters of an object. By definition a parameter is constant throughout a simulation, but it maybe useful when tracking externally-set inputs, for example. You can know which attributes are recordable by checking the attributes attribute of the object ( pop.attributes or proj.attributes ). period : the period in ms at which recordings should be made. By default, recording is done after each simulation step ( dt ), but this may be overkill in long simulations. start : boolean value stating if the recordings should start immediately after the creation of the monitor (default), or if it should be started later. Some examples: m = Monitor ( pop , 'r' ) # record r in all neurons of pop m = Monitor ( pop , [ 'r' , 'v' ]) # record r and v of all neurons m = Monitor ( pop [: 100 ], 'r' , period = 10.0 ) # record r in the first 100 neurons of pop, every 10 ms m = Monitor ( pop , 'r' , start = False ) # record r in all neurons, but do not start recording Spiking networks additionally allow to record the spike events in a population (see later). You also can record conductances (e.g. g_exc ) and weighted sums of inputs in rate-coded networks ( sum(exc) ) the same way: m = Monitor ( pop , [ 'spike' , 'g_exc' , 'g_inh' ]) m = Monitor ( pop , [ 'r' , 'sum(exc)' , 'sum(inh)' ])","title":"Neural variables"},{"location":"manual/Recording/#starting-the-recordings","text":"If start is set to False , recordings can be started later by calling the start() method: m = Monitor ( pop , 'r' , start = False ) simulate ( 100. ) m . start () simulate ( 100. ) In this case, only the last 100 ms of the simulation will be recorded. Otherwise, recording would start immediately after the creation of the object.","title":"Starting the recordings"},{"location":"manual/Recording/#pausingresuming-the-recordings","text":"If you are interested in recording only specific periods of the simulation, you can ause and resume recordings: m = Monitor ( pop , 'r' ) simulate ( 100. ) m . pause () simulate ( 1000. ) m . resume () simulate ( 100. ) In this example, only the first and last 100 ms of the simulation are recorded.","title":"Pausing/resuming the recordings"},{"location":"manual/Recording/#retrieving-the-recordings","text":"The recorded values are obtained through the get() method. If no argument is passed, a dictionary is returned with one element per recorded variable. If the name of a variable is passed (for example get('r') ), the recorded values for this variable are directly returned: m = Monitor ( pop , [ 'r' , 'v' ]) simulate ( 100. ) data = m . get () simulate ( 100. ) r = m . get ( 'r' ) v = m . get ( 'v' ) In the example above, data is a dictionary with two keys 'r' and 'v' , while r and v are directly the recorded arrays. The recorded values are Numpy arrays with two dimensions, the first one representing time , the second one representing the ranks of the recorded neurons. For example, the time course of the firing rate of the neuron of rank 15 is accessed through: data [ 'r' ][:, 15 ] The firing rates of the whole population after 50 ms of simulation are accessed with: data [ 'r' ][ 50 , :] Note Once you call get() , the internal data is erased, so calling it immediately afterwards will return an empty recording data. You need to simulate again in order to retrieve new values. Representation of time The time indices are in simulation steps (integers), not in real time (ms). If dt is different from 1.0, this indices must be multiplied by dt() in order to plot real times: setup ( dt = 0.1 ) # ... m = Monitor ( pop , 'r' ) simulate ( 100. ) r = m . get ( 'r' ) plt . plot ( dt () * np . arange ( 100 ), r [:, 15 ]) If recordings used the pause() and resume() methods, get() returns only one array with all recordings concatenated. You can access the steps at which the recording started or paused with the times() method: m = Monitor ( pop , 'r' ) simulate ( 100. ) m . pause () simulate ( 1000. ) m . resume () simulate ( 100. ) r = m . get ( 'r' ) # A (200, N) Numpy array print ( m . times ()) # {'start': [0, 1100], 'stop': [100, 1200]}","title":"Retrieving the recordings"},{"location":"manual/Recording/#special-case-for-spiking-neurons","text":"Any variable defined in the neuron type can be recorded. An exception for spiking neurons is the spike variable itself, which is never explicitely defined in the neuron type but can be recorded: m = Monitor ( pop , [ 'v' , 'spike' ]) Unlike other variables, the binary value of spike is not recorded at each time step, which would lead to very sparse matrices, but only the times (in steps, not milliseconds) at which spikes actually occur. As each neuron fires differently (so each neuron will have recorded spikes of different lengths), get() in this case does not return a Numpy array, but a dictionary associating to each recorded neuron a list of spike times: m = Monitor ( pop , [ 'v' , 'spike' ]) simulate ( 100.0 ) data = m . get ( 'spike' ) print ( data [ 0 ]) # [23, 76, 98] In the example above, the neuron of rank 0 has spiked 3 times (at t = 23, 76 and 98 ms if dt = 1.0 ) during the first 100 ms of the simulation. Raster plots In order to easily display raster plots, the method raster_plot() is provided to transform this data into an easily plottable format: spike_times , ranks = m . raster_plot ( data ) plt . plot ( spike_times , ranks , '.' ) raster_plot() returns two Numpy arrays, whose length is the total number of spikes emitted during the simulation. The first array contains the spike times (\u00edn ms) while the second contains the ranks of the neurons who fired. They can be directly used t produce the raster plot with Matplotlib. An example of the use of raster_plot() can be seen in the Izhikevich pulse network section. Mean firing rate The mean firing rate in the population can be easily calculated using the length of the arrays returned by raster_plot : N = 1000 # number of neurons duration = 500. # duration of the simulation data = m . get ( 'spike' ) spike_times , ranks = m . raster_plot ( data ) print ( 'Mean firing rate:' , len ( spike_times ) / float ( N ) / duration * 1000. , 'Hz.' ) For convenience, this value is returned by the mean_fr() method, which has access to the number of recorded neurons and the duration of the recordings: print ( 'Mean firing rate:' , m . mean_fr ( data ), 'Hz.' ) Firing rates Another useful method is smoothed_rate() . It allows to display the instantaneous firing rate of each neuron based on the spike recordings: rates = m . smoothed_rate ( data ) plt . imshow ( rates , aspect = 'auto' ) For each neuron, it returns an array with the instantaneous firing rate during the whole simulation. The instantaneous firing rate is computed by inverting the inter-spike interval (ISI) between two consecutive spikes, and assigning it to all simulation steps between the two spikes. As this value can be quite fluctuating, a smooth argument in milliseconds can be passed to smoothed_rate() to apply a low-pass filter on the firing rates: rates = m . smoothed_rate ( data , smooth = 200.0 ) plt . imshow ( rates , aspect = 'auto' ) A smoothed firing rate for the whole population is also accessible through population_rate() : fr = m . population_rate ( data , smooth = 200.0 ) Histogram histogram() allows to count the spikes emitted in the whole population during successive bins of the recording duration: histo = m . histogram ( data , bins = 1.0 ) plt . plot ( histo ) bins represents the size of each bin, here 1 ms. By default, the bin size is dt . Note : the methods to analyse the spike patterns are also available outside the monitors. For example if you save the spike recordings into a file using numpy: spikes = m . get ( 'spike' ) np . save ( 'spikes.npy' , spikes ) you can analyze them in a separate file like this: # Load the data spikes = np . load ( 'spikes.npy' ) . item () # Compute the raster plot t , n = raster_plot ( spikes ) # Compute the population firing rate fr = histogram ( spikes , bins = 1. ) # Smoothed firing rate sr = smoothed_rate ( spikes , smooth = 10.0 ) # Population firing rate pr = population_rate ( spikes , smooth = 10.0 ) # Global firing rate mfr = mean_fr ( spikes )","title":"Special case for spiking neurons"},{"location":"manual/Recording/#synaptic-variables","text":"Recording of synaptic variables such as weights w during learning is also possible using the monitor object. However, it can very easily lead to important memory consumption. Let's suppose we have a network composed of two populations of 1000 neurons each, fully connected: each neuron of the second population receives 1000 synapses. This makes a total of 1 million synapses for the projection and, supposing the weights w use the double floating precision, requires 4 MB of memory. If you record w during a simulation of 1 second (1000 steps, with dt=1.0 ), the total added memory consumption would already be around 4GB. To avoid fast memory fills, you should either 1) record the projection variables infrequently (by setting the period argument of the Monitor), or 2) selectively record particular dendrites. The corresponding dendrite should be simply passed to the monitor: dendrite = proj . dendrite ( 12 ) # or simply proj[12] m = Monitor ( dendrite , 'w' ) simulate ( 1000.0 ) data = m . get ( 'w' ) The Monitor object has the same start() , pause() , resume() and get() methods as for populations. get() returns also 2D Numpy arrays, the first index being time, the second being the index of the synapse. To know to which pre-synaptic neuron it corresponds, use the pre_ranks attribute of the dendrite: dendrite . pre_ranks # [0, 3, 12, ..] To record a complete projection, simply pass it to the Monitor: m = Monitor ( proj , 'w' , period = 1000. ) simulate ( 10000.0 ) data = m . get ( 'w' ) One last time, do not record all weights of a projection at each time step! Warning Recording synaptic variables with CUDA is not available.","title":"Synaptic variables"},{"location":"manual/Reporting/","text":"Reporting # ANNarchy includes an utility allowing to automatically generate a report based on the current structure of the network: report ( filename = \"model_description.tex\" ) report ( filename = \"model_description.md\" ) If the filename ends with .tex , the LaTeX report will be generated based on the specifications provided in: Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8):e1000456. If the filename ends with .md , the report will be generated in Markdown, so it can later be exported to pdf or html using pandoc . report() accepts several arguments: filename : name of the file where the report will be written (default: \\\"./report.tex\\\") standalone : tells if the generated TeX file should be directly compilable or only includable. Ignored in Markdown. gather_subprojections : if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False). title : title of the document (Markdown only) author : author of the document (Markdown only) date : date of the document (Markdown only) net_id : id of the network to be used for reporting (default: 0, everything that was declared) Content of the TeX file # report() produces a .tex file (by default report.tex in the current directory, but this can be changed by passing the filename argument) which can be directly compiled with pdflatex or integrated into a larger file: pdflatex model_description.tex This report consists of different tables describing several aspects of the model: Summary : A summary of the network, with a list of populations, neuron and synapse models, topologies, etc. This section may have to be adapted, as for example, ANNarchy does not make a distinction between synapse and plasticity models. Populations : A list of populations, with their respective neural models and geometries. Projections : A list of projections, with the pre- and post-synaptic populations, the target, the synapse model if any, and a description of the connection pattern. Neuron models : For each neuron model, a description of its dynamics with equations parsed using SymPy and translated to the LaTeX mathematical language. Synapse models : For each synapse model, a description of its dynamics if any. Parameters : The initial value (before the call to compile() ) of the parameters of each population and projection (if any). Input : Inputs set to the network (has to be filled manually). Measurements : Measurements done in the network (has to be filled manually). Content of the Markdown file # The generated Mardown file is globally similar to the LaTeX one, with additional information that make it more useful for debugging (locality of attributes, type...). The Markown file is readable by design, but it can be translated to many markup languages (html, epub, latex, pdf...) using pandoc . To obtain a pdf from the Markdown file (supposing you have a LaTeX distribution available), just type: pandoc model_description.md -sN -V geometry:margin = 1in -o model_description.pdf The -V argument tells LaTex to use the full page instead of the default booklet format. To obtain a html file, use: pandoc model_description.md -sSN --mathjax -o model_description.html You can omit the -S option if you only want to include the code into a webpage, otherwise it is a standalone file. --mathjax is needed to display mathematical equations using the javascript library MathJax . By default, the html file has no styling, and tables can be very ugly. With a simple css file like this one , the html page looks nicer (feel free to edit): pandoc model_description.md -sSN --mathjax --css = simple.css -o model_description.html If you upload your model to a github-like service (bitbucket, gitlab, gogs...), it could be a good idea to generate the README.md directly with report() . Do not forget to set a title+author+date then. Documenting the network # The report is generated based entirely on the Python script. For it to make sense, the user has to provide the necessary information while defining the network: Populations must be assigned a unique name. If no name is given, generic names such as pop0 or pop1 will be used. If two populations have the same name, the connectivity will be unreadable: pop1 = Population ( geometry = ( 100 , 100 ), neuron = Izhikevich , name = \"Excitatory\" ) pop2 = Population ( geometry = ( 20 , 20 ), neuron = Izhikevich , name = \"Inhibitory\" ) User-defined neuron and synapse models should be assigned a name and description. The name should be relatively short and generic (e.g. \"Izhikevich\", \"BCM learning rule\"), while the description should be more specific. They can contain LaTeX code, but remember to double the \\ which is the escape symbol in Python strings: LIF = Neuron ( parameters = \"\"\" tau = 10.0 \"\"\" , equations = \"\"\" tau * dv/dt + v = g_exc \"\"\" , spike = \"v > 30.0\" , reset = \"v = 0.0\" name = \"LIF\" , description = \"Leaky Integrate-and-Fire spiking neuron with time constant $ \\\\ tau$.\" ) Oja = Synapse ( parameters = \"\"\" eta = 10.0 tau = 10.0 : postsynaptic \"\"\" , equations = \"\"\" tau * dalpha/dt + alpha = pos(post.r - 1.0) : postsynaptic eta * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=0.0 \"\"\" , name = \"Oja learning rule\" , description = \"\"\"Oja learning rule ensuring regularization of the synaptic weights.\"\"\" ) Choose simple parameter and variable names for the description of equations. If a parameter/variable name uses only one character, it will be treated as a mathematical variable in the equations (ex: v becomes \\(v\\) ), otherwise the plain text representation will be used (ugly). If the name corresponds to a greek letter ( alpha , tau , etc.), it will be represented by the corresponding greek letter ( \\(\\alpha\\) , \\(\\tau\\) ). If the name is composed of two terms separated by an underscore ( tau_exc ), a subscript will be used ( \\(\\tau_\\text{exc}\\) ). If more than one underscore is used, the text representation is used instead (LaTeX does not allow multiple subscripts). Example # Let's take the homeostatic STDP ramp example provided in examples/homeostatic_stdp/Ramp.py and add names/descriptions to the objects: from ANNarchy import * # Izhikevich RS neuron RSNeuron = Neuron ( parameters = \"\"\" a = 0.02 : population b = 0.2 : population c = -65. : population d = 8. : population tau_ampa = 5. : population tau_nmda = 150. : population vrev = 0.0 : population \"\"\" , equations = \"\"\" # Inputs I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v) # Midpoint scheme dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint du/dt = a * (b*v - u) : init=-13., midpoint # Izhikevich scheme # new_v = v + 0.5*(0.04 * v^2 + 5.0 * v + 140.0 - u + I) : init=-65. # v = new_v + 0.5*(0.04 * new_v^2 + 5.0 * new_v + 140.0 - u + I) : init=-65. # u += a * (b*v - u) : init=-13. # Conductances tau_ampa * dg_ampa/dt = -g_ampa : exponential tau_nmda * dg_nmda/dt = -g_nmda : exponential \"\"\" , spike = \"\"\" v >= 30. \"\"\" , reset = \"\"\" v = c u += d \"\"\" , functions = \"\"\" nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2) \"\"\" , name = \"Regular-spiking Izhikevich\" , description = \"Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\" ) # Input population inp = PoissonPopulation ( 100 , rates = np . linspace ( 0.2 , 20. , 100 ), name = \"Poisson input\" ) # RS neuron without homeostatic mechanism pop1 = Population ( 1 , RSNeuron , name = \"RS neuron without homeostasis\" ) pop1 . compute_firing_rate ( 5000. ) # RS neuron with homeostatic mechanism pop2 = Population ( 1 , RSNeuron , name = \"RS neuron with homeostasis\" ) pop2 . compute_firing_rate ( 5000. ) # Nearest Neighbour STDP nearest_neighbour_stdp = Synapse ( parameters = \"\"\" tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_max = 0.03 : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Nearest-neighbour w += if t_post >= t_pre: ltp else: - ltd : min=0.0, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" , name = \"Nearest-neighbour STDP\" , description = \"Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\" ) # STDP with homeostatic regulation homeo_stdp = Synapse ( parameters = \"\"\" # STDP tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_min = 0.0 : projection w_max = 0.03 : projection # Homeostatic regulation alpha = 0.1 : projection beta = 1.0 : projection gamma = 50. : projection Rtarget = 35. : projection T = 5000. : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Homeostatic values R = post.r : postsynaptic K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic # Nearest-neighbour stdp = if t_post >= t_pre: ltp else: - ltd w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" , name = \"Nearest-neighbour STDP with homeostasis\" , description = \"Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \" ) # Projection without homeostatic mechanism proj1 = Projection ( inp , pop1 , [ 'ampa' , 'nmda' ], synapse = nearest_neighbour_stdp ) proj1 . connect_all_to_all ( Uniform ( 0.01 , 0.03 )) # Projection with homeostatic mechanism proj2 = Projection ( inp , pop2 , [ 'ampa' , 'nmda' ], synapse = homeo_stdp ) proj2 . connect_all_to_all ( weights = Uniform ( 0.01 , 0.03 )) # Record m1 = Monitor ( pop1 , 'r' ) m2 = Monitor ( pop2 , 'r' ) report ( 'ramp.md' , title = \"Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\" , author = \"Carlson, Richert, Dutt and Krichmar\" , date = \"Neural Networks (IJCNN) 2013\" ) This generates the following Markdown file: --- title: Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks author: Carlson, Richert, Dutt and Krichmar date: Neural Networks (IJCNN) 2013 --- # Structure of the network * ANNarchy 4.6.2b using the default backend. * Numerical step size: 1.0 ms. ## Populations | **Population** | **Size** | **Neuron type** | | ----------------------------- | -------- | -------------------------- | | Poisson input | 100 | Poisson | | RS neuron without homeostasis | 1 | Regular-spiking Izhikevich | | RS neuron with homeostasis | 1 | Regular-spiking Izhikevich | ## Projections | **Source** | **Destination** | **Target** | **Synapse type** | **Pattern** | | ------------- | ----------------------------- | ----------- | --------------------------------------- | --------------------------------------------------------- | | Poisson input | RS neuron without homeostasis | ampa / nmda | Nearest-neighbour STDP | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | | Poisson input | RS neuron with homeostasis | ampa / nmda | Nearest-neighbour STDP with homeostasis | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | ## Monitors | **Object** | **Variables** | **Period** | | ----------------------------- | ------------- | ---------- | | RS neuron without homeostasis | r | 1.0 | | RS neuron with homeostasis | r | 1.0 | # Neuron models ## Regular-spiking Izhikevich Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | -------------------- | ----------------- | -------------- | -------- | | $a$ | 0.02 | per population | double | | $b$ | 0.2 | per population | double | | $c$ | -65.0 | per population | double | | $d$ | 8.0 | per population | double | | $\\tau_{\\text{ampa}}$ | 5.0 | per population | double | | $\\tau_{\\text{nmda}}$ | 150.0 | per population | double | | ${\\text{vrev}}$ | 0.0 | per population | double | **Equations:** * Variable $I$ : per neuron, initial value: 0.0 $$ {I}(t) = {g_{\\text{ampa}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) + {g_{\\text{nmda}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) \\cdot \\operatorname{nmda}{\\left ({v}(t),-80.0,60.0 \\right )} $$ * Variable $v$ : per neuron, initial value: -65.0, midpoint numerical method $$ \\frac{d{v}(t)}{dt} = {I}(t) - {u}(t) + {v}(t) \\cdot \\left(0.04 \\cdot {v}(t) + 5.0\\right) + 140.0 $$ * Variable $u$ : per neuron, initial value: -13.0, midpoint numerical method $$ \\frac{d{u}(t)}{dt} = a \\cdot \\left(b \\cdot {v}(t) - {u}(t)\\right) $$ * Variable $g_{\\text{ampa}}$ : per neuron, initial value: 0.0, exponential numerical method $$ \\frac{d{g_{\\text{ampa}}}(t)}{dt} \\cdot \\tau_{\\text{ampa}} = - {g_{\\text{ampa}}}(t) $$ * Variable $g_{\\text{nmda}}$ : per neuron, initial value: 0.0, exponential numerical method $$ \\frac{d{g_{\\text{nmda}}}(t)}{dt} \\cdot \\tau_{\\text{nmda}} = - {g_{\\text{nmda}}}(t) $$ **Spike emission:** if ${v}(t) \\geq 30.0$ : * Emit a spike a time $t$. * ${v}(t) = c$ * ${u}(t) \\mathrel{+}= d$ **Functions** $${\\text{nmda}}(v, t, s) = \\frac{\\left(- t + v\\right)^{2}}{s^{2} \\cdot \\left(1.0 + \\frac{1}{s^{2}} \\cdot \\left(- t + v\\right)^{2}\\right)}$$ ## Poisson Spiking neuron with spikes emitted according to a Poisson distribution. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | ---------------- | ----------------- | ------------ | -------- | | ${\\text{rates}}$ | 10.0 | per neuron | double | **Equations:** * Variable $p$ : per neuron, initial value: 0.0 $$ {p}(t) = \\frac{1000.0}{\\Delta t} \\cdot \\mathcal{U}{\\left (0.0,1.0 \\right )} $$ **Spike emission:** if ${p}(t) < {\\text{rates}}$ : * Emit a spike a time $t$. # Synapse models ## Nearest-neighbour STDP Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | --------------------- | ----------------- | -------------- | -------- | | $\\tau_{\\text{plus}}$ | 20.0 | per projection | double | | $\\tau_{\\text{minus}}$ | 60.0 | per projection | double | | $A_{\\text{plus}}$ | 0.0002 | per projection | double | | $A_{\\text{minus}}$ | 6.6e-05 | per projection | double | | $w_{\\text{max}}$ | 0.03 | per projection | double | **Equations:** * Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t) $$ * Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t) $$ * Variable $w$ : per synapse, initial value: 0.0, minimum: 0.0, maximum: w_max $$ {w}(t) \\mathrel{+}= \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t _{\\text{pos}} \\geq t_ {\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases} $$ **Pre-synaptic event at $t_\\text{pre} + d$:** $$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$ $${{\\text{ltp}}}(t) = A_{\\text{plus}}$$ **Post-synaptic event at $t_\\text{post}$:** $${{\\text{ltd}}}(t) = A_{\\text{minus}}$$ ## Nearest-neighbour STDP with homeostasis Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | --------------------- | ----------------- | -------------- | -------- | | $\\tau_{\\text{plus}}$ | 20.0 | per projection | double | | $\\tau_{\\text{minus}}$ | 60.0 | per projection | double | | $A_{\\text{plus}}$ | 0.0002 | per projection | double | | $A_{\\text{minus}}$ | 6.6e-05 | per projection | double | | $w_{\\text{min}}$ | 0.0 | per projection | double | | $w_{\\text{max}}$ | 0.03 | per projection | double | | $\\alpha$ | 0.1 | per projection | double | | $\\beta$ | 1.0 | per projection | double | | $\\gamma$ | 50.0 | per projection | double | | ${\\text{Rtarget}}$ | 35.0 | per projection | double | | $T$ | 5000.0 | per projection | double | **Equations:** * Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t) $$ * Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t) $$ * Variable $R$ : per post-synaptic neuron, initial value: 0.0 $$ {R}(t) = {r}^{\\text{post}}(t) $$ * Variable $K$ : per post-synaptic neuron, initial value: 0.0 $$ {K}(t) = \\frac{{R}(t)}{T \\cdot \\left(\\gamma \\cdot \\left|{f}\\right|{\\left (- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1.0 \\right )} + 1.0\\right)} $$ * Variable ${\\text{stdp}}$ : per synapse, initial value: 0.0 $$ {{\\text{stdp}}}(t) = \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t _{\\text{pos}} \\geq t_ {\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases} $$ * Variable $w$ : per synapse, initial value: 0.0, minimum: w _min, maximum: w_ max $$ {w}(t) \\mathrel{+}= {K}(t) \\cdot \\left(\\alpha \\cdot {w}(t) \\cdot \\left(- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1\\right) + \\beta \\cdot {{\\text{stdp}}}(t)\\right) $$ **Pre-synaptic event at $t_\\text{pre} + d$:** $$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$ $${{\\text{ltp}}}(t) = A_{\\text{plus}}$$ **Post-synaptic event at $t_\\text{post}$:** $${{\\text{ltd}}}(t) = A_{\\text{minus}}$$ # Parameters ## Population parameters | **Population** | **Neuron type** | **Name** | **Value** | | ----------------------------- | -------------------------- | -------------------- | ------------- | | Poisson input | Poisson | ${\\text{rates}}$ | $[0.2, 20.0]$ | | RS neuron without homeostasis | Regular-spiking Izhikevich | $a$ | 0.02 | | | | $b$ | 0.2 | | | | $c$ | -65.0 | | | | $d$ | 8.0 | | | | $\\tau_{\\text{ampa}}$ | 5.0 | | | | $\\tau_{\\text{nmda}}$ | 150.0 | | | | ${\\text{vrev}}$ | 0.0 | | RS neuron with homeostasis | Regular-spiking Izhikevich | $a$ | 0.02 | | | | $b$ | 0.2 | | | | $c$ | -65.0 | | | | $d$ | 8.0 | | | | $\\tau_{\\text{ampa}}$ | 5.0 | | | | $\\tau_{\\text{nmda}}$ | 150.0 | | | | ${\\text{vrev}}$ | 0.0 | ## Projection parameters | **Projection** | **Synapse type** | **Name** | **Value** | | ---------------------------------------------------------------------------------- | --------------------------------------- | --------------------- | --------- | | Poisson input $\\rightarrow$ RS neuron without homeostasis with target ampa / nmda | Nearest-neighbour STDP | $\\tau_{\\text{plus}}$ | 20.0 | | | | $\\tau_{\\text{minus}}$ | 60.0 | | | | $A_{\\text{plus}}$ | 0.0002 | | | | $A_{\\text{minus}}$ | 6.6e-05 | | | | $w_{\\text{max}}$ | 0.03 | | Poisson input $\\rightarrow$ RS neuron with homeostasis with target ampa / nmda | Nearest-neighbour STDP with homeostasis | $\\tau_{\\text{plus}}$ | 20.0 | | | | $\\tau_{\\text{minus}}$ | 60.0 | | | | $A_{\\text{plus}}$ | 0.0002 | | | | $A_{\\text{minus}}$ | 6.6e-05 | | | | $w_{\\text{min}}$ | 0.0 | | | | $w_{\\text{max}}$ | 0.03 | | | | $\\alpha$ | 0.1 | | | | $\\beta$ | 1.0 | | | | $\\gamma$ | 50.0 | | | | ${\\text{Rtarget}}$ | 35.0 | | | | $T$ | 5000.0 |","title":"Reporting"},{"location":"manual/Reporting/#reporting","text":"ANNarchy includes an utility allowing to automatically generate a report based on the current structure of the network: report ( filename = \"model_description.tex\" ) report ( filename = \"model_description.md\" ) If the filename ends with .tex , the LaTeX report will be generated based on the specifications provided in: Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8):e1000456. If the filename ends with .md , the report will be generated in Markdown, so it can later be exported to pdf or html using pandoc . report() accepts several arguments: filename : name of the file where the report will be written (default: \\\"./report.tex\\\") standalone : tells if the generated TeX file should be directly compilable or only includable. Ignored in Markdown. gather_subprojections : if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False). title : title of the document (Markdown only) author : author of the document (Markdown only) date : date of the document (Markdown only) net_id : id of the network to be used for reporting (default: 0, everything that was declared)","title":"Reporting"},{"location":"manual/Reporting/#content-of-the-tex-file","text":"report() produces a .tex file (by default report.tex in the current directory, but this can be changed by passing the filename argument) which can be directly compiled with pdflatex or integrated into a larger file: pdflatex model_description.tex This report consists of different tables describing several aspects of the model: Summary : A summary of the network, with a list of populations, neuron and synapse models, topologies, etc. This section may have to be adapted, as for example, ANNarchy does not make a distinction between synapse and plasticity models. Populations : A list of populations, with their respective neural models and geometries. Projections : A list of projections, with the pre- and post-synaptic populations, the target, the synapse model if any, and a description of the connection pattern. Neuron models : For each neuron model, a description of its dynamics with equations parsed using SymPy and translated to the LaTeX mathematical language. Synapse models : For each synapse model, a description of its dynamics if any. Parameters : The initial value (before the call to compile() ) of the parameters of each population and projection (if any). Input : Inputs set to the network (has to be filled manually). Measurements : Measurements done in the network (has to be filled manually).","title":"Content of the TeX file"},{"location":"manual/Reporting/#content-of-the-markdown-file","text":"The generated Mardown file is globally similar to the LaTeX one, with additional information that make it more useful for debugging (locality of attributes, type...). The Markown file is readable by design, but it can be translated to many markup languages (html, epub, latex, pdf...) using pandoc . To obtain a pdf from the Markdown file (supposing you have a LaTeX distribution available), just type: pandoc model_description.md -sN -V geometry:margin = 1in -o model_description.pdf The -V argument tells LaTex to use the full page instead of the default booklet format. To obtain a html file, use: pandoc model_description.md -sSN --mathjax -o model_description.html You can omit the -S option if you only want to include the code into a webpage, otherwise it is a standalone file. --mathjax is needed to display mathematical equations using the javascript library MathJax . By default, the html file has no styling, and tables can be very ugly. With a simple css file like this one , the html page looks nicer (feel free to edit): pandoc model_description.md -sSN --mathjax --css = simple.css -o model_description.html If you upload your model to a github-like service (bitbucket, gitlab, gogs...), it could be a good idea to generate the README.md directly with report() . Do not forget to set a title+author+date then.","title":"Content of the Markdown file"},{"location":"manual/Reporting/#documenting-the-network","text":"The report is generated based entirely on the Python script. For it to make sense, the user has to provide the necessary information while defining the network: Populations must be assigned a unique name. If no name is given, generic names such as pop0 or pop1 will be used. If two populations have the same name, the connectivity will be unreadable: pop1 = Population ( geometry = ( 100 , 100 ), neuron = Izhikevich , name = \"Excitatory\" ) pop2 = Population ( geometry = ( 20 , 20 ), neuron = Izhikevich , name = \"Inhibitory\" ) User-defined neuron and synapse models should be assigned a name and description. The name should be relatively short and generic (e.g. \"Izhikevich\", \"BCM learning rule\"), while the description should be more specific. They can contain LaTeX code, but remember to double the \\ which is the escape symbol in Python strings: LIF = Neuron ( parameters = \"\"\" tau = 10.0 \"\"\" , equations = \"\"\" tau * dv/dt + v = g_exc \"\"\" , spike = \"v > 30.0\" , reset = \"v = 0.0\" name = \"LIF\" , description = \"Leaky Integrate-and-Fire spiking neuron with time constant $ \\\\ tau$.\" ) Oja = Synapse ( parameters = \"\"\" eta = 10.0 tau = 10.0 : postsynaptic \"\"\" , equations = \"\"\" tau * dalpha/dt + alpha = pos(post.r - 1.0) : postsynaptic eta * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=0.0 \"\"\" , name = \"Oja learning rule\" , description = \"\"\"Oja learning rule ensuring regularization of the synaptic weights.\"\"\" ) Choose simple parameter and variable names for the description of equations. If a parameter/variable name uses only one character, it will be treated as a mathematical variable in the equations (ex: v becomes \\(v\\) ), otherwise the plain text representation will be used (ugly). If the name corresponds to a greek letter ( alpha , tau , etc.), it will be represented by the corresponding greek letter ( \\(\\alpha\\) , \\(\\tau\\) ). If the name is composed of two terms separated by an underscore ( tau_exc ), a subscript will be used ( \\(\\tau_\\text{exc}\\) ). If more than one underscore is used, the text representation is used instead (LaTeX does not allow multiple subscripts).","title":"Documenting the network"},{"location":"manual/Reporting/#example","text":"Let's take the homeostatic STDP ramp example provided in examples/homeostatic_stdp/Ramp.py and add names/descriptions to the objects: from ANNarchy import * # Izhikevich RS neuron RSNeuron = Neuron ( parameters = \"\"\" a = 0.02 : population b = 0.2 : population c = -65. : population d = 8. : population tau_ampa = 5. : population tau_nmda = 150. : population vrev = 0.0 : population \"\"\" , equations = \"\"\" # Inputs I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v) # Midpoint scheme dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint du/dt = a * (b*v - u) : init=-13., midpoint # Izhikevich scheme # new_v = v + 0.5*(0.04 * v^2 + 5.0 * v + 140.0 - u + I) : init=-65. # v = new_v + 0.5*(0.04 * new_v^2 + 5.0 * new_v + 140.0 - u + I) : init=-65. # u += a * (b*v - u) : init=-13. # Conductances tau_ampa * dg_ampa/dt = -g_ampa : exponential tau_nmda * dg_nmda/dt = -g_nmda : exponential \"\"\" , spike = \"\"\" v >= 30. \"\"\" , reset = \"\"\" v = c u += d \"\"\" , functions = \"\"\" nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2) \"\"\" , name = \"Regular-spiking Izhikevich\" , description = \"Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\" ) # Input population inp = PoissonPopulation ( 100 , rates = np . linspace ( 0.2 , 20. , 100 ), name = \"Poisson input\" ) # RS neuron without homeostatic mechanism pop1 = Population ( 1 , RSNeuron , name = \"RS neuron without homeostasis\" ) pop1 . compute_firing_rate ( 5000. ) # RS neuron with homeostatic mechanism pop2 = Population ( 1 , RSNeuron , name = \"RS neuron with homeostasis\" ) pop2 . compute_firing_rate ( 5000. ) # Nearest Neighbour STDP nearest_neighbour_stdp = Synapse ( parameters = \"\"\" tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_max = 0.03 : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Nearest-neighbour w += if t_post >= t_pre: ltp else: - ltd : min=0.0, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" , name = \"Nearest-neighbour STDP\" , description = \"Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\" ) # STDP with homeostatic regulation homeo_stdp = Synapse ( parameters = \"\"\" # STDP tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_min = 0.0 : projection w_max = 0.03 : projection # Homeostatic regulation alpha = 0.1 : projection beta = 1.0 : projection gamma = 50. : projection Rtarget = 35. : projection T = 5000. : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Homeostatic values R = post.r : postsynaptic K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic # Nearest-neighbour stdp = if t_post >= t_pre: ltp else: - ltd w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" , name = \"Nearest-neighbour STDP with homeostasis\" , description = \"Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \" ) # Projection without homeostatic mechanism proj1 = Projection ( inp , pop1 , [ 'ampa' , 'nmda' ], synapse = nearest_neighbour_stdp ) proj1 . connect_all_to_all ( Uniform ( 0.01 , 0.03 )) # Projection with homeostatic mechanism proj2 = Projection ( inp , pop2 , [ 'ampa' , 'nmda' ], synapse = homeo_stdp ) proj2 . connect_all_to_all ( weights = Uniform ( 0.01 , 0.03 )) # Record m1 = Monitor ( pop1 , 'r' ) m2 = Monitor ( pop2 , 'r' ) report ( 'ramp.md' , title = \"Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\" , author = \"Carlson, Richert, Dutt and Krichmar\" , date = \"Neural Networks (IJCNN) 2013\" ) This generates the following Markdown file: --- title: Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks author: Carlson, Richert, Dutt and Krichmar date: Neural Networks (IJCNN) 2013 --- # Structure of the network * ANNarchy 4.6.2b using the default backend. * Numerical step size: 1.0 ms. ## Populations | **Population** | **Size** | **Neuron type** | | ----------------------------- | -------- | -------------------------- | | Poisson input | 100 | Poisson | | RS neuron without homeostasis | 1 | Regular-spiking Izhikevich | | RS neuron with homeostasis | 1 | Regular-spiking Izhikevich | ## Projections | **Source** | **Destination** | **Target** | **Synapse type** | **Pattern** | | ------------- | ----------------------------- | ----------- | --------------------------------------- | --------------------------------------------------------- | | Poisson input | RS neuron without homeostasis | ampa / nmda | Nearest-neighbour STDP | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | | Poisson input | RS neuron with homeostasis | ampa / nmda | Nearest-neighbour STDP with homeostasis | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | ## Monitors | **Object** | **Variables** | **Period** | | ----------------------------- | ------------- | ---------- | | RS neuron without homeostasis | r | 1.0 | | RS neuron with homeostasis | r | 1.0 | # Neuron models ## Regular-spiking Izhikevich Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | -------------------- | ----------------- | -------------- | -------- | | $a$ | 0.02 | per population | double | | $b$ | 0.2 | per population | double | | $c$ | -65.0 | per population | double | | $d$ | 8.0 | per population | double | | $\\tau_{\\text{ampa}}$ | 5.0 | per population | double | | $\\tau_{\\text{nmda}}$ | 150.0 | per population | double | | ${\\text{vrev}}$ | 0.0 | per population | double | **Equations:** * Variable $I$ : per neuron, initial value: 0.0 $$ {I}(t) = {g_{\\text{ampa}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) + {g_{\\text{nmda}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) \\cdot \\operatorname{nmda}{\\left ({v}(t),-80.0,60.0 \\right )} $$ * Variable $v$ : per neuron, initial value: -65.0, midpoint numerical method $$ \\frac{d{v}(t)}{dt} = {I}(t) - {u}(t) + {v}(t) \\cdot \\left(0.04 \\cdot {v}(t) + 5.0\\right) + 140.0 $$ * Variable $u$ : per neuron, initial value: -13.0, midpoint numerical method $$ \\frac{d{u}(t)}{dt} = a \\cdot \\left(b \\cdot {v}(t) - {u}(t)\\right) $$ * Variable $g_{\\text{ampa}}$ : per neuron, initial value: 0.0, exponential numerical method $$ \\frac{d{g_{\\text{ampa}}}(t)}{dt} \\cdot \\tau_{\\text{ampa}} = - {g_{\\text{ampa}}}(t) $$ * Variable $g_{\\text{nmda}}$ : per neuron, initial value: 0.0, exponential numerical method $$ \\frac{d{g_{\\text{nmda}}}(t)}{dt} \\cdot \\tau_{\\text{nmda}} = - {g_{\\text{nmda}}}(t) $$ **Spike emission:** if ${v}(t) \\geq 30.0$ : * Emit a spike a time $t$. * ${v}(t) = c$ * ${u}(t) \\mathrel{+}= d$ **Functions** $${\\text{nmda}}(v, t, s) = \\frac{\\left(- t + v\\right)^{2}}{s^{2} \\cdot \\left(1.0 + \\frac{1}{s^{2}} \\cdot \\left(- t + v\\right)^{2}\\right)}$$ ## Poisson Spiking neuron with spikes emitted according to a Poisson distribution. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | ---------------- | ----------------- | ------------ | -------- | | ${\\text{rates}}$ | 10.0 | per neuron | double | **Equations:** * Variable $p$ : per neuron, initial value: 0.0 $$ {p}(t) = \\frac{1000.0}{\\Delta t} \\cdot \\mathcal{U}{\\left (0.0,1.0 \\right )} $$ **Spike emission:** if ${p}(t) < {\\text{rates}}$ : * Emit a spike a time $t$. # Synapse models ## Nearest-neighbour STDP Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | --------------------- | ----------------- | -------------- | -------- | | $\\tau_{\\text{plus}}$ | 20.0 | per projection | double | | $\\tau_{\\text{minus}}$ | 60.0 | per projection | double | | $A_{\\text{plus}}$ | 0.0002 | per projection | double | | $A_{\\text{minus}}$ | 6.6e-05 | per projection | double | | $w_{\\text{max}}$ | 0.03 | per projection | double | **Equations:** * Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t) $$ * Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t) $$ * Variable $w$ : per synapse, initial value: 0.0, minimum: 0.0, maximum: w_max $$ {w}(t) \\mathrel{+}= \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t _{\\text{pos}} \\geq t_ {\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases} $$ **Pre-synaptic event at $t_\\text{pre} + d$:** $$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$ $${{\\text{ltp}}}(t) = A_{\\text{plus}}$$ **Post-synaptic event at $t_\\text{post}$:** $${{\\text{ltd}}}(t) = A_{\\text{minus}}$$ ## Nearest-neighbour STDP with homeostasis Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | --------------------- | ----------------- | -------------- | -------- | | $\\tau_{\\text{plus}}$ | 20.0 | per projection | double | | $\\tau_{\\text{minus}}$ | 60.0 | per projection | double | | $A_{\\text{plus}}$ | 0.0002 | per projection | double | | $A_{\\text{minus}}$ | 6.6e-05 | per projection | double | | $w_{\\text{min}}$ | 0.0 | per projection | double | | $w_{\\text{max}}$ | 0.03 | per projection | double | | $\\alpha$ | 0.1 | per projection | double | | $\\beta$ | 1.0 | per projection | double | | $\\gamma$ | 50.0 | per projection | double | | ${\\text{Rtarget}}$ | 35.0 | per projection | double | | $T$ | 5000.0 | per projection | double | **Equations:** * Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t) $$ * Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t) $$ * Variable $R$ : per post-synaptic neuron, initial value: 0.0 $$ {R}(t) = {r}^{\\text{post}}(t) $$ * Variable $K$ : per post-synaptic neuron, initial value: 0.0 $$ {K}(t) = \\frac{{R}(t)}{T \\cdot \\left(\\gamma \\cdot \\left|{f}\\right|{\\left (- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1.0 \\right )} + 1.0\\right)} $$ * Variable ${\\text{stdp}}$ : per synapse, initial value: 0.0 $$ {{\\text{stdp}}}(t) = \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t _{\\text{pos}} \\geq t_ {\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases} $$ * Variable $w$ : per synapse, initial value: 0.0, minimum: w _min, maximum: w_ max $$ {w}(t) \\mathrel{+}= {K}(t) \\cdot \\left(\\alpha \\cdot {w}(t) \\cdot \\left(- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1\\right) + \\beta \\cdot {{\\text{stdp}}}(t)\\right) $$ **Pre-synaptic event at $t_\\text{pre} + d$:** $$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$ $${{\\text{ltp}}}(t) = A_{\\text{plus}}$$ **Post-synaptic event at $t_\\text{post}$:** $${{\\text{ltd}}}(t) = A_{\\text{minus}}$$ # Parameters ## Population parameters | **Population** | **Neuron type** | **Name** | **Value** | | ----------------------------- | -------------------------- | -------------------- | ------------- | | Poisson input | Poisson | ${\\text{rates}}$ | $[0.2, 20.0]$ | | RS neuron without homeostasis | Regular-spiking Izhikevich | $a$ | 0.02 | | | | $b$ | 0.2 | | | | $c$ | -65.0 | | | | $d$ | 8.0 | | | | $\\tau_{\\text{ampa}}$ | 5.0 | | | | $\\tau_{\\text{nmda}}$ | 150.0 | | | | ${\\text{vrev}}$ | 0.0 | | RS neuron with homeostasis | Regular-spiking Izhikevich | $a$ | 0.02 | | | | $b$ | 0.2 | | | | $c$ | -65.0 | | | | $d$ | 8.0 | | | | $\\tau_{\\text{ampa}}$ | 5.0 | | | | $\\tau_{\\text{nmda}}$ | 150.0 | | | | ${\\text{vrev}}$ | 0.0 | ## Projection parameters | **Projection** | **Synapse type** | **Name** | **Value** | | ---------------------------------------------------------------------------------- | --------------------------------------- | --------------------- | --------- | | Poisson input $\\rightarrow$ RS neuron without homeostasis with target ampa / nmda | Nearest-neighbour STDP | $\\tau_{\\text{plus}}$ | 20.0 | | | | $\\tau_{\\text{minus}}$ | 60.0 | | | | $A_{\\text{plus}}$ | 0.0002 | | | | $A_{\\text{minus}}$ | 6.6e-05 | | | | $w_{\\text{max}}$ | 0.03 | | Poisson input $\\rightarrow$ RS neuron with homeostasis with target ampa / nmda | Nearest-neighbour STDP with homeostasis | $\\tau_{\\text{plus}}$ | 20.0 | | | | $\\tau_{\\text{minus}}$ | 60.0 | | | | $A_{\\text{plus}}$ | 0.0002 | | | | $A_{\\text{minus}}$ | 6.6e-05 | | | | $w_{\\text{min}}$ | 0.0 | | | | $w_{\\text{max}}$ | 0.03 | | | | $\\alpha$ | 0.1 | | | | $\\beta$ | 1.0 | | | | $\\gamma$ | 50.0 | | | | ${\\text{Rtarget}}$ | 35.0 | | | | $T$ | 5000.0 |","title":"Example"},{"location":"manual/Saving/","text":"Saving and loading a network # For the complete APIs, see IO in the library reference. Global parameters # The global parameters of the network (flagged with population or projection in the Neuron/Synapse definitions) can be saved to and loaded from a JSON file using the functions save_parameters() and load_parameters() : save_parameters ( 'network.json' ) load_parameters ( 'network.json' ) The saved JSON file for a network of two populations of Izhikevich neurons connected with STDP will look like: { \"populations\" : { \"pop0\" : { \"a\" : 0.1 , \"b\" : 0.2 , \"c\" : -65.0 , \"d\" : 8.0 , \"tau_ampa\" : 5.0 , \"tau_nmda\" : 150.0 , \"v_rev\" : 0.0 , \"v_thresh\" : 30.0 }, \"pop1\" : { \"a\" : 0.02 , \"b\" : 0.2 , \"c\" : -65.0 , \"d\" : 8.0 , \"tau_ampa\" : 5.0 , \"tau_nmda\" : 150.0 , \"v_rev\" : 0.0 , \"v_thresh\" : 30.0 } }, \"projections\" : { \"proj0\" : { \"tau_plus\" : 20.0 , \"tau_minus\" : 60.0 , \"A_plus\" : 0.0002 , \"A_minus\" : 6.6e-05 , \"w_max\" : 0.03 } }, \"network\" : {}, } By default, populations and projections have names like pop0 and proj1 . For readability, we advise setting explicit (and unique ) names in their constructor: pop = Population ( 100 , Izhikevich , name = \"PFC_exc\" ) proj = Projection ( pop , pop2 , 'exc' , STDP , name = \"PFC_exc_to_inh\" ) Only global parameters can be saved (no array is allowed in the JSON file). By default, only global parameters will be loaded, except if the global_only argument to load_parameters() is set to False. In that case, even local parameters can be set by the JSON file, but they will all use the same values. If you want to initialize other things than population/projection global parameters, you can define arbitrary values in the \"network\" dictionary: { \"network\" : { \"pop1_r_min\" : 0.1 , \"pop1_r_max\" : 1.3 , }, } load_parameters() will return the corresponding dictionary: params = load_parameters ( 'network.json' ) You can then use them to initialize programmatically non-global parameters or variables: pop1 . r = Uniform ( params [ 'pop1_r_min' ], params [ 'pop1_r_max' ]) Complete state of the network # The state of all variables, including the synaptic weights, can be saved in a text file, compressed binary file or Matlab file using the save() method: save ( 'data.txt' ) save ( 'data.txt.gz' ) save ( 'data.mat' ) Filenames ending with .mat correspond to Matlab files (it requires the installation of Scipy), filenames ending with .gz are compressed using gzip (normally standard to all Python distributions, but may require installation), other extensions are normal text files using cPickle (standard). save() also accepts the populations and projections boolean flags. If True (the default), the neural resp. synaptic variables will be saved. For example, if you only care about synaptic plasticity but not the neural variables, you can set populations to False , and only synaptic variables will be saved. save ( 'data.txt' , populations = False ) Except for the Matlab format, you can also load the state of variables stored in these files once the network is compiled : load ( 'data.txt' ) Warning The structure of the network must of course be the same as when the file was saved: number of populations, neurons and projections. The neuron and synapse types must define the same variables. If a variable was saved but does not exist anymore, it will be skipped. If the variable did not exist, its current value will be kept, what can lead to crashes. load() also accepts the populations and projections boolean flags (for example if you want to load only the synaptic weights but not to restore the neural variables). Populations and projections individually # Population and Projection objects also have save() and load() methods, allowing to save the corresponding information individually: pop1 . save ( 'pop1.npz' ) proj . save ( 'proj.npz' ) pop1 . load ( 'pop1.npz' ) proj . load ( 'proj.npz' ) The allowed file formats are: .npz : compressed Numpy binary format ( np.savez_compressed ), preferred. *.gz : gunzipped binary text file. *.mat : Matlab 7.2. * : binary text file. As before, .mat can only be used for saving, not loading.","title":"Saving and loading a network"},{"location":"manual/Saving/#saving-and-loading-a-network","text":"For the complete APIs, see IO in the library reference.","title":"Saving and loading a network"},{"location":"manual/Saving/#global-parameters","text":"The global parameters of the network (flagged with population or projection in the Neuron/Synapse definitions) can be saved to and loaded from a JSON file using the functions save_parameters() and load_parameters() : save_parameters ( 'network.json' ) load_parameters ( 'network.json' ) The saved JSON file for a network of two populations of Izhikevich neurons connected with STDP will look like: { \"populations\" : { \"pop0\" : { \"a\" : 0.1 , \"b\" : 0.2 , \"c\" : -65.0 , \"d\" : 8.0 , \"tau_ampa\" : 5.0 , \"tau_nmda\" : 150.0 , \"v_rev\" : 0.0 , \"v_thresh\" : 30.0 }, \"pop1\" : { \"a\" : 0.02 , \"b\" : 0.2 , \"c\" : -65.0 , \"d\" : 8.0 , \"tau_ampa\" : 5.0 , \"tau_nmda\" : 150.0 , \"v_rev\" : 0.0 , \"v_thresh\" : 30.0 } }, \"projections\" : { \"proj0\" : { \"tau_plus\" : 20.0 , \"tau_minus\" : 60.0 , \"A_plus\" : 0.0002 , \"A_minus\" : 6.6e-05 , \"w_max\" : 0.03 } }, \"network\" : {}, } By default, populations and projections have names like pop0 and proj1 . For readability, we advise setting explicit (and unique ) names in their constructor: pop = Population ( 100 , Izhikevich , name = \"PFC_exc\" ) proj = Projection ( pop , pop2 , 'exc' , STDP , name = \"PFC_exc_to_inh\" ) Only global parameters can be saved (no array is allowed in the JSON file). By default, only global parameters will be loaded, except if the global_only argument to load_parameters() is set to False. In that case, even local parameters can be set by the JSON file, but they will all use the same values. If you want to initialize other things than population/projection global parameters, you can define arbitrary values in the \"network\" dictionary: { \"network\" : { \"pop1_r_min\" : 0.1 , \"pop1_r_max\" : 1.3 , }, } load_parameters() will return the corresponding dictionary: params = load_parameters ( 'network.json' ) You can then use them to initialize programmatically non-global parameters or variables: pop1 . r = Uniform ( params [ 'pop1_r_min' ], params [ 'pop1_r_max' ])","title":"Global parameters"},{"location":"manual/Saving/#complete-state-of-the-network","text":"The state of all variables, including the synaptic weights, can be saved in a text file, compressed binary file or Matlab file using the save() method: save ( 'data.txt' ) save ( 'data.txt.gz' ) save ( 'data.mat' ) Filenames ending with .mat correspond to Matlab files (it requires the installation of Scipy), filenames ending with .gz are compressed using gzip (normally standard to all Python distributions, but may require installation), other extensions are normal text files using cPickle (standard). save() also accepts the populations and projections boolean flags. If True (the default), the neural resp. synaptic variables will be saved. For example, if you only care about synaptic plasticity but not the neural variables, you can set populations to False , and only synaptic variables will be saved. save ( 'data.txt' , populations = False ) Except for the Matlab format, you can also load the state of variables stored in these files once the network is compiled : load ( 'data.txt' ) Warning The structure of the network must of course be the same as when the file was saved: number of populations, neurons and projections. The neuron and synapse types must define the same variables. If a variable was saved but does not exist anymore, it will be skipped. If the variable did not exist, its current value will be kept, what can lead to crashes. load() also accepts the populations and projections boolean flags (for example if you want to load only the synaptic weights but not to restore the neural variables).","title":"Complete state of the network"},{"location":"manual/Saving/#populations-and-projections-individually","text":"Population and Projection objects also have save() and load() methods, allowing to save the corresponding information individually: pop1 . save ( 'pop1.npz' ) proj . save ( 'proj.npz' ) pop1 . load ( 'pop1.npz' ) proj . load ( 'proj.npz' ) The allowed file formats are: .npz : compressed Numpy binary format ( np.savez_compressed ), preferred. *.gz : gunzipped binary text file. *.mat : Matlab 7.2. * : binary text file. As before, .mat can only be used for saving, not loading.","title":"Populations and projections individually"},{"location":"manual/Simulation/","text":"Simulation # Compiling the network # Once all the relevant information has been defined, one needs to actually compile the network, by calling the ANNarchy.compile() method: compile () The optimized C++ code will be generated in the annarchy/ subfolder relative to your script, compiled, the underlying objects created and made available to the Python interface. You can specify the following arguments to compile() : directory : relative path to the directory where files will be generated and compiled (default: annarchy/ ) populations and projections : to compile only a subpart of the network, see Network . compiler : to select which C++ compiler will be used. By default g++ on Linux and clang++ on OS X are used, you can change it here. Note that only these two compilers are supported for now, and that they must be in your $PATH . compiler_flags : to select which flags are passed to the compiler. By default it is -march=native -O2 , but you can fine-tune it here. Beware that -O3 is most often a bad idea! Simulating the network # After the network is correctly compiled, the simulation can be run for the specified duration (in milliseconds) through the ANNarchy.simulate() method: simulate ( 1000.0 ) # Simulate for 1 second The provided duration should be a multiple of dt . If not, the number of simulation steps performed will be approximated. In some cases, you may want to perform only one step of the simulation, instead of specifing the duration. The ANNarchy.step() can then be used. step () # Simulate for 1 step Early-stopping # In some cases, it is desired to stop the simulation whenever a criterion is fulfilled (for example, a neural integrator exceeds a certain threshold), not after a fixed amount of time. There is the possibility to define a stop_condition at the Population level: pop1 = Population ( ... , stop_condition = \"r > 1.0\" ) When calling the simulate_until() method instead of simulate() : t = simulate_until ( max_duration = 1000.0 , populations = pop1 ) the simulation will be stopped whenever the stop_condition of pop1 is met, i.e. when the firing rate of any neuron of pop1 is above 1.0. If the condition is never met, the simulation will last maximally max_duration . The methods returns the effective duration of the simulation (to compute reaction times, for example). The stop_condition can use any logical operation on the parameters and variables of the neuron associated to the population: pop1 = Population ( ... , stop_condition = \"(r > 1.0) and (mp < 2.0)\" ) By default, the simulation stops when at least one neuron in the population fulfills the criterion. If you want to stop the simulation when all neurons fulfill the condition, you can use the flag all after the condition: pop1 = Population ( ... , stop_condition = \"r > 1.0 : all\" ) The flag any is the default behavior and can be omitted. The stop criterion can depend on several populations, by providing a list of populations to the populations argument instead of a single population: t = simulate_until ( max_duration = 1000.0 , populations = [ pop1 , pop2 ]) The simulation will then stop when the criterion is met in both populations at the same time. If you want that the simulation stops when at least one population meets its criterion, you can specify the operator argument: t = simulate_until ( max_duration = 1000.0 , populations = [ pop1 , pop2 ], operator = 'or' ) The default value of operator is a 'and' function between the populations' criteria. Warning Global operations (min, max, mean) are not possible inside the stop_condition . If you need them, store them in a variable in the equations argument of the neuron and use it as the condition: equations = \"\"\" r = ... max_r = max(r) \"\"\" Setting inputs periodically # In most cases, your simulation will be decomposed into a series of fixed-duration trials, where you basically set inputs at the beginning of the trial, run the simulation for a fixed duration, and possibly read out results at the end: # Iterate over 100 trials result = [] for trial in range ( 100 ): # Set inputs to the network pop . I = Uniform ( 0.0 , 1.0 ) # Simulate for 1 second simulate ( 1000. ) # Save the output result . append ( pop . r ) For convenience, we provide the decorator every , which allows to register a python method and call it automatically during the simulation with a fixed period: result = [] @every ( period = 1000. ) def set inputs ( n ): # Set inputs to the network pop . I = Uniform ( 0.0 , 1.0 ) # Save the output of the previous step if n > 0 : result . append ( pop . r ) simulate ( 100 * 1000. ) In this example, set_inputs() will be executed just before the steps corresponding to times t = 0., 1000., 2000., and so on until t = 100000. The method can have any name, but must accept only one argument, the integer n which will be incremented at each call of the method (i.e. it will take the values 0, 1, 2 until 99). This can for example be used to access data in a numpy array: images = np . random . random (( 100 , 640 , 480 )) @every ( period = 1000. ) def set inputs ( n ): # Set inputs to the network pop . I = images [ n , :, :] simulate ( 100 * 1000. ) One can define several methods that will be called in the order of their definition: @every ( period = 1000. ) def set inputs ( n ): pop . I = 1.0 @every ( period = 1000. ) def reset inputs ( n ): pop . I = 0.0 In this example, set_inputs() will be called first, followed by reset_inputs , so pop.I will finally be 0.0. The decorator every accepts an argument offset defining a delay within the period to call the method: @every ( period = 1000. ) def set inputs ( n ): pop . I = 1.0 @every ( period = 1000. , offset = 500. ) def reset inputs ( n ): pop . I = 0.0 In this case, set_inputs() will be called at times 0, 1000, 2000... while reset_inputs() will be called at times 500, 1500, 2500..., allowing to structure a trial more effectively. The offset can be set negative, in which case it will be relative to the end of the trial: @every ( period = 1000. , offset =- 100. ) def reset inputs ( n ): pop . I = 0.0 In this example, the method will be called at times 900, 1900, 2900 and so on. The offset value can not be longer than the period , by definition. If you try to do so, a modulo operation will anyway be applied (i.e. an offset of 1500 with a period of 1000 becomes 500). Finally, the wait argument allows to delay the first call to the method from a fixed interval: @every ( period = 1000. , wait = 5000. ) def reset inputs ( n ): pop . I = 0.0 In this case, the method will be called at times 5000, 6000 and so on. Between two calls to simulate() , the callbacks can be disabled or re-enabled using the following methods: @every ( period = 1000. ) def reset inputs ( n ): pop . I = 0.0 # Simulate with callbacks simulate ( 10000. ) # Disable callbacks disable_callbacks () # Simulate without callbacks simulate ( 10000. ) # Re-enable callbacks enable_callbacks () # Simulate with callbacks simulate ( 10000. ) Note that the period is always relative to the time when simulate() is called, so if no offset is defined, the callbacks will be called before the first step of a simulation, no matter how long the previous simulation lasted. In the current state, it is not possible yet to enable/disable callbacks selectively, it is all or none. Callbacks can only be used with simulate() , not with step() or simulate_until() .","title":"Simulation"},{"location":"manual/Simulation/#simulation","text":"","title":"Simulation"},{"location":"manual/Simulation/#compiling-the-network","text":"Once all the relevant information has been defined, one needs to actually compile the network, by calling the ANNarchy.compile() method: compile () The optimized C++ code will be generated in the annarchy/ subfolder relative to your script, compiled, the underlying objects created and made available to the Python interface. You can specify the following arguments to compile() : directory : relative path to the directory where files will be generated and compiled (default: annarchy/ ) populations and projections : to compile only a subpart of the network, see Network . compiler : to select which C++ compiler will be used. By default g++ on Linux and clang++ on OS X are used, you can change it here. Note that only these two compilers are supported for now, and that they must be in your $PATH . compiler_flags : to select which flags are passed to the compiler. By default it is -march=native -O2 , but you can fine-tune it here. Beware that -O3 is most often a bad idea!","title":"Compiling the network"},{"location":"manual/Simulation/#simulating-the-network","text":"After the network is correctly compiled, the simulation can be run for the specified duration (in milliseconds) through the ANNarchy.simulate() method: simulate ( 1000.0 ) # Simulate for 1 second The provided duration should be a multiple of dt . If not, the number of simulation steps performed will be approximated. In some cases, you may want to perform only one step of the simulation, instead of specifing the duration. The ANNarchy.step() can then be used. step () # Simulate for 1 step","title":"Simulating the network"},{"location":"manual/Simulation/#early-stopping","text":"In some cases, it is desired to stop the simulation whenever a criterion is fulfilled (for example, a neural integrator exceeds a certain threshold), not after a fixed amount of time. There is the possibility to define a stop_condition at the Population level: pop1 = Population ( ... , stop_condition = \"r > 1.0\" ) When calling the simulate_until() method instead of simulate() : t = simulate_until ( max_duration = 1000.0 , populations = pop1 ) the simulation will be stopped whenever the stop_condition of pop1 is met, i.e. when the firing rate of any neuron of pop1 is above 1.0. If the condition is never met, the simulation will last maximally max_duration . The methods returns the effective duration of the simulation (to compute reaction times, for example). The stop_condition can use any logical operation on the parameters and variables of the neuron associated to the population: pop1 = Population ( ... , stop_condition = \"(r > 1.0) and (mp < 2.0)\" ) By default, the simulation stops when at least one neuron in the population fulfills the criterion. If you want to stop the simulation when all neurons fulfill the condition, you can use the flag all after the condition: pop1 = Population ( ... , stop_condition = \"r > 1.0 : all\" ) The flag any is the default behavior and can be omitted. The stop criterion can depend on several populations, by providing a list of populations to the populations argument instead of a single population: t = simulate_until ( max_duration = 1000.0 , populations = [ pop1 , pop2 ]) The simulation will then stop when the criterion is met in both populations at the same time. If you want that the simulation stops when at least one population meets its criterion, you can specify the operator argument: t = simulate_until ( max_duration = 1000.0 , populations = [ pop1 , pop2 ], operator = 'or' ) The default value of operator is a 'and' function between the populations' criteria. Warning Global operations (min, max, mean) are not possible inside the stop_condition . If you need them, store them in a variable in the equations argument of the neuron and use it as the condition: equations = \"\"\" r = ... max_r = max(r) \"\"\"","title":"Early-stopping"},{"location":"manual/Simulation/#setting-inputs-periodically","text":"In most cases, your simulation will be decomposed into a series of fixed-duration trials, where you basically set inputs at the beginning of the trial, run the simulation for a fixed duration, and possibly read out results at the end: # Iterate over 100 trials result = [] for trial in range ( 100 ): # Set inputs to the network pop . I = Uniform ( 0.0 , 1.0 ) # Simulate for 1 second simulate ( 1000. ) # Save the output result . append ( pop . r ) For convenience, we provide the decorator every , which allows to register a python method and call it automatically during the simulation with a fixed period: result = [] @every ( period = 1000. ) def set inputs ( n ): # Set inputs to the network pop . I = Uniform ( 0.0 , 1.0 ) # Save the output of the previous step if n > 0 : result . append ( pop . r ) simulate ( 100 * 1000. ) In this example, set_inputs() will be executed just before the steps corresponding to times t = 0., 1000., 2000., and so on until t = 100000. The method can have any name, but must accept only one argument, the integer n which will be incremented at each call of the method (i.e. it will take the values 0, 1, 2 until 99). This can for example be used to access data in a numpy array: images = np . random . random (( 100 , 640 , 480 )) @every ( period = 1000. ) def set inputs ( n ): # Set inputs to the network pop . I = images [ n , :, :] simulate ( 100 * 1000. ) One can define several methods that will be called in the order of their definition: @every ( period = 1000. ) def set inputs ( n ): pop . I = 1.0 @every ( period = 1000. ) def reset inputs ( n ): pop . I = 0.0 In this example, set_inputs() will be called first, followed by reset_inputs , so pop.I will finally be 0.0. The decorator every accepts an argument offset defining a delay within the period to call the method: @every ( period = 1000. ) def set inputs ( n ): pop . I = 1.0 @every ( period = 1000. , offset = 500. ) def reset inputs ( n ): pop . I = 0.0 In this case, set_inputs() will be called at times 0, 1000, 2000... while reset_inputs() will be called at times 500, 1500, 2500..., allowing to structure a trial more effectively. The offset can be set negative, in which case it will be relative to the end of the trial: @every ( period = 1000. , offset =- 100. ) def reset inputs ( n ): pop . I = 0.0 In this example, the method will be called at times 900, 1900, 2900 and so on. The offset value can not be longer than the period , by definition. If you try to do so, a modulo operation will anyway be applied (i.e. an offset of 1500 with a period of 1000 becomes 500). Finally, the wait argument allows to delay the first call to the method from a fixed interval: @every ( period = 1000. , wait = 5000. ) def reset inputs ( n ): pop . I = 0.0 In this case, the method will be called at times 5000, 6000 and so on. Between two calls to simulate() , the callbacks can be disabled or re-enabled using the following methods: @every ( period = 1000. ) def reset inputs ( n ): pop . I = 0.0 # Simulate with callbacks simulate ( 10000. ) # Disable callbacks disable_callbacks () # Simulate without callbacks simulate ( 10000. ) # Re-enable callbacks enable_callbacks () # Simulate with callbacks simulate ( 10000. ) Note that the period is always relative to the time when simulate() is called, so if no offset is defined, the callbacks will be called before the first step of a simulation, no matter how long the previous simulation lasted. In the current state, it is not possible yet to enable/disable callbacks selectively, it is all or none. Callbacks can only be used with simulate() , not with step() or simulate_until() .","title":"Setting inputs periodically"},{"location":"manual/SpikeNeuron/","text":"Spiking neurons # Contrary to rate-coded neurons, the use of spiking neurons requires the additional definition of a spike condition (the criteria defining the emission of a spike, typically when the membrane potential exceeds a threshold) and reset equations, governing the evolution of all variables after a spike is emitted. Built-in neurons # ANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN ( http://neuralensemble.org/docs/PyNN/reference/neuronmodels.html ). Their definition (parameters, equations) are described in Specific Neurons . The classes can be used directly when creating the populations (no need to instantiate them). Example: pop = Population ( geometry = 1000 , neuron = Izhikevich ) User-defined neurons # Let's consider a simple leaky integrate-and-fire spiking neuron model (LIF) using a voltage-gated excitatory conductance: \\[\\tau \\cdot \\frac{ d v(t) }{ dt } = (E_r - v(t) ) + g_\\text{exc}(t) \\cdot (E_e - v(t) )\\] where \\(v(t)\\) is the membrane potential, \\(\\tau\\) is the membrane time constant (in milliseconds), \\(E_r\\) the resting potential, \\(E_e\\) the target potential for excitatory synapses and \\(g_\\text{exc}(t)\\) the total current induced by excitatory synapses. This neural model can be defined in ANNarchy by: LIF = Neuron ( parameters = \"\"\" tau = 10.0 : population Er = -60.0 : population Ee = 0.0 : population T = -45.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0 \"\"\" , spike = \"v > T\" , reset = \"v = Er\" , refractory = 5.0 ) As for rate-coded neurons, the parameters are defined in the parameters description, here globally for the population. equations contains the description of the ODE followed by the membrane potential. The additional information to provide is: spike : a boolean condition on a single variable (typically the membrane potential) deciding when a spike is emitted. reset : the modifications to the neuron's variables after a spike is emitted (typically, clamping the membrane potential to its reset potential). refractory : optionally a refractory period in ms. Spike condition # The spike condition is a single constraint definition. You may use the different available comparison operators (>, \\<, ==, etc) on a single neuron variable, using as many parameters as you want. The use of assignment statements or ODEs will lead to an error. Conditional statements can be used. Example: parameters = \"\"\" ... T = -45.0 \"\"\" , equations = \"\"\" prev_v = v noise = Uniform (-5.0, 5.0) tau*dv/dt = E - v + g_exc \"\"\" , spike = \"\"\" (v > T + noise) and (prev_v < T + noise) \"\"\" Reset # Here you define the variables which should be set to certain values after a spike occured. Any assignment statements is allowed ( = , += , etc), but the use of ODEs is not possible, as the reset is performed only once at the end of the time step. Example: reset = \"\"\" v = Er u += 0.1 \"\"\" Conductances # Contrary to rate-coded neurons, spiking neurons use conductance variables to encode the received inputs, not weighted sums. In ANNarchy, the conductances are defined by g_ followed by the target name. For example, if a population receives excitatory input (target exc ) from another one, you can access the total conductance provoked by exc spikes with: tau * dv / dt + v = g_exc The dynamics of the conductance can be specified after its usage in the membrane potential equation. The default behaviour for conductances is an instantaneous reset (or infinitely fast exponential decay). In practice, this means that all incoming spikes are summed up (weighted by the synaptic efficiency) at the beginning of a simulation step, and the resulting conductance is reset to 0.0 at the end of the step. This default behaviour is equivalent to : LIF = Neuron ( parameters = \"\"\" ... \"\"\" , equations = \"\"\" tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0 g_exc = 0.0 \"\"\" , spike = \" ... \" , reset = \" ... \" ) Incoming spikes increase g_exc and can provoke a post-synaptic spike at the next step, but leave no trace beyond that point. Most models however use exponentially decaying synapses , where the conductance decays with a short time constant after a spike is received. This behavior should be explicitely specified in the neuron's equations: LIF = Neuron ( parameters = \"\"\" ... \"\"\" , equations = \"\"\" tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0 tau_exc * dg_exc/dt = - g_exc \"\"\" , spike = \" ... \" , reset = \" ... \" ) g_exc is increased by incoming spikes, and slowly decays back to 0.0 until the next spikes arrive. Refractory period # The refractory period in milliseconds is specified by the refractory parameter of Neuron . LIF = Neuron ( parameters = \"\"\" ... \"\"\" , equations = \"\"\" ... \"\"\" , spike = \"\"\" ... \"\"\" , reset = \"\"\" v = c u += d \"\"\" , refractory = 5.0 ) The refractory argument can be a floating value or the name of a parameter/variable (string). If dt = 0.1 , this means that the equations will not be evaluated for 50 consecutive steps after a spike is emitted, except for the conductances (starting with g_ ) which are evaluated normally during the refractory period (the neuron is not \\\"deaf\\\", it only is frozen in a refractory state). refractory becomes an attribute of a spiking Population object, so it can be set specifically for a population even when omitted in the neuron definition: LIF = Neuron ( parameters = \" ... \" , equations = \" ... \" , spike = \" ... \" , reset = \"\"\" v = c u += d \"\"\" ) pop = Population ( geometry = 1000 , neuron = LIF ) pop . refractory = Uniform ( 1.0 , 10.0 ) It can be either a single value, a RandomDistribution object or a Numpy array of the same size/geometry as the population. Instantaneous firing rate # Method 1: ISI Spiking neurons define an additional variable t_last which represents the timestamp (in ms) of the last emitted spike (updated at the end of the reset statement). The time elapsed since the last spike is then t - t_last . This can be used to update the instantaneous firing rate of a neuron, by inverting the inter-spike interval (ISI) during the reset statement following the emission of a spike: neuron = Neuron ( parameters = \"tau = 20.0; tauf = 1000.\" , equations = \"\"\" tau * dv/dt + v = ... tauf * df/dt = -f \"\"\" , spike = \"v > 1.0\" , reset = \"\"\" v = 0.0 f = 1000./(t - t_last) \"\"\" ) Here, a leaky integrator on f is needed to 1) smooth the firing rate and 2) slowly decay to 0 when the neuron stops firing. This method reflects very fast changes in the firing rate, but is also very sensible to noise. Method 2: Window A more stable way to compute the firing rate of a neuron is to count at each time step the number of spikes emitted during a sliding temporal window (of 100 ms or 1s for example). By default, spiking neurons only record the time of the last spike they emitted ( t_last ), so this mechanism has to be explicitely enabled by calling the compute_firing_rate() method of the desired population: pop = Population ( 100 , Izhikevich ) pop . compute_firing_rate ( window = 1000.0 ) The window argument represents the period in milliseconds over which the spikes will be counted. The resulting firing rate (in Hz) will be stored in the local variable r (as for rate-coded neurons), which can be accessed by the neuron itself or by incoming and outgoing synapse ( pre.r and post.r ). If the method has not been called, the variable r of a spiking neuron will be constantly 0.0. Warning The window method is not available on CUDA yet.","title":"Spiking neurons"},{"location":"manual/SpikeNeuron/#spiking-neurons","text":"Contrary to rate-coded neurons, the use of spiking neurons requires the additional definition of a spike condition (the criteria defining the emission of a spike, typically when the membrane potential exceeds a threshold) and reset equations, governing the evolution of all variables after a spike is emitted.","title":"Spiking neurons"},{"location":"manual/SpikeNeuron/#built-in-neurons","text":"ANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN ( http://neuralensemble.org/docs/PyNN/reference/neuronmodels.html ). Their definition (parameters, equations) are described in Specific Neurons . The classes can be used directly when creating the populations (no need to instantiate them). Example: pop = Population ( geometry = 1000 , neuron = Izhikevich )","title":"Built-in neurons"},{"location":"manual/SpikeNeuron/#user-defined-neurons","text":"Let's consider a simple leaky integrate-and-fire spiking neuron model (LIF) using a voltage-gated excitatory conductance: \\[\\tau \\cdot \\frac{ d v(t) }{ dt } = (E_r - v(t) ) + g_\\text{exc}(t) \\cdot (E_e - v(t) )\\] where \\(v(t)\\) is the membrane potential, \\(\\tau\\) is the membrane time constant (in milliseconds), \\(E_r\\) the resting potential, \\(E_e\\) the target potential for excitatory synapses and \\(g_\\text{exc}(t)\\) the total current induced by excitatory synapses. This neural model can be defined in ANNarchy by: LIF = Neuron ( parameters = \"\"\" tau = 10.0 : population Er = -60.0 : population Ee = 0.0 : population T = -45.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0 \"\"\" , spike = \"v > T\" , reset = \"v = Er\" , refractory = 5.0 ) As for rate-coded neurons, the parameters are defined in the parameters description, here globally for the population. equations contains the description of the ODE followed by the membrane potential. The additional information to provide is: spike : a boolean condition on a single variable (typically the membrane potential) deciding when a spike is emitted. reset : the modifications to the neuron's variables after a spike is emitted (typically, clamping the membrane potential to its reset potential). refractory : optionally a refractory period in ms.","title":"User-defined neurons"},{"location":"manual/SpikeNeuron/#spike-condition","text":"The spike condition is a single constraint definition. You may use the different available comparison operators (>, \\<, ==, etc) on a single neuron variable, using as many parameters as you want. The use of assignment statements or ODEs will lead to an error. Conditional statements can be used. Example: parameters = \"\"\" ... T = -45.0 \"\"\" , equations = \"\"\" prev_v = v noise = Uniform (-5.0, 5.0) tau*dv/dt = E - v + g_exc \"\"\" , spike = \"\"\" (v > T + noise) and (prev_v < T + noise) \"\"\"","title":"Spike condition"},{"location":"manual/SpikeNeuron/#reset","text":"Here you define the variables which should be set to certain values after a spike occured. Any assignment statements is allowed ( = , += , etc), but the use of ODEs is not possible, as the reset is performed only once at the end of the time step. Example: reset = \"\"\" v = Er u += 0.1 \"\"\"","title":"Reset"},{"location":"manual/SpikeNeuron/#conductances","text":"Contrary to rate-coded neurons, spiking neurons use conductance variables to encode the received inputs, not weighted sums. In ANNarchy, the conductances are defined by g_ followed by the target name. For example, if a population receives excitatory input (target exc ) from another one, you can access the total conductance provoked by exc spikes with: tau * dv / dt + v = g_exc The dynamics of the conductance can be specified after its usage in the membrane potential equation. The default behaviour for conductances is an instantaneous reset (or infinitely fast exponential decay). In practice, this means that all incoming spikes are summed up (weighted by the synaptic efficiency) at the beginning of a simulation step, and the resulting conductance is reset to 0.0 at the end of the step. This default behaviour is equivalent to : LIF = Neuron ( parameters = \"\"\" ... \"\"\" , equations = \"\"\" tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0 g_exc = 0.0 \"\"\" , spike = \" ... \" , reset = \" ... \" ) Incoming spikes increase g_exc and can provoke a post-synaptic spike at the next step, but leave no trace beyond that point. Most models however use exponentially decaying synapses , where the conductance decays with a short time constant after a spike is received. This behavior should be explicitely specified in the neuron's equations: LIF = Neuron ( parameters = \"\"\" ... \"\"\" , equations = \"\"\" tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0 tau_exc * dg_exc/dt = - g_exc \"\"\" , spike = \" ... \" , reset = \" ... \" ) g_exc is increased by incoming spikes, and slowly decays back to 0.0 until the next spikes arrive.","title":"Conductances"},{"location":"manual/SpikeNeuron/#refractory-period","text":"The refractory period in milliseconds is specified by the refractory parameter of Neuron . LIF = Neuron ( parameters = \"\"\" ... \"\"\" , equations = \"\"\" ... \"\"\" , spike = \"\"\" ... \"\"\" , reset = \"\"\" v = c u += d \"\"\" , refractory = 5.0 ) The refractory argument can be a floating value or the name of a parameter/variable (string). If dt = 0.1 , this means that the equations will not be evaluated for 50 consecutive steps after a spike is emitted, except for the conductances (starting with g_ ) which are evaluated normally during the refractory period (the neuron is not \\\"deaf\\\", it only is frozen in a refractory state). refractory becomes an attribute of a spiking Population object, so it can be set specifically for a population even when omitted in the neuron definition: LIF = Neuron ( parameters = \" ... \" , equations = \" ... \" , spike = \" ... \" , reset = \"\"\" v = c u += d \"\"\" ) pop = Population ( geometry = 1000 , neuron = LIF ) pop . refractory = Uniform ( 1.0 , 10.0 ) It can be either a single value, a RandomDistribution object or a Numpy array of the same size/geometry as the population.","title":"Refractory period"},{"location":"manual/SpikeNeuron/#instantaneous-firing-rate","text":"Method 1: ISI Spiking neurons define an additional variable t_last which represents the timestamp (in ms) of the last emitted spike (updated at the end of the reset statement). The time elapsed since the last spike is then t - t_last . This can be used to update the instantaneous firing rate of a neuron, by inverting the inter-spike interval (ISI) during the reset statement following the emission of a spike: neuron = Neuron ( parameters = \"tau = 20.0; tauf = 1000.\" , equations = \"\"\" tau * dv/dt + v = ... tauf * df/dt = -f \"\"\" , spike = \"v > 1.0\" , reset = \"\"\" v = 0.0 f = 1000./(t - t_last) \"\"\" ) Here, a leaky integrator on f is needed to 1) smooth the firing rate and 2) slowly decay to 0 when the neuron stops firing. This method reflects very fast changes in the firing rate, but is also very sensible to noise. Method 2: Window A more stable way to compute the firing rate of a neuron is to count at each time step the number of spikes emitted during a sliding temporal window (of 100 ms or 1s for example). By default, spiking neurons only record the time of the last spike they emitted ( t_last ), so this mechanism has to be explicitely enabled by calling the compute_firing_rate() method of the desired population: pop = Population ( 100 , Izhikevich ) pop . compute_firing_rate ( window = 1000.0 ) The window argument represents the period in milliseconds over which the spikes will be counted. The resulting firing rate (in Hz) will be stored in the local variable r (as for rate-coded neurons), which can be accessed by the neuron itself or by incoming and outgoing synapse ( pre.r and post.r ). If the method has not been called, the variable r of a spiking neuron will be constantly 0.0. Warning The window method is not available on CUDA yet.","title":"Instantaneous firing rate"},{"location":"manual/SpikeSynapse/","text":"Spiking synapses # Synapses in spiking networks differ from rate-coded synapses in that they are event-driven, i.e. the most important changes occur whenever a pre- or post-synaptic spike is emitted. For this reason, additional arguments have to be passed to the Synapse object. Increase of conductance after a pre-synaptic spike # In the simplest case, a pre-synaptic spike increases a target conductance value in the post-synaptic neuron. The rule defining how this conductance is modified has to be placed in the pre_spike argument of a Synapse object. The default spiking synapse in ANNarchy is equivalent to: DefaultSynapse = Synapse ( parameters = \"w=0.0\" , equations = \"\" , pre_spike = \"\"\" g_target += w \"\"\" ) The only thing it does is to increase the conductance g_target of the post-synaptic neuron (for example g_exc if the target is exc ) every time a pre-synaptic spike arrives at the synapse, proportionally to the synaptic efficiency w of the synapse. Note that w is implicitely defined in all synapses, you will never need to define it explicitely. You can override this default behavior by providing a new Synapse object when building a Projection . For example, you may want to implement a \\\"fatigue\\\" mechanism for the synapse, transciently reducing the synaptic efficiency when the pre-synaptic neuron fires too strongly. One solution would be to decrease a synaptic variable everytime a pre-synaptic spike is received and increase the post-synaptic conductance proportionally to this value. When no spike is received, this trace variable should slowly return to its maximal value. FatigueSynapse = Synapse ( parameters = \"\"\" tau = 1000 : postsynaptic # Time constant of the trace is 1 second dec = 0.05 : postsynaptic # Decrement of the trace \"\"\" , equations = \"\"\" tau * dtrace/dt + trace = 1.0 : min = 0.0 \"\"\" , pre_spike = \"\"\" g_target += w * trace trace -= dec \"\"\" ) Each time a pre-synaptic spike occurs, the post-synaptic conductance is increased from w*trace . As the baseline of trace is 1.0 (as defined in equations ), this means that a \\\"fresh\\\" synapse will use the full synaptic efficiency. However, after each pre-synaptic spike, trace is decreased from dec = 0.05 , meaning that the \\\"real\\\" synaptic efficiency can go down to 0.0 (the minimal value of trace) if the pre-synaptic neuron fires too often. It is important here to restrict trace to positive values with the flags min=0.0 , as it could otherwise transform an excitatory synapse into an inhibitory one. Hint It is obligatory to use the keyword g_target for the post-synaptic conductance. This value relates to the corresponding value in post-synaptic neuron: The target will be replaced with the projection's target (for example exc or inh ). So if you use this synapse in a projection with target = \\'exc\\', the value of g_exc in post-synaptic neuron will be automatically replaced. Synaptic plasticity # In spiking networks, there are usually two methods to implement event-driven synaptic plasticity (see the entry on STDP at Scholarpedia ): by using the difference in spike times between the pre- and post-synaptic neurons; by using online implementations. Using spike-time differences # A Synapse has access to two specific variables: t_pre corresponding to the time of the last pre-synaptic spike in milliseconds. t_post corresponding to the time of the last post-synaptic spike in milliseconds. These times are relative to the creation of the network, so they only make sense when compared to each other or to t . Spike-timing dependent plasticity can for example be implemented the following way: STDP = Synapse ( parameters = \"\"\" tau_pre = 10.0 : projection tau_post = 10.0 : projection cApre = 0.01 : projection cApost = 0.0105 : projection wmax = 0.01 : projection \"\"\" , pre_spike = \"\"\" g_target += w w = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \"\"\" , post_spike = \"\"\" w = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax) \"\"\" ) Every time a pre-synaptic spike arrives at the synapse ( pre_spike ), the post-synaptic conductance is increased from the current value of the synaptic efficiency. g_target += w When a synapse object is defined, this behavior should be explicitely declared. The value w is then decreased using a decreasing exponential function of the time elapsed since the last post-synaptic spike: w = clip ( w - cApost * exp (( t_post - t ) / tau_post ) , 0.0 , wmax ) The clip() global function is there to ensure that w is bounded between 0.0 and wmax . As t >= t_post , the exponential part is smaller than 1.0. The pre_spike argument therefore ensures that the synapse is depressed is a pre-synaptic spike occurs shortly after a post-synaptic one. \\\"Shortly\\\" is quantified by the time constant tau_post , usually in the range of 10 ms. Every time a post-synaptic spike is emitted ( post_spike ), the value w is increased proportionally to the time elapsed since the last pre-synaptic spike: w = clip ( w + cApre * exp (( t_pre - t ) / tau_pre ) , 0.0 , wmax ) This term defines the potentiation of a synapse when a pre-synaptic spike is followed immediately by a post-synaptic one: the inferred causality between the two events should be reinforced. Warning Only the last pre- and post-synaptic spikes are accessible, not the whole history. Only nearest-neighbor spike-interactions are possible using ANNarchy, not temporal all-to-all interactions where the whole spike history is used for learning (see the entry on STDP at Scholarpedia ). Some networks may not work properly when using this simulation mode. For example, whenever the pre-synaptic neurons fires twice in a very short interval and causes a post-synaptic spike, the corresponding weight should be reinforced twice. With the proposed STDP rule, it would be reinforced only once. It is therefore generally advised to use online versions of STDP. Online version # The online version of STDP requires two synaptic traces, which are increased whenever a pre- resp. post-synaptic spike is perceived, and decay with their own dynamics in between. Using the same vocabulary as Brian, such an implementation would be: STDP_online = Synapse ( parameters = \"\"\" tau_pre = 10.0 : projection tau_post = 10.0 : projection cApre = 0.01 : projection cApost = 0.0105 : projection wmax = 0.01 : projection \"\"\" , equations = \"\"\" tau_pre * dApre/dt = - Apre : event-driven tau_post * dApost/dt = - Apost : event-driven \"\"\" , pre_spike = \"\"\" g_target += w Apre += cApre * wmax w = clip(w - Apost, 0.0 , wmax) \"\"\" , post_spike = \"\"\" Apost += cApost * wmax w = clip(w + Apre, 0.0 , wmax) \"\"\" ) The variables Apre and Apost are exponentially decreasing traces of pre- and post-synaptic spikes, as shown by the leaky integration in equations . When a pre-synaptic spike is emitted, Apre is incremented, the conductance level of the post-synaptic neuron g_target too, and the synaptic efficiency is decreased proportionally to Apost (this means that if a post-synaptic spike was emitted shortly before, LTD will strongly be applied, while if it was longer ago, no major change will be observed). When a post-synaptic spike is observed, Apost increases and the synaptic efficiency is increased proportionally to Apre . The effect of this online version is globally the same as the spike timing dependent version, except that the history of pre- and post-synaptic spikes is fully contained in the variables Apre and Apost . The event-driven keyword allows event-driven integration of the variables Apre and Apost . This means the equations are not updated at each time step, but only when a pre- or post-synaptic spike occurs at the synapse. This is only possible because the two variables follow linear first-order ODEs. The event-driven integration method allows to spare a lot of computations if the number of spikes is not too high in the network. Order of evaluation # Three types of updates are potentially executed at every time step: Pre-synaptic events, defined by pre_spike and triggered after each pre-synaptic spike, after a delay of at least dt . Synaptic variables defined by equations . Post-synaptic events, defined by post_spike and triggered after each post-synaptic spike, without delay. These updates are conducted in that order at each time step. First, all spikes emitted in the previous step (or earlier if there are delays) are propagated to the corresponding synapses and influence variables there (especially conductance increases), then all synaptic variables are updated according to their ODE (after the neurons\\' equations are updated), then all neurons which have emitted a spike in the current step modify their synapses. A potential problem arises when a pre-synaptic and a post-synaptic spike are emitted at the same time. STDP-like plasticity rules are usually not defined when the spike time difference is 0, as the two spikes can not be correlated in that case (the pre-spike can not possibly be the cause of the post-spike). By default, both event-driven updates ( pre_spike leading to LTD, post_spike leading to LTP) will be conducted when the spikes are emitted at the same time. This can be problematic for some plastic models, for example the simple_stdp example provided in the source code. To avoid this problem, the flag unless_post can be specified in pre_spike to indicate that the corresponding variable should be updated after each pre-synaptic spike, unless the post-synaptic neuron also fired at the previous time step. Without even-driven integration, the online STDP learning rule would become: STDP_online = Synapse ( parameters = \"\"\" tau_pre = 10.0 : projection tau_post = 10.0 : projection cApre = 0.01 : projection cApost = 0.0105 : projection wmax = 0.01 : projection \"\"\" , equations = \"\"\" tau_pre * dApre/dt = - Apre tau_post * dApost/dt = - Apost \"\"\" , pre_spike = \"\"\" g_target += w Apre += cApre : unless_post w = clip(w - Apost, 0.0 , wmax) : unless_post \"\"\" , post_spike = \"\"\" Apost += cApost w = clip(w + Apre, 0.0 , wmax) \"\"\" ) Continuous synaptic transmission # In some cases, synaptic transmission cannot be described in an event-driven framework. Synapses using the NMDA neurotransmitter are for example often modeled as non-linear synapses. Non-linear synapses can require the post-synaptic conductance to be a sum of synapse-specific variables, as for rate-coded neurons, and not simply incremented when a pre-synaptic spike occurs. NMDA synapses can be represented by two variables \\(x(t)\\) and \\(g(t)\\) following first-order ODEs: \\[\\begin{aligned} \\begin{aligned} \\tau \\cdot \\frac{dx(t)}{dt} &= - x(t) \\\\ \\tau \\cdot \\frac{dg(t)}{dt} &= - g(t) + x(t) \\cdot (1 - g(t)) \\end{aligned} \\end{aligned}\\] When a pre-synaptic spike occurs, \\(x(t)\\) is incremented by the weight \\(w(t)\\) . However, it does not influence directly the post-synaptic neuron, as the output of a synapse is the signal \\(g(t)\\) . The post-synaptic conductance is defined at each time \\(t\\) as the sum over all synapses of the same type of their variable \\(g(t)\\) : \\[g_\\text{exc}(t) = \\sum_{i=1}^{N_\\text{exc}} g_i (t)\\] Such a synapse could be implemented the following way: NMDA = Synapse( parameters = \"\"\" tau = 10.0 : projection \"\"\", equations = \"\"\" tau * dx/dt = -x tau * dg/dt = -g + x * (1 -g) \"\"\", pre_spike = \"x += w\", psp = \"g\" ) The synapse defines a psp argument which means that the output of this synapse is non-linear and the post-synaptic conductance should be summed over this value ( g in this case). It is not possible to use the event-driven integration scheme for such non-linear synapses.","title":"Spiking synapses"},{"location":"manual/SpikeSynapse/#spiking-synapses","text":"Synapses in spiking networks differ from rate-coded synapses in that they are event-driven, i.e. the most important changes occur whenever a pre- or post-synaptic spike is emitted. For this reason, additional arguments have to be passed to the Synapse object.","title":"Spiking synapses"},{"location":"manual/SpikeSynapse/#increase-of-conductance-after-a-pre-synaptic-spike","text":"In the simplest case, a pre-synaptic spike increases a target conductance value in the post-synaptic neuron. The rule defining how this conductance is modified has to be placed in the pre_spike argument of a Synapse object. The default spiking synapse in ANNarchy is equivalent to: DefaultSynapse = Synapse ( parameters = \"w=0.0\" , equations = \"\" , pre_spike = \"\"\" g_target += w \"\"\" ) The only thing it does is to increase the conductance g_target of the post-synaptic neuron (for example g_exc if the target is exc ) every time a pre-synaptic spike arrives at the synapse, proportionally to the synaptic efficiency w of the synapse. Note that w is implicitely defined in all synapses, you will never need to define it explicitely. You can override this default behavior by providing a new Synapse object when building a Projection . For example, you may want to implement a \\\"fatigue\\\" mechanism for the synapse, transciently reducing the synaptic efficiency when the pre-synaptic neuron fires too strongly. One solution would be to decrease a synaptic variable everytime a pre-synaptic spike is received and increase the post-synaptic conductance proportionally to this value. When no spike is received, this trace variable should slowly return to its maximal value. FatigueSynapse = Synapse ( parameters = \"\"\" tau = 1000 : postsynaptic # Time constant of the trace is 1 second dec = 0.05 : postsynaptic # Decrement of the trace \"\"\" , equations = \"\"\" tau * dtrace/dt + trace = 1.0 : min = 0.0 \"\"\" , pre_spike = \"\"\" g_target += w * trace trace -= dec \"\"\" ) Each time a pre-synaptic spike occurs, the post-synaptic conductance is increased from w*trace . As the baseline of trace is 1.0 (as defined in equations ), this means that a \\\"fresh\\\" synapse will use the full synaptic efficiency. However, after each pre-synaptic spike, trace is decreased from dec = 0.05 , meaning that the \\\"real\\\" synaptic efficiency can go down to 0.0 (the minimal value of trace) if the pre-synaptic neuron fires too often. It is important here to restrict trace to positive values with the flags min=0.0 , as it could otherwise transform an excitatory synapse into an inhibitory one. Hint It is obligatory to use the keyword g_target for the post-synaptic conductance. This value relates to the corresponding value in post-synaptic neuron: The target will be replaced with the projection's target (for example exc or inh ). So if you use this synapse in a projection with target = \\'exc\\', the value of g_exc in post-synaptic neuron will be automatically replaced.","title":"Increase of conductance after a pre-synaptic spike"},{"location":"manual/SpikeSynapse/#synaptic-plasticity","text":"In spiking networks, there are usually two methods to implement event-driven synaptic plasticity (see the entry on STDP at Scholarpedia ): by using the difference in spike times between the pre- and post-synaptic neurons; by using online implementations.","title":"Synaptic plasticity"},{"location":"manual/SpikeSynapse/#using-spike-time-differences","text":"A Synapse has access to two specific variables: t_pre corresponding to the time of the last pre-synaptic spike in milliseconds. t_post corresponding to the time of the last post-synaptic spike in milliseconds. These times are relative to the creation of the network, so they only make sense when compared to each other or to t . Spike-timing dependent plasticity can for example be implemented the following way: STDP = Synapse ( parameters = \"\"\" tau_pre = 10.0 : projection tau_post = 10.0 : projection cApre = 0.01 : projection cApost = 0.0105 : projection wmax = 0.01 : projection \"\"\" , pre_spike = \"\"\" g_target += w w = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \"\"\" , post_spike = \"\"\" w = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax) \"\"\" ) Every time a pre-synaptic spike arrives at the synapse ( pre_spike ), the post-synaptic conductance is increased from the current value of the synaptic efficiency. g_target += w When a synapse object is defined, this behavior should be explicitely declared. The value w is then decreased using a decreasing exponential function of the time elapsed since the last post-synaptic spike: w = clip ( w - cApost * exp (( t_post - t ) / tau_post ) , 0.0 , wmax ) The clip() global function is there to ensure that w is bounded between 0.0 and wmax . As t >= t_post , the exponential part is smaller than 1.0. The pre_spike argument therefore ensures that the synapse is depressed is a pre-synaptic spike occurs shortly after a post-synaptic one. \\\"Shortly\\\" is quantified by the time constant tau_post , usually in the range of 10 ms. Every time a post-synaptic spike is emitted ( post_spike ), the value w is increased proportionally to the time elapsed since the last pre-synaptic spike: w = clip ( w + cApre * exp (( t_pre - t ) / tau_pre ) , 0.0 , wmax ) This term defines the potentiation of a synapse when a pre-synaptic spike is followed immediately by a post-synaptic one: the inferred causality between the two events should be reinforced. Warning Only the last pre- and post-synaptic spikes are accessible, not the whole history. Only nearest-neighbor spike-interactions are possible using ANNarchy, not temporal all-to-all interactions where the whole spike history is used for learning (see the entry on STDP at Scholarpedia ). Some networks may not work properly when using this simulation mode. For example, whenever the pre-synaptic neurons fires twice in a very short interval and causes a post-synaptic spike, the corresponding weight should be reinforced twice. With the proposed STDP rule, it would be reinforced only once. It is therefore generally advised to use online versions of STDP.","title":"Using spike-time differences"},{"location":"manual/SpikeSynapse/#online-version","text":"The online version of STDP requires two synaptic traces, which are increased whenever a pre- resp. post-synaptic spike is perceived, and decay with their own dynamics in between. Using the same vocabulary as Brian, such an implementation would be: STDP_online = Synapse ( parameters = \"\"\" tau_pre = 10.0 : projection tau_post = 10.0 : projection cApre = 0.01 : projection cApost = 0.0105 : projection wmax = 0.01 : projection \"\"\" , equations = \"\"\" tau_pre * dApre/dt = - Apre : event-driven tau_post * dApost/dt = - Apost : event-driven \"\"\" , pre_spike = \"\"\" g_target += w Apre += cApre * wmax w = clip(w - Apost, 0.0 , wmax) \"\"\" , post_spike = \"\"\" Apost += cApost * wmax w = clip(w + Apre, 0.0 , wmax) \"\"\" ) The variables Apre and Apost are exponentially decreasing traces of pre- and post-synaptic spikes, as shown by the leaky integration in equations . When a pre-synaptic spike is emitted, Apre is incremented, the conductance level of the post-synaptic neuron g_target too, and the synaptic efficiency is decreased proportionally to Apost (this means that if a post-synaptic spike was emitted shortly before, LTD will strongly be applied, while if it was longer ago, no major change will be observed). When a post-synaptic spike is observed, Apost increases and the synaptic efficiency is increased proportionally to Apre . The effect of this online version is globally the same as the spike timing dependent version, except that the history of pre- and post-synaptic spikes is fully contained in the variables Apre and Apost . The event-driven keyword allows event-driven integration of the variables Apre and Apost . This means the equations are not updated at each time step, but only when a pre- or post-synaptic spike occurs at the synapse. This is only possible because the two variables follow linear first-order ODEs. The event-driven integration method allows to spare a lot of computations if the number of spikes is not too high in the network.","title":"Online version"},{"location":"manual/SpikeSynapse/#order-of-evaluation","text":"Three types of updates are potentially executed at every time step: Pre-synaptic events, defined by pre_spike and triggered after each pre-synaptic spike, after a delay of at least dt . Synaptic variables defined by equations . Post-synaptic events, defined by post_spike and triggered after each post-synaptic spike, without delay. These updates are conducted in that order at each time step. First, all spikes emitted in the previous step (or earlier if there are delays) are propagated to the corresponding synapses and influence variables there (especially conductance increases), then all synaptic variables are updated according to their ODE (after the neurons\\' equations are updated), then all neurons which have emitted a spike in the current step modify their synapses. A potential problem arises when a pre-synaptic and a post-synaptic spike are emitted at the same time. STDP-like plasticity rules are usually not defined when the spike time difference is 0, as the two spikes can not be correlated in that case (the pre-spike can not possibly be the cause of the post-spike). By default, both event-driven updates ( pre_spike leading to LTD, post_spike leading to LTP) will be conducted when the spikes are emitted at the same time. This can be problematic for some plastic models, for example the simple_stdp example provided in the source code. To avoid this problem, the flag unless_post can be specified in pre_spike to indicate that the corresponding variable should be updated after each pre-synaptic spike, unless the post-synaptic neuron also fired at the previous time step. Without even-driven integration, the online STDP learning rule would become: STDP_online = Synapse ( parameters = \"\"\" tau_pre = 10.0 : projection tau_post = 10.0 : projection cApre = 0.01 : projection cApost = 0.0105 : projection wmax = 0.01 : projection \"\"\" , equations = \"\"\" tau_pre * dApre/dt = - Apre tau_post * dApost/dt = - Apost \"\"\" , pre_spike = \"\"\" g_target += w Apre += cApre : unless_post w = clip(w - Apost, 0.0 , wmax) : unless_post \"\"\" , post_spike = \"\"\" Apost += cApost w = clip(w + Apre, 0.0 , wmax) \"\"\" )","title":"Order of evaluation"},{"location":"manual/SpikeSynapse/#continuous-synaptic-transmission","text":"In some cases, synaptic transmission cannot be described in an event-driven framework. Synapses using the NMDA neurotransmitter are for example often modeled as non-linear synapses. Non-linear synapses can require the post-synaptic conductance to be a sum of synapse-specific variables, as for rate-coded neurons, and not simply incremented when a pre-synaptic spike occurs. NMDA synapses can be represented by two variables \\(x(t)\\) and \\(g(t)\\) following first-order ODEs: \\[\\begin{aligned} \\begin{aligned} \\tau \\cdot \\frac{dx(t)}{dt} &= - x(t) \\\\ \\tau \\cdot \\frac{dg(t)}{dt} &= - g(t) + x(t) \\cdot (1 - g(t)) \\end{aligned} \\end{aligned}\\] When a pre-synaptic spike occurs, \\(x(t)\\) is incremented by the weight \\(w(t)\\) . However, it does not influence directly the post-synaptic neuron, as the output of a synapse is the signal \\(g(t)\\) . The post-synaptic conductance is defined at each time \\(t\\) as the sum over all synapses of the same type of their variable \\(g(t)\\) : \\[g_\\text{exc}(t) = \\sum_{i=1}^{N_\\text{exc}} g_i (t)\\] Such a synapse could be implemented the following way: NMDA = Synapse( parameters = \"\"\" tau = 10.0 : projection \"\"\", equations = \"\"\" tau * dx/dt = -x tau * dg/dt = -g + x * (1 -g) \"\"\", pre_spike = \"x += w\", psp = \"g\" ) The synapse defines a psp argument which means that the output of this synapse is non-linear and the post-synaptic conductance should be summed over this value ( g in this case). It is not possible to use the event-driven integration scheme for such non-linear synapses.","title":"Continuous synaptic transmission"},{"location":"manual/StructuralPlasticity/","text":"Structural plasticity # ANNarchy supports the dynamic addition/suppression of synapses during the simulation (i.e. after compilation). Warning Structural plasticity is not available with the CUDA backend and will likely never be... Because structural plasticity adds some complexity to the generated code, it has to be enabled before compilation by setting the structural_plasticity flag to True in the call to setup() : setup ( structural_plasticity = True ) If the flag is not set, the following methods will do nothing. There are two possibilities to dynamically create or delete synapses: Externally, using methods at the dendrite level from Python. Internally, by defining conditions for creating/pruning in the synapse description. Dendrite level # Two methods of the Dendrite class are available for creating/deleting synapses: create_synapse() prune_synapse() Creating synapses # Let's suppose that we want to add regularly new synapses between strongly active but not yet connected neurons with a low probability. One could for example define a neuron type with an additional variable averaging the firing rate over a long period of time. LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 tau_mean = 100000.0 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) tau_mean * dmean_r/dt = (r - mean_r) : init = 0.0 \"\"\" ) Two populations are created and connected using a sparse connectivity: pop1 = Population ( 1000 , LeakyIntegratorNeuron ) pop2 = Population ( 1000 , LeakyIntegratorNeuron ) proj = Projection ( pop1 , pop2 , 'exc' , Oja ) proj . connect_fixed_probability ( weights = 1.0 , probability = 0.1 ) After an initial period of simulation, one could add new synapses between strongly active pair of neurons: # For all post-synaptic neurons for post in xrange ( pop2 . size ): # For all pre-synaptic neurons for pre in xrange ( pop1 . size ): # If the neurons are not connected yet if not pre in proj [ post ] . ranks : # If they are both sufficientely active if pop1 [ pre ] . mean_r * pop2 [ post ] . mean_r > 0.7 : # Add a synapse with weight 1.0 and the default delay proj [ post ] . create_synapse ( pre , 1.0 ) create_synapse only allows to specify the value of the weight and the delay. Other syanptic variables will take the value they would have had before compile(). If another value is desired, it should be explicitely set afterwards. Removing synapses # Removing useless synapses (pruning) is also possible. Let's consider a synapse type whose \\\"age\\\" is incremented as long as both pre- and post-synaptic neurons are inactive at the same time: AgingSynapse = Synapse ( equations = \"\"\" age = if pre.r * post.r > 0.0 : 0 else : age + 1 : init = 0, int \"\"\" ) One could periodically track the too \\\"old\\\" synapses and remove them: # Threshold on the age: T = 100000 # For all post-synaptic neurons receiving synapses for post in proj . post_ranks : # For all existing synapses for pre in proj [ post ] . ranks : # If the synapse is too old if proj [ post ][ pre ] . age > T : # Remove it proj [ post ] . prune_synapse ( pre ) Warning This form of structural plasticity is rather slow because: The for loops are in Python, not C++. Implementing this structural plasticity in Cython should already help. The memory allocated for the synapses of a projection may have to be displaced at another location. This can lead to massive transfer of data, slowing the simulation down. It is of course the user's responsability to balance synapse creation/destruction, otherwise projections could become either empty or fully connected on the long-term. Synapse level # Conditions for creating or deleting synapses can also be specified in the synapse description, through the creating or pruning arguments. Thise arguments accept string descriptions of the boolean conditions at which a synapse should be created/deleted, using the same notation as other arguments. Creating synapses # The creation of a synapse must be described by a boolean expression: CreatingSynapse = Synapse ( parameters = \" ... \" , equations = \" ... \" , creating = \"pre.mean_r * post.mean_r > 0.7 : proba = 0.5, w = 1.0\" ) The condition can make use of any pre- or post-synaptic variable, but NOT synaptic variables, as they obviously do not exist yet. Global parameters (defined with the postsynaptic or projection flags) can nevertheless be used. Several flags can be passed to the expression: proba specifies the probability according to which a synapse will be created, if the condition is met. The default is 1.0 (i.e. a synapse will be created whenever the condition is fulfilled). w specifies the value for the weight which will be created (default: 0.0). d specifies the delay (default: the same as all other synapses if the delay is constant in the projection, dt otherwise). Warning Note that the new value for the delay can not exceed the maximal delay in the projection, nor be different from the others if they were all equal. Other synaptic variables will take the default value after creation. Synapse creation is not automatically enabled at the start of the simulation: the Projectiom method start_creating() must be called: proj . start_creating ( period = 100.0 ) This method accepts a period parameter specifying how often the conditions for creating synapses will be checked (in ms). By default they would be checked at each time step ( dt ), what would be too costly. Similarly, the stop_creating() method can be called to stop the creation conditions from being checked. Deleting synapses # Synaptic pruning also rely on a boolean expression: PruningSynapse = Synapse ( parameters = \" T = 100000 : int, projection \" , equations = \"\"\" age = if pre.r * post.r > 0.0 : 0 else : age + 1 : init = 0, int\"\"\" , pruning = \"age > T : proba = 0.5\" ) A synapse type can combine creating and pruning arguments. The pruning argument can rely on synaptic variables (here age ), as the synapse already exist. Only the proba flag can be passed to specify the probability at which the synapse will be deleted if the condition is met. Pruning has to be started/stopped with the start_pruning() and stop_pruning() methods. start_pruning() accepts a period argument.","title":"Structural plasticity"},{"location":"manual/StructuralPlasticity/#structural-plasticity","text":"ANNarchy supports the dynamic addition/suppression of synapses during the simulation (i.e. after compilation). Warning Structural plasticity is not available with the CUDA backend and will likely never be... Because structural plasticity adds some complexity to the generated code, it has to be enabled before compilation by setting the structural_plasticity flag to True in the call to setup() : setup ( structural_plasticity = True ) If the flag is not set, the following methods will do nothing. There are two possibilities to dynamically create or delete synapses: Externally, using methods at the dendrite level from Python. Internally, by defining conditions for creating/pruning in the synapse description.","title":"Structural plasticity"},{"location":"manual/StructuralPlasticity/#dendrite-level","text":"Two methods of the Dendrite class are available for creating/deleting synapses: create_synapse() prune_synapse()","title":"Dendrite level"},{"location":"manual/StructuralPlasticity/#creating-synapses","text":"Let's suppose that we want to add regularly new synapses between strongly active but not yet connected neurons with a low probability. One could for example define a neuron type with an additional variable averaging the firing rate over a long period of time. LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 tau_mean = 100000.0 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) tau_mean * dmean_r/dt = (r - mean_r) : init = 0.0 \"\"\" ) Two populations are created and connected using a sparse connectivity: pop1 = Population ( 1000 , LeakyIntegratorNeuron ) pop2 = Population ( 1000 , LeakyIntegratorNeuron ) proj = Projection ( pop1 , pop2 , 'exc' , Oja ) proj . connect_fixed_probability ( weights = 1.0 , probability = 0.1 ) After an initial period of simulation, one could add new synapses between strongly active pair of neurons: # For all post-synaptic neurons for post in xrange ( pop2 . size ): # For all pre-synaptic neurons for pre in xrange ( pop1 . size ): # If the neurons are not connected yet if not pre in proj [ post ] . ranks : # If they are both sufficientely active if pop1 [ pre ] . mean_r * pop2 [ post ] . mean_r > 0.7 : # Add a synapse with weight 1.0 and the default delay proj [ post ] . create_synapse ( pre , 1.0 ) create_synapse only allows to specify the value of the weight and the delay. Other syanptic variables will take the value they would have had before compile(). If another value is desired, it should be explicitely set afterwards.","title":"Creating synapses"},{"location":"manual/StructuralPlasticity/#removing-synapses","text":"Removing useless synapses (pruning) is also possible. Let's consider a synapse type whose \\\"age\\\" is incremented as long as both pre- and post-synaptic neurons are inactive at the same time: AgingSynapse = Synapse ( equations = \"\"\" age = if pre.r * post.r > 0.0 : 0 else : age + 1 : init = 0, int \"\"\" ) One could periodically track the too \\\"old\\\" synapses and remove them: # Threshold on the age: T = 100000 # For all post-synaptic neurons receiving synapses for post in proj . post_ranks : # For all existing synapses for pre in proj [ post ] . ranks : # If the synapse is too old if proj [ post ][ pre ] . age > T : # Remove it proj [ post ] . prune_synapse ( pre ) Warning This form of structural plasticity is rather slow because: The for loops are in Python, not C++. Implementing this structural plasticity in Cython should already help. The memory allocated for the synapses of a projection may have to be displaced at another location. This can lead to massive transfer of data, slowing the simulation down. It is of course the user's responsability to balance synapse creation/destruction, otherwise projections could become either empty or fully connected on the long-term.","title":"Removing synapses"},{"location":"manual/StructuralPlasticity/#synapse-level","text":"Conditions for creating or deleting synapses can also be specified in the synapse description, through the creating or pruning arguments. Thise arguments accept string descriptions of the boolean conditions at which a synapse should be created/deleted, using the same notation as other arguments.","title":"Synapse level"},{"location":"manual/StructuralPlasticity/#creating-synapses_1","text":"The creation of a synapse must be described by a boolean expression: CreatingSynapse = Synapse ( parameters = \" ... \" , equations = \" ... \" , creating = \"pre.mean_r * post.mean_r > 0.7 : proba = 0.5, w = 1.0\" ) The condition can make use of any pre- or post-synaptic variable, but NOT synaptic variables, as they obviously do not exist yet. Global parameters (defined with the postsynaptic or projection flags) can nevertheless be used. Several flags can be passed to the expression: proba specifies the probability according to which a synapse will be created, if the condition is met. The default is 1.0 (i.e. a synapse will be created whenever the condition is fulfilled). w specifies the value for the weight which will be created (default: 0.0). d specifies the delay (default: the same as all other synapses if the delay is constant in the projection, dt otherwise). Warning Note that the new value for the delay can not exceed the maximal delay in the projection, nor be different from the others if they were all equal. Other synaptic variables will take the default value after creation. Synapse creation is not automatically enabled at the start of the simulation: the Projectiom method start_creating() must be called: proj . start_creating ( period = 100.0 ) This method accepts a period parameter specifying how often the conditions for creating synapses will be checked (in ms). By default they would be checked at each time step ( dt ), what would be too costly. Similarly, the stop_creating() method can be called to stop the creation conditions from being checked.","title":"Creating synapses"},{"location":"manual/StructuralPlasticity/#deleting-synapses","text":"Synaptic pruning also rely on a boolean expression: PruningSynapse = Synapse ( parameters = \" T = 100000 : int, projection \" , equations = \"\"\" age = if pre.r * post.r > 0.0 : 0 else : age + 1 : init = 0, int\"\"\" , pruning = \"age > T : proba = 0.5\" ) A synapse type can combine creating and pruning arguments. The pruning argument can rely on synaptic variables (here age ), as the synapse already exist. Only the proba flag can be passed to specify the probability at which the synapse will be deleted if the condition is met. Pruning has to be started/stopped with the start_pruning() and stop_pruning() methods. start_pruning() accepts a period argument.","title":"Deleting synapses"},{"location":"manual/Structure/","text":"General structure # Definition of a neural network # A neural network in ANNarchy is a collection of interconnected Populations . Each population comprises a set of similar artificial Neurons , whose mean-firing rate or spiking behavior is governed by one or many ordinary differential equations (ODE). These ODEs are dependent on the activity of other neurons through Synapses . The connection pattern between two populations is called a Projection . The efficiency of the connections received by a neuron is stored in a connectivity matrix, depending on the type that was assigned to them: excitatory, inhibitory, modulatory... This typed organization of afferent connections also allows to easily apply them different learning rules (Hebbian, three-factor, Oja, BCM, STDP...). To define a neural network and simulate its behavior, you need to define the following information: The number of populations, their geometry (number of neurons, optionally the spatial structure - 1D/2D/3D). For each population, the type of neuron composing it, with all the necessary ODEs. For each projection between two populations, the connection pattern (all-to-all, one-to-one, distance-dependent...), the initial synaptic weights, and optionally the delays in synaptic transmission. For plastic synapses, the ODEs describing the evolution of synaptic weights during the simulation (learning). The interaction of the network with its environment (I/O relationships, rewarded tasks, fitting procedure...) ANNarchy provides a convenient way to define this information in a single Python script. In this manual, we will focus on an simple rate-coded network composed of two interconnected populations pop1 and pop2 , but more complex architectures are of course possible (see the examples in section Examples ). Basic structure of a script # In a script file (e.g. MyNetwork.py ), you first need to import the ANNarchy package: from ANNarchy import * All the necessary objects and class definitions are then imported. The next step is to define the neurons and synapses needed by your network. To keep things simple, we will define a simple neuron model, whose firing rate is determined by the leaky-integration of excitatory inputs: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) \"\"\" ) mp is an internal variable integrating with the time constant tau the weighted sum of excitatory inputs sum(exc) to this neuron plus its baseline activity. r is the instantaneous firing rate of the neuron, defined as the positive part of mp . More details on the difference between parameters and variables, as well as details on the mathematical parser are to be found in the sections Parser and Rate-coded neurons . The synapse type between the two populations will implement a simple Oja learning rule, which is a Hebbian learning rule with an additional regularization term: Oja = Synapse ( parameters = \"\"\" tau = 5000.0 alpha = 8.0 \"\"\" , equations = \"\"\" tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w \"\"\" ) w represents the synaptic efficiency (or weight value). Its evolution over time depends on a time constant tau , the regularization parameter alpha , the pre-synaptic firing rate pre.r and the post-synaptic firing rate post.r . See Rate-coded synapses for more details. Once these objects are defined, the populations can be created (section Populations ). We create here two populations pop1 and pop2 containing 100 neurons each and using the LeakyIntegratorNeuron neural model: pop1 = Population ( name = 'pop1' , geometry = 100 , neuron = LeakyIntegratorNeuron ) pop2 = Population ( name = 'pop2' , geometry = 100 , neuron = LeakyIntegratorNeuron ) We additionally define an excitatory projection between the neurons of pop1 and pop2 , with a target exc and a all_to_all connection pattern (section Projections ). The synaptic weights are initialized randomly between 0.0 and 1.0: proj = Projection ( pre = pop1 , post = pop2 , target = 'exc' , synapse = Oja ) proj . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) Now that the structure of the network is defined, it can be analyzed to generate optimized C++ code in the annarchy/ subfolder and create the objects: compile () The network is now ready to be simulated for the desired amount of time: simulate ( 1000.0 ) # simulate for 1 second It remains to set inputs, record variables and analyze the results, but the structure of the network is already there.","title":"General structure"},{"location":"manual/Structure/#general-structure","text":"","title":"General structure"},{"location":"manual/Structure/#definition-of-a-neural-network","text":"A neural network in ANNarchy is a collection of interconnected Populations . Each population comprises a set of similar artificial Neurons , whose mean-firing rate or spiking behavior is governed by one or many ordinary differential equations (ODE). These ODEs are dependent on the activity of other neurons through Synapses . The connection pattern between two populations is called a Projection . The efficiency of the connections received by a neuron is stored in a connectivity matrix, depending on the type that was assigned to them: excitatory, inhibitory, modulatory... This typed organization of afferent connections also allows to easily apply them different learning rules (Hebbian, three-factor, Oja, BCM, STDP...). To define a neural network and simulate its behavior, you need to define the following information: The number of populations, their geometry (number of neurons, optionally the spatial structure - 1D/2D/3D). For each population, the type of neuron composing it, with all the necessary ODEs. For each projection between two populations, the connection pattern (all-to-all, one-to-one, distance-dependent...), the initial synaptic weights, and optionally the delays in synaptic transmission. For plastic synapses, the ODEs describing the evolution of synaptic weights during the simulation (learning). The interaction of the network with its environment (I/O relationships, rewarded tasks, fitting procedure...) ANNarchy provides a convenient way to define this information in a single Python script. In this manual, we will focus on an simple rate-coded network composed of two interconnected populations pop1 and pop2 , but more complex architectures are of course possible (see the examples in section Examples ).","title":"Definition of a neural network"},{"location":"manual/Structure/#basic-structure-of-a-script","text":"In a script file (e.g. MyNetwork.py ), you first need to import the ANNarchy package: from ANNarchy import * All the necessary objects and class definitions are then imported. The next step is to define the neurons and synapses needed by your network. To keep things simple, we will define a simple neuron model, whose firing rate is determined by the leaky-integration of excitatory inputs: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) \"\"\" ) mp is an internal variable integrating with the time constant tau the weighted sum of excitatory inputs sum(exc) to this neuron plus its baseline activity. r is the instantaneous firing rate of the neuron, defined as the positive part of mp . More details on the difference between parameters and variables, as well as details on the mathematical parser are to be found in the sections Parser and Rate-coded neurons . The synapse type between the two populations will implement a simple Oja learning rule, which is a Hebbian learning rule with an additional regularization term: Oja = Synapse ( parameters = \"\"\" tau = 5000.0 alpha = 8.0 \"\"\" , equations = \"\"\" tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w \"\"\" ) w represents the synaptic efficiency (or weight value). Its evolution over time depends on a time constant tau , the regularization parameter alpha , the pre-synaptic firing rate pre.r and the post-synaptic firing rate post.r . See Rate-coded synapses for more details. Once these objects are defined, the populations can be created (section Populations ). We create here two populations pop1 and pop2 containing 100 neurons each and using the LeakyIntegratorNeuron neural model: pop1 = Population ( name = 'pop1' , geometry = 100 , neuron = LeakyIntegratorNeuron ) pop2 = Population ( name = 'pop2' , geometry = 100 , neuron = LeakyIntegratorNeuron ) We additionally define an excitatory projection between the neurons of pop1 and pop2 , with a target exc and a all_to_all connection pattern (section Projections ). The synaptic weights are initialized randomly between 0.0 and 1.0: proj = Projection ( pre = pop1 , post = pop2 , target = 'exc' , synapse = Oja ) proj . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) Now that the structure of the network is defined, it can be analyzed to generate optimized C++ code in the annarchy/ subfolder and create the objects: compile () The network is now ready to be simulated for the desired amount of time: simulate ( 1000.0 ) # simulate for 1 second It remains to set inputs, record variables and analyze the results, but the structure of the network is already there.","title":"Basic structure of a script"}]}