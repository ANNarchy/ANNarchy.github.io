{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Documentation for ANNarchy # ANNarchy (Artificial Neural Networks architect) is a neural simulator designed for distributed rate-coded or spiking neural networks. The core of the library is written in C++ and distributed using openMP or CUDA. It provides an interface in Python for the definition of the networks. It is released under the GNU GPL v2 or later . The source code of ANNarchy is available at: https://github.com/ANNarchy/ANNarchy The documentation is at: https://annarchy.github.io A forum for discussion is set at: https://groups.google.com/forum/#!forum/annarchy Bug reports should be done through the Issue Tracker of ANNarchy on Github. Citation If you use ANNarchy for your research, we would appreciate if you cite the following paper: Vitay J, Dinkelbach H\u00dc and Hamker FH (2015). ANNarchy: a code generation approach to neural simulations on parallel hardware. Frontiers in Neuroinformatics 9:19. doi:10.3389/fninf.2015.00019 @article{Vitay2015, title = {{{ANNarchy}}: A Code Generation Approach to Neural Simulations on Parallel Hardware}, author = {Vitay, Julien and Dinkelbach, Helge {\\\"U}. and Hamker, Fred H.}, year = {2015}, journal = {Frontiers in Neuroinformatics}, volume = {9}, number = {19}, doi = {10.3389/fninf.2015.00019}, url = {https://www.frontiersin.org/articles/10.3389/fninf.2015.00019}, abstract = {Many modern neural simulators focus on the simulation of networks of spiking neurons on parallel hardware. Another important framework in computational neuroscience, rate-coded neural networks, is mostly difficult or impossible to implement using these simulators. We present here the ANNarchy (Artificial Neural Networks architect) neural simulator, which allows to easily define and simulate rate-coded and spiking networks, as well as combinations of both. The interface in Python has been designed to be close to the PyNN interface, while the definition of neuron and synapse models can be specified using an equation-oriented mathematical description similar to the Brian neural simulator. This information is used to generate C++ code that will efficiently perform the simulation on the chosen parallel hardware (multi-core system or graphical processing unit). Several numerical methods are available to transform ordinary differential equations into an efficient C++code. We compare the parallel performance of the simulator to existing solutions.} }","title":"Home"},{"location":"index.html#documentation-for-annarchy","text":"ANNarchy (Artificial Neural Networks architect) is a neural simulator designed for distributed rate-coded or spiking neural networks. The core of the library is written in C++ and distributed using openMP or CUDA. It provides an interface in Python for the definition of the networks. It is released under the GNU GPL v2 or later . The source code of ANNarchy is available at: https://github.com/ANNarchy/ANNarchy The documentation is at: https://annarchy.github.io A forum for discussion is set at: https://groups.google.com/forum/#!forum/annarchy Bug reports should be done through the Issue Tracker of ANNarchy on Github. Citation If you use ANNarchy for your research, we would appreciate if you cite the following paper: Vitay J, Dinkelbach H\u00dc and Hamker FH (2015). ANNarchy: a code generation approach to neural simulations on parallel hardware. Frontiers in Neuroinformatics 9:19. doi:10.3389/fninf.2015.00019 @article{Vitay2015, title = {{{ANNarchy}}: A Code Generation Approach to Neural Simulations on Parallel Hardware}, author = {Vitay, Julien and Dinkelbach, Helge {\\\"U}. and Hamker, Fred H.}, year = {2015}, journal = {Frontiers in Neuroinformatics}, volume = {9}, number = {19}, doi = {10.3389/fninf.2015.00019}, url = {https://www.frontiersin.org/articles/10.3389/fninf.2015.00019}, abstract = {Many modern neural simulators focus on the simulation of networks of spiking neurons on parallel hardware. Another important framework in computational neuroscience, rate-coded neural networks, is mostly difficult or impossible to implement using these simulators. We present here the ANNarchy (Artificial Neural Networks architect) neural simulator, which allows to easily define and simulate rate-coded and spiking networks, as well as combinations of both. The interface in Python has been designed to be close to the PyNN interface, while the definition of neuron and synapse models can be specified using an equation-oriented mathematical description similar to the Brian neural simulator. This information is used to generate C++ code that will efficiently perform the simulation on the chosen parallel hardware (multi-core system or graphical processing unit). Several numerical methods are available to transform ordinary differential equations into an efficient C++code. We compare the parallel performance of the simulator to existing solutions.} }","title":"Documentation for ANNarchy"},{"location":"Installation.html","text":"Installation of ANNarchy # ANNarchy is designed to run on GNU/Linux and OSX. It relies mostly on a C++ compiler (g++ or clang++), Cython (C for Python extension) and Python (Numpy, Sympy) libraries. Installation on Windows is not possible. Download # The source code of ANNarchy can be downloaded on Bitbucket: git clone https://github.com/ANNarchy/ANNarchy.git Installation on GNU/Linux # Dependencies # ANNarchy depends on a number of packages which should be easily accessible on recent GNU/Linux distributions. The classical way to install these dependencies is through your package manager, or using full Python distributions such as Anaconda. Older versions of these packages may work but have not been tested. g++ >= 4.8 make >= 3.0 python >= 3.6 (with the development files, e.g. python-dev or python-devel ) cython >= 0.20 setuptools >= 40.0 numpy >= 1.13 sympy >= 1.6 scipy >= 0.19 matplotlib >= 2.0 Additionally, the following packages are optional but strongly recommended: pyqtgraph >= 0.9.8 (to visualize some of the provided examples. The OpenGL backend can also be needed). lxml >= 3.0 (to save the networks in .xml format). pandoc >= 2.0 (for reporting). tensorboardX (for the logging extension). To use the CUDA backend: the CUDA-SDK is available on the official website (we recommend to use at least a SDK version > 6.x). For further details on installation etc., please consider the corresponding Quickstart guides ( Quickstart_8.0 for the SDK 8.x). ANNarchy works with full Python distributions such as Anaconda, as well as in virtual environments and Jupyter notebooks. On a fresh install of Ubuntu 20.04, here are the packages to install before ANNarchy: sudo apt install build-essential gcc git python3-dev python3-setuptools python3-pip python3-numpy python3-scipy python3-matplotlib cython3 sudo pip install sympy tensorboardx sudo apt install python3-pyqtgraph python3-pyqt5.qtopengl python3-lxml pandoc Note that the default version of sympy for this distribution is 1.5.1, which is not supported, so it has to be installed with pip. Installation # Using pip # Stable releases of ANNarchy are available on PyPi: pip install ANNarchy or: pip install ANNarchy --user if you do not have administrator permissions. Omit --user in a virtual environment. You may also install directly the latest commit in the master (stable) or develop branches with: pip install git+https://github.com/ANNarchy/ANNarchy.git@master Using the source code # Installation of ANNarchy from source is possible using pip in the top-level directory: pip install . or in development mode: pip install -e . Using python setup.py install is deprecated, but still works. C++ compiler # By default, ANNarchy will use the GNU C++ compiler g++ , which should be in your PATH. If you want to use another compiler (clang++, icc), you can edit the configuration file located at $HOME/.config/ANNarchy/annarchy.json (created during installation) accordingly. By default, it is: { \"openmp\" : { \"compiler\" : \"g++\" , \"flags\" : \"-march=native -O2\" }, \"cuda\" : { \"compiler\" : \"nvcc\" , \"flags\" : \"\" , \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } The (path to the) compiler can be changed in the openmp section (ignore the cuda section if you do not have a GPU). You can also change the compiler flags if you know what you are doing. -O3 does not always lead to faster simulation times, but it is worth a shot. CUDA # If ANNarchy detects the CUDA SDK during installation, it will prepare the required modules. You need to make sure that the CUDA compiler nvcc is accessible in your path. The main problem with CUDA is that the binaries, headers and libraries are installed at different locations depending on the version: /usr/local/cuda , /usr/local/cuda-7.0 or /usr/local/cuda-8.0 . There is unfortunately no way for ANNarchy to guess the installation path. A first thing to help ANNarchy find the CUDA libraries is to define the LD_LIBRARY_PATH environment variable and have point at the lib64/ subfolder: export LD_LIBRARY_PATH = /usr/local/cuda-8.0/lib64/: $LD_LIBRARY_PATH This should in most cases work if you have only one CUDA installation. Otherwise, it is needed that you indicate where the CUDA libraries are, by modifying the ANNarchy configuration file located at $HOME/.config/ANNarchy/annarchy.json : { \"openmp\" : { \"compiler\" : \"g++\" , \"flags\" : \"-march=native -O2\" }, \"cuda\" : { \"compiler\" : \"nvcc\" , \"flags\" : \"\" , \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } Simply point the ['cuda']['path'] field to the right location (without lib64/ ). If the nvcc compiler binary is at a different location, the absolute path to the nvcc can be provided by ['cuda']['compiler'] field. It can happen that the detection of CUDA fails during installation, as some environment variables are not set. In this case try: env \"PATH= $PATH \" \"LIBRARY_PATH= $LIBRARY_PATH \" pip install . Installation on MacOS X # Installation on MacOS X is in principle similar to GNU/Linux: pip install ANNarchy We advise using a full Python distribution such as Miniforge , which allows to install all dependencies of ANNarchy, rather than using the default python provided by Apple. The main issue if the choice of the C++ compiler: Using Apple's LLVM/clang # If not done already, you should first install the Xcode Command Line Tools , either through Apple's website or through Homebrew (see https://mac.install.guide/commandlinetools/ for a guide) to get the LLVM clang++ compiler. The major drawback is that Apple's clang++ still does not support OpenMP for parallel computing. Any attempt to use OpenMP with ANNarchy using this compiler will crash. If you have a M1 arm64 processor, it might be beneficial to tell clang++ to use optimizations for that hardware. Open the configuration file at $HOME/.config/ANNarchy/annarchy.json and add the following compiler flag (Xcode > 13.0): { \"openmp\" : { \"compiler\" : \"clang++\" , \"flags\" : \"-mcpu=apple-m1 -O2\" }, \"cuda\" : { \"compiler\" : \"nvcc\" , \"flags\" : \"\" , \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } Using gcc # In order to benefit from OpenMP parallelization, you should install gcc , the GNU C compiler, using Homebrew : brew install gcc You will get the command-line C++ compiler with a version number , e.g.: g++-11 The g++ executable is a symlink to Apple's clang++, do not use it... You now have to tell ANNarchy which compiler to use, even if it is in your PATH. After installing ANNarchy, a config file is created in $HOME/.config/ANNarchy/annarchy.json . Open it and change the openmp entry to: { \"openmp\" : { \"compiler\" : \"g++-11\" , \"flags\" : \"-march=native -O2\" }, \"cuda\" : { \"compiler\" : \"nvcc\" , \"flags\" : \"\" , \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } Note A potential problem with Anaconda/miniforge is that the compiler will use by default the Python shared library provided by Apple, leading to the following crash when simulating: Fatal Python error: PyThreadState_Get: no current thread Abort trap: 6 The solution is to set the environment variable DYLD_FALLBACK_LIBRARY_PATH to point at the correct library libpython3.6.dylib in your .bash_profile . For a standard Anaconda installation, this should be: export DYLD_FALLBACK_LIBRARY_PATH = $HOME /anaconda/lib: $DYLD_FALLBACK_LIBRARY_PATH Note The CUDA backend is not available on OS X.","title":"Installation"},{"location":"Installation.html#installation-of-annarchy","text":"ANNarchy is designed to run on GNU/Linux and OSX. It relies mostly on a C++ compiler (g++ or clang++), Cython (C for Python extension) and Python (Numpy, Sympy) libraries. Installation on Windows is not possible.","title":"Installation of ANNarchy"},{"location":"Installation.html#download","text":"The source code of ANNarchy can be downloaded on Bitbucket: git clone https://github.com/ANNarchy/ANNarchy.git","title":"Download"},{"location":"Installation.html#installation-on-gnulinux","text":"","title":"Installation on GNU/Linux"},{"location":"Installation.html#dependencies","text":"ANNarchy depends on a number of packages which should be easily accessible on recent GNU/Linux distributions. The classical way to install these dependencies is through your package manager, or using full Python distributions such as Anaconda. Older versions of these packages may work but have not been tested. g++ >= 4.8 make >= 3.0 python >= 3.6 (with the development files, e.g. python-dev or python-devel ) cython >= 0.20 setuptools >= 40.0 numpy >= 1.13 sympy >= 1.6 scipy >= 0.19 matplotlib >= 2.0 Additionally, the following packages are optional but strongly recommended: pyqtgraph >= 0.9.8 (to visualize some of the provided examples. The OpenGL backend can also be needed). lxml >= 3.0 (to save the networks in .xml format). pandoc >= 2.0 (for reporting). tensorboardX (for the logging extension). To use the CUDA backend: the CUDA-SDK is available on the official website (we recommend to use at least a SDK version > 6.x). For further details on installation etc., please consider the corresponding Quickstart guides ( Quickstart_8.0 for the SDK 8.x). ANNarchy works with full Python distributions such as Anaconda, as well as in virtual environments and Jupyter notebooks. On a fresh install of Ubuntu 20.04, here are the packages to install before ANNarchy: sudo apt install build-essential gcc git python3-dev python3-setuptools python3-pip python3-numpy python3-scipy python3-matplotlib cython3 sudo pip install sympy tensorboardx sudo apt install python3-pyqtgraph python3-pyqt5.qtopengl python3-lxml pandoc Note that the default version of sympy for this distribution is 1.5.1, which is not supported, so it has to be installed with pip.","title":"Dependencies"},{"location":"Installation.html#installation","text":"","title":"Installation"},{"location":"Installation.html#using-pip","text":"Stable releases of ANNarchy are available on PyPi: pip install ANNarchy or: pip install ANNarchy --user if you do not have administrator permissions. Omit --user in a virtual environment. You may also install directly the latest commit in the master (stable) or develop branches with: pip install git+https://github.com/ANNarchy/ANNarchy.git@master","title":"Using pip"},{"location":"Installation.html#using-the-source-code","text":"Installation of ANNarchy from source is possible using pip in the top-level directory: pip install . or in development mode: pip install -e . Using python setup.py install is deprecated, but still works.","title":"Using the source code"},{"location":"Installation.html#c-compiler","text":"By default, ANNarchy will use the GNU C++ compiler g++ , which should be in your PATH. If you want to use another compiler (clang++, icc), you can edit the configuration file located at $HOME/.config/ANNarchy/annarchy.json (created during installation) accordingly. By default, it is: { \"openmp\" : { \"compiler\" : \"g++\" , \"flags\" : \"-march=native -O2\" }, \"cuda\" : { \"compiler\" : \"nvcc\" , \"flags\" : \"\" , \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } The (path to the) compiler can be changed in the openmp section (ignore the cuda section if you do not have a GPU). You can also change the compiler flags if you know what you are doing. -O3 does not always lead to faster simulation times, but it is worth a shot.","title":"C++ compiler"},{"location":"Installation.html#cuda","text":"If ANNarchy detects the CUDA SDK during installation, it will prepare the required modules. You need to make sure that the CUDA compiler nvcc is accessible in your path. The main problem with CUDA is that the binaries, headers and libraries are installed at different locations depending on the version: /usr/local/cuda , /usr/local/cuda-7.0 or /usr/local/cuda-8.0 . There is unfortunately no way for ANNarchy to guess the installation path. A first thing to help ANNarchy find the CUDA libraries is to define the LD_LIBRARY_PATH environment variable and have point at the lib64/ subfolder: export LD_LIBRARY_PATH = /usr/local/cuda-8.0/lib64/: $LD_LIBRARY_PATH This should in most cases work if you have only one CUDA installation. Otherwise, it is needed that you indicate where the CUDA libraries are, by modifying the ANNarchy configuration file located at $HOME/.config/ANNarchy/annarchy.json : { \"openmp\" : { \"compiler\" : \"g++\" , \"flags\" : \"-march=native -O2\" }, \"cuda\" : { \"compiler\" : \"nvcc\" , \"flags\" : \"\" , \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } Simply point the ['cuda']['path'] field to the right location (without lib64/ ). If the nvcc compiler binary is at a different location, the absolute path to the nvcc can be provided by ['cuda']['compiler'] field. It can happen that the detection of CUDA fails during installation, as some environment variables are not set. In this case try: env \"PATH= $PATH \" \"LIBRARY_PATH= $LIBRARY_PATH \" pip install .","title":"CUDA"},{"location":"Installation.html#installation-on-macos-x","text":"Installation on MacOS X is in principle similar to GNU/Linux: pip install ANNarchy We advise using a full Python distribution such as Miniforge , which allows to install all dependencies of ANNarchy, rather than using the default python provided by Apple. The main issue if the choice of the C++ compiler:","title":"Installation on MacOS X"},{"location":"Installation.html#using-apples-llvmclang","text":"If not done already, you should first install the Xcode Command Line Tools , either through Apple's website or through Homebrew (see https://mac.install.guide/commandlinetools/ for a guide) to get the LLVM clang++ compiler. The major drawback is that Apple's clang++ still does not support OpenMP for parallel computing. Any attempt to use OpenMP with ANNarchy using this compiler will crash. If you have a M1 arm64 processor, it might be beneficial to tell clang++ to use optimizations for that hardware. Open the configuration file at $HOME/.config/ANNarchy/annarchy.json and add the following compiler flag (Xcode > 13.0): { \"openmp\" : { \"compiler\" : \"clang++\" , \"flags\" : \"-mcpu=apple-m1 -O2\" }, \"cuda\" : { \"compiler\" : \"nvcc\" , \"flags\" : \"\" , \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } }","title":"Using Apple's LLVM/clang"},{"location":"Installation.html#using-gcc","text":"In order to benefit from OpenMP parallelization, you should install gcc , the GNU C compiler, using Homebrew : brew install gcc You will get the command-line C++ compiler with a version number , e.g.: g++-11 The g++ executable is a symlink to Apple's clang++, do not use it... You now have to tell ANNarchy which compiler to use, even if it is in your PATH. After installing ANNarchy, a config file is created in $HOME/.config/ANNarchy/annarchy.json . Open it and change the openmp entry to: { \"openmp\" : { \"compiler\" : \"g++-11\" , \"flags\" : \"-march=native -O2\" }, \"cuda\" : { \"compiler\" : \"nvcc\" , \"flags\" : \"\" , \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } Note A potential problem with Anaconda/miniforge is that the compiler will use by default the Python shared library provided by Apple, leading to the following crash when simulating: Fatal Python error: PyThreadState_Get: no current thread Abort trap: 6 The solution is to set the environment variable DYLD_FALLBACK_LIBRARY_PATH to point at the correct library libpython3.6.dylib in your .bash_profile . For a standard Anaconda installation, this should be: export DYLD_FALLBACK_LIBRARY_PATH = $HOME /anaconda/lib: $DYLD_FALLBACK_LIBRARY_PATH Note The CUDA backend is not available on OS X.","title":"Using gcc"},{"location":"License.html","text":"License # GNU GENERAL PUBLIC LICENSE # Version 2, June 1991 Copyright (C) 1989, 1991 Free Software Foundation, Inc. 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble # The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Lesser General Public License instead.) You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things. To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it. For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software. Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations. Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION # 0. This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The \"Program\", below, refers to any such program or work, and a \"work based on the Program\" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term \"modification\".) Each licensee is addressed as \"you\". Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does. 1. You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program. You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee. 2. You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions: a) You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change. b) You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License. c) If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.) These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it. Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program. In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License. 3. You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following: a) Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, b) Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, c) Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.) The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable. If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code. 4. You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. 5. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it. 6. Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License. 7. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program. If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances. It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice. This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License. 8. If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License. 9. The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation. 10. If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally. NO WARRANTY 11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. 12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.","title":"License"},{"location":"License.html#license","text":"","title":"License"},{"location":"License.html#gnu-general-public-license","text":"Version 2, June 1991 Copyright (C) 1989, 1991 Free Software Foundation, Inc. 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.","title":"GNU GENERAL PUBLIC LICENSE"},{"location":"License.html#preamble","text":"The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Lesser General Public License instead.) You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things. To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it. For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software. Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations. Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all. The precise terms and conditions for copying, distribution and modification follow.","title":"Preamble"},{"location":"License.html#terms-and-conditions-for-copying-distribution-and-modification","text":"0. This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The \"Program\", below, refers to any such program or work, and a \"work based on the Program\" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term \"modification\".) Each licensee is addressed as \"you\". Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does. 1. You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program. You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee. 2. You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions: a) You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change. b) You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License. c) If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.) These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it. Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program. In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License. 3. You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following: a) Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, b) Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or, c) Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.) The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable. If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code. 4. You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance. 5. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it. 6. Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License. 7. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program. If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances. It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice. This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License. 8. If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License. 9. The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation. 10. If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally. NO WARRANTY 11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. 12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.","title":"TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION"},{"location":"API/ANNarchy.html","text":"Top-level methods # These methods are directly available in the main namespace when importing ANNarhcy: from ANNarchy import * Note that numpy is automatically imported as: import numpy as np Configuration and compilation # Contrary to other simulators, ANNarchy is entirely based on code generation. It provides a set of first level functions to ensure the network is correctly created. It is important to call these functions in the right order. setup ( ** keyValueArgs ) # The setup function is used to configure ANNarchy simulation environment. It takes various optional arguments: dt: simulation step size (default: 1.0 ms). paradigm: parallel framework for code generation. Accepted values: \"openmp\" or \"cuda\" (default: \"openmp\"). method: default method to numerize ODEs. Default is the explicit forward Euler method ('explicit'). sparse_matrix_format: the default matrix format for projections in ANNarchy (by default: List-In-List for CPUs and Compressed Sparse Row) precision: default floating precision for variables in ANNarchy. Accepted values: \"float\" or \"double\" (default: \"double\") num_threads: number of treads used by openMP (overrides the environment variable OMP_NUM_THREADS when set, default = None). visible_cores: allows a fine-grained control which cores are useable for the created threads (default = [] for no limitation). It can be used to limit created openMP threads to a physical socket. structural_plasticity: allows synapses to be dynamically added/removed during the simulation (default: False). seed: the seed (integer) to be used in the random number generators (default = -1 is equivalent to time(NULL)). The following parameters are mainly for debugging and profiling, and should be ignored by most users: verbose: shows details about compilation process on console (by default False). Additional some information of the network construction will be shown. suppress_warnings: if True, warnings (e. g. from the mathematical parser) are suppressed. show_time: if True, initialization times are shown. Attention: verbose should be set to True additionally. disable_shared_library_time_offset: by default False. If set to True, the shared library generated by ANNarchy will not be extended by time offset. Note: This function should be used before any other functions of ANNarchy (including importing a network definition), right after from ANNarchy import * : from ANNarchy import * setup ( dt = 1.0 , method = 'midpoint' , num_threads = 2 ) compile ( directory = 'annarchy' , clean = False , populations = None , projections = None , compiler = 'default' , compiler_flags = 'default' , add_sources = '' , extra_libs = '' , cuda_config = { 'device' : 0 }, annarchy_json = '' , silent = False , debug_build = False , profile_enabled = False , net_id = 0 ) # This method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation. The compiler , compiler_flags and part of cuda_config take their default value from the configuration file ~/.config/ANNarchy/annarchy.json . The following arguments are for internal development use only: debug_build : creates a debug version of ANNarchy, which logs the creation of objects and some other data (default: False). profile_enabled : creates a profilable version of ANNarchy, which logs several computation timings (default: False). Parameters: Name Type Description Default directory name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\". 'annarchy' clean boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False). False populations list of populations which should be compiled. If set to None, all available populations will be used. None projections list of projection which should be compiled. If set to None, all available projections will be used. None compiler C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++]. 'default' compiler_flags platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended. 'default' cuda_config dictionary defining the CUDA configuration for each population and projection. {'device': 0} annarchy_json compiler flags etc can be stored in a .json file normally placed in the home directory (see comment below). With this flag one can directly assign a file location. '' silent defines if status message like \"Compiling... OK\" should be printed. False clear () # Clears all variables (erasing already defined populations, projections, monitors and constants), as if you had just imported ANNarchy. Useful when re-running Jupyter/IPython notebooks multiple times: from ANNarchy import * clear () Simulation # Different methods are available to run the simulation: simulate ( duration , measure_time = False , progress_bar = False , callbacks = True , net_id = 0 ) # Simulates the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step dt declared in setup() (default: 1ms): simulate ( 1000.0 ) Parameters: Name Type Description Default duration the duration in milliseconds. required measure_time defines whether the simulation time should be printed. Default: False. False progress_bar defines whether a progress bar should be printed. Default: False False callbacks defines if the callback method (decorator every should be called). Default: True. True Source code in ANNarchy/core/Simulate.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def simulate ( duration , measure_time = False , progress_bar = False , callbacks = True , net_id = 0 ): \"\"\" Simulates the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step ``dt`` declared in ``setup()`` (default: 1ms): ```python simulate(1000.0) ``` :param duration: the duration in milliseconds. :param measure_time: defines whether the simulation time should be printed. Default: False. :param progress_bar: defines whether a progress bar should be printed. Default: False :param callbacks: defines if the callback method (decorator ``every`` should be called). Default: True. \"\"\" if Global . _profiler : t0 = time . time () if not _network [ net_id ][ 'instance' ]: _error ( 'simulate(): the network is not compiled yet.' ) # Compute the number of steps nb_steps = ceil ( float ( duration ) / dt ()) if measure_time : tstart = time . time () if callbacks and _callbacks_enabled [ net_id ] and len ( _callbacks [ net_id ]) > 0 : _simulate_with_callbacks ( duration , net_id ) else : _network [ net_id ][ 'instance' ] . pyx_run ( nb_steps , progress_bar ) if measure_time : if net_id > 0 : _print ( 'Simulating' , duration / 1000.0 , 'seconds of the network' , net_id , 'took' , time . time () - tstart , 'seconds.' ) else : _print ( 'Simulating' , duration / 1000.0 , 'seconds of the network took' , time . time () - tstart , 'seconds.' ) # Store the Python and C++ timings. Please note, that the C++ core # measures in ms and Python measures in s if Global . _profiler : t1 = time . time () Global . _profiler . add_entry ( t0 , t1 , \"simulate\" , \"simulate\" ) # network single step overall_avg , _ = Global . _profiler . _cpp_profiler . get_timing ( \"network\" , \"step\" ) Global . _profiler . add_entry ( overall_avg / 1000.0 , 100.0 , \"overall\" , \"cpp core\" ) # single operations for populations for pop in _network [ net_id ][ 'populations' ]: for func in [ \"step\" , \"rng\" , \"delay\" , \"spike\" ]: avg_time , std_time = Global . _profiler . _cpp_profiler . get_timing ( pop . name , func ) Global . _profiler . add_entry ( avg_time / 1000.0 , ( avg_time / overall_avg ) * 100.0 , pop . name + \"_\" + func , \"cpp core\" ) # single operations for projections for proj in _network [ net_id ][ 'projections' ]: for func in [ \"psp\" , \"step\" , \"post_event\" ]: avg_time , std_time = Global . _profiler . _cpp_profiler . get_timing ( proj . name , func ) Global . _profiler . add_entry ( avg_time / 1000.0 , ( avg_time / overall_avg ) * 100.0 , proj . name + \"_\" + func , \"cpp core\" ) monitor_avg , _ = Global . _profiler . _cpp_profiler . get_timing ( \"network\" , \"record\" ) Global . _profiler . add_entry ( monitor_avg / 1000.0 , ( monitor_avg / overall_avg ) * 100.0 , \"record\" , \"cpp core\" ) simulate_until ( max_duration , population , operator = 'and' , measure_time = False , net_id = 0 ) # Runs the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: pop1 = Population ( ... , stop_condition = \"r > 1.0 : any\" ) compile () simulate_until ( max_duration = 1000.0 . population = pop1 ) Parameters: Name Type Description Default max_duration the maximum duration of the simulation in milliseconds. required population the (list of) population whose stop_condition should be checked to stop the simulation. required operator operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). 'and' measure_time defines whether the simulation time should be printed (default=False). False Returns: Type Description the actual duration of the simulation in milliseconds. Source code in ANNarchy/core/Simulate.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def simulate_until ( max_duration , population , operator = 'and' , measure_time = False , net_id = 0 ): \"\"\" Runs the network for the maximal duration in milliseconds. If the ``stop_condition`` defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: ```python pop1 = Population( ..., stop_condition = \"r > 1.0 : any\") compile() simulate_until(max_duration=1000.0. population=pop1) ``` :param max_duration: the maximum duration of the simulation in milliseconds. :param population: the (list of) population whose ``stop_condition`` should be checked to stop the simulation. :param operator: operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). :param measure_time: defines whether the simulation time should be printed (default=False). :return: the actual duration of the simulation in milliseconds. \"\"\" if not _network [ net_id ][ 'instance' ]: _error ( 'simulate_until(): the network is not compiled yet.' ) nb_steps = ceil ( float ( max_duration ) / dt ()) if not isinstance ( population , list ): population = [ population ] if measure_time : tstart = time . time () nb = _network [ net_id ][ 'instance' ] . pyx_run_until ( nb_steps , [ pop . id for pop in population ], True if operator == 'and' else False ) sim_time = float ( nb ) / dt () if measure_time : _print ( 'Simulating' , nb / dt () / 1000.0 , 'seconds of the network took' , time . time () - tstart , 'seconds.' ) return sim_time step ( net_id = 0 ) # Performs a single simulation step (duration = dt ). Source code in ANNarchy/core/Simulate.py 140 141 142 143 144 145 146 147 148 def step ( net_id = 0 ): \"\"\" Performs a single simulation step (duration = ``dt``). \"\"\" if not _network [ net_id ][ 'instance' ]: _error ( 'simulate_until(): the network is not compiled yet.' ) _network [ net_id ][ 'instance' ] . pyx_step () every # Bases: object Decorator to declare a callback method that will be called periodically during the simulation. Example of setting increasing inputs to a population every 100 ms, with an offset of 90 ms (or -10 ms relative to the period): @every ( period = 100. , offset =- 10. ) def step_input ( n ): pop . I = float ( n ) / 100. simulate ( 10000. ) step_input() will be called at times 90, 190, ..., 9990 ms during the call to simulate() . The method must accept only n as parameter (an integer being 0 the first time the method is called, and incremented afterwards) and can not return anything. The times at which the method is called are relative to the time when simulate() is called (if t is already 150 before calling simulate() , the first call will then be made at t=240 with the previous example). If multiple callbacks are defined, they will be called in the order of their declaration if they occur at the same time. Source code in ANNarchy/core/Simulate.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 class every ( object ): \"\"\" Decorator to declare a callback method that will be called periodically during the simulation. Example of setting increasing inputs to a population every 100 ms, with an offset of 90 ms (or -10 ms relative to the period): ```python @every(period=100., offset=-10.) def step_input(n): pop.I = float(n) / 100. simulate(10000.) ``` ``step_input()`` will be called at times 90, 190, ..., 9990 ms during the call to ``simulate()``. The method must accept only ``n`` as parameter (an integer being 0 the first time the method is called, and incremented afterwards) and can not return anything. The times at which the method is called are relative to the time when ``simulate()`` is called (if ``t`` is already 150 before calling ``simulate()``, the first call will then be made at ``t=240`` with the previous example). If multiple callbacks are defined, they will be called in the order of their declaration if they occur at the same time. \"\"\" def __init__ ( self , period , offset = 0. , wait = 0.0 , net_id = 0 ): \"\"\" :param period: interval in ms between two calls to the function. If less than ``dt``, will be called every step. :param offset: by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period. :param wait: allows to wait for a certain amount of time (in ms) before starting to call the method. ``wait`` can be combined with ``offset``, so if ``period=100.``, ``offset=50.`` and ``wait=500.``, the first call will be made 550 ms after the call to ``simulate()`` \"\"\" self . period = max ( float ( period ), dt ()) self . offset = min ( float ( offset ), self . period ) self . wait = max ( float ( wait ), 0.0 ) _callbacks [ net_id ] . append ( self ) def __call__ ( self , f ): # If there are decorator arguments, __call__() is only called # once, as part of the decoration process! You can only give # it a single argument, which is the function object. self . func = f return f __init__ ( period , offset = 0.0 , wait = 0.0 , net_id = 0 ) # Parameters: Name Type Description Default period interval in ms between two calls to the function. If less than dt , will be called every step. required offset by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period. 0.0 wait allows to wait for a certain amount of time (in ms) before starting to call the method. wait can be combined with offset , so if period=100. , offset=50. and wait=500. , the first call will be made 550 ms after the call to simulate() 0.0 Source code in ANNarchy/core/Simulate.py 206 207 208 209 210 211 212 213 214 215 216 217 218 def __init__ ( self , period , offset = 0. , wait = 0.0 , net_id = 0 ): \"\"\" :param period: interval in ms between two calls to the function. If less than ``dt``, will be called every step. :param offset: by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period. :param wait: allows to wait for a certain amount of time (in ms) before starting to call the method. ``wait`` can be combined with ``offset``, so if ``period=100.``, ``offset=50.`` and ``wait=500.``, the first call will be made 550 ms after the call to ``simulate()`` \"\"\" self . period = max ( float ( period ), dt ()) self . offset = min ( float ( offset ), self . period ) self . wait = max ( float ( wait ), 0.0 ) _callbacks [ net_id ] . append ( self ) enable_callbacks ( net_id = 0 ) # Enables all declared callbacks for the network. Source code in ANNarchy/core/Simulate.py 167 168 169 170 171 def enable_callbacks ( net_id = 0 ): \"\"\" Enables all declared callbacks for the network. \"\"\" _callbacks_enabled [ net_id ] = True disable_callbacks ( net_id = 0 ) # Disables all callbacks for the network. Source code in ANNarchy/core/Simulate.py 161 162 163 164 165 def disable_callbacks ( net_id = 0 ): \"\"\" Disables all callbacks for the network. \"\"\" _callbacks_enabled [ net_id ] = False clear_all_callbacks ( net_id = 0 ) # Clears the list of declared callbacks for the network. Cannot be undone! Source code in ANNarchy/core/Simulate.py 173 174 175 176 177 178 179 def clear_all_callbacks ( net_id = 0 ): \"\"\" Clears the list of declared callbacks for the network. Cannot be undone! \"\"\" _callbacks [ net_id ] . clear () Reset the network # If you want to run multiple experiments with the same network, or if your experiment setup requires a pre learning phase, you can reset selectively neural or synaptic variables to their initial values. reset ( populations = True , projections = False , synapses = False , monitors = True , net_id = 0 ) # Reinitialises the network to its state before the call to compile. The network time will be set to 0ms. All monitors are emptied. Parameters: Name Type Description Default populations if True (default), the neural parameters and variables will be reset to their initial value. True projections if True, the synaptic parameters and variables (except the connections) will be reset (default=False). False synapses if True, the synaptic weights will be erased and recreated (default=False). False monitors if True, the monitors will be emptied and reset (default=True). True Access to populations # get_population ( name , net_id = 0 ) # Returns the population with the given name . Parameters: Name Type Description Default name name of the population. required Returns: Type Description The requested Population object if existing, None otherwise. get_projection ( name , net_id = 0 ) # Returns the projection with the given name . Parameters: Name Type Description Default name name of the projection. required Returns: Type Description The requested Projection object if existing, None otherwise. Functions # add_function ( function ) # Defines a global function which can be used by all neurons and synapses. The function must have only one return value and use only the passed arguments. Examples of valid functions: logistic ( x ) = 1 / ( 1 + exp ( - x )) piecewise ( x , a , b ) = if x < a : a else : if x > b : b else : x Please refer to the manual to know the allowed mathematical functions. functions ( name , net_id = 0 ) # Allows to access a global function defined with add_function and use it from Python using arrays after compilation . The name of the function is not added to the global namespace to avoid overloading. add_function ( \"logistic(x) = 1. / (1. + exp(-x))\" ) compile () result = functions ( 'logistic' )([ 0. , 1. , 2. , 3. , 4. ]) Only lists or 1D Numpy arrays can be passed as arguments, not single values nor multidimensional arrays. When passing several arguments, make sure they have the same size. Constants # Constant # Bases: float Constant parameter that can be used by all neurons and synapses. The class Constant derives from float , so any legal operation on floats (addition, multiplication) can be used. If a Neuron/Synapse defines a parameter with the same name, the constant parameters will not be visible. Example: tau = Constant ( 'tau' , 20 ) factor = Constant ( 'factor' , 0.1 ) real_tau = Constant ( 'real_tau' , tau * factor ) neuron = Neuron ( equations = ''' real_tau*dr/dt + r =1.0 ''' ) The value of the constant can be changed anytime with the set() method. Assignments will have no effect (e.g. tau = 10.0 only creates a new float). The value of constants defined as combination of other constants ( real_tau ) is not updated if the value of these constants changes (changing tau with tau.set(10.0) will not modify the value of real_tau ). __doc__ = \" \\n Constant parameter that can be used by all neurons and synapses. \\n\\n The class ``Constant`` derives from ``float``, so any legal operation on floats (addition, multiplication) can be used. \\n\\n If a Neuron/Synapse defines a parameter with the same name, the constant parameters will not be visible. \\n\\n Example: \\n\\n ```python \\n\\n tau = Constant('tau', 20) \\n factor = Constant('factor', 0.1) \\n real_tau = Constant('real_tau', tau*factor) \\n\\n neuron = Neuron( \\n equations=''' \\n real_tau*dr/dt + r =1.0 \\n ''' \\n ) \\n ``` \\n\\n The value of the constant can be changed anytime with the ``set()`` method. Assignments will have no effect (e.g. ``tau = 10.0`` only creates a new float). \\n\\n The value of constants defined as combination of other constants (``real_tau``) is not updated if the value of these constants changes (changing ``tau`` with ``tau.set(10.0)`` will not modify the value of ``real_tau``). \\n\\n \" # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.core.Global' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'Constant' objects > # list of weak references to the object (if defined) imag = < attribute 'imag' of 'float' objects > # the imaginary part of a complex number real = < attribute 'real' of 'float' objects > # the real part of a complex number __abs__ () method descriptor # abs(self) __add__ ( value ) method descriptor # Return self+value. __bool__ () method descriptor # True if self else False __ceil__ () method descriptor # Return the ceiling as an Integral. __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __divmod__ ( value ) method descriptor # Return divmod(self, value). __eq__ ( value ) method descriptor # Return self==value. __float__ () method descriptor # float(self) __floor__ () method descriptor # Return the floor as an Integral. __floordiv__ ( value ) method descriptor # Return self//value. __format__ ( format_spec ) method descriptor # Formats the float according to format_spec. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __getformat__ ( typestr ) builtin # You probably don't want to use this function. typestr Must be 'double' or 'float'. It exists mainly to be used in Python's test suite. This function returns whichever of 'unknown', 'IEEE, big-endian' or 'IEEE, little-endian' best describes the format of floating point numbers used by the C type named by typestr. __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init__ ( name , value , net_id = 0 ) # Parameters: Name Type Description Default name name of the constant (unique), which can be used in equations. required value the value of the constant, which must be a float, or a combination of Constants. required __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __int__ () method descriptor # int(self) __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __mod__ ( value ) method descriptor # Return self%value. __mul__ ( value ) method descriptor # Return self*value. __ne__ ( value ) method descriptor # Return self!=value. __neg__ () method descriptor # -self __pos__ () method descriptor # +self __pow__ ( value , mod = None ) method descriptor # Return pow(self, value, mod). __radd__ ( value ) method descriptor # Return value+self. __rdivmod__ ( value ) method descriptor # Return divmod(value, self). __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __rfloordiv__ ( value ) method descriptor # Return value//self. __rmod__ ( value ) method descriptor # Return value%self. __rmul__ ( value ) method descriptor # Return value*self. __round__ ( ndigits = None ) method descriptor # Return the Integral closest to x, rounding half toward even. When an argument is passed, work like built-in round(x, ndigits). __rpow__ ( value , mod = None ) method descriptor # Return pow(value, self, mod). __rsub__ ( value ) method descriptor # Return value-self. __rtruediv__ ( value ) method descriptor # Return value/self. __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __setformat__ ( typestr , fmt ) builtin # You probably don't want to use this function. typestr Must be 'double' or 'float'. fmt Must be one of 'unknown', 'IEEE, big-endian' or 'IEEE, little-endian', and in addition can only be one of the latter two if it appears to match the underlying C reality. It exists mainly to be used in Python's test suite. Override the automatic determination of C-level floating point type. This affects how floats are converted to and from binary strings. __sizeof__ () method descriptor # Size of object in memory, in bytes. __sub__ ( value ) method descriptor # Return self-value. __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). __truediv__ ( value ) method descriptor # Return self/value. __trunc__ () method descriptor # Return the Integral closest to x between 0 and x. as_integer_ratio () method descriptor # Return integer ratio. Return a pair of integers, whose ratio is exactly equal to the original float and with a positive denominator. Raise OverflowError on infinities and a ValueError on NaNs. (10.0).as_integer_ratio() (10, 1) (0.0).as_integer_ratio() (0, 1) (-.25).as_integer_ratio() (-1, 4) conjugate () method descriptor # Return self, the complex conjugate of any float. fromhex ( string ) builtin # Create a floating-point number from a hexadecimal string. float.fromhex('0x1.ffffp10') 2047.984375 float.fromhex('-0x1p-1074') -5e-324 hex () method descriptor # Return a hexadecimal representation of a floating-point number. (-0.1).hex() '-0x1.999999999999ap-4' 3.14159.hex() '0x1.921f9f01b866ep+1' is_integer () method descriptor # Return True if the float is an integer. set ( value ) # Changes the value of the constant. Learning # enable_learning ( projections = None , period = None , offset = None , net_id = 0 ) # Enables learning for all projections. Optionally period and offset can be changed for all projections. Parameters: Name Type Description Default projections the projections whose learning should be enabled. By default, all the existing projections are enabled. None period determines how often the synaptic variables will be updated. None offset determines the offset at which the synaptic variables will be updated relative to the current time. None disable_learning ( projections = None , net_id = 0 ) # Disables learning for all projections. Parameters: Name Type Description Default projections the projections whose learning should be disabled. By default, all the existing projections are disabled. None Access to simulation times # get_time ( net_id = 0 ) # Returns the current time in ms. set_time ( t , net_id = 0 ) # Sets the current time in ms. Warning: can be dangerous for some spiking models. get_current_step ( net_id = 0 ) # Returns the current simulation step. set_current_step ( t , net_id = 0 ) # Sets the current simulation step (integer). Warning: can be dangerous for some spiking models. dt () # Returns the simulation step size dt used in the simulation.","title":"Top-level methods"},{"location":"API/ANNarchy.html#top-level-methods","text":"These methods are directly available in the main namespace when importing ANNarhcy: from ANNarchy import * Note that numpy is automatically imported as: import numpy as np","title":"Top-level methods"},{"location":"API/ANNarchy.html#configuration-and-compilation","text":"Contrary to other simulators, ANNarchy is entirely based on code generation. It provides a set of first level functions to ensure the network is correctly created. It is important to call these functions in the right order.","title":"Configuration and compilation"},{"location":"API/ANNarchy.html#ANNarchy.setup","text":"The setup function is used to configure ANNarchy simulation environment. It takes various optional arguments: dt: simulation step size (default: 1.0 ms). paradigm: parallel framework for code generation. Accepted values: \"openmp\" or \"cuda\" (default: \"openmp\"). method: default method to numerize ODEs. Default is the explicit forward Euler method ('explicit'). sparse_matrix_format: the default matrix format for projections in ANNarchy (by default: List-In-List for CPUs and Compressed Sparse Row) precision: default floating precision for variables in ANNarchy. Accepted values: \"float\" or \"double\" (default: \"double\") num_threads: number of treads used by openMP (overrides the environment variable OMP_NUM_THREADS when set, default = None). visible_cores: allows a fine-grained control which cores are useable for the created threads (default = [] for no limitation). It can be used to limit created openMP threads to a physical socket. structural_plasticity: allows synapses to be dynamically added/removed during the simulation (default: False). seed: the seed (integer) to be used in the random number generators (default = -1 is equivalent to time(NULL)). The following parameters are mainly for debugging and profiling, and should be ignored by most users: verbose: shows details about compilation process on console (by default False). Additional some information of the network construction will be shown. suppress_warnings: if True, warnings (e. g. from the mathematical parser) are suppressed. show_time: if True, initialization times are shown. Attention: verbose should be set to True additionally. disable_shared_library_time_offset: by default False. If set to True, the shared library generated by ANNarchy will not be extended by time offset. Note: This function should be used before any other functions of ANNarchy (including importing a network definition), right after from ANNarchy import * : from ANNarchy import * setup ( dt = 1.0 , method = 'midpoint' , num_threads = 2 )","title":"setup()"},{"location":"API/ANNarchy.html#ANNarchy.compile","text":"This method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation. The compiler , compiler_flags and part of cuda_config take their default value from the configuration file ~/.config/ANNarchy/annarchy.json . The following arguments are for internal development use only: debug_build : creates a debug version of ANNarchy, which logs the creation of objects and some other data (default: False). profile_enabled : creates a profilable version of ANNarchy, which logs several computation timings (default: False). Parameters: Name Type Description Default directory name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\". 'annarchy' clean boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False). False populations list of populations which should be compiled. If set to None, all available populations will be used. None projections list of projection which should be compiled. If set to None, all available projections will be used. None compiler C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++]. 'default' compiler_flags platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended. 'default' cuda_config dictionary defining the CUDA configuration for each population and projection. {'device': 0} annarchy_json compiler flags etc can be stored in a .json file normally placed in the home directory (see comment below). With this flag one can directly assign a file location. '' silent defines if status message like \"Compiling... OK\" should be printed. False","title":"compile()"},{"location":"API/ANNarchy.html#ANNarchy.clear","text":"Clears all variables (erasing already defined populations, projections, monitors and constants), as if you had just imported ANNarchy. Useful when re-running Jupyter/IPython notebooks multiple times: from ANNarchy import * clear ()","title":"clear()"},{"location":"API/ANNarchy.html#simulation","text":"Different methods are available to run the simulation:","title":"Simulation"},{"location":"API/ANNarchy.html#ANNarchy.simulate","text":"Simulates the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step dt declared in setup() (default: 1ms): simulate ( 1000.0 ) Parameters: Name Type Description Default duration the duration in milliseconds. required measure_time defines whether the simulation time should be printed. Default: False. False progress_bar defines whether a progress bar should be printed. Default: False False callbacks defines if the callback method (decorator every should be called). Default: True. True Source code in ANNarchy/core/Simulate.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def simulate ( duration , measure_time = False , progress_bar = False , callbacks = True , net_id = 0 ): \"\"\" Simulates the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step ``dt`` declared in ``setup()`` (default: 1ms): ```python simulate(1000.0) ``` :param duration: the duration in milliseconds. :param measure_time: defines whether the simulation time should be printed. Default: False. :param progress_bar: defines whether a progress bar should be printed. Default: False :param callbacks: defines if the callback method (decorator ``every`` should be called). Default: True. \"\"\" if Global . _profiler : t0 = time . time () if not _network [ net_id ][ 'instance' ]: _error ( 'simulate(): the network is not compiled yet.' ) # Compute the number of steps nb_steps = ceil ( float ( duration ) / dt ()) if measure_time : tstart = time . time () if callbacks and _callbacks_enabled [ net_id ] and len ( _callbacks [ net_id ]) > 0 : _simulate_with_callbacks ( duration , net_id ) else : _network [ net_id ][ 'instance' ] . pyx_run ( nb_steps , progress_bar ) if measure_time : if net_id > 0 : _print ( 'Simulating' , duration / 1000.0 , 'seconds of the network' , net_id , 'took' , time . time () - tstart , 'seconds.' ) else : _print ( 'Simulating' , duration / 1000.0 , 'seconds of the network took' , time . time () - tstart , 'seconds.' ) # Store the Python and C++ timings. Please note, that the C++ core # measures in ms and Python measures in s if Global . _profiler : t1 = time . time () Global . _profiler . add_entry ( t0 , t1 , \"simulate\" , \"simulate\" ) # network single step overall_avg , _ = Global . _profiler . _cpp_profiler . get_timing ( \"network\" , \"step\" ) Global . _profiler . add_entry ( overall_avg / 1000.0 , 100.0 , \"overall\" , \"cpp core\" ) # single operations for populations for pop in _network [ net_id ][ 'populations' ]: for func in [ \"step\" , \"rng\" , \"delay\" , \"spike\" ]: avg_time , std_time = Global . _profiler . _cpp_profiler . get_timing ( pop . name , func ) Global . _profiler . add_entry ( avg_time / 1000.0 , ( avg_time / overall_avg ) * 100.0 , pop . name + \"_\" + func , \"cpp core\" ) # single operations for projections for proj in _network [ net_id ][ 'projections' ]: for func in [ \"psp\" , \"step\" , \"post_event\" ]: avg_time , std_time = Global . _profiler . _cpp_profiler . get_timing ( proj . name , func ) Global . _profiler . add_entry ( avg_time / 1000.0 , ( avg_time / overall_avg ) * 100.0 , proj . name + \"_\" + func , \"cpp core\" ) monitor_avg , _ = Global . _profiler . _cpp_profiler . get_timing ( \"network\" , \"record\" ) Global . _profiler . add_entry ( monitor_avg / 1000.0 , ( monitor_avg / overall_avg ) * 100.0 , \"record\" , \"cpp core\" )","title":"simulate()"},{"location":"API/ANNarchy.html#ANNarchy.simulate_until","text":"Runs the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: pop1 = Population ( ... , stop_condition = \"r > 1.0 : any\" ) compile () simulate_until ( max_duration = 1000.0 . population = pop1 ) Parameters: Name Type Description Default max_duration the maximum duration of the simulation in milliseconds. required population the (list of) population whose stop_condition should be checked to stop the simulation. required operator operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). 'and' measure_time defines whether the simulation time should be printed (default=False). False Returns: Type Description the actual duration of the simulation in milliseconds. Source code in ANNarchy/core/Simulate.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def simulate_until ( max_duration , population , operator = 'and' , measure_time = False , net_id = 0 ): \"\"\" Runs the network for the maximal duration in milliseconds. If the ``stop_condition`` defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: ```python pop1 = Population( ..., stop_condition = \"r > 1.0 : any\") compile() simulate_until(max_duration=1000.0. population=pop1) ``` :param max_duration: the maximum duration of the simulation in milliseconds. :param population: the (list of) population whose ``stop_condition`` should be checked to stop the simulation. :param operator: operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). :param measure_time: defines whether the simulation time should be printed (default=False). :return: the actual duration of the simulation in milliseconds. \"\"\" if not _network [ net_id ][ 'instance' ]: _error ( 'simulate_until(): the network is not compiled yet.' ) nb_steps = ceil ( float ( max_duration ) / dt ()) if not isinstance ( population , list ): population = [ population ] if measure_time : tstart = time . time () nb = _network [ net_id ][ 'instance' ] . pyx_run_until ( nb_steps , [ pop . id for pop in population ], True if operator == 'and' else False ) sim_time = float ( nb ) / dt () if measure_time : _print ( 'Simulating' , nb / dt () / 1000.0 , 'seconds of the network took' , time . time () - tstart , 'seconds.' ) return sim_time","title":"simulate_until()"},{"location":"API/ANNarchy.html#ANNarchy.step","text":"Performs a single simulation step (duration = dt ). Source code in ANNarchy/core/Simulate.py 140 141 142 143 144 145 146 147 148 def step ( net_id = 0 ): \"\"\" Performs a single simulation step (duration = ``dt``). \"\"\" if not _network [ net_id ][ 'instance' ]: _error ( 'simulate_until(): the network is not compiled yet.' ) _network [ net_id ][ 'instance' ] . pyx_step ()","title":"step()"},{"location":"API/ANNarchy.html#ANNarchy.every","text":"Bases: object Decorator to declare a callback method that will be called periodically during the simulation. Example of setting increasing inputs to a population every 100 ms, with an offset of 90 ms (or -10 ms relative to the period): @every ( period = 100. , offset =- 10. ) def step_input ( n ): pop . I = float ( n ) / 100. simulate ( 10000. ) step_input() will be called at times 90, 190, ..., 9990 ms during the call to simulate() . The method must accept only n as parameter (an integer being 0 the first time the method is called, and incremented afterwards) and can not return anything. The times at which the method is called are relative to the time when simulate() is called (if t is already 150 before calling simulate() , the first call will then be made at t=240 with the previous example). If multiple callbacks are defined, they will be called in the order of their declaration if they occur at the same time. Source code in ANNarchy/core/Simulate.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 class every ( object ): \"\"\" Decorator to declare a callback method that will be called periodically during the simulation. Example of setting increasing inputs to a population every 100 ms, with an offset of 90 ms (or -10 ms relative to the period): ```python @every(period=100., offset=-10.) def step_input(n): pop.I = float(n) / 100. simulate(10000.) ``` ``step_input()`` will be called at times 90, 190, ..., 9990 ms during the call to ``simulate()``. The method must accept only ``n`` as parameter (an integer being 0 the first time the method is called, and incremented afterwards) and can not return anything. The times at which the method is called are relative to the time when ``simulate()`` is called (if ``t`` is already 150 before calling ``simulate()``, the first call will then be made at ``t=240`` with the previous example). If multiple callbacks are defined, they will be called in the order of their declaration if they occur at the same time. \"\"\" def __init__ ( self , period , offset = 0. , wait = 0.0 , net_id = 0 ): \"\"\" :param period: interval in ms between two calls to the function. If less than ``dt``, will be called every step. :param offset: by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period. :param wait: allows to wait for a certain amount of time (in ms) before starting to call the method. ``wait`` can be combined with ``offset``, so if ``period=100.``, ``offset=50.`` and ``wait=500.``, the first call will be made 550 ms after the call to ``simulate()`` \"\"\" self . period = max ( float ( period ), dt ()) self . offset = min ( float ( offset ), self . period ) self . wait = max ( float ( wait ), 0.0 ) _callbacks [ net_id ] . append ( self ) def __call__ ( self , f ): # If there are decorator arguments, __call__() is only called # once, as part of the decoration process! You can only give # it a single argument, which is the function object. self . func = f return f","title":"every"},{"location":"API/ANNarchy.html#ANNarchy.core.Simulate.every.__init__","text":"Parameters: Name Type Description Default period interval in ms between two calls to the function. If less than dt , will be called every step. required offset by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period. 0.0 wait allows to wait for a certain amount of time (in ms) before starting to call the method. wait can be combined with offset , so if period=100. , offset=50. and wait=500. , the first call will be made 550 ms after the call to simulate() 0.0 Source code in ANNarchy/core/Simulate.py 206 207 208 209 210 211 212 213 214 215 216 217 218 def __init__ ( self , period , offset = 0. , wait = 0.0 , net_id = 0 ): \"\"\" :param period: interval in ms between two calls to the function. If less than ``dt``, will be called every step. :param offset: by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period. :param wait: allows to wait for a certain amount of time (in ms) before starting to call the method. ``wait`` can be combined with ``offset``, so if ``period=100.``, ``offset=50.`` and ``wait=500.``, the first call will be made 550 ms after the call to ``simulate()`` \"\"\" self . period = max ( float ( period ), dt ()) self . offset = min ( float ( offset ), self . period ) self . wait = max ( float ( wait ), 0.0 ) _callbacks [ net_id ] . append ( self )","title":"__init__()"},{"location":"API/ANNarchy.html#ANNarchy.enable_callbacks","text":"Enables all declared callbacks for the network. Source code in ANNarchy/core/Simulate.py 167 168 169 170 171 def enable_callbacks ( net_id = 0 ): \"\"\" Enables all declared callbacks for the network. \"\"\" _callbacks_enabled [ net_id ] = True","title":"enable_callbacks()"},{"location":"API/ANNarchy.html#ANNarchy.disable_callbacks","text":"Disables all callbacks for the network. Source code in ANNarchy/core/Simulate.py 161 162 163 164 165 def disable_callbacks ( net_id = 0 ): \"\"\" Disables all callbacks for the network. \"\"\" _callbacks_enabled [ net_id ] = False","title":"disable_callbacks()"},{"location":"API/ANNarchy.html#ANNarchy.clear_all_callbacks","text":"Clears the list of declared callbacks for the network. Cannot be undone! Source code in ANNarchy/core/Simulate.py 173 174 175 176 177 178 179 def clear_all_callbacks ( net_id = 0 ): \"\"\" Clears the list of declared callbacks for the network. Cannot be undone! \"\"\" _callbacks [ net_id ] . clear ()","title":"clear_all_callbacks()"},{"location":"API/ANNarchy.html#reset-the-network","text":"If you want to run multiple experiments with the same network, or if your experiment setup requires a pre learning phase, you can reset selectively neural or synaptic variables to their initial values.","title":"Reset the network"},{"location":"API/ANNarchy.html#ANNarchy.reset","text":"Reinitialises the network to its state before the call to compile. The network time will be set to 0ms. All monitors are emptied. Parameters: Name Type Description Default populations if True (default), the neural parameters and variables will be reset to their initial value. True projections if True, the synaptic parameters and variables (except the connections) will be reset (default=False). False synapses if True, the synaptic weights will be erased and recreated (default=False). False monitors if True, the monitors will be emptied and reset (default=True). True","title":"reset()"},{"location":"API/ANNarchy.html#access-to-populations","text":"","title":"Access to populations"},{"location":"API/ANNarchy.html#ANNarchy.get_population","text":"Returns the population with the given name . Parameters: Name Type Description Default name name of the population. required Returns: Type Description The requested Population object if existing, None otherwise.","title":"get_population()"},{"location":"API/ANNarchy.html#ANNarchy.get_projection","text":"Returns the projection with the given name . Parameters: Name Type Description Default name name of the projection. required Returns: Type Description The requested Projection object if existing, None otherwise.","title":"get_projection()"},{"location":"API/ANNarchy.html#functions","text":"","title":"Functions"},{"location":"API/ANNarchy.html#ANNarchy.add_function","text":"Defines a global function which can be used by all neurons and synapses. The function must have only one return value and use only the passed arguments. Examples of valid functions: logistic ( x ) = 1 / ( 1 + exp ( - x )) piecewise ( x , a , b ) = if x < a : a else : if x > b : b else : x Please refer to the manual to know the allowed mathematical functions.","title":"add_function()"},{"location":"API/ANNarchy.html#ANNarchy.functions","text":"Allows to access a global function defined with add_function and use it from Python using arrays after compilation . The name of the function is not added to the global namespace to avoid overloading. add_function ( \"logistic(x) = 1. / (1. + exp(-x))\" ) compile () result = functions ( 'logistic' )([ 0. , 1. , 2. , 3. , 4. ]) Only lists or 1D Numpy arrays can be passed as arguments, not single values nor multidimensional arrays. When passing several arguments, make sure they have the same size.","title":"functions()"},{"location":"API/ANNarchy.html#constants","text":"","title":"Constants"},{"location":"API/ANNarchy.html#ANNarchy.Constant","text":"Bases: float Constant parameter that can be used by all neurons and synapses. The class Constant derives from float , so any legal operation on floats (addition, multiplication) can be used. If a Neuron/Synapse defines a parameter with the same name, the constant parameters will not be visible. Example: tau = Constant ( 'tau' , 20 ) factor = Constant ( 'factor' , 0.1 ) real_tau = Constant ( 'real_tau' , tau * factor ) neuron = Neuron ( equations = ''' real_tau*dr/dt + r =1.0 ''' ) The value of the constant can be changed anytime with the set() method. Assignments will have no effect (e.g. tau = 10.0 only creates a new float). The value of constants defined as combination of other constants ( real_tau ) is not updated if the value of these constants changes (changing tau with tau.set(10.0) will not modify the value of real_tau ).","title":"Constant"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.imag","text":"the imaginary part of a complex number","title":"imag"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.real","text":"the real part of a complex number","title":"real"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__abs__","text":"abs(self)","title":"__abs__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__add__","text":"Return self+value.","title":"__add__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__bool__","text":"True if self else False","title":"__bool__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__ceil__","text":"Return the ceiling as an Integral.","title":"__ceil__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__divmod__","text":"Return divmod(self, value).","title":"__divmod__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__float__","text":"float(self)","title":"__float__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__floor__","text":"Return the floor as an Integral.","title":"__floor__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__floordiv__","text":"Return self//value.","title":"__floordiv__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__format__","text":"Formats the float according to format_spec.","title":"__format__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__getformat__","text":"You probably don't want to use this function. typestr Must be 'double' or 'float'. It exists mainly to be used in Python's test suite. This function returns whichever of 'unknown', 'IEEE, big-endian' or 'IEEE, little-endian' best describes the format of floating point numbers used by the C type named by typestr.","title":"__getformat__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__init__","text":"Parameters: Name Type Description Default name name of the constant (unique), which can be used in equations. required value the value of the constant, which must be a float, or a combination of Constants. required","title":"__init__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__int__","text":"int(self)","title":"__int__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__mod__","text":"Return self%value.","title":"__mod__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__mul__","text":"Return self*value.","title":"__mul__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__neg__","text":"-self","title":"__neg__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__pos__","text":"+self","title":"__pos__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__pow__","text":"Return pow(self, value, mod).","title":"__pow__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__radd__","text":"Return value+self.","title":"__radd__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__rdivmod__","text":"Return divmod(value, self).","title":"__rdivmod__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__rfloordiv__","text":"Return value//self.","title":"__rfloordiv__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__rmod__","text":"Return value%self.","title":"__rmod__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__rmul__","text":"Return value*self.","title":"__rmul__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__round__","text":"Return the Integral closest to x, rounding half toward even. When an argument is passed, work like built-in round(x, ndigits).","title":"__round__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__rpow__","text":"Return pow(value, self, mod).","title":"__rpow__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__rsub__","text":"Return value-self.","title":"__rsub__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__rtruediv__","text":"Return value/self.","title":"__rtruediv__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__setformat__","text":"You probably don't want to use this function. typestr Must be 'double' or 'float'. fmt Must be one of 'unknown', 'IEEE, big-endian' or 'IEEE, little-endian', and in addition can only be one of the latter two if it appears to match the underlying C reality. It exists mainly to be used in Python's test suite. Override the automatic determination of C-level floating point type. This affects how floats are converted to and from binary strings.","title":"__setformat__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__sub__","text":"Return self-value.","title":"__sub__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__truediv__","text":"Return self/value.","title":"__truediv__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__trunc__","text":"Return the Integral closest to x between 0 and x.","title":"__trunc__()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.as_integer_ratio","text":"Return integer ratio. Return a pair of integers, whose ratio is exactly equal to the original float and with a positive denominator. Raise OverflowError on infinities and a ValueError on NaNs. (10.0).as_integer_ratio() (10, 1) (0.0).as_integer_ratio() (0, 1) (-.25).as_integer_ratio() (-1, 4)","title":"as_integer_ratio()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.conjugate","text":"Return self, the complex conjugate of any float.","title":"conjugate()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.fromhex","text":"Create a floating-point number from a hexadecimal string. float.fromhex('0x1.ffffp10') 2047.984375 float.fromhex('-0x1p-1074') -5e-324","title":"fromhex()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.hex","text":"Return a hexadecimal representation of a floating-point number. (-0.1).hex() '-0x1.999999999999ap-4' 3.14159.hex() '0x1.921f9f01b866ep+1'","title":"hex()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.is_integer","text":"Return True if the float is an integer.","title":"is_integer()"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.set","text":"Changes the value of the constant.","title":"set()"},{"location":"API/ANNarchy.html#learning","text":"","title":"Learning"},{"location":"API/ANNarchy.html#ANNarchy.enable_learning","text":"Enables learning for all projections. Optionally period and offset can be changed for all projections. Parameters: Name Type Description Default projections the projections whose learning should be enabled. By default, all the existing projections are enabled. None period determines how often the synaptic variables will be updated. None offset determines the offset at which the synaptic variables will be updated relative to the current time. None","title":"enable_learning()"},{"location":"API/ANNarchy.html#ANNarchy.disable_learning","text":"Disables learning for all projections. Parameters: Name Type Description Default projections the projections whose learning should be disabled. By default, all the existing projections are disabled. None","title":"disable_learning()"},{"location":"API/ANNarchy.html#access-to-simulation-times","text":"","title":"Access to simulation times"},{"location":"API/ANNarchy.html#ANNarchy.get_time","text":"Returns the current time in ms.","title":"get_time()"},{"location":"API/ANNarchy.html#ANNarchy.set_time","text":"Sets the current time in ms. Warning: can be dangerous for some spiking models.","title":"set_time()"},{"location":"API/ANNarchy.html#ANNarchy.get_current_step","text":"Returns the current simulation step.","title":"get_current_step()"},{"location":"API/ANNarchy.html#ANNarchy.set_current_step","text":"Sets the current simulation step (integer). Warning: can be dangerous for some spiking models.","title":"set_current_step()"},{"location":"API/ANNarchy.html#ANNarchy.dt","text":"Returns the simulation step size dt used in the simulation.","title":"dt()"},{"location":"API/BOLD.html","text":"BOLD monitoring # BOLD monitoring utilities are provided in the module ANNarchy.extensions.bold , which must be explicitely imported: from ANNarchy import * from ANNarchy.extensions.bold import BoldMonitor BoldMonitor # BoldMonitor # Bases: object Monitors the BOLD signal for several populations using a computational model. The BOLD monitor transforms one or two input population variables (such as the mean firing rate) into a recordable BOLD signal according to a computational model (for example a variation of the Balloon model). Source code in ANNarchy/extensions/bold/BoldMonitor.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 class BoldMonitor ( object ): \"\"\" Monitors the BOLD signal for several populations using a computational model. The BOLD monitor transforms one or two input population variables (such as the mean firing rate) into a recordable BOLD signal according to a computational model (for example a variation of the Balloon model). \"\"\" def __init__ ( self , populations = None , bold_model = balloon_RN , mapping = { 'I_CBF' : 'r' }, scale_factor = None , normalize_input = None , recorded_variables = None , start = False , net_id = 0 , copied = False ): \"\"\" :param populations: list of recorded populations. :param bold_model: computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is `balloon_RN`. :param mapping: mapping dictionary between the inputs of the BOLD model (`I_CBF` for single inputs, `I_CBF` and `I_CMRO2` for double inputs in the provided examples) and the variables of the input populations. By default, `{'I_CBF': 'r'}` maps the firing rate `r` of the input population(s) to the variable `I_CBF` of the BOLD model. :param scale_factor: list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region. :param normalize_input: list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time). :param recorded_variables: which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples). \"\"\" self . net_id = net_id # instantiate if necessary, please note # that population will make a deepcopy on this objects if inspect . isclass ( bold_model ): bold_model = bold_model () # for reporting bold_model . _model_instantiated = True # The usage of [] as default arguments in the __init__ call lead to strange side effects. # We decided therefore to use None as default and create the lists locally. if populations is None : Global . _error ( \"Either a population or a list of populations must be provided to the BOLD monitor (populations=...)\" ) if scale_factor is None : scale_factor = [] if normalize_input is None : normalize_input = [] if recorded_variables is None : recorded_variables = [] # argument check if not ( isinstance ( populations , list )): populations = [ populations ] if not ( isinstance ( scale_factor , list )): scale_factor = [ scale_factor ] * len ( populations ) if not ( isinstance ( normalize_input , list )): normalize_input = [ normalize_input ] * len ( populations ) if isinstance ( recorded_variables , str ): recorded_variables = [ recorded_variables ] if len ( scale_factor ) > 0 : if len ( populations ) != len ( scale_factor ): Global . _error ( \"BoldMonitor: Length of scale_factor must be equal to number of populations\" ) if len ( normalize_input ) > 0 : if len ( populations ) != len ( normalize_input ): Global . _error ( \"BoldMonitor: Length of normalize_input must be equal to number of populations\" ) # Check mapping for target , input_var in mapping . items (): if not target in bold_model . _inputs : Global . _error ( \"BoldMonitor: the key \" + target + \" of mapping is not part of the BOLD model.\" ) # Check recorded variables if len ( recorded_variables ) == 0 : recorded_variables = bold_model . _output else : # Add the output variables (and remove doublons) l1 = bold_model . _output l2 = [ recorded_variables ] if isinstance ( recorded_variables , str ) else recorded_variables recorded_variables = list ( set ( l2 + l1 )) recorded_variables . sort () if not copied : # Add the container to the object management Global . _network [ 0 ][ 'extensions' ] . append ( self ) self . id = len ( Global . _network [ self . net_id ][ 'extensions' ]) # create the population self . _bold_pop = Population ( 1 , neuron = bold_model , name = bold_model . name ) self . _bold_pop . enabled = start # create the monitor self . _monitor = Monitor ( self . _bold_pop , recorded_variables , start = start ) # create the projection(s) self . _acc_proj = [] if len ( scale_factor ) == 0 : pop_overall_size = 0 for _ , pop in enumerate ( populations ): pop_overall_size += pop . size # the conductance is normalized between [0 .. 1]. This scale factor # should balance different population sizes for _ , pop in enumerate ( populations ): scale_factor_conductance = float ( pop . size ) / float ( pop_overall_size ) scale_factor . append ( scale_factor_conductance ) if len ( normalize_input ) == 0 : normalize_input = [ 0 ] * len ( populations ) # TODO: can we check if users used NormProjections? If not, this will crash ... for target , input_var in mapping . items (): for pop , scale , normalize in zip ( populations , scale_factor , normalize_input ): tmp_proj = AccProjection ( pre = pop , post = self . _bold_pop , target = target , variable = input_var , scale_factor = scale , normalize_input = normalize ) tmp_proj . connect_all_to_all ( weights = 1.0 ) self . _acc_proj . append ( tmp_proj ) else : # Add the container to the object management Global . _network [ net_id ][ 'extensions' ] . append ( self ) self . id = len ( Global . _network [ self . net_id ][ 'extensions' ]) # instances are assigned by the copying instance self . _bold_pop = None self . _monitor = None self . _acc_proj = [] self . name = \"bold_monitor\" # store arguments for copy self . _populations = populations self . _mapping = mapping self . _recorded_variables = recorded_variables self . _bold_model = bold_model self . _start = start # Finalize initialization self . _initialized = True if not copied else False # # MONITOR functions # def start ( self ): \"\"\" Same as `ANNarchy.core.Monitor.start()` \"\"\" self . _monitor . start () # enable ODEs self . _bold_pop . cyInstance . activate ( True ) # check if we have projections with baseline for proj in self . _acc_proj : if proj . _normalize_input > 0 : proj . cyInstance . start ( proj . _normalize_input / Global . config [ \"dt\" ]) def stop ( self ): \"\"\" Same as `ANNarchy.core.Monitor.stop()` \"\"\" self . _monitor . stop () # enable ODEs self . _bold_pop . cyInstance . activate ( False ) def get ( self , variable ): \"\"\" Same as `ANNarchy.core.Monitor.get()` \"\"\" return self . _monitor . get ( variable ) # # POPULATION functions i. e. access to model parameter # # Method called when accessing an attribute. # We overload the default to allow access to monitor variables. def __getattr__ ( self , name ): if name == '_initialized' or not hasattr ( self , '_initialized' ): # Before the end of the constructor return object . __getattribute__ ( self , name ) if self . _initialized : if self . _bold_pop . initialized == False : Global . _error ( \"BoldMonitor: attributes can not modified before compile()\" ) if name in self . _bold_pop . attributes : return getattr ( self . _bold_pop , name ) return object . __getattribute__ ( self , name ) # Method called when accessing an attribute. # We overload the default to allow access to monitor variables. def __setattr__ ( self , name , value ): if name == '_initialized' or not hasattr ( self , '_initialized' ): # Before the end of the constructor return object . __setattr__ ( self , name , value ) if self . _initialized : if self . _bold_pop . initialized == False : Global . _error ( \"BoldMonitor: attributes can not modified before compile()\" ) if name in self . _bold_pop . attributes : setattr ( self . _bold_pop , name , value ) else : raise AttributeError ( \"the variable '\" + str ( name ) + \"' is not an attribute of the bold model.\" ) else : object . __setattr__ ( self , name , value ) __init__ ( populations = None , bold_model = balloon_RN , mapping = { 'I_CBF' : 'r' }, scale_factor = None , normalize_input = None , recorded_variables = None , start = False , net_id = 0 , copied = False ) # Parameters: Name Type Description Default populations list of recorded populations. None bold_model computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is balloon_RN . balloon_RN mapping mapping dictionary between the inputs of the BOLD model ( I_CBF for single inputs, I_CBF and I_CMRO2 for double inputs in the provided examples) and the variables of the input populations. By default, {'I_CBF': 'r'} maps the firing rate r of the input population(s) to the variable I_CBF of the BOLD model. {'I_CBF': 'r'} scale_factor list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region. None normalize_input list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time). None recorded_variables which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples). None Source code in ANNarchy/extensions/bold/BoldMonitor.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def __init__ ( self , populations = None , bold_model = balloon_RN , mapping = { 'I_CBF' : 'r' }, scale_factor = None , normalize_input = None , recorded_variables = None , start = False , net_id = 0 , copied = False ): \"\"\" :param populations: list of recorded populations. :param bold_model: computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is `balloon_RN`. :param mapping: mapping dictionary between the inputs of the BOLD model (`I_CBF` for single inputs, `I_CBF` and `I_CMRO2` for double inputs in the provided examples) and the variables of the input populations. By default, `{'I_CBF': 'r'}` maps the firing rate `r` of the input population(s) to the variable `I_CBF` of the BOLD model. :param scale_factor: list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region. :param normalize_input: list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time). :param recorded_variables: which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples). \"\"\" self . net_id = net_id # instantiate if necessary, please note # that population will make a deepcopy on this objects if inspect . isclass ( bold_model ): bold_model = bold_model () # for reporting bold_model . _model_instantiated = True # The usage of [] as default arguments in the __init__ call lead to strange side effects. # We decided therefore to use None as default and create the lists locally. if populations is None : Global . _error ( \"Either a population or a list of populations must be provided to the BOLD monitor (populations=...)\" ) if scale_factor is None : scale_factor = [] if normalize_input is None : normalize_input = [] if recorded_variables is None : recorded_variables = [] # argument check if not ( isinstance ( populations , list )): populations = [ populations ] if not ( isinstance ( scale_factor , list )): scale_factor = [ scale_factor ] * len ( populations ) if not ( isinstance ( normalize_input , list )): normalize_input = [ normalize_input ] * len ( populations ) if isinstance ( recorded_variables , str ): recorded_variables = [ recorded_variables ] if len ( scale_factor ) > 0 : if len ( populations ) != len ( scale_factor ): Global . _error ( \"BoldMonitor: Length of scale_factor must be equal to number of populations\" ) if len ( normalize_input ) > 0 : if len ( populations ) != len ( normalize_input ): Global . _error ( \"BoldMonitor: Length of normalize_input must be equal to number of populations\" ) # Check mapping for target , input_var in mapping . items (): if not target in bold_model . _inputs : Global . _error ( \"BoldMonitor: the key \" + target + \" of mapping is not part of the BOLD model.\" ) # Check recorded variables if len ( recorded_variables ) == 0 : recorded_variables = bold_model . _output else : # Add the output variables (and remove doublons) l1 = bold_model . _output l2 = [ recorded_variables ] if isinstance ( recorded_variables , str ) else recorded_variables recorded_variables = list ( set ( l2 + l1 )) recorded_variables . sort () if not copied : # Add the container to the object management Global . _network [ 0 ][ 'extensions' ] . append ( self ) self . id = len ( Global . _network [ self . net_id ][ 'extensions' ]) # create the population self . _bold_pop = Population ( 1 , neuron = bold_model , name = bold_model . name ) self . _bold_pop . enabled = start # create the monitor self . _monitor = Monitor ( self . _bold_pop , recorded_variables , start = start ) # create the projection(s) self . _acc_proj = [] if len ( scale_factor ) == 0 : pop_overall_size = 0 for _ , pop in enumerate ( populations ): pop_overall_size += pop . size # the conductance is normalized between [0 .. 1]. This scale factor # should balance different population sizes for _ , pop in enumerate ( populations ): scale_factor_conductance = float ( pop . size ) / float ( pop_overall_size ) scale_factor . append ( scale_factor_conductance ) if len ( normalize_input ) == 0 : normalize_input = [ 0 ] * len ( populations ) # TODO: can we check if users used NormProjections? If not, this will crash ... for target , input_var in mapping . items (): for pop , scale , normalize in zip ( populations , scale_factor , normalize_input ): tmp_proj = AccProjection ( pre = pop , post = self . _bold_pop , target = target , variable = input_var , scale_factor = scale , normalize_input = normalize ) tmp_proj . connect_all_to_all ( weights = 1.0 ) self . _acc_proj . append ( tmp_proj ) else : # Add the container to the object management Global . _network [ net_id ][ 'extensions' ] . append ( self ) self . id = len ( Global . _network [ self . net_id ][ 'extensions' ]) # instances are assigned by the copying instance self . _bold_pop = None self . _monitor = None self . _acc_proj = [] self . name = \"bold_monitor\" # store arguments for copy self . _populations = populations self . _mapping = mapping self . _recorded_variables = recorded_variables self . _bold_model = bold_model self . _start = start # Finalize initialization self . _initialized = True if not copied else False get ( variable ) # Same as ANNarchy.core.Monitor.get() Source code in ANNarchy/extensions/bold/BoldMonitor.py 202 203 204 205 206 def get ( self , variable ): \"\"\" Same as `ANNarchy.core.Monitor.get()` \"\"\" return self . _monitor . get ( variable ) start () # Same as ANNarchy.core.Monitor.start() Source code in ANNarchy/extensions/bold/BoldMonitor.py 179 180 181 182 183 184 185 186 187 188 189 190 191 def start ( self ): \"\"\" Same as `ANNarchy.core.Monitor.start()` \"\"\" self . _monitor . start () # enable ODEs self . _bold_pop . cyInstance . activate ( True ) # check if we have projections with baseline for proj in self . _acc_proj : if proj . _normalize_input > 0 : proj . cyInstance . start ( proj . _normalize_input / Global . config [ \"dt\" ]) stop () # Same as ANNarchy.core.Monitor.stop() Source code in ANNarchy/extensions/bold/BoldMonitor.py 193 194 195 196 197 198 199 200 def stop ( self ): \"\"\" Same as `ANNarchy.core.Monitor.stop()` \"\"\" self . _monitor . stop () # enable ODEs self . _bold_pop . cyInstance . activate ( False ) BOLD models # The provided BOLD models follow the Balloon model (Buxton et al., 1998) with the different variations studied in (Stephan et al., 2007). Those models all compute the vascular response to neural activity through a dampened oscillator: \\[ \\frac{ds}{dt} = \\phi \\, I_\\text{CBF} - \\kappa \\, s - \\gamma \\, (f_{in} - 1) \\] \\[ \\frac{df_{in}}{dt} = s \\] This allows to compute the oxygen extraction fraction: \\[ E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} } \\] The (normalized) venous blood volume is computed as: \\[ \\tau_0 \\, \\frac{dv}{dt} = (f_{in} - f_{out}) \\] \\[ f_{out} = v^{\\frac{1}{\\alpha}} \\] The level of deoxyhemoglobin into the venous compartment is computed by: \\[ \\tau_0 \\, \\frac{dq}{dt} = f_{in} \\, \\frac{E}{E_0} - \\frac{q}{v} \\, f_{out} \\] Using the two signals \\(v\\) and \\(q\\) , there are two ways to compute the corresponding BOLD signal: N: Non-linear BOLD equation: \\[ BOLD = v_0 \\, ( k_1 \\, (1-q) + k_2 \\, (1- \\dfrac{q}{v}) + k_3 \\, (1 - v) ) \\] L: Linear BOLD equation: \\[ BOLD = v_0 \\, ((k_1 + k_2) \\, (1 - q) + (k_3 - k_2) \\, (1 - v)) \\] Additionally, the three coefficients \\(k_1\\) , \\(k_2\\) , \\(k_3\\) can be computed in two different ways: C: classical coefficients from (Buxton et al., 1998): \\[k_1 = (1 - v_0) \\, 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2 = 2 \\, E_0\\] \\[k_3 = 1 - \\epsilon\\] R: revised coefficients from (Obata et al., 2004): \\[k_1 = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2 = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\\] \\[k_3 = 1 - \\epsilon\\] This makes a total of four different BOLD model (RN, RL, CN, CL) which are provided by the extension. The different parameters can be modified in the constructor. Additionally, we also provide the model that was used in (Maith et al., 2021) and the two-inputs model of (Maith et al, 2022). BoldModel # BoldModel # Bases: Neuron Base class to define a BOLD model to be used in a BOLD monitor. A BOLD model is quite similar to a regular rate-coded neuron. It gets a weighted sum of inputs with a specific target (e.g. I_CBF) and compute a single output variable (called BOLD in the predefined models, but it could be anything). The main difference is that a BOLD model should also declare which targets are used for the input signal: bold_model = BoldModel ( parameters = ''' tau = 1000. ''' , equations = ''' I_CBF = sum(I_CBF) # ... tau * dBOLD/dt = I_CBF - BOLD ''' , inputs = \"I_CBF\" ) Source code in ANNarchy/extensions/bold/BoldModel.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class BoldModel ( Neuron ): \"\"\" Base class to define a BOLD model to be used in a BOLD monitor. A BOLD model is quite similar to a regular rate-coded neuron. It gets a weighted sum of inputs with a specific target (e.g. I_CBF) and compute a single output variable (called `BOLD` in the predefined models, but it could be anything). The main difference is that a BOLD model should also declare which targets are used for the input signal: ```python bold_model = BoldModel( parameters = ''' tau = 1000. ''', equations = ''' I_CBF = sum(I_CBF) # ... tau * dBOLD/dt = I_CBF - BOLD ''', inputs = \"I_CBF\" ) ``` \"\"\" def __init__ ( self , parameters , equations , inputs , output = [ \"BOLD\" ], name = \"Custom BOLD model\" , description = \"\" ): \"\"\" See ANNarchy.extensions.bold.PredefinedModels.py for some example models. :param parameters: parameters of the model and their initial value. :param equations: equations defining the temporal evolution of variables. :param inputs: single variable or list of input signals (e.g. 'I_CBF' or ['I_CBF', 'I_CMRO2']). :param output: output variable of the model (default is 'BOLD'). :param name: optional model name. :param description: optional model description. \"\"\" # The processing in BoldMonitor expects lists, but the interface # should allow also single strings (if only one variable is considered) self . _inputs = [ inputs ] if isinstance ( inputs , str ) else inputs self . _output = [ output ] if isinstance ( output , str ) else output Neuron . __init__ ( self , parameters = parameters , equations = equations , name = name , description = description ) self . _model_instantiated = False # activated by BoldMonitor __init__ ( parameters , equations , inputs , output = [ 'BOLD' ], name = 'Custom BOLD model' , description = '' ) # See ANNarchy.extensions.bold.PredefinedModels.py for some example models. Parameters: Name Type Description Default parameters parameters of the model and their initial value. required equations equations defining the temporal evolution of variables. required inputs single variable or list of input signals (e.g. 'I_CBF' or ['I_CBF', 'I_CMRO2']). required output output variable of the model (default is 'BOLD'). ['BOLD'] name optional model name. 'Custom BOLD model' description optional model description. '' Source code in ANNarchy/extensions/bold/BoldModel.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , parameters , equations , inputs , output = [ \"BOLD\" ], name = \"Custom BOLD model\" , description = \"\" ): \"\"\" See ANNarchy.extensions.bold.PredefinedModels.py for some example models. :param parameters: parameters of the model and their initial value. :param equations: equations defining the temporal evolution of variables. :param inputs: single variable or list of input signals (e.g. 'I_CBF' or ['I_CBF', 'I_CMRO2']). :param output: output variable of the model (default is 'BOLD'). :param name: optional model name. :param description: optional model description. \"\"\" # The processing in BoldMonitor expects lists, but the interface # should allow also single strings (if only one variable is considered) self . _inputs = [ inputs ] if isinstance ( inputs , str ) else inputs self . _output = [ output ] if isinstance ( output , str ) else output Neuron . __init__ ( self , parameters = parameters , equations = equations , name = name , description = description ) self . _model_instantiated = False # activated by BoldMonitor balloon_RN # Bases: BoldModel A balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_RN = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 r_0 = 25. ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 ''' , inputs = \"I_CBF\" , ) Source code in ANNarchy/extensions/bold/PredefinedModels.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 class balloon_RN ( BoldModel ): \"\"\" A balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007). Equivalent code: ```python balloon_RN = BoldModel( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 r_0 = 25. ''', equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 ''', inputs=\"I_CBF\", ) ``` \"\"\" def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , r_0 = 25 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal :param r_0: slope of the relation between the intravascular relaxation rate and oxygen saturation \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population r_0 = %(r_0)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , 'r_0' : r_0 , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BOLD model RN\" description = \"BOLD computation with revised coefficients and non-linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description ) __init__ ( phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000.0 , epsilon = 1.43 , r_0 = 25 ) # Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 1 / 1.54 gamma feedback regulation 1 / 2.46 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 40 / 1000.0 epsilon ratio of intra- and extravascular signal 1.43 r_0 slope of the relation between the intravascular relaxation rate and oxygen saturation 25 Source code in ANNarchy/extensions/bold/PredefinedModels.py 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , r_0 = 25 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal :param r_0: slope of the relation between the intravascular relaxation rate and oxygen saturation \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population r_0 = %(r_0)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , 'r_0' : r_0 , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BOLD model RN\" description = \"BOLD computation with revised coefficients and non-linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description ) balloon_RL # Bases: BoldModel A balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_RL = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 r_0 = 25. ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 ''' , inputs = \"I_CBF\" , ) Source code in ANNarchy/extensions/bold/PredefinedModels.py 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 class balloon_RL ( BoldModel ): \"\"\" A balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007). Equivalent code: ```python balloon_RL = BoldModel( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 r_0 = 25. ''', equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 ''', inputs=\"I_CBF\", ) ``` \"\"\" def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , r_0 = 25 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal :param r_0: slope of the relation between the intravascular relaxation rate and oxygen saturation \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population r_0 = %(r_0)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , 'r_0' : r_0 , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 tau*dq/dt = f_in * E / E_0 - (q / v) * f_out : init=1, min=0.01 tau*dv/dt = f_in - f_out : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coeeficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 \"\"\" name = \"BOLD model RL\" description = \"BOLD computation with revised coefficients and linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description ) __init__ ( phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000.0 , epsilon = 1.43 , r_0 = 25 ) # Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 1 / 1.54 gamma feedback regulation 1 / 2.46 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 40 / 1000.0 epsilon ratio of intra- and extravascular signal 1.43 r_0 slope of the relation between the intravascular relaxation rate and oxygen saturation 25 Source code in ANNarchy/extensions/bold/PredefinedModels.py 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , r_0 = 25 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal :param r_0: slope of the relation between the intravascular relaxation rate and oxygen saturation \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population r_0 = %(r_0)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , 'r_0' : r_0 , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 tau*dq/dt = f_in * E / E_0 - (q / v) * f_out : init=1, min=0.01 tau*dv/dt = f_in - f_out : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coeeficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 \"\"\" name = \"BOLD model RL\" description = \"BOLD computation with revised coefficients and linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description ) balloon_CL # Bases: BoldModel A balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_CL = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 ''' , inputs = \"I_CBF\" , ) Source code in ANNarchy/extensions/bold/PredefinedModels.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 class balloon_CL ( BoldModel ): \"\"\" A balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007). Equivalent code: ```python balloon_CL = BoldModel( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 ''', equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 ''', inputs=\"I_CBF\", ) ``` \"\"\" def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 \"\"\" name = \"BoldNeuron CL\" description = \"BOLD computation with classic coefficients and linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description ) __init__ ( phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000.0 , epsilon = 1.43 ) # Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 1 / 1.54 gamma feedback regulation 1 / 2.46 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 40 / 1000.0 epsilon ratio of intra- and extravascular signal 1.43 Source code in ANNarchy/extensions/bold/PredefinedModels.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 \"\"\" name = \"BoldNeuron CL\" description = \"BOLD computation with classic coefficients and linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description ) balloon_CN # Bases: BoldModel A balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_CN = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 ''' , inputs = \"I_CBF\" , ) Source code in ANNarchy/extensions/bold/PredefinedModels.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 class balloon_CN ( BoldModel ): \"\"\" A balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007). Equivalent code: ```python balloon_CN = BoldModel( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 ''', equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 ''', inputs=\"I_CBF\", ) ``` \"\"\" def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BoldNeuron CN\" , description = \"BOLD computation with classic coefficients and non-linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description ) __init__ ( phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000.0 , epsilon = 1.43 ) # Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 1 / 1.54 gamma feedback regulation 1 / 2.46 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 40 / 1000.0 epsilon ratio of intra- and extravascular signal 1.43 Source code in ANNarchy/extensions/bold/PredefinedModels.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BoldNeuron CN\" , description = \"BOLD computation with classic coefficients and non-linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description ) balloon_maith2021 # Bases: BoldModel The balloon model as used in Maith et al. (2021). Source code in ANNarchy/extensions/bold/PredefinedModels.py 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 class balloon_maith2021 ( BoldModel ): \"\"\" The balloon model as used in Maith et al. (2021). \"\"\" def __init__ ( self ): \"Constructor\" parameters = \"\"\" second = 1000.0 phi = 1.0 kappa = 0.665 gamma = 0.412 E_0 = 0.3424 tau = 1.0368 alpha = 0.3215 V_0 = 0.02 \"\"\" equations = \"\"\" I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 k_1 = 7 * E_0 k_2 = 2 k_3 = 2 * E_0 - 0.2 BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"Maith2021 BOLD model\" description = \"BOLD computation from Maith et al. (2021).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description ) __init__ () # Constructor Source code in ANNarchy/extensions/bold/PredefinedModels.py 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 def __init__ ( self ): \"Constructor\" parameters = \"\"\" second = 1000.0 phi = 1.0 kappa = 0.665 gamma = 0.412 E_0 = 0.3424 tau = 1.0368 alpha = 0.3215 V_0 = 0.02 \"\"\" equations = \"\"\" I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 k_1 = 7 * E_0 k_2 = 2 k_3 = 2 * E_0 - 0.2 BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"Maith2021 BOLD model\" description = \"BOLD computation from Maith et al. (2021).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description ) balloon_two_inputs # Bases: BoldModel BOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007). Source code in ANNarchy/extensions/bold/PredefinedModels.py 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 class balloon_two_inputs ( BoldModel ): \"\"\" BOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007). \"\"\" def __init__ ( self ): \"Constructor\" # damped harmonic oscillators, gamma->spring coefficient, kappa->damping coefficient # CBF --> gamma from Friston # CMRO2 --> faster --> gamma=gamma_CBF*10 (therefore scaling of I_CMRO2 by (gamma_CMRO2 / gamma_CBF) --> if same input (I_CBF==I_CMRO2) CMRO2 and CBF same steady-state) # critical kappa --> kappa**2-4*gamma = 0 --> kappa=sqrt(4*gamma) # CBF underdamped for undershoot --> kappa = 0.6*sqrt(4*gamma) # CMRO2 critical --> kappa = sqrt(4*gamma) # after CBF and CMRO2 standard balloon model with revised coefficients, parameter values = Friston et al. (2000) parameters = \"\"\" kappa_CBF = 0.7650920556760059 gamma_CBF = 1/2.46 kappa_CMRO2 = 4.032389192727559 gamma_CMRO2 = 10/2.46 phi_CBF = 1.0 phi_CMRO2 = 1.0 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1 r_0 = 25 tau_out1 = 0 tau_out2 = 20 second = 1000 \"\"\" equations = \"\"\" # CBF input I_CBF = sum(I_CBF) : init=0 second*ds_CBF/dt = phi_CBF * I_CBF - kappa_CBF * s_CBF - gamma_CBF * (f_in - 1) : init=0 second*df_in/dt = s_CBF : init=1, min=0.01 # CMRO2 input I_CMRO2 = sum(I_CMRO2) : init=0 second*ds_CMRO2/dt = phi_CMRO2 * I_CMRO2 * (gamma_CMRO2 / gamma_CBF) - kappa_CMRO2 * s_CMRO2 - gamma_CMRO2 * (r - 1) : init=0 second*dr/dt = s_CMRO2 : init=1, min=0.01 dv = f_in - v**(1 / alpha) tau_out = if dv>0: tau_out1 else: tau_out2 f_out = v**(1/alpha) + tau_out * dv / (tau + tau_out) : init=1, min=0.01 dq/dt = (r - (q / v) * f_out) / (second*tau) : init=1, min=0.01 dv/dt = dv / (tau + tau_out) / second : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BOLD model with two inputs\" description = \"BOLD model with two inputs (CBF-driving and CMRO2-driving). Combination of neurovascular coupling of Friston et al. (2000) and non-linear Balloon model with revised coefficients (Buxton et al, 1998, Stephan et al, 2007)\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = [ 'I_CBF' , 'I_CMRO2' ], output = \"BOLD\" , name = name , description = description ) __init__ () # Constructor Source code in ANNarchy/extensions/bold/PredefinedModels.py 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 def __init__ ( self ): \"Constructor\" # damped harmonic oscillators, gamma->spring coefficient, kappa->damping coefficient # CBF --> gamma from Friston # CMRO2 --> faster --> gamma=gamma_CBF*10 (therefore scaling of I_CMRO2 by (gamma_CMRO2 / gamma_CBF) --> if same input (I_CBF==I_CMRO2) CMRO2 and CBF same steady-state) # critical kappa --> kappa**2-4*gamma = 0 --> kappa=sqrt(4*gamma) # CBF underdamped for undershoot --> kappa = 0.6*sqrt(4*gamma) # CMRO2 critical --> kappa = sqrt(4*gamma) # after CBF and CMRO2 standard balloon model with revised coefficients, parameter values = Friston et al. (2000) parameters = \"\"\" kappa_CBF = 0.7650920556760059 gamma_CBF = 1/2.46 kappa_CMRO2 = 4.032389192727559 gamma_CMRO2 = 10/2.46 phi_CBF = 1.0 phi_CMRO2 = 1.0 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1 r_0 = 25 tau_out1 = 0 tau_out2 = 20 second = 1000 \"\"\" equations = \"\"\" # CBF input I_CBF = sum(I_CBF) : init=0 second*ds_CBF/dt = phi_CBF * I_CBF - kappa_CBF * s_CBF - gamma_CBF * (f_in - 1) : init=0 second*df_in/dt = s_CBF : init=1, min=0.01 # CMRO2 input I_CMRO2 = sum(I_CMRO2) : init=0 second*ds_CMRO2/dt = phi_CMRO2 * I_CMRO2 * (gamma_CMRO2 / gamma_CBF) - kappa_CMRO2 * s_CMRO2 - gamma_CMRO2 * (r - 1) : init=0 second*dr/dt = s_CMRO2 : init=1, min=0.01 dv = f_in - v**(1 / alpha) tau_out = if dv>0: tau_out1 else: tau_out2 f_out = v**(1/alpha) + tau_out * dv / (tau + tau_out) : init=1, min=0.01 dq/dt = (r - (q / v) * f_out) / (second*tau) : init=1, min=0.01 dv/dt = dv / (tau + tau_out) / second : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BOLD model with two inputs\" description = \"BOLD model with two inputs (CBF-driving and CMRO2-driving). Combination of neurovascular coupling of Friston et al. (2000) and non-linear Balloon model with revised coefficients (Buxton et al, 1998, Stephan et al, 2007)\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = [ 'I_CBF' , 'I_CMRO2' ], output = \"BOLD\" , name = name , description = description ) References # Buxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855\u2013864. doi:10.1002/mrm.1910390602 Friston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466\u2013477 Buxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220\u2013S233. doi:10.1016/j.neuroimage.2004.07.013 Stephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387\u2013401. doi:10.1016/j.neuroimage.2007.07.040 Maith et al. (2021) A computational model-based analysis of basal ganglia pathway changes in Parkinson\u2019s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278\u2013 2295. doi:10.1111/ejn.14868 Maith et al. (2022) BOLD monitoring in the neural simulator ANNarchy. Frontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966.","title":"BOLD monitoring"},{"location":"API/BOLD.html#bold-monitoring","text":"BOLD monitoring utilities are provided in the module ANNarchy.extensions.bold , which must be explicitely imported: from ANNarchy import * from ANNarchy.extensions.bold import BoldMonitor","title":"BOLD monitoring"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldMonitor","text":"","title":"BoldMonitor"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor","text":"Bases: object Monitors the BOLD signal for several populations using a computational model. The BOLD monitor transforms one or two input population variables (such as the mean firing rate) into a recordable BOLD signal according to a computational model (for example a variation of the Balloon model). Source code in ANNarchy/extensions/bold/BoldMonitor.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 class BoldMonitor ( object ): \"\"\" Monitors the BOLD signal for several populations using a computational model. The BOLD monitor transforms one or two input population variables (such as the mean firing rate) into a recordable BOLD signal according to a computational model (for example a variation of the Balloon model). \"\"\" def __init__ ( self , populations = None , bold_model = balloon_RN , mapping = { 'I_CBF' : 'r' }, scale_factor = None , normalize_input = None , recorded_variables = None , start = False , net_id = 0 , copied = False ): \"\"\" :param populations: list of recorded populations. :param bold_model: computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is `balloon_RN`. :param mapping: mapping dictionary between the inputs of the BOLD model (`I_CBF` for single inputs, `I_CBF` and `I_CMRO2` for double inputs in the provided examples) and the variables of the input populations. By default, `{'I_CBF': 'r'}` maps the firing rate `r` of the input population(s) to the variable `I_CBF` of the BOLD model. :param scale_factor: list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region. :param normalize_input: list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time). :param recorded_variables: which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples). \"\"\" self . net_id = net_id # instantiate if necessary, please note # that population will make a deepcopy on this objects if inspect . isclass ( bold_model ): bold_model = bold_model () # for reporting bold_model . _model_instantiated = True # The usage of [] as default arguments in the __init__ call lead to strange side effects. # We decided therefore to use None as default and create the lists locally. if populations is None : Global . _error ( \"Either a population or a list of populations must be provided to the BOLD monitor (populations=...)\" ) if scale_factor is None : scale_factor = [] if normalize_input is None : normalize_input = [] if recorded_variables is None : recorded_variables = [] # argument check if not ( isinstance ( populations , list )): populations = [ populations ] if not ( isinstance ( scale_factor , list )): scale_factor = [ scale_factor ] * len ( populations ) if not ( isinstance ( normalize_input , list )): normalize_input = [ normalize_input ] * len ( populations ) if isinstance ( recorded_variables , str ): recorded_variables = [ recorded_variables ] if len ( scale_factor ) > 0 : if len ( populations ) != len ( scale_factor ): Global . _error ( \"BoldMonitor: Length of scale_factor must be equal to number of populations\" ) if len ( normalize_input ) > 0 : if len ( populations ) != len ( normalize_input ): Global . _error ( \"BoldMonitor: Length of normalize_input must be equal to number of populations\" ) # Check mapping for target , input_var in mapping . items (): if not target in bold_model . _inputs : Global . _error ( \"BoldMonitor: the key \" + target + \" of mapping is not part of the BOLD model.\" ) # Check recorded variables if len ( recorded_variables ) == 0 : recorded_variables = bold_model . _output else : # Add the output variables (and remove doublons) l1 = bold_model . _output l2 = [ recorded_variables ] if isinstance ( recorded_variables , str ) else recorded_variables recorded_variables = list ( set ( l2 + l1 )) recorded_variables . sort () if not copied : # Add the container to the object management Global . _network [ 0 ][ 'extensions' ] . append ( self ) self . id = len ( Global . _network [ self . net_id ][ 'extensions' ]) # create the population self . _bold_pop = Population ( 1 , neuron = bold_model , name = bold_model . name ) self . _bold_pop . enabled = start # create the monitor self . _monitor = Monitor ( self . _bold_pop , recorded_variables , start = start ) # create the projection(s) self . _acc_proj = [] if len ( scale_factor ) == 0 : pop_overall_size = 0 for _ , pop in enumerate ( populations ): pop_overall_size += pop . size # the conductance is normalized between [0 .. 1]. This scale factor # should balance different population sizes for _ , pop in enumerate ( populations ): scale_factor_conductance = float ( pop . size ) / float ( pop_overall_size ) scale_factor . append ( scale_factor_conductance ) if len ( normalize_input ) == 0 : normalize_input = [ 0 ] * len ( populations ) # TODO: can we check if users used NormProjections? If not, this will crash ... for target , input_var in mapping . items (): for pop , scale , normalize in zip ( populations , scale_factor , normalize_input ): tmp_proj = AccProjection ( pre = pop , post = self . _bold_pop , target = target , variable = input_var , scale_factor = scale , normalize_input = normalize ) tmp_proj . connect_all_to_all ( weights = 1.0 ) self . _acc_proj . append ( tmp_proj ) else : # Add the container to the object management Global . _network [ net_id ][ 'extensions' ] . append ( self ) self . id = len ( Global . _network [ self . net_id ][ 'extensions' ]) # instances are assigned by the copying instance self . _bold_pop = None self . _monitor = None self . _acc_proj = [] self . name = \"bold_monitor\" # store arguments for copy self . _populations = populations self . _mapping = mapping self . _recorded_variables = recorded_variables self . _bold_model = bold_model self . _start = start # Finalize initialization self . _initialized = True if not copied else False # # MONITOR functions # def start ( self ): \"\"\" Same as `ANNarchy.core.Monitor.start()` \"\"\" self . _monitor . start () # enable ODEs self . _bold_pop . cyInstance . activate ( True ) # check if we have projections with baseline for proj in self . _acc_proj : if proj . _normalize_input > 0 : proj . cyInstance . start ( proj . _normalize_input / Global . config [ \"dt\" ]) def stop ( self ): \"\"\" Same as `ANNarchy.core.Monitor.stop()` \"\"\" self . _monitor . stop () # enable ODEs self . _bold_pop . cyInstance . activate ( False ) def get ( self , variable ): \"\"\" Same as `ANNarchy.core.Monitor.get()` \"\"\" return self . _monitor . get ( variable ) # # POPULATION functions i. e. access to model parameter # # Method called when accessing an attribute. # We overload the default to allow access to monitor variables. def __getattr__ ( self , name ): if name == '_initialized' or not hasattr ( self , '_initialized' ): # Before the end of the constructor return object . __getattribute__ ( self , name ) if self . _initialized : if self . _bold_pop . initialized == False : Global . _error ( \"BoldMonitor: attributes can not modified before compile()\" ) if name in self . _bold_pop . attributes : return getattr ( self . _bold_pop , name ) return object . __getattribute__ ( self , name ) # Method called when accessing an attribute. # We overload the default to allow access to monitor variables. def __setattr__ ( self , name , value ): if name == '_initialized' or not hasattr ( self , '_initialized' ): # Before the end of the constructor return object . __setattr__ ( self , name , value ) if self . _initialized : if self . _bold_pop . initialized == False : Global . _error ( \"BoldMonitor: attributes can not modified before compile()\" ) if name in self . _bold_pop . attributes : setattr ( self . _bold_pop , name , value ) else : raise AttributeError ( \"the variable '\" + str ( name ) + \"' is not an attribute of the bold model.\" ) else : object . __setattr__ ( self , name , value )","title":"BoldMonitor"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor.__init__","text":"Parameters: Name Type Description Default populations list of recorded populations. None bold_model computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is balloon_RN . balloon_RN mapping mapping dictionary between the inputs of the BOLD model ( I_CBF for single inputs, I_CBF and I_CMRO2 for double inputs in the provided examples) and the variables of the input populations. By default, {'I_CBF': 'r'} maps the firing rate r of the input population(s) to the variable I_CBF of the BOLD model. {'I_CBF': 'r'} scale_factor list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region. None normalize_input list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time). None recorded_variables which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples). None Source code in ANNarchy/extensions/bold/BoldMonitor.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def __init__ ( self , populations = None , bold_model = balloon_RN , mapping = { 'I_CBF' : 'r' }, scale_factor = None , normalize_input = None , recorded_variables = None , start = False , net_id = 0 , copied = False ): \"\"\" :param populations: list of recorded populations. :param bold_model: computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is `balloon_RN`. :param mapping: mapping dictionary between the inputs of the BOLD model (`I_CBF` for single inputs, `I_CBF` and `I_CMRO2` for double inputs in the provided examples) and the variables of the input populations. By default, `{'I_CBF': 'r'}` maps the firing rate `r` of the input population(s) to the variable `I_CBF` of the BOLD model. :param scale_factor: list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region. :param normalize_input: list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time). :param recorded_variables: which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples). \"\"\" self . net_id = net_id # instantiate if necessary, please note # that population will make a deepcopy on this objects if inspect . isclass ( bold_model ): bold_model = bold_model () # for reporting bold_model . _model_instantiated = True # The usage of [] as default arguments in the __init__ call lead to strange side effects. # We decided therefore to use None as default and create the lists locally. if populations is None : Global . _error ( \"Either a population or a list of populations must be provided to the BOLD monitor (populations=...)\" ) if scale_factor is None : scale_factor = [] if normalize_input is None : normalize_input = [] if recorded_variables is None : recorded_variables = [] # argument check if not ( isinstance ( populations , list )): populations = [ populations ] if not ( isinstance ( scale_factor , list )): scale_factor = [ scale_factor ] * len ( populations ) if not ( isinstance ( normalize_input , list )): normalize_input = [ normalize_input ] * len ( populations ) if isinstance ( recorded_variables , str ): recorded_variables = [ recorded_variables ] if len ( scale_factor ) > 0 : if len ( populations ) != len ( scale_factor ): Global . _error ( \"BoldMonitor: Length of scale_factor must be equal to number of populations\" ) if len ( normalize_input ) > 0 : if len ( populations ) != len ( normalize_input ): Global . _error ( \"BoldMonitor: Length of normalize_input must be equal to number of populations\" ) # Check mapping for target , input_var in mapping . items (): if not target in bold_model . _inputs : Global . _error ( \"BoldMonitor: the key \" + target + \" of mapping is not part of the BOLD model.\" ) # Check recorded variables if len ( recorded_variables ) == 0 : recorded_variables = bold_model . _output else : # Add the output variables (and remove doublons) l1 = bold_model . _output l2 = [ recorded_variables ] if isinstance ( recorded_variables , str ) else recorded_variables recorded_variables = list ( set ( l2 + l1 )) recorded_variables . sort () if not copied : # Add the container to the object management Global . _network [ 0 ][ 'extensions' ] . append ( self ) self . id = len ( Global . _network [ self . net_id ][ 'extensions' ]) # create the population self . _bold_pop = Population ( 1 , neuron = bold_model , name = bold_model . name ) self . _bold_pop . enabled = start # create the monitor self . _monitor = Monitor ( self . _bold_pop , recorded_variables , start = start ) # create the projection(s) self . _acc_proj = [] if len ( scale_factor ) == 0 : pop_overall_size = 0 for _ , pop in enumerate ( populations ): pop_overall_size += pop . size # the conductance is normalized between [0 .. 1]. This scale factor # should balance different population sizes for _ , pop in enumerate ( populations ): scale_factor_conductance = float ( pop . size ) / float ( pop_overall_size ) scale_factor . append ( scale_factor_conductance ) if len ( normalize_input ) == 0 : normalize_input = [ 0 ] * len ( populations ) # TODO: can we check if users used NormProjections? If not, this will crash ... for target , input_var in mapping . items (): for pop , scale , normalize in zip ( populations , scale_factor , normalize_input ): tmp_proj = AccProjection ( pre = pop , post = self . _bold_pop , target = target , variable = input_var , scale_factor = scale , normalize_input = normalize ) tmp_proj . connect_all_to_all ( weights = 1.0 ) self . _acc_proj . append ( tmp_proj ) else : # Add the container to the object management Global . _network [ net_id ][ 'extensions' ] . append ( self ) self . id = len ( Global . _network [ self . net_id ][ 'extensions' ]) # instances are assigned by the copying instance self . _bold_pop = None self . _monitor = None self . _acc_proj = [] self . name = \"bold_monitor\" # store arguments for copy self . _populations = populations self . _mapping = mapping self . _recorded_variables = recorded_variables self . _bold_model = bold_model self . _start = start # Finalize initialization self . _initialized = True if not copied else False","title":"__init__()"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor.get","text":"Same as ANNarchy.core.Monitor.get() Source code in ANNarchy/extensions/bold/BoldMonitor.py 202 203 204 205 206 def get ( self , variable ): \"\"\" Same as `ANNarchy.core.Monitor.get()` \"\"\" return self . _monitor . get ( variable )","title":"get()"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor.start","text":"Same as ANNarchy.core.Monitor.start() Source code in ANNarchy/extensions/bold/BoldMonitor.py 179 180 181 182 183 184 185 186 187 188 189 190 191 def start ( self ): \"\"\" Same as `ANNarchy.core.Monitor.start()` \"\"\" self . _monitor . start () # enable ODEs self . _bold_pop . cyInstance . activate ( True ) # check if we have projections with baseline for proj in self . _acc_proj : if proj . _normalize_input > 0 : proj . cyInstance . start ( proj . _normalize_input / Global . config [ \"dt\" ])","title":"start()"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor.stop","text":"Same as ANNarchy.core.Monitor.stop() Source code in ANNarchy/extensions/bold/BoldMonitor.py 193 194 195 196 197 198 199 200 def stop ( self ): \"\"\" Same as `ANNarchy.core.Monitor.stop()` \"\"\" self . _monitor . stop () # enable ODEs self . _bold_pop . cyInstance . activate ( False )","title":"stop()"},{"location":"API/BOLD.html#bold-models","text":"The provided BOLD models follow the Balloon model (Buxton et al., 1998) with the different variations studied in (Stephan et al., 2007). Those models all compute the vascular response to neural activity through a dampened oscillator: \\[ \\frac{ds}{dt} = \\phi \\, I_\\text{CBF} - \\kappa \\, s - \\gamma \\, (f_{in} - 1) \\] \\[ \\frac{df_{in}}{dt} = s \\] This allows to compute the oxygen extraction fraction: \\[ E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} } \\] The (normalized) venous blood volume is computed as: \\[ \\tau_0 \\, \\frac{dv}{dt} = (f_{in} - f_{out}) \\] \\[ f_{out} = v^{\\frac{1}{\\alpha}} \\] The level of deoxyhemoglobin into the venous compartment is computed by: \\[ \\tau_0 \\, \\frac{dq}{dt} = f_{in} \\, \\frac{E}{E_0} - \\frac{q}{v} \\, f_{out} \\] Using the two signals \\(v\\) and \\(q\\) , there are two ways to compute the corresponding BOLD signal: N: Non-linear BOLD equation: \\[ BOLD = v_0 \\, ( k_1 \\, (1-q) + k_2 \\, (1- \\dfrac{q}{v}) + k_3 \\, (1 - v) ) \\] L: Linear BOLD equation: \\[ BOLD = v_0 \\, ((k_1 + k_2) \\, (1 - q) + (k_3 - k_2) \\, (1 - v)) \\] Additionally, the three coefficients \\(k_1\\) , \\(k_2\\) , \\(k_3\\) can be computed in two different ways: C: classical coefficients from (Buxton et al., 1998): \\[k_1 = (1 - v_0) \\, 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2 = 2 \\, E_0\\] \\[k_3 = 1 - \\epsilon\\] R: revised coefficients from (Obata et al., 2004): \\[k_1 = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2 = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\\] \\[k_3 = 1 - \\epsilon\\] This makes a total of four different BOLD model (RN, RL, CN, CL) which are provided by the extension. The different parameters can be modified in the constructor. Additionally, we also provide the model that was used in (Maith et al., 2021) and the two-inputs model of (Maith et al, 2022).","title":"BOLD models"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldModel","text":"","title":"BoldModel"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldModel.BoldModel","text":"Bases: Neuron Base class to define a BOLD model to be used in a BOLD monitor. A BOLD model is quite similar to a regular rate-coded neuron. It gets a weighted sum of inputs with a specific target (e.g. I_CBF) and compute a single output variable (called BOLD in the predefined models, but it could be anything). The main difference is that a BOLD model should also declare which targets are used for the input signal: bold_model = BoldModel ( parameters = ''' tau = 1000. ''' , equations = ''' I_CBF = sum(I_CBF) # ... tau * dBOLD/dt = I_CBF - BOLD ''' , inputs = \"I_CBF\" ) Source code in ANNarchy/extensions/bold/BoldModel.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class BoldModel ( Neuron ): \"\"\" Base class to define a BOLD model to be used in a BOLD monitor. A BOLD model is quite similar to a regular rate-coded neuron. It gets a weighted sum of inputs with a specific target (e.g. I_CBF) and compute a single output variable (called `BOLD` in the predefined models, but it could be anything). The main difference is that a BOLD model should also declare which targets are used for the input signal: ```python bold_model = BoldModel( parameters = ''' tau = 1000. ''', equations = ''' I_CBF = sum(I_CBF) # ... tau * dBOLD/dt = I_CBF - BOLD ''', inputs = \"I_CBF\" ) ``` \"\"\" def __init__ ( self , parameters , equations , inputs , output = [ \"BOLD\" ], name = \"Custom BOLD model\" , description = \"\" ): \"\"\" See ANNarchy.extensions.bold.PredefinedModels.py for some example models. :param parameters: parameters of the model and their initial value. :param equations: equations defining the temporal evolution of variables. :param inputs: single variable or list of input signals (e.g. 'I_CBF' or ['I_CBF', 'I_CMRO2']). :param output: output variable of the model (default is 'BOLD'). :param name: optional model name. :param description: optional model description. \"\"\" # The processing in BoldMonitor expects lists, but the interface # should allow also single strings (if only one variable is considered) self . _inputs = [ inputs ] if isinstance ( inputs , str ) else inputs self . _output = [ output ] if isinstance ( output , str ) else output Neuron . __init__ ( self , parameters = parameters , equations = equations , name = name , description = description ) self . _model_instantiated = False # activated by BoldMonitor","title":"BoldModel"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldModel.BoldModel.__init__","text":"See ANNarchy.extensions.bold.PredefinedModels.py for some example models. Parameters: Name Type Description Default parameters parameters of the model and their initial value. required equations equations defining the temporal evolution of variables. required inputs single variable or list of input signals (e.g. 'I_CBF' or ['I_CBF', 'I_CMRO2']). required output output variable of the model (default is 'BOLD'). ['BOLD'] name optional model name. 'Custom BOLD model' description optional model description. '' Source code in ANNarchy/extensions/bold/BoldModel.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , parameters , equations , inputs , output = [ \"BOLD\" ], name = \"Custom BOLD model\" , description = \"\" ): \"\"\" See ANNarchy.extensions.bold.PredefinedModels.py for some example models. :param parameters: parameters of the model and their initial value. :param equations: equations defining the temporal evolution of variables. :param inputs: single variable or list of input signals (e.g. 'I_CBF' or ['I_CBF', 'I_CMRO2']). :param output: output variable of the model (default is 'BOLD'). :param name: optional model name. :param description: optional model description. \"\"\" # The processing in BoldMonitor expects lists, but the interface # should allow also single strings (if only one variable is considered) self . _inputs = [ inputs ] if isinstance ( inputs , str ) else inputs self . _output = [ output ] if isinstance ( output , str ) else output Neuron . __init__ ( self , parameters = parameters , equations = equations , name = name , description = description ) self . _model_instantiated = False # activated by BoldMonitor","title":"__init__()"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.balloon_RN","text":"Bases: BoldModel A balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_RN = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 r_0 = 25. ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 ''' , inputs = \"I_CBF\" , ) Source code in ANNarchy/extensions/bold/PredefinedModels.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 class balloon_RN ( BoldModel ): \"\"\" A balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007). Equivalent code: ```python balloon_RN = BoldModel( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 r_0 = 25. ''', equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 ''', inputs=\"I_CBF\", ) ``` \"\"\" def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , r_0 = 25 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal :param r_0: slope of the relation between the intravascular relaxation rate and oxygen saturation \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population r_0 = %(r_0)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , 'r_0' : r_0 , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BOLD model RN\" description = \"BOLD computation with revised coefficients and non-linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description )","title":"balloon_RN"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.PredefinedModels.balloon_RN.__init__","text":"Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 1 / 1.54 gamma feedback regulation 1 / 2.46 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 40 / 1000.0 epsilon ratio of intra- and extravascular signal 1.43 r_0 slope of the relation between the intravascular relaxation rate and oxygen saturation 25 Source code in ANNarchy/extensions/bold/PredefinedModels.py 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , r_0 = 25 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal :param r_0: slope of the relation between the intravascular relaxation rate and oxygen saturation \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population r_0 = %(r_0)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , 'r_0' : r_0 , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BOLD model RN\" description = \"BOLD computation with revised coefficients and non-linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description )","title":"__init__()"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.balloon_RL","text":"Bases: BoldModel A balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_RL = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 r_0 = 25. ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 ''' , inputs = \"I_CBF\" , ) Source code in ANNarchy/extensions/bold/PredefinedModels.py 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 class balloon_RL ( BoldModel ): \"\"\" A balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007). Equivalent code: ```python balloon_RL = BoldModel( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 r_0 = 25. ''', equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 ''', inputs=\"I_CBF\", ) ``` \"\"\" def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , r_0 = 25 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal :param r_0: slope of the relation between the intravascular relaxation rate and oxygen saturation \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population r_0 = %(r_0)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , 'r_0' : r_0 , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 tau*dq/dt = f_in * E / E_0 - (q / v) * f_out : init=1, min=0.01 tau*dv/dt = f_in - f_out : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coeeficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 \"\"\" name = \"BOLD model RL\" description = \"BOLD computation with revised coefficients and linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description )","title":"balloon_RL"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.PredefinedModels.balloon_RL.__init__","text":"Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 1 / 1.54 gamma feedback regulation 1 / 2.46 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 40 / 1000.0 epsilon ratio of intra- and extravascular signal 1.43 r_0 slope of the relation between the intravascular relaxation rate and oxygen saturation 25 Source code in ANNarchy/extensions/bold/PredefinedModels.py 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , r_0 = 25 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal :param r_0: slope of the relation between the intravascular relaxation rate and oxygen saturation \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population r_0 = %(r_0)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , 'r_0' : r_0 , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 tau*dq/dt = f_in * E / E_0 - (q / v) * f_out : init=1, min=0.01 tau*dv/dt = f_in - f_out : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coeeficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 \"\"\" name = \"BOLD model RL\" description = \"BOLD computation with revised coefficients and linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description )","title":"__init__()"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.balloon_CL","text":"Bases: BoldModel A balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_CL = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 ''' , inputs = \"I_CBF\" , ) Source code in ANNarchy/extensions/bold/PredefinedModels.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 class balloon_CL ( BoldModel ): \"\"\" A balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007). Equivalent code: ```python balloon_CL = BoldModel( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 ''', equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 ''', inputs=\"I_CBF\", ) ``` \"\"\" def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 \"\"\" name = \"BoldNeuron CL\" description = \"BOLD computation with classic coefficients and linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description )","title":"balloon_CL"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.PredefinedModels.balloon_CL.__init__","text":"Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 1 / 1.54 gamma feedback regulation 1 / 2.46 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 40 / 1000.0 epsilon ratio of intra- and extravascular signal 1.43 Source code in ANNarchy/extensions/bold/PredefinedModels.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Linear equation BOLD = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v)) : init=0 \"\"\" name = \"BoldNeuron CL\" description = \"BOLD computation with classic coefficients and linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description )","title":"__init__()"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.balloon_CN","text":"Bases: BoldModel A balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007). Equivalent code: balloon_CN = BoldModel ( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 ''' , equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 ''' , inputs = \"I_CBF\" , ) Source code in ANNarchy/extensions/bold/PredefinedModels.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 class balloon_CN ( BoldModel ): \"\"\" A balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007). Equivalent code: ```python balloon_CN = BoldModel( parameters = ''' second = 1000.0 phi = 1.0 kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1.43 ''', equations = ''' # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 ''', inputs=\"I_CBF\", ) ``` \"\"\" def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BoldNeuron CN\" , description = \"BOLD computation with classic coefficients and non-linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description )","title":"balloon_CN"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.PredefinedModels.balloon_CN.__init__","text":"Parameters: Name Type Description Default phi input coefficient 1.0 kappa signal decay 1 / 1.54 gamma feedback regulation 1 / 2.46 E_0 oxygen extraction fraction at rest 0.34 tau time constant (in s!) 0.98 alpha vessel stiffness 0.33 V_0 resting venous blood volume fraction 0.02 v_0 frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T 40.3 TE echo time 40 / 1000.0 epsilon ratio of intra- and extravascular signal 1.43 Source code in ANNarchy/extensions/bold/PredefinedModels.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def __init__ ( self , phi = 1.0 , kappa = 1 / 1.54 , gamma = 1 / 2.46 , E_0 = 0.34 , tau = 0.98 , alpha = 0.33 , V_0 = 0.02 , v_0 = 40.3 , TE = 40 / 1000. , epsilon = 1.43 , ): \"\"\" :param phi: input coefficient :param kappa: signal decay :param gamma: feedback regulation :param E_0: oxygen extraction fraction at rest :param tau: time constant (in s!) :param alpha: vessel stiffness :param V_0: resting venous blood volume fraction :param v_0: frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T :param TE: echo time :param epsilon: ratio of intra- and extravascular signal \"\"\" parameters = \"\"\" second = 1000.0 : population phi = %(phi)s : population kappa = %(kappa)s : population gamma = %(gamma)s : population E_0 = %(E_0)s : population tau = %(tau)s : population alpha = %(alpha)s : population V_0 = %(V_0)s : population v_0 = %(v_0)s : population TE = %(TE)s : population epsilon = %(epsilon)s : population \"\"\" % { 'phi' : phi , 'kappa' : kappa , 'gamma' : gamma , 'E_0' : E_0 , 'tau' : tau , 'alpha' : alpha , 'V_0' : V_0 , 'v_0' : v_0 , 'TE' : TE , 'epsilon' : epsilon , } equations = \"\"\" # Single input I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Classic coefficients k_1 = (1 - V_0) * 4.3 * v_0 * E_0 * TE k_2 = 2 * E_0 k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BoldNeuron CN\" , description = \"BOLD computation with classic coefficients and non-linear BOLD equation (Stephan et al., 2007).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description )","title":"__init__()"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.balloon_maith2021","text":"Bases: BoldModel The balloon model as used in Maith et al. (2021). Source code in ANNarchy/extensions/bold/PredefinedModels.py 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 class balloon_maith2021 ( BoldModel ): \"\"\" The balloon model as used in Maith et al. (2021). \"\"\" def __init__ ( self ): \"Constructor\" parameters = \"\"\" second = 1000.0 phi = 1.0 kappa = 0.665 gamma = 0.412 E_0 = 0.3424 tau = 1.0368 alpha = 0.3215 V_0 = 0.02 \"\"\" equations = \"\"\" I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 k_1 = 7 * E_0 k_2 = 2 k_3 = 2 * E_0 - 0.2 BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"Maith2021 BOLD model\" description = \"BOLD computation from Maith et al. (2021).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description )","title":"balloon_maith2021"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.PredefinedModels.balloon_maith2021.__init__","text":"Constructor Source code in ANNarchy/extensions/bold/PredefinedModels.py 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 def __init__ ( self ): \"Constructor\" parameters = \"\"\" second = 1000.0 phi = 1.0 kappa = 0.665 gamma = 0.412 E_0 = 0.3424 tau = 1.0368 alpha = 0.3215 V_0 = 0.02 \"\"\" equations = \"\"\" I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 k_1 = 7 * E_0 k_2 = 2 k_3 = 2 * E_0 - 0.2 BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"Maith2021 BOLD model\" description = \"BOLD computation from Maith et al. (2021).\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = \"I_CBF\" , output = \"BOLD\" , name = name , description = description )","title":"__init__()"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.balloon_two_inputs","text":"Bases: BoldModel BOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007). Source code in ANNarchy/extensions/bold/PredefinedModels.py 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 class balloon_two_inputs ( BoldModel ): \"\"\" BOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007). \"\"\" def __init__ ( self ): \"Constructor\" # damped harmonic oscillators, gamma->spring coefficient, kappa->damping coefficient # CBF --> gamma from Friston # CMRO2 --> faster --> gamma=gamma_CBF*10 (therefore scaling of I_CMRO2 by (gamma_CMRO2 / gamma_CBF) --> if same input (I_CBF==I_CMRO2) CMRO2 and CBF same steady-state) # critical kappa --> kappa**2-4*gamma = 0 --> kappa=sqrt(4*gamma) # CBF underdamped for undershoot --> kappa = 0.6*sqrt(4*gamma) # CMRO2 critical --> kappa = sqrt(4*gamma) # after CBF and CMRO2 standard balloon model with revised coefficients, parameter values = Friston et al. (2000) parameters = \"\"\" kappa_CBF = 0.7650920556760059 gamma_CBF = 1/2.46 kappa_CMRO2 = 4.032389192727559 gamma_CMRO2 = 10/2.46 phi_CBF = 1.0 phi_CMRO2 = 1.0 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1 r_0 = 25 tau_out1 = 0 tau_out2 = 20 second = 1000 \"\"\" equations = \"\"\" # CBF input I_CBF = sum(I_CBF) : init=0 second*ds_CBF/dt = phi_CBF * I_CBF - kappa_CBF * s_CBF - gamma_CBF * (f_in - 1) : init=0 second*df_in/dt = s_CBF : init=1, min=0.01 # CMRO2 input I_CMRO2 = sum(I_CMRO2) : init=0 second*ds_CMRO2/dt = phi_CMRO2 * I_CMRO2 * (gamma_CMRO2 / gamma_CBF) - kappa_CMRO2 * s_CMRO2 - gamma_CMRO2 * (r - 1) : init=0 second*dr/dt = s_CMRO2 : init=1, min=0.01 dv = f_in - v**(1 / alpha) tau_out = if dv>0: tau_out1 else: tau_out2 f_out = v**(1/alpha) + tau_out * dv / (tau + tau_out) : init=1, min=0.01 dq/dt = (r - (q / v) * f_out) / (second*tau) : init=1, min=0.01 dv/dt = dv / (tau + tau_out) / second : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BOLD model with two inputs\" description = \"BOLD model with two inputs (CBF-driving and CMRO2-driving). Combination of neurovascular coupling of Friston et al. (2000) and non-linear Balloon model with revised coefficients (Buxton et al, 1998, Stephan et al, 2007)\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = [ 'I_CBF' , 'I_CMRO2' ], output = \"BOLD\" , name = name , description = description )","title":"balloon_two_inputs"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.PredefinedModels.balloon_two_inputs.__init__","text":"Constructor Source code in ANNarchy/extensions/bold/PredefinedModels.py 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 def __init__ ( self ): \"Constructor\" # damped harmonic oscillators, gamma->spring coefficient, kappa->damping coefficient # CBF --> gamma from Friston # CMRO2 --> faster --> gamma=gamma_CBF*10 (therefore scaling of I_CMRO2 by (gamma_CMRO2 / gamma_CBF) --> if same input (I_CBF==I_CMRO2) CMRO2 and CBF same steady-state) # critical kappa --> kappa**2-4*gamma = 0 --> kappa=sqrt(4*gamma) # CBF underdamped for undershoot --> kappa = 0.6*sqrt(4*gamma) # CMRO2 critical --> kappa = sqrt(4*gamma) # after CBF and CMRO2 standard balloon model with revised coefficients, parameter values = Friston et al. (2000) parameters = \"\"\" kappa_CBF = 0.7650920556760059 gamma_CBF = 1/2.46 kappa_CMRO2 = 4.032389192727559 gamma_CMRO2 = 10/2.46 phi_CBF = 1.0 phi_CMRO2 = 1.0 E_0 = 0.34 tau = 0.98 alpha = 0.33 V_0 = 0.02 v_0 = 40.3 TE = 40/1000. epsilon = 1 r_0 = 25 tau_out1 = 0 tau_out2 = 20 second = 1000 \"\"\" equations = \"\"\" # CBF input I_CBF = sum(I_CBF) : init=0 second*ds_CBF/dt = phi_CBF * I_CBF - kappa_CBF * s_CBF - gamma_CBF * (f_in - 1) : init=0 second*df_in/dt = s_CBF : init=1, min=0.01 # CMRO2 input I_CMRO2 = sum(I_CMRO2) : init=0 second*ds_CMRO2/dt = phi_CMRO2 * I_CMRO2 * (gamma_CMRO2 / gamma_CBF) - kappa_CMRO2 * s_CMRO2 - gamma_CMRO2 * (r - 1) : init=0 second*dr/dt = s_CMRO2 : init=1, min=0.01 dv = f_in - v**(1 / alpha) tau_out = if dv>0: tau_out1 else: tau_out2 f_out = v**(1/alpha) + tau_out * dv / (tau + tau_out) : init=1, min=0.01 dq/dt = (r - (q / v) * f_out) / (second*tau) : init=1, min=0.01 dv/dt = dv / (tau + tau_out) / second : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1 - epsilon # Non-linear equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0 \"\"\" name = \"BOLD model with two inputs\" description = \"BOLD model with two inputs (CBF-driving and CMRO2-driving). Combination of neurovascular coupling of Friston et al. (2000) and non-linear Balloon model with revised coefficients (Buxton et al, 1998, Stephan et al, 2007)\" BoldModel . __init__ ( self , parameters = parameters , equations = equations , inputs = [ 'I_CBF' , 'I_CMRO2' ], output = \"BOLD\" , name = name , description = description )","title":"__init__()"},{"location":"API/BOLD.html#references","text":"Buxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855\u2013864. doi:10.1002/mrm.1910390602 Friston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466\u2013477 Buxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220\u2013S233. doi:10.1016/j.neuroimage.2004.07.013 Stephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387\u2013401. doi:10.1016/j.neuroimage.2007.07.040 Maith et al. (2021) A computational model-based analysis of basal ganglia pathway changes in Parkinson\u2019s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278\u2013 2295. doi:10.1111/ejn.14868 Maith et al. (2022) BOLD monitoring in the neural simulator ANNarchy. Frontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966.","title":"References"},{"location":"API/Convolution.html","text":"Convolution and Pooling # Convolution and pooling operations are provided in the module ANNarchy.extensions.convolution . They must be explicitely imported: from ANNarchy import * from ANNarchy.extensions.convolution import * ANNarchy.extensions.convolution.Convolution # Bases: Projection Performs a convolution of a weight kernel on the pre-synaptic population. Despite its name, the operation performed is actually a cross-correlation, as is usual in computer vision and convolutional neural networks: \\[g(x) = \\sum_{k=-n}^n h(k) \\, f(x + k)\\] The convolution operation benefits from giving a multi-dimensional geometry to the populations and filters, for example in 2D: inp = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) pop = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Convolution ( inp , pop , 'exc' ) proj . connect_filter ( [ [ - 1. , 0. , 1. ], [ - 1. , 0. , 1. ], [ - 1. , 0. , 1. ] ]) The maximum number of dimensions for populations and filters is 4, an error is thrown otherwise. Depending on the number of dimensions of the pre- and post-synaptic populations, as well as of the kernel, the convolution is implemented differentely. Method connect_filter() If the pre- and post-populations have the same dimension as the kernel, the convolution is regular. Example: (100, 100) * (3, 3) -> (100, 100) If the post-population has one dimension less than the pre-synaptic one, the last dimension of the kernel must match the last one of the pre-synaptic population. Example: (100, 100, 3) * (3, 3, 3) -> (100, 100) If the kernel has less dimensions than the two populations, the number of neurons in the last dimension of the populations must be the same. The convolution will be calculated for each feature map in the last dimension. In this case, you must set keep_last_dimension to True . Example: (100, 100, 16) * (3, 3) -> (100, 100, 16) Method connect_filters() If the kernel has more dimensions than the pre-synaptic population, this means a bank of different filters will be applied on the pre-synaptic population (like a convolutional layer in a CNN). Attention: the first index of weights corresponds to the different filters, while the result will be accessible in the last dimension of the post-synaptic population. You must set the multiple argument to True. Example: (100, 100) * (16, 3, 3) -> (100, 100, 16) The convolution always uses padding for elements that would be outside the array (no equivalent of valid in tensorflow). It is 0.0 by default, but can be changed using the padding argument. Setting padding to the string border will repeat the value of the border elements. Sub-sampling will be automatically performed according to the populations' geometry. If these geometries do not match, an error will be thrown. Example: (100, 100) * (3, 3) -> (50, 50) You can redefine the sub-sampling by providing a list subsampling as argument, defining for each post-synaptic neuron the coordinates of the pre-synaptic neuron which will be the center of the filter/kernel. Source code in ANNarchy/extensions/convolution/Convolve.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 class Convolution ( Projection ): \"\"\" Performs a convolution of a weight kernel on the pre-synaptic population. Despite its name, the operation performed is actually a cross-correlation, as is usual in computer vision and convolutional neural networks: $$g(x) = \\sum_{k=-n}^n h(k) \\, f(x + k)$$ The convolution operation benefits from giving a multi-dimensional geometry to the populations and filters, for example in 2D: ```python inp = Population(geometry=(100, 100), neuron=Neuron(parameters=\"r = 0.0\")) pop = Population(geometry=(100, 100), neuron=Neuron(equations=\"r = sum(exc)\")) proj = Convolution(inp, pop, 'exc') proj.connect_filter( [ [-1., 0., 1.], [-1., 0., 1.], [-1., 0., 1.] ]) ``` The maximum number of dimensions for populations and filters is 4, an error is thrown otherwise. Depending on the number of dimensions of the pre- and post-synaptic populations, as well as of the kernel, the convolution is implemented differentely. **Method connect_filter()** * If the pre- and post-populations have the same dimension as the kernel, the convolution is regular. Example: (100, 100) * (3, 3) -> (100, 100) * If the post-population has one dimension less than the pre-synaptic one, the last dimension of the kernel must match the last one of the pre-synaptic population. Example: (100, 100, 3) * (3, 3, 3) -> (100, 100) * If the kernel has less dimensions than the two populations, the number of neurons in the last dimension of the populations must be the same. The convolution will be calculated for each feature map in the last dimension. In this case, you must set ``keep_last_dimension`` to ``True``. Example: (100, 100, 16) * (3, 3) -> (100, 100, 16) **Method connect_filters()** * If the kernel has more dimensions than the pre-synaptic population, this means a bank of different filters will be applied on the pre-synaptic population (like a convolutional layer in a CNN). Attention: the first index of ``weights`` corresponds to the different filters, while the result will be accessible in the last dimension of the post-synaptic population. You must set the ``multiple`` argument to True. Example: (100, 100) * (16, 3, 3) -> (100, 100, 16) The convolution **always** uses padding for elements that would be outside the array (no equivalent of ``valid`` in tensorflow). It is 0.0 by default, but can be changed using the ``padding`` argument. Setting ``padding`` to the string ``border`` will repeat the value of the border elements. Sub-sampling will be automatically performed according to the populations' geometry. If these geometries do not match, an error will be thrown. Example: (100, 100) * (3, 3) -> (50, 50) You can redefine the sub-sampling by providing a list ``subsampling`` as argument, defining for each post-synaptic neuron the coordinates of the pre-synaptic neuron which will be the center of the filter/kernel. \"\"\" def __init__ ( self , pre , post , target , psp = \"pre.r * w\" , operation = \"sum\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum). \"\"\" # Sanity check #if not pre.neuron_type.type == 'rate': # Global._error('Convolution: only implemented for rate-coded populations.') # Create the description, but it will not be used for generation Projection . __init__ ( self , pre , post , target , synapse = SharedSynapse ( psp = psp , operation = operation , name = \"Convolution operation\" , description = \"Convoluted kernel over the pre-synaptic population.\" ), name = name , copied = copied ) # Disable saving self . _saveable = False # For copy self . _used_single_filter = False self . _used_bank_of_filters = False self . operation = operation @property def weights ( self ): if not self . initialized : return self . init [ \"weights\" ] else : return self . cyInstance . get_w () @weights . setter def weights ( self , value ): if not self . initialized : self . init [ \"weights\" ] = value else : if self . dim_kernel != value . ndim : raise AttributeError ( \"Mismatch between filter dimensions\" ) self . cyInstance . set_w ( value ) def connect_filter ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ): \"\"\" Applies a single filter on the pre-synaptic population. :param weights: numpy array or list of lists representing the matrix of weights for the filter. :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons. :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. \"\"\" # Process the weights self . weights = np . array ( weights ) # Process the delays self . delays = float ( delays ) if not isinstance ( delays , ( int , float )): Global . _error ( 'Convolutions can only have constant delays.' ) self . subsampling = subsampling self . keep_last_dimension = keep_last_dimension self . padding = padding self . multiple = False # Check dimensions of populations and weight matrix self . dim_kernel = self . weights . ndim self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the pre-synaptic population (maximum 4).' ) if self . dim_kernel > 5 or ( not self . multiple and self . dim_kernel > 4 ): print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the kernel (maximum 4).' ) # Check if the last axes match for parallel convolution (e.g. 3-2-3) if self . dim_kernel < self . dim_pre : if not self . keep_last_dimension : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.' ) if self . pre . geometry [ - 1 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.' ) # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less. if self . dim_post < self . dim_pre : # OK, but check the last dimension of the kernel has the same size as the post-population if self . weights . shape [ - 1 ] != self . pre . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.' ) # Check if it is a bank of filters if self . dim_kernel > self . dim_pre : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has more dimensions than the pre-synaptic population, you need to use the connect_filters() method.' ) # Generate the pre-synaptic coordinates self . _generate_pre_coordinates () # Finish building the synapses self . _create () # For copy self . _used_single_filter = True return self def connect_filters ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ): \"\"\" Applies a set of different filters on the pre-synaptic population. The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters. :param weights: numpy array or list of lists representing the matrix of weights for the filter. :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons. :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. \"\"\" # Process the weights self . weights = np . array ( weights ) # Process the delays self . delays = float ( delays ) if not isinstance ( delays , ( int , float )): Global . _error ( 'Convolutions can only have constant delays.' ) self . subsampling = subsampling self . keep_last_dimension = keep_last_dimension self . padding = padding self . multiple = True # Check dimensions of populations and weight matrix self . dim_kernel = self . weights . ndim self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the pre-synaptic population (maximum 4).' ) if self . dim_kernel > 5 or ( not self . multiple and self . dim_kernel > 4 ): print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the kernel (maximum 4).' ) # Check if the last axes match for parallel convolution (e.g. 3-2-3) if self . dim_kernel < self . dim_pre : if not self . keep_last_dimension : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.' ) if self . pre . geometry [ - 1 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.' ) # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less. if self . dim_post < self . dim_pre : # OK, but check the last dimension of the kernel has the same size as the post-population if self . weights . shape [ - 1 ] != self . pre . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.' ) # The last dimension of the post population must correspond to the number of filters if self . weights . shape [ 0 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: For multiple filters, the last dimension of the post-synaptic population must have as many neurons as there are filters.' ) # Generate the pre-synaptic coordinates self . _generate_pre_coordinates_bank () # Finish building the synapses self . _create () # For copy self . _used_bank_of_filters = True return self def _copy ( self , pre , post ): \"Returns a copy of the projection when creating networks. Internal use only.\" copied_proj = Convolution ( pre = pre , post = post , target = self . target , psp = self . synapse_type . psp , operation = self . operation , name = self . name , copied = True ) copied_proj . delays = self . delays copied_proj . weights = self . weights copied_proj . subsampling = self . subsampling copied_proj . keep_last_dimension = self . keep_last_dimension copied_proj . padding = self . padding copied_proj . multiple = self . multiple copied_proj . dim_kernel = self . weights . ndim copied_proj . dim_pre = self . pre . dimension copied_proj . dim_post = self . post . dimension if self . _used_single_filter : copied_proj . _generate_pre_coordinates () elif self . _used_bank_of_filters : copied_proj . _generate_pre_coordinates_bank () else : raise ValueError ( \"Either use single filter or bank of filter must be True! (Missing connect?)\" ) copied_proj . _create () copied_proj . _connection_method = self . _connection_method copied_proj . _connection_args = self . _connection_args copied_proj . _connection_delay = self . _connection_delay copied_proj . _storage_format = self . _storage_format return copied_proj def _create ( self ): # create fake LIL object, just for compilation. try : from ANNarchy.core.cython_ext.Connector import LILConnectivity except Exception as e : Global . _print ( e ) Global . _error ( 'ANNarchy was not successfully installed.' ) lil = LILConnectivity () lil . max_delay = self . delays lil . uniform_delay = self . delays self . connector_name = \"Convolution\" self . connector_description = \"Convolution\" self . _store_connectivity ( self . _load_from_lil , ( lil , ), self . delays , storage_format = \"lil\" , storage_order = \"post_to_pre\" ) ################################ ### Create connection pattern ################################ def _connect ( self , module ): \"\"\" Builds up dendrites either from list or dictionary. Called by instantiate(). \"\"\" if not self . _connection_method : Global . _error ( 'Convolution: The projection between ' + self . pre . name + ' and ' + self . post . name + ' is declared but not connected.' ) # Create the Cython instance proj = getattr ( module , 'proj' + str ( self . id ) + '_wrapper' ) self . cyInstance = proj ( self . weights , self . pre_coordinates ) # Set delays after instantiation if self . delays > 0.0 : self . cyInstance . set_delay ( self . delays / Global . config [ 'dt' ]) return True def _generate_pre_coordinates ( self ): \" Returns a list for each post neuron of the corresponding center coordinates.\" # Check if the list is already defined: if self . subsampling : try : shape = np . array ( self . subsampling ) . shape except : Global . _error ( 'Convolution: The sub-sampling list must have' , self . post . size , 'elements of size' , self . pre . dimension ) return if shape != ( self . post . size , self . pre . dimension ): Global . _error ( 'Convolution: The sub-sampling list must have' , self . post . size , 'elements of size' , self . pre . dimension ) return self . pre_coordinates = self . subsampling return # Otherwise create it, possibly with sub-sampling coords = [[] for i in range ( self . post . size )] # Compute pre-indices idx_range = [] for dim in range ( self . dim_pre ): if dim < self . dim_post : pre_size = int ( self . pre . geometry [ dim ]) post_size = int ( self . post . geometry [ dim ]) sample = int ( pre_size / post_size ) if post_size * sample != pre_size : Global . _error ( 'Convolution: The pre-synaptic dimensions must be a multiple of the post-synaptic ones for down-sampling to work.' ) idx_range . append ([ int (( sample - 1 ) / 2 ) + sample * i for i in range ( post_size )]) else : # extra dimension if self . keep_last_dimension : idx_range . append ( range ( self . post . geometry [ dim ])) else : idx_range . append ([ self . _center_filter ( self . weights . shape [ dim ])]) # Generates coordinates TODO: Find a more robust way! if self . dim_pre == 1 : rk = 0 for i in idx_range [ 0 ]: coords [ rk ] = [ i ] rk += 1 elif self . dim_pre == 2 : rk = 0 for i in idx_range [ 0 ]: for j in idx_range [ 1 ]: coords [ rk ] = [ i , j ] rk += 1 elif self . dim_pre == 3 : rk = 0 for i in idx_range [ 0 ]: for j in idx_range [ 1 ]: for k in idx_range [ 2 ]: coords [ rk ] = [ i , j , k ] rk += 1 elif self . dim_pre == 4 : rk = 0 for i in idx_range [ 0 ]: for j in idx_range [ 1 ]: for k in idx_range [ 2 ]: for l in idx_range [ 3 ]: coords [ rk ] = [ i , j , k , l ] rk += 1 # Save the result self . pre_coordinates = coords def _generate_pre_coordinates_bank ( self ): \" Returns a list for each post neuron of the corresponding center coordinates, when the filter is a bank.\" self . nb_filters = self . weights . shape [ 0 ] self . dim_single_filter = self . weights . shape [ 1 :] # Check if the list is already defined: if self . subsampling : try : shape = np . array ( self . subsampling ) . shape except : Global . _error ( 'Convolution: The sub-sampling list must have' , self . post . size / self . post . geometry [ - 1 ], 'elements of size' , self . pre . dimension ) return if shape != ( self . post . size / self . post . geometry [ - 1 ], self . pre . dimension ): Global . _error ( 'Convolution: The sub-sampling list must have' , self . post . size / self . post . geometry [ - 1 ], 'elements of size' , self . pre . dimension ) return self . pre_coordinates = [ c + [ d ] for c in self . subsampling for d in range ( self . nb_filters )] return # Otherwise create it, possibly with sub-sampling coords = [[] for i in range ( self . post . size )] # Compute pre-indices idx_range = [] for dim in range ( self . dim_pre ): if dim < self . dim_post - 1 : pre_size = self . pre . geometry [ dim ] post_size = self . post . geometry [ dim ] sample = int ( pre_size / post_size ) if post_size * sample != pre_size : Global . _error ( 'Convolution: The pre-synaptic dimensions must be a multiple of the post-synaptic ones for down-sampling to work.' ) idx_range . append ([ int (( sample - 1 ) / 2 ) + sample * i for i in range ( post_size )]) else : # extra dimension if self . keep_last_dimension : idx_range . append ( range ( self . post . geometry [ dim ])) else : idx_range . append ([ self . _center_filter ( self . weights . shape [ dim + 1 ])]) # Generates coordinates TODO: Find a more robust way! if self . dim_pre == 1 : rk = 0 for i in idx_range [ 0 ]: for d in range ( self . nb_filters ): coords [ rk ] = [ i , d ] rk += 1 elif self . dim_pre == 2 : rk = 0 for i in idx_range [ 0 ]: for j in idx_range [ 1 ]: for d in range ( self . nb_filters ): coords [ rk ] = [ i , j , d ] rk += 1 elif self . dim_pre == 3 : rk = 0 for i in idx_range [ 0 ]: for j in idx_range [ 1 ]: for k in idx_range [ 2 ]: for d in range ( self . nb_filters ): coords [ rk ] = [ i , j , k , d ] rk += 1 elif self . dim_pre == 4 : rk = 0 for i in idx_range [ 0 ]: for j in idx_range [ 1 ]: for k in idx_range [ 2 ]: for l in idx_range [ 3 ]: for d in range ( self . nb_filters ): coords [ rk ] = [ i , j , k , l , d ] rk += 1 # Save the result self . pre_coordinates = coords ################################ # Code generation ################################ def _generate ( self ): \"\"\" Overrides default code generation. This function is called during the code generation procedure. \"\"\" # Filter definition filter_definition , filter_pyx_definition = self . _filter_definition () # Convolve_code if not self . multiple : convolve_code , sum_code = self . _generate_convolve_code () else : convolve_code , sum_code = self . _generate_bank_code () if Global . _check_paradigm ( \"openmp\" ): self . _generate_omp ( filter_definition , filter_pyx_definition , convolve_code , sum_code ) elif Global . _check_paradigm ( \"cuda\" ): raise Global . ANNarchyException ( \"Convolution is not available on CUDA devices yet.\" , True ) else : raise NotImplementedError def _generate_omp ( self , filter_definition , filter_pyx_definition , convolve_code , sum_code , kernel = True ): \"\"\" OpenMP code generation. \"\"\" # Basic ids base_ids = { 'id_proj' : self . id , 'size_post' : self . post . size , 'float_prec' : Global . config [ 'precision' ] } # Fill the basic definitions conv_dict = deepcopy ( convole_template_omp ) for key , value in conv_dict . items (): value = value % base_ids conv_dict [ key ] = value self . _specific_template . update ( conv_dict ) # Kernel-based method: specify w with the correct dimension if kernel : self . _specific_template [ 'declare_parameters_variables' ] = tabify ( filter_definition . strip (), 1 ) self . _specific_template [ 'export_parameters_variables' ] = \"\" self . _specific_template [ 'access_parameters_variables' ] = \"\"\" // Local parameter w %(type_w)s get_w() { return w; } void set_w( %(type_w)s value) { w = value; } \"\"\" % { 'type_w' : filter_definition . replace ( ' w;' , '' )} self . _specific_template [ 'export_connectivity' ] += \"\"\" # Local variable w %(type_w)s get_w() void set_w( %(type_w)s ) \"\"\" % { 'type_w' : filter_pyx_definition . replace ( ' w' , '' )} self . _specific_template [ 'wrapper_init_connectivity' ] += \"\"\" proj %(id_proj)s .set_w(weights) \"\"\" % { 'id_proj' : self . id } self . _specific_template [ 'wrapper_access_connectivity' ] += \"\"\" # Local variable w def get_w(self): return proj %(id_proj)s .get_w() def set_w(self, value): proj %(id_proj)s .set_w( value ) def get_dendrite_w(self, int rank): return proj %(id_proj)s .get_w() def set_dendrite_w(self, int rank, value): proj %(id_proj)s .set_w(value) def get_synapse_w(self, int rank_post, int rank_pre): return 0.0 def set_synapse_w(self, int rank_post, int rank_pre, %(float_prec)s value): pass \"\"\" % { 'id_proj' : self . id , 'float_prec' : Global . config [ 'precision' ]} # Override the monitor to avoid recording the weights self . _specific_template [ 'monitor_class' ] = \"\" self . _specific_template [ 'monitor_export' ] = \"\" self . _specific_template [ 'monitor_wrapper' ] = \"\" # OMP code omp_code = \"\" if Global . config [ 'num_threads' ] > 1 : omp_code = \"\"\" #pragma omp for private(sum, rk_pre, coord) %(psp_schedule)s \"\"\" % { 'psp_schedule' : \"\" if not 'psp_schedule' in self . _omp_config . keys () else self . _omp_config [ 'psp_schedule' ]} # HD ( 16.10.2015 ): # pre-load delayed firing rate in a local array, so we # prevent multiple accesses to pop%(id_pre)s._delayed_r[delay-1] # wheareas delay is set available as variable # TODO HD: wouldn't it be much better to reduce delay globaly, instead of the substraction here??? if self . delays > Global . config [ 'dt' ]: pre_load_r = \"\"\" // pre-load delayed firing rate auto delayed_r = pop %(id_pre)s ._delayed_r[delay-1]; \"\"\" % { 'id_pre' : self . pre . id } else : pre_load_r = \"\" # Target variable depends on neuron type target_code = \"_sum_ %(target)s \" if self . post . neuron_type . type == \"rate\" else \"g_ %(target)s \" target_code %= { 'target' : self . target } # Compute sum wsum = \"\"\" if ( _transmission && pop %(id_pre)s ._active ) { int* coord; \"\"\" + pre_load_r + \"\"\" %(omp_code)s for(int i = 0; i < %(size_post)s ; i++){ coord = pre_coords[i].data(); // perform the convolution \"\"\" + tabify ( convolve_code , 1 ) + \"\"\" // store result pop %(id_post)s . %(target)s [i] += \"\"\" + sum_code + \"\"\"; } // for } // if \"\"\" self . _specific_template [ 'psp_code' ] = wsum % \\ { 'id_proj' : self . id , 'target' : target_code , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'size_pre' : self . pre . size , 'id_post' : self . post . id , 'name_post' : self . post . name , 'size_post' : self . post . size , 'omp_code' : omp_code , 'convolve_code' : convolve_code } self . _specific_template [ 'size_in_bytes' ] = \"\"\" // post-ranks size_in_bytes += sizeof(std::vector<int>); size_in_bytes += post_rank.capacity() * sizeof(int); // pre-coords size_in_bytes += sizeof(std::vector<std::vector<int>>); size_in_bytes += pre_coords.capacity() * sizeof(std::vector<int>); for (auto it = pre_coords.begin(); it != pre_coords.end(); it++) { size_in_bytes += it->capacity() * sizeof(int); } // filter // TODO: \"\"\" self . _specific_template [ 'clear' ] = \"\"\" // post-ranks post_rank.clear(); post_rank.shrink_to_fit(); // pre-coords for (auto it = pre_coords.begin(); it != pre_coords.end(); it++) { it->clear(); it->shrink_to_fit(); } pre_coords.clear(); pre_coords.shrink_to_fit(); // filter // TODO: \"\"\" ################################ ### Utilities ################################ def _center_filter ( self , i ): return int ( i / 2 ) if i % 2 == 1 else int ( i / 2 ) - 1 def _filter_definition ( self ): dim = self . dim_kernel cpp = Global . config [ 'precision' ] pyx = Global . config [ 'precision' ] for d in range ( dim ): cpp = 'std::vector< ' + cpp + ' >' pyx = 'vector[' + pyx + ']' cpp += ' w;' pyx += ' w' return cpp , pyx def _coordinates_to_rank ( self , name , geometry ): dim = len ( geometry ) txt = \"\" for d in range ( dim ): if txt == \"\" : # first coordinate is special txt = indices [ 0 ] + \"_\" + name else : txt = str ( geometry [ d ]) + '*(' + txt + ') + ' + indices [ d ] + '_' + name return txt def _generate_convolve_code ( self ): # Operation to be performed: sum, max, min, mean operation = self . synapse_type . operation # Main code code = tabify ( \"sum = 0.0; \\n \" , 3 ) # Generate for loops for dim in range ( self . dim_kernel ): if dim == self . dim_kernel - 1 : inner_idx = \"\" for i in range ( self . dim_kernel - 1 ): inner_idx += \"[\" + indices [ i ] + \"_w]\" code += \"auto inner_line = w\" + inner_idx + \".data(); \\n \" code += tabify ( \"\"\" for(int %(index)s _w = 0; %(index)s _w < %(size)s ; %(index)s _w++) { \"\"\" % { 'index' : indices [ dim ], 'size' : self . weights . shape [ dim ]}, dim ) # Compute indices if dim < self . dim_kernel : code += tabify ( \"\"\"int %(index)s_pre = coord[%(dim)s] %(operator)s (%(index)s_w - %(center)s);\"\"\" % { 'id_proj' : self . id , 'index' : indices [ dim ], 'dim' : dim , 'operator' : '+' , 'center' : self . _center_filter ( self . weights . shape [ dim ]) }, 1 ) else : code += tabify ( \"\"\"int %(index)s_pre = coord[%(dim)s];\"\"\" % { 'id_proj' : self . id , 'index' : indices [ dim ], 'dim' : dim }, 1 ) # Check indices if operation in [ 'sum' , 'mean' ]: if isinstance ( self . padding , str ): # 'border' code += tabify ( \"\"\" if ( %(index)s _pre < 0) %(index)s _pre = 0 ; if ( %(index)s _pre > %(max_size)s ) %(index)s _pre = %(max_size)s ; \"\"\" % { 'index' : indices [ dim ], 'dim' : dim , 'max_size' : self . pre . geometry [ dim ] - 1 }, dim ) else : code += tabify ( \"\"\" if (( %(index)s _pre < 0) || ( %(index)s _pre > %(max_size)s )){ sum += %(padding)s ; continue; } \"\"\" % { 'index' : indices [ dim ], 'padding' : self . padding , 'max_size' : self . pre . geometry [ dim ] - 1 }, dim ) else : # min, max code += \"\"\" if (( %(index)s _pre < 0) || ( %(index)s _pre > %(max_size)s )) { continue; } \"\"\" % { 'index' : indices [ dim ], 'max_size' : self . pre . geometry [ dim ] - 1 } # if True, we need to take the last dimension from coords if self . keep_last_dimension : id_dict = { 'index' : indices [ self . dim_kernel ], 'dim' : self . dim_kernel } code += \"int %(index)s _pre = coord[ %(dim)s ];\" % id_dict # Compute pre-synaptic rank code += tabify ( \"\"\" rk_pre = %(value)s ;\"\"\" % { 'value' : self . _coordinates_to_rank ( 'pre' , self . pre . geometry )}, dim ) # Compute the increment index = \"\" for dim in range ( self . dim_kernel ): index += '[' + indices [ dim ] + '_w]' increment = self . synapse_type . description [ 'psp' ][ 'cpp' ] % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'local_index' : index , 'global_index' : '[i]' , 'pre_index' : '[rk_pre]' , 'post_index' : '[rk_post]' , 'pre_prefix' : 'pop' + str ( self . pre . id ) + '.' , 'post_prefix' : 'pop' + str ( self . post . id ) + '.' } # Delays if self . delays > Global . config [ 'dt' ]: increment = increment . replace ( 'pop %(id_pre)s .r[rk_pre]' % { 'id_pre' : self . pre . id }, 'delayed_r[rk_pre]' ) # Apply the operation if operation == \"sum\" : if self . dim_kernel == 1 : code += tabify ( \"\"\" sum += %(increment)s \"\"\" % { 'increment' : increment }, dim ) else : code += tabify ( \"\"\" sum += %(increment)s \"\"\" % { 'increment' : increment . replace ( 'w' + inner_idx , 'inner_line' )}, dim ) elif operation == \"max\" : code += tabify ( \"\"\" %(float_prec)s _psp = %(increment)s if(_psp > sum) sum = _psp;\"\"\" % { 'increment' : increment , 'float_prec' : Global . config [ 'precision' ]}, dim ) elif operation == \"min\" : code += tabify ( \"\"\" %(float_prec)s _psp = %(increment)s if(_psp < sum) sum = _psp;\"\"\" % { 'increment' : increment , 'float_prec' : Global . config [ 'precision' ]}, dim ) elif operation == \"mean\" : code += tabify ( \"\"\" sum += %(increment)s \"\"\" % { 'increment' : increment }, dim ) else : Global . _error ( 'Convolution: Operation' , operation , 'is not implemented yet for shared projections.' ) # Close for loops for dim in range ( self . dim_kernel ): code += tabify ( \"\"\" }\"\"\" , self . dim_kernel - 1 - dim ) impl_code = code % { 'id_proj' : self . id , 'target' : self . target , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'size_pre' : self . pre . size , 'id_post' : self . post . id , 'name_post' : self . post . name , 'size_post' : self . post . size } # sum code self . weights . size if operation == \"mean\" : sum_code = \"\"\"sum/ %(filter_size)s \"\"\" % { 'filter_size' : self . weights . size } else : sum_code = \"sum\" return impl_code , sum_code def _generate_bank_code ( self ): # Operation to be performed: sum, max, min, mean operation = self . synapse_type . operation # Main code code = tabify ( \"sum = 0.0; \\n \" , 3 ) # Generate for loops for dim in range ( self . dim_kernel - 1 ): code += tabify ( \"\"\" for(int %(index)s _w = 0; %(index)s _w < %(size)s ; %(index)s _w++) { \"\"\" % { 'index' : indices [ dim ], 'size' : self . weights . shape [ dim + 1 ]}, dim ) # Compute indices if dim < self . dim_kernel : code += tabify ( \"\"\"int %(index)s_pre = coord[%(dim)s] %(operator)s (%(index)s_w - %(center)s);\"\"\" % { 'id_proj' : self . id , 'index' : indices [ dim ], 'dim' : dim , 'operator' : '+' , 'center' : self . _center_filter ( self . weights . shape [ dim + 1 ]) }, 1 ) else : code += tabify ( \"\"\"int %(index)s_pre = coord[%(dim)s];\"\"\" % { 'id_proj' : self . id , 'index' : indices [ dim ], 'dim' : dim }, 1 ) # Check indices if operation in [ 'sum' , 'mean' ]: if isinstance ( self . padding , str ): # 'border' code += tabify ( \"\"\" if ( %(index)s _pre < 0) %(index)s _pre = 0 ; if ( %(index)s _pre > %(max_size)s ) %(index)s _pre = %(max_size)s ; \"\"\" % { 'index' : indices [ dim ], 'dim' : dim , 'max_size' : self . pre . geometry [ dim ] - 1 }, 1 + dim ) else : code += tabify ( \"\"\" if (( %(index)s _pre < 0) || ( %(index)s _pre > %(max_size)s )) { sum += %(padding)s ; continue; } \"\"\" % { 'index' : indices [ dim ], 'padding' : self . padding , 'max_size' : self . pre . geometry [ dim ] - 1 }, 1 + dim ) else : # min, max code += tabify ( \"\"\" if (( %(index)s _pre < 0) || ( %(index)s _pre > %(max_size)s )){ continue; } \"\"\" % { 'index' : indices [ dim ], 'max_size' : self . pre . geometry [ dim ] - 1 }, 1 + dim ) # Compute pre-synaptic rank code += tabify ( \"\"\" rk_pre = %(value)s ;\"\"\" % { 'value' : self . _coordinates_to_rank ( 'pre' , self . pre . geometry )}, 1 + dim ) # Compute the increment index = \"[coord[\" + str ( self . dim_pre ) + \"]]\" for dim in range ( self . dim_kernel - 1 ): index += '[' + indices [ dim ] + '_w]' increment = self . synapse_type . description [ 'psp' ][ 'cpp' ] % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'local_index' : index , 'global_index' : '[i]' , 'pre_index' : '[rk_pre]' , 'post_index' : '[rk_post]' , 'pre_prefix' : 'pop' + str ( self . pre . id ) + '.' , 'post_prefix' : 'pop' + str ( self . post . id ) + '.' } # Delays if self . delays > Global . config [ 'dt' ]: increment = increment . replace ( 'pop %(id_pre)s .r[rk_pre]' % { 'id_pre' : self . pre . id }, 'delayed_r[rk_pre]' ) # Apply the operation if operation == \"sum\" : code += tabify ( \"\"\" sum += %(increment)s \"\"\" % { 'increment' : increment }, 1 + dim ) elif operation == \"max\" : code += tabify ( \"\"\" %(float_prec)s _psp = %(increment)s if(_psp > sum) sum = _psp;\"\"\" % { 'increment' : increment , 'float_prec' : Global . config [ 'precision' ]}, 1 + dim ) elif operation == \"min\" : code += tabify ( \"\"\" %(float_prec)s _psp = %(increment)s if(_psp < sum) sum = _psp;\"\"\" % { 'increment' : increment , 'float_prec' : Global . config [ 'precision' ]}, 1 + dim ) elif operation == \"mean\" : code += tabify ( \"\"\" sum += %(increment)s \"\"\" % { 'increment' : increment }, 1 + dim ) else : Global . _error ( 'SharedProjection: Operation' , operation , 'is not implemented yet for shared projections.' ) # Close for loops for dim in range ( self . dim_kernel - 1 ): code += tabify ( \"\"\" }\"\"\" , self . dim_kernel - 1 - dim ) impl_code = code % { 'id_proj' : self . id , 'target' : self . target , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'size_pre' : self . pre . size , 'id_post' : self . post . id , 'name_post' : self . post . name , 'size_post' : self . post . size } # sum code if operation == \"mean\" : sum_code = \"\"\"sum/ %(filter_size)s \"\"\" % { 'filter_size' : self . weights . size } else : sum_code = \"sum\" return impl_code , sum_code ############################## ## Override useless methods ############################## def _data ( self ): \"Disable saving.\" desc = {} desc [ 'post_ranks' ] = self . post_ranks desc [ 'attributes' ] = self . attributes desc [ 'parameters' ] = self . parameters desc [ 'variables' ] = self . variables desc [ 'dendrites' ] = [] desc [ 'number_of_synapses' ] = 0 return desc def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be saved.' ) def save ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be saved.' ) def load ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be loaded.' ) def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Convolutional projections can not display receptive fields.' ) def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Convolutional projections can not display connectivity matrices.' ) __init__ ( pre , post , target , psp = 'pre.r * w' , operation = 'sum' , name = None , copied = False ) # Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection required psp continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r ). 'pre.r * w' operation operation (sum, max, min, mean) performed by the kernel (default: sum). 'sum' Source code in ANNarchy/extensions/convolution/Convolve.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , pre , post , target , psp = \"pre.r * w\" , operation = \"sum\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum). \"\"\" # Sanity check #if not pre.neuron_type.type == 'rate': # Global._error('Convolution: only implemented for rate-coded populations.') # Create the description, but it will not be used for generation Projection . __init__ ( self , pre , post , target , synapse = SharedSynapse ( psp = psp , operation = operation , name = \"Convolution operation\" , description = \"Convoluted kernel over the pre-synaptic population.\" ), name = name , copied = copied ) # Disable saving self . _saveable = False # For copy self . _used_single_filter = False self . _used_bank_of_filters = False self . operation = operation connect_filter ( weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ) # Applies a single filter on the pre-synaptic population. Parameters: Name Type Description Default weights numpy array or list of lists representing the matrix of weights for the filter. required delays delay in synaptic transmission (default: dt). Can only be the same value for all neurons. 0.0 keep_last_dimension defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. False padding value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. 0.0 subsampling list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. None Source code in ANNarchy/extensions/convolution/Convolve.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def connect_filter ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ): \"\"\" Applies a single filter on the pre-synaptic population. :param weights: numpy array or list of lists representing the matrix of weights for the filter. :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons. :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. \"\"\" # Process the weights self . weights = np . array ( weights ) # Process the delays self . delays = float ( delays ) if not isinstance ( delays , ( int , float )): Global . _error ( 'Convolutions can only have constant delays.' ) self . subsampling = subsampling self . keep_last_dimension = keep_last_dimension self . padding = padding self . multiple = False # Check dimensions of populations and weight matrix self . dim_kernel = self . weights . ndim self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the pre-synaptic population (maximum 4).' ) if self . dim_kernel > 5 or ( not self . multiple and self . dim_kernel > 4 ): print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the kernel (maximum 4).' ) # Check if the last axes match for parallel convolution (e.g. 3-2-3) if self . dim_kernel < self . dim_pre : if not self . keep_last_dimension : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.' ) if self . pre . geometry [ - 1 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.' ) # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less. if self . dim_post < self . dim_pre : # OK, but check the last dimension of the kernel has the same size as the post-population if self . weights . shape [ - 1 ] != self . pre . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.' ) # Check if it is a bank of filters if self . dim_kernel > self . dim_pre : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has more dimensions than the pre-synaptic population, you need to use the connect_filters() method.' ) # Generate the pre-synaptic coordinates self . _generate_pre_coordinates () # Finish building the synapses self . _create () # For copy self . _used_single_filter = True return self connect_filters ( weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ) # Applies a set of different filters on the pre-synaptic population. The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters. Parameters: Name Type Description Default weights numpy array or list of lists representing the matrix of weights for the filter. required delays delay in synaptic transmission (default: dt). Can only be the same value for all neurons. 0.0 keep_last_dimension defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. False padding value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. 0.0 subsampling list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. None Source code in ANNarchy/extensions/convolution/Convolve.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def connect_filters ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ): \"\"\" Applies a set of different filters on the pre-synaptic population. The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters. :param weights: numpy array or list of lists representing the matrix of weights for the filter. :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons. :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. \"\"\" # Process the weights self . weights = np . array ( weights ) # Process the delays self . delays = float ( delays ) if not isinstance ( delays , ( int , float )): Global . _error ( 'Convolutions can only have constant delays.' ) self . subsampling = subsampling self . keep_last_dimension = keep_last_dimension self . padding = padding self . multiple = True # Check dimensions of populations and weight matrix self . dim_kernel = self . weights . ndim self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the pre-synaptic population (maximum 4).' ) if self . dim_kernel > 5 or ( not self . multiple and self . dim_kernel > 4 ): print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the kernel (maximum 4).' ) # Check if the last axes match for parallel convolution (e.g. 3-2-3) if self . dim_kernel < self . dim_pre : if not self . keep_last_dimension : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.' ) if self . pre . geometry [ - 1 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.' ) # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less. if self . dim_post < self . dim_pre : # OK, but check the last dimension of the kernel has the same size as the post-population if self . weights . shape [ - 1 ] != self . pre . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.' ) # The last dimension of the post population must correspond to the number of filters if self . weights . shape [ 0 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: For multiple filters, the last dimension of the post-synaptic population must have as many neurons as there are filters.' ) # Generate the pre-synaptic coordinates self . _generate_pre_coordinates_bank () # Finish building the synapses self . _create () # For copy self . _used_bank_of_filters = True return self connectivity_matrix ( fill = 0.0 ) # Not available. Source code in ANNarchy/extensions/convolution/Convolve.py 985 986 987 def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Convolutional projections can not display connectivity matrices.' ) load ( filename ) # Not available. Source code in ANNarchy/extensions/convolution/Convolve.py 979 980 981 def load ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be loaded.' ) receptive_fields ( variable = 'w' , in_post_geometry = True ) # Not available. Source code in ANNarchy/extensions/convolution/Convolve.py 982 983 984 def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Convolutional projections can not display receptive fields.' ) save ( filename ) # Not available. Source code in ANNarchy/extensions/convolution/Convolve.py 976 977 978 def save ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be saved.' ) save_connectivity ( filename ) # Not available. Source code in ANNarchy/extensions/convolution/Convolve.py 973 974 975 def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be saved.' ) ANNarchy.extensions.convolution.Pooling # Pooling # Bases: Projection Performs a pooling operation (e.g. max.pooling) on the pre-synaptic population. Each post-synaptic neuron covers a specific region ( extent ) of the pre-synaptic population, over which the result of the operation on firing rates will be assigned to sum(target). The extent is automatically computed using the geometry of the populations, but can be specified in the `connect_pooling()`` methods. Example: inp = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) pop = Population ( geometry = ( 50 , 50 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Pooling ( inp , pop , 'exc' , operation = 'max' ) # max-pooling proj . connect_pooling () # extent=(2, 2) is implicit Source code in ANNarchy/extensions/convolution/Pooling.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 class Pooling ( Projection ): \"\"\" Performs a pooling operation (e.g. max.pooling) on the pre-synaptic population. Each post-synaptic neuron covers a specific region (``extent``) of the pre-synaptic population, over which the result of the operation on firing rates will be assigned to sum(target). The extent is automatically computed using the geometry of the populations, but can be specified in the `connect_pooling()`` methods. Example: ```python inp = Population(geometry=(100, 100), neuron=Neuron(parameters=\"r = 0.0\")) pop = Population(geometry=(50, 50), neuron=Neuron(equations=\"r = sum(exc)\")) proj = Pooling(inp, pop, 'exc', operation='max') # max-pooling proj.connect_pooling() # extent=(2, 2) is implicit ``` \"\"\" def __init__ ( self , pre , post , target , psp = \"pre.r\" , operation = \"max\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param operation: pooling function to be applied (\"max\", \"min\", \"mean\") \"\"\" # Sanity check #if not pre.neuron_type.type == 'rate': # Global._error('Pooling: only implemented for rate-coded populations.') # Sanity check if not operation in [ \"max\" , \"mean\" , \"min\" ]: Global . _error ( \"Pooling: the operation must be either 'max', 'mean' or 'min'.\" ) self . operation = operation # Store for _copy self . psp = psp Projection . __init__ ( self , pre , post , target , synapse = SharedSynapse ( psp = psp , operation = operation , name = \"Pooling operation\" , description = operation + \"-pooling operation over the pre-synaptic population.\" ), name = name , copied = copied ) # check dimensions of populations, should not exceed 4 self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : Global . _error ( 'Pooling: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : Global . _error ( 'Pooling: Too many dimensions for the pre-synaptic population (maximum 4).' ) # Disable saving self . _saveable = False def connect_pooling ( self , extent = None , delays = 0.0 ): \"\"\" :param extent: extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g ``(2, 2)``). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None. :param delays: synaptic delay in ms \"\"\" # process extent self . extent_init = extent if extent is None : # compute the extent automatically if self . pre . dimension != self . post . dimension : Global . _error ( 'Pooling: If you do not provide the extent parameter, the two populations must have the same number of dimensions.' ) extent = list ( self . pre . geometry ) for dim in range ( self . pre . dimension ): extent [ dim ] /= self . post . geometry [ dim ] if self . pre . geometry [ dim ] != extent [ dim ] * self . post . geometry [ dim ]: Global . _error ( 'Pooling: Unable to compute the extent of the pooling area: the number of neurons do not match.' ) elif not isinstance ( extent , tuple ): Global . _error ( 'Pooling: You must provide a tuple for the extent of the pooling operation.' ) self . extent = list ( extent ) if len ( self . extent ) < self . pre . dimension : Global . _error ( 'Pooling: You must provide a tuple for the extent of the pooling operation.' ) # process delays self . delays = delays # Generate the pre-synaptic coordinates self . _generate_extent_coordinates () # create fake LIL self . _create () return self def _copy ( self , pre , post ): \"Returns a copy of the projection when creating networks. Internal use only.\" copied_proj = Pooling ( pre = pre , post = post , target = self . target , psp = self . psp , operation = self . operation , name = self . name , copied = True ) copied_proj . extent = self . extent copied_proj . delays = self . delays copied_proj . _generate_extent_coordinates () copied_proj . _create () copied_proj . _connection_method = self . _connection_method copied_proj . _connection_args = self . _connection_args copied_proj . _connection_delay = self . _connection_delay copied_proj . _storage_format = self . _storage_format return copied_proj def _create ( self ): \"\"\" create fake LIL object, just for compilation process :return: no return value \"\"\" try : from ANNarchy.core.cython_ext.Connector import LILConnectivity except Exception as e : Global . _print ( e ) Global . _error ( 'ANNarchy was not successfully installed.' ) lil = LILConnectivity () lil . max_delay = self . delays lil . uniform_delay = self . delays self . connector_name = \"Pooling\" self . connector_description = \"Pooling\" self . _store_connectivity ( self . _load_from_lil , ( lil , ), self . delays , storage_format = \"lil\" , storage_order = \"post_to_pre\" ) def _connect ( self , module ): \"\"\" Builds up dendrites either from list or dictionary. Called by instantiate(). \"\"\" if not self . _connection_method : Global . _error ( 'Pooling: The projection between ' + self . pre . name + ' and ' + self . post . name + ' is declared but not connected.' ) # Create the Cython instance proj = getattr ( module , 'proj' + str ( self . id ) + '_wrapper' ) self . cyInstance = proj ([], self . pre_coordinates ) return True def _generate_extent_coordinates ( self ): \"\"\" Generates for each post-neuron the position of the top-left corner, where the pooling should be applied. :return: a list for each post neuron of the corresponding top-left coordinates \"\"\" # Generates coordinates TODO: Find a more robust way! coords = [[] for i in range ( self . post . size )] if self . dim_pre == 1 : rk = 0 for i in range ( self . post . geometry [ 0 ]): coords [ rk ] = [ i * self . extent [ 0 ]] rk += 1 elif self . dim_pre == 2 : rk = 0 for i in range ( self . post . geometry [ 0 ]): if self . dim_post > 1 : for j in range ( self . post . geometry [ 1 ]): coords [ rk ] = [ i * self . extent [ 0 ], j * self . extent [ 1 ]] rk += 1 else : # over the whole second axis coords [ rk ] = [ i * self . extent [ 0 ], 0 ] rk += 1 elif self . dim_pre == 3 : rk = 0 for i in range ( self . post . geometry [ 0 ]): for j in range ( self . post . geometry [ 1 ]): if self . dim_post > 2 : for k in range ( self . post . geometry [ 2 ]): coords [ rk ] = [ i * self . extent [ 0 ], j * self . extent [ 1 ], k * self . extent [ 2 ]] rk += 1 else : # over the whole third axis coords [ rk ] = [ i * self . extent [ 0 ], j * self . extent [ 1 ], 0 ] rk += 1 elif self . dim_pre == 4 : # TODO: post has less than 4 dimensions rk = 0 for i in range ( self . post . geometry [ 0 ]): for j in range ( self . post . geometry [ 1 ]): for k in range ( self . post . geometry [ 2 ]): for l in range ( self . post . geometry [ 3 ]): coords [ rk ] = [ i * self . extent [ 0 ], j * self . extent [ 1 ], k * self . extent [ 2 ], l * self . extent [ 3 ]] rk += 1 # Save the result self . pre_coordinates = coords def _generate ( self ): \"\"\" Overrides the default code generation. \"\"\" # Convolve_code convolve_code , sum_code = self . _generate_pooling_code () # Generate the code if Global . _check_paradigm ( \"openmp\" ): self . _generate_omp ( convolve_code , sum_code ) elif Global . _check_paradigm ( \"cuda\" ): Global . _error ( \"Pooling: not available on GPU devices\" ) #self._generate_cuda(convolve_code, sum_code) else : Global . _error ( \"Pooling: not implemented for the configured paradigm\" ) def _generate_pooling_code ( self ): \"\"\" Generate loop statements for the desired pooling operation. \"\"\" # Operation to be performed: sum, max, min, mean operation = self . synapse_type . operation # Main code # default value for sum in code depends on operation sum_default = \"0.0\" if self . synapse_type . operation == \"min\" : sum_default = \"std::numeric_limits< %(float_prec)s >::max()\" % { 'float_prec' : Global . config [ 'precision' ]} elif self . synapse_type . operation == \"max\" : sum_default = \"std::numeric_limits< %(float_prec)s >::min()\" % { 'float_prec' : Global . config [ 'precision' ]} code = \"\"\" sum = %(sum_default)s ; \"\"\" % { 'sum_default' : sum_default } # Generate for loops for dim in range ( self . dim_pre ): ind_dict = { 'index' : indices [ dim ], 'size' : self . extent [ dim ] } if self . extent [ dim ] > 1 : code += \"\"\" for(int %(index)s _w = 0; %(index)s _w < %(size)s ; %(index)s _w++){ \"\"\" % ind_dict # Compute indices for dim in range ( self . dim_pre ): ind_dict = { 'index' : indices [ dim ], 'dim' : dim } if self . extent [ dim ] > 1 : code += \"\"\" int %(index)s _pre = coord[ %(dim)s ] + %(index)s _w;\"\"\" % ind_dict else : code += \"\"\" int %(index)s _pre = coord[ %(dim)s ];\"\"\" % ind_dict # Check indices for dim in range ( self . dim_pre ): ind_dict = { 'index' : indices [ dim ], 'max_size' : self . pre . geometry [ dim ] - 1 } code += \"\"\" if (( %(index)s _pre < 0) ||( %(index)s _pre > %(max_size)s )){ continue; }\"\"\" % ind_dict # Compute pre-synaptic rank code += \"\"\" rk_pre = %(value)s ;\"\"\" % { 'value' : self . _coordinates_to_rank ( 'pre' , self . pre . geometry )} # Compute the value to pool psp = self . synapse_type . description [ 'psp' ][ 'cpp' ] % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'local_index' : '[i][j]' , 'global_index' : '[i]' , 'pre_index' : '[rk_pre]' , 'post_index' : '[rk_post]' , 'pre_prefix' : 'pop' + str ( self . pre . id ) + '.' , 'post_prefix' : 'pop' + str ( self . post . id ) + '.' } # Delays if self . delays > Global . config [ 'dt' ]: psp = psp . replace ( 'pop %(id_pre)s .r[rk_pre]' % { 'id_pre' : self . pre . id }, 'pop %(id_pre)s ._delayed_r[ %(delay)s ][rk_pre]' % { 'id_pre' : self . pre . id , 'delay' : str ( int ( self . delays / Global . config [ 'dt' ]) - 1 )} ) # Apply the operation if operation == \"max\" : code += \"\"\" %(float_prec)s _psp = %(psp)s ; if(_psp > sum) sum = _psp;\"\"\" elif operation == \"min\" : code += \"\"\" %(float_prec)s _psp = %(psp)s ; if(_psp < sum) sum = _psp;\"\"\" elif operation == \"sum\" : code += \"\"\" sum += %(psp)s ;\"\"\" elif operation == \"mean\" : code += \"\"\" sum += %(psp)s ;\"\"\" else : Global . _error ( 'SharedProjection: Operation' , operation , 'is not implemented yet for shared projections with pooling.' ) # Close for loops for dim in range ( self . dim_pre ): if self . extent [ dim ] > 1 : code += \"\"\" }\"\"\" impl_code = code % { 'id_proj' : self . id , 'target' : self . target , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'size_pre' : self . pre . size , 'id_post' : self . post . id , 'name_post' : self . post . name , 'size_post' : self . post . size , 'psp' : psp , 'float_prec' : Global . config [ 'precision' ] } if operation == \"mean\" : size = 1 for dim in range ( self . pre . dimension ): size *= self . extent [ dim ] sum_code = \"sum/\" + str ( size ) else : sum_code = \"sum\" return impl_code , sum_code def _generate_omp ( self , convolve_code , sum_code ): \"\"\" Update the ProjectionGenerator._specific_template structure and bypass the standard openMP code generation. :param convolve_code: :param sum_code: \"\"\" # default value for sum in code depends on operation sum_default = \"0.0\" if self . synapse_type . operation == \"min\" : sum_default = \"std::numeric_limits< %(float_prec)s >::max()\" % { 'float_prec' : Global . config [ 'precision' ]} elif self . synapse_type . operation == \"max\" : sum_default = \"std::numeric_limits< %(float_prec)s >::min()\" % { 'float_prec' : Global . config [ 'precision' ]} # Specific template for generation pool_dict = deepcopy ( pooling_template_omp ) for key , value in pool_dict . items (): value = value % { 'id_proj' : self . id , 'size_post' : self . post . size , 'sum_default' : sum_default , 'float_prec' : Global . config [ 'precision' ] } pool_dict [ key ] = value self . _specific_template . update ( pool_dict ) # OMP code omp_code = \"\" if Global . config [ 'num_threads' ] > 1 : omp_code = \"\"\" #pragma omp for private(sum, rk_pre, coord) %(psp_schedule)s \"\"\" % { 'psp_schedule' : \"\" if not 'psp_schedule' in self . _omp_config . keys () else self . _omp_config [ 'psp_schedule' ]} # HD ( 16.10.2015 ): # pre-load delayed firing rate in a local array, so we # prevent multiple accesses to pop%(id_pre)s._delayed_r[%(delay)s] if self . delays > Global . config [ 'dt' ]: pre_load_r = \"\"\" // pre-load delayed firing rate auto delayed_r = pop %(id_pre)s ._delayed_r[ %(delay)s ]; \"\"\" % { 'id_pre' : self . pre . id , 'delay' : str ( int ( self . delays / Global . config [ 'dt' ]) - 1 )} else : pre_load_r = \"\" # Target variable depends on neuron type target_code = \"_sum_ %(target)s \" if self . post . neuron_type . type == \"rate\" else \"g_ %(target)s \" target_code %= { 'target' : self . target } # Compute sum wsum = \"\"\" if ( _transmission && pop %(id_pre)s ._active ) { std::vector<int> coord; \"\"\" + pre_load_r + \"\"\" %(omp_code)s for(int i = 0; i < %(size_post)s ; i++){ coord = pre_rank[i]; \"\"\" + convolve_code + \"\"\" pop %(id_post)s . %(target)s [i] += \"\"\" + sum_code + \"\"\"; } // for } // if \"\"\" # Delays self . _specific_template [ 'wrapper_init_delay' ] = \"\" # Dictionary keys psp_dict = { 'id_proj' : self . id , 'target' : target_code , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'size_pre' : self . pre . size , 'id_post' : self . post . id , 'name_post' : self . post . name , 'size_post' : self . post . size , 'omp_code' : omp_code , 'convolve_code' : convolve_code } # Psp code self . _specific_template [ 'psp_code' ] = wsum % psp_dict self . _specific_template [ 'size_in_bytes' ] = \"\"\" // connectivity size_in_bytes += sizeof(std::vector<int>); size_in_bytes += pre_rank.capacity() * sizeof(int); size_in_bytes += sizeof(std::vector<std::vector<int>>); size_in_bytes += pre_rank.capacity() * sizeof(std::vector<int>); for (auto it = pre_rank.begin(); it != pre_rank.end(); it++) { size_in_bytes += it->capacity() * sizeof(int); } \"\"\" self . _specific_template [ 'clear' ] = \"\"\" // post-ranks post_rank.clear(); post_rank.shrink_to_fit(); // pre-ranks sub-lists for (auto it = pre_rank.begin(); it != pre_rank.end(); it++) { it->clear(); it->shrink_to_fit(); } // pre-ranks top-list pre_rank.clear(); pre_rank.shrink_to_fit(); \"\"\" def _generate_cuda ( self , convolve_code , sum_code ): \"\"\" Update the ProjectionGenerator._specific_template structure and bypass the standard CUDA code generation. \"\"\" pool_operation = self . synapse_type . operation # default value for sum in code depends on operation sum_default = \"0.0\" if pool_operation == \"min\" : sum_default = \"FLT_MAX\" elif pool_operation == \"max\" : sum_default = \"FLT_MIN\" # operation to perform pool_op_code = cuda_op_code [ pool_operation ] % { 'float_prec' : Global . config [ 'precision' ]} # result dictionary with code for # body, call and header pool_template = {} base_ids = { 'id_proj' : self . id , 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'target' : self . target , 'float_prec' : Global . config [ 'precision' ], 'size_post' : self . post . size # TODO: population views? } # The correct templates depends on both # kernel-geometry and extent if len ( self . pre . geometry ) == 2 : # For small extents, we compute multiple coords within one warp. If one extent can fill alone # a half-warp we switch to the other implementation. if self . extent [ 0 ] < 6 : pool_op_reduce_code = cuda_pooling_code_2d_small_extent [ 'reduce_code' ][ pool_operation ] % { 'float_prec' : Global . config [ 'precision' ], 'row_extent' : int ( self . extent [ 0 ]), 'col_extent' : int ( self . extent [ 1 ]) } pool_dict = deepcopy ( base_ids ) pool_dict . update ({ 'sum_default' : sum_default , 'row_extent' : int ( self . extent [ 0 ]), 'col_extent' : int ( self . extent [ 1 ]), 'row_size' : int ( self . pre . geometry [ 0 ]), 'col_size' : int ( self . pre . geometry [ 1 ]), 'operation' : tabify ( pool_op_code , 3 ), 'operation_reduce' : pool_op_reduce_code }) pool_template [ 'psp_body' ] = cuda_pooling_code_2d_small_extent [ 'psp_body' ] % pool_dict pool_template [ 'psp_header' ] = cuda_pooling_code_2d_small_extent [ 'psp_header' ] % pool_dict pool_template [ 'psp_call' ] = cuda_pooling_code_2d_small_extent [ 'psp_call' ] % pool_dict else : pool_op_reduce_code = cuda_pooling_code_2d [ 'reduce_code' ][ pool_operation ] % { 'float_prec' : Global . config [ 'precision' ], 'row_extent' : int ( self . extent [ 0 ]), 'col_extent' : int ( self . extent [ 1 ]) } pool_dict = deepcopy ( base_ids ) pool_dict . update ({ 'sum_default' : sum_default , 'row_extent' : int ( self . extent [ 0 ]), 'col_extent' : int ( self . extent [ 1 ]), 'row_size' : int ( self . pre . geometry [ 0 ]), 'col_size' : int ( self . pre . geometry [ 1 ]), 'operation' : tabify ( pool_op_code , 3 ), 'operation_reduce' : tabify ( pool_op_reduce_code , 2 ) }) pool_template [ 'psp_body' ] = remove_trailing_spaces ( cuda_pooling_code_2d [ 'psp_body' ] % pool_dict ) pool_template [ 'psp_header' ] = cuda_pooling_code_2d [ 'psp_header' ] % pool_dict pool_template [ 'psp_call' ] = cuda_pooling_code_2d [ 'psp_call' ] % pool_dict elif len ( self . pre . geometry ) == 3 : pool_dict = deepcopy ( base_ids ) pool_dict . update ({ 'sum_default' : sum_default , 'row_extent' : self . extent [ 0 ], 'col_extent' : self . extent [ 1 ], 'plane_extent' : self . extent [ 2 ], 'row_size' : self . pre . geometry [ 0 ], 'col_size' : self . pre . geometry [ 1 ], 'plane_size' : self . pre . geometry [ 2 ], 'operation' : tabify ( pool_op_code , 4 ) }) pool_template [ 'psp_body' ] = remove_trailing_spaces ( cuda_pooling_code_3d [ 'psp_body' ] % pool_dict ) pool_template [ 'psp_header' ] = cuda_pooling_code_3d [ 'psp_header' ] % pool_dict pool_template [ 'psp_call' ] = cuda_pooling_code_3d [ 'psp_header' ] % pool_dict else : raise NotImplementedError # Update psp fields self . _specific_template . update ( pool_template ) # Specific template for generation (wrapper, etc) pool_dict = deepcopy ( pooling_template_cuda ) for key , value in pool_dict . items (): value = value % base_ids pool_dict [ key ] = value self . _specific_template . update ( pool_dict ) self . _specific_template [ 'wrapper_connector_call' ] = \"\" self . _specific_template [ 'access_parameters_variables' ] = \"\" self . _specific_template [ 'size_in_bytes' ] = \"//TODO: \\n \" @staticmethod def _coordinates_to_rank ( name , geometry ): \"\"\" Generate the code for array access, for instance used for pre-synaptic ranks. \"\"\" dim = len ( geometry ) txt = \"\" for d in range ( dim ): if txt == \"\" : # first coordinate is special txt = indices [ 0 ] + \"_\" + name else : txt = str ( geometry [ d ]) + '*(' + txt + ') + ' + indices [ d ] + '_' + name return txt ############################## ## Override useless methods ############################## def _data ( self ): \"Disable saving.\" desc = {} desc [ 'post_ranks' ] = self . post_ranks desc [ 'attributes' ] = self . attributes desc [ 'parameters' ] = self . parameters desc [ 'variables' ] = self . variables desc [ 'dendrites' ] = [] desc [ 'number_of_synapses' ] = 0 return desc def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be saved.' ) def save ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be saved.' ) def load ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be loaded.' ) def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Pooling projections can not display receptive fields.' ) def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Pooling projections can not display connectivity matrices.' ) __init__ ( pre , post , target , psp = 'pre.r' , operation = 'max' , name = None , copied = False ) # Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection required operation pooling function to be applied (\"max\", \"min\", \"mean\") 'max' Source code in ANNarchy/extensions/convolution/Pooling.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , pre , post , target , psp = \"pre.r\" , operation = \"max\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param operation: pooling function to be applied (\"max\", \"min\", \"mean\") \"\"\" # Sanity check #if not pre.neuron_type.type == 'rate': # Global._error('Pooling: only implemented for rate-coded populations.') # Sanity check if not operation in [ \"max\" , \"mean\" , \"min\" ]: Global . _error ( \"Pooling: the operation must be either 'max', 'mean' or 'min'.\" ) self . operation = operation # Store for _copy self . psp = psp Projection . __init__ ( self , pre , post , target , synapse = SharedSynapse ( psp = psp , operation = operation , name = \"Pooling operation\" , description = operation + \"-pooling operation over the pre-synaptic population.\" ), name = name , copied = copied ) # check dimensions of populations, should not exceed 4 self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : Global . _error ( 'Pooling: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : Global . _error ( 'Pooling: Too many dimensions for the pre-synaptic population (maximum 4).' ) # Disable saving self . _saveable = False connect_pooling ( extent = None , delays = 0.0 ) # Parameters: Name Type Description Default extent extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g (2, 2) ). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None. None delays synaptic delay in ms 0.0 Source code in ANNarchy/extensions/convolution/Pooling.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def connect_pooling ( self , extent = None , delays = 0.0 ): \"\"\" :param extent: extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g ``(2, 2)``). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None. :param delays: synaptic delay in ms \"\"\" # process extent self . extent_init = extent if extent is None : # compute the extent automatically if self . pre . dimension != self . post . dimension : Global . _error ( 'Pooling: If you do not provide the extent parameter, the two populations must have the same number of dimensions.' ) extent = list ( self . pre . geometry ) for dim in range ( self . pre . dimension ): extent [ dim ] /= self . post . geometry [ dim ] if self . pre . geometry [ dim ] != extent [ dim ] * self . post . geometry [ dim ]: Global . _error ( 'Pooling: Unable to compute the extent of the pooling area: the number of neurons do not match.' ) elif not isinstance ( extent , tuple ): Global . _error ( 'Pooling: You must provide a tuple for the extent of the pooling operation.' ) self . extent = list ( extent ) if len ( self . extent ) < self . pre . dimension : Global . _error ( 'Pooling: You must provide a tuple for the extent of the pooling operation.' ) # process delays self . delays = delays # Generate the pre-synaptic coordinates self . _generate_extent_coordinates () # create fake LIL self . _create () return self connectivity_matrix ( fill = 0.0 ) # Not available. Source code in ANNarchy/extensions/convolution/Pooling.py 634 635 636 def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Pooling projections can not display connectivity matrices.' ) load ( filename ) # Not available. Source code in ANNarchy/extensions/convolution/Pooling.py 628 629 630 def load ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be loaded.' ) receptive_fields ( variable = 'w' , in_post_geometry = True ) # Not available. Source code in ANNarchy/extensions/convolution/Pooling.py 631 632 633 def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Pooling projections can not display receptive fields.' ) save ( filename ) # Not available. Source code in ANNarchy/extensions/convolution/Pooling.py 625 626 627 def save ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be saved.' ) save_connectivity ( filename ) # Not available. Source code in ANNarchy/extensions/convolution/Pooling.py 622 623 624 def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be saved.' ) ANNarchy.extensions.convolution.Copy # Copy # Bases: Projection Creates a virtual projection reusing the weights and delays of an already-defined projection. Although the original projection can be learnable, this one can not. Changes in the original weights will be reflected in this projection. The only possible modifications are psp and operation . The pre- and post-synaptic populations of both projections must have the same geometry. Example: proj = Projection ( pop1 , pop2 , \"exc\" ) proj . connect_fixed_probability ( 0.1 , 0.5 ) copy_proj = Copy ( pop1 , pop3 , \"exc\" ) copy_proj . connect_copy ( proj ) Source code in ANNarchy/extensions/convolution/Copy.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 class Copy ( Projection ): \"\"\" Creates a virtual projection reusing the weights and delays of an already-defined projection. Although the original projection can be learnable, this one can not. Changes in the original weights will be reflected in this projection. The only possible modifications are ``psp`` and ``operation``. The pre- and post-synaptic populations of both projections must have the same geometry. Example: ```python proj = Projection(pop1, pop2, \"exc\") proj.connect_fixed_probability(0.1, 0.5) copy_proj = Copy(pop1, pop3, \"exc\") copy_proj.connect_copy(proj) ``` \"\"\" def __init__ ( self , pre , post , target , psp = \"pre.r * w\" , operation = \"sum\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum). \"\"\" # Create the description, but it will not be used for generation Projection . __init__ ( self , pre = pre , post = post , target = target , synapse = SharedSynapse ( psp = psp , operation = operation ), name = name , copied = copied ) def connect_copy ( self , projection ): \"\"\" :param projection: Existing projection to copy. \"\"\" self . projection = projection # Sanity checks if not isinstance ( self . projection , Projection ): Global . _error ( 'Copy: You must provide an existing projection to copy().' ) if isinstance ( self . projection , ( ConvolutionProjection , PoolingProjection )): Global . _error ( 'Copy: You can only copy regular projections, not shared projections.' ) if not self . pre . geometry == self . projection . pre . geometry or not self . post . geometry == self . projection . post . geometry : Global . _error ( 'Copy: When copying a projection, the geometries must be the same.' ) # Dummy weights self . weights = None self . pre_coordinates = [] # Finish building the synapses self . _create () return self def _copy ( self , pre , post ): \"Returns a copy of the projection when creating networks. Internal use only.\" raise NotImplementedError def _create ( self ): # create fake LIL object, just for compilation. try : from ANNarchy.core.cython_ext.Connector import LILConnectivity except Exception as e : Global . _print ( e ) Global . _error ( 'ANNarchy was not successfully installed.' ) lil = LILConnectivity () lil . max_delay = self . delays lil . uniform_delay = self . delays self . connector_name = \"Copy\" self . connector_description = \"Copy projection\" self . _store_connectivity ( self . _load_from_lil , ( lil , ), self . delays ) def _connect ( self , module ): \"\"\" Builds up dendrites either from list or dictionary. Called by instantiate(). \"\"\" if not self . _connection_method : Global . _error ( 'Copy: The projection between ' + self . pre . name + ' and ' + self . post . name + ' is declared but not connected.' ) # Create the Cython instance proj = getattr ( module , 'proj' + str ( self . id ) + '_wrapper' ) self . cyInstance = proj ( self . weights , self . pre_coordinates ) # Define the list of postsynaptic neurons self . post_ranks = list ( range ( self . post . size )) # Set delays after instantiation if self . delays > 0.0 : self . cyInstance . set_delay ( self . delays / Global . config [ 'dt' ]) return True def _generate ( self ): \"\"\" Overrides default code generation. This function is called during the code generation procedure. \"\"\" if Global . _check_paradigm ( \"openmp\" ): self . _generate_omp () elif Global . _check_paradigm ( \"cuda\" ): self . _generate_cuda () else : raise NotImplementedError def generate_omp ( self ): \"\"\" Code generation of CopyProjection object for the openMP paradigm. \"\"\" # Set the projection specific parameters copy_proj_dict = deepcopy ( copy_proj_template ) for key , value in copy_proj_dict . items (): value = value % { 'id_proj' : self . id , 'id_copy' : self . projection . id , 'float_prec' : Global . config [ 'precision' ] } copy_proj_dict [ key ] = value # Update specific template self . _specific_template . update ( copy_proj_dict ) # OMP code if more then one thread if Global . config [ 'num_threads' ] > 1 : omp_code = '#pragma omp for private(sum)' if self . post . size > Global . OMP_MIN_NB_NEURONS else '' else : omp_code = \"\" # PSP psp = self . synapse_type . description [ 'psp' ][ 'cpp' ] % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'local_index' : '[i][j]' , 'global_index' : '[i]' , 'pre_index' : '[pre_rank[i][j]]' , 'post_index' : '[post_rank[i]]' , 'pre_prefix' : 'pop' + str ( self . pre . id ) + '.' , 'post_prefix' : 'pop' + str ( self . post . id ) + '.' } psp = psp . replace ( 'rk_pre' , 'pre_rank[i][j]' ) . replace ( ';' , '' ) # Take delays into account if any if self . delays > Global . config [ 'dt' ]: psp = psp . replace ( 'pop %(id_pre)s .r[rk_pre]' % { 'id_pre' : self . pre . id }, 'pop %(id_pre)s ._delayed_r[delay-1][rk_pre]' % { 'id_pre' : self . pre . id } # TODO HD: wouldn't it be much better to reduce delay globaly, instead of the substraction here??? ) # Select template for operation to be performed: sum, max, min, mean try : sum_code = copy_sum_template [ self . synapse_type . operation ] except KeyError : Global . _error ( \"CopyProjection: the operation \" , self . synapse_type . operation , ' is not available.' ) # Finalize code self . generator [ 'omp' ][ 'body_compute_psp' ] = sum_code % { 'id_proj' : self . id , 'target' : self . target , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'id_post' : self . post . id , 'name_post' : self . post . name , 'id' : self . projection . id , 'float_prec' : Global . config [ 'precision' ], 'omp_code' : omp_code , 'psp' : psp } def _generate_cuda ( self ): \"\"\" Code generation of CopyProjection object for the CUDA paradigm. Note: currently not implemented (TODO HD) \"\"\" raise NotImplementedError ############################## ## Override useless methods ############################## def _data ( self ): \"Disable saving.\" desc = {} desc [ 'post_ranks' ] = self . post_ranks desc [ 'attributes' ] = self . attributes desc [ 'parameters' ] = self . parameters desc [ 'variables' ] = self . variables desc [ 'dendrites' ] = [] desc [ 'number_of_synapses' ] = 0 return desc def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be saved.' ) def save ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be saved.' ) def load ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be loaded.' ) def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Copied projections can not display receptive fields.' ) def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Copied projections can not display connectivity matrices.' ) __init__ ( pre , post , target , psp = 'pre.r * w' , operation = 'sum' , name = None , copied = False ) # Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection required psp continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r ). 'pre.r * w' operation operation (sum, max, min, mean) performed by the kernel (default: sum). 'sum' Source code in ANNarchy/extensions/convolution/Copy.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , pre , post , target , psp = \"pre.r * w\" , operation = \"sum\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum). \"\"\" # Create the description, but it will not be used for generation Projection . __init__ ( self , pre = pre , post = post , target = target , synapse = SharedSynapse ( psp = psp , operation = operation ), name = name , copied = copied ) connect_copy ( projection ) # Parameters: Name Type Description Default projection Existing projection to copy. required Source code in ANNarchy/extensions/convolution/Copy.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def connect_copy ( self , projection ): \"\"\" :param projection: Existing projection to copy. \"\"\" self . projection = projection # Sanity checks if not isinstance ( self . projection , Projection ): Global . _error ( 'Copy: You must provide an existing projection to copy().' ) if isinstance ( self . projection , ( ConvolutionProjection , PoolingProjection )): Global . _error ( 'Copy: You can only copy regular projections, not shared projections.' ) if not self . pre . geometry == self . projection . pre . geometry or not self . post . geometry == self . projection . post . geometry : Global . _error ( 'Copy: When copying a projection, the geometries must be the same.' ) # Dummy weights self . weights = None self . pre_coordinates = [] # Finish building the synapses self . _create () return self connectivity_matrix ( fill = 0.0 ) # Not available. Source code in ANNarchy/extensions/convolution/Copy.py 242 243 244 def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Copied projections can not display connectivity matrices.' ) generate_omp () # Code generation of CopyProjection object for the openMP paradigm. Source code in ANNarchy/extensions/convolution/Copy.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def generate_omp ( self ): \"\"\" Code generation of CopyProjection object for the openMP paradigm. \"\"\" # Set the projection specific parameters copy_proj_dict = deepcopy ( copy_proj_template ) for key , value in copy_proj_dict . items (): value = value % { 'id_proj' : self . id , 'id_copy' : self . projection . id , 'float_prec' : Global . config [ 'precision' ] } copy_proj_dict [ key ] = value # Update specific template self . _specific_template . update ( copy_proj_dict ) # OMP code if more then one thread if Global . config [ 'num_threads' ] > 1 : omp_code = '#pragma omp for private(sum)' if self . post . size > Global . OMP_MIN_NB_NEURONS else '' else : omp_code = \"\" # PSP psp = self . synapse_type . description [ 'psp' ][ 'cpp' ] % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'local_index' : '[i][j]' , 'global_index' : '[i]' , 'pre_index' : '[pre_rank[i][j]]' , 'post_index' : '[post_rank[i]]' , 'pre_prefix' : 'pop' + str ( self . pre . id ) + '.' , 'post_prefix' : 'pop' + str ( self . post . id ) + '.' } psp = psp . replace ( 'rk_pre' , 'pre_rank[i][j]' ) . replace ( ';' , '' ) # Take delays into account if any if self . delays > Global . config [ 'dt' ]: psp = psp . replace ( 'pop %(id_pre)s .r[rk_pre]' % { 'id_pre' : self . pre . id }, 'pop %(id_pre)s ._delayed_r[delay-1][rk_pre]' % { 'id_pre' : self . pre . id } # TODO HD: wouldn't it be much better to reduce delay globaly, instead of the substraction here??? ) # Select template for operation to be performed: sum, max, min, mean try : sum_code = copy_sum_template [ self . synapse_type . operation ] except KeyError : Global . _error ( \"CopyProjection: the operation \" , self . synapse_type . operation , ' is not available.' ) # Finalize code self . generator [ 'omp' ][ 'body_compute_psp' ] = sum_code % { 'id_proj' : self . id , 'target' : self . target , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'id_post' : self . post . id , 'name_post' : self . post . name , 'id' : self . projection . id , 'float_prec' : Global . config [ 'precision' ], 'omp_code' : omp_code , 'psp' : psp } load ( filename ) # Not available. Source code in ANNarchy/extensions/convolution/Copy.py 236 237 238 def load ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be loaded.' ) receptive_fields ( variable = 'w' , in_post_geometry = True ) # Not available. Source code in ANNarchy/extensions/convolution/Copy.py 239 240 241 def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Copied projections can not display receptive fields.' ) save ( filename ) # Not available. Source code in ANNarchy/extensions/convolution/Copy.py 233 234 235 def save ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be saved.' ) save_connectivity ( filename ) # Not available. Source code in ANNarchy/extensions/convolution/Copy.py 230 231 232 def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be saved.' )","title":"Convolution and Pooling"},{"location":"API/Convolution.html#convolution-and-pooling","text":"Convolution and pooling operations are provided in the module ANNarchy.extensions.convolution . They must be explicitely imported: from ANNarchy import * from ANNarchy.extensions.convolution import *","title":"Convolution and Pooling"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolution","text":"Bases: Projection Performs a convolution of a weight kernel on the pre-synaptic population. Despite its name, the operation performed is actually a cross-correlation, as is usual in computer vision and convolutional neural networks: \\[g(x) = \\sum_{k=-n}^n h(k) \\, f(x + k)\\] The convolution operation benefits from giving a multi-dimensional geometry to the populations and filters, for example in 2D: inp = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) pop = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Convolution ( inp , pop , 'exc' ) proj . connect_filter ( [ [ - 1. , 0. , 1. ], [ - 1. , 0. , 1. ], [ - 1. , 0. , 1. ] ]) The maximum number of dimensions for populations and filters is 4, an error is thrown otherwise. Depending on the number of dimensions of the pre- and post-synaptic populations, as well as of the kernel, the convolution is implemented differentely. Method connect_filter() If the pre- and post-populations have the same dimension as the kernel, the convolution is regular. Example: (100, 100) * (3, 3) -> (100, 100) If the post-population has one dimension less than the pre-synaptic one, the last dimension of the kernel must match the last one of the pre-synaptic population. Example: (100, 100, 3) * (3, 3, 3) -> (100, 100) If the kernel has less dimensions than the two populations, the number of neurons in the last dimension of the populations must be the same. The convolution will be calculated for each feature map in the last dimension. In this case, you must set keep_last_dimension to True . Example: (100, 100, 16) * (3, 3) -> (100, 100, 16) Method connect_filters() If the kernel has more dimensions than the pre-synaptic population, this means a bank of different filters will be applied on the pre-synaptic population (like a convolutional layer in a CNN). Attention: the first index of weights corresponds to the different filters, while the result will be accessible in the last dimension of the post-synaptic population. You must set the multiple argument to True. Example: (100, 100) * (16, 3, 3) -> (100, 100, 16) The convolution always uses padding for elements that would be outside the array (no equivalent of valid in tensorflow). It is 0.0 by default, but can be changed using the padding argument. Setting padding to the string border will repeat the value of the border elements. Sub-sampling will be automatically performed according to the populations' geometry. If these geometries do not match, an error will be thrown. Example: (100, 100) * (3, 3) -> (50, 50) You can redefine the sub-sampling by providing a list subsampling as argument, defining for each post-synaptic neuron the coordinates of the pre-synaptic neuron which will be the center of the filter/kernel. Source code in ANNarchy/extensions/convolution/Convolve.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 class Convolution ( Projection ): \"\"\" Performs a convolution of a weight kernel on the pre-synaptic population. Despite its name, the operation performed is actually a cross-correlation, as is usual in computer vision and convolutional neural networks: $$g(x) = \\sum_{k=-n}^n h(k) \\, f(x + k)$$ The convolution operation benefits from giving a multi-dimensional geometry to the populations and filters, for example in 2D: ```python inp = Population(geometry=(100, 100), neuron=Neuron(parameters=\"r = 0.0\")) pop = Population(geometry=(100, 100), neuron=Neuron(equations=\"r = sum(exc)\")) proj = Convolution(inp, pop, 'exc') proj.connect_filter( [ [-1., 0., 1.], [-1., 0., 1.], [-1., 0., 1.] ]) ``` The maximum number of dimensions for populations and filters is 4, an error is thrown otherwise. Depending on the number of dimensions of the pre- and post-synaptic populations, as well as of the kernel, the convolution is implemented differentely. **Method connect_filter()** * If the pre- and post-populations have the same dimension as the kernel, the convolution is regular. Example: (100, 100) * (3, 3) -> (100, 100) * If the post-population has one dimension less than the pre-synaptic one, the last dimension of the kernel must match the last one of the pre-synaptic population. Example: (100, 100, 3) * (3, 3, 3) -> (100, 100) * If the kernel has less dimensions than the two populations, the number of neurons in the last dimension of the populations must be the same. The convolution will be calculated for each feature map in the last dimension. In this case, you must set ``keep_last_dimension`` to ``True``. Example: (100, 100, 16) * (3, 3) -> (100, 100, 16) **Method connect_filters()** * If the kernel has more dimensions than the pre-synaptic population, this means a bank of different filters will be applied on the pre-synaptic population (like a convolutional layer in a CNN). Attention: the first index of ``weights`` corresponds to the different filters, while the result will be accessible in the last dimension of the post-synaptic population. You must set the ``multiple`` argument to True. Example: (100, 100) * (16, 3, 3) -> (100, 100, 16) The convolution **always** uses padding for elements that would be outside the array (no equivalent of ``valid`` in tensorflow). It is 0.0 by default, but can be changed using the ``padding`` argument. Setting ``padding`` to the string ``border`` will repeat the value of the border elements. Sub-sampling will be automatically performed according to the populations' geometry. If these geometries do not match, an error will be thrown. Example: (100, 100) * (3, 3) -> (50, 50) You can redefine the sub-sampling by providing a list ``subsampling`` as argument, defining for each post-synaptic neuron the coordinates of the pre-synaptic neuron which will be the center of the filter/kernel. \"\"\" def __init__ ( self , pre , post , target , psp = \"pre.r * w\" , operation = \"sum\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum). \"\"\" # Sanity check #if not pre.neuron_type.type == 'rate': # Global._error('Convolution: only implemented for rate-coded populations.') # Create the description, but it will not be used for generation Projection . __init__ ( self , pre , post , target , synapse = SharedSynapse ( psp = psp , operation = operation , name = \"Convolution operation\" , description = \"Convoluted kernel over the pre-synaptic population.\" ), name = name , copied = copied ) # Disable saving self . _saveable = False # For copy self . _used_single_filter = False self . _used_bank_of_filters = False self . operation = operation @property def weights ( self ): if not self . initialized : return self . init [ \"weights\" ] else : return self . cyInstance . get_w () @weights . setter def weights ( self , value ): if not self . initialized : self . init [ \"weights\" ] = value else : if self . dim_kernel != value . ndim : raise AttributeError ( \"Mismatch between filter dimensions\" ) self . cyInstance . set_w ( value ) def connect_filter ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ): \"\"\" Applies a single filter on the pre-synaptic population. :param weights: numpy array or list of lists representing the matrix of weights for the filter. :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons. :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. \"\"\" # Process the weights self . weights = np . array ( weights ) # Process the delays self . delays = float ( delays ) if not isinstance ( delays , ( int , float )): Global . _error ( 'Convolutions can only have constant delays.' ) self . subsampling = subsampling self . keep_last_dimension = keep_last_dimension self . padding = padding self . multiple = False # Check dimensions of populations and weight matrix self . dim_kernel = self . weights . ndim self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the pre-synaptic population (maximum 4).' ) if self . dim_kernel > 5 or ( not self . multiple and self . dim_kernel > 4 ): print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the kernel (maximum 4).' ) # Check if the last axes match for parallel convolution (e.g. 3-2-3) if self . dim_kernel < self . dim_pre : if not self . keep_last_dimension : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.' ) if self . pre . geometry [ - 1 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.' ) # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less. if self . dim_post < self . dim_pre : # OK, but check the last dimension of the kernel has the same size as the post-population if self . weights . shape [ - 1 ] != self . pre . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.' ) # Check if it is a bank of filters if self . dim_kernel > self . dim_pre : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has more dimensions than the pre-synaptic population, you need to use the connect_filters() method.' ) # Generate the pre-synaptic coordinates self . _generate_pre_coordinates () # Finish building the synapses self . _create () # For copy self . _used_single_filter = True return self def connect_filters ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ): \"\"\" Applies a set of different filters on the pre-synaptic population. The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters. :param weights: numpy array or list of lists representing the matrix of weights for the filter. :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons. :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. \"\"\" # Process the weights self . weights = np . array ( weights ) # Process the delays self . delays = float ( delays ) if not isinstance ( delays , ( int , float )): Global . _error ( 'Convolutions can only have constant delays.' ) self . subsampling = subsampling self . keep_last_dimension = keep_last_dimension self . padding = padding self . multiple = True # Check dimensions of populations and weight matrix self . dim_kernel = self . weights . ndim self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the pre-synaptic population (maximum 4).' ) if self . dim_kernel > 5 or ( not self . multiple and self . dim_kernel > 4 ): print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the kernel (maximum 4).' ) # Check if the last axes match for parallel convolution (e.g. 3-2-3) if self . dim_kernel < self . dim_pre : if not self . keep_last_dimension : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.' ) if self . pre . geometry [ - 1 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.' ) # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less. if self . dim_post < self . dim_pre : # OK, but check the last dimension of the kernel has the same size as the post-population if self . weights . shape [ - 1 ] != self . pre . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.' ) # The last dimension of the post population must correspond to the number of filters if self . weights . shape [ 0 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: For multiple filters, the last dimension of the post-synaptic population must have as many neurons as there are filters.' ) # Generate the pre-synaptic coordinates self . _generate_pre_coordinates_bank () # Finish building the synapses self . _create () # For copy self . _used_bank_of_filters = True return self def _copy ( self , pre , post ): \"Returns a copy of the projection when creating networks. Internal use only.\" copied_proj = Convolution ( pre = pre , post = post , target = self . target , psp = self . synapse_type . psp , operation = self . operation , name = self . name , copied = True ) copied_proj . delays = self . delays copied_proj . weights = self . weights copied_proj . subsampling = self . subsampling copied_proj . keep_last_dimension = self . keep_last_dimension copied_proj . padding = self . padding copied_proj . multiple = self . multiple copied_proj . dim_kernel = self . weights . ndim copied_proj . dim_pre = self . pre . dimension copied_proj . dim_post = self . post . dimension if self . _used_single_filter : copied_proj . _generate_pre_coordinates () elif self . _used_bank_of_filters : copied_proj . _generate_pre_coordinates_bank () else : raise ValueError ( \"Either use single filter or bank of filter must be True! (Missing connect?)\" ) copied_proj . _create () copied_proj . _connection_method = self . _connection_method copied_proj . _connection_args = self . _connection_args copied_proj . _connection_delay = self . _connection_delay copied_proj . _storage_format = self . _storage_format return copied_proj def _create ( self ): # create fake LIL object, just for compilation. try : from ANNarchy.core.cython_ext.Connector import LILConnectivity except Exception as e : Global . _print ( e ) Global . _error ( 'ANNarchy was not successfully installed.' ) lil = LILConnectivity () lil . max_delay = self . delays lil . uniform_delay = self . delays self . connector_name = \"Convolution\" self . connector_description = \"Convolution\" self . _store_connectivity ( self . _load_from_lil , ( lil , ), self . delays , storage_format = \"lil\" , storage_order = \"post_to_pre\" ) ################################ ### Create connection pattern ################################ def _connect ( self , module ): \"\"\" Builds up dendrites either from list or dictionary. Called by instantiate(). \"\"\" if not self . _connection_method : Global . _error ( 'Convolution: The projection between ' + self . pre . name + ' and ' + self . post . name + ' is declared but not connected.' ) # Create the Cython instance proj = getattr ( module , 'proj' + str ( self . id ) + '_wrapper' ) self . cyInstance = proj ( self . weights , self . pre_coordinates ) # Set delays after instantiation if self . delays > 0.0 : self . cyInstance . set_delay ( self . delays / Global . config [ 'dt' ]) return True def _generate_pre_coordinates ( self ): \" Returns a list for each post neuron of the corresponding center coordinates.\" # Check if the list is already defined: if self . subsampling : try : shape = np . array ( self . subsampling ) . shape except : Global . _error ( 'Convolution: The sub-sampling list must have' , self . post . size , 'elements of size' , self . pre . dimension ) return if shape != ( self . post . size , self . pre . dimension ): Global . _error ( 'Convolution: The sub-sampling list must have' , self . post . size , 'elements of size' , self . pre . dimension ) return self . pre_coordinates = self . subsampling return # Otherwise create it, possibly with sub-sampling coords = [[] for i in range ( self . post . size )] # Compute pre-indices idx_range = [] for dim in range ( self . dim_pre ): if dim < self . dim_post : pre_size = int ( self . pre . geometry [ dim ]) post_size = int ( self . post . geometry [ dim ]) sample = int ( pre_size / post_size ) if post_size * sample != pre_size : Global . _error ( 'Convolution: The pre-synaptic dimensions must be a multiple of the post-synaptic ones for down-sampling to work.' ) idx_range . append ([ int (( sample - 1 ) / 2 ) + sample * i for i in range ( post_size )]) else : # extra dimension if self . keep_last_dimension : idx_range . append ( range ( self . post . geometry [ dim ])) else : idx_range . append ([ self . _center_filter ( self . weights . shape [ dim ])]) # Generates coordinates TODO: Find a more robust way! if self . dim_pre == 1 : rk = 0 for i in idx_range [ 0 ]: coords [ rk ] = [ i ] rk += 1 elif self . dim_pre == 2 : rk = 0 for i in idx_range [ 0 ]: for j in idx_range [ 1 ]: coords [ rk ] = [ i , j ] rk += 1 elif self . dim_pre == 3 : rk = 0 for i in idx_range [ 0 ]: for j in idx_range [ 1 ]: for k in idx_range [ 2 ]: coords [ rk ] = [ i , j , k ] rk += 1 elif self . dim_pre == 4 : rk = 0 for i in idx_range [ 0 ]: for j in idx_range [ 1 ]: for k in idx_range [ 2 ]: for l in idx_range [ 3 ]: coords [ rk ] = [ i , j , k , l ] rk += 1 # Save the result self . pre_coordinates = coords def _generate_pre_coordinates_bank ( self ): \" Returns a list for each post neuron of the corresponding center coordinates, when the filter is a bank.\" self . nb_filters = self . weights . shape [ 0 ] self . dim_single_filter = self . weights . shape [ 1 :] # Check if the list is already defined: if self . subsampling : try : shape = np . array ( self . subsampling ) . shape except : Global . _error ( 'Convolution: The sub-sampling list must have' , self . post . size / self . post . geometry [ - 1 ], 'elements of size' , self . pre . dimension ) return if shape != ( self . post . size / self . post . geometry [ - 1 ], self . pre . dimension ): Global . _error ( 'Convolution: The sub-sampling list must have' , self . post . size / self . post . geometry [ - 1 ], 'elements of size' , self . pre . dimension ) return self . pre_coordinates = [ c + [ d ] for c in self . subsampling for d in range ( self . nb_filters )] return # Otherwise create it, possibly with sub-sampling coords = [[] for i in range ( self . post . size )] # Compute pre-indices idx_range = [] for dim in range ( self . dim_pre ): if dim < self . dim_post - 1 : pre_size = self . pre . geometry [ dim ] post_size = self . post . geometry [ dim ] sample = int ( pre_size / post_size ) if post_size * sample != pre_size : Global . _error ( 'Convolution: The pre-synaptic dimensions must be a multiple of the post-synaptic ones for down-sampling to work.' ) idx_range . append ([ int (( sample - 1 ) / 2 ) + sample * i for i in range ( post_size )]) else : # extra dimension if self . keep_last_dimension : idx_range . append ( range ( self . post . geometry [ dim ])) else : idx_range . append ([ self . _center_filter ( self . weights . shape [ dim + 1 ])]) # Generates coordinates TODO: Find a more robust way! if self . dim_pre == 1 : rk = 0 for i in idx_range [ 0 ]: for d in range ( self . nb_filters ): coords [ rk ] = [ i , d ] rk += 1 elif self . dim_pre == 2 : rk = 0 for i in idx_range [ 0 ]: for j in idx_range [ 1 ]: for d in range ( self . nb_filters ): coords [ rk ] = [ i , j , d ] rk += 1 elif self . dim_pre == 3 : rk = 0 for i in idx_range [ 0 ]: for j in idx_range [ 1 ]: for k in idx_range [ 2 ]: for d in range ( self . nb_filters ): coords [ rk ] = [ i , j , k , d ] rk += 1 elif self . dim_pre == 4 : rk = 0 for i in idx_range [ 0 ]: for j in idx_range [ 1 ]: for k in idx_range [ 2 ]: for l in idx_range [ 3 ]: for d in range ( self . nb_filters ): coords [ rk ] = [ i , j , k , l , d ] rk += 1 # Save the result self . pre_coordinates = coords ################################ # Code generation ################################ def _generate ( self ): \"\"\" Overrides default code generation. This function is called during the code generation procedure. \"\"\" # Filter definition filter_definition , filter_pyx_definition = self . _filter_definition () # Convolve_code if not self . multiple : convolve_code , sum_code = self . _generate_convolve_code () else : convolve_code , sum_code = self . _generate_bank_code () if Global . _check_paradigm ( \"openmp\" ): self . _generate_omp ( filter_definition , filter_pyx_definition , convolve_code , sum_code ) elif Global . _check_paradigm ( \"cuda\" ): raise Global . ANNarchyException ( \"Convolution is not available on CUDA devices yet.\" , True ) else : raise NotImplementedError def _generate_omp ( self , filter_definition , filter_pyx_definition , convolve_code , sum_code , kernel = True ): \"\"\" OpenMP code generation. \"\"\" # Basic ids base_ids = { 'id_proj' : self . id , 'size_post' : self . post . size , 'float_prec' : Global . config [ 'precision' ] } # Fill the basic definitions conv_dict = deepcopy ( convole_template_omp ) for key , value in conv_dict . items (): value = value % base_ids conv_dict [ key ] = value self . _specific_template . update ( conv_dict ) # Kernel-based method: specify w with the correct dimension if kernel : self . _specific_template [ 'declare_parameters_variables' ] = tabify ( filter_definition . strip (), 1 ) self . _specific_template [ 'export_parameters_variables' ] = \"\" self . _specific_template [ 'access_parameters_variables' ] = \"\"\" // Local parameter w %(type_w)s get_w() { return w; } void set_w( %(type_w)s value) { w = value; } \"\"\" % { 'type_w' : filter_definition . replace ( ' w;' , '' )} self . _specific_template [ 'export_connectivity' ] += \"\"\" # Local variable w %(type_w)s get_w() void set_w( %(type_w)s ) \"\"\" % { 'type_w' : filter_pyx_definition . replace ( ' w' , '' )} self . _specific_template [ 'wrapper_init_connectivity' ] += \"\"\" proj %(id_proj)s .set_w(weights) \"\"\" % { 'id_proj' : self . id } self . _specific_template [ 'wrapper_access_connectivity' ] += \"\"\" # Local variable w def get_w(self): return proj %(id_proj)s .get_w() def set_w(self, value): proj %(id_proj)s .set_w( value ) def get_dendrite_w(self, int rank): return proj %(id_proj)s .get_w() def set_dendrite_w(self, int rank, value): proj %(id_proj)s .set_w(value) def get_synapse_w(self, int rank_post, int rank_pre): return 0.0 def set_synapse_w(self, int rank_post, int rank_pre, %(float_prec)s value): pass \"\"\" % { 'id_proj' : self . id , 'float_prec' : Global . config [ 'precision' ]} # Override the monitor to avoid recording the weights self . _specific_template [ 'monitor_class' ] = \"\" self . _specific_template [ 'monitor_export' ] = \"\" self . _specific_template [ 'monitor_wrapper' ] = \"\" # OMP code omp_code = \"\" if Global . config [ 'num_threads' ] > 1 : omp_code = \"\"\" #pragma omp for private(sum, rk_pre, coord) %(psp_schedule)s \"\"\" % { 'psp_schedule' : \"\" if not 'psp_schedule' in self . _omp_config . keys () else self . _omp_config [ 'psp_schedule' ]} # HD ( 16.10.2015 ): # pre-load delayed firing rate in a local array, so we # prevent multiple accesses to pop%(id_pre)s._delayed_r[delay-1] # wheareas delay is set available as variable # TODO HD: wouldn't it be much better to reduce delay globaly, instead of the substraction here??? if self . delays > Global . config [ 'dt' ]: pre_load_r = \"\"\" // pre-load delayed firing rate auto delayed_r = pop %(id_pre)s ._delayed_r[delay-1]; \"\"\" % { 'id_pre' : self . pre . id } else : pre_load_r = \"\" # Target variable depends on neuron type target_code = \"_sum_ %(target)s \" if self . post . neuron_type . type == \"rate\" else \"g_ %(target)s \" target_code %= { 'target' : self . target } # Compute sum wsum = \"\"\" if ( _transmission && pop %(id_pre)s ._active ) { int* coord; \"\"\" + pre_load_r + \"\"\" %(omp_code)s for(int i = 0; i < %(size_post)s ; i++){ coord = pre_coords[i].data(); // perform the convolution \"\"\" + tabify ( convolve_code , 1 ) + \"\"\" // store result pop %(id_post)s . %(target)s [i] += \"\"\" + sum_code + \"\"\"; } // for } // if \"\"\" self . _specific_template [ 'psp_code' ] = wsum % \\ { 'id_proj' : self . id , 'target' : target_code , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'size_pre' : self . pre . size , 'id_post' : self . post . id , 'name_post' : self . post . name , 'size_post' : self . post . size , 'omp_code' : omp_code , 'convolve_code' : convolve_code } self . _specific_template [ 'size_in_bytes' ] = \"\"\" // post-ranks size_in_bytes += sizeof(std::vector<int>); size_in_bytes += post_rank.capacity() * sizeof(int); // pre-coords size_in_bytes += sizeof(std::vector<std::vector<int>>); size_in_bytes += pre_coords.capacity() * sizeof(std::vector<int>); for (auto it = pre_coords.begin(); it != pre_coords.end(); it++) { size_in_bytes += it->capacity() * sizeof(int); } // filter // TODO: \"\"\" self . _specific_template [ 'clear' ] = \"\"\" // post-ranks post_rank.clear(); post_rank.shrink_to_fit(); // pre-coords for (auto it = pre_coords.begin(); it != pre_coords.end(); it++) { it->clear(); it->shrink_to_fit(); } pre_coords.clear(); pre_coords.shrink_to_fit(); // filter // TODO: \"\"\" ################################ ### Utilities ################################ def _center_filter ( self , i ): return int ( i / 2 ) if i % 2 == 1 else int ( i / 2 ) - 1 def _filter_definition ( self ): dim = self . dim_kernel cpp = Global . config [ 'precision' ] pyx = Global . config [ 'precision' ] for d in range ( dim ): cpp = 'std::vector< ' + cpp + ' >' pyx = 'vector[' + pyx + ']' cpp += ' w;' pyx += ' w' return cpp , pyx def _coordinates_to_rank ( self , name , geometry ): dim = len ( geometry ) txt = \"\" for d in range ( dim ): if txt == \"\" : # first coordinate is special txt = indices [ 0 ] + \"_\" + name else : txt = str ( geometry [ d ]) + '*(' + txt + ') + ' + indices [ d ] + '_' + name return txt def _generate_convolve_code ( self ): # Operation to be performed: sum, max, min, mean operation = self . synapse_type . operation # Main code code = tabify ( \"sum = 0.0; \\n \" , 3 ) # Generate for loops for dim in range ( self . dim_kernel ): if dim == self . dim_kernel - 1 : inner_idx = \"\" for i in range ( self . dim_kernel - 1 ): inner_idx += \"[\" + indices [ i ] + \"_w]\" code += \"auto inner_line = w\" + inner_idx + \".data(); \\n \" code += tabify ( \"\"\" for(int %(index)s _w = 0; %(index)s _w < %(size)s ; %(index)s _w++) { \"\"\" % { 'index' : indices [ dim ], 'size' : self . weights . shape [ dim ]}, dim ) # Compute indices if dim < self . dim_kernel : code += tabify ( \"\"\"int %(index)s_pre = coord[%(dim)s] %(operator)s (%(index)s_w - %(center)s);\"\"\" % { 'id_proj' : self . id , 'index' : indices [ dim ], 'dim' : dim , 'operator' : '+' , 'center' : self . _center_filter ( self . weights . shape [ dim ]) }, 1 ) else : code += tabify ( \"\"\"int %(index)s_pre = coord[%(dim)s];\"\"\" % { 'id_proj' : self . id , 'index' : indices [ dim ], 'dim' : dim }, 1 ) # Check indices if operation in [ 'sum' , 'mean' ]: if isinstance ( self . padding , str ): # 'border' code += tabify ( \"\"\" if ( %(index)s _pre < 0) %(index)s _pre = 0 ; if ( %(index)s _pre > %(max_size)s ) %(index)s _pre = %(max_size)s ; \"\"\" % { 'index' : indices [ dim ], 'dim' : dim , 'max_size' : self . pre . geometry [ dim ] - 1 }, dim ) else : code += tabify ( \"\"\" if (( %(index)s _pre < 0) || ( %(index)s _pre > %(max_size)s )){ sum += %(padding)s ; continue; } \"\"\" % { 'index' : indices [ dim ], 'padding' : self . padding , 'max_size' : self . pre . geometry [ dim ] - 1 }, dim ) else : # min, max code += \"\"\" if (( %(index)s _pre < 0) || ( %(index)s _pre > %(max_size)s )) { continue; } \"\"\" % { 'index' : indices [ dim ], 'max_size' : self . pre . geometry [ dim ] - 1 } # if True, we need to take the last dimension from coords if self . keep_last_dimension : id_dict = { 'index' : indices [ self . dim_kernel ], 'dim' : self . dim_kernel } code += \"int %(index)s _pre = coord[ %(dim)s ];\" % id_dict # Compute pre-synaptic rank code += tabify ( \"\"\" rk_pre = %(value)s ;\"\"\" % { 'value' : self . _coordinates_to_rank ( 'pre' , self . pre . geometry )}, dim ) # Compute the increment index = \"\" for dim in range ( self . dim_kernel ): index += '[' + indices [ dim ] + '_w]' increment = self . synapse_type . description [ 'psp' ][ 'cpp' ] % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'local_index' : index , 'global_index' : '[i]' , 'pre_index' : '[rk_pre]' , 'post_index' : '[rk_post]' , 'pre_prefix' : 'pop' + str ( self . pre . id ) + '.' , 'post_prefix' : 'pop' + str ( self . post . id ) + '.' } # Delays if self . delays > Global . config [ 'dt' ]: increment = increment . replace ( 'pop %(id_pre)s .r[rk_pre]' % { 'id_pre' : self . pre . id }, 'delayed_r[rk_pre]' ) # Apply the operation if operation == \"sum\" : if self . dim_kernel == 1 : code += tabify ( \"\"\" sum += %(increment)s \"\"\" % { 'increment' : increment }, dim ) else : code += tabify ( \"\"\" sum += %(increment)s \"\"\" % { 'increment' : increment . replace ( 'w' + inner_idx , 'inner_line' )}, dim ) elif operation == \"max\" : code += tabify ( \"\"\" %(float_prec)s _psp = %(increment)s if(_psp > sum) sum = _psp;\"\"\" % { 'increment' : increment , 'float_prec' : Global . config [ 'precision' ]}, dim ) elif operation == \"min\" : code += tabify ( \"\"\" %(float_prec)s _psp = %(increment)s if(_psp < sum) sum = _psp;\"\"\" % { 'increment' : increment , 'float_prec' : Global . config [ 'precision' ]}, dim ) elif operation == \"mean\" : code += tabify ( \"\"\" sum += %(increment)s \"\"\" % { 'increment' : increment }, dim ) else : Global . _error ( 'Convolution: Operation' , operation , 'is not implemented yet for shared projections.' ) # Close for loops for dim in range ( self . dim_kernel ): code += tabify ( \"\"\" }\"\"\" , self . dim_kernel - 1 - dim ) impl_code = code % { 'id_proj' : self . id , 'target' : self . target , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'size_pre' : self . pre . size , 'id_post' : self . post . id , 'name_post' : self . post . name , 'size_post' : self . post . size } # sum code self . weights . size if operation == \"mean\" : sum_code = \"\"\"sum/ %(filter_size)s \"\"\" % { 'filter_size' : self . weights . size } else : sum_code = \"sum\" return impl_code , sum_code def _generate_bank_code ( self ): # Operation to be performed: sum, max, min, mean operation = self . synapse_type . operation # Main code code = tabify ( \"sum = 0.0; \\n \" , 3 ) # Generate for loops for dim in range ( self . dim_kernel - 1 ): code += tabify ( \"\"\" for(int %(index)s _w = 0; %(index)s _w < %(size)s ; %(index)s _w++) { \"\"\" % { 'index' : indices [ dim ], 'size' : self . weights . shape [ dim + 1 ]}, dim ) # Compute indices if dim < self . dim_kernel : code += tabify ( \"\"\"int %(index)s_pre = coord[%(dim)s] %(operator)s (%(index)s_w - %(center)s);\"\"\" % { 'id_proj' : self . id , 'index' : indices [ dim ], 'dim' : dim , 'operator' : '+' , 'center' : self . _center_filter ( self . weights . shape [ dim + 1 ]) }, 1 ) else : code += tabify ( \"\"\"int %(index)s_pre = coord[%(dim)s];\"\"\" % { 'id_proj' : self . id , 'index' : indices [ dim ], 'dim' : dim }, 1 ) # Check indices if operation in [ 'sum' , 'mean' ]: if isinstance ( self . padding , str ): # 'border' code += tabify ( \"\"\" if ( %(index)s _pre < 0) %(index)s _pre = 0 ; if ( %(index)s _pre > %(max_size)s ) %(index)s _pre = %(max_size)s ; \"\"\" % { 'index' : indices [ dim ], 'dim' : dim , 'max_size' : self . pre . geometry [ dim ] - 1 }, 1 + dim ) else : code += tabify ( \"\"\" if (( %(index)s _pre < 0) || ( %(index)s _pre > %(max_size)s )) { sum += %(padding)s ; continue; } \"\"\" % { 'index' : indices [ dim ], 'padding' : self . padding , 'max_size' : self . pre . geometry [ dim ] - 1 }, 1 + dim ) else : # min, max code += tabify ( \"\"\" if (( %(index)s _pre < 0) || ( %(index)s _pre > %(max_size)s )){ continue; } \"\"\" % { 'index' : indices [ dim ], 'max_size' : self . pre . geometry [ dim ] - 1 }, 1 + dim ) # Compute pre-synaptic rank code += tabify ( \"\"\" rk_pre = %(value)s ;\"\"\" % { 'value' : self . _coordinates_to_rank ( 'pre' , self . pre . geometry )}, 1 + dim ) # Compute the increment index = \"[coord[\" + str ( self . dim_pre ) + \"]]\" for dim in range ( self . dim_kernel - 1 ): index += '[' + indices [ dim ] + '_w]' increment = self . synapse_type . description [ 'psp' ][ 'cpp' ] % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'local_index' : index , 'global_index' : '[i]' , 'pre_index' : '[rk_pre]' , 'post_index' : '[rk_post]' , 'pre_prefix' : 'pop' + str ( self . pre . id ) + '.' , 'post_prefix' : 'pop' + str ( self . post . id ) + '.' } # Delays if self . delays > Global . config [ 'dt' ]: increment = increment . replace ( 'pop %(id_pre)s .r[rk_pre]' % { 'id_pre' : self . pre . id }, 'delayed_r[rk_pre]' ) # Apply the operation if operation == \"sum\" : code += tabify ( \"\"\" sum += %(increment)s \"\"\" % { 'increment' : increment }, 1 + dim ) elif operation == \"max\" : code += tabify ( \"\"\" %(float_prec)s _psp = %(increment)s if(_psp > sum) sum = _psp;\"\"\" % { 'increment' : increment , 'float_prec' : Global . config [ 'precision' ]}, 1 + dim ) elif operation == \"min\" : code += tabify ( \"\"\" %(float_prec)s _psp = %(increment)s if(_psp < sum) sum = _psp;\"\"\" % { 'increment' : increment , 'float_prec' : Global . config [ 'precision' ]}, 1 + dim ) elif operation == \"mean\" : code += tabify ( \"\"\" sum += %(increment)s \"\"\" % { 'increment' : increment }, 1 + dim ) else : Global . _error ( 'SharedProjection: Operation' , operation , 'is not implemented yet for shared projections.' ) # Close for loops for dim in range ( self . dim_kernel - 1 ): code += tabify ( \"\"\" }\"\"\" , self . dim_kernel - 1 - dim ) impl_code = code % { 'id_proj' : self . id , 'target' : self . target , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'size_pre' : self . pre . size , 'id_post' : self . post . id , 'name_post' : self . post . name , 'size_post' : self . post . size } # sum code if operation == \"mean\" : sum_code = \"\"\"sum/ %(filter_size)s \"\"\" % { 'filter_size' : self . weights . size } else : sum_code = \"sum\" return impl_code , sum_code ############################## ## Override useless methods ############################## def _data ( self ): \"Disable saving.\" desc = {} desc [ 'post_ranks' ] = self . post_ranks desc [ 'attributes' ] = self . attributes desc [ 'parameters' ] = self . parameters desc [ 'variables' ] = self . variables desc [ 'dendrites' ] = [] desc [ 'number_of_synapses' ] = 0 return desc def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be saved.' ) def save ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be saved.' ) def load ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be loaded.' ) def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Convolutional projections can not display receptive fields.' ) def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Convolutional projections can not display connectivity matrices.' )","title":"Convolution"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.__init__","text":"Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection required psp continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r ). 'pre.r * w' operation operation (sum, max, min, mean) performed by the kernel (default: sum). 'sum' Source code in ANNarchy/extensions/convolution/Convolve.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , pre , post , target , psp = \"pre.r * w\" , operation = \"sum\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum). \"\"\" # Sanity check #if not pre.neuron_type.type == 'rate': # Global._error('Convolution: only implemented for rate-coded populations.') # Create the description, but it will not be used for generation Projection . __init__ ( self , pre , post , target , synapse = SharedSynapse ( psp = psp , operation = operation , name = \"Convolution operation\" , description = \"Convoluted kernel over the pre-synaptic population.\" ), name = name , copied = copied ) # Disable saving self . _saveable = False # For copy self . _used_single_filter = False self . _used_bank_of_filters = False self . operation = operation","title":"__init__()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.connect_filter","text":"Applies a single filter on the pre-synaptic population. Parameters: Name Type Description Default weights numpy array or list of lists representing the matrix of weights for the filter. required delays delay in synaptic transmission (default: dt). Can only be the same value for all neurons. 0.0 keep_last_dimension defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. False padding value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. 0.0 subsampling list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. None Source code in ANNarchy/extensions/convolution/Convolve.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def connect_filter ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ): \"\"\" Applies a single filter on the pre-synaptic population. :param weights: numpy array or list of lists representing the matrix of weights for the filter. :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons. :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. \"\"\" # Process the weights self . weights = np . array ( weights ) # Process the delays self . delays = float ( delays ) if not isinstance ( delays , ( int , float )): Global . _error ( 'Convolutions can only have constant delays.' ) self . subsampling = subsampling self . keep_last_dimension = keep_last_dimension self . padding = padding self . multiple = False # Check dimensions of populations and weight matrix self . dim_kernel = self . weights . ndim self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the pre-synaptic population (maximum 4).' ) if self . dim_kernel > 5 or ( not self . multiple and self . dim_kernel > 4 ): print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the kernel (maximum 4).' ) # Check if the last axes match for parallel convolution (e.g. 3-2-3) if self . dim_kernel < self . dim_pre : if not self . keep_last_dimension : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.' ) if self . pre . geometry [ - 1 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.' ) # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less. if self . dim_post < self . dim_pre : # OK, but check the last dimension of the kernel has the same size as the post-population if self . weights . shape [ - 1 ] != self . pre . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.' ) # Check if it is a bank of filters if self . dim_kernel > self . dim_pre : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has more dimensions than the pre-synaptic population, you need to use the connect_filters() method.' ) # Generate the pre-synaptic coordinates self . _generate_pre_coordinates () # Finish building the synapses self . _create () # For copy self . _used_single_filter = True return self","title":"connect_filter()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.connect_filters","text":"Applies a set of different filters on the pre-synaptic population. The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters. Parameters: Name Type Description Default weights numpy array or list of lists representing the matrix of weights for the filter. required delays delay in synaptic transmission (default: dt). Can only be the same value for all neurons. 0.0 keep_last_dimension defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. False padding value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. 0.0 subsampling list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. None Source code in ANNarchy/extensions/convolution/Convolve.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def connect_filters ( self , weights , delays = 0.0 , keep_last_dimension = False , padding = 0.0 , subsampling = None ): \"\"\" Applies a set of different filters on the pre-synaptic population. The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters. :param weights: numpy array or list of lists representing the matrix of weights for the filter. :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons. :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False. :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0. :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None. \"\"\" # Process the weights self . weights = np . array ( weights ) # Process the delays self . delays = float ( delays ) if not isinstance ( delays , ( int , float )): Global . _error ( 'Convolutions can only have constant delays.' ) self . subsampling = subsampling self . keep_last_dimension = keep_last_dimension self . padding = padding self . multiple = True # Check dimensions of populations and weight matrix self . dim_kernel = self . weights . ndim self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the pre-synaptic population (maximum 4).' ) if self . dim_kernel > 5 or ( not self . multiple and self . dim_kernel > 4 ): print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: Too many dimensions for the kernel (maximum 4).' ) # Check if the last axes match for parallel convolution (e.g. 3-2-3) if self . dim_kernel < self . dim_pre : if not self . keep_last_dimension : print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.' ) if self . pre . geometry [ - 1 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.' ) # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less. if self . dim_post < self . dim_pre : # OK, but check the last dimension of the kernel has the same size as the post-population if self . weights . shape [ - 1 ] != self . pre . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.' ) # The last dimension of the post population must correspond to the number of filters if self . weights . shape [ 0 ] != self . post . geometry [ - 1 ]: print ( \"Convolution:\" , self . dim_pre , '*' , self . dim_kernel , '->' , self . dim_post ) Global . _error ( 'Convolution: For multiple filters, the last dimension of the post-synaptic population must have as many neurons as there are filters.' ) # Generate the pre-synaptic coordinates self . _generate_pre_coordinates_bank () # Finish building the synapses self . _create () # For copy self . _used_bank_of_filters = True return self","title":"connect_filters()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.connectivity_matrix","text":"Not available. Source code in ANNarchy/extensions/convolution/Convolve.py 985 986 987 def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Convolutional projections can not display connectivity matrices.' )","title":"connectivity_matrix()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.load","text":"Not available. Source code in ANNarchy/extensions/convolution/Convolve.py 979 980 981 def load ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be loaded.' )","title":"load()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.receptive_fields","text":"Not available. Source code in ANNarchy/extensions/convolution/Convolve.py 982 983 984 def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Convolutional projections can not display receptive fields.' )","title":"receptive_fields()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.save","text":"Not available. Source code in ANNarchy/extensions/convolution/Convolve.py 976 977 978 def save ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be saved.' )","title":"save()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.save_connectivity","text":"Not available. Source code in ANNarchy/extensions/convolution/Convolve.py 973 974 975 def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Convolutional projections can not be saved.' )","title":"save_connectivity()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling","text":"","title":"Pooling"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling","text":"Bases: Projection Performs a pooling operation (e.g. max.pooling) on the pre-synaptic population. Each post-synaptic neuron covers a specific region ( extent ) of the pre-synaptic population, over which the result of the operation on firing rates will be assigned to sum(target). The extent is automatically computed using the geometry of the populations, but can be specified in the `connect_pooling()`` methods. Example: inp = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) pop = Population ( geometry = ( 50 , 50 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Pooling ( inp , pop , 'exc' , operation = 'max' ) # max-pooling proj . connect_pooling () # extent=(2, 2) is implicit Source code in ANNarchy/extensions/convolution/Pooling.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 class Pooling ( Projection ): \"\"\" Performs a pooling operation (e.g. max.pooling) on the pre-synaptic population. Each post-synaptic neuron covers a specific region (``extent``) of the pre-synaptic population, over which the result of the operation on firing rates will be assigned to sum(target). The extent is automatically computed using the geometry of the populations, but can be specified in the `connect_pooling()`` methods. Example: ```python inp = Population(geometry=(100, 100), neuron=Neuron(parameters=\"r = 0.0\")) pop = Population(geometry=(50, 50), neuron=Neuron(equations=\"r = sum(exc)\")) proj = Pooling(inp, pop, 'exc', operation='max') # max-pooling proj.connect_pooling() # extent=(2, 2) is implicit ``` \"\"\" def __init__ ( self , pre , post , target , psp = \"pre.r\" , operation = \"max\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param operation: pooling function to be applied (\"max\", \"min\", \"mean\") \"\"\" # Sanity check #if not pre.neuron_type.type == 'rate': # Global._error('Pooling: only implemented for rate-coded populations.') # Sanity check if not operation in [ \"max\" , \"mean\" , \"min\" ]: Global . _error ( \"Pooling: the operation must be either 'max', 'mean' or 'min'.\" ) self . operation = operation # Store for _copy self . psp = psp Projection . __init__ ( self , pre , post , target , synapse = SharedSynapse ( psp = psp , operation = operation , name = \"Pooling operation\" , description = operation + \"-pooling operation over the pre-synaptic population.\" ), name = name , copied = copied ) # check dimensions of populations, should not exceed 4 self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : Global . _error ( 'Pooling: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : Global . _error ( 'Pooling: Too many dimensions for the pre-synaptic population (maximum 4).' ) # Disable saving self . _saveable = False def connect_pooling ( self , extent = None , delays = 0.0 ): \"\"\" :param extent: extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g ``(2, 2)``). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None. :param delays: synaptic delay in ms \"\"\" # process extent self . extent_init = extent if extent is None : # compute the extent automatically if self . pre . dimension != self . post . dimension : Global . _error ( 'Pooling: If you do not provide the extent parameter, the two populations must have the same number of dimensions.' ) extent = list ( self . pre . geometry ) for dim in range ( self . pre . dimension ): extent [ dim ] /= self . post . geometry [ dim ] if self . pre . geometry [ dim ] != extent [ dim ] * self . post . geometry [ dim ]: Global . _error ( 'Pooling: Unable to compute the extent of the pooling area: the number of neurons do not match.' ) elif not isinstance ( extent , tuple ): Global . _error ( 'Pooling: You must provide a tuple for the extent of the pooling operation.' ) self . extent = list ( extent ) if len ( self . extent ) < self . pre . dimension : Global . _error ( 'Pooling: You must provide a tuple for the extent of the pooling operation.' ) # process delays self . delays = delays # Generate the pre-synaptic coordinates self . _generate_extent_coordinates () # create fake LIL self . _create () return self def _copy ( self , pre , post ): \"Returns a copy of the projection when creating networks. Internal use only.\" copied_proj = Pooling ( pre = pre , post = post , target = self . target , psp = self . psp , operation = self . operation , name = self . name , copied = True ) copied_proj . extent = self . extent copied_proj . delays = self . delays copied_proj . _generate_extent_coordinates () copied_proj . _create () copied_proj . _connection_method = self . _connection_method copied_proj . _connection_args = self . _connection_args copied_proj . _connection_delay = self . _connection_delay copied_proj . _storage_format = self . _storage_format return copied_proj def _create ( self ): \"\"\" create fake LIL object, just for compilation process :return: no return value \"\"\" try : from ANNarchy.core.cython_ext.Connector import LILConnectivity except Exception as e : Global . _print ( e ) Global . _error ( 'ANNarchy was not successfully installed.' ) lil = LILConnectivity () lil . max_delay = self . delays lil . uniform_delay = self . delays self . connector_name = \"Pooling\" self . connector_description = \"Pooling\" self . _store_connectivity ( self . _load_from_lil , ( lil , ), self . delays , storage_format = \"lil\" , storage_order = \"post_to_pre\" ) def _connect ( self , module ): \"\"\" Builds up dendrites either from list or dictionary. Called by instantiate(). \"\"\" if not self . _connection_method : Global . _error ( 'Pooling: The projection between ' + self . pre . name + ' and ' + self . post . name + ' is declared but not connected.' ) # Create the Cython instance proj = getattr ( module , 'proj' + str ( self . id ) + '_wrapper' ) self . cyInstance = proj ([], self . pre_coordinates ) return True def _generate_extent_coordinates ( self ): \"\"\" Generates for each post-neuron the position of the top-left corner, where the pooling should be applied. :return: a list for each post neuron of the corresponding top-left coordinates \"\"\" # Generates coordinates TODO: Find a more robust way! coords = [[] for i in range ( self . post . size )] if self . dim_pre == 1 : rk = 0 for i in range ( self . post . geometry [ 0 ]): coords [ rk ] = [ i * self . extent [ 0 ]] rk += 1 elif self . dim_pre == 2 : rk = 0 for i in range ( self . post . geometry [ 0 ]): if self . dim_post > 1 : for j in range ( self . post . geometry [ 1 ]): coords [ rk ] = [ i * self . extent [ 0 ], j * self . extent [ 1 ]] rk += 1 else : # over the whole second axis coords [ rk ] = [ i * self . extent [ 0 ], 0 ] rk += 1 elif self . dim_pre == 3 : rk = 0 for i in range ( self . post . geometry [ 0 ]): for j in range ( self . post . geometry [ 1 ]): if self . dim_post > 2 : for k in range ( self . post . geometry [ 2 ]): coords [ rk ] = [ i * self . extent [ 0 ], j * self . extent [ 1 ], k * self . extent [ 2 ]] rk += 1 else : # over the whole third axis coords [ rk ] = [ i * self . extent [ 0 ], j * self . extent [ 1 ], 0 ] rk += 1 elif self . dim_pre == 4 : # TODO: post has less than 4 dimensions rk = 0 for i in range ( self . post . geometry [ 0 ]): for j in range ( self . post . geometry [ 1 ]): for k in range ( self . post . geometry [ 2 ]): for l in range ( self . post . geometry [ 3 ]): coords [ rk ] = [ i * self . extent [ 0 ], j * self . extent [ 1 ], k * self . extent [ 2 ], l * self . extent [ 3 ]] rk += 1 # Save the result self . pre_coordinates = coords def _generate ( self ): \"\"\" Overrides the default code generation. \"\"\" # Convolve_code convolve_code , sum_code = self . _generate_pooling_code () # Generate the code if Global . _check_paradigm ( \"openmp\" ): self . _generate_omp ( convolve_code , sum_code ) elif Global . _check_paradigm ( \"cuda\" ): Global . _error ( \"Pooling: not available on GPU devices\" ) #self._generate_cuda(convolve_code, sum_code) else : Global . _error ( \"Pooling: not implemented for the configured paradigm\" ) def _generate_pooling_code ( self ): \"\"\" Generate loop statements for the desired pooling operation. \"\"\" # Operation to be performed: sum, max, min, mean operation = self . synapse_type . operation # Main code # default value for sum in code depends on operation sum_default = \"0.0\" if self . synapse_type . operation == \"min\" : sum_default = \"std::numeric_limits< %(float_prec)s >::max()\" % { 'float_prec' : Global . config [ 'precision' ]} elif self . synapse_type . operation == \"max\" : sum_default = \"std::numeric_limits< %(float_prec)s >::min()\" % { 'float_prec' : Global . config [ 'precision' ]} code = \"\"\" sum = %(sum_default)s ; \"\"\" % { 'sum_default' : sum_default } # Generate for loops for dim in range ( self . dim_pre ): ind_dict = { 'index' : indices [ dim ], 'size' : self . extent [ dim ] } if self . extent [ dim ] > 1 : code += \"\"\" for(int %(index)s _w = 0; %(index)s _w < %(size)s ; %(index)s _w++){ \"\"\" % ind_dict # Compute indices for dim in range ( self . dim_pre ): ind_dict = { 'index' : indices [ dim ], 'dim' : dim } if self . extent [ dim ] > 1 : code += \"\"\" int %(index)s _pre = coord[ %(dim)s ] + %(index)s _w;\"\"\" % ind_dict else : code += \"\"\" int %(index)s _pre = coord[ %(dim)s ];\"\"\" % ind_dict # Check indices for dim in range ( self . dim_pre ): ind_dict = { 'index' : indices [ dim ], 'max_size' : self . pre . geometry [ dim ] - 1 } code += \"\"\" if (( %(index)s _pre < 0) ||( %(index)s _pre > %(max_size)s )){ continue; }\"\"\" % ind_dict # Compute pre-synaptic rank code += \"\"\" rk_pre = %(value)s ;\"\"\" % { 'value' : self . _coordinates_to_rank ( 'pre' , self . pre . geometry )} # Compute the value to pool psp = self . synapse_type . description [ 'psp' ][ 'cpp' ] % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'local_index' : '[i][j]' , 'global_index' : '[i]' , 'pre_index' : '[rk_pre]' , 'post_index' : '[rk_post]' , 'pre_prefix' : 'pop' + str ( self . pre . id ) + '.' , 'post_prefix' : 'pop' + str ( self . post . id ) + '.' } # Delays if self . delays > Global . config [ 'dt' ]: psp = psp . replace ( 'pop %(id_pre)s .r[rk_pre]' % { 'id_pre' : self . pre . id }, 'pop %(id_pre)s ._delayed_r[ %(delay)s ][rk_pre]' % { 'id_pre' : self . pre . id , 'delay' : str ( int ( self . delays / Global . config [ 'dt' ]) - 1 )} ) # Apply the operation if operation == \"max\" : code += \"\"\" %(float_prec)s _psp = %(psp)s ; if(_psp > sum) sum = _psp;\"\"\" elif operation == \"min\" : code += \"\"\" %(float_prec)s _psp = %(psp)s ; if(_psp < sum) sum = _psp;\"\"\" elif operation == \"sum\" : code += \"\"\" sum += %(psp)s ;\"\"\" elif operation == \"mean\" : code += \"\"\" sum += %(psp)s ;\"\"\" else : Global . _error ( 'SharedProjection: Operation' , operation , 'is not implemented yet for shared projections with pooling.' ) # Close for loops for dim in range ( self . dim_pre ): if self . extent [ dim ] > 1 : code += \"\"\" }\"\"\" impl_code = code % { 'id_proj' : self . id , 'target' : self . target , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'size_pre' : self . pre . size , 'id_post' : self . post . id , 'name_post' : self . post . name , 'size_post' : self . post . size , 'psp' : psp , 'float_prec' : Global . config [ 'precision' ] } if operation == \"mean\" : size = 1 for dim in range ( self . pre . dimension ): size *= self . extent [ dim ] sum_code = \"sum/\" + str ( size ) else : sum_code = \"sum\" return impl_code , sum_code def _generate_omp ( self , convolve_code , sum_code ): \"\"\" Update the ProjectionGenerator._specific_template structure and bypass the standard openMP code generation. :param convolve_code: :param sum_code: \"\"\" # default value for sum in code depends on operation sum_default = \"0.0\" if self . synapse_type . operation == \"min\" : sum_default = \"std::numeric_limits< %(float_prec)s >::max()\" % { 'float_prec' : Global . config [ 'precision' ]} elif self . synapse_type . operation == \"max\" : sum_default = \"std::numeric_limits< %(float_prec)s >::min()\" % { 'float_prec' : Global . config [ 'precision' ]} # Specific template for generation pool_dict = deepcopy ( pooling_template_omp ) for key , value in pool_dict . items (): value = value % { 'id_proj' : self . id , 'size_post' : self . post . size , 'sum_default' : sum_default , 'float_prec' : Global . config [ 'precision' ] } pool_dict [ key ] = value self . _specific_template . update ( pool_dict ) # OMP code omp_code = \"\" if Global . config [ 'num_threads' ] > 1 : omp_code = \"\"\" #pragma omp for private(sum, rk_pre, coord) %(psp_schedule)s \"\"\" % { 'psp_schedule' : \"\" if not 'psp_schedule' in self . _omp_config . keys () else self . _omp_config [ 'psp_schedule' ]} # HD ( 16.10.2015 ): # pre-load delayed firing rate in a local array, so we # prevent multiple accesses to pop%(id_pre)s._delayed_r[%(delay)s] if self . delays > Global . config [ 'dt' ]: pre_load_r = \"\"\" // pre-load delayed firing rate auto delayed_r = pop %(id_pre)s ._delayed_r[ %(delay)s ]; \"\"\" % { 'id_pre' : self . pre . id , 'delay' : str ( int ( self . delays / Global . config [ 'dt' ]) - 1 )} else : pre_load_r = \"\" # Target variable depends on neuron type target_code = \"_sum_ %(target)s \" if self . post . neuron_type . type == \"rate\" else \"g_ %(target)s \" target_code %= { 'target' : self . target } # Compute sum wsum = \"\"\" if ( _transmission && pop %(id_pre)s ._active ) { std::vector<int> coord; \"\"\" + pre_load_r + \"\"\" %(omp_code)s for(int i = 0; i < %(size_post)s ; i++){ coord = pre_rank[i]; \"\"\" + convolve_code + \"\"\" pop %(id_post)s . %(target)s [i] += \"\"\" + sum_code + \"\"\"; } // for } // if \"\"\" # Delays self . _specific_template [ 'wrapper_init_delay' ] = \"\" # Dictionary keys psp_dict = { 'id_proj' : self . id , 'target' : target_code , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'size_pre' : self . pre . size , 'id_post' : self . post . id , 'name_post' : self . post . name , 'size_post' : self . post . size , 'omp_code' : omp_code , 'convolve_code' : convolve_code } # Psp code self . _specific_template [ 'psp_code' ] = wsum % psp_dict self . _specific_template [ 'size_in_bytes' ] = \"\"\" // connectivity size_in_bytes += sizeof(std::vector<int>); size_in_bytes += pre_rank.capacity() * sizeof(int); size_in_bytes += sizeof(std::vector<std::vector<int>>); size_in_bytes += pre_rank.capacity() * sizeof(std::vector<int>); for (auto it = pre_rank.begin(); it != pre_rank.end(); it++) { size_in_bytes += it->capacity() * sizeof(int); } \"\"\" self . _specific_template [ 'clear' ] = \"\"\" // post-ranks post_rank.clear(); post_rank.shrink_to_fit(); // pre-ranks sub-lists for (auto it = pre_rank.begin(); it != pre_rank.end(); it++) { it->clear(); it->shrink_to_fit(); } // pre-ranks top-list pre_rank.clear(); pre_rank.shrink_to_fit(); \"\"\" def _generate_cuda ( self , convolve_code , sum_code ): \"\"\" Update the ProjectionGenerator._specific_template structure and bypass the standard CUDA code generation. \"\"\" pool_operation = self . synapse_type . operation # default value for sum in code depends on operation sum_default = \"0.0\" if pool_operation == \"min\" : sum_default = \"FLT_MAX\" elif pool_operation == \"max\" : sum_default = \"FLT_MIN\" # operation to perform pool_op_code = cuda_op_code [ pool_operation ] % { 'float_prec' : Global . config [ 'precision' ]} # result dictionary with code for # body, call and header pool_template = {} base_ids = { 'id_proj' : self . id , 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'target' : self . target , 'float_prec' : Global . config [ 'precision' ], 'size_post' : self . post . size # TODO: population views? } # The correct templates depends on both # kernel-geometry and extent if len ( self . pre . geometry ) == 2 : # For small extents, we compute multiple coords within one warp. If one extent can fill alone # a half-warp we switch to the other implementation. if self . extent [ 0 ] < 6 : pool_op_reduce_code = cuda_pooling_code_2d_small_extent [ 'reduce_code' ][ pool_operation ] % { 'float_prec' : Global . config [ 'precision' ], 'row_extent' : int ( self . extent [ 0 ]), 'col_extent' : int ( self . extent [ 1 ]) } pool_dict = deepcopy ( base_ids ) pool_dict . update ({ 'sum_default' : sum_default , 'row_extent' : int ( self . extent [ 0 ]), 'col_extent' : int ( self . extent [ 1 ]), 'row_size' : int ( self . pre . geometry [ 0 ]), 'col_size' : int ( self . pre . geometry [ 1 ]), 'operation' : tabify ( pool_op_code , 3 ), 'operation_reduce' : pool_op_reduce_code }) pool_template [ 'psp_body' ] = cuda_pooling_code_2d_small_extent [ 'psp_body' ] % pool_dict pool_template [ 'psp_header' ] = cuda_pooling_code_2d_small_extent [ 'psp_header' ] % pool_dict pool_template [ 'psp_call' ] = cuda_pooling_code_2d_small_extent [ 'psp_call' ] % pool_dict else : pool_op_reduce_code = cuda_pooling_code_2d [ 'reduce_code' ][ pool_operation ] % { 'float_prec' : Global . config [ 'precision' ], 'row_extent' : int ( self . extent [ 0 ]), 'col_extent' : int ( self . extent [ 1 ]) } pool_dict = deepcopy ( base_ids ) pool_dict . update ({ 'sum_default' : sum_default , 'row_extent' : int ( self . extent [ 0 ]), 'col_extent' : int ( self . extent [ 1 ]), 'row_size' : int ( self . pre . geometry [ 0 ]), 'col_size' : int ( self . pre . geometry [ 1 ]), 'operation' : tabify ( pool_op_code , 3 ), 'operation_reduce' : tabify ( pool_op_reduce_code , 2 ) }) pool_template [ 'psp_body' ] = remove_trailing_spaces ( cuda_pooling_code_2d [ 'psp_body' ] % pool_dict ) pool_template [ 'psp_header' ] = cuda_pooling_code_2d [ 'psp_header' ] % pool_dict pool_template [ 'psp_call' ] = cuda_pooling_code_2d [ 'psp_call' ] % pool_dict elif len ( self . pre . geometry ) == 3 : pool_dict = deepcopy ( base_ids ) pool_dict . update ({ 'sum_default' : sum_default , 'row_extent' : self . extent [ 0 ], 'col_extent' : self . extent [ 1 ], 'plane_extent' : self . extent [ 2 ], 'row_size' : self . pre . geometry [ 0 ], 'col_size' : self . pre . geometry [ 1 ], 'plane_size' : self . pre . geometry [ 2 ], 'operation' : tabify ( pool_op_code , 4 ) }) pool_template [ 'psp_body' ] = remove_trailing_spaces ( cuda_pooling_code_3d [ 'psp_body' ] % pool_dict ) pool_template [ 'psp_header' ] = cuda_pooling_code_3d [ 'psp_header' ] % pool_dict pool_template [ 'psp_call' ] = cuda_pooling_code_3d [ 'psp_header' ] % pool_dict else : raise NotImplementedError # Update psp fields self . _specific_template . update ( pool_template ) # Specific template for generation (wrapper, etc) pool_dict = deepcopy ( pooling_template_cuda ) for key , value in pool_dict . items (): value = value % base_ids pool_dict [ key ] = value self . _specific_template . update ( pool_dict ) self . _specific_template [ 'wrapper_connector_call' ] = \"\" self . _specific_template [ 'access_parameters_variables' ] = \"\" self . _specific_template [ 'size_in_bytes' ] = \"//TODO: \\n \" @staticmethod def _coordinates_to_rank ( name , geometry ): \"\"\" Generate the code for array access, for instance used for pre-synaptic ranks. \"\"\" dim = len ( geometry ) txt = \"\" for d in range ( dim ): if txt == \"\" : # first coordinate is special txt = indices [ 0 ] + \"_\" + name else : txt = str ( geometry [ d ]) + '*(' + txt + ') + ' + indices [ d ] + '_' + name return txt ############################## ## Override useless methods ############################## def _data ( self ): \"Disable saving.\" desc = {} desc [ 'post_ranks' ] = self . post_ranks desc [ 'attributes' ] = self . attributes desc [ 'parameters' ] = self . parameters desc [ 'variables' ] = self . variables desc [ 'dendrites' ] = [] desc [ 'number_of_synapses' ] = 0 return desc def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be saved.' ) def save ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be saved.' ) def load ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be loaded.' ) def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Pooling projections can not display receptive fields.' ) def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Pooling projections can not display connectivity matrices.' )","title":"Pooling"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.__init__","text":"Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection required operation pooling function to be applied (\"max\", \"min\", \"mean\") 'max' Source code in ANNarchy/extensions/convolution/Pooling.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , pre , post , target , psp = \"pre.r\" , operation = \"max\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param operation: pooling function to be applied (\"max\", \"min\", \"mean\") \"\"\" # Sanity check #if not pre.neuron_type.type == 'rate': # Global._error('Pooling: only implemented for rate-coded populations.') # Sanity check if not operation in [ \"max\" , \"mean\" , \"min\" ]: Global . _error ( \"Pooling: the operation must be either 'max', 'mean' or 'min'.\" ) self . operation = operation # Store for _copy self . psp = psp Projection . __init__ ( self , pre , post , target , synapse = SharedSynapse ( psp = psp , operation = operation , name = \"Pooling operation\" , description = operation + \"-pooling operation over the pre-synaptic population.\" ), name = name , copied = copied ) # check dimensions of populations, should not exceed 4 self . dim_pre = self . pre . dimension self . dim_post = self . post . dimension if self . dim_post > 4 : Global . _error ( 'Pooling: Too many dimensions for the post-synaptic population (maximum 4).' ) if self . dim_pre > 4 : Global . _error ( 'Pooling: Too many dimensions for the pre-synaptic population (maximum 4).' ) # Disable saving self . _saveable = False","title":"__init__()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.connect_pooling","text":"Parameters: Name Type Description Default extent extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g (2, 2) ). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None. None delays synaptic delay in ms 0.0 Source code in ANNarchy/extensions/convolution/Pooling.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def connect_pooling ( self , extent = None , delays = 0.0 ): \"\"\" :param extent: extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g ``(2, 2)``). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None. :param delays: synaptic delay in ms \"\"\" # process extent self . extent_init = extent if extent is None : # compute the extent automatically if self . pre . dimension != self . post . dimension : Global . _error ( 'Pooling: If you do not provide the extent parameter, the two populations must have the same number of dimensions.' ) extent = list ( self . pre . geometry ) for dim in range ( self . pre . dimension ): extent [ dim ] /= self . post . geometry [ dim ] if self . pre . geometry [ dim ] != extent [ dim ] * self . post . geometry [ dim ]: Global . _error ( 'Pooling: Unable to compute the extent of the pooling area: the number of neurons do not match.' ) elif not isinstance ( extent , tuple ): Global . _error ( 'Pooling: You must provide a tuple for the extent of the pooling operation.' ) self . extent = list ( extent ) if len ( self . extent ) < self . pre . dimension : Global . _error ( 'Pooling: You must provide a tuple for the extent of the pooling operation.' ) # process delays self . delays = delays # Generate the pre-synaptic coordinates self . _generate_extent_coordinates () # create fake LIL self . _create () return self","title":"connect_pooling()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.connectivity_matrix","text":"Not available. Source code in ANNarchy/extensions/convolution/Pooling.py 634 635 636 def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Pooling projections can not display connectivity matrices.' )","title":"connectivity_matrix()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.load","text":"Not available. Source code in ANNarchy/extensions/convolution/Pooling.py 628 629 630 def load ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be loaded.' )","title":"load()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.receptive_fields","text":"Not available. Source code in ANNarchy/extensions/convolution/Pooling.py 631 632 633 def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Pooling projections can not display receptive fields.' )","title":"receptive_fields()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.save","text":"Not available. Source code in ANNarchy/extensions/convolution/Pooling.py 625 626 627 def save ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be saved.' )","title":"save()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.save_connectivity","text":"Not available. Source code in ANNarchy/extensions/convolution/Pooling.py 622 623 624 def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Pooling projections can not be saved.' )","title":"save_connectivity()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy","text":"","title":"Copy"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy","text":"Bases: Projection Creates a virtual projection reusing the weights and delays of an already-defined projection. Although the original projection can be learnable, this one can not. Changes in the original weights will be reflected in this projection. The only possible modifications are psp and operation . The pre- and post-synaptic populations of both projections must have the same geometry. Example: proj = Projection ( pop1 , pop2 , \"exc\" ) proj . connect_fixed_probability ( 0.1 , 0.5 ) copy_proj = Copy ( pop1 , pop3 , \"exc\" ) copy_proj . connect_copy ( proj ) Source code in ANNarchy/extensions/convolution/Copy.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 class Copy ( Projection ): \"\"\" Creates a virtual projection reusing the weights and delays of an already-defined projection. Although the original projection can be learnable, this one can not. Changes in the original weights will be reflected in this projection. The only possible modifications are ``psp`` and ``operation``. The pre- and post-synaptic populations of both projections must have the same geometry. Example: ```python proj = Projection(pop1, pop2, \"exc\") proj.connect_fixed_probability(0.1, 0.5) copy_proj = Copy(pop1, pop3, \"exc\") copy_proj.connect_copy(proj) ``` \"\"\" def __init__ ( self , pre , post , target , psp = \"pre.r * w\" , operation = \"sum\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum). \"\"\" # Create the description, but it will not be used for generation Projection . __init__ ( self , pre = pre , post = post , target = target , synapse = SharedSynapse ( psp = psp , operation = operation ), name = name , copied = copied ) def connect_copy ( self , projection ): \"\"\" :param projection: Existing projection to copy. \"\"\" self . projection = projection # Sanity checks if not isinstance ( self . projection , Projection ): Global . _error ( 'Copy: You must provide an existing projection to copy().' ) if isinstance ( self . projection , ( ConvolutionProjection , PoolingProjection )): Global . _error ( 'Copy: You can only copy regular projections, not shared projections.' ) if not self . pre . geometry == self . projection . pre . geometry or not self . post . geometry == self . projection . post . geometry : Global . _error ( 'Copy: When copying a projection, the geometries must be the same.' ) # Dummy weights self . weights = None self . pre_coordinates = [] # Finish building the synapses self . _create () return self def _copy ( self , pre , post ): \"Returns a copy of the projection when creating networks. Internal use only.\" raise NotImplementedError def _create ( self ): # create fake LIL object, just for compilation. try : from ANNarchy.core.cython_ext.Connector import LILConnectivity except Exception as e : Global . _print ( e ) Global . _error ( 'ANNarchy was not successfully installed.' ) lil = LILConnectivity () lil . max_delay = self . delays lil . uniform_delay = self . delays self . connector_name = \"Copy\" self . connector_description = \"Copy projection\" self . _store_connectivity ( self . _load_from_lil , ( lil , ), self . delays ) def _connect ( self , module ): \"\"\" Builds up dendrites either from list or dictionary. Called by instantiate(). \"\"\" if not self . _connection_method : Global . _error ( 'Copy: The projection between ' + self . pre . name + ' and ' + self . post . name + ' is declared but not connected.' ) # Create the Cython instance proj = getattr ( module , 'proj' + str ( self . id ) + '_wrapper' ) self . cyInstance = proj ( self . weights , self . pre_coordinates ) # Define the list of postsynaptic neurons self . post_ranks = list ( range ( self . post . size )) # Set delays after instantiation if self . delays > 0.0 : self . cyInstance . set_delay ( self . delays / Global . config [ 'dt' ]) return True def _generate ( self ): \"\"\" Overrides default code generation. This function is called during the code generation procedure. \"\"\" if Global . _check_paradigm ( \"openmp\" ): self . _generate_omp () elif Global . _check_paradigm ( \"cuda\" ): self . _generate_cuda () else : raise NotImplementedError def generate_omp ( self ): \"\"\" Code generation of CopyProjection object for the openMP paradigm. \"\"\" # Set the projection specific parameters copy_proj_dict = deepcopy ( copy_proj_template ) for key , value in copy_proj_dict . items (): value = value % { 'id_proj' : self . id , 'id_copy' : self . projection . id , 'float_prec' : Global . config [ 'precision' ] } copy_proj_dict [ key ] = value # Update specific template self . _specific_template . update ( copy_proj_dict ) # OMP code if more then one thread if Global . config [ 'num_threads' ] > 1 : omp_code = '#pragma omp for private(sum)' if self . post . size > Global . OMP_MIN_NB_NEURONS else '' else : omp_code = \"\" # PSP psp = self . synapse_type . description [ 'psp' ][ 'cpp' ] % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'local_index' : '[i][j]' , 'global_index' : '[i]' , 'pre_index' : '[pre_rank[i][j]]' , 'post_index' : '[post_rank[i]]' , 'pre_prefix' : 'pop' + str ( self . pre . id ) + '.' , 'post_prefix' : 'pop' + str ( self . post . id ) + '.' } psp = psp . replace ( 'rk_pre' , 'pre_rank[i][j]' ) . replace ( ';' , '' ) # Take delays into account if any if self . delays > Global . config [ 'dt' ]: psp = psp . replace ( 'pop %(id_pre)s .r[rk_pre]' % { 'id_pre' : self . pre . id }, 'pop %(id_pre)s ._delayed_r[delay-1][rk_pre]' % { 'id_pre' : self . pre . id } # TODO HD: wouldn't it be much better to reduce delay globaly, instead of the substraction here??? ) # Select template for operation to be performed: sum, max, min, mean try : sum_code = copy_sum_template [ self . synapse_type . operation ] except KeyError : Global . _error ( \"CopyProjection: the operation \" , self . synapse_type . operation , ' is not available.' ) # Finalize code self . generator [ 'omp' ][ 'body_compute_psp' ] = sum_code % { 'id_proj' : self . id , 'target' : self . target , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'id_post' : self . post . id , 'name_post' : self . post . name , 'id' : self . projection . id , 'float_prec' : Global . config [ 'precision' ], 'omp_code' : omp_code , 'psp' : psp } def _generate_cuda ( self ): \"\"\" Code generation of CopyProjection object for the CUDA paradigm. Note: currently not implemented (TODO HD) \"\"\" raise NotImplementedError ############################## ## Override useless methods ############################## def _data ( self ): \"Disable saving.\" desc = {} desc [ 'post_ranks' ] = self . post_ranks desc [ 'attributes' ] = self . attributes desc [ 'parameters' ] = self . parameters desc [ 'variables' ] = self . variables desc [ 'dendrites' ] = [] desc [ 'number_of_synapses' ] = 0 return desc def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be saved.' ) def save ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be saved.' ) def load ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be loaded.' ) def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Copied projections can not display receptive fields.' ) def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Copied projections can not display connectivity matrices.' )","title":"Copy"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.__init__","text":"Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection required psp continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r ). 'pre.r * w' operation operation (sum, max, min, mean) performed by the kernel (default: sum). 'sum' Source code in ANNarchy/extensions/convolution/Copy.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , pre , post , target , psp = \"pre.r * w\" , operation = \"sum\" , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``). :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum). \"\"\" # Create the description, but it will not be used for generation Projection . __init__ ( self , pre = pre , post = post , target = target , synapse = SharedSynapse ( psp = psp , operation = operation ), name = name , copied = copied )","title":"__init__()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.connect_copy","text":"Parameters: Name Type Description Default projection Existing projection to copy. required Source code in ANNarchy/extensions/convolution/Copy.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def connect_copy ( self , projection ): \"\"\" :param projection: Existing projection to copy. \"\"\" self . projection = projection # Sanity checks if not isinstance ( self . projection , Projection ): Global . _error ( 'Copy: You must provide an existing projection to copy().' ) if isinstance ( self . projection , ( ConvolutionProjection , PoolingProjection )): Global . _error ( 'Copy: You can only copy regular projections, not shared projections.' ) if not self . pre . geometry == self . projection . pre . geometry or not self . post . geometry == self . projection . post . geometry : Global . _error ( 'Copy: When copying a projection, the geometries must be the same.' ) # Dummy weights self . weights = None self . pre_coordinates = [] # Finish building the synapses self . _create () return self","title":"connect_copy()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.connectivity_matrix","text":"Not available. Source code in ANNarchy/extensions/convolution/Copy.py 242 243 244 def connectivity_matrix ( self , fill = 0.0 ): \"Not available.\" Global . _warning ( 'Copied projections can not display connectivity matrices.' )","title":"connectivity_matrix()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.generate_omp","text":"Code generation of CopyProjection object for the openMP paradigm. Source code in ANNarchy/extensions/convolution/Copy.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def generate_omp ( self ): \"\"\" Code generation of CopyProjection object for the openMP paradigm. \"\"\" # Set the projection specific parameters copy_proj_dict = deepcopy ( copy_proj_template ) for key , value in copy_proj_dict . items (): value = value % { 'id_proj' : self . id , 'id_copy' : self . projection . id , 'float_prec' : Global . config [ 'precision' ] } copy_proj_dict [ key ] = value # Update specific template self . _specific_template . update ( copy_proj_dict ) # OMP code if more then one thread if Global . config [ 'num_threads' ] > 1 : omp_code = '#pragma omp for private(sum)' if self . post . size > Global . OMP_MIN_NB_NEURONS else '' else : omp_code = \"\" # PSP psp = self . synapse_type . description [ 'psp' ][ 'cpp' ] % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'local_index' : '[i][j]' , 'global_index' : '[i]' , 'pre_index' : '[pre_rank[i][j]]' , 'post_index' : '[post_rank[i]]' , 'pre_prefix' : 'pop' + str ( self . pre . id ) + '.' , 'post_prefix' : 'pop' + str ( self . post . id ) + '.' } psp = psp . replace ( 'rk_pre' , 'pre_rank[i][j]' ) . replace ( ';' , '' ) # Take delays into account if any if self . delays > Global . config [ 'dt' ]: psp = psp . replace ( 'pop %(id_pre)s .r[rk_pre]' % { 'id_pre' : self . pre . id }, 'pop %(id_pre)s ._delayed_r[delay-1][rk_pre]' % { 'id_pre' : self . pre . id } # TODO HD: wouldn't it be much better to reduce delay globaly, instead of the substraction here??? ) # Select template for operation to be performed: sum, max, min, mean try : sum_code = copy_sum_template [ self . synapse_type . operation ] except KeyError : Global . _error ( \"CopyProjection: the operation \" , self . synapse_type . operation , ' is not available.' ) # Finalize code self . generator [ 'omp' ][ 'body_compute_psp' ] = sum_code % { 'id_proj' : self . id , 'target' : self . target , 'id_pre' : self . pre . id , 'name_pre' : self . pre . name , 'id_post' : self . post . id , 'name_post' : self . post . name , 'id' : self . projection . id , 'float_prec' : Global . config [ 'precision' ], 'omp_code' : omp_code , 'psp' : psp }","title":"generate_omp()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.load","text":"Not available. Source code in ANNarchy/extensions/convolution/Copy.py 236 237 238 def load ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be loaded.' )","title":"load()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.receptive_fields","text":"Not available. Source code in ANNarchy/extensions/convolution/Copy.py 239 240 241 def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"Not available.\" Global . _warning ( 'Copied projections can not display receptive fields.' )","title":"receptive_fields()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.save","text":"Not available. Source code in ANNarchy/extensions/convolution/Copy.py 233 234 235 def save ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be saved.' )","title":"save()"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.save_connectivity","text":"Not available. Source code in ANNarchy/extensions/convolution/Copy.py 230 231 232 def save_connectivity ( self , filename ): \"Not available.\" Global . _warning ( 'Copied projections can not be saved.' )","title":"save_connectivity()"},{"location":"API/Dendrite.html","text":"Dendrite class # A Dendrite is a sub-group of a Projection , gathering the synapses between the pre-synaptic population and a single post-synaptic neuron. Dendrite # Bases: object A Dendrite is a sub-group of a Projection , gathering the synapses between the pre-synaptic population and a single post-synaptic neuron. It can not be created directly, only through a call to Projection.dendrite(rank) : dendrite = proj . dendrite ( 6 ) Source code in ANNarchy/core/Dendrite.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 class Dendrite ( object ): \"\"\" A ``Dendrite`` is a sub-group of a ``Projection``, gathering the synapses between the pre-synaptic population and a single post-synaptic neuron. It can not be created directly, only through a call to ``Projection.dendrite(rank)``: ```python dendrite = proj.dendrite(6) ``` \"\"\" def __init__ ( self , proj , post_rank , idx ): self . post_rank = post_rank self . idx = idx self . proj = proj self . pre = proj . pre self . target = self . proj . target self . attributes = self . proj . attributes self . parameters = self . proj . parameters self . variables = self . proj . variables @property def size ( self ): \"\"\" Number of synapses. \"\"\" if self . proj . cyInstance : return self . proj . cyInstance . dendrite_size ( self . idx ) return 0 @property def pre_ranks ( self ): \"\"\" List of ranks of pre-synaptic neurons. \"\"\" if self . proj . cyInstance : return self . proj . cyInstance . pre_rank ( self . idx ) return [] def __len__ ( self ): # Number of synapses. return self . size @property def synapses ( self ): \"\"\" Iteratively returns the synapses corresponding to this dendrite. \"\"\" for n in self . pre_ranks : yield IndividualSynapse ( self , n ) def synapse ( self , pos ): \"\"\" Returns the synapse coming from the corresponding presynaptic neuron. :param pos: can be either the rank or the coordinates of the presynaptic neuron \"\"\" if isinstance ( pos , int ): rank = pos else : rank = self . proj . pre . rank_from_coordinates ( pos ) if rank in self . pre_ranks : return IndividualSynapse ( self , rank ) else : Global . _error ( \" The neuron of rank \" + str ( rank ) + \" has no synapse in this dendrite.\" ) return None # Iterators def __getitem__ ( self , * args , ** kwds ): # Returns the synapse of the given position in the presynaptic population. # If only one argument is given, it is a rank. If it is a tuple, it is coordinates. if len ( args ) == 1 : return self . synapse ( args [ 0 ]) return self . synapse ( args ) def __iter__ ( self ): # Returns iteratively each synapse in the dendrite in ascending pre-synaptic rank order. for n in self . pre_ranks : yield IndividualSynapse ( self , n ) ######################### ### Access to attributes ######################### def __getattr__ ( self , name ): # Method called when accessing an attribute. if name == 'proj' : return object . __getattribute__ ( self , name ) elif hasattr ( self , 'proj' ): if name == 'rank' : # TODO: remove 'rank' in a future version Global . _warning ( \"Dendrite.rank: the attribute is deprecated, use Dendrite.pre_ranks instead.\" ) return self . proj . cyInstance . pre_rank ( self . idx ) elif name == 'pre_rank' : return self . proj . cyInstance . pre_rank ( self . idx ) elif name == 'delay' : if self . proj . uniform_delay == - 1 : return [ d * Global . config [ 'dt' ] for d in self . proj . cyInstance . get_dendrite_delay ( self . idx )] else : return self . proj . max_delay * Global . config [ 'dt' ] elif name == \"w\" and self . proj . _has_single_weight (): return self . proj . cyInstance . get_global_attribute ( name , Global . config [ \"precision\" ]) elif name in self . proj . attributes : # Determine C++ data type ctype = None for var in self . proj . synapse_type . description [ 'variables' ] + self . proj . synapse_type . description [ 'parameters' ]: if var [ 'name' ] == name : ctype = var [ 'ctype' ] if name in self . proj . synapse_type . description [ 'local' ]: return self . proj . cyInstance . get_local_attribute_row ( name , self . idx , ctype ) elif name in self . proj . synapse_type . description [ 'semiglobal' ]: return self . proj . cyInstance . get_semiglobal_attribute ( name , self . idx , ctype ) else : return self . proj . cyInstance . get_global_attribute ( name , ctype ) else : return object . __getattribute__ ( self , name ) else : return object . __getattribute__ ( self , name ) def __setattr__ ( self , name , value ): # Method called when setting an attribute. if name == 'proj' : object . __setattr__ ( self , 'proj' , value ) elif name == 'attributes' : object . __setattr__ ( self , 'attributes' , value ) elif hasattr ( self , 'proj' ): if name in self . proj . attributes : # Determine C++ data type ctype = None for var in self . proj . synapse_type . description [ 'variables' ] + self . proj . synapse_type . description [ 'parameters' ]: if var [ 'name' ] == name : ctype = var [ 'ctype' ] if name in self . proj . synapse_type . description [ 'local' ]: if isinstance ( value , ( np . ndarray , list )): self . proj . cyInstance . set_local_attribute_row ( name , self . idx , value , ctype ) else : self . proj . cyInstance . set_local_attribute_row ( name , self . idx , value * np . ones ( self . size ), ctype ) elif name in self . proj . synapse_type . description [ 'semiglobal' ]: self . proj . cyInstance . set_semiglobal_attribute ( name , self . idx , value , ctype ) else : raise Global . _error ( \"Projection attributes marked as *projection* should not be updated through dendrites.\" ) else : object . __setattr__ ( self , name , value ) else : object . __setattr__ ( self , name , value ) def set ( self , value ): \"\"\" Sets the value of a parameter/variable of all synapses. Example: ```python dendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } ) ``` :param value: a dictionary containing the parameter/variable names as keys. \"\"\" for key , value in value . items (): # sanity check and then forward to __setattr__ if key in self . attributes : setattr ( self , key , value ) else : Global . _error ( \"Dendrite has no parameter/variable called\" , key ) def get ( self , name ): \"\"\" Returns the value of a variable/parameter. Example: ```python dendrite.get('w') ``` :param name: name of the parameter/variable. \"\"\" if name == 'rank' : Global . _warning ( \"Dendrite.get('rank'): the attribute is deprecated, use Dendrite.pre_ranks instead.\" ) return self . proj . cyInstance . pre_rank ( self . idx ) elif name == 'pre_ranks' : return self . proj . cyInstance . pre_rank ( self . idx ) elif name in self . attributes : return getattr ( self , name ) else : Global . _error ( \"Dendrite has no parameter/variable called\" , name ) ######################### ### Formatting ######################### def receptive_field ( self , variable = 'w' , fill = 0.0 ): \"\"\" Returns the given variable as a receptive field. A Numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value ``fill``). :param variable: name of the variable (default = 'w') :param fill: value to use when a synapse does not exist (default: 0.0). \"\"\" values = getattr ( self . proj . cyInstance , 'get_dendrite_' + variable )( self . idx ) pre_ranks = self . proj . cyInstance . pre_rank ( self . idx ) m = fill * np . ones ( self . pre . size ) m [ pre_ranks ] = values return m . reshape ( self . pre . geometry ) ######################### ### Structural plasticity ######################### def create_synapse ( self , rank , w = 0.0 , delay = 0 ): \"\"\" Creates a synapse for this dendrite with the given pre-synaptic neuron. :param rank: rank of the pre-synaptic neuron :param w: synaptic weight (defalt: 0.0). :param delay: synaptic delay (default = dt) \"\"\" if not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), can not add the synapse.' ) return if self . proj . cyInstance . dendrite_index ( self . post_rank , rank ) != - 1 : Global . _error ( 'the synapse of rank ' + str ( rank ) + ' already exists.' ) return # Set default values for the additional variables extra_attributes = {} for var in self . proj . synapse_type . description [ 'parameters' ] + self . proj . synapse_type . description [ 'variables' ]: if not var [ 'name' ] in [ 'w' , 'delay' ] and var [ 'name' ] in self . proj . synapse_type . description [ 'local' ]: if not isinstance ( self . proj . init [ var [ 'name' ]], ( int , float , bool )): init = var [ 'init' ] else : init = self . proj . init [ var [ 'name' ]] extra_attributes [ var [ 'name' ]] = init try : self . proj . cyInstance . add_synapse ( self . post_rank , rank , w , int ( delay / Global . config [ 'dt' ]), ** extra_attributes ) except Exception as e : Global . _print ( e ) def prune_synapse ( self , rank ): \"\"\" Removes the synapse with the given pre-synaptic neuron from the dendrite. :param rank: rank of the pre-synaptic neuron \"\"\" if not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), can not remove the synapse.' ) return if not rank in self . pre_ranks : Global . _error ( 'the synapse with the pre-synaptic neuron of rank ' + str ( rank ) + ' did not already exist.' ) return self . proj . cyInstance . remove_synapse ( self . post_rank , rank ) pre_ranks property # List of ranks of pre-synaptic neurons. size property # Number of synapses. synapses property # Iteratively returns the synapses corresponding to this dendrite. create_synapse ( rank , w = 0.0 , delay = 0 ) # Creates a synapse for this dendrite with the given pre-synaptic neuron. Parameters: Name Type Description Default rank rank of the pre-synaptic neuron required w synaptic weight (defalt: 0.0). 0.0 delay synaptic delay (default = dt) 0 Source code in ANNarchy/core/Dendrite.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def create_synapse ( self , rank , w = 0.0 , delay = 0 ): \"\"\" Creates a synapse for this dendrite with the given pre-synaptic neuron. :param rank: rank of the pre-synaptic neuron :param w: synaptic weight (defalt: 0.0). :param delay: synaptic delay (default = dt) \"\"\" if not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), can not add the synapse.' ) return if self . proj . cyInstance . dendrite_index ( self . post_rank , rank ) != - 1 : Global . _error ( 'the synapse of rank ' + str ( rank ) + ' already exists.' ) return # Set default values for the additional variables extra_attributes = {} for var in self . proj . synapse_type . description [ 'parameters' ] + self . proj . synapse_type . description [ 'variables' ]: if not var [ 'name' ] in [ 'w' , 'delay' ] and var [ 'name' ] in self . proj . synapse_type . description [ 'local' ]: if not isinstance ( self . proj . init [ var [ 'name' ]], ( int , float , bool )): init = var [ 'init' ] else : init = self . proj . init [ var [ 'name' ]] extra_attributes [ var [ 'name' ]] = init try : self . proj . cyInstance . add_synapse ( self . post_rank , rank , w , int ( delay / Global . config [ 'dt' ]), ** extra_attributes ) except Exception as e : Global . _print ( e ) get ( name ) # Returns the value of a variable/parameter. Example: dendrite . get ( 'w' ) Parameters: Name Type Description Default name name of the parameter/variable. required Source code in ANNarchy/core/Dendrite.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def get ( self , name ): \"\"\" Returns the value of a variable/parameter. Example: ```python dendrite.get('w') ``` :param name: name of the parameter/variable. \"\"\" if name == 'rank' : Global . _warning ( \"Dendrite.get('rank'): the attribute is deprecated, use Dendrite.pre_ranks instead.\" ) return self . proj . cyInstance . pre_rank ( self . idx ) elif name == 'pre_ranks' : return self . proj . cyInstance . pre_rank ( self . idx ) elif name in self . attributes : return getattr ( self , name ) else : Global . _error ( \"Dendrite has no parameter/variable called\" , name ) prune_synapse ( rank ) # Removes the synapse with the given pre-synaptic neuron from the dendrite. Parameters: Name Type Description Default rank rank of the pre-synaptic neuron required Source code in ANNarchy/core/Dendrite.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def prune_synapse ( self , rank ): \"\"\" Removes the synapse with the given pre-synaptic neuron from the dendrite. :param rank: rank of the pre-synaptic neuron \"\"\" if not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), can not remove the synapse.' ) return if not rank in self . pre_ranks : Global . _error ( 'the synapse with the pre-synaptic neuron of rank ' + str ( rank ) + ' did not already exist.' ) return self . proj . cyInstance . remove_synapse ( self . post_rank , rank ) receptive_field ( variable = 'w' , fill = 0.0 ) # Returns the given variable as a receptive field. A Numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value fill ). Parameters: Name Type Description Default variable name of the variable (default = 'w') 'w' fill value to use when a synapse does not exist (default: 0.0). 0.0 Source code in ANNarchy/core/Dendrite.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def receptive_field ( self , variable = 'w' , fill = 0.0 ): \"\"\" Returns the given variable as a receptive field. A Numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value ``fill``). :param variable: name of the variable (default = 'w') :param fill: value to use when a synapse does not exist (default: 0.0). \"\"\" values = getattr ( self . proj . cyInstance , 'get_dendrite_' + variable )( self . idx ) pre_ranks = self . proj . cyInstance . pre_rank ( self . idx ) m = fill * np . ones ( self . pre . size ) m [ pre_ranks ] = values return m . reshape ( self . pre . geometry ) set ( value ) # Sets the value of a parameter/variable of all synapses. Example: dendrite . set ( { 'tau' : 20 , 'w' = Uniform ( 0.0 , 1.0 ) } ) Parameters: Name Type Description Default value a dictionary containing the parameter/variable names as keys. required Source code in ANNarchy/core/Dendrite.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def set ( self , value ): \"\"\" Sets the value of a parameter/variable of all synapses. Example: ```python dendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } ) ``` :param value: a dictionary containing the parameter/variable names as keys. \"\"\" for key , value in value . items (): # sanity check and then forward to __setattr__ if key in self . attributes : setattr ( self , key , value ) else : Global . _error ( \"Dendrite has no parameter/variable called\" , key ) synapse ( pos ) # Returns the synapse coming from the corresponding presynaptic neuron. Parameters: Name Type Description Default pos can be either the rank or the coordinates of the presynaptic neuron required Source code in ANNarchy/core/Dendrite.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def synapse ( self , pos ): \"\"\" Returns the synapse coming from the corresponding presynaptic neuron. :param pos: can be either the rank or the coordinates of the presynaptic neuron \"\"\" if isinstance ( pos , int ): rank = pos else : rank = self . proj . pre . rank_from_coordinates ( pos ) if rank in self . pre_ranks : return IndividualSynapse ( self , rank ) else : Global . _error ( \" The neuron of rank \" + str ( rank ) + \" has no synapse in this dendrite.\" ) return None","title":"Dendrite class"},{"location":"API/Dendrite.html#dendrite-class","text":"A Dendrite is a sub-group of a Projection , gathering the synapses between the pre-synaptic population and a single post-synaptic neuron.","title":"Dendrite class"},{"location":"API/Dendrite.html#ANNarchy.Dendrite","text":"Bases: object A Dendrite is a sub-group of a Projection , gathering the synapses between the pre-synaptic population and a single post-synaptic neuron. It can not be created directly, only through a call to Projection.dendrite(rank) : dendrite = proj . dendrite ( 6 ) Source code in ANNarchy/core/Dendrite.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 class Dendrite ( object ): \"\"\" A ``Dendrite`` is a sub-group of a ``Projection``, gathering the synapses between the pre-synaptic population and a single post-synaptic neuron. It can not be created directly, only through a call to ``Projection.dendrite(rank)``: ```python dendrite = proj.dendrite(6) ``` \"\"\" def __init__ ( self , proj , post_rank , idx ): self . post_rank = post_rank self . idx = idx self . proj = proj self . pre = proj . pre self . target = self . proj . target self . attributes = self . proj . attributes self . parameters = self . proj . parameters self . variables = self . proj . variables @property def size ( self ): \"\"\" Number of synapses. \"\"\" if self . proj . cyInstance : return self . proj . cyInstance . dendrite_size ( self . idx ) return 0 @property def pre_ranks ( self ): \"\"\" List of ranks of pre-synaptic neurons. \"\"\" if self . proj . cyInstance : return self . proj . cyInstance . pre_rank ( self . idx ) return [] def __len__ ( self ): # Number of synapses. return self . size @property def synapses ( self ): \"\"\" Iteratively returns the synapses corresponding to this dendrite. \"\"\" for n in self . pre_ranks : yield IndividualSynapse ( self , n ) def synapse ( self , pos ): \"\"\" Returns the synapse coming from the corresponding presynaptic neuron. :param pos: can be either the rank or the coordinates of the presynaptic neuron \"\"\" if isinstance ( pos , int ): rank = pos else : rank = self . proj . pre . rank_from_coordinates ( pos ) if rank in self . pre_ranks : return IndividualSynapse ( self , rank ) else : Global . _error ( \" The neuron of rank \" + str ( rank ) + \" has no synapse in this dendrite.\" ) return None # Iterators def __getitem__ ( self , * args , ** kwds ): # Returns the synapse of the given position in the presynaptic population. # If only one argument is given, it is a rank. If it is a tuple, it is coordinates. if len ( args ) == 1 : return self . synapse ( args [ 0 ]) return self . synapse ( args ) def __iter__ ( self ): # Returns iteratively each synapse in the dendrite in ascending pre-synaptic rank order. for n in self . pre_ranks : yield IndividualSynapse ( self , n ) ######################### ### Access to attributes ######################### def __getattr__ ( self , name ): # Method called when accessing an attribute. if name == 'proj' : return object . __getattribute__ ( self , name ) elif hasattr ( self , 'proj' ): if name == 'rank' : # TODO: remove 'rank' in a future version Global . _warning ( \"Dendrite.rank: the attribute is deprecated, use Dendrite.pre_ranks instead.\" ) return self . proj . cyInstance . pre_rank ( self . idx ) elif name == 'pre_rank' : return self . proj . cyInstance . pre_rank ( self . idx ) elif name == 'delay' : if self . proj . uniform_delay == - 1 : return [ d * Global . config [ 'dt' ] for d in self . proj . cyInstance . get_dendrite_delay ( self . idx )] else : return self . proj . max_delay * Global . config [ 'dt' ] elif name == \"w\" and self . proj . _has_single_weight (): return self . proj . cyInstance . get_global_attribute ( name , Global . config [ \"precision\" ]) elif name in self . proj . attributes : # Determine C++ data type ctype = None for var in self . proj . synapse_type . description [ 'variables' ] + self . proj . synapse_type . description [ 'parameters' ]: if var [ 'name' ] == name : ctype = var [ 'ctype' ] if name in self . proj . synapse_type . description [ 'local' ]: return self . proj . cyInstance . get_local_attribute_row ( name , self . idx , ctype ) elif name in self . proj . synapse_type . description [ 'semiglobal' ]: return self . proj . cyInstance . get_semiglobal_attribute ( name , self . idx , ctype ) else : return self . proj . cyInstance . get_global_attribute ( name , ctype ) else : return object . __getattribute__ ( self , name ) else : return object . __getattribute__ ( self , name ) def __setattr__ ( self , name , value ): # Method called when setting an attribute. if name == 'proj' : object . __setattr__ ( self , 'proj' , value ) elif name == 'attributes' : object . __setattr__ ( self , 'attributes' , value ) elif hasattr ( self , 'proj' ): if name in self . proj . attributes : # Determine C++ data type ctype = None for var in self . proj . synapse_type . description [ 'variables' ] + self . proj . synapse_type . description [ 'parameters' ]: if var [ 'name' ] == name : ctype = var [ 'ctype' ] if name in self . proj . synapse_type . description [ 'local' ]: if isinstance ( value , ( np . ndarray , list )): self . proj . cyInstance . set_local_attribute_row ( name , self . idx , value , ctype ) else : self . proj . cyInstance . set_local_attribute_row ( name , self . idx , value * np . ones ( self . size ), ctype ) elif name in self . proj . synapse_type . description [ 'semiglobal' ]: self . proj . cyInstance . set_semiglobal_attribute ( name , self . idx , value , ctype ) else : raise Global . _error ( \"Projection attributes marked as *projection* should not be updated through dendrites.\" ) else : object . __setattr__ ( self , name , value ) else : object . __setattr__ ( self , name , value ) def set ( self , value ): \"\"\" Sets the value of a parameter/variable of all synapses. Example: ```python dendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } ) ``` :param value: a dictionary containing the parameter/variable names as keys. \"\"\" for key , value in value . items (): # sanity check and then forward to __setattr__ if key in self . attributes : setattr ( self , key , value ) else : Global . _error ( \"Dendrite has no parameter/variable called\" , key ) def get ( self , name ): \"\"\" Returns the value of a variable/parameter. Example: ```python dendrite.get('w') ``` :param name: name of the parameter/variable. \"\"\" if name == 'rank' : Global . _warning ( \"Dendrite.get('rank'): the attribute is deprecated, use Dendrite.pre_ranks instead.\" ) return self . proj . cyInstance . pre_rank ( self . idx ) elif name == 'pre_ranks' : return self . proj . cyInstance . pre_rank ( self . idx ) elif name in self . attributes : return getattr ( self , name ) else : Global . _error ( \"Dendrite has no parameter/variable called\" , name ) ######################### ### Formatting ######################### def receptive_field ( self , variable = 'w' , fill = 0.0 ): \"\"\" Returns the given variable as a receptive field. A Numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value ``fill``). :param variable: name of the variable (default = 'w') :param fill: value to use when a synapse does not exist (default: 0.0). \"\"\" values = getattr ( self . proj . cyInstance , 'get_dendrite_' + variable )( self . idx ) pre_ranks = self . proj . cyInstance . pre_rank ( self . idx ) m = fill * np . ones ( self . pre . size ) m [ pre_ranks ] = values return m . reshape ( self . pre . geometry ) ######################### ### Structural plasticity ######################### def create_synapse ( self , rank , w = 0.0 , delay = 0 ): \"\"\" Creates a synapse for this dendrite with the given pre-synaptic neuron. :param rank: rank of the pre-synaptic neuron :param w: synaptic weight (defalt: 0.0). :param delay: synaptic delay (default = dt) \"\"\" if not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), can not add the synapse.' ) return if self . proj . cyInstance . dendrite_index ( self . post_rank , rank ) != - 1 : Global . _error ( 'the synapse of rank ' + str ( rank ) + ' already exists.' ) return # Set default values for the additional variables extra_attributes = {} for var in self . proj . synapse_type . description [ 'parameters' ] + self . proj . synapse_type . description [ 'variables' ]: if not var [ 'name' ] in [ 'w' , 'delay' ] and var [ 'name' ] in self . proj . synapse_type . description [ 'local' ]: if not isinstance ( self . proj . init [ var [ 'name' ]], ( int , float , bool )): init = var [ 'init' ] else : init = self . proj . init [ var [ 'name' ]] extra_attributes [ var [ 'name' ]] = init try : self . proj . cyInstance . add_synapse ( self . post_rank , rank , w , int ( delay / Global . config [ 'dt' ]), ** extra_attributes ) except Exception as e : Global . _print ( e ) def prune_synapse ( self , rank ): \"\"\" Removes the synapse with the given pre-synaptic neuron from the dendrite. :param rank: rank of the pre-synaptic neuron \"\"\" if not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), can not remove the synapse.' ) return if not rank in self . pre_ranks : Global . _error ( 'the synapse with the pre-synaptic neuron of rank ' + str ( rank ) + ' did not already exist.' ) return self . proj . cyInstance . remove_synapse ( self . post_rank , rank )","title":"Dendrite"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.pre_ranks","text":"List of ranks of pre-synaptic neurons.","title":"pre_ranks"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.size","text":"Number of synapses.","title":"size"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.synapses","text":"Iteratively returns the synapses corresponding to this dendrite.","title":"synapses"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.create_synapse","text":"Creates a synapse for this dendrite with the given pre-synaptic neuron. Parameters: Name Type Description Default rank rank of the pre-synaptic neuron required w synaptic weight (defalt: 0.0). 0.0 delay synaptic delay (default = dt) 0 Source code in ANNarchy/core/Dendrite.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def create_synapse ( self , rank , w = 0.0 , delay = 0 ): \"\"\" Creates a synapse for this dendrite with the given pre-synaptic neuron. :param rank: rank of the pre-synaptic neuron :param w: synaptic weight (defalt: 0.0). :param delay: synaptic delay (default = dt) \"\"\" if not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), can not add the synapse.' ) return if self . proj . cyInstance . dendrite_index ( self . post_rank , rank ) != - 1 : Global . _error ( 'the synapse of rank ' + str ( rank ) + ' already exists.' ) return # Set default values for the additional variables extra_attributes = {} for var in self . proj . synapse_type . description [ 'parameters' ] + self . proj . synapse_type . description [ 'variables' ]: if not var [ 'name' ] in [ 'w' , 'delay' ] and var [ 'name' ] in self . proj . synapse_type . description [ 'local' ]: if not isinstance ( self . proj . init [ var [ 'name' ]], ( int , float , bool )): init = var [ 'init' ] else : init = self . proj . init [ var [ 'name' ]] extra_attributes [ var [ 'name' ]] = init try : self . proj . cyInstance . add_synapse ( self . post_rank , rank , w , int ( delay / Global . config [ 'dt' ]), ** extra_attributes ) except Exception as e : Global . _print ( e )","title":"create_synapse()"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.get","text":"Returns the value of a variable/parameter. Example: dendrite . get ( 'w' ) Parameters: Name Type Description Default name name of the parameter/variable. required Source code in ANNarchy/core/Dendrite.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def get ( self , name ): \"\"\" Returns the value of a variable/parameter. Example: ```python dendrite.get('w') ``` :param name: name of the parameter/variable. \"\"\" if name == 'rank' : Global . _warning ( \"Dendrite.get('rank'): the attribute is deprecated, use Dendrite.pre_ranks instead.\" ) return self . proj . cyInstance . pre_rank ( self . idx ) elif name == 'pre_ranks' : return self . proj . cyInstance . pre_rank ( self . idx ) elif name in self . attributes : return getattr ( self , name ) else : Global . _error ( \"Dendrite has no parameter/variable called\" , name )","title":"get()"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.prune_synapse","text":"Removes the synapse with the given pre-synaptic neuron from the dendrite. Parameters: Name Type Description Default rank rank of the pre-synaptic neuron required Source code in ANNarchy/core/Dendrite.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def prune_synapse ( self , rank ): \"\"\" Removes the synapse with the given pre-synaptic neuron from the dendrite. :param rank: rank of the pre-synaptic neuron \"\"\" if not Global . config [ 'structural_plasticity' ]: Global . _error ( '\"structural_plasticity\" has not been set to True in setup(), can not remove the synapse.' ) return if not rank in self . pre_ranks : Global . _error ( 'the synapse with the pre-synaptic neuron of rank ' + str ( rank ) + ' did not already exist.' ) return self . proj . cyInstance . remove_synapse ( self . post_rank , rank )","title":"prune_synapse()"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.receptive_field","text":"Returns the given variable as a receptive field. A Numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value fill ). Parameters: Name Type Description Default variable name of the variable (default = 'w') 'w' fill value to use when a synapse does not exist (default: 0.0). 0.0 Source code in ANNarchy/core/Dendrite.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def receptive_field ( self , variable = 'w' , fill = 0.0 ): \"\"\" Returns the given variable as a receptive field. A Numpy array of the same geometry as the pre-synaptic population is returned. Non-existing synapses are replaced by zeros (or the value ``fill``). :param variable: name of the variable (default = 'w') :param fill: value to use when a synapse does not exist (default: 0.0). \"\"\" values = getattr ( self . proj . cyInstance , 'get_dendrite_' + variable )( self . idx ) pre_ranks = self . proj . cyInstance . pre_rank ( self . idx ) m = fill * np . ones ( self . pre . size ) m [ pre_ranks ] = values return m . reshape ( self . pre . geometry )","title":"receptive_field()"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.set","text":"Sets the value of a parameter/variable of all synapses. Example: dendrite . set ( { 'tau' : 20 , 'w' = Uniform ( 0.0 , 1.0 ) } ) Parameters: Name Type Description Default value a dictionary containing the parameter/variable names as keys. required Source code in ANNarchy/core/Dendrite.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def set ( self , value ): \"\"\" Sets the value of a parameter/variable of all synapses. Example: ```python dendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } ) ``` :param value: a dictionary containing the parameter/variable names as keys. \"\"\" for key , value in value . items (): # sanity check and then forward to __setattr__ if key in self . attributes : setattr ( self , key , value ) else : Global . _error ( \"Dendrite has no parameter/variable called\" , key )","title":"set()"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.synapse","text":"Returns the synapse coming from the corresponding presynaptic neuron. Parameters: Name Type Description Default pos can be either the rank or the coordinates of the presynaptic neuron required Source code in ANNarchy/core/Dendrite.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def synapse ( self , pos ): \"\"\" Returns the synapse coming from the corresponding presynaptic neuron. :param pos: can be either the rank or the coordinates of the presynaptic neuron \"\"\" if isinstance ( pos , int ): rank = pos else : rank = self . proj . pre . rank_from_coordinates ( pos ) if rank in self . pre_ranks : return IndividualSynapse ( self , rank ) else : Global . _error ( \" The neuron of rank \" + str ( rank ) + \" has no synapse in this dendrite.\" ) return None","title":"synapse()"},{"location":"API/Hybrid.html","text":"Hybrid networks # Converting a rate-coded population to a spiking population requires connecting a PoissonPopulation with the rate-coded one. Converting a spiking population with a rate-coded one requires the use of a DecodingProjection , which can connected using any connector method available for Projection . DecodingProjection # Bases: SpecificProjection Decoding projection to transform spike trains into firing rates. The pre-synaptic population must be a spiking population, while the post-synaptic one must be rate-coded. Pre-synaptic spikes are accumulated for each post-synaptic neuron. A sliding window can be used to smoothen the results with the window parameter. The decoded firing rate is accessible in the post-synaptic neurons with sum(target) . The projection can be connected using any method available in Projection (although all-to-all or many-to-one makes mostly sense). Delays are ignored. The weight value allows to scale the firing rate: if you want a pre-synaptic firing rate of 100 Hz to correspond to a post-synaptic rate of 1.0, use w = 1./100. . Example: pop1 = PoissonPopulation ( 1000 , rates = 100. ) pop2 = Population ( 1 , Neuron ( equations = \"r=sum(exc)\" )) proj = DecodingProjection ( pop1 , pop2 , 'exc' , window = 10.0 ) proj . connect_all_to_all ( 1.0 , force_multiple_weights = True ) Source code in ANNarchy/core/SpecificProjection.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 class DecodingProjection ( SpecificProjection ): \"\"\" Decoding projection to transform spike trains into firing rates. The pre-synaptic population must be a spiking population, while the post-synaptic one must be rate-coded. Pre-synaptic spikes are accumulated for each post-synaptic neuron. A sliding window can be used to smoothen the results with the ``window`` parameter. The decoded firing rate is accessible in the post-synaptic neurons with ``sum(target)``. The projection can be connected using any method available in ``Projection`` (although all-to-all or many-to-one makes mostly sense). Delays are ignored. The weight value allows to scale the firing rate: if you want a pre-synaptic firing rate of 100 Hz to correspond to a post-synaptic rate of 1.0, use ``w = 1./100.``. Example: ```python pop1 = PoissonPopulation(1000, rates=100.) pop2 = Population(1, Neuron(equations=\"r=sum(exc)\")) proj = DecodingProjection(pop1, pop2, 'exc', window=10.0) proj.connect_all_to_all(1.0, force_multiple_weights=True) ``` \"\"\" def __init__ ( self , pre , post , target , window = 0.0 , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population. :param post: post-synaptic population. :param target: type of the connection. :param window: duration of the time window to collect spikes (default: dt). \"\"\" # Instantiate the projection SpecificProjection . __init__ ( self , pre , post , target , None , name , copied ) # Check populations if not self . pre . neuron_type . type == 'spike' : Global . _error ( 'The pre-synaptic population of a DecodingProjection must be spiking.' ) if not self . post . neuron_type . type == 'rate' : Global . _error ( 'The post-synaptic population of a DecodingProjection must be rate-coded.' ) # Process window argument if window == 0.0 : window = Global . config [ 'dt' ] self . window = window # Disable openMP post-synaptic matrix split self . _no_split_matrix = True # Not on CUDA if Global . _check_paradigm ( 'cuda' ): Global . _error ( 'DecodingProjections are not available on CUDA yet.' ) def _copy ( self , pre , post ): \"Returns a copy of the population when creating networks. Internal use only.\" copied_proj = DecodingProjection ( pre = pre , post = post , target = self . target , window = self . window , name = self . name , copied = True ) copied_proj . _no_split_matrix = True return copied_proj def _generate_st ( self ): # Generate the code self . _specific_template [ 'declare_additional' ] = \"\"\" // Window int window = %(window)s ; std::deque< std::vector< %(float_prec)s > > rates_history ; \"\"\" % { 'window' : int ( self . window / Global . config [ 'dt' ]), 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'init_additional' ] = \"\"\" rates_history = std::deque< std::vector< %(float_prec)s > >( %(window)s , std::vector< %(float_prec)s >( %(post_size)s , 0.0)); \"\"\" % { 'window' : int ( self . window / Global . config [ 'dt' ]), 'post_size' : self . post . size , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'psp_code' ] = \"\"\" if (pop %(id_post)s ._active) { std::vector< std::pair<int, int> > inv_post; std::vector< %(float_prec)s > rates = std::vector< %(float_prec)s >( %(post_size)s , 0.0); // Iterate over all incoming spikes for(int _idx_j = 0; _idx_j < pop %(id_pre)s .spiked.size(); _idx_j++){ rk_j = pop %(id_pre)s .spiked[_idx_j]; inv_post = inv_pre_rank[rk_j]; nb_post = inv_post.size(); // Iterate over connected post neurons for(int _idx_i = 0; _idx_i < nb_post; _idx_i++){ // Retrieve the correct indices i = inv_post[_idx_i].first; j = inv_post[_idx_i].second; // Increase the post-synaptic conductance rates[post_rank[i]] += %(weight)s ; } } rates_history.push_front(rates); rates_history.pop_back(); for(int i=0; i<post_rank.size(); i++){ sum = 0.0; for(int step=0; step<window; step++){ sum += rates_history[step][post_rank[i]]; } pop %(id_post)s ._sum_ %(target)s [post_rank[i]] += sum /float(window) * 1000. / dt / float(pre_rank[i].size()); } } // active \"\"\" % { 'id_proj' : self . id , 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'target' : self . target , 'post_size' : self . post . size , 'float_prec' : Global . config [ 'precision' ], 'weight' : \"w\" if self . _has_single_weight () else \"w[i][j]\" } self . _specific_template [ 'psp_prefix' ] = \"\"\" int nb_post, i, j, rk_j, rk_post, rk_pre; %(float_prec)s sum; \"\"\" % { 'float_prec' : Global . config [ 'precision' ] } def _generate_omp ( self ): # Generate the code self . _specific_template [ 'declare_additional' ] = \"\"\" // Window int window = %(window)s ; std::deque< std::vector< %(float_prec)s > > rates_history ; \"\"\" % { 'window' : int ( self . window / Global . config [ 'dt' ]), 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'init_additional' ] = \"\"\" rates_history = std::deque< std::vector< %(float_prec)s > >( %(window)s , std::vector< %(float_prec)s >( %(post_size)s , 0.0)); \"\"\" % { 'window' : int ( self . window / Global . config [ 'dt' ]), 'post_size' : self . post . size , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'psp_code' ] = \"\"\" #pragma omp single { if (pop %(id_post)s ._active) { std::vector< std::pair<int, int> > inv_post; std::vector< %(float_prec)s > rates = std::vector< %(float_prec)s >( %(post_size)s , 0.0); // Iterate over all incoming spikes for(int _idx_j = 0; _idx_j < pop %(id_pre)s .spiked.size(); _idx_j++){ rk_j = pop %(id_pre)s .spiked[_idx_j]; inv_post = inv_pre_rank[rk_j]; nb_post = inv_post.size(); // Iterate over connected post neurons for(int _idx_i = 0; _idx_i < nb_post; _idx_i++){ // Retrieve the correct indices i = inv_post[_idx_i].first; j = inv_post[_idx_i].second; // Increase the post-synaptic conductance rates[post_rank[i]] += %(weight)s ; } } rates_history.push_front(rates); rates_history.pop_back(); for(int i=0; i<post_rank.size(); i++){ sum = 0.0; for(int step=0; step<window; step++){ sum += rates_history[step][post_rank[i]]; } pop %(id_post)s ._sum_ %(target)s [post_rank[i]] += sum /float(window) * 1000. / dt / float(pre_rank[i].size()); } } // active } \"\"\" % { 'id_proj' : self . id , 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'target' : self . target , 'post_size' : self . post . size , 'float_prec' : Global . config [ 'precision' ], 'weight' : \"w\" if self . _has_single_weight () else \"w[i][j]\" } self . _specific_template [ 'psp_prefix' ] = \"\"\" int nb_post, i, j, rk_j, rk_post, rk_pre; %(float_prec)s sum; \"\"\" % { 'float_prec' : Global . config [ 'precision' ] } def _generate_cuda ( self ): raise Global . ANNarchyException ( \"The DecodingProjection is not available on CUDA devices.\" , True ) __init__ ( pre , post , target , window = 0.0 , name = None , copied = False ) # Parameters: Name Type Description Default pre pre-synaptic population. required post post-synaptic population. required target type of the connection. required window duration of the time window to collect spikes (default: dt). 0.0 Source code in ANNarchy/core/SpecificProjection.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def __init__ ( self , pre , post , target , window = 0.0 , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population. :param post: post-synaptic population. :param target: type of the connection. :param window: duration of the time window to collect spikes (default: dt). \"\"\" # Instantiate the projection SpecificProjection . __init__ ( self , pre , post , target , None , name , copied ) # Check populations if not self . pre . neuron_type . type == 'spike' : Global . _error ( 'The pre-synaptic population of a DecodingProjection must be spiking.' ) if not self . post . neuron_type . type == 'rate' : Global . _error ( 'The post-synaptic population of a DecodingProjection must be rate-coded.' ) # Process window argument if window == 0.0 : window = Global . config [ 'dt' ] self . window = window # Disable openMP post-synaptic matrix split self . _no_split_matrix = True # Not on CUDA if Global . _check_paradigm ( 'cuda' ): Global . _error ( 'DecodingProjections are not available on CUDA yet.' )","title":"Hybrid networks"},{"location":"API/Hybrid.html#hybrid-networks","text":"Converting a rate-coded population to a spiking population requires connecting a PoissonPopulation with the rate-coded one. Converting a spiking population with a rate-coded one requires the use of a DecodingProjection , which can connected using any connector method available for Projection .","title":"Hybrid networks"},{"location":"API/Hybrid.html#ANNarchy.DecodingProjection","text":"Bases: SpecificProjection Decoding projection to transform spike trains into firing rates. The pre-synaptic population must be a spiking population, while the post-synaptic one must be rate-coded. Pre-synaptic spikes are accumulated for each post-synaptic neuron. A sliding window can be used to smoothen the results with the window parameter. The decoded firing rate is accessible in the post-synaptic neurons with sum(target) . The projection can be connected using any method available in Projection (although all-to-all or many-to-one makes mostly sense). Delays are ignored. The weight value allows to scale the firing rate: if you want a pre-synaptic firing rate of 100 Hz to correspond to a post-synaptic rate of 1.0, use w = 1./100. . Example: pop1 = PoissonPopulation ( 1000 , rates = 100. ) pop2 = Population ( 1 , Neuron ( equations = \"r=sum(exc)\" )) proj = DecodingProjection ( pop1 , pop2 , 'exc' , window = 10.0 ) proj . connect_all_to_all ( 1.0 , force_multiple_weights = True ) Source code in ANNarchy/core/SpecificProjection.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 class DecodingProjection ( SpecificProjection ): \"\"\" Decoding projection to transform spike trains into firing rates. The pre-synaptic population must be a spiking population, while the post-synaptic one must be rate-coded. Pre-synaptic spikes are accumulated for each post-synaptic neuron. A sliding window can be used to smoothen the results with the ``window`` parameter. The decoded firing rate is accessible in the post-synaptic neurons with ``sum(target)``. The projection can be connected using any method available in ``Projection`` (although all-to-all or many-to-one makes mostly sense). Delays are ignored. The weight value allows to scale the firing rate: if you want a pre-synaptic firing rate of 100 Hz to correspond to a post-synaptic rate of 1.0, use ``w = 1./100.``. Example: ```python pop1 = PoissonPopulation(1000, rates=100.) pop2 = Population(1, Neuron(equations=\"r=sum(exc)\")) proj = DecodingProjection(pop1, pop2, 'exc', window=10.0) proj.connect_all_to_all(1.0, force_multiple_weights=True) ``` \"\"\" def __init__ ( self , pre , post , target , window = 0.0 , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population. :param post: post-synaptic population. :param target: type of the connection. :param window: duration of the time window to collect spikes (default: dt). \"\"\" # Instantiate the projection SpecificProjection . __init__ ( self , pre , post , target , None , name , copied ) # Check populations if not self . pre . neuron_type . type == 'spike' : Global . _error ( 'The pre-synaptic population of a DecodingProjection must be spiking.' ) if not self . post . neuron_type . type == 'rate' : Global . _error ( 'The post-synaptic population of a DecodingProjection must be rate-coded.' ) # Process window argument if window == 0.0 : window = Global . config [ 'dt' ] self . window = window # Disable openMP post-synaptic matrix split self . _no_split_matrix = True # Not on CUDA if Global . _check_paradigm ( 'cuda' ): Global . _error ( 'DecodingProjections are not available on CUDA yet.' ) def _copy ( self , pre , post ): \"Returns a copy of the population when creating networks. Internal use only.\" copied_proj = DecodingProjection ( pre = pre , post = post , target = self . target , window = self . window , name = self . name , copied = True ) copied_proj . _no_split_matrix = True return copied_proj def _generate_st ( self ): # Generate the code self . _specific_template [ 'declare_additional' ] = \"\"\" // Window int window = %(window)s ; std::deque< std::vector< %(float_prec)s > > rates_history ; \"\"\" % { 'window' : int ( self . window / Global . config [ 'dt' ]), 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'init_additional' ] = \"\"\" rates_history = std::deque< std::vector< %(float_prec)s > >( %(window)s , std::vector< %(float_prec)s >( %(post_size)s , 0.0)); \"\"\" % { 'window' : int ( self . window / Global . config [ 'dt' ]), 'post_size' : self . post . size , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'psp_code' ] = \"\"\" if (pop %(id_post)s ._active) { std::vector< std::pair<int, int> > inv_post; std::vector< %(float_prec)s > rates = std::vector< %(float_prec)s >( %(post_size)s , 0.0); // Iterate over all incoming spikes for(int _idx_j = 0; _idx_j < pop %(id_pre)s .spiked.size(); _idx_j++){ rk_j = pop %(id_pre)s .spiked[_idx_j]; inv_post = inv_pre_rank[rk_j]; nb_post = inv_post.size(); // Iterate over connected post neurons for(int _idx_i = 0; _idx_i < nb_post; _idx_i++){ // Retrieve the correct indices i = inv_post[_idx_i].first; j = inv_post[_idx_i].second; // Increase the post-synaptic conductance rates[post_rank[i]] += %(weight)s ; } } rates_history.push_front(rates); rates_history.pop_back(); for(int i=0; i<post_rank.size(); i++){ sum = 0.0; for(int step=0; step<window; step++){ sum += rates_history[step][post_rank[i]]; } pop %(id_post)s ._sum_ %(target)s [post_rank[i]] += sum /float(window) * 1000. / dt / float(pre_rank[i].size()); } } // active \"\"\" % { 'id_proj' : self . id , 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'target' : self . target , 'post_size' : self . post . size , 'float_prec' : Global . config [ 'precision' ], 'weight' : \"w\" if self . _has_single_weight () else \"w[i][j]\" } self . _specific_template [ 'psp_prefix' ] = \"\"\" int nb_post, i, j, rk_j, rk_post, rk_pre; %(float_prec)s sum; \"\"\" % { 'float_prec' : Global . config [ 'precision' ] } def _generate_omp ( self ): # Generate the code self . _specific_template [ 'declare_additional' ] = \"\"\" // Window int window = %(window)s ; std::deque< std::vector< %(float_prec)s > > rates_history ; \"\"\" % { 'window' : int ( self . window / Global . config [ 'dt' ]), 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'init_additional' ] = \"\"\" rates_history = std::deque< std::vector< %(float_prec)s > >( %(window)s , std::vector< %(float_prec)s >( %(post_size)s , 0.0)); \"\"\" % { 'window' : int ( self . window / Global . config [ 'dt' ]), 'post_size' : self . post . size , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'psp_code' ] = \"\"\" #pragma omp single { if (pop %(id_post)s ._active) { std::vector< std::pair<int, int> > inv_post; std::vector< %(float_prec)s > rates = std::vector< %(float_prec)s >( %(post_size)s , 0.0); // Iterate over all incoming spikes for(int _idx_j = 0; _idx_j < pop %(id_pre)s .spiked.size(); _idx_j++){ rk_j = pop %(id_pre)s .spiked[_idx_j]; inv_post = inv_pre_rank[rk_j]; nb_post = inv_post.size(); // Iterate over connected post neurons for(int _idx_i = 0; _idx_i < nb_post; _idx_i++){ // Retrieve the correct indices i = inv_post[_idx_i].first; j = inv_post[_idx_i].second; // Increase the post-synaptic conductance rates[post_rank[i]] += %(weight)s ; } } rates_history.push_front(rates); rates_history.pop_back(); for(int i=0; i<post_rank.size(); i++){ sum = 0.0; for(int step=0; step<window; step++){ sum += rates_history[step][post_rank[i]]; } pop %(id_post)s ._sum_ %(target)s [post_rank[i]] += sum /float(window) * 1000. / dt / float(pre_rank[i].size()); } } // active } \"\"\" % { 'id_proj' : self . id , 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'target' : self . target , 'post_size' : self . post . size , 'float_prec' : Global . config [ 'precision' ], 'weight' : \"w\" if self . _has_single_weight () else \"w[i][j]\" } self . _specific_template [ 'psp_prefix' ] = \"\"\" int nb_post, i, j, rk_j, rk_post, rk_pre; %(float_prec)s sum; \"\"\" % { 'float_prec' : Global . config [ 'precision' ] } def _generate_cuda ( self ): raise Global . ANNarchyException ( \"The DecodingProjection is not available on CUDA devices.\" , True )","title":"DecodingProjection"},{"location":"API/Hybrid.html#ANNarchy.core.SpecificProjection.DecodingProjection.__init__","text":"Parameters: Name Type Description Default pre pre-synaptic population. required post post-synaptic population. required target type of the connection. required window duration of the time window to collect spikes (default: dt). 0.0 Source code in ANNarchy/core/SpecificProjection.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def __init__ ( self , pre , post , target , window = 0.0 , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population. :param post: post-synaptic population. :param target: type of the connection. :param window: duration of the time window to collect spikes (default: dt). \"\"\" # Instantiate the projection SpecificProjection . __init__ ( self , pre , post , target , None , name , copied ) # Check populations if not self . pre . neuron_type . type == 'spike' : Global . _error ( 'The pre-synaptic population of a DecodingProjection must be spiking.' ) if not self . post . neuron_type . type == 'rate' : Global . _error ( 'The post-synaptic population of a DecodingProjection must be rate-coded.' ) # Process window argument if window == 0.0 : window = Global . config [ 'dt' ] self . window = window # Disable openMP post-synaptic matrix split self . _no_split_matrix = True # Not on CUDA if Global . _check_paradigm ( 'cuda' ): Global . _error ( 'DecodingProjections are not available on CUDA yet.' )","title":"__init__()"},{"location":"API/IO.html","text":"Saving / Loading # Saving / loading the state of the network # To save or load the network state you can use the following methods: save ( filename , populations = True , projections = True , net_id = 0 ) # Save the current network state (parameters and variables) to a file. If the extension is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the extension is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. If the extension ends with '.gz', the data will be pickled into a binary file and compressed using gzip. Otherwise, the data will be pickled into a simple binary text file using cPickle. Warning: The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: save ( 'results/init.npz' ) save ( 'results/init.data' ) save ( 'results/init.txt.gz' ) save ( '1000_trials.mat' ) Parameters: Name Type Description Default filename filename, may contain relative or absolute path. required populations if True, population data will be saved (by default True) True projections if True, projection data will be saved (by default True) True load ( filename , populations = True , projections = True , net_id = 0 ) # Loads a saved state of the network. Warning: Matlab data can not be loaded. Example: load ( 'results/network.npz' ) Parameters: Name Type Description Default filename the filename with relative or absolute path. required populations if True, population data will be loaded (by default True) True projections if True, projection data will be loaded (by default True) True Please note that these functions are only usable after the call to ANNarchy.compile() . Saving / loading the parameters of the network # save_parameters ( filename , net_id = 0 ) # Saves the global parameters of a network (flag population for neurons, projection for synapses) to a JSON file. Parameters: Name Type Description Default filename path to the JSON file. required net_id ID of the network (default: 0, the global network). 0 load_parameters ( filename , global_only = True , verbose = False , net_id = 0 ) # Loads the global parameters of a network (flag population for neurons, projection for synapses) from a JSON file. It is advised to generate the JSON file first with save_parameters() and later edit it manually. A strong restriction is that population/projection names cannot change between saving and loading. By default, they take names such as pop0 or proj2 , we advise setting explicitly a name in their constructor for readability. If you add a parameter name to the JSON file but it does not exist in te neuron/synapse, it will be silently skipped. Enable verbose=True to see which parameters are effectively changed. If you set global_only to True, you will be able to set values for non-global parameters (e.g. synapse-specific), but a single value will be loaded for all. The JSON file cannot contain arrays. If you want to save/load the value of variables after a simulation, please refer to save() or load() . Parameters: Name Type Description Default filename path to the JSON file. required global_only True if only global parameters (flags population and projection ) should be loaded, the other values are ignored. (default: True) True verbose True if the old and new values of the parameters should be printed (default: False). False net_id ID of the network (default: 0, the global network). 0 Returns: Type Description a dictionary of additional parameters not related to populations or projections (keyword network in the JSON file).","title":"Saving / Loading"},{"location":"API/IO.html#saving-loading","text":"","title":"Saving / Loading"},{"location":"API/IO.html#saving-loading-the-state-of-the-network","text":"To save or load the network state you can use the following methods:","title":"Saving / loading the state of the network"},{"location":"API/IO.html#ANNarchy.save","text":"Save the current network state (parameters and variables) to a file. If the extension is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the extension is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. If the extension ends with '.gz', the data will be pickled into a binary file and compressed using gzip. Otherwise, the data will be pickled into a simple binary text file using cPickle. Warning: The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: save ( 'results/init.npz' ) save ( 'results/init.data' ) save ( 'results/init.txt.gz' ) save ( '1000_trials.mat' ) Parameters: Name Type Description Default filename filename, may contain relative or absolute path. required populations if True, population data will be saved (by default True) True projections if True, projection data will be saved (by default True) True","title":"save()"},{"location":"API/IO.html#ANNarchy.load","text":"Loads a saved state of the network. Warning: Matlab data can not be loaded. Example: load ( 'results/network.npz' ) Parameters: Name Type Description Default filename the filename with relative or absolute path. required populations if True, population data will be loaded (by default True) True projections if True, projection data will be loaded (by default True) True Please note that these functions are only usable after the call to ANNarchy.compile() .","title":"load()"},{"location":"API/IO.html#saving-loading-the-parameters-of-the-network","text":"","title":"Saving / loading the parameters of the network"},{"location":"API/IO.html#ANNarchy.save_parameters","text":"Saves the global parameters of a network (flag population for neurons, projection for synapses) to a JSON file. Parameters: Name Type Description Default filename path to the JSON file. required net_id ID of the network (default: 0, the global network). 0","title":"save_parameters()"},{"location":"API/IO.html#ANNarchy.load_parameters","text":"Loads the global parameters of a network (flag population for neurons, projection for synapses) from a JSON file. It is advised to generate the JSON file first with save_parameters() and later edit it manually. A strong restriction is that population/projection names cannot change between saving and loading. By default, they take names such as pop0 or proj2 , we advise setting explicitly a name in their constructor for readability. If you add a parameter name to the JSON file but it does not exist in te neuron/synapse, it will be silently skipped. Enable verbose=True to see which parameters are effectively changed. If you set global_only to True, you will be able to set values for non-global parameters (e.g. synapse-specific), but a single value will be loaded for all. The JSON file cannot contain arrays. If you want to save/load the value of variables after a simulation, please refer to save() or load() . Parameters: Name Type Description Default filename path to the JSON file. required global_only True if only global parameters (flags population and projection ) should be loaded, the other values are ignored. (default: True) True verbose True if the old and new values of the parameters should be printed (default: False). False net_id ID of the network (default: 0, the global network). 0 Returns: Type Description a dictionary of additional parameters not related to populations or projections (keyword network in the JSON file).","title":"load_parameters()"},{"location":"API/Logging.html","text":"Logging with tensorboard # Logging utilities based on tensorboard are provided in the module ANNarchy.extensions.tensorboard , which must be explicitely imported: from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger The main object in that module is the Logger class. Logger # Logger # Bases: object Logger class to use tensorboard to visualize ANNarchy simulations. Requires the tensorboardX package (pip install tensorboardX). The Logger class is a thin wrapper around tensorboardX.SummaryWriter, which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io/ . Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch. The extension has to be imported explictly: from ANNarchy.extensions.tensorboard import Logger The Logger class has to be closed properly at the end of the script, so it is advised to use a context: with Logger () as logger : logger . add_scalar ( \"Accuracy\" , acc , trial ) You can also make sure to close it: logger = Logger () logger . add_scalar ( \"Accuracy\" , acc , trial ) logger . close () By default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine . You can control these two elements by passing arguments to Logger() : with Logger ( logdir = \"/tmp/annarchy\" , experiment = \"trial1\" ): # logs in /tmp/annarchy/trial1 The add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc. A tag should be given to each plot. In the example above, the figure with the accuracy will be labelled \"Accuracy\" in tensorboard. You can also group plots together with tags such as \"Global performance/Accuracy\", \"Global performance/Error rate\", \"Neural activity/Population 1\", etc. After (or while) logging data within your simulation, run tensorboard in the terminal by specifying the log directory: tensorboard --logdir runs TensorboardX enqueues the data in memory before writing to disk. You can force flushing with: logger . flush () Source code in ANNarchy/extensions/tensorboard/Logger.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 class Logger ( object ): \"\"\" Logger class to use tensorboard to visualize ANNarchy simulations. Requires the `tensorboardX` package (pip install tensorboardX). The Logger class is a thin wrapper around tensorboardX.SummaryWriter, which you could also use directly. The doc is available at <https://tensorboardx.readthedocs.io/>. Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch. The extension has to be imported explictly: ```python from ANNarchy.extensions.tensorboard import Logger ``` The ``Logger`` class has to be closed properly at the end of the script, so it is advised to use a context: ```python with Logger() as logger: logger.add_scalar(\"Accuracy\", acc, trial) ``` You can also make sure to close it: ```python logger = Logger() logger.add_scalar(\"Accuracy\", acc, trial) logger.close() ``` By default, the logs will be written in a subfolder of ``./runs/`` (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ``./runs/Apr22_12-11-22_machine``. You can control these two elements by passing arguments to ``Logger()``: ```python with Logger(logdir=\"/tmp/annarchy\", experiment=\"trial1\"): # logs in /tmp/annarchy/trial1 ``` The ``add_*`` methods allow you to log various structures, such as scalars, images, histograms, figures, etc. A tag should be given to each plot. In the example above, the figure with the accuracy will be labelled \"Accuracy\" in tensorboard. You can also group plots together with tags such as \"Global performance/Accuracy\", \"Global performance/Error rate\", \"Neural activity/Population 1\", etc. After (or while) logging data within your simulation, run `tensorboard` in the terminal by specifying the log directory: ```bash tensorboard --logdir runs ``` TensorboardX enqueues the data in memory before writing to disk. You can force flushing with: ```python logger.flush() ``` \"\"\" def __init__ ( self , logdir = \"runs/\" , experiment = None ): \"\"\" :param logdir: path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\" :param experiment: name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended. \"\"\" self . logdir = logdir self . experiment = experiment # Create the logdir if it does not exist if not os . path . exists ( self . logdir ): os . makedirs ( self . logdir ) if not experiment : current_time = datetime . now () . strftime ( '%b %d _%H-%M-%S' ) self . currentlogdir = os . path . join ( self . logdir , current_time + '_' + socket . gethostname ()) else : self . currentlogdir = self . logdir + \"/\" + self . experiment print ( \"Logging in\" , self . currentlogdir ) self . _create_summary_writer () def _create_summary_writer ( self ): self . _summary = SummaryWriter ( self . currentlogdir , comment = \"\" , purge_step = None , max_queue = 10 , flush_secs = 10 , filename_suffix = '' , write_to_disk = True ) # Logging methods def add_scalar ( self , tag , value , step = None ): \"\"\" Logs a single scalar value, e.g. a success rate at various stages of learning. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) accuracy = ... logger.add_scalar(\"Accuracy\", accuracy, trial) ``` :param tag: name of the figure in tensorboard. :param value: value. :param step: time index. \"\"\" self . _summary . add_scalar ( tag = tag , scalar_value = value , global_step = step , walltime = None ) def add_scalars ( self , tag , value , step = None ): \"\"\" Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) act1 = pop.r[0] act2 = pop.r[1] logger.add_scalars( \"Accuracy\", {'First neuron': act1, 'Second neuron': act2}, trial) ``` :param tag: name of the figure in tensorboard. :param value: dictionary of values. :param step: time index. \"\"\" self . _summary . add_scalars ( main_tag = tag , tag_scalar_dict = value , global_step = step , walltime = None ) def add_image ( self , tag , img , step = None , equalize = False ): \"\"\" Logs an image. The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values. Example:: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) img = pop.r.reshape((10, 10)) logger.add_image(\"Population / Firing rate\", img, trial, equalize=True) ``` :param tag: name of the figure in tensorboard. :param img: array for the image. :param step: time index. :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array. \"\"\" if img . ndim == 2 : if equalize : img = img . astype ( np . float ) img = ( img - img . min ()) / ( img . max () - img . min ()) self . _summary . add_image ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'HW' ) elif img . ndim == 3 : if not img . shape [ 2 ] == 3 : Global . _error ( \"Logger.add_image: color images must be of shape (H, W, 3).\" ) if equalize : img = np . array ( img ) . astype ( np . float ) img = ( img - img . min ()) / ( img . max () - img . min ()) self . _summary . add_image ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'HWC' ) else : Global . _error ( \"Logger.add_image: images must be of shape (H, W) or (H, W, 3).\" ) def add_images ( self , tag , img , step = None , equalize = False , equalize_per_image = False ): \"\"\" Logs a set of images (e.g. receptive fields). The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True) ``` :param tag: name of the figure in tensorboard. :param img: array for the images. :param step: time index. :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array. :param equalize_per_image: whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False. \"\"\" if img . ndim == 3 : img = np . expand_dims ( img , axis = 3 ) if equalize : img = np . array ( img ) . astype ( np . float ) if not equalize_per_image : img = ( img - img . min ()) / ( img . max () - img . min ()) else : for i in range ( img . shape [ 0 ]): img [ i , ... ] = ( img [ i , ... ] - img [ i , ... ] . min ()) / ( img [ i , ... ] . max () - img [ i , ... ] . min ()) self . _summary . add_images ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'NHWC' ) def add_parameters ( self , params , metrics ): \"\"\" Logs parameters of a simulation. This should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc. Example: ```python with Logger() as logger: # ... logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy}) ``` :param params: dictionary of parameters. :param metrics: dictionary of metrics. \"\"\" self . _summary . add_hparams ( params , metrics ) def add_histogram ( self , tag , hist , step = None ): \"\"\" Logs an histogram. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) weights= proj.w.flatten() logger.add_histogram(\"Weight distribution\", weights, trial) ``` :param tag: name of the figure in tensorboard. :param hist: a list or 1D numpy array of values. :param step: time index. \"\"\" self . _summary . add_histogram ( tag , hist , step ) def add_figure ( self , tag , figure , step = None , close = True ): \"\"\" Logs a Matplotlib figure. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) fig = plt.figure() plt.plot(pop.r) logger.add_figure(\"Activity\", fig, trial) ``` :param tag: name of the image in tensorboard. :param figure: a list or 1D numpy array of values. :param step: time index. :param close: whether the logger will close the figure when done (default: True). \"\"\" import matplotlib.pyplot as plt import matplotlib.backends.backend_agg as plt_backend_agg canvas = plt_backend_agg . FigureCanvasAgg ( figure ) canvas . draw () data = np . frombuffer ( canvas . buffer_rgba (), dtype = np . uint8 ) w , h = figure . canvas . get_width_height () image_hwc = data . reshape ([ h , w , 4 ])[:, :, 0 : 3 ] image_chw = np . moveaxis ( image_hwc , source = 2 , destination = 0 ) if close : plt . close ( figure ) self . _summary . add_image ( tag , image_chw , step ) # Resource management def flush ( self ): \"Forces the logged data to be flushed to disk.\" self . _summary . flush () def close ( self ): \"Closes the logger.\" self . _summary . close () def __enter__ ( self ): return self def __exit__ ( self , exc_type , exc_val , exc_tb ): self . close () __init__ ( logdir = 'runs/' , experiment = None ) # Parameters: Name Type Description Default logdir path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\" 'runs/' experiment name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended. None Source code in ANNarchy/extensions/tensorboard/Logger.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def __init__ ( self , logdir = \"runs/\" , experiment = None ): \"\"\" :param logdir: path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\" :param experiment: name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended. \"\"\" self . logdir = logdir self . experiment = experiment # Create the logdir if it does not exist if not os . path . exists ( self . logdir ): os . makedirs ( self . logdir ) if not experiment : current_time = datetime . now () . strftime ( '%b %d _%H-%M-%S' ) self . currentlogdir = os . path . join ( self . logdir , current_time + '_' + socket . gethostname ()) else : self . currentlogdir = self . logdir + \"/\" + self . experiment print ( \"Logging in\" , self . currentlogdir ) self . _create_summary_writer () add_figure ( tag , figure , step = None , close = True ) # Logs a Matplotlib figure. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) fig = plt . figure () plt . plot ( pop . r ) logger . add_figure ( \"Activity\" , fig , trial ) Parameters: Name Type Description Default tag name of the image in tensorboard. required figure a list or 1D numpy array of values. required step time index. None close whether the logger will close the figure when done (default: True). True Source code in ANNarchy/extensions/tensorboard/Logger.py 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 def add_figure ( self , tag , figure , step = None , close = True ): \"\"\" Logs a Matplotlib figure. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) fig = plt.figure() plt.plot(pop.r) logger.add_figure(\"Activity\", fig, trial) ``` :param tag: name of the image in tensorboard. :param figure: a list or 1D numpy array of values. :param step: time index. :param close: whether the logger will close the figure when done (default: True). \"\"\" import matplotlib.pyplot as plt import matplotlib.backends.backend_agg as plt_backend_agg canvas = plt_backend_agg . FigureCanvasAgg ( figure ) canvas . draw () data = np . frombuffer ( canvas . buffer_rgba (), dtype = np . uint8 ) w , h = figure . canvas . get_width_height () image_hwc = data . reshape ([ h , w , 4 ])[:, :, 0 : 3 ] image_chw = np . moveaxis ( image_hwc , source = 2 , destination = 0 ) if close : plt . close ( figure ) self . _summary . add_image ( tag , image_chw , step ) add_histogram ( tag , hist , step = None ) # Logs an histogram. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . flatten () logger . add_histogram ( \"Weight distribution\" , weights , trial ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required hist a list or 1D numpy array of values. required step time index. None Source code in ANNarchy/extensions/tensorboard/Logger.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def add_histogram ( self , tag , hist , step = None ): \"\"\" Logs an histogram. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) weights= proj.w.flatten() logger.add_histogram(\"Weight distribution\", weights, trial) ``` :param tag: name of the figure in tensorboard. :param hist: a list or 1D numpy array of values. :param step: time index. \"\"\" self . _summary . add_histogram ( tag , hist , step ) add_image ( tag , img , step = None , equalize = False ) # Logs an image. The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values. Example:: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) img = pop . r . reshape (( 10 , 10 )) logger . add_image ( \"Population / Firing rate\" , img , trial , equalize = True ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required img array for the image. required step time index. None equalize rescales the pixels between 0 and 1 using the min and max values of the array. False Source code in ANNarchy/extensions/tensorboard/Logger.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def add_image ( self , tag , img , step = None , equalize = False ): \"\"\" Logs an image. The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values. Example:: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) img = pop.r.reshape((10, 10)) logger.add_image(\"Population / Firing rate\", img, trial, equalize=True) ``` :param tag: name of the figure in tensorboard. :param img: array for the image. :param step: time index. :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array. \"\"\" if img . ndim == 2 : if equalize : img = img . astype ( np . float ) img = ( img - img . min ()) / ( img . max () - img . min ()) self . _summary . add_image ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'HW' ) elif img . ndim == 3 : if not img . shape [ 2 ] == 3 : Global . _error ( \"Logger.add_image: color images must be of shape (H, W, 3).\" ) if equalize : img = np . array ( img ) . astype ( np . float ) img = ( img - img . min ()) / ( img . max () - img . min ()) self . _summary . add_image ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'HWC' ) else : Global . _error ( \"Logger.add_image: images must be of shape (H, W) or (H, W, 3).\" ) add_images ( tag , img , step = None , equalize = False , equalize_per_image = False ) # Logs a set of images (e.g. receptive fields). The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . reshape ( 100 , 10 , 10 ) # 100 post neurons, 10*10 pre neurons logger . add_images ( \"Projection/Receptive fields\" , weights , trial , equalize = True ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required img array for the images. required step time index. None equalize rescales the pixels between 0 and 1 using the min and max values of the array. False equalize_per_image whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False. False Source code in ANNarchy/extensions/tensorboard/Logger.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def add_images ( self , tag , img , step = None , equalize = False , equalize_per_image = False ): \"\"\" Logs a set of images (e.g. receptive fields). The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True) ``` :param tag: name of the figure in tensorboard. :param img: array for the images. :param step: time index. :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array. :param equalize_per_image: whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False. \"\"\" if img . ndim == 3 : img = np . expand_dims ( img , axis = 3 ) if equalize : img = np . array ( img ) . astype ( np . float ) if not equalize_per_image : img = ( img - img . min ()) / ( img . max () - img . min ()) else : for i in range ( img . shape [ 0 ]): img [ i , ... ] = ( img [ i , ... ] - img [ i , ... ] . min ()) / ( img [ i , ... ] . max () - img [ i , ... ] . min ()) self . _summary . add_images ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'NHWC' ) add_parameters ( params , metrics ) # Logs parameters of a simulation. This should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc. Example: with Logger () as logger : # ... logger . add_parameters ({ 'learning_rate' : lr , 'tau' : tau }, { 'accuracy' : accuracy }) Parameters: Name Type Description Default params dictionary of parameters. required metrics dictionary of metrics. required Source code in ANNarchy/extensions/tensorboard/Logger.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def add_parameters ( self , params , metrics ): \"\"\" Logs parameters of a simulation. This should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc. Example: ```python with Logger() as logger: # ... logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy}) ``` :param params: dictionary of parameters. :param metrics: dictionary of metrics. \"\"\" self . _summary . add_hparams ( params , metrics ) add_scalar ( tag , value , step = None ) # Logs a single scalar value, e.g. a success rate at various stages of learning. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) accuracy = ... logger . add_scalar ( \"Accuracy\" , accuracy , trial ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required value value. required step time index. None Source code in ANNarchy/extensions/tensorboard/Logger.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def add_scalar ( self , tag , value , step = None ): \"\"\" Logs a single scalar value, e.g. a success rate at various stages of learning. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) accuracy = ... logger.add_scalar(\"Accuracy\", accuracy, trial) ``` :param tag: name of the figure in tensorboard. :param value: value. :param step: time index. \"\"\" self . _summary . add_scalar ( tag = tag , scalar_value = value , global_step = step , walltime = None ) add_scalars ( tag , value , step = None ) # Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) act1 = pop . r [ 0 ] act2 = pop . r [ 1 ] logger . add_scalars ( \"Accuracy\" , { 'First neuron' : act1 , 'Second neuron' : act2 }, trial ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required value dictionary of values. required step time index. None Source code in ANNarchy/extensions/tensorboard/Logger.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def add_scalars ( self , tag , value , step = None ): \"\"\" Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) act1 = pop.r[0] act2 = pop.r[1] logger.add_scalars( \"Accuracy\", {'First neuron': act1, 'Second neuron': act2}, trial) ``` :param tag: name of the figure in tensorboard. :param value: dictionary of values. :param step: time index. \"\"\" self . _summary . add_scalars ( main_tag = tag , tag_scalar_dict = value , global_step = step , walltime = None ) close () # Closes the logger. Source code in ANNarchy/extensions/tensorboard/Logger.py 326 327 328 def close ( self ): \"Closes the logger.\" self . _summary . close () flush () # Forces the logged data to be flushed to disk. Source code in ANNarchy/extensions/tensorboard/Logger.py 322 323 324 def flush ( self ): \"Forces the logged data to be flushed to disk.\" self . _summary . flush ()","title":"Logging with tensorboard"},{"location":"API/Logging.html#logging-with-tensorboard","text":"Logging utilities based on tensorboard are provided in the module ANNarchy.extensions.tensorboard , which must be explicitely imported: from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger The main object in that module is the Logger class.","title":"Logging with tensorboard"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger","text":"","title":"Logger"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger","text":"Bases: object Logger class to use tensorboard to visualize ANNarchy simulations. Requires the tensorboardX package (pip install tensorboardX). The Logger class is a thin wrapper around tensorboardX.SummaryWriter, which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io/ . Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch. The extension has to be imported explictly: from ANNarchy.extensions.tensorboard import Logger The Logger class has to be closed properly at the end of the script, so it is advised to use a context: with Logger () as logger : logger . add_scalar ( \"Accuracy\" , acc , trial ) You can also make sure to close it: logger = Logger () logger . add_scalar ( \"Accuracy\" , acc , trial ) logger . close () By default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine . You can control these two elements by passing arguments to Logger() : with Logger ( logdir = \"/tmp/annarchy\" , experiment = \"trial1\" ): # logs in /tmp/annarchy/trial1 The add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc. A tag should be given to each plot. In the example above, the figure with the accuracy will be labelled \"Accuracy\" in tensorboard. You can also group plots together with tags such as \"Global performance/Accuracy\", \"Global performance/Error rate\", \"Neural activity/Population 1\", etc. After (or while) logging data within your simulation, run tensorboard in the terminal by specifying the log directory: tensorboard --logdir runs TensorboardX enqueues the data in memory before writing to disk. You can force flushing with: logger . flush () Source code in ANNarchy/extensions/tensorboard/Logger.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 class Logger ( object ): \"\"\" Logger class to use tensorboard to visualize ANNarchy simulations. Requires the `tensorboardX` package (pip install tensorboardX). The Logger class is a thin wrapper around tensorboardX.SummaryWriter, which you could also use directly. The doc is available at <https://tensorboardx.readthedocs.io/>. Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch. The extension has to be imported explictly: ```python from ANNarchy.extensions.tensorboard import Logger ``` The ``Logger`` class has to be closed properly at the end of the script, so it is advised to use a context: ```python with Logger() as logger: logger.add_scalar(\"Accuracy\", acc, trial) ``` You can also make sure to close it: ```python logger = Logger() logger.add_scalar(\"Accuracy\", acc, trial) logger.close() ``` By default, the logs will be written in a subfolder of ``./runs/`` (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ``./runs/Apr22_12-11-22_machine``. You can control these two elements by passing arguments to ``Logger()``: ```python with Logger(logdir=\"/tmp/annarchy\", experiment=\"trial1\"): # logs in /tmp/annarchy/trial1 ``` The ``add_*`` methods allow you to log various structures, such as scalars, images, histograms, figures, etc. A tag should be given to each plot. In the example above, the figure with the accuracy will be labelled \"Accuracy\" in tensorboard. You can also group plots together with tags such as \"Global performance/Accuracy\", \"Global performance/Error rate\", \"Neural activity/Population 1\", etc. After (or while) logging data within your simulation, run `tensorboard` in the terminal by specifying the log directory: ```bash tensorboard --logdir runs ``` TensorboardX enqueues the data in memory before writing to disk. You can force flushing with: ```python logger.flush() ``` \"\"\" def __init__ ( self , logdir = \"runs/\" , experiment = None ): \"\"\" :param logdir: path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\" :param experiment: name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended. \"\"\" self . logdir = logdir self . experiment = experiment # Create the logdir if it does not exist if not os . path . exists ( self . logdir ): os . makedirs ( self . logdir ) if not experiment : current_time = datetime . now () . strftime ( '%b %d _%H-%M-%S' ) self . currentlogdir = os . path . join ( self . logdir , current_time + '_' + socket . gethostname ()) else : self . currentlogdir = self . logdir + \"/\" + self . experiment print ( \"Logging in\" , self . currentlogdir ) self . _create_summary_writer () def _create_summary_writer ( self ): self . _summary = SummaryWriter ( self . currentlogdir , comment = \"\" , purge_step = None , max_queue = 10 , flush_secs = 10 , filename_suffix = '' , write_to_disk = True ) # Logging methods def add_scalar ( self , tag , value , step = None ): \"\"\" Logs a single scalar value, e.g. a success rate at various stages of learning. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) accuracy = ... logger.add_scalar(\"Accuracy\", accuracy, trial) ``` :param tag: name of the figure in tensorboard. :param value: value. :param step: time index. \"\"\" self . _summary . add_scalar ( tag = tag , scalar_value = value , global_step = step , walltime = None ) def add_scalars ( self , tag , value , step = None ): \"\"\" Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) act1 = pop.r[0] act2 = pop.r[1] logger.add_scalars( \"Accuracy\", {'First neuron': act1, 'Second neuron': act2}, trial) ``` :param tag: name of the figure in tensorboard. :param value: dictionary of values. :param step: time index. \"\"\" self . _summary . add_scalars ( main_tag = tag , tag_scalar_dict = value , global_step = step , walltime = None ) def add_image ( self , tag , img , step = None , equalize = False ): \"\"\" Logs an image. The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values. Example:: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) img = pop.r.reshape((10, 10)) logger.add_image(\"Population / Firing rate\", img, trial, equalize=True) ``` :param tag: name of the figure in tensorboard. :param img: array for the image. :param step: time index. :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array. \"\"\" if img . ndim == 2 : if equalize : img = img . astype ( np . float ) img = ( img - img . min ()) / ( img . max () - img . min ()) self . _summary . add_image ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'HW' ) elif img . ndim == 3 : if not img . shape [ 2 ] == 3 : Global . _error ( \"Logger.add_image: color images must be of shape (H, W, 3).\" ) if equalize : img = np . array ( img ) . astype ( np . float ) img = ( img - img . min ()) / ( img . max () - img . min ()) self . _summary . add_image ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'HWC' ) else : Global . _error ( \"Logger.add_image: images must be of shape (H, W) or (H, W, 3).\" ) def add_images ( self , tag , img , step = None , equalize = False , equalize_per_image = False ): \"\"\" Logs a set of images (e.g. receptive fields). The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True) ``` :param tag: name of the figure in tensorboard. :param img: array for the images. :param step: time index. :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array. :param equalize_per_image: whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False. \"\"\" if img . ndim == 3 : img = np . expand_dims ( img , axis = 3 ) if equalize : img = np . array ( img ) . astype ( np . float ) if not equalize_per_image : img = ( img - img . min ()) / ( img . max () - img . min ()) else : for i in range ( img . shape [ 0 ]): img [ i , ... ] = ( img [ i , ... ] - img [ i , ... ] . min ()) / ( img [ i , ... ] . max () - img [ i , ... ] . min ()) self . _summary . add_images ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'NHWC' ) def add_parameters ( self , params , metrics ): \"\"\" Logs parameters of a simulation. This should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc. Example: ```python with Logger() as logger: # ... logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy}) ``` :param params: dictionary of parameters. :param metrics: dictionary of metrics. \"\"\" self . _summary . add_hparams ( params , metrics ) def add_histogram ( self , tag , hist , step = None ): \"\"\" Logs an histogram. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) weights= proj.w.flatten() logger.add_histogram(\"Weight distribution\", weights, trial) ``` :param tag: name of the figure in tensorboard. :param hist: a list or 1D numpy array of values. :param step: time index. \"\"\" self . _summary . add_histogram ( tag , hist , step ) def add_figure ( self , tag , figure , step = None , close = True ): \"\"\" Logs a Matplotlib figure. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) fig = plt.figure() plt.plot(pop.r) logger.add_figure(\"Activity\", fig, trial) ``` :param tag: name of the image in tensorboard. :param figure: a list or 1D numpy array of values. :param step: time index. :param close: whether the logger will close the figure when done (default: True). \"\"\" import matplotlib.pyplot as plt import matplotlib.backends.backend_agg as plt_backend_agg canvas = plt_backend_agg . FigureCanvasAgg ( figure ) canvas . draw () data = np . frombuffer ( canvas . buffer_rgba (), dtype = np . uint8 ) w , h = figure . canvas . get_width_height () image_hwc = data . reshape ([ h , w , 4 ])[:, :, 0 : 3 ] image_chw = np . moveaxis ( image_hwc , source = 2 , destination = 0 ) if close : plt . close ( figure ) self . _summary . add_image ( tag , image_chw , step ) # Resource management def flush ( self ): \"Forces the logged data to be flushed to disk.\" self . _summary . flush () def close ( self ): \"Closes the logger.\" self . _summary . close () def __enter__ ( self ): return self def __exit__ ( self , exc_type , exc_val , exc_tb ): self . close ()","title":"Logger"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.__init__","text":"Parameters: Name Type Description Default logdir path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\" 'runs/' experiment name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended. None Source code in ANNarchy/extensions/tensorboard/Logger.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def __init__ ( self , logdir = \"runs/\" , experiment = None ): \"\"\" :param logdir: path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\" :param experiment: name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended. \"\"\" self . logdir = logdir self . experiment = experiment # Create the logdir if it does not exist if not os . path . exists ( self . logdir ): os . makedirs ( self . logdir ) if not experiment : current_time = datetime . now () . strftime ( '%b %d _%H-%M-%S' ) self . currentlogdir = os . path . join ( self . logdir , current_time + '_' + socket . gethostname ()) else : self . currentlogdir = self . logdir + \"/\" + self . experiment print ( \"Logging in\" , self . currentlogdir ) self . _create_summary_writer ()","title":"__init__()"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_figure","text":"Logs a Matplotlib figure. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) fig = plt . figure () plt . plot ( pop . r ) logger . add_figure ( \"Activity\" , fig , trial ) Parameters: Name Type Description Default tag name of the image in tensorboard. required figure a list or 1D numpy array of values. required step time index. None close whether the logger will close the figure when done (default: True). True Source code in ANNarchy/extensions/tensorboard/Logger.py 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 def add_figure ( self , tag , figure , step = None , close = True ): \"\"\" Logs a Matplotlib figure. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) fig = plt.figure() plt.plot(pop.r) logger.add_figure(\"Activity\", fig, trial) ``` :param tag: name of the image in tensorboard. :param figure: a list or 1D numpy array of values. :param step: time index. :param close: whether the logger will close the figure when done (default: True). \"\"\" import matplotlib.pyplot as plt import matplotlib.backends.backend_agg as plt_backend_agg canvas = plt_backend_agg . FigureCanvasAgg ( figure ) canvas . draw () data = np . frombuffer ( canvas . buffer_rgba (), dtype = np . uint8 ) w , h = figure . canvas . get_width_height () image_hwc = data . reshape ([ h , w , 4 ])[:, :, 0 : 3 ] image_chw = np . moveaxis ( image_hwc , source = 2 , destination = 0 ) if close : plt . close ( figure ) self . _summary . add_image ( tag , image_chw , step )","title":"add_figure()"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_histogram","text":"Logs an histogram. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . flatten () logger . add_histogram ( \"Weight distribution\" , weights , trial ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required hist a list or 1D numpy array of values. required step time index. None Source code in ANNarchy/extensions/tensorboard/Logger.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def add_histogram ( self , tag , hist , step = None ): \"\"\" Logs an histogram. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) weights= proj.w.flatten() logger.add_histogram(\"Weight distribution\", weights, trial) ``` :param tag: name of the figure in tensorboard. :param hist: a list or 1D numpy array of values. :param step: time index. \"\"\" self . _summary . add_histogram ( tag , hist , step )","title":"add_histogram()"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_image","text":"Logs an image. The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values. Example:: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) img = pop . r . reshape (( 10 , 10 )) logger . add_image ( \"Population / Firing rate\" , img , trial , equalize = True ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required img array for the image. required step time index. None equalize rescales the pixels between 0 and 1 using the min and max values of the array. False Source code in ANNarchy/extensions/tensorboard/Logger.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def add_image ( self , tag , img , step = None , equalize = False ): \"\"\" Logs an image. The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values. Example:: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) img = pop.r.reshape((10, 10)) logger.add_image(\"Population / Firing rate\", img, trial, equalize=True) ``` :param tag: name of the figure in tensorboard. :param img: array for the image. :param step: time index. :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array. \"\"\" if img . ndim == 2 : if equalize : img = img . astype ( np . float ) img = ( img - img . min ()) / ( img . max () - img . min ()) self . _summary . add_image ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'HW' ) elif img . ndim == 3 : if not img . shape [ 2 ] == 3 : Global . _error ( \"Logger.add_image: color images must be of shape (H, W, 3).\" ) if equalize : img = np . array ( img ) . astype ( np . float ) img = ( img - img . min ()) / ( img . max () - img . min ()) self . _summary . add_image ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'HWC' ) else : Global . _error ( \"Logger.add_image: images must be of shape (H, W) or (H, W, 3).\" )","title":"add_image()"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_images","text":"Logs a set of images (e.g. receptive fields). The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter equalize forces the values to be between 0 and 1 by equalizing using the min/max values. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . reshape ( 100 , 10 , 10 ) # 100 post neurons, 10*10 pre neurons logger . add_images ( \"Projection/Receptive fields\" , weights , trial , equalize = True ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required img array for the images. required step time index. None equalize rescales the pixels between 0 and 1 using the min and max values of the array. False equalize_per_image whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False. False Source code in ANNarchy/extensions/tensorboard/Logger.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def add_images ( self , tag , img , step = None , equalize = False , equalize_per_image = False ): \"\"\" Logs a set of images (e.g. receptive fields). The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True) ``` :param tag: name of the figure in tensorboard. :param img: array for the images. :param step: time index. :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array. :param equalize_per_image: whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False. \"\"\" if img . ndim == 3 : img = np . expand_dims ( img , axis = 3 ) if equalize : img = np . array ( img ) . astype ( np . float ) if not equalize_per_image : img = ( img - img . min ()) / ( img . max () - img . min ()) else : for i in range ( img . shape [ 0 ]): img [ i , ... ] = ( img [ i , ... ] - img [ i , ... ] . min ()) / ( img [ i , ... ] . max () - img [ i , ... ] . min ()) self . _summary . add_images ( tag = tag , img_tensor = img , global_step = step , walltime = None , dataformats = 'NHWC' )","title":"add_images()"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_parameters","text":"Logs parameters of a simulation. This should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc. Example: with Logger () as logger : # ... logger . add_parameters ({ 'learning_rate' : lr , 'tau' : tau }, { 'accuracy' : accuracy }) Parameters: Name Type Description Default params dictionary of parameters. required metrics dictionary of metrics. required Source code in ANNarchy/extensions/tensorboard/Logger.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def add_parameters ( self , params , metrics ): \"\"\" Logs parameters of a simulation. This should be run only once per simulation, generally at the end. This allows to compare different runs of the same network using different parameter values and study how they influence the global output metrics, such as accuracy, error rate, reaction speed, etc. Example: ```python with Logger() as logger: # ... logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy}) ``` :param params: dictionary of parameters. :param metrics: dictionary of metrics. \"\"\" self . _summary . add_hparams ( params , metrics )","title":"add_parameters()"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_scalar","text":"Logs a single scalar value, e.g. a success rate at various stages of learning. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) accuracy = ... logger . add_scalar ( \"Accuracy\" , accuracy , trial ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required value value. required step time index. None Source code in ANNarchy/extensions/tensorboard/Logger.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def add_scalar ( self , tag , value , step = None ): \"\"\" Logs a single scalar value, e.g. a success rate at various stages of learning. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) accuracy = ... logger.add_scalar(\"Accuracy\", accuracy, trial) ``` :param tag: name of the figure in tensorboard. :param value: value. :param step: time index. \"\"\" self . _summary . add_scalar ( tag = tag , scalar_value = value , global_step = step , walltime = None )","title":"add_scalar()"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_scalars","text":"Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities. Example: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) act1 = pop . r [ 0 ] act2 = pop . r [ 1 ] logger . add_scalars ( \"Accuracy\" , { 'First neuron' : act1 , 'Second neuron' : act2 }, trial ) Parameters: Name Type Description Default tag name of the figure in tensorboard. required value dictionary of values. required step time index. None Source code in ANNarchy/extensions/tensorboard/Logger.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def add_scalars ( self , tag , value , step = None ): \"\"\" Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities. Example: ```python with Logger() as logger: for trial in range(100): simulate(1000.0) act1 = pop.r[0] act2 = pop.r[1] logger.add_scalars( \"Accuracy\", {'First neuron': act1, 'Second neuron': act2}, trial) ``` :param tag: name of the figure in tensorboard. :param value: dictionary of values. :param step: time index. \"\"\" self . _summary . add_scalars ( main_tag = tag , tag_scalar_dict = value , global_step = step , walltime = None )","title":"add_scalars()"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.close","text":"Closes the logger. Source code in ANNarchy/extensions/tensorboard/Logger.py 326 327 328 def close ( self ): \"Closes the logger.\" self . _summary . close ()","title":"close()"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.flush","text":"Forces the logged data to be flushed to disk. Source code in ANNarchy/extensions/tensorboard/Logger.py 322 323 324 def flush ( self ): \"Forces the logged data to be flushed to disk.\" self . _summary . flush ()","title":"flush()"},{"location":"API/Monitor.html","text":"Monitoring # Recording of neural or synaptic variables during the simulation is possible through a Monitor object. Monitor # Monitoring class allowing to record easily parameters or variables from Population, PopulationView, Dendrite or Projection objects. Example: m = Monitor ( pop , [ 'g_exc' , 'v' , 'spike' ], period = 10.0 ) It is also possible to record the sum of inputs to each neuron in a rate-coded population: m = Monitor ( pop , [ 'sum(exc)' , 'r' ]) __doc__ = \" \\n Monitoring class allowing to record easily parameters or variables from Population, PopulationView, Dendrite or Projection objects. \\n \\n Example: \\n\\n ```python \\n m = Monitor(pop, ['g_exc', 'v', 'spike'], period=10.0) \\n ``` \\n\\n It is also possible to record the sum of inputs to each neuron in a rate-coded population: \\n\\n ```python \\n m = Monitor(pop, ['sum(exc)', 'r']) \\n ``` \\n\\n \" # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.core.Monitor' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'Monitor' objects > # list of weak references to the object (if defined) period property # Period of recording in ms period_offset property # Shift of moment of time of recording in ms within a period variables property # Returns a copy of the current variable list. __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init__ ( obj , variables = [], period = None , period_offset = None , start = True , net_id = 0 ) # Parameters: Name Type Description Default obj object to monitor. Must be a Population, PopulationView, Dendrite or Projection object. required variables single variable name or list of variable names to record (default: []). [] period delay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View). None period_offset determine the moment in ms of recording within the period (default 0). Must be smaller than period . None start defines if the recording should start immediately (default: True). If not, you should later start the recordings with the start() method. True __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __repr__ () method descriptor # Return repr(self). __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __str__ () method descriptor # Return str(self). __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). get ( variables = None , keep = False , reshape = False , force_dict = False ) # Returns the recorded variables as a Numpy array (first dimension is time, second is neuron index). If a single variable name is provided, the recorded values for this variable are directly returned. If a list is provided or the argument left empty, a dictionary with all recorded variables is returned. The spike variable of a population will be returned as a dictionary of lists, where the spike times (in steps) for each recorded neurons are returned. Parameters: Name Type Description Default variables (list of) variables. By default, a dictionary with all variables is returned. None keep defines if the content in memory for each variable should be kept (default: False). False reshape transforms the second axis of the array to match the population's geometry (default: False). False histogram ( spikes = None , bins = None ) # Returns a histogram for the recorded spikes in the population. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) histo = m . histogram () plt . plot ( histo ) or: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) histo = m . histogram ( spikes ) plt . plot ( histo ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None bins the bin size in ms (default: dt). None mean_fr ( spikes = None ) # Computes the mean firing rate in the population during the recordings. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) fr = m . mean_fr () or: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) fr = m . mean_fr ( spikes ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None pause () # Pauses the recordings. population_rate ( spikes = None , smooth = 0.0 ) # Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons. This method is faster than calling smoothed_rate and then averaging. The first axis is the neuron index, the second is time. If spikes is left empty, get('spike') will be called. Beware: this erases the data from memory. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) r = m . population_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . None smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0 raster_plot ( spikes = None ) # Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spike_times , spike_ranks = m . raster_plot () plt . plot ( spike_times , spike_ranks , '.' ) or: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) spike_times , spike_ranks = m . raster_plot ( spikes ) plt . plot ( spike_times , spike_ranks , '.' ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None reset () # Reset the monitor to its initial state. resume () # Resumes the recordings. size_in_bytes () # Get the size of allocated memory on C++ side. Please note, this is only valid if compile() was invoked. Returns: Type Description size in bytes of all allocated C++ data. smoothed_rate ( spikes = None , smooth = 0.0 ) # Computes the smoothed firing rate of the recorded spiking neurons. The first axis is the neuron index, the second is time. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) r = m . smoothed_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0 start ( variables = None , period = None ) # Starts recording the variables. It is called automatically after compile() if the flag start was not passed to the constructor. Parameters: Name Type Description Default variables single variable name or list of variable names to start recording (default: the variables argument passed to the constructor). None period delay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View). None stop () # Stops the recording. Warning: This will delete the content of the C++ object and all data not previously retrieved is lost. times ( variables = None ) # Returns the start and stop times (in ms) of the recorded variables. It should only be called after a call to get() , so that it describes when the variables have been recorded. Parameters: Name Type Description Default variables (list of) variables. By default, the times for all variables is returned. None raster_plot ( spikes ) # Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) spike_times , spike_ranks = raster_plot ( spikes ) plt . plot ( spike_times , spike_ranks , '.' ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required histogram ( spikes , bins = None ) # Returns a histogram for the recorded spikes in the population. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) histo = histogram ( spikes ) plt . plot ( histo ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required bins the bin size in ms (default: dt). None mean_fr ( spikes , duration = None ) # Computes the mean firing rate in the population during the recordings. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) fr = mean_fr ( spikes ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required duration duration of the recordings. By default, the mean firing rate is computed between the first and last spikes of the recordings. None smoothed_rate ( spikes , smooth = 0.0 ) # Computes the smoothed firing rate of the recorded spiking neurons. The first axis is the neuron index, the second is time. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) r = smoothed_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. required smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0 population_rate ( spikes , smooth = 0.0 ) # Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons. This method is faster than calling smoothed_rate and then averaging. The first axis is the neuron index, the second is time. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) r = population_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0","title":"Monitoring"},{"location":"API/Monitor.html#monitoring","text":"Recording of neural or synaptic variables during the simulation is possible through a Monitor object.","title":"Monitoring"},{"location":"API/Monitor.html#ANNarchy.Monitor","text":"Monitoring class allowing to record easily parameters or variables from Population, PopulationView, Dendrite or Projection objects. Example: m = Monitor ( pop , [ 'g_exc' , 'v' , 'spike' ], period = 10.0 ) It is also possible to record the sum of inputs to each neuron in a rate-coded population: m = Monitor ( pop , [ 'sum(exc)' , 'r' ])","title":"Monitor"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.period","text":"Period of recording in ms","title":"period"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.period_offset","text":"Shift of moment of time of recording in ms within a period","title":"period_offset"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.variables","text":"Returns a copy of the current variable list.","title":"variables"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__init__","text":"Parameters: Name Type Description Default obj object to monitor. Must be a Population, PopulationView, Dendrite or Projection object. required variables single variable name or list of variable names to record (default: []). [] period delay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View). None period_offset determine the moment in ms of recording within the period (default 0). Must be smaller than period . None start defines if the recording should start immediately (default: True). If not, you should later start the recordings with the start() method. True","title":"__init__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__repr__","text":"Return repr(self).","title":"__repr__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__str__","text":"Return str(self).","title":"__str__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.get","text":"Returns the recorded variables as a Numpy array (first dimension is time, second is neuron index). If a single variable name is provided, the recorded values for this variable are directly returned. If a list is provided or the argument left empty, a dictionary with all recorded variables is returned. The spike variable of a population will be returned as a dictionary of lists, where the spike times (in steps) for each recorded neurons are returned. Parameters: Name Type Description Default variables (list of) variables. By default, a dictionary with all variables is returned. None keep defines if the content in memory for each variable should be kept (default: False). False reshape transforms the second axis of the array to match the population's geometry (default: False). False","title":"get()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.histogram","text":"Returns a histogram for the recorded spikes in the population. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) histo = m . histogram () plt . plot ( histo ) or: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) histo = m . histogram ( spikes ) plt . plot ( histo ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None bins the bin size in ms (default: dt). None","title":"histogram()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.mean_fr","text":"Computes the mean firing rate in the population during the recordings. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) fr = m . mean_fr () or: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) fr = m . mean_fr ( spikes ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None","title":"mean_fr()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.pause","text":"Pauses the recordings.","title":"pause()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.population_rate","text":"Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons. This method is faster than calling smoothed_rate and then averaging. The first axis is the neuron index, the second is time. If spikes is left empty, get('spike') will be called. Beware: this erases the data from memory. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) r = m . population_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . None smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0","title":"population_rate()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.raster_plot","text":"Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spike_times , spike_ranks = m . raster_plot () plt . plot ( spike_times , spike_ranks , '.' ) or: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) spike_times , spike_ranks = m . raster_plot ( spikes ) plt . plot ( spike_times , spike_ranks , '.' ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None","title":"raster_plot()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.reset","text":"Reset the monitor to its initial state.","title":"reset()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.resume","text":"Resumes the recordings.","title":"resume()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.size_in_bytes","text":"Get the size of allocated memory on C++ side. Please note, this is only valid if compile() was invoked. Returns: Type Description size in bytes of all allocated C++ data.","title":"size_in_bytes()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.smoothed_rate","text":"Computes the smoothed firing rate of the recorded spiking neurons. The first axis is the neuron index, the second is time. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) r = m . smoothed_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. None smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0","title":"smoothed_rate()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.start","text":"Starts recording the variables. It is called automatically after compile() if the flag start was not passed to the constructor. Parameters: Name Type Description Default variables single variable name or list of variable names to start recording (default: the variables argument passed to the constructor). None period delay in ms between two recording (default: dt). Not valid for the spike variable of a Population(View). None","title":"start()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.stop","text":"Stops the recording. Warning: This will delete the content of the C++ object and all data not previously retrieved is lost.","title":"stop()"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.times","text":"Returns the start and stop times (in ms) of the recorded variables. It should only be called after a call to get() , so that it describes when the variables have been recorded. Parameters: Name Type Description Default variables (list of) variables. By default, the times for all variables is returned. None","title":"times()"},{"location":"API/Monitor.html#ANNarchy.raster_plot","text":"Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) spike_times , spike_ranks = raster_plot ( spikes ) plt . plot ( spike_times , spike_ranks , '.' ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required","title":"raster_plot()"},{"location":"API/Monitor.html#ANNarchy.histogram","text":"Returns a histogram for the recorded spikes in the population. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) histo = histogram ( spikes ) plt . plot ( histo ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required bins the bin size in ms (default: dt). None","title":"histogram()"},{"location":"API/Monitor.html#ANNarchy.mean_fr","text":"Computes the mean firing rate in the population during the recordings. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) fr = mean_fr ( spikes ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required duration duration of the recordings. By default, the mean firing rate is computed between the first and last spikes of the recordings. None","title":"mean_fr()"},{"location":"API/Monitor.html#ANNarchy.smoothed_rate","text":"Computes the smoothed firing rate of the recorded spiking neurons. The first axis is the neuron index, the second is time. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) r = smoothed_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . If left empty, get('spike') will be called. Beware: this erases the data from memory. required smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0","title":"smoothed_rate()"},{"location":"API/Monitor.html#ANNarchy.population_rate","text":"Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons. This method is faster than calling smoothed_rate and then averaging. The first axis is the neuron index, the second is time. Example: m = Monitor ( P [: 1000 ], 'spike' ) simulate ( 1000.0 ) spikes = m . get ( 'spike' ) r = population_rate ( smooth = 100. ) Parameters: Name Type Description Default spikes the dictionary of spikes returned by get('spike') . required smooth smoothing time constant. Default: 0.0 (no smoothing). 0.0","title":"population_rate()"},{"location":"API/Network.html","text":"Network class # A Network object holds copies of previously defined populations, projections or monitors in order to simulate them independently. The parallel_run() method can be used to simulate different networks in parallel. Network # A network gathers already defined populations, projections and monitors in order to run them independently. This is particularly useful when varying single parameters of a network and comparing the results (see the parallel_run() method). Only objects declared before the creation of the network can be used. Global methods such as simulate() must be used on the network object. The objects must be accessed through the get() method, as the original ones will not be part of the network (a copy is made). Each network must be individually compiled, but it does not matter if the original objects were already compiled. When passing everything=True to the constructor, all populations/projections/monitors already defined at the global level will be added to the network. If not, you can select which object will be added to network with the add() method. Example with everything=True : pop = Population ( 100 , Izhikevich ) proj = Projection ( pop , pop , 'exc' ) proj . connect_all_to_all ( 1.0 ) m = Monitor ( pop , 'spike' ) compile () # Optional net = Network ( everything = True ) net . get ( pop ) . a = 0.02 net . compile () net . simulate ( 1000. ) net2 = Network ( everything = True ) net2 . get ( pop ) . a = 0.05 net2 . compile () net2 . simulate ( 1000. ) t , n = net . get ( m ) . raster_plot () t2 , n2 = net2 . get ( m ) . raster_plot () Example with everything=False (the default): pop = Population ( 100 , Izhikevich ) proj1 = Projection ( pop , pop , 'exc' ) proj1 . connect_all_to_all ( 1.0 ) proj2 = Projection ( pop , pop , 'exc' ) proj2 . connect_all_to_all ( 2.0 ) m = Monitor ( pop , 'spike' ) net = Network () net . add ([ pop , proj1 , m ]) net . compile () net . simulate ( 1000. ) net2 = Network () net2 . add ([ pop , proj2 , m ]) net2 . compile () net2 . simulate ( 1000. ) t , n = net . get ( m ) . raster_plot () t2 , n2 = net2 . get ( m ) . raster_plot () __doc__ = \" \\n A network gathers already defined populations, projections and monitors in order to run them independently. \\n\\n This is particularly useful when varying single parameters of a network and comparing the results (see the ``parallel_run()`` method). \\n\\n Only objects declared before the creation of the network can be used. Global methods such as ``simulate()`` must be used on the network object. \\n The objects must be accessed through the ``get()`` method, as the original ones will not be part of the network (a copy is made). \\n\\n Each network must be individually compiled, but it does not matter if the original objects were already compiled. \\n\\n When passing ``everything=True`` to the constructor, all populations/projections/monitors already defined at the global level will be added to the network. \\n\\n If not, you can select which object will be added to network with the ``add()`` method. \\n\\n Example with ``everything=True``: \\n\\n ```python \\n pop = Population(100, Izhikevich) \\n proj = Projection(pop, pop, 'exc') \\n proj.connect_all_to_all(1.0) \\n m = Monitor(pop, 'spike') \\n\\n compile() # Optional \\n\\n net = Network(everything=True) \\n net.get(pop).a = 0.02 \\n net.compile() \\n net.simulate(1000.) \\n\\n net2 = Network(everything=True) \\n net2.get(pop).a = 0.05 \\n net2.compile() \\n net2.simulate(1000.) \\n\\n t, n = net.get(m).raster_plot() \\n t2, n2 = net2.get(m).raster_plot() \\n ``` \\n\\n Example with ``everything=False`` (the default): \\n\\n ```python \\n pop = Population(100, Izhikevich) \\n proj1 = Projection(pop, pop, 'exc') \\n proj1.connect_all_to_all(1.0) \\n proj2 = Projection(pop, pop, 'exc') \\n proj2.connect_all_to_all(2.0) \\n m = Monitor(pop, 'spike') \\n\\n net = Network() \\n net.add([pop, proj1, m]) \\n net.compile() \\n net.simulate(1000.) \\n\\n net2 = Network() \\n net2.add([pop, proj2, m]) \\n net2.compile() \\n net2.simulate(1000.) \\n\\n t, n = net.get(m).raster_plot() \\n t2, n2 = net2.get(m).raster_plot() \\n ``` \\n\\n \" # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.core.Network' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'Network' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init__ ( everything = False ) # Parameters: Name Type Description Default everything defines if all existing populations and projections should be automatically added (default: False). False __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __repr__ () method descriptor # Return repr(self). __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __str__ () method descriptor # Return str(self). __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). add ( objects ) # Adds a Population, Projection or Monitor to the network. Parameters: Name Type Description Default objects A single object or a list to add to the network. required compile ( directory = 'annarchy' , clean = False , compiler = 'default' , compiler_flags = 'default' , add_sources = '' , extra_libs = '' , cuda_config = { 'device' : 0 }, annarchy_json = '' , silent = False , debug_build = False , profile_enabled = False ) # Compiles the network. Parameters: Name Type Description Default directory name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\". 'annarchy' clean boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False). False compiler C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++]. 'default' compiler_flags platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended. 'default' cuda_config dictionary defining the CUDA configuration for each population and projection. {'device': 0} annarchy_json compiler flags etc are stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location. '' silent defines if the \"Compiling... OK\" should be printed. False disable_learning ( projections = None ) # Disables learning for all projections. Parameters: Name Type Description Default projections the projections whose learning should be disabled. By default, all the existing projections are disabled. None enable_learning ( projections = None , period = None , offset = None ) # Enables learning for all projections. Parameters: Name Type Description Default projections the projections whose learning should be enabled. By default, all the existing projections are disabled. None get ( obj ) # Returns the local Population, Projection or Monitor identical to the provided argument. Example: pop = Population ( 100 , Izhikevich ) net = Network () net . add ( pop ) net . compile () net . simulate ( 100. ) print net . get ( pop ) . v Parameters: Name Type Description Default obj A single object or a list of objects. required Returns: Type Description The corresponding object or list of objects. get_current_step () # Returns the current simulation step. get_population ( name ) # Returns the population with the given name . Parameters: Name Type Description Default name name of the population required Returns: Type Description The requested Population object if existing, None otherwise. get_populations () # Returns a list of all declared populations in this network. get_projection ( name ) # Returns the projection with the given name . Parameters: Name Type Description Default name name of the projection required Returns: Type Description The requested Projection object if existing, None otherwise. get_projections ( post = None , pre = None , target = None , suppress_error = False ) # Get a list of declared projections for the current network. By default, the method returns all connections within the network. By setting the arguments, post, pre and target one can select a subset. Parameters: Name Type Description Default post all returned projections should have this population as post. None pre all returned projections should have this population as pre. None target all returned projections should have this target. None suppress_error by default, ANNarchy throws an error if the list of assigned projections is empty. If this flag is set to True, the error message is suppressed. False Returns: Type Description A list of all assigned projections in this network or a subset according to the arguments. get_time () # Returns the current time in ms. load ( filename , populations = True , projections = True ) # Loads a saved state of the current network by calling ANNarchy.core.IO.load(). Parameters: Name Type Description Default filename filename, may contain relative or absolute path. required populations if True, population data will be saved (by default True) True projections if True, projection data will be saved (by default True) True reset ( populations = True , projections = False , monitors = True , synapses = False ) # Reinitialises the network to its state before the call to compile. Parameters: Name Type Description Default populations if True (default), the neural parameters and variables will be reset to their initial value. True projections if True, the synaptic parameters and variables (except the connections) will be reset (default=False). False synapses if True, the synaptic weights will be erased and recreated (default=False). False save ( filename , populations = True , projections = True ) # Saves the current network by calling ANNarchy.core.IO.save(). Parameters: Name Type Description Default filename filename, may contain relative or absolute path. required populations if True, population data will be saved (by default True) True projections if True, projection data will be saved (by default True) True set_current_step ( t ) # Sets the current simulation step. Warning: can be dangerous for some spiking models. set_seed ( seed , use_seed_seq = True ) # Sets the seed of the random number generators for this network. set_time ( t , net_id = 0 ) # Sets the current time in ms. Warning: can be dangerous for some spiking models. simulate ( duration , measure_time = False ) # Runs the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step dt declared in setup() (default: 1ms): simulate ( 1000.0 ) Parameters: Name Type Description Default duration the duration in milliseconds. required measure_time defines whether the simulation time should be printed (default=False). False simulate_until ( max_duration , population , operator = 'and' , measure_time = False ) # Runs the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: pop1 = Population ( ... , stop_condition = \"r > 1.0 : any\" ) compile () simulate_until ( max_duration = 1000.0 . population = pop1 ) Parameters: Name Type Description Default max_duration the maximum duration of the simulation in milliseconds. required population the (list of) population whose stop_condition should be checked to stop the simulation. required operator operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). 'and' measure_time defines whether the simulation time should be printed (default=False). False Returns: Type Description the actual duration of the simulation in milliseconds. step () # Performs a single simulation step (duration = dt ). parallel_run ( method , networks = None , number = 0 , max_processes =- 1 , measure_time = False , sequential = False , same_seed = False , annarchy_json = '' , visible_cores = [], ** args ) # Allows to run multiple networks in parallel using multiprocessing. If the networks argument is provided as a list of Network objects, the given method will be executed for each of these networks. If number is given instead, the same number of networks will be created and the method is applied. If number is used, the created networks are not returned, you should return what you need to analyse. Example: pop1 = PoissonPopulation ( 100 , rates = 10.0 ) pop2 = Population ( 100 , Izhikevich ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_probability ( weights = 5.0 , probability = 0.2 ) m = Monitor ( pop2 , 'spike' ) compile () def simulation ( idx , net ): net . get ( pop1 ) . rates = 10. * idx net . simulate ( 1000. ) return net . get ( m ) . raster_plot () results = parallel_run ( method = simulation , number = 3 ) t1 , n1 = results [ 0 ] t2 , n2 = results [ 1 ] t3 , n3 = results [ 2 ] Parameters: Name Type Description Default method a Python method which will be executed for each network. This function must accept an integer as first argument (id of the simulation) and a Network object as second argument. required networks a list of networks to simulate in parallel. None number the number of identical networks to run in parallel. 0 max_processes maximal number of processes to start concurrently (default: the available number of cores on the machine). -1 measure_time if the total simulation time should be printed out. False sequential if True, runs the simulations sequentially instead of in parallel (default: False). False same_seed if True, all networks will use the same seed. If not, the seed will be randomly initialized with time(0) for each network (default). It has no influence when the networks argument is set (the seed has to be set individually for each network using net.set_seed() ), only when number is used. False annarchy.json path to a different configuration file if needed (default \"\"). required visible_cores a list of CPU core ids to simulate on (must have max_processes entries and max_processes must be != -1) [] args other named arguments you want to pass to the simulation method. required Returns: Type Description a list of the values returned by method .","title":"Network class"},{"location":"API/Network.html#network-class","text":"A Network object holds copies of previously defined populations, projections or monitors in order to simulate them independently. The parallel_run() method can be used to simulate different networks in parallel.","title":"Network class"},{"location":"API/Network.html#ANNarchy.Network","text":"A network gathers already defined populations, projections and monitors in order to run them independently. This is particularly useful when varying single parameters of a network and comparing the results (see the parallel_run() method). Only objects declared before the creation of the network can be used. Global methods such as simulate() must be used on the network object. The objects must be accessed through the get() method, as the original ones will not be part of the network (a copy is made). Each network must be individually compiled, but it does not matter if the original objects were already compiled. When passing everything=True to the constructor, all populations/projections/monitors already defined at the global level will be added to the network. If not, you can select which object will be added to network with the add() method. Example with everything=True : pop = Population ( 100 , Izhikevich ) proj = Projection ( pop , pop , 'exc' ) proj . connect_all_to_all ( 1.0 ) m = Monitor ( pop , 'spike' ) compile () # Optional net = Network ( everything = True ) net . get ( pop ) . a = 0.02 net . compile () net . simulate ( 1000. ) net2 = Network ( everything = True ) net2 . get ( pop ) . a = 0.05 net2 . compile () net2 . simulate ( 1000. ) t , n = net . get ( m ) . raster_plot () t2 , n2 = net2 . get ( m ) . raster_plot () Example with everything=False (the default): pop = Population ( 100 , Izhikevich ) proj1 = Projection ( pop , pop , 'exc' ) proj1 . connect_all_to_all ( 1.0 ) proj2 = Projection ( pop , pop , 'exc' ) proj2 . connect_all_to_all ( 2.0 ) m = Monitor ( pop , 'spike' ) net = Network () net . add ([ pop , proj1 , m ]) net . compile () net . simulate ( 1000. ) net2 = Network () net2 . add ([ pop , proj2 , m ]) net2 . compile () net2 . simulate ( 1000. ) t , n = net . get ( m ) . raster_plot () t2 , n2 = net2 . get ( m ) . raster_plot ()","title":"Network"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__init__","text":"Parameters: Name Type Description Default everything defines if all existing populations and projections should be automatically added (default: False). False","title":"__init__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__repr__","text":"Return repr(self).","title":"__repr__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__str__","text":"Return str(self).","title":"__str__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.add","text":"Adds a Population, Projection or Monitor to the network. Parameters: Name Type Description Default objects A single object or a list to add to the network. required","title":"add()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.compile","text":"Compiles the network. Parameters: Name Type Description Default directory name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\". 'annarchy' clean boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False). False compiler C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++]. 'default' compiler_flags platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended. 'default' cuda_config dictionary defining the CUDA configuration for each population and projection. {'device': 0} annarchy_json compiler flags etc are stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location. '' silent defines if the \"Compiling... OK\" should be printed. False","title":"compile()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.disable_learning","text":"Disables learning for all projections. Parameters: Name Type Description Default projections the projections whose learning should be disabled. By default, all the existing projections are disabled. None","title":"disable_learning()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.enable_learning","text":"Enables learning for all projections. Parameters: Name Type Description Default projections the projections whose learning should be enabled. By default, all the existing projections are disabled. None","title":"enable_learning()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get","text":"Returns the local Population, Projection or Monitor identical to the provided argument. Example: pop = Population ( 100 , Izhikevich ) net = Network () net . add ( pop ) net . compile () net . simulate ( 100. ) print net . get ( pop ) . v Parameters: Name Type Description Default obj A single object or a list of objects. required Returns: Type Description The corresponding object or list of objects.","title":"get()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get_current_step","text":"Returns the current simulation step.","title":"get_current_step()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get_population","text":"Returns the population with the given name . Parameters: Name Type Description Default name name of the population required Returns: Type Description The requested Population object if existing, None otherwise.","title":"get_population()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get_populations","text":"Returns a list of all declared populations in this network.","title":"get_populations()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get_projection","text":"Returns the projection with the given name . Parameters: Name Type Description Default name name of the projection required Returns: Type Description The requested Projection object if existing, None otherwise.","title":"get_projection()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get_projections","text":"Get a list of declared projections for the current network. By default, the method returns all connections within the network. By setting the arguments, post, pre and target one can select a subset. Parameters: Name Type Description Default post all returned projections should have this population as post. None pre all returned projections should have this population as pre. None target all returned projections should have this target. None suppress_error by default, ANNarchy throws an error if the list of assigned projections is empty. If this flag is set to True, the error message is suppressed. False Returns: Type Description A list of all assigned projections in this network or a subset according to the arguments.","title":"get_projections()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get_time","text":"Returns the current time in ms.","title":"get_time()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.load","text":"Loads a saved state of the current network by calling ANNarchy.core.IO.load(). Parameters: Name Type Description Default filename filename, may contain relative or absolute path. required populations if True, population data will be saved (by default True) True projections if True, projection data will be saved (by default True) True","title":"load()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.reset","text":"Reinitialises the network to its state before the call to compile. Parameters: Name Type Description Default populations if True (default), the neural parameters and variables will be reset to their initial value. True projections if True, the synaptic parameters and variables (except the connections) will be reset (default=False). False synapses if True, the synaptic weights will be erased and recreated (default=False). False","title":"reset()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.save","text":"Saves the current network by calling ANNarchy.core.IO.save(). Parameters: Name Type Description Default filename filename, may contain relative or absolute path. required populations if True, population data will be saved (by default True) True projections if True, projection data will be saved (by default True) True","title":"save()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.set_current_step","text":"Sets the current simulation step. Warning: can be dangerous for some spiking models.","title":"set_current_step()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.set_seed","text":"Sets the seed of the random number generators for this network.","title":"set_seed()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.set_time","text":"Sets the current time in ms. Warning: can be dangerous for some spiking models.","title":"set_time()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.simulate","text":"Runs the network for the given duration in milliseconds. The number of simulation steps is computed relative to the discretization step dt declared in setup() (default: 1ms): simulate ( 1000.0 ) Parameters: Name Type Description Default duration the duration in milliseconds. required measure_time defines whether the simulation time should be printed (default=False). False","title":"simulate()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.simulate_until","text":"Runs the network for the maximal duration in milliseconds. If the stop_condition defined in the population becomes true during the simulation, it is stopped. One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function). Example: pop1 = Population ( ... , stop_condition = \"r > 1.0 : any\" ) compile () simulate_until ( max_duration = 1000.0 . population = pop1 ) Parameters: Name Type Description Default max_duration the maximum duration of the simulation in milliseconds. required population the (list of) population whose stop_condition should be checked to stop the simulation. required operator operator to be used ('and' or 'or') when multiple populations are provided (default: 'and'). 'and' measure_time defines whether the simulation time should be printed (default=False). False Returns: Type Description the actual duration of the simulation in milliseconds.","title":"simulate_until()"},{"location":"API/Network.html#ANNarchy.core.Network.Network.step","text":"Performs a single simulation step (duration = dt ).","title":"step()"},{"location":"API/Network.html#ANNarchy.parallel_run","text":"Allows to run multiple networks in parallel using multiprocessing. If the networks argument is provided as a list of Network objects, the given method will be executed for each of these networks. If number is given instead, the same number of networks will be created and the method is applied. If number is used, the created networks are not returned, you should return what you need to analyse. Example: pop1 = PoissonPopulation ( 100 , rates = 10.0 ) pop2 = Population ( 100 , Izhikevich ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_probability ( weights = 5.0 , probability = 0.2 ) m = Monitor ( pop2 , 'spike' ) compile () def simulation ( idx , net ): net . get ( pop1 ) . rates = 10. * idx net . simulate ( 1000. ) return net . get ( m ) . raster_plot () results = parallel_run ( method = simulation , number = 3 ) t1 , n1 = results [ 0 ] t2 , n2 = results [ 1 ] t3 , n3 = results [ 2 ] Parameters: Name Type Description Default method a Python method which will be executed for each network. This function must accept an integer as first argument (id of the simulation) and a Network object as second argument. required networks a list of networks to simulate in parallel. None number the number of identical networks to run in parallel. 0 max_processes maximal number of processes to start concurrently (default: the available number of cores on the machine). -1 measure_time if the total simulation time should be printed out. False sequential if True, runs the simulations sequentially instead of in parallel (default: False). False same_seed if True, all networks will use the same seed. If not, the seed will be randomly initialized with time(0) for each network (default). It has no influence when the networks argument is set (the seed has to be set individually for each network using net.set_seed() ), only when number is used. False annarchy.json path to a different configuration file if needed (default \"\"). required visible_cores a list of CPU core ids to simulate on (must have max_processes entries and max_processes must be != -1) [] args other named arguments you want to pass to the simulation method. required Returns: Type Description a list of the values returned by method .","title":"parallel_run()"},{"location":"API/Neuron.html","text":"Neuron class # Neurons are container objects for all information corresponding to a special neuron type. This encapsulation allows a higher readability of the code. Through derivation of ANNarchy.Neuron , the user can define the neuron types he needs in his model. The type of the neuron (rate-coded or spiking) depends on the presence of the spike argument. Neuron # Base class to define a neuron. __doc__ = ' \\n Base class to define a neuron. \\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.core.Neuron' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'Neuron' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init__ ( parameters = '' , equations = '' , spike = None , axon_spike = None , reset = None , axon_reset = None , refractory = None , functions = None , name = '' , description = '' , extra_values = {}) # Parameters: Name Type Description Default parameters parameters of the neuron and their initial value. '' equations equations defining the temporal evolution of variables. '' functions additional functions used in the variables' equations. None spike condition to emit a spike (only for spiking neurons). None axon_spike condition to emit an axonal spike (only for spiking neurons and optional). The axonal spike can appear additional to the spike and is independent from refractoriness of a neuron. None reset changes to the variables after a spike (only for spiking neurons). None axon_reset changes to the variables after an axonal spike (only for spiking neurons). None refractory refractory period of a neuron after a spike (only for spiking neurons). None name name of the neuron type (used for reporting only). '' description short description of the neuron type (used for reporting). '' __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __str__ () method descriptor # Return str(self). __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"Neuron class"},{"location":"API/Neuron.html#neuron-class","text":"Neurons are container objects for all information corresponding to a special neuron type. This encapsulation allows a higher readability of the code. Through derivation of ANNarchy.Neuron , the user can define the neuron types he needs in his model. The type of the neuron (rate-coded or spiking) depends on the presence of the spike argument.","title":"Neuron class"},{"location":"API/Neuron.html#ANNarchy.Neuron","text":"Base class to define a neuron.","title":"Neuron"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__init__","text":"Parameters: Name Type Description Default parameters parameters of the neuron and their initial value. '' equations equations defining the temporal evolution of variables. '' functions additional functions used in the variables' equations. None spike condition to emit a spike (only for spiking neurons). None axon_spike condition to emit an axonal spike (only for spiking neurons and optional). The axonal spike can appear additional to the spike and is independent from refractoriness of a neuron. None reset changes to the variables after a spike (only for spiking neurons). None axon_reset changes to the variables after an axonal spike (only for spiking neurons). None refractory refractory period of a neuron after a spike (only for spiking neurons). None name name of the neuron type (used for reporting only). '' description short description of the neuron type (used for reporting). ''","title":"__init__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__str__","text":"Return str(self).","title":"__str__()"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/Population.html","text":"Population class # A Population object represents a group of identical neurons. It is associated with a geometry (defining the number of neurons and optionally its spatial structure), a neuron type and optionally a name. Population # Container for a population of homogeneous neurons. __doc__ = ' \\n Container for a population of homogeneous neurons. \\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.core.Population' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'Population' objects > # list of weak references to the object (if defined) neurons property # Returns iteratively each neuron in the population. For instance, if you want to iterate over all neurons of a population: for neuron in pop . neurons : neuron . r = 0.0 Alternatively, one could also benefit from the __iter__ special command. The following code is equivalent: for neuron in pop : neuron . r = 0.0 __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __getitem__ ( * args , ** kwds ) # Returns neurons froms coordinates in the population. If only one argument is given, it is interpeted as a rank and returns a single neuron. If slices are given, it returns a PopulationView object. __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init__ ( geometry , neuron , name = None , stop_condition = None , storage_order = 'post_to_pre' , copied = False ) # Parameters: Name Type Description Default geometry population geometry as tuple. If an integer is given, it is the size of the population. required neuron instance of ANNarchy.Neuron . It can be user-defined or a built-in model. required name unique name of the population (optional, it defaults to pop0 , pop1 , etc). None stop_condition a single condition on a neural variable which can stop the simulation whenever it is true. Example: python pop = Population(100, neuron=Izhikevich, name=\"Excitatory population\") None __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __repr__ () method descriptor # Return repr(self). __sizeof__ () method descriptor # Size of object in memory, in bytes. __str__ () method descriptor # Return str(self). __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). clear () # Clears all spiking events previously emitted (history of spikes, delayed spikes). Can be useful if you do not want to totally reset a population (i.e. all variables), only to clear the spiking history between two trials. Note: does nothing for rate-coded networks. compute_firing_rate ( window ) # Tells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable r . This method has an effect on spiking neurons only. If this method is not called, r will always be 0.0. r can of course be accessed and recorded as any other variable. Parameters: Name Type Description Default window window in ms over which the spikes will be counted. required coordinates_from_rank ( rank ) # Returns the coordinates of a neuron based on its rank. Parameters: Name Type Description Default rank rank of the neuron. required disable () # Temporarily disables computations in this population (including the projections leading to it). You can re-enable it with the enable() method. enable () # (Re)-enables computations in this population, after they were disabled by the disable() method. The status of the population is accessible through the enabled flag. get ( name ) # Returns the value of neural variables and parameters. Parameters: Name Type Description Default name attribute name as a string. required load ( filename ) # Load the saved state of the population by Population.save() . Warning: Matlab data can not be loaded. Example: pop . load ( 'pop1.npz' ) pop . load ( 'pop1.txt' ) pop . load ( 'pop1.txt.gz' ) Parameters: Name Type Description Default filename the filename with relative or absolute path. required neuron ( * coord ) # Returns an IndividualNeuron object wrapping the neuron with the provided rank or coordinates. normalized_coordinates_from_rank ( rank , norm = 1.0 ) # Returns normalized coordinates of a neuron based on its rank. The geometry of the population is mapped to the hypercube \\([0, 1]^d\\) Parameters: Name Type Description Default rank rank of the neuron required norm norm of the cube (default = 1.0) 1.0 rank_from_coordinates ( coord ) # Returns the rank of a neuron based on coordinates. Parameters: Name Type Description Default coord coordinate tuple, can be multidimensional. required reset ( attributes =- 1 ) # Resets all parameters and variables of the population to the value they had before the call to compile(). Parameters: Name Type Description Default attributes list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. -1 save ( filename ) # Saves all information about the population (structure, current value of parameters and variables) into a file. If the file name is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. Otherwise, the data will be pickled into a simple binary text file using pickle. Warning: The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Parameters: Name Type Description Default filename filename, may contain relative or absolute path. Example: python pop.save('pop1.npz') pop.save('pop1.txt') pop.save('pop1.txt.gz') pop.save('pop1.mat') required set ( values ) # Sets the value of neural variables and parameters. Example: pop . set ({ 'tau' : 20.0 , 'r' = np . random . rand (( 8 , 8 )) } ) Parameters: Name Type Description Default values dictionary of attributes to be updated. required size_in_bytes () # Returns the size of allocated memory on the C++ side. Please note that this does not contain monitored data and works only if compile() was invoked. sum ( target ) # Returns the array of weighted sums corresponding to the target: excitatory = pop . sum ( 'exc' ) For spiking networks, this is equivalent to accessing the conductances directly: excitatory = pop . g_exc If no incoming projection has the given target, the method returns zeros. Note: it is not possible to distinguish the original population when the same target is used. Parameters: Name Type Description Default target the desired projection target. required","title":"Population class"},{"location":"API/Population.html#population-class","text":"A Population object represents a group of identical neurons. It is associated with a geometry (defining the number of neurons and optionally its spatial structure), a neuron type and optionally a name.","title":"Population class"},{"location":"API/Population.html#ANNarchy.Population","text":"Container for a population of homogeneous neurons.","title":"Population"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/Population.html#ANNarchy.core.Population.Population.neurons","text":"Returns iteratively each neuron in the population. For instance, if you want to iterate over all neurons of a population: for neuron in pop . neurons : neuron . r = 0.0 Alternatively, one could also benefit from the __iter__ special command. The following code is equivalent: for neuron in pop : neuron . r = 0.0","title":"neurons"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__getitem__","text":"Returns neurons froms coordinates in the population. If only one argument is given, it is interpeted as a rank and returns a single neuron. If slices are given, it returns a PopulationView object.","title":"__getitem__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__init__","text":"Parameters: Name Type Description Default geometry population geometry as tuple. If an integer is given, it is the size of the population. required neuron instance of ANNarchy.Neuron . It can be user-defined or a built-in model. required name unique name of the population (optional, it defaults to pop0 , pop1 , etc). None stop_condition a single condition on a neural variable which can stop the simulation whenever it is true. Example: python pop = Population(100, neuron=Izhikevich, name=\"Excitatory population\") None","title":"__init__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__repr__","text":"Return repr(self).","title":"__repr__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__str__","text":"Return str(self).","title":"__str__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.clear","text":"Clears all spiking events previously emitted (history of spikes, delayed spikes). Can be useful if you do not want to totally reset a population (i.e. all variables), only to clear the spiking history between two trials. Note: does nothing for rate-coded networks.","title":"clear()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.compute_firing_rate","text":"Tells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable r . This method has an effect on spiking neurons only. If this method is not called, r will always be 0.0. r can of course be accessed and recorded as any other variable. Parameters: Name Type Description Default window window in ms over which the spikes will be counted. required","title":"compute_firing_rate()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.coordinates_from_rank","text":"Returns the coordinates of a neuron based on its rank. Parameters: Name Type Description Default rank rank of the neuron. required","title":"coordinates_from_rank()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.disable","text":"Temporarily disables computations in this population (including the projections leading to it). You can re-enable it with the enable() method.","title":"disable()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.enable","text":"(Re)-enables computations in this population, after they were disabled by the disable() method. The status of the population is accessible through the enabled flag.","title":"enable()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.get","text":"Returns the value of neural variables and parameters. Parameters: Name Type Description Default name attribute name as a string. required","title":"get()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.load","text":"Load the saved state of the population by Population.save() . Warning: Matlab data can not be loaded. Example: pop . load ( 'pop1.npz' ) pop . load ( 'pop1.txt' ) pop . load ( 'pop1.txt.gz' ) Parameters: Name Type Description Default filename the filename with relative or absolute path. required","title":"load()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.neuron","text":"Returns an IndividualNeuron object wrapping the neuron with the provided rank or coordinates.","title":"neuron()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.normalized_coordinates_from_rank","text":"Returns normalized coordinates of a neuron based on its rank. The geometry of the population is mapped to the hypercube \\([0, 1]^d\\) Parameters: Name Type Description Default rank rank of the neuron required norm norm of the cube (default = 1.0) 1.0","title":"normalized_coordinates_from_rank()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.rank_from_coordinates","text":"Returns the rank of a neuron based on coordinates. Parameters: Name Type Description Default coord coordinate tuple, can be multidimensional. required","title":"rank_from_coordinates()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.reset","text":"Resets all parameters and variables of the population to the value they had before the call to compile(). Parameters: Name Type Description Default attributes list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. -1","title":"reset()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.save","text":"Saves all information about the population (structure, current value of parameters and variables) into a file. If the file name is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. Otherwise, the data will be pickled into a simple binary text file using pickle. Warning: The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Parameters: Name Type Description Default filename filename, may contain relative or absolute path. Example: python pop.save('pop1.npz') pop.save('pop1.txt') pop.save('pop1.txt.gz') pop.save('pop1.mat') required","title":"save()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.set","text":"Sets the value of neural variables and parameters. Example: pop . set ({ 'tau' : 20.0 , 'r' = np . random . rand (( 8 , 8 )) } ) Parameters: Name Type Description Default values dictionary of attributes to be updated. required","title":"set()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.size_in_bytes","text":"Returns the size of allocated memory on the C++ side. Please note that this does not contain monitored data and works only if compile() was invoked.","title":"size_in_bytes()"},{"location":"API/Population.html#ANNarchy.core.Population.Population.sum","text":"Returns the array of weighted sums corresponding to the target: excitatory = pop . sum ( 'exc' ) For spiking networks, this is equivalent to accessing the conductances directly: excitatory = pop . g_exc If no incoming projection has the given target, the method returns zeros. Note: it is not possible to distinguish the original population when the same target is used. Parameters: Name Type Description Default target the desired projection target. required","title":"sum()"},{"location":"API/Projection.html","text":"Projection class # The class ANNarchy.Projection defines projections at the population level. A projection is an ensemble of connections (or synapses) between a subset of a population (called the pre-synaptic population) and a subset of another population (the post-synaptic population), with a specific connection type. The pre- and post-synaptic populations may be the same. Projection # Bases: object Container for all the synapses of the same type between two populations. Source code in ANNarchy/core/Projection.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 class Projection ( object ): \"\"\" Container for all the synapses of the same type between two populations. \"\"\" def __init__ ( self , pre , post , target , synapse = None , name = None , disable_omp = True , copied = False ): \"\"\" By default, the synapse only ensures linear synaptic transmission: * For rate-coded populations: ``psp = w * pre.r`` * For spiking populations: ``g_target += w`` to modify this behavior one need to provide a Synapse object. :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection. :param synapse: a ``Synapse`` instance. :param name: unique name of the projection (optional, it defaults to ``proj0``, ``proj1``, etc). :param disable_omp: especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to `False`. \"\"\" # Check if the network has already been compiled if Global . _network [ 0 ][ 'compiled' ] and not copied : Global . _error ( 'you cannot add a projection after the network has been compiled.' ) # Store the pre and post synaptic populations # the user provide either a string or a population object # in case of string, we need to search for the corresponding object if isinstance ( pre , str ): for pop in Global . _network [ 0 ][ 'populations' ]: if pop . name == pre : self . pre = pop else : self . pre = pre if isinstance ( post , str ): for pop in Global . _network [ 0 ][ 'populations' ]: if pop . name == post : self . post = pop else : self . post = post # Store the arguments if isinstance ( target , list ) and len ( target ) == 1 : self . target = target [ 0 ] else : self . target = target # Add the target(s) to the postsynaptic population if isinstance ( self . target , list ): for _target in self . target : self . post . targets . append ( _target ) else : self . post . targets . append ( self . target ) # check if a synapse description is attached if not synapse : # No synapse attached assume default synapse based on # presynaptic population. if self . pre . neuron_type . type == 'rate' : from ANNarchy.models.Synapses import DefaultRateCodedSynapse self . synapse_type = DefaultRateCodedSynapse () self . synapse_type . type = 'rate' else : from ANNarchy.models.Synapses import DefaultSpikingSynapse self . synapse_type = DefaultSpikingSynapse () self . synapse_type . type = 'spike' elif inspect . isclass ( synapse ): self . synapse_type = synapse () self . synapse_type . type = self . pre . neuron_type . type else : self . synapse_type = copy . deepcopy ( synapse ) self . synapse_type . type = self . pre . neuron_type . type # Disable omp for spiking networks self . disable_omp = disable_omp # Analyse the parameters and variables self . synapse_type . _analyse () # Create a default name self . id = len ( Global . _network [ 0 ][ 'projections' ]) if name : self . name = name else : self . name = 'proj' + str ( self . id ) # Get a list of parameters and variables self . parameters = [] self . init = {} for param in self . synapse_type . description [ 'parameters' ]: self . parameters . append ( param [ 'name' ]) self . init [ param [ 'name' ]] = param [ 'init' ] self . variables = [] for var in self . synapse_type . description [ 'variables' ]: self . variables . append ( var [ 'name' ]) self . init [ var [ 'name' ]] = var [ 'init' ] self . attributes = self . parameters + self . variables # Get a list of user-defined functions self . functions = [ func [ 'name' ] for func in self . synapse_type . description [ 'functions' ]] # Add the population to the global network Global . _network [ 0 ][ 'projections' ] . append ( self ) # Finalize initialization self . initialized = False # Cython instance self . cyInstance = None # Connectivity self . _synapses = None self . _connection_method = None self . _connection_args = None self . _connection_delay = None self . _connector = None self . _lil_connectivity = None # Default configuration for connectivity self . _storage_format = \"lil\" self . _storage_order = \"post_to_pre\" # If a single weight value is used self . _single_constant_weight = False # Are random distribution used for weights/delays self . connector_weight_dist = None self . connector_delay_dist = None # Reporting self . connector_name = \"Specific\" self . connector_description = \"Specific\" # Overwritten by derived classes, to add # additional code self . _specific_template = {} # Set to False by derived classes to prevent saving of # data, e. g. in case of weight-sharing projections self . _saveable = True # To allow case-specific adjustment of parallelization # parameters, e. g. openMP schedule, we introduce a # dictionary read by the ProjectionGenerator. # # Will be overwritten either by inherited classes or # by an omp_config provided to the compile() method. self . _omp_config = { #'psp_schedule': 'schedule(dynamic)' } # If set to true, the code generator is not allowed to # split the matrix. This will be the case for many # SpecificProjections defined by the user or is disabled # globally. if self . synapse_type . type == \"rate\" : # Normally, the split should not be used for rate-coded models # but maybe there are cases where we want to enable it ... self . _no_split_matrix = Global . config [ \"disable_split_matrix\" ] # If the number of elements is too small, the split # might not be efficient. if self . post . size < Global . OMP_MIN_NB_NEURONS : self . _no_split_matrix = True else : # If the number of elements is too small, the split # might not be efficient. if self . post . size < Global . OMP_MIN_NB_NEURONS : self . _no_split_matrix = True else : self . _no_split_matrix = False # In particular for spiking models, the parallelization on the # inner or outer loop can make a performance difference if self . _no_split_matrix : # LIL and CSR are parallelized on inner loop # to prevent cost of atomic operations self . _parallel_pattern = 'inner_loop' else : # splitted matrices are always parallelized on outer loop! self . _parallel_pattern = 'outer_loop' # For dense matrix format: do we use an optimization for population views? if self . synapse_type . type == \"rate\" : # HD (9th Nov. 2022): currently this optimization is only intended for spiking models self . _has_pop_view = False else : # HD (9th Nov. 2022): currently disabled, more testing is required ... self . _has_pop_view = False #isinstance(self.pre, PopulationView) or isinstance(self.post, PopulationView) # Add defined connectors connect_one_to_one = ConnectorMethods . connect_one_to_one connect_all_to_all = ConnectorMethods . connect_all_to_all connect_gaussian = ConnectorMethods . connect_gaussian connect_dog = ConnectorMethods . connect_dog connect_fixed_probability = ConnectorMethods . connect_fixed_probability connect_fixed_number_pre = ConnectorMethods . connect_fixed_number_pre connect_fixed_number_post = ConnectorMethods . connect_fixed_number_post connect_with_func = ConnectorMethods . connect_with_func connect_from_matrix = ConnectorMethods . connect_from_matrix connect_from_matrix_market = ConnectorMethods . connect_from_matrix_market _load_from_matrix = ConnectorMethods . _load_from_matrix connect_from_sparse = ConnectorMethods . connect_from_sparse _load_from_sparse = ConnectorMethods . _load_from_sparse connect_from_file = ConnectorMethods . connect_from_file _load_from_lil = ConnectorMethods . _load_from_lil def _copy ( self , pre , post ): \"Returns a copy of the projection when creating networks. Internal use only.\" copied_proj = Projection ( pre = pre , post = post , target = self . target , synapse = self . synapse_type , name = self . name , disable_omp = self . disable_omp , copied = True ) # these flags are modified during connect_XXX called before Network() copied_proj . _single_constant_weight = self . _single_constant_weight copied_proj . connector_weight_dist = self . connector_weight_dist copied_proj . connector_delay_dist = self . connector_delay_dist copied_proj . connector_name = self . connector_name # Control flags for code generation (maybe modified by connect_XXX()) copied_proj . _storage_format = self . _storage_format copied_proj . _storage_order = self . _storage_order copied_proj . _no_split_matrix = self . _no_split_matrix # for some projection types saving is not allowed (e. g. Convolution, Pooling) copied_proj . _saveable = self . _saveable # optional flags if hasattr ( self , \"_bsr_size\" ): copied_proj . _bsr_size = self . _bsr_size return copied_proj def _generate ( self ): \"Overriden by specific projections to generate the code\" pass def _instantiate ( self , module ): \"\"\" Instantiates the projection after compilation. The function should be called by Compiler._instantiate(). :param: module cython module (ANNarchyCore instance) \"\"\" if Global . config [ \"profiling\" ]: import time t1 = time . time () self . initialized = self . _connect ( module ) if Global . config [ \"profiling\" ]: t2 = time . time () Global . _profiler . add_entry ( t1 , t2 , \"proj\" + str ( self . id ), \"instantiate\" ) def _init_attributes ( self ): \"\"\" Method used after compilation to initialize the attributes. The function should be called by Compiler._instantiate \"\"\" for name , val in self . init . items (): # the weights ('w') are already inited by the _connect() method. if not name in [ 'w' ]: self . __setattr__ ( name , val ) def _connect ( self , module ): \"\"\" Builds up dendrites either from list or dictionary. Called by instantiate(). :param: module cython module (ANNarchyCore instance) :return: True, if the connector was successfully instantiated. Potential errors are kept by Python exceptions. If the Cython - connector call fails (return False) the most likely reason is that there was not enough memory available. \"\"\" # Local import to prevent circular import (HD: 28th June 2021) from ANNarchy.generator.Utils import cpp_connector_available # Sanity check if not self . _connection_method : Global . _error ( 'The projection between ' + self . pre . name + ' and ' + self . post . name + ' is declared but not connected.' ) # Debug printout if Global . config [ \"verbose\" ]: print ( \"Connectivity parameter (\" + self . name + \"):\" , self . _connection_args ) # Instantiate the Cython wrapper if not self . cyInstance : cy_wrapper = getattr ( module , 'proj' + str ( self . id ) + '_wrapper' ) self . cyInstance = cy_wrapper () # Check if there is a specialized CPP connector if not cpp_connector_available ( self . connector_name , self . _storage_format , self . _storage_order ): # No default connector -> initialize from LIL if self . _lil_connectivity : return self . cyInstance . init_from_lil_connectivity ( self . _lil_connectivity ) else : return self . cyInstance . init_from_lil_connectivity ( self . _connection_method ( * (( self . pre , self . post ,) + self . _connection_args ))) else : # fixed probability pattern if self . connector_name == \"Random\" : p = self . _connection_args [ 0 ] allow_self_connections = self . _connection_args [ 3 ] if isinstance ( self . _connection_args [ 1 ], RandomDistribution ): #some kind of distribution w_dist_arg1 , w_dist_arg2 = self . _connection_args [ 1 ] . get_cpp_args () else : # constant w_dist_arg1 = self . _connection_args [ 1 ] w_dist_arg2 = self . _connection_args [ 1 ] if isinstance ( self . _connection_args [ 2 ], RandomDistribution ): #some kind of distribution d_dist_arg1 , d_dist_arg2 = self . _connection_args [ 2 ] . get_cpp_args () else : # constant d_dist_arg1 = self . _connection_args [ 2 ] d_dist_arg2 = self . _connection_args [ 2 ] return self . cyInstance . fixed_probability ( self . post . ranks , self . pre . ranks , p , w_dist_arg1 , w_dist_arg2 , d_dist_arg1 , d_dist_arg2 , allow_self_connections ) # fixed number pre prattern elif self . connector_name == \"Random Convergent\" : number_nonzero = self . _connection_args [ 0 ] if isinstance ( self . _connection_args [ 1 ], RandomDistribution ): #some kind of distribution w_dist_arg1 , w_dist_arg2 = self . _connection_args [ 1 ] . get_cpp_args () else : # constant w_dist_arg1 = self . _connection_args [ 1 ] w_dist_arg2 = self . _connection_args [ 1 ] if isinstance ( self . _connection_args [ 2 ], RandomDistribution ): #some kind of distribution d_dist_arg1 , d_dist_arg2 = self . _connection_args [ 2 ] . get_cpp_args () else : # constant d_dist_arg1 = self . _connection_args [ 2 ] d_dist_arg2 = self . _connection_args [ 2 ] return self . cyInstance . fixed_number_pre ( self . post . ranks , self . pre . ranks , number_nonzero , w_dist_arg1 , w_dist_arg2 , d_dist_arg1 , d_dist_arg2 ) else : # This should never happen ... Global . _error ( \"No initialization for CPP-connector defined ...\" ) # should be never reached ... return False def _store_connectivity ( self , method , args , delay , storage_format , storage_order ): \"\"\" Store connectivity data. This function is called from cython_ext.Connectors module. \"\"\" # No format specified for this projection by the user, so fall-back to Global setting if storage_format is None : if Global . config [ 'sparse_matrix_format' ] == \"default\" : if Global . _check_paradigm ( \"openmp\" ): storage_format = \"lil\" elif Global . _check_paradigm ( \"cuda\" ): storage_format = \"csr\" else : raise NotImplementedError else : storage_format = Global . config [ \"sparse_matrix_format\" ] # Sanity checks if self . _connection_method != None : Global . _warning ( \"Projection \" , self . name , \" was already connected ... data will be overwritten.\" ) # Store connectivity pattern parameters self . _connection_method = method self . _connection_args = args self . _connection_delay = delay self . _storage_format = storage_format self . _storage_order = storage_order # The user selected nothing therefore we use the standard since ANNarchy 4.4.0 if storage_format == None : self . _storage_format = \"lil\" if storage_order == None : if storage_format == \"auto\" : storage_order = \"auto\" else : self . _storage_order = \"post_to_pre\" # The user selected automatic format selection using heuristics if storage_format == \"auto\" : self . _storage_format = self . _automatic_format_selection () if storage_order == \"auto\" : self . _storage_order = self . _automatic_order_selection () # Analyse the delay if isinstance ( delay , ( int , float )): # Uniform delay self . max_delay = round ( delay / Global . config [ 'dt' ]) self . uniform_delay = round ( delay / Global . config [ 'dt' ]) elif isinstance ( delay , RandomDistribution ): # Non-uniform delay self . uniform_delay = - 1 # Ensure no negative delays are generated if delay . min is None or delay . min < Global . config [ 'dt' ]: delay . min = Global . config [ 'dt' ] # The user needs to provide a max in order to compute max_delay if delay . max is None : Global . _error ( 'Projection.connect_xxx(): if you use a non-bounded random distribution for the delays (e.g. Normal), you need to set the max argument to limit the maximal delay.' ) self . max_delay = round ( delay . max / Global . config [ 'dt' ]) elif isinstance ( delay , ( list , np . ndarray )): # connect_from_matrix/sparse if len ( delay ) > 0 : self . uniform_delay = - 1 self . max_delay = round ( max ([ max ( l ) for l in delay ]) / Global . config [ 'dt' ]) else : # list is empty, no delay self . max_delay = - 1 self . uniform_delay = - 1 else : Global . _error ( 'Projection.connect_xxx(): delays are not valid!' ) # Transmit the max delay to the pre pop if isinstance ( self . pre , PopulationView ): self . pre . population . max_delay = max ( self . max_delay , self . pre . population . max_delay ) else : self . pre . max_delay = max ( self . max_delay , self . pre . max_delay ) def _automatic_format_selection ( self ): \"\"\" We check some heuristics to select a specific format implemented as decision tree: - If the filling degree is high enough a full matrix representation might be better - if the average row length is below a threshold the ELLPACK-R might be better - if the average row length is higher than a threshold the CSR might be better HD (17th Jan. 2022): Currently structural plasticity is only usable with LIL. But one could also apply it for dense matrices in the future. For CSR and in particular the ELL- like formats the potential memory-reallocations make the structural plasticity a costly operation. \"\"\" # Connection pattern / Feature specific selection if Global . config [ \"structural_plasticity\" ]: storage_format = \"lil\" elif self . connector_name == \"All-to-All\" : storage_format = \"dense\" elif self . connector_name == \"One-to-One\" : if Global . _check_paradigm ( \"cuda\" ): storage_format = \"csr\" else : storage_format = \"lil\" else : if self . synapse_type . type == \"spike\" : # we need to build up the matrix to analyze self . _lil_connectivity = self . _connection_method ( * (( self . pre , self . post ,) + self . _connection_args )) # get the decision parameter density = float ( self . _lil_connectivity . nb_synapses ) / float ( self . pre . size * self . post . size ) if density >= 0.6 : if Global . _check_paradigm ( \"cuda\" ): storage_format = \"csr\" # HD (11th Nov. 2022): there is no Dense_T for spiking and CUDA yet else : storage_format = \"dense\" else : storage_format = \"csr\" else : # we need to build up the matrix to analyze self . _lil_connectivity = self . _connection_method ( * (( self . pre , self . post ,) + self . _connection_args )) # get the decision parameter density = float ( self . _lil_connectivity . nb_synapses ) / float ( self . pre . size * self . post . size ) avg_nnz_per_row , _ = self . _lil_connectivity . compute_average_row_length () # heuristic decision tree if density >= 0.6 : storage_format = \"dense\" else : if Global . _check_paradigm ( \"cuda\" ): if avg_nnz_per_row <= 128 : storage_format = \"ellr\" else : storage_format = \"csr\" else : storage_format = \"csr\" Global . _info ( \"Automatic format selection for\" , self . name , \":\" , storage_format ) return storage_format def _automatic_order_selection ( self ): \"\"\" Contrary to the matrix format, the decision for the matrix order is majorly dependent on the synapse type. \"\"\" if self . synapse_type == \"rate\" : storage_order = \"post_to_pre\" else : if Global . _check_paradigm ( \"cuda\" ): # HD (11th Nov. 2022): there is no Dense_T / CSRC_T for spiking and CUDA yet storage_order = \"post_to_pre\" else : # pre-to-post is not implemented for all formats if self . _storage_format in [ \"dense\" , \"csr\" ]: storage_order = \"pre_to_post\" else : storage_order = \"post_to_pre\" Global . _info ( \"Automatic matrix order selection for\" , self . name , \":\" , storage_order ) return storage_order def _has_single_weight ( self ): \"If a single weight should be generated instead of a LIL\" is_cpu = Global . config [ 'paradigm' ] == \"openmp\" has_constant_weight = self . _single_constant_weight not_dense = not ( self . _storage_format == \"dense\" ) no_structural_plasticity = not Global . config [ 'structural_plasticity' ] no_synaptic_plasticity = not self . synapse_type . description [ 'plasticity' ] return has_constant_weight and no_structural_plasticity and no_synaptic_plasticity and is_cpu and not_dense def reset ( self , attributes =- 1 , synapses = False ): \"\"\" Resets all parameters and variables of the projection to the value they had before the call to compile. **Note:** Only parameters and variables are reinitialized, not the connectivity structure (including the weights and delays). The parameter ``synapses`` will be used in a future release to also reinitialize the connectivity structure. :param attributes: list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. \"\"\" if attributes == - 1 : attributes = self . attributes if synapses : # destroy the previous C++ content self . _clear () # call the init connectivity again self . _connect ( None ) self . initialized = True for var in attributes : # Skip w if var == 'w' : continue # check it exists if not var in self . attributes : Global . _warning ( \"Projection.reset():\" , var , \"is not an attribute of the population, won't reset.\" ) continue # Set the value try : self . __setattr__ ( var , self . init [ var ]) except Exception as e : Global . _print ( e ) Global . _warning ( \"Projection.reset(): something went wrong while resetting\" , var ) #Global._warning('Projection.reset(): only parameters and variables are reinitialized, not the connectivity structure (including the weights)...') ################################ ## Dendrite access ################################ @property def size ( self ): \"Number of post-synaptic neurons receiving synapses.\" if self . cyInstance == None : Global . _warning ( \"Access 'size or len()' attribute of a Projection is only valid after compile()\" ) return 0 return len ( self . cyInstance . post_rank ()) def __len__ ( self ): # Number of postsynaptic neurons receiving synapses in this projection. return self . size @property def nb_synapses ( self ): \"Total number of synapses in the projection.\" if self . cyInstance is None : Global . _warning ( \"Access 'nb_synapses' attribute of a Projection is only valid after compile()\" ) return 0 return self . cyInstance . nb_synapses () def nb_synapses_per_dendrite ( self ): \"Total number of synapses for each dendrite as a list.\" if self . cyInstance is None : Global . _warning ( \"Access 'nb_synapses_per_dendrite' attribute of a Projection is only valid after compile()\" ) return [] return [ self . cyInstance . dendrite_size ( n ) for n in range ( self . size )] def nb_efferent_synapses ( self ): \"Number of efferent connections. Intended only for spiking models.\" if self . synapse_type . type == \"rate\" : Global . _error ( \"Projection.nb_efferent_synapses() is not available for rate-coded projections.\" ) return self . cyInstance . nb_efferent_synapses () @property def post_ranks ( self ): if self . cyInstance : return self . cyInstance . post_rank () else : Global . _warning ( \"Access 'post_ranks' attribute of a Projection is only valid after compile()\" ) return None @property def dendrites ( self ): \"\"\" Iteratively returns the dendrites corresponding to this projection. \"\"\" for idx , n in enumerate ( self . post_ranks ): yield Dendrite ( self , n , idx ) def dendrite ( self , post ): \"\"\" Returns the dendrite of a postsynaptic neuron according to its rank. :param post: can be either the rank or the coordinates of the post-synaptic neuron. \"\"\" if not self . initialized : Global . _error ( 'dendrites can only be accessed after compilation.' ) if isinstance ( post , int ): rank = post else : rank = self . post . rank_from_coordinates ( post ) if rank in self . post_ranks : return Dendrite ( self , rank , self . post_ranks . index ( rank )) else : Global . _error ( \" The neuron of rank \" + str ( rank ) + \" has no dendrite in this projection.\" , exit = True ) def synapse ( self , pre , post ): \"\"\" Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise. :param pre: rank of the pre-synaptic neuron. :param post: rank of the post-synaptic neuron. \"\"\" if not isinstance ( pre , int ) or not isinstance ( post , int ): Global . _error ( 'Projection.synapse() only accepts ranks for the pre and post neurons.' ) return self . dendrite ( post ) . synapse ( pre ) # Iterators def __getitem__ ( self , * args , ** kwds ): # Returns dendrite of the given position in the postsynaptic population. # If only one argument is given, it is a rank. If it is a tuple, it is coordinates. if len ( args ) == 1 : return self . dendrite ( args [ 0 ]) return self . dendrite ( args ) def __iter__ ( self ): # Returns iteratively each dendrite in the population in ascending postsynaptic rank order. for idx , n in enumerate ( self . post_ranks ): yield Dendrite ( self , n , idx ) ################################ ## Access to attributes ################################ def get ( self , name ): \"\"\" Returns a list of parameters/variables values for each dendrite in the projection. The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it. :param name: the name of the parameter or variable \"\"\" return self . __getattr__ ( name ) def set ( self , value ): \"\"\" Sets the parameters/variables values for each dendrite in the projection. For parameters, you can provide: * a single value, which will be the same for all dendrites. * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). For variables, you can provide: * a single value, which will be the same for all synapses of all dendrites. * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value. **Warning:** it is not possible to set different values to each synapse using this method. One should iterate over the dendrites: ```python for dendrite in proj.dendrites: dendrite.w = np.ones(dendrite.size) ``` :param value: a dictionary with the name of the parameter/variable as key. \"\"\" for name , val in value . items (): self . __setattr__ ( name , val ) def __getattr__ ( self , name ): # Method called when accessing an attribute. if name == 'initialized' or not hasattr ( self , 'initialized' ): # Before the end of the constructor return object . __getattribute__ ( self , name ) elif hasattr ( self , 'attributes' ): if name in [ 'plasticity' , 'transmission' , 'update' ]: return self . _get_flag ( name ) if name in [ 'delay' ]: return self . _get_delay () if name in self . attributes : if not self . initialized : return self . init [ name ] else : return self . _get_cython_attribute ( name ) elif name in self . functions : return self . _function ( name ) else : return object . __getattribute__ ( self , name ) return object . __getattribute__ ( self , name ) def __setattr__ ( self , name , value ): # Method called when setting an attribute. if name == 'initialized' or not hasattr ( self , 'initialized' ): # Before the end of the constructor object . __setattr__ ( self , name , value ) elif hasattr ( self , 'attributes' ): if name in [ 'plasticity' , 'transmission' , 'update' ]: self . _set_flag ( name , bool ( value )) return if name in [ 'delay' ]: self . _set_delay ( value ) return if name in self . attributes : if not self . initialized : self . init [ name ] = value else : self . _set_cython_attribute ( name , value ) else : object . __setattr__ ( self , name , value ) else : object . __setattr__ ( self , name , value ) def _get_cython_attribute ( self , attribute ): \"\"\" Returns the value of the given attribute for all neurons in the population, as a list of lists having the same geometry as the population if it is local. :param attribute: a string representing the variables's name. \"\"\" # Determine C++ data type ctype = self . _get_attribute_cpp_type ( attribute = attribute ) # retrieve the value from C++ core if attribute == \"w\" and self . _has_single_weight (): return self . cyInstance . get_global_attribute ( attribute , ctype ) elif attribute in self . synapse_type . description [ 'local' ]: return self . cyInstance . get_local_attribute_all ( attribute , ctype ) elif attribute in self . synapse_type . description [ 'semiglobal' ]: return self . cyInstance . get_semiglobal_attribute_all ( attribute , ctype ) else : return self . cyInstance . get_global_attribute ( attribute , ctype ) def _set_cython_attribute ( self , attribute , value ): \"\"\" Sets the value of the given attribute for all post-synaptic neurons in the projection, as a NumPy array having the same geometry as the population if it is local. :param attribute: a string representing the variables's name. :param value: the value it should take. \"\"\" # Determine C++ data type ctype = self . _get_attribute_cpp_type ( attribute = attribute ) # Convert np.arrays into lists/constants for better iteration if isinstance ( value , np . ndarray ): if np . ndim ( value ) == 0 : value = float ( value ) else : value = list ( value ) # A list is given if isinstance ( value , list ): if len ( value ) == len ( self . post_ranks ): if attribute in self . synapse_type . description [ 'local' ]: for idx , n in enumerate ( self . post_ranks ): if not len ( value [ idx ]) == self . cyInstance . dendrite_size ( idx ): Global . _error ( 'The postynaptic neuron ' + str ( n ) + ' receives ' + str ( self . cyInstance . dendrite_size ( idx )) + ' synapses.' ) self . cyInstance . set_local_attribute_row ( attribute , idx , value [ idx ], ctype ) elif attribute in self . synapse_type . description [ 'semiglobal' ]: self . cyInstance . set_semiglobal_attribute_all ( attribute , value , ctype ) else : Global . _error ( 'The parameter' , attribute , 'is global to the population, cannot assign a list.' ) else : Global . _error ( 'The projection has' , self . size , 'post-synaptic neurons, the list must have the same size.' ) # A Random Distribution is given elif isinstance ( value , RandomDistribution ): if attribute == \"w\" and self . _has_single_weight (): self . cyInstance . set_global_attribute ( attribute , value . get_values ( 1 ), ctype ) elif attribute in self . synapse_type . description [ 'local' ]: for idx , n in enumerate ( self . post_ranks ): self . cyInstance . set_local_attribute_row ( attribute , idx , value . get_values ( self . cyInstance . dendrite_size ( idx )), ctype ) elif attribute in self . synapse_type . description [ 'semiglobal' ]: self . cyInstance . set_semiglobal_attribute_all ( attribute , value . get_values ( len ( self . post_ranks )), ctype ) elif attribute in self . synapse_type . description [ 'global' ]: self . cyInstance . set_global_attribute ( attribute , value . get_values ( 1 ), ctype ) # A single value is given else : if attribute == \"w\" and self . _has_single_weight (): self . cyInstance . set_global_attribute ( attribute , value , ctype ) elif attribute in self . synapse_type . description [ 'local' ]: for idx , n in enumerate ( self . post_ranks ): self . cyInstance . set_local_attribute_row ( attribute , idx , value * np . ones ( self . cyInstance . dendrite_size ( idx )), ctype ) elif attribute in self . synapse_type . description [ 'semiglobal' ]: self . cyInstance . set_semiglobal_attribute_all ( attribute , value * np . ones ( len ( self . post_ranks )), ctype ) else : self . cyInstance . set_global_attribute ( attribute , value , ctype ) def _get_attribute_cpp_type ( self , attribute ): \"\"\" Determine C++ data type for a given attribute \"\"\" ctype = None for var in self . synapse_type . description [ 'variables' ] + self . synapse_type . description [ 'parameters' ]: if var [ 'name' ] == attribute : ctype = var [ 'ctype' ] return ctype def _get_flag ( self , attribute ): \"flags such as learning, transmission\" return getattr ( self . cyInstance , '_get_' + attribute )() def _set_flag ( self , attribute , value ): \"flags such as learning, transmission\" getattr ( self . cyInstance , '_set_' + attribute )( value ) ################################ ## Access to delays ################################ def _get_delay ( self ): if not hasattr ( self . cyInstance , 'get_delay' ): if self . max_delay <= 1 : return Global . config [ 'dt' ] elif self . uniform_delay != - 1 : return self . uniform_delay * Global . config [ 'dt' ] else : return [[ pre * Global . config [ 'dt' ] for pre in post ] for post in self . cyInstance . get_delay ()] def _set_delay ( self , value ): if self . cyInstance : # After compile() if not hasattr ( self . cyInstance , 'get_delay' ): if self . max_delay <= 1 and value != Global . config [ 'dt' ]: Global . _error ( \"set_delay: the projection was instantiated without delays, it is too late to create them...\" ) elif self . uniform_delay != - 1 : if isinstance ( value , np . ndarray ): if value . ndim > 0 : Global . _error ( \"set_delay: the projection was instantiated with uniform delays, it is too late to load non-uniform values...\" ) else : value = max ( 1 , round ( float ( value ) / Global . config [ 'dt' ])) elif isinstance ( value , ( float , int )): value = max ( 1 , round ( float ( value ) / Global . config [ 'dt' ])) else : Global . _error ( \"set_delay: only float, int or np.array values are possible.\" ) # The new max_delay is higher than before if value > self . max_delay : self . max_delay = value self . uniform_delay = value self . cyInstance . set_delay ( value ) if isinstance ( self . pre , PopulationView ): self . pre . population . max_delay = max ( self . max_delay , self . pre . population . max_delay ) self . pre . population . cyInstance . update_max_delay ( self . pre . population . max_delay ) else : self . pre . max_delay = max ( self . max_delay , self . pre . max_delay ) self . pre . cyInstance . update_max_delay ( self . pre . max_delay ) return else : self . uniform_delay = value self . cyInstance . set_delay ( value ) else : # variable delays if not isinstance ( value , ( np . ndarray , list )): Global . _error ( \"set_delay with variable delays: you must provide a list of lists of exactly the same size as before.\" ) # Check the number of delays nb_values = sum ([ len ( s ) for s in value ]) if nb_values != self . nb_synapses : Global . _error ( \"set_delay with variable delays: the sizes do not match. You have to provide one value for each existing synapse.\" ) if len ( value ) != len ( self . post_ranks ): Global . _error ( \"set_delay with variable delays: the sizes do not match. You have to provide one value for each existing synapse.\" ) # Convert to steps if isinstance ( value , np . ndarray ): delays = [[ max ( 1 , round ( value [ i , j ] / Global . config [ 'dt' ])) for j in range ( value . shape [ 1 ])] for i in range ( value . shape [ 0 ])] else : delays = [[ max ( 1 , round ( v / Global . config [ 'dt' ])) for v in c ] for c in value ] # Max delay max_delay = max ([ max ( l ) for l in delays ]) if max_delay > self . max_delay : self . max_delay = max_delay # Send the max delay to the pre population if isinstance ( self . pre , PopulationView ): self . pre . population . max_delay = max ( self . max_delay , self . pre . population . max_delay ) self . pre . population . cyInstance . update_max_delay ( self . pre . population . max_delay ) else : self . pre . max_delay = max ( self . max_delay , self . pre . max_delay ) self . pre . cyInstance . update_max_delay ( self . pre . max_delay ) # Send the new values to the projection self . cyInstance . set_delay ( delays ) # Update ring buffers (if there exist) self . cyInstance . update_max_delay ( self . max_delay ) else : # before compile() Global . _error ( \"set_delay before compile(): not implemented yet.\" ) ################################ ## Access to functions ################################ def _function ( self , func ): \"Access a user defined function\" if not self . initialized : Global . _error ( 'the network is not compiled yet, cannot access the function ' + func ) return getattr ( self . cyInstance , func ) ################################ ## Learning flags ################################ def enable_learning ( self , period = None , offset = None ): \"\"\" Enables learning for all the synapses of this projection. For example, providing the following parameters at time 10 ms: ```python enable_learning(period=10., offset=5.) ``` would call the updating methods at times 15, 25, 35, etc... The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of ``dt``. :param period: determines how often the synaptic variables will be updated. :param offset: determines the offset at which the synaptic variables will be updated relative to the current time. \"\"\" # Check arguments if not period is None and not offset is None : if offset >= period : Global . _error ( 'enable_learning(): the offset must be smaller than the period.' ) if period is None and not offset is None : Global . _error ( 'enable_learning(): if you define an offset, you have to define a period.' ) try : self . cyInstance . _set_update ( True ) self . cyInstance . _set_plasticity ( True ) if period != None : self . cyInstance . _set_update_period ( int ( period / Global . config [ 'dt' ])) else : self . cyInstance . _set_update_period ( int ( 1 )) period = Global . config [ 'dt' ] if offset != None : relative_offset = Global . get_time () % period + offset self . cyInstance . _set_update_offset ( int ( int ( relative_offset % period ) / Global . config [ 'dt' ])) else : self . cyInstance . _set_update_offset ( int ( 0 )) except : Global . _warning ( 'Enable_learning() is only possible after compile()' ) def disable_learning ( self , update = None ): \"\"\" Disables learning for all synapses of this projection. The effect depends on the rate-coded or spiking nature of the projection: * **Rate-coded**: the updating of all synaptic variables is disabled (including the weights ``w``). This is equivalent to ``proj.update = False``. * **Spiking**: the updating of the weights ``w`` is disabled, but all other variables are updated. This is equivalent to ``proj.plasticity = False``. This method is useful when performing some tests on a trained network without messing with the learned weights. \"\"\" try : if self . synapse_type . type == 'rate' : self . cyInstance . _set_update ( False ) else : self . cyInstance . _set_plasticity ( False ) except : Global . _warning ( 'disabling learning is only possible after compile().' ) ################################ ## Methods on connectivity matrix ################################ def save_connectivity ( self , filename ): \"\"\" Saves the connectivity of the projection into a file. Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables. The generated data can be used to create a projection in another network: ```python proj.connect_from_file(filename) ``` * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. :param filename: file name, may contain relative or absolute path. \"\"\" # Check that the network is compiled if not self . initialized : Global . _error ( 'save_connectivity(): the network has not been compiled yet.' ) return # Check if the repertory exist ( path , fname ) = os . path . split ( filename ) if not path == '' : if not os . path . isdir ( path ): Global . _print ( 'Creating folder' , path ) os . mkdir ( path ) extension = os . path . splitext ( fname )[ 1 ] # Gathering the data data = { 'name' : self . name , 'post_ranks' : self . post_ranks , 'pre_ranks' : np . array ( self . cyInstance . pre_rank_all (), dtype = object ), 'w' : np . array ( self . w , dtype = object ), 'delay' : np . array ( self . cyInstance . get_delay (), dtype = object ) if hasattr ( self . cyInstance , 'get_delay' ) else None , 'max_delay' : self . max_delay , 'uniform_delay' : self . uniform_delay , 'size' : self . size , 'nb_synapses' : self . cyInstance . nb_synapses () } # Save the data if extension == '.gz' : Global . _print ( \"Saving connectivity in gunzipped binary format...\" ) try : import gzip except : Global . _error ( 'gzip is not installed.' ) return with gzip . open ( filename , mode = 'wb' ) as w_file : try : pickle . dump ( data , w_file , protocol = pickle . HIGHEST_PROTOCOL ) except Exception as e : Global . _print ( 'Error while saving in gzipped binary format.' ) Global . _print ( e ) return elif extension == '.npz' : Global . _print ( \"Saving connectivity in Numpy format...\" ) np . savez_compressed ( filename , ** data ) elif extension == '.mat' : Global . _print ( \"Saving connectivity in Matlab format...\" ) if data [ 'delay' ] is None : data [ 'delay' ] = 0 try : import scipy.io as sio sio . savemat ( filename , data ) except Exception as e : Global . _error ( 'Error while saving in Matlab format.' ) Global . _print ( e ) return else : Global . _print ( \"Saving connectivity in text format...\" ) # save in Pythons pickle format with open ( filename , mode = 'wb' ) as w_file : try : pickle . dump ( data , w_file , protocol = pickle . HIGHEST_PROTOCOL ) except Exception as e : Global . _print ( 'Error while saving in text format.' ) Global . _print ( e ) return return def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"\"\" Gathers all receptive fields within this projection. :param variable: name of the variable :param in_post_geometry: if False, the data will be plotted as square grid. (default = True) \"\"\" if in_post_geometry : x_size = self . post . geometry [ 1 ] y_size = self . post . geometry [ 0 ] else : x_size = int ( math . floor ( math . sqrt ( self . post . size )) ) y_size = int ( math . ceil ( math . sqrt ( self . post . size )) ) def get_rf ( rank ): # TODO: IMPROVE res = np . zeros ( self . pre . size ) for n in range ( len ( self . post_ranks )): if self . post_ranks [ n ] == n : pre_ranks = self . cyInstance . pre_rank ( n ) data = self . cyInstance . get_local_attribute_row ( variable , rank , Global . config [ \"precision\" ]) for j in range ( len ( pre_ranks )): res [ pre_ranks [ j ]] = data [ j ] return res . reshape ( self . pre . geometry ) res = np . zeros (( 1 , x_size * self . pre . geometry [ 1 ])) for y in range ( y_size ): row = np . concatenate ( [ get_rf ( self . post . rank_from_coordinates ( ( y , x ) ) ) for x in range ( x_size ) ], axis = 1 ) res = np . concatenate (( res , row )) return res def connectivity_matrix ( self , fill = 0.0 ): \"\"\" Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations. The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones. If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default. :param fill: value to put in the matrix when there is no connection (default: 0.0). \"\"\" if not self . initialized : Global . _error ( 'The connectivity matrix can only be accessed after compilation' ) # get correct dimensions for dense matrix if isinstance ( self . pre , PopulationView ): size_pre = self . pre . population . size else : size_pre = self . pre . size if isinstance ( self . post , PopulationView ): size_post = self . post . population . size else : size_post = self . post . size # create empty dense matrix with default values res = np . ones (( size_post , size_pre )) * fill # fill row-by-row with real values for rank in self . post_ranks : # row-rank if self . _storage_format == \"dense\" : idx = rank else : idx = self . post_ranks . index ( rank ) # pre-ranks preranks = self . cyInstance . pre_rank ( idx ) # get the values if \"w\" in self . synapse_type . description [ 'local' ] and ( not self . _has_single_weight ()): w = self . cyInstance . get_local_attribute_row ( \"w\" , idx , Global . config [ \"precision\" ]) elif \"w\" in self . synapse_type . description [ 'semiglobal' ]: w = self . cyInstance . get_semiglobal_attribute ( \"w\" , idx , Global . config [ \"precision\" ]) * np . ones ( self . cyInstance . dendrite_size ( idx )) else : w = self . cyInstance . get_global_attribute ( \"w\" , Global . config [ \"precision\" ]) * np . ones ( self . cyInstance . dendrite_size ( idx )) res [ rank , preranks ] = w return res ################################ ## Save/load methods ################################ def _data ( self ): \"Method gathering all info about the projection when calling save()\" if not self . initialized : Global . _error ( 'save_connectivity(): the network has not been compiled yet.' ) desc = {} desc [ 'name' ] = self . name desc [ 'pre' ] = self . pre . name desc [ 'post' ] = self . post . name desc [ 'target' ] = self . target desc [ 'post_ranks' ] = self . post_ranks desc [ 'attributes' ] = self . attributes desc [ 'parameters' ] = self . parameters desc [ 'variables' ] = self . variables desc [ 'delays' ] = self . _get_delay () # Determine if we have varying number of elements per row # based on the pre-synaptic ranks pre_ranks = self . cyInstance . pre_rank_all () dend_size = len ( pre_ranks [ 0 ]) ragged_list = False for i in range ( 1 , len ( pre_ranks )): if len ( pre_ranks [ i ]) != dend_size : ragged_list = True break # Save pre_ranks if ragged_list : desc [ 'pre_ranks' ] = np . array ( self . cyInstance . pre_rank_all (), dtype = object ) else : desc [ 'pre_ranks' ] = np . array ( self . cyInstance . pre_rank_all ()) # Attributes to save attributes = self . attributes if not 'w' in self . attributes : attributes . append ( 'w' ) # Save all attributes for var in attributes : try : ctype = self . _get_attribute_cpp_type ( var ) if var == \"w\" and self . _has_single_weight (): desc [ var ] = self . cyInstance . get_global_attribute ( \"w\" , ctype ) elif var in self . synapse_type . description [ 'local' ]: if ragged_list : desc [ var ] = np . array ( self . cyInstance . get_local_attribute_all ( var , ctype ), dtype = object ) else : desc [ var ] = self . cyInstance . get_local_attribute_all ( var , ctype ) elif var in self . synapse_type . description [ 'semiglobal' ]: desc [ var ] = self . cyInstance . get_semiglobal_attribute_all ( var , ctype ) else : desc [ var ] = self . cyInstance . get_global_attribute ( var , ctype ) # linear array or single constant except : Global . _warning ( 'Can not save the attribute ' + var + ' in the projection.' ) return desc def save ( self , filename ): \"\"\" Saves all information about the projection (connectivity, current value of parameters and variables) into a file. * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. :param filename: file name, may contain relative or absolute path. **Warning:** the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: ```python proj.save('proj1.npz') proj.save('proj1.txt') proj.save('proj1.txt.gz') proj.save('proj1.mat') ``` \"\"\" from ANNarchy.core.IO import _save_data _save_data ( filename , self . _data ()) def load ( self , filename ): \"\"\" Loads the saved state of the projection by `Projection.save()`. Warning: Matlab data can not be loaded. Example: ```python proj.load('proj1.npz') proj.load('proj1.txt') proj.load('proj1.txt.gz') ``` :param filename: the file name with relative or absolute path. \"\"\" from ANNarchy.core.IO import _load_connectivity_data self . _load_proj_data ( _load_connectivity_data ( filename )) def _load_proj_data ( self , desc ): \"\"\" Updates the projection with the stored data set. \"\"\" # Sanity check if desc == None : # _load_proj should have printed an error message return # If it's not saveable there is nothing to load if not self . _saveable : return # Check deprecation if not 'attributes' in desc . keys (): Global . _error ( 'The file was saved using a deprecated version of ANNarchy.' ) return if 'dendrites' in desc : # Saved before 4.5.3 Global . _error ( \"The file was saved using a deprecated version of ANNarchy.\" ) return # If the post ranks and/or pre-ranks have changed, overwrite connectivity_changed = False if 'post_ranks' in desc and not np . all (( desc [ 'post_ranks' ]) == self . post_ranks ): connectivity_changed = True if 'pre_ranks' in desc and not np . all (( desc [ 'pre_ranks' ]) == np . array ( self . cyInstance . pre_rank_all (), dtype = object )): connectivity_changed = True # synaptic weights weights = desc [ \"w\" ] # Delays can be either uniform (int, float) or non-uniform (np.ndarray). # HD (30th May 2022): # Unfortunately, the storage of constants changed over the time. At the # end of this code block, we should have either a single constant or a # numpy nd-array delays = 0 if 'delays' in desc : delays = desc [ 'delays' ] if isinstance ( delays , ( float , int )): # will be handled below pass elif isinstance ( delays , np . ndarray ): # constants are stored as 0-darray if delays . ndim == 0 : # transform into single float delays = float ( delays ) else : # nothing to do as it is numpy nd-array pass else : # ragged list to nd-array delays = np . array ( delays , dtype = object ) # Some patterns like fixed_number_pre/post or fixed_probability change the # connectivity. If this is not the case, we can simply set the values. if connectivity_changed : # (re-)initialize connectivity if isinstance ( delays , ( float , int )): delays = [[ delays ]] # wrapper expects list from list self . cyInstance . init_from_lil ( desc [ 'post_ranks' ], desc [ 'pre_ranks' ], weights , delays ) else : # set weights self . _set_cython_attribute ( \"w\" , weights ) # set delays if there were some self . _set_delay ( delays ) # Other variables for var in desc [ 'attributes' ]: if var == \"w\" : continue # already done try : self . _set_cython_attribute ( var , desc [ var ]) except Exception as e : Global . _print ( e ) Global . _warning ( 'load(): the variable' , var , 'does not exist in the current version of the network, skipping it.' ) continue ################################ ## Structural plasticity ################################ def start_pruning ( self , period = None ): \"\"\" Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). :param period: how often pruning should be evaluated (default: dt, i.e. each step) \"\"\" if not period : period = Global . config [ 'dt' ] if not self . cyInstance : Global . _error ( 'Can not start pruning if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . start_pruning ( int ( period / Global . config [ 'dt' ]), Global . get_current_step ()) except : Global . _error ( \"The synapse does not define a 'pruning' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start pruning connections.\" ) def stop_pruning ( self ): \"\"\" Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). \"\"\" if not self . cyInstance : Global . _error ( 'Can not stop pruning if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . stop_pruning () except : Global . _error ( \"The synapse does not define a 'pruning' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start pruning connections.\" ) def start_creating ( self , period = None ): \"\"\" Starts creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). :param period: how often creating should be evaluated (default: dt, i.e. each step) \"\"\" if not period : period = Global . config [ 'dt' ] if not self . cyInstance : Global . _error ( 'Can not start creating if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . start_creating ( int ( period / Global . config [ 'dt' ]), Global . get_current_step ()) except : Global . _error ( \"The synapse does not define a 'creating' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start creating connections.\" ) def stop_creating ( self ): \"\"\" Stops creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). \"\"\" if not self . cyInstance : Global . _error ( 'Can not stop creating if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . stop_creating () except : Global . _error ( \"The synapse does not define a 'creating' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start creating connections.\" ) ################################ # Paradigm specific functions ################################ def update_launch_config ( self , nb_blocks =- 1 , threads_per_block = 32 ): \"\"\" Since ANNarchy 4.7.2 we allow the adjustment of the CUDA launch config. Parameters: :nb_blocks: number of CUDA blocks which can be 65535 at maximum. If set to -1 the number of launched blocks is computed by ANNarchy. :threads_per_block: number of CUDA threads for one block which can be maximum 1024. \"\"\" if not Global . _check_paradigm ( \"cuda\" ): Global . _warning ( \"Projection.update_launch_config() is intended for usage on CUDA devices\" ) return if self . initialized : self . cyInstance . update_launch_config ( nb_blocks = nb_blocks , threads_per_block = threads_per_block ) else : Global . _error ( \"Projection.update_launch_config() should be called after compile()\" ) ################################ ## Memory Management ################################ def size_in_bytes ( self ): \"\"\" Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked. \"\"\" if self . initialized : return self . cyInstance . size_in_bytes () else : return 0 def _clear ( self ): \"\"\" Deallocates the container within the C++ instance. The population object is not usable anymore after calling this function. Warning: should be only called by the net deconstructor (in the context of parallel_run). \"\"\" if self . initialized : self . cyInstance . clear () self . initialized = False dendrites property # Iteratively returns the dendrites corresponding to this projection. nb_synapses property # Total number of synapses in the projection. size property # Number of post-synaptic neurons receiving synapses. __init__ ( pre , post , target , synapse = None , name = None , disable_omp = True , copied = False ) # By default, the synapse only ensures linear synaptic transmission: For rate-coded populations: psp = w * pre.r For spiking populations: g_target += w to modify this behavior one need to provide a Synapse object. Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection. required synapse a Synapse instance. None name unique name of the projection (optional, it defaults to proj0 , proj1 , etc). None disable_omp especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to False . True Source code in ANNarchy/core/Projection.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def __init__ ( self , pre , post , target , synapse = None , name = None , disable_omp = True , copied = False ): \"\"\" By default, the synapse only ensures linear synaptic transmission: * For rate-coded populations: ``psp = w * pre.r`` * For spiking populations: ``g_target += w`` to modify this behavior one need to provide a Synapse object. :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection. :param synapse: a ``Synapse`` instance. :param name: unique name of the projection (optional, it defaults to ``proj0``, ``proj1``, etc). :param disable_omp: especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to `False`. \"\"\" # Check if the network has already been compiled if Global . _network [ 0 ][ 'compiled' ] and not copied : Global . _error ( 'you cannot add a projection after the network has been compiled.' ) # Store the pre and post synaptic populations # the user provide either a string or a population object # in case of string, we need to search for the corresponding object if isinstance ( pre , str ): for pop in Global . _network [ 0 ][ 'populations' ]: if pop . name == pre : self . pre = pop else : self . pre = pre if isinstance ( post , str ): for pop in Global . _network [ 0 ][ 'populations' ]: if pop . name == post : self . post = pop else : self . post = post # Store the arguments if isinstance ( target , list ) and len ( target ) == 1 : self . target = target [ 0 ] else : self . target = target # Add the target(s) to the postsynaptic population if isinstance ( self . target , list ): for _target in self . target : self . post . targets . append ( _target ) else : self . post . targets . append ( self . target ) # check if a synapse description is attached if not synapse : # No synapse attached assume default synapse based on # presynaptic population. if self . pre . neuron_type . type == 'rate' : from ANNarchy.models.Synapses import DefaultRateCodedSynapse self . synapse_type = DefaultRateCodedSynapse () self . synapse_type . type = 'rate' else : from ANNarchy.models.Synapses import DefaultSpikingSynapse self . synapse_type = DefaultSpikingSynapse () self . synapse_type . type = 'spike' elif inspect . isclass ( synapse ): self . synapse_type = synapse () self . synapse_type . type = self . pre . neuron_type . type else : self . synapse_type = copy . deepcopy ( synapse ) self . synapse_type . type = self . pre . neuron_type . type # Disable omp for spiking networks self . disable_omp = disable_omp # Analyse the parameters and variables self . synapse_type . _analyse () # Create a default name self . id = len ( Global . _network [ 0 ][ 'projections' ]) if name : self . name = name else : self . name = 'proj' + str ( self . id ) # Get a list of parameters and variables self . parameters = [] self . init = {} for param in self . synapse_type . description [ 'parameters' ]: self . parameters . append ( param [ 'name' ]) self . init [ param [ 'name' ]] = param [ 'init' ] self . variables = [] for var in self . synapse_type . description [ 'variables' ]: self . variables . append ( var [ 'name' ]) self . init [ var [ 'name' ]] = var [ 'init' ] self . attributes = self . parameters + self . variables # Get a list of user-defined functions self . functions = [ func [ 'name' ] for func in self . synapse_type . description [ 'functions' ]] # Add the population to the global network Global . _network [ 0 ][ 'projections' ] . append ( self ) # Finalize initialization self . initialized = False # Cython instance self . cyInstance = None # Connectivity self . _synapses = None self . _connection_method = None self . _connection_args = None self . _connection_delay = None self . _connector = None self . _lil_connectivity = None # Default configuration for connectivity self . _storage_format = \"lil\" self . _storage_order = \"post_to_pre\" # If a single weight value is used self . _single_constant_weight = False # Are random distribution used for weights/delays self . connector_weight_dist = None self . connector_delay_dist = None # Reporting self . connector_name = \"Specific\" self . connector_description = \"Specific\" # Overwritten by derived classes, to add # additional code self . _specific_template = {} # Set to False by derived classes to prevent saving of # data, e. g. in case of weight-sharing projections self . _saveable = True # To allow case-specific adjustment of parallelization # parameters, e. g. openMP schedule, we introduce a # dictionary read by the ProjectionGenerator. # # Will be overwritten either by inherited classes or # by an omp_config provided to the compile() method. self . _omp_config = { #'psp_schedule': 'schedule(dynamic)' } # If set to true, the code generator is not allowed to # split the matrix. This will be the case for many # SpecificProjections defined by the user or is disabled # globally. if self . synapse_type . type == \"rate\" : # Normally, the split should not be used for rate-coded models # but maybe there are cases where we want to enable it ... self . _no_split_matrix = Global . config [ \"disable_split_matrix\" ] # If the number of elements is too small, the split # might not be efficient. if self . post . size < Global . OMP_MIN_NB_NEURONS : self . _no_split_matrix = True else : # If the number of elements is too small, the split # might not be efficient. if self . post . size < Global . OMP_MIN_NB_NEURONS : self . _no_split_matrix = True else : self . _no_split_matrix = False # In particular for spiking models, the parallelization on the # inner or outer loop can make a performance difference if self . _no_split_matrix : # LIL and CSR are parallelized on inner loop # to prevent cost of atomic operations self . _parallel_pattern = 'inner_loop' else : # splitted matrices are always parallelized on outer loop! self . _parallel_pattern = 'outer_loop' # For dense matrix format: do we use an optimization for population views? if self . synapse_type . type == \"rate\" : # HD (9th Nov. 2022): currently this optimization is only intended for spiking models self . _has_pop_view = False else : # HD (9th Nov. 2022): currently disabled, more testing is required ... self . _has_pop_view = False #isinstance(self.pre, PopulationView) or isinstance(self.post, PopulationView) connectivity_matrix ( fill = 0.0 ) # Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations. The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones. If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default. Parameters: Name Type Description Default fill value to put in the matrix when there is no connection (default: 0.0). 0.0 Source code in ANNarchy/core/Projection.py 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 def connectivity_matrix ( self , fill = 0.0 ): \"\"\" Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations. The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones. If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default. :param fill: value to put in the matrix when there is no connection (default: 0.0). \"\"\" if not self . initialized : Global . _error ( 'The connectivity matrix can only be accessed after compilation' ) # get correct dimensions for dense matrix if isinstance ( self . pre , PopulationView ): size_pre = self . pre . population . size else : size_pre = self . pre . size if isinstance ( self . post , PopulationView ): size_post = self . post . population . size else : size_post = self . post . size # create empty dense matrix with default values res = np . ones (( size_post , size_pre )) * fill # fill row-by-row with real values for rank in self . post_ranks : # row-rank if self . _storage_format == \"dense\" : idx = rank else : idx = self . post_ranks . index ( rank ) # pre-ranks preranks = self . cyInstance . pre_rank ( idx ) # get the values if \"w\" in self . synapse_type . description [ 'local' ] and ( not self . _has_single_weight ()): w = self . cyInstance . get_local_attribute_row ( \"w\" , idx , Global . config [ \"precision\" ]) elif \"w\" in self . synapse_type . description [ 'semiglobal' ]: w = self . cyInstance . get_semiglobal_attribute ( \"w\" , idx , Global . config [ \"precision\" ]) * np . ones ( self . cyInstance . dendrite_size ( idx )) else : w = self . cyInstance . get_global_attribute ( \"w\" , Global . config [ \"precision\" ]) * np . ones ( self . cyInstance . dendrite_size ( idx )) res [ rank , preranks ] = w return res dendrite ( post ) # Returns the dendrite of a postsynaptic neuron according to its rank. Parameters: Name Type Description Default post can be either the rank or the coordinates of the post-synaptic neuron. required Source code in ANNarchy/core/Projection.py 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 def dendrite ( self , post ): \"\"\" Returns the dendrite of a postsynaptic neuron according to its rank. :param post: can be either the rank or the coordinates of the post-synaptic neuron. \"\"\" if not self . initialized : Global . _error ( 'dendrites can only be accessed after compilation.' ) if isinstance ( post , int ): rank = post else : rank = self . post . rank_from_coordinates ( post ) if rank in self . post_ranks : return Dendrite ( self , rank , self . post_ranks . index ( rank )) else : Global . _error ( \" The neuron of rank \" + str ( rank ) + \" has no dendrite in this projection.\" , exit = True ) disable_learning ( update = None ) # Disables learning for all synapses of this projection. The effect depends on the rate-coded or spiking nature of the projection: Rate-coded : the updating of all synaptic variables is disabled (including the weights w ). This is equivalent to proj.update = False . Spiking : the updating of the weights w is disabled, but all other variables are updated. This is equivalent to proj.plasticity = False . This method is useful when performing some tests on a trained network without messing with the learned weights. Source code in ANNarchy/core/Projection.py 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 def disable_learning ( self , update = None ): \"\"\" Disables learning for all synapses of this projection. The effect depends on the rate-coded or spiking nature of the projection: * **Rate-coded**: the updating of all synaptic variables is disabled (including the weights ``w``). This is equivalent to ``proj.update = False``. * **Spiking**: the updating of the weights ``w`` is disabled, but all other variables are updated. This is equivalent to ``proj.plasticity = False``. This method is useful when performing some tests on a trained network without messing with the learned weights. \"\"\" try : if self . synapse_type . type == 'rate' : self . cyInstance . _set_update ( False ) else : self . cyInstance . _set_plasticity ( False ) except : Global . _warning ( 'disabling learning is only possible after compile().' ) enable_learning ( period = None , offset = None ) # Enables learning for all the synapses of this projection. For example, providing the following parameters at time 10 ms: enable_learning ( period = 10. , offset = 5. ) would call the updating methods at times 15, 25, 35, etc... The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of dt . Parameters: Name Type Description Default period determines how often the synaptic variables will be updated. None offset determines the offset at which the synaptic variables will be updated relative to the current time. None Source code in ANNarchy/core/Projection.py 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 def enable_learning ( self , period = None , offset = None ): \"\"\" Enables learning for all the synapses of this projection. For example, providing the following parameters at time 10 ms: ```python enable_learning(period=10., offset=5.) ``` would call the updating methods at times 15, 25, 35, etc... The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of ``dt``. :param period: determines how often the synaptic variables will be updated. :param offset: determines the offset at which the synaptic variables will be updated relative to the current time. \"\"\" # Check arguments if not period is None and not offset is None : if offset >= period : Global . _error ( 'enable_learning(): the offset must be smaller than the period.' ) if period is None and not offset is None : Global . _error ( 'enable_learning(): if you define an offset, you have to define a period.' ) try : self . cyInstance . _set_update ( True ) self . cyInstance . _set_plasticity ( True ) if period != None : self . cyInstance . _set_update_period ( int ( period / Global . config [ 'dt' ])) else : self . cyInstance . _set_update_period ( int ( 1 )) period = Global . config [ 'dt' ] if offset != None : relative_offset = Global . get_time () % period + offset self . cyInstance . _set_update_offset ( int ( int ( relative_offset % period ) / Global . config [ 'dt' ])) else : self . cyInstance . _set_update_offset ( int ( 0 )) except : Global . _warning ( 'Enable_learning() is only possible after compile()' ) get ( name ) # Returns a list of parameters/variables values for each dendrite in the projection. The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it. Parameters: Name Type Description Default name the name of the parameter or variable required Source code in ANNarchy/core/Projection.py 696 697 698 699 700 701 702 703 704 def get ( self , name ): \"\"\" Returns a list of parameters/variables values for each dendrite in the projection. The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it. :param name: the name of the parameter or variable \"\"\" return self . __getattr__ ( name ) load ( filename ) # Loads the saved state of the projection by Projection.save() . Warning: Matlab data can not be loaded. Example: proj . load ( 'proj1.npz' ) proj . load ( 'proj1.txt' ) proj . load ( 'proj1.txt.gz' ) Parameters: Name Type Description Default filename the file name with relative or absolute path. required Source code in ANNarchy/core/Projection.py 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 def load ( self , filename ): \"\"\" Loads the saved state of the projection by `Projection.save()`. Warning: Matlab data can not be loaded. Example: ```python proj.load('proj1.npz') proj.load('proj1.txt') proj.load('proj1.txt.gz') ``` :param filename: the file name with relative or absolute path. \"\"\" from ANNarchy.core.IO import _load_connectivity_data self . _load_proj_data ( _load_connectivity_data ( filename )) nb_efferent_synapses () # Number of efferent connections. Intended only for spiking models. Source code in ANNarchy/core/Projection.py 623 624 625 626 627 628 def nb_efferent_synapses ( self ): \"Number of efferent connections. Intended only for spiking models.\" if self . synapse_type . type == \"rate\" : Global . _error ( \"Projection.nb_efferent_synapses() is not available for rate-coded projections.\" ) return self . cyInstance . nb_efferent_synapses () nb_synapses_per_dendrite () # Total number of synapses for each dendrite as a list. Source code in ANNarchy/core/Projection.py 616 617 618 619 620 621 def nb_synapses_per_dendrite ( self ): \"Total number of synapses for each dendrite as a list.\" if self . cyInstance is None : Global . _warning ( \"Access 'nb_synapses_per_dendrite' attribute of a Projection is only valid after compile()\" ) return [] return [ self . cyInstance . dendrite_size ( n ) for n in range ( self . size )] receptive_fields ( variable = 'w' , in_post_geometry = True ) # Gathers all receptive fields within this projection. Parameters: Name Type Description Default variable name of the variable 'w' in_post_geometry if False, the data will be plotted as square grid. (default = True) True Source code in ANNarchy/core/Projection.py 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"\"\" Gathers all receptive fields within this projection. :param variable: name of the variable :param in_post_geometry: if False, the data will be plotted as square grid. (default = True) \"\"\" if in_post_geometry : x_size = self . post . geometry [ 1 ] y_size = self . post . geometry [ 0 ] else : x_size = int ( math . floor ( math . sqrt ( self . post . size )) ) y_size = int ( math . ceil ( math . sqrt ( self . post . size )) ) def get_rf ( rank ): # TODO: IMPROVE res = np . zeros ( self . pre . size ) for n in range ( len ( self . post_ranks )): if self . post_ranks [ n ] == n : pre_ranks = self . cyInstance . pre_rank ( n ) data = self . cyInstance . get_local_attribute_row ( variable , rank , Global . config [ \"precision\" ]) for j in range ( len ( pre_ranks )): res [ pre_ranks [ j ]] = data [ j ] return res . reshape ( self . pre . geometry ) res = np . zeros (( 1 , x_size * self . pre . geometry [ 1 ])) for y in range ( y_size ): row = np . concatenate ( [ get_rf ( self . post . rank_from_coordinates ( ( y , x ) ) ) for x in range ( x_size ) ], axis = 1 ) res = np . concatenate (( res , row )) return res reset ( attributes =- 1 , synapses = False ) # Resets all parameters and variables of the projection to the value they had before the call to compile. Note: Only parameters and variables are reinitialized, not the connectivity structure (including the weights and delays). The parameter synapses will be used in a future release to also reinitialize the connectivity structure. Parameters: Name Type Description Default attributes list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. -1 Source code in ANNarchy/core/Projection.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 def reset ( self , attributes =- 1 , synapses = False ): \"\"\" Resets all parameters and variables of the projection to the value they had before the call to compile. **Note:** Only parameters and variables are reinitialized, not the connectivity structure (including the weights and delays). The parameter ``synapses`` will be used in a future release to also reinitialize the connectivity structure. :param attributes: list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. \"\"\" if attributes == - 1 : attributes = self . attributes if synapses : # destroy the previous C++ content self . _clear () # call the init connectivity again self . _connect ( None ) self . initialized = True for var in attributes : # Skip w if var == 'w' : continue # check it exists if not var in self . attributes : Global . _warning ( \"Projection.reset():\" , var , \"is not an attribute of the population, won't reset.\" ) continue # Set the value try : self . __setattr__ ( var , self . init [ var ]) except Exception as e : Global . _print ( e ) Global . _warning ( \"Projection.reset(): something went wrong while resetting\" , var ) save ( filename ) # Saves all information about the projection (connectivity, current value of parameters and variables) into a file. If the file name is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. Otherwise, the data will be pickled into a simple binary text file using pickle. Parameters: Name Type Description Default filename file name, may contain relative or absolute path. Warning: the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: python proj.save('proj1.npz') proj.save('proj1.txt') proj.save('proj1.txt.gz') proj.save('proj1.mat') required Source code in ANNarchy/core/Projection.py 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 def save ( self , filename ): \"\"\" Saves all information about the projection (connectivity, current value of parameters and variables) into a file. * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. :param filename: file name, may contain relative or absolute path. **Warning:** the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: ```python proj.save('proj1.npz') proj.save('proj1.txt') proj.save('proj1.txt.gz') proj.save('proj1.mat') ``` \"\"\" from ANNarchy.core.IO import _save_data _save_data ( filename , self . _data ()) save_connectivity ( filename ) # Saves the connectivity of the projection into a file. Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables. The generated data can be used to create a projection in another network: proj . connect_from_file ( filename ) If the file name is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. Otherwise, the data will be pickled into a simple binary text file using pickle. Parameters: Name Type Description Default filename file name, may contain relative or absolute path. required Source code in ANNarchy/core/Projection.py 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 def save_connectivity ( self , filename ): \"\"\" Saves the connectivity of the projection into a file. Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables. The generated data can be used to create a projection in another network: ```python proj.connect_from_file(filename) ``` * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. :param filename: file name, may contain relative or absolute path. \"\"\" # Check that the network is compiled if not self . initialized : Global . _error ( 'save_connectivity(): the network has not been compiled yet.' ) return # Check if the repertory exist ( path , fname ) = os . path . split ( filename ) if not path == '' : if not os . path . isdir ( path ): Global . _print ( 'Creating folder' , path ) os . mkdir ( path ) extension = os . path . splitext ( fname )[ 1 ] # Gathering the data data = { 'name' : self . name , 'post_ranks' : self . post_ranks , 'pre_ranks' : np . array ( self . cyInstance . pre_rank_all (), dtype = object ), 'w' : np . array ( self . w , dtype = object ), 'delay' : np . array ( self . cyInstance . get_delay (), dtype = object ) if hasattr ( self . cyInstance , 'get_delay' ) else None , 'max_delay' : self . max_delay , 'uniform_delay' : self . uniform_delay , 'size' : self . size , 'nb_synapses' : self . cyInstance . nb_synapses () } # Save the data if extension == '.gz' : Global . _print ( \"Saving connectivity in gunzipped binary format...\" ) try : import gzip except : Global . _error ( 'gzip is not installed.' ) return with gzip . open ( filename , mode = 'wb' ) as w_file : try : pickle . dump ( data , w_file , protocol = pickle . HIGHEST_PROTOCOL ) except Exception as e : Global . _print ( 'Error while saving in gzipped binary format.' ) Global . _print ( e ) return elif extension == '.npz' : Global . _print ( \"Saving connectivity in Numpy format...\" ) np . savez_compressed ( filename , ** data ) elif extension == '.mat' : Global . _print ( \"Saving connectivity in Matlab format...\" ) if data [ 'delay' ] is None : data [ 'delay' ] = 0 try : import scipy.io as sio sio . savemat ( filename , data ) except Exception as e : Global . _error ( 'Error while saving in Matlab format.' ) Global . _print ( e ) return else : Global . _print ( \"Saving connectivity in text format...\" ) # save in Pythons pickle format with open ( filename , mode = 'wb' ) as w_file : try : pickle . dump ( data , w_file , protocol = pickle . HIGHEST_PROTOCOL ) except Exception as e : Global . _print ( 'Error while saving in text format.' ) Global . _print ( e ) return return set ( value ) # Sets the parameters/variables values for each dendrite in the projection. For parameters, you can provide: a single value, which will be the same for all dendrites. a list or 1D numpy array of the same length as the number of actual dendrites (self.size). For variables, you can provide: a single value, which will be the same for all synapses of all dendrites. a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value. Warning: it is not possible to set different values to each synapse using this method. One should iterate over the dendrites: for dendrite in proj . dendrites : dendrite . w = np . ones ( dendrite . size ) Parameters: Name Type Description Default value a dictionary with the name of the parameter/variable as key. required Source code in ANNarchy/core/Projection.py 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 def set ( self , value ): \"\"\" Sets the parameters/variables values for each dendrite in the projection. For parameters, you can provide: * a single value, which will be the same for all dendrites. * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). For variables, you can provide: * a single value, which will be the same for all synapses of all dendrites. * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value. **Warning:** it is not possible to set different values to each synapse using this method. One should iterate over the dendrites: ```python for dendrite in proj.dendrites: dendrite.w = np.ones(dendrite.size) ``` :param value: a dictionary with the name of the parameter/variable as key. \"\"\" for name , val in value . items (): self . __setattr__ ( name , val ) size_in_bytes () # Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked. Source code in ANNarchy/core/Projection.py 1521 1522 1523 1524 1525 1526 1527 1528 def size_in_bytes ( self ): \"\"\" Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked. \"\"\" if self . initialized : return self . cyInstance . size_in_bytes () else : return 0 start_creating ( period = None ) # Starts creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). Parameters: Name Type Description Default period how often creating should be evaluated (default: dt, i.e. each step) None Source code in ANNarchy/core/Projection.py 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 def start_creating ( self , period = None ): \"\"\" Starts creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). :param period: how often creating should be evaluated (default: dt, i.e. each step) \"\"\" if not period : period = Global . config [ 'dt' ] if not self . cyInstance : Global . _error ( 'Can not start creating if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . start_creating ( int ( period / Global . config [ 'dt' ]), Global . get_current_step ()) except : Global . _error ( \"The synapse does not define a 'creating' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start creating connections.\" ) start_pruning ( period = None ) # Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). Parameters: Name Type Description Default period how often pruning should be evaluated (default: dt, i.e. each step) None Source code in ANNarchy/core/Projection.py 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 def start_pruning ( self , period = None ): \"\"\" Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). :param period: how often pruning should be evaluated (default: dt, i.e. each step) \"\"\" if not period : period = Global . config [ 'dt' ] if not self . cyInstance : Global . _error ( 'Can not start pruning if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . start_pruning ( int ( period / Global . config [ 'dt' ]), Global . get_current_step ()) except : Global . _error ( \"The synapse does not define a 'pruning' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start pruning connections.\" ) stop_creating () # Stops creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). Source code in ANNarchy/core/Projection.py 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 def stop_creating ( self ): \"\"\" Stops creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). \"\"\" if not self . cyInstance : Global . _error ( 'Can not stop creating if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . stop_creating () except : Global . _error ( \"The synapse does not define a 'creating' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start creating connections.\" ) stop_pruning () # Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). Source code in ANNarchy/core/Projection.py 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 def stop_pruning ( self ): \"\"\" Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). \"\"\" if not self . cyInstance : Global . _error ( 'Can not stop pruning if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . stop_pruning () except : Global . _error ( \"The synapse does not define a 'pruning' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start pruning connections.\" ) synapse ( pre , post ) # Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise. Parameters: Name Type Description Default pre rank of the pre-synaptic neuron. required post rank of the post-synaptic neuron. required Source code in ANNarchy/core/Projection.py 666 667 668 669 670 671 672 673 674 675 676 def synapse ( self , pre , post ): \"\"\" Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise. :param pre: rank of the pre-synaptic neuron. :param post: rank of the post-synaptic neuron. \"\"\" if not isinstance ( pre , int ) or not isinstance ( post , int ): Global . _error ( 'Projection.synapse() only accepts ranks for the pre and post neurons.' ) return self . dendrite ( post ) . synapse ( pre ) update_launch_config ( nb_blocks =- 1 , threads_per_block = 32 ) # Since ANNarchy 4.7.2 we allow the adjustment of the CUDA launch config. Parameters: :nb_blocks: number of CUDA blocks which can be 65535 at maximum. If set to -1 the number of launched blocks is computed by ANNarchy. :threads_per_block: number of CUDA threads for one block which can be maximum 1024. Source code in ANNarchy/core/Projection.py 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 def update_launch_config ( self , nb_blocks =- 1 , threads_per_block = 32 ): \"\"\" Since ANNarchy 4.7.2 we allow the adjustment of the CUDA launch config. Parameters: :nb_blocks: number of CUDA blocks which can be 65535 at maximum. If set to -1 the number of launched blocks is computed by ANNarchy. :threads_per_block: number of CUDA threads for one block which can be maximum 1024. \"\"\" if not Global . _check_paradigm ( \"cuda\" ): Global . _warning ( \"Projection.update_launch_config() is intended for usage on CUDA devices\" ) return if self . initialized : self . cyInstance . update_launch_config ( nb_blocks = nb_blocks , threads_per_block = threads_per_block ) else : Global . _error ( \"Projection.update_launch_config() should be called after compile()\" )","title":"Projection class"},{"location":"API/Projection.html#projection-class","text":"The class ANNarchy.Projection defines projections at the population level. A projection is an ensemble of connections (or synapses) between a subset of a population (called the pre-synaptic population) and a subset of another population (the post-synaptic population), with a specific connection type. The pre- and post-synaptic populations may be the same.","title":"Projection class"},{"location":"API/Projection.html#ANNarchy.Projection","text":"Bases: object Container for all the synapses of the same type between two populations. Source code in ANNarchy/core/Projection.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 class Projection ( object ): \"\"\" Container for all the synapses of the same type between two populations. \"\"\" def __init__ ( self , pre , post , target , synapse = None , name = None , disable_omp = True , copied = False ): \"\"\" By default, the synapse only ensures linear synaptic transmission: * For rate-coded populations: ``psp = w * pre.r`` * For spiking populations: ``g_target += w`` to modify this behavior one need to provide a Synapse object. :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection. :param synapse: a ``Synapse`` instance. :param name: unique name of the projection (optional, it defaults to ``proj0``, ``proj1``, etc). :param disable_omp: especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to `False`. \"\"\" # Check if the network has already been compiled if Global . _network [ 0 ][ 'compiled' ] and not copied : Global . _error ( 'you cannot add a projection after the network has been compiled.' ) # Store the pre and post synaptic populations # the user provide either a string or a population object # in case of string, we need to search for the corresponding object if isinstance ( pre , str ): for pop in Global . _network [ 0 ][ 'populations' ]: if pop . name == pre : self . pre = pop else : self . pre = pre if isinstance ( post , str ): for pop in Global . _network [ 0 ][ 'populations' ]: if pop . name == post : self . post = pop else : self . post = post # Store the arguments if isinstance ( target , list ) and len ( target ) == 1 : self . target = target [ 0 ] else : self . target = target # Add the target(s) to the postsynaptic population if isinstance ( self . target , list ): for _target in self . target : self . post . targets . append ( _target ) else : self . post . targets . append ( self . target ) # check if a synapse description is attached if not synapse : # No synapse attached assume default synapse based on # presynaptic population. if self . pre . neuron_type . type == 'rate' : from ANNarchy.models.Synapses import DefaultRateCodedSynapse self . synapse_type = DefaultRateCodedSynapse () self . synapse_type . type = 'rate' else : from ANNarchy.models.Synapses import DefaultSpikingSynapse self . synapse_type = DefaultSpikingSynapse () self . synapse_type . type = 'spike' elif inspect . isclass ( synapse ): self . synapse_type = synapse () self . synapse_type . type = self . pre . neuron_type . type else : self . synapse_type = copy . deepcopy ( synapse ) self . synapse_type . type = self . pre . neuron_type . type # Disable omp for spiking networks self . disable_omp = disable_omp # Analyse the parameters and variables self . synapse_type . _analyse () # Create a default name self . id = len ( Global . _network [ 0 ][ 'projections' ]) if name : self . name = name else : self . name = 'proj' + str ( self . id ) # Get a list of parameters and variables self . parameters = [] self . init = {} for param in self . synapse_type . description [ 'parameters' ]: self . parameters . append ( param [ 'name' ]) self . init [ param [ 'name' ]] = param [ 'init' ] self . variables = [] for var in self . synapse_type . description [ 'variables' ]: self . variables . append ( var [ 'name' ]) self . init [ var [ 'name' ]] = var [ 'init' ] self . attributes = self . parameters + self . variables # Get a list of user-defined functions self . functions = [ func [ 'name' ] for func in self . synapse_type . description [ 'functions' ]] # Add the population to the global network Global . _network [ 0 ][ 'projections' ] . append ( self ) # Finalize initialization self . initialized = False # Cython instance self . cyInstance = None # Connectivity self . _synapses = None self . _connection_method = None self . _connection_args = None self . _connection_delay = None self . _connector = None self . _lil_connectivity = None # Default configuration for connectivity self . _storage_format = \"lil\" self . _storage_order = \"post_to_pre\" # If a single weight value is used self . _single_constant_weight = False # Are random distribution used for weights/delays self . connector_weight_dist = None self . connector_delay_dist = None # Reporting self . connector_name = \"Specific\" self . connector_description = \"Specific\" # Overwritten by derived classes, to add # additional code self . _specific_template = {} # Set to False by derived classes to prevent saving of # data, e. g. in case of weight-sharing projections self . _saveable = True # To allow case-specific adjustment of parallelization # parameters, e. g. openMP schedule, we introduce a # dictionary read by the ProjectionGenerator. # # Will be overwritten either by inherited classes or # by an omp_config provided to the compile() method. self . _omp_config = { #'psp_schedule': 'schedule(dynamic)' } # If set to true, the code generator is not allowed to # split the matrix. This will be the case for many # SpecificProjections defined by the user or is disabled # globally. if self . synapse_type . type == \"rate\" : # Normally, the split should not be used for rate-coded models # but maybe there are cases where we want to enable it ... self . _no_split_matrix = Global . config [ \"disable_split_matrix\" ] # If the number of elements is too small, the split # might not be efficient. if self . post . size < Global . OMP_MIN_NB_NEURONS : self . _no_split_matrix = True else : # If the number of elements is too small, the split # might not be efficient. if self . post . size < Global . OMP_MIN_NB_NEURONS : self . _no_split_matrix = True else : self . _no_split_matrix = False # In particular for spiking models, the parallelization on the # inner or outer loop can make a performance difference if self . _no_split_matrix : # LIL and CSR are parallelized on inner loop # to prevent cost of atomic operations self . _parallel_pattern = 'inner_loop' else : # splitted matrices are always parallelized on outer loop! self . _parallel_pattern = 'outer_loop' # For dense matrix format: do we use an optimization for population views? if self . synapse_type . type == \"rate\" : # HD (9th Nov. 2022): currently this optimization is only intended for spiking models self . _has_pop_view = False else : # HD (9th Nov. 2022): currently disabled, more testing is required ... self . _has_pop_view = False #isinstance(self.pre, PopulationView) or isinstance(self.post, PopulationView) # Add defined connectors connect_one_to_one = ConnectorMethods . connect_one_to_one connect_all_to_all = ConnectorMethods . connect_all_to_all connect_gaussian = ConnectorMethods . connect_gaussian connect_dog = ConnectorMethods . connect_dog connect_fixed_probability = ConnectorMethods . connect_fixed_probability connect_fixed_number_pre = ConnectorMethods . connect_fixed_number_pre connect_fixed_number_post = ConnectorMethods . connect_fixed_number_post connect_with_func = ConnectorMethods . connect_with_func connect_from_matrix = ConnectorMethods . connect_from_matrix connect_from_matrix_market = ConnectorMethods . connect_from_matrix_market _load_from_matrix = ConnectorMethods . _load_from_matrix connect_from_sparse = ConnectorMethods . connect_from_sparse _load_from_sparse = ConnectorMethods . _load_from_sparse connect_from_file = ConnectorMethods . connect_from_file _load_from_lil = ConnectorMethods . _load_from_lil def _copy ( self , pre , post ): \"Returns a copy of the projection when creating networks. Internal use only.\" copied_proj = Projection ( pre = pre , post = post , target = self . target , synapse = self . synapse_type , name = self . name , disable_omp = self . disable_omp , copied = True ) # these flags are modified during connect_XXX called before Network() copied_proj . _single_constant_weight = self . _single_constant_weight copied_proj . connector_weight_dist = self . connector_weight_dist copied_proj . connector_delay_dist = self . connector_delay_dist copied_proj . connector_name = self . connector_name # Control flags for code generation (maybe modified by connect_XXX()) copied_proj . _storage_format = self . _storage_format copied_proj . _storage_order = self . _storage_order copied_proj . _no_split_matrix = self . _no_split_matrix # for some projection types saving is not allowed (e. g. Convolution, Pooling) copied_proj . _saveable = self . _saveable # optional flags if hasattr ( self , \"_bsr_size\" ): copied_proj . _bsr_size = self . _bsr_size return copied_proj def _generate ( self ): \"Overriden by specific projections to generate the code\" pass def _instantiate ( self , module ): \"\"\" Instantiates the projection after compilation. The function should be called by Compiler._instantiate(). :param: module cython module (ANNarchyCore instance) \"\"\" if Global . config [ \"profiling\" ]: import time t1 = time . time () self . initialized = self . _connect ( module ) if Global . config [ \"profiling\" ]: t2 = time . time () Global . _profiler . add_entry ( t1 , t2 , \"proj\" + str ( self . id ), \"instantiate\" ) def _init_attributes ( self ): \"\"\" Method used after compilation to initialize the attributes. The function should be called by Compiler._instantiate \"\"\" for name , val in self . init . items (): # the weights ('w') are already inited by the _connect() method. if not name in [ 'w' ]: self . __setattr__ ( name , val ) def _connect ( self , module ): \"\"\" Builds up dendrites either from list or dictionary. Called by instantiate(). :param: module cython module (ANNarchyCore instance) :return: True, if the connector was successfully instantiated. Potential errors are kept by Python exceptions. If the Cython - connector call fails (return False) the most likely reason is that there was not enough memory available. \"\"\" # Local import to prevent circular import (HD: 28th June 2021) from ANNarchy.generator.Utils import cpp_connector_available # Sanity check if not self . _connection_method : Global . _error ( 'The projection between ' + self . pre . name + ' and ' + self . post . name + ' is declared but not connected.' ) # Debug printout if Global . config [ \"verbose\" ]: print ( \"Connectivity parameter (\" + self . name + \"):\" , self . _connection_args ) # Instantiate the Cython wrapper if not self . cyInstance : cy_wrapper = getattr ( module , 'proj' + str ( self . id ) + '_wrapper' ) self . cyInstance = cy_wrapper () # Check if there is a specialized CPP connector if not cpp_connector_available ( self . connector_name , self . _storage_format , self . _storage_order ): # No default connector -> initialize from LIL if self . _lil_connectivity : return self . cyInstance . init_from_lil_connectivity ( self . _lil_connectivity ) else : return self . cyInstance . init_from_lil_connectivity ( self . _connection_method ( * (( self . pre , self . post ,) + self . _connection_args ))) else : # fixed probability pattern if self . connector_name == \"Random\" : p = self . _connection_args [ 0 ] allow_self_connections = self . _connection_args [ 3 ] if isinstance ( self . _connection_args [ 1 ], RandomDistribution ): #some kind of distribution w_dist_arg1 , w_dist_arg2 = self . _connection_args [ 1 ] . get_cpp_args () else : # constant w_dist_arg1 = self . _connection_args [ 1 ] w_dist_arg2 = self . _connection_args [ 1 ] if isinstance ( self . _connection_args [ 2 ], RandomDistribution ): #some kind of distribution d_dist_arg1 , d_dist_arg2 = self . _connection_args [ 2 ] . get_cpp_args () else : # constant d_dist_arg1 = self . _connection_args [ 2 ] d_dist_arg2 = self . _connection_args [ 2 ] return self . cyInstance . fixed_probability ( self . post . ranks , self . pre . ranks , p , w_dist_arg1 , w_dist_arg2 , d_dist_arg1 , d_dist_arg2 , allow_self_connections ) # fixed number pre prattern elif self . connector_name == \"Random Convergent\" : number_nonzero = self . _connection_args [ 0 ] if isinstance ( self . _connection_args [ 1 ], RandomDistribution ): #some kind of distribution w_dist_arg1 , w_dist_arg2 = self . _connection_args [ 1 ] . get_cpp_args () else : # constant w_dist_arg1 = self . _connection_args [ 1 ] w_dist_arg2 = self . _connection_args [ 1 ] if isinstance ( self . _connection_args [ 2 ], RandomDistribution ): #some kind of distribution d_dist_arg1 , d_dist_arg2 = self . _connection_args [ 2 ] . get_cpp_args () else : # constant d_dist_arg1 = self . _connection_args [ 2 ] d_dist_arg2 = self . _connection_args [ 2 ] return self . cyInstance . fixed_number_pre ( self . post . ranks , self . pre . ranks , number_nonzero , w_dist_arg1 , w_dist_arg2 , d_dist_arg1 , d_dist_arg2 ) else : # This should never happen ... Global . _error ( \"No initialization for CPP-connector defined ...\" ) # should be never reached ... return False def _store_connectivity ( self , method , args , delay , storage_format , storage_order ): \"\"\" Store connectivity data. This function is called from cython_ext.Connectors module. \"\"\" # No format specified for this projection by the user, so fall-back to Global setting if storage_format is None : if Global . config [ 'sparse_matrix_format' ] == \"default\" : if Global . _check_paradigm ( \"openmp\" ): storage_format = \"lil\" elif Global . _check_paradigm ( \"cuda\" ): storage_format = \"csr\" else : raise NotImplementedError else : storage_format = Global . config [ \"sparse_matrix_format\" ] # Sanity checks if self . _connection_method != None : Global . _warning ( \"Projection \" , self . name , \" was already connected ... data will be overwritten.\" ) # Store connectivity pattern parameters self . _connection_method = method self . _connection_args = args self . _connection_delay = delay self . _storage_format = storage_format self . _storage_order = storage_order # The user selected nothing therefore we use the standard since ANNarchy 4.4.0 if storage_format == None : self . _storage_format = \"lil\" if storage_order == None : if storage_format == \"auto\" : storage_order = \"auto\" else : self . _storage_order = \"post_to_pre\" # The user selected automatic format selection using heuristics if storage_format == \"auto\" : self . _storage_format = self . _automatic_format_selection () if storage_order == \"auto\" : self . _storage_order = self . _automatic_order_selection () # Analyse the delay if isinstance ( delay , ( int , float )): # Uniform delay self . max_delay = round ( delay / Global . config [ 'dt' ]) self . uniform_delay = round ( delay / Global . config [ 'dt' ]) elif isinstance ( delay , RandomDistribution ): # Non-uniform delay self . uniform_delay = - 1 # Ensure no negative delays are generated if delay . min is None or delay . min < Global . config [ 'dt' ]: delay . min = Global . config [ 'dt' ] # The user needs to provide a max in order to compute max_delay if delay . max is None : Global . _error ( 'Projection.connect_xxx(): if you use a non-bounded random distribution for the delays (e.g. Normal), you need to set the max argument to limit the maximal delay.' ) self . max_delay = round ( delay . max / Global . config [ 'dt' ]) elif isinstance ( delay , ( list , np . ndarray )): # connect_from_matrix/sparse if len ( delay ) > 0 : self . uniform_delay = - 1 self . max_delay = round ( max ([ max ( l ) for l in delay ]) / Global . config [ 'dt' ]) else : # list is empty, no delay self . max_delay = - 1 self . uniform_delay = - 1 else : Global . _error ( 'Projection.connect_xxx(): delays are not valid!' ) # Transmit the max delay to the pre pop if isinstance ( self . pre , PopulationView ): self . pre . population . max_delay = max ( self . max_delay , self . pre . population . max_delay ) else : self . pre . max_delay = max ( self . max_delay , self . pre . max_delay ) def _automatic_format_selection ( self ): \"\"\" We check some heuristics to select a specific format implemented as decision tree: - If the filling degree is high enough a full matrix representation might be better - if the average row length is below a threshold the ELLPACK-R might be better - if the average row length is higher than a threshold the CSR might be better HD (17th Jan. 2022): Currently structural plasticity is only usable with LIL. But one could also apply it for dense matrices in the future. For CSR and in particular the ELL- like formats the potential memory-reallocations make the structural plasticity a costly operation. \"\"\" # Connection pattern / Feature specific selection if Global . config [ \"structural_plasticity\" ]: storage_format = \"lil\" elif self . connector_name == \"All-to-All\" : storage_format = \"dense\" elif self . connector_name == \"One-to-One\" : if Global . _check_paradigm ( \"cuda\" ): storage_format = \"csr\" else : storage_format = \"lil\" else : if self . synapse_type . type == \"spike\" : # we need to build up the matrix to analyze self . _lil_connectivity = self . _connection_method ( * (( self . pre , self . post ,) + self . _connection_args )) # get the decision parameter density = float ( self . _lil_connectivity . nb_synapses ) / float ( self . pre . size * self . post . size ) if density >= 0.6 : if Global . _check_paradigm ( \"cuda\" ): storage_format = \"csr\" # HD (11th Nov. 2022): there is no Dense_T for spiking and CUDA yet else : storage_format = \"dense\" else : storage_format = \"csr\" else : # we need to build up the matrix to analyze self . _lil_connectivity = self . _connection_method ( * (( self . pre , self . post ,) + self . _connection_args )) # get the decision parameter density = float ( self . _lil_connectivity . nb_synapses ) / float ( self . pre . size * self . post . size ) avg_nnz_per_row , _ = self . _lil_connectivity . compute_average_row_length () # heuristic decision tree if density >= 0.6 : storage_format = \"dense\" else : if Global . _check_paradigm ( \"cuda\" ): if avg_nnz_per_row <= 128 : storage_format = \"ellr\" else : storage_format = \"csr\" else : storage_format = \"csr\" Global . _info ( \"Automatic format selection for\" , self . name , \":\" , storage_format ) return storage_format def _automatic_order_selection ( self ): \"\"\" Contrary to the matrix format, the decision for the matrix order is majorly dependent on the synapse type. \"\"\" if self . synapse_type == \"rate\" : storage_order = \"post_to_pre\" else : if Global . _check_paradigm ( \"cuda\" ): # HD (11th Nov. 2022): there is no Dense_T / CSRC_T for spiking and CUDA yet storage_order = \"post_to_pre\" else : # pre-to-post is not implemented for all formats if self . _storage_format in [ \"dense\" , \"csr\" ]: storage_order = \"pre_to_post\" else : storage_order = \"post_to_pre\" Global . _info ( \"Automatic matrix order selection for\" , self . name , \":\" , storage_order ) return storage_order def _has_single_weight ( self ): \"If a single weight should be generated instead of a LIL\" is_cpu = Global . config [ 'paradigm' ] == \"openmp\" has_constant_weight = self . _single_constant_weight not_dense = not ( self . _storage_format == \"dense\" ) no_structural_plasticity = not Global . config [ 'structural_plasticity' ] no_synaptic_plasticity = not self . synapse_type . description [ 'plasticity' ] return has_constant_weight and no_structural_plasticity and no_synaptic_plasticity and is_cpu and not_dense def reset ( self , attributes =- 1 , synapses = False ): \"\"\" Resets all parameters and variables of the projection to the value they had before the call to compile. **Note:** Only parameters and variables are reinitialized, not the connectivity structure (including the weights and delays). The parameter ``synapses`` will be used in a future release to also reinitialize the connectivity structure. :param attributes: list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. \"\"\" if attributes == - 1 : attributes = self . attributes if synapses : # destroy the previous C++ content self . _clear () # call the init connectivity again self . _connect ( None ) self . initialized = True for var in attributes : # Skip w if var == 'w' : continue # check it exists if not var in self . attributes : Global . _warning ( \"Projection.reset():\" , var , \"is not an attribute of the population, won't reset.\" ) continue # Set the value try : self . __setattr__ ( var , self . init [ var ]) except Exception as e : Global . _print ( e ) Global . _warning ( \"Projection.reset(): something went wrong while resetting\" , var ) #Global._warning('Projection.reset(): only parameters and variables are reinitialized, not the connectivity structure (including the weights)...') ################################ ## Dendrite access ################################ @property def size ( self ): \"Number of post-synaptic neurons receiving synapses.\" if self . cyInstance == None : Global . _warning ( \"Access 'size or len()' attribute of a Projection is only valid after compile()\" ) return 0 return len ( self . cyInstance . post_rank ()) def __len__ ( self ): # Number of postsynaptic neurons receiving synapses in this projection. return self . size @property def nb_synapses ( self ): \"Total number of synapses in the projection.\" if self . cyInstance is None : Global . _warning ( \"Access 'nb_synapses' attribute of a Projection is only valid after compile()\" ) return 0 return self . cyInstance . nb_synapses () def nb_synapses_per_dendrite ( self ): \"Total number of synapses for each dendrite as a list.\" if self . cyInstance is None : Global . _warning ( \"Access 'nb_synapses_per_dendrite' attribute of a Projection is only valid after compile()\" ) return [] return [ self . cyInstance . dendrite_size ( n ) for n in range ( self . size )] def nb_efferent_synapses ( self ): \"Number of efferent connections. Intended only for spiking models.\" if self . synapse_type . type == \"rate\" : Global . _error ( \"Projection.nb_efferent_synapses() is not available for rate-coded projections.\" ) return self . cyInstance . nb_efferent_synapses () @property def post_ranks ( self ): if self . cyInstance : return self . cyInstance . post_rank () else : Global . _warning ( \"Access 'post_ranks' attribute of a Projection is only valid after compile()\" ) return None @property def dendrites ( self ): \"\"\" Iteratively returns the dendrites corresponding to this projection. \"\"\" for idx , n in enumerate ( self . post_ranks ): yield Dendrite ( self , n , idx ) def dendrite ( self , post ): \"\"\" Returns the dendrite of a postsynaptic neuron according to its rank. :param post: can be either the rank or the coordinates of the post-synaptic neuron. \"\"\" if not self . initialized : Global . _error ( 'dendrites can only be accessed after compilation.' ) if isinstance ( post , int ): rank = post else : rank = self . post . rank_from_coordinates ( post ) if rank in self . post_ranks : return Dendrite ( self , rank , self . post_ranks . index ( rank )) else : Global . _error ( \" The neuron of rank \" + str ( rank ) + \" has no dendrite in this projection.\" , exit = True ) def synapse ( self , pre , post ): \"\"\" Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise. :param pre: rank of the pre-synaptic neuron. :param post: rank of the post-synaptic neuron. \"\"\" if not isinstance ( pre , int ) or not isinstance ( post , int ): Global . _error ( 'Projection.synapse() only accepts ranks for the pre and post neurons.' ) return self . dendrite ( post ) . synapse ( pre ) # Iterators def __getitem__ ( self , * args , ** kwds ): # Returns dendrite of the given position in the postsynaptic population. # If only one argument is given, it is a rank. If it is a tuple, it is coordinates. if len ( args ) == 1 : return self . dendrite ( args [ 0 ]) return self . dendrite ( args ) def __iter__ ( self ): # Returns iteratively each dendrite in the population in ascending postsynaptic rank order. for idx , n in enumerate ( self . post_ranks ): yield Dendrite ( self , n , idx ) ################################ ## Access to attributes ################################ def get ( self , name ): \"\"\" Returns a list of parameters/variables values for each dendrite in the projection. The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it. :param name: the name of the parameter or variable \"\"\" return self . __getattr__ ( name ) def set ( self , value ): \"\"\" Sets the parameters/variables values for each dendrite in the projection. For parameters, you can provide: * a single value, which will be the same for all dendrites. * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). For variables, you can provide: * a single value, which will be the same for all synapses of all dendrites. * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value. **Warning:** it is not possible to set different values to each synapse using this method. One should iterate over the dendrites: ```python for dendrite in proj.dendrites: dendrite.w = np.ones(dendrite.size) ``` :param value: a dictionary with the name of the parameter/variable as key. \"\"\" for name , val in value . items (): self . __setattr__ ( name , val ) def __getattr__ ( self , name ): # Method called when accessing an attribute. if name == 'initialized' or not hasattr ( self , 'initialized' ): # Before the end of the constructor return object . __getattribute__ ( self , name ) elif hasattr ( self , 'attributes' ): if name in [ 'plasticity' , 'transmission' , 'update' ]: return self . _get_flag ( name ) if name in [ 'delay' ]: return self . _get_delay () if name in self . attributes : if not self . initialized : return self . init [ name ] else : return self . _get_cython_attribute ( name ) elif name in self . functions : return self . _function ( name ) else : return object . __getattribute__ ( self , name ) return object . __getattribute__ ( self , name ) def __setattr__ ( self , name , value ): # Method called when setting an attribute. if name == 'initialized' or not hasattr ( self , 'initialized' ): # Before the end of the constructor object . __setattr__ ( self , name , value ) elif hasattr ( self , 'attributes' ): if name in [ 'plasticity' , 'transmission' , 'update' ]: self . _set_flag ( name , bool ( value )) return if name in [ 'delay' ]: self . _set_delay ( value ) return if name in self . attributes : if not self . initialized : self . init [ name ] = value else : self . _set_cython_attribute ( name , value ) else : object . __setattr__ ( self , name , value ) else : object . __setattr__ ( self , name , value ) def _get_cython_attribute ( self , attribute ): \"\"\" Returns the value of the given attribute for all neurons in the population, as a list of lists having the same geometry as the population if it is local. :param attribute: a string representing the variables's name. \"\"\" # Determine C++ data type ctype = self . _get_attribute_cpp_type ( attribute = attribute ) # retrieve the value from C++ core if attribute == \"w\" and self . _has_single_weight (): return self . cyInstance . get_global_attribute ( attribute , ctype ) elif attribute in self . synapse_type . description [ 'local' ]: return self . cyInstance . get_local_attribute_all ( attribute , ctype ) elif attribute in self . synapse_type . description [ 'semiglobal' ]: return self . cyInstance . get_semiglobal_attribute_all ( attribute , ctype ) else : return self . cyInstance . get_global_attribute ( attribute , ctype ) def _set_cython_attribute ( self , attribute , value ): \"\"\" Sets the value of the given attribute for all post-synaptic neurons in the projection, as a NumPy array having the same geometry as the population if it is local. :param attribute: a string representing the variables's name. :param value: the value it should take. \"\"\" # Determine C++ data type ctype = self . _get_attribute_cpp_type ( attribute = attribute ) # Convert np.arrays into lists/constants for better iteration if isinstance ( value , np . ndarray ): if np . ndim ( value ) == 0 : value = float ( value ) else : value = list ( value ) # A list is given if isinstance ( value , list ): if len ( value ) == len ( self . post_ranks ): if attribute in self . synapse_type . description [ 'local' ]: for idx , n in enumerate ( self . post_ranks ): if not len ( value [ idx ]) == self . cyInstance . dendrite_size ( idx ): Global . _error ( 'The postynaptic neuron ' + str ( n ) + ' receives ' + str ( self . cyInstance . dendrite_size ( idx )) + ' synapses.' ) self . cyInstance . set_local_attribute_row ( attribute , idx , value [ idx ], ctype ) elif attribute in self . synapse_type . description [ 'semiglobal' ]: self . cyInstance . set_semiglobal_attribute_all ( attribute , value , ctype ) else : Global . _error ( 'The parameter' , attribute , 'is global to the population, cannot assign a list.' ) else : Global . _error ( 'The projection has' , self . size , 'post-synaptic neurons, the list must have the same size.' ) # A Random Distribution is given elif isinstance ( value , RandomDistribution ): if attribute == \"w\" and self . _has_single_weight (): self . cyInstance . set_global_attribute ( attribute , value . get_values ( 1 ), ctype ) elif attribute in self . synapse_type . description [ 'local' ]: for idx , n in enumerate ( self . post_ranks ): self . cyInstance . set_local_attribute_row ( attribute , idx , value . get_values ( self . cyInstance . dendrite_size ( idx )), ctype ) elif attribute in self . synapse_type . description [ 'semiglobal' ]: self . cyInstance . set_semiglobal_attribute_all ( attribute , value . get_values ( len ( self . post_ranks )), ctype ) elif attribute in self . synapse_type . description [ 'global' ]: self . cyInstance . set_global_attribute ( attribute , value . get_values ( 1 ), ctype ) # A single value is given else : if attribute == \"w\" and self . _has_single_weight (): self . cyInstance . set_global_attribute ( attribute , value , ctype ) elif attribute in self . synapse_type . description [ 'local' ]: for idx , n in enumerate ( self . post_ranks ): self . cyInstance . set_local_attribute_row ( attribute , idx , value * np . ones ( self . cyInstance . dendrite_size ( idx )), ctype ) elif attribute in self . synapse_type . description [ 'semiglobal' ]: self . cyInstance . set_semiglobal_attribute_all ( attribute , value * np . ones ( len ( self . post_ranks )), ctype ) else : self . cyInstance . set_global_attribute ( attribute , value , ctype ) def _get_attribute_cpp_type ( self , attribute ): \"\"\" Determine C++ data type for a given attribute \"\"\" ctype = None for var in self . synapse_type . description [ 'variables' ] + self . synapse_type . description [ 'parameters' ]: if var [ 'name' ] == attribute : ctype = var [ 'ctype' ] return ctype def _get_flag ( self , attribute ): \"flags such as learning, transmission\" return getattr ( self . cyInstance , '_get_' + attribute )() def _set_flag ( self , attribute , value ): \"flags such as learning, transmission\" getattr ( self . cyInstance , '_set_' + attribute )( value ) ################################ ## Access to delays ################################ def _get_delay ( self ): if not hasattr ( self . cyInstance , 'get_delay' ): if self . max_delay <= 1 : return Global . config [ 'dt' ] elif self . uniform_delay != - 1 : return self . uniform_delay * Global . config [ 'dt' ] else : return [[ pre * Global . config [ 'dt' ] for pre in post ] for post in self . cyInstance . get_delay ()] def _set_delay ( self , value ): if self . cyInstance : # After compile() if not hasattr ( self . cyInstance , 'get_delay' ): if self . max_delay <= 1 and value != Global . config [ 'dt' ]: Global . _error ( \"set_delay: the projection was instantiated without delays, it is too late to create them...\" ) elif self . uniform_delay != - 1 : if isinstance ( value , np . ndarray ): if value . ndim > 0 : Global . _error ( \"set_delay: the projection was instantiated with uniform delays, it is too late to load non-uniform values...\" ) else : value = max ( 1 , round ( float ( value ) / Global . config [ 'dt' ])) elif isinstance ( value , ( float , int )): value = max ( 1 , round ( float ( value ) / Global . config [ 'dt' ])) else : Global . _error ( \"set_delay: only float, int or np.array values are possible.\" ) # The new max_delay is higher than before if value > self . max_delay : self . max_delay = value self . uniform_delay = value self . cyInstance . set_delay ( value ) if isinstance ( self . pre , PopulationView ): self . pre . population . max_delay = max ( self . max_delay , self . pre . population . max_delay ) self . pre . population . cyInstance . update_max_delay ( self . pre . population . max_delay ) else : self . pre . max_delay = max ( self . max_delay , self . pre . max_delay ) self . pre . cyInstance . update_max_delay ( self . pre . max_delay ) return else : self . uniform_delay = value self . cyInstance . set_delay ( value ) else : # variable delays if not isinstance ( value , ( np . ndarray , list )): Global . _error ( \"set_delay with variable delays: you must provide a list of lists of exactly the same size as before.\" ) # Check the number of delays nb_values = sum ([ len ( s ) for s in value ]) if nb_values != self . nb_synapses : Global . _error ( \"set_delay with variable delays: the sizes do not match. You have to provide one value for each existing synapse.\" ) if len ( value ) != len ( self . post_ranks ): Global . _error ( \"set_delay with variable delays: the sizes do not match. You have to provide one value for each existing synapse.\" ) # Convert to steps if isinstance ( value , np . ndarray ): delays = [[ max ( 1 , round ( value [ i , j ] / Global . config [ 'dt' ])) for j in range ( value . shape [ 1 ])] for i in range ( value . shape [ 0 ])] else : delays = [[ max ( 1 , round ( v / Global . config [ 'dt' ])) for v in c ] for c in value ] # Max delay max_delay = max ([ max ( l ) for l in delays ]) if max_delay > self . max_delay : self . max_delay = max_delay # Send the max delay to the pre population if isinstance ( self . pre , PopulationView ): self . pre . population . max_delay = max ( self . max_delay , self . pre . population . max_delay ) self . pre . population . cyInstance . update_max_delay ( self . pre . population . max_delay ) else : self . pre . max_delay = max ( self . max_delay , self . pre . max_delay ) self . pre . cyInstance . update_max_delay ( self . pre . max_delay ) # Send the new values to the projection self . cyInstance . set_delay ( delays ) # Update ring buffers (if there exist) self . cyInstance . update_max_delay ( self . max_delay ) else : # before compile() Global . _error ( \"set_delay before compile(): not implemented yet.\" ) ################################ ## Access to functions ################################ def _function ( self , func ): \"Access a user defined function\" if not self . initialized : Global . _error ( 'the network is not compiled yet, cannot access the function ' + func ) return getattr ( self . cyInstance , func ) ################################ ## Learning flags ################################ def enable_learning ( self , period = None , offset = None ): \"\"\" Enables learning for all the synapses of this projection. For example, providing the following parameters at time 10 ms: ```python enable_learning(period=10., offset=5.) ``` would call the updating methods at times 15, 25, 35, etc... The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of ``dt``. :param period: determines how often the synaptic variables will be updated. :param offset: determines the offset at which the synaptic variables will be updated relative to the current time. \"\"\" # Check arguments if not period is None and not offset is None : if offset >= period : Global . _error ( 'enable_learning(): the offset must be smaller than the period.' ) if period is None and not offset is None : Global . _error ( 'enable_learning(): if you define an offset, you have to define a period.' ) try : self . cyInstance . _set_update ( True ) self . cyInstance . _set_plasticity ( True ) if period != None : self . cyInstance . _set_update_period ( int ( period / Global . config [ 'dt' ])) else : self . cyInstance . _set_update_period ( int ( 1 )) period = Global . config [ 'dt' ] if offset != None : relative_offset = Global . get_time () % period + offset self . cyInstance . _set_update_offset ( int ( int ( relative_offset % period ) / Global . config [ 'dt' ])) else : self . cyInstance . _set_update_offset ( int ( 0 )) except : Global . _warning ( 'Enable_learning() is only possible after compile()' ) def disable_learning ( self , update = None ): \"\"\" Disables learning for all synapses of this projection. The effect depends on the rate-coded or spiking nature of the projection: * **Rate-coded**: the updating of all synaptic variables is disabled (including the weights ``w``). This is equivalent to ``proj.update = False``. * **Spiking**: the updating of the weights ``w`` is disabled, but all other variables are updated. This is equivalent to ``proj.plasticity = False``. This method is useful when performing some tests on a trained network without messing with the learned weights. \"\"\" try : if self . synapse_type . type == 'rate' : self . cyInstance . _set_update ( False ) else : self . cyInstance . _set_plasticity ( False ) except : Global . _warning ( 'disabling learning is only possible after compile().' ) ################################ ## Methods on connectivity matrix ################################ def save_connectivity ( self , filename ): \"\"\" Saves the connectivity of the projection into a file. Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables. The generated data can be used to create a projection in another network: ```python proj.connect_from_file(filename) ``` * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. :param filename: file name, may contain relative or absolute path. \"\"\" # Check that the network is compiled if not self . initialized : Global . _error ( 'save_connectivity(): the network has not been compiled yet.' ) return # Check if the repertory exist ( path , fname ) = os . path . split ( filename ) if not path == '' : if not os . path . isdir ( path ): Global . _print ( 'Creating folder' , path ) os . mkdir ( path ) extension = os . path . splitext ( fname )[ 1 ] # Gathering the data data = { 'name' : self . name , 'post_ranks' : self . post_ranks , 'pre_ranks' : np . array ( self . cyInstance . pre_rank_all (), dtype = object ), 'w' : np . array ( self . w , dtype = object ), 'delay' : np . array ( self . cyInstance . get_delay (), dtype = object ) if hasattr ( self . cyInstance , 'get_delay' ) else None , 'max_delay' : self . max_delay , 'uniform_delay' : self . uniform_delay , 'size' : self . size , 'nb_synapses' : self . cyInstance . nb_synapses () } # Save the data if extension == '.gz' : Global . _print ( \"Saving connectivity in gunzipped binary format...\" ) try : import gzip except : Global . _error ( 'gzip is not installed.' ) return with gzip . open ( filename , mode = 'wb' ) as w_file : try : pickle . dump ( data , w_file , protocol = pickle . HIGHEST_PROTOCOL ) except Exception as e : Global . _print ( 'Error while saving in gzipped binary format.' ) Global . _print ( e ) return elif extension == '.npz' : Global . _print ( \"Saving connectivity in Numpy format...\" ) np . savez_compressed ( filename , ** data ) elif extension == '.mat' : Global . _print ( \"Saving connectivity in Matlab format...\" ) if data [ 'delay' ] is None : data [ 'delay' ] = 0 try : import scipy.io as sio sio . savemat ( filename , data ) except Exception as e : Global . _error ( 'Error while saving in Matlab format.' ) Global . _print ( e ) return else : Global . _print ( \"Saving connectivity in text format...\" ) # save in Pythons pickle format with open ( filename , mode = 'wb' ) as w_file : try : pickle . dump ( data , w_file , protocol = pickle . HIGHEST_PROTOCOL ) except Exception as e : Global . _print ( 'Error while saving in text format.' ) Global . _print ( e ) return return def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"\"\" Gathers all receptive fields within this projection. :param variable: name of the variable :param in_post_geometry: if False, the data will be plotted as square grid. (default = True) \"\"\" if in_post_geometry : x_size = self . post . geometry [ 1 ] y_size = self . post . geometry [ 0 ] else : x_size = int ( math . floor ( math . sqrt ( self . post . size )) ) y_size = int ( math . ceil ( math . sqrt ( self . post . size )) ) def get_rf ( rank ): # TODO: IMPROVE res = np . zeros ( self . pre . size ) for n in range ( len ( self . post_ranks )): if self . post_ranks [ n ] == n : pre_ranks = self . cyInstance . pre_rank ( n ) data = self . cyInstance . get_local_attribute_row ( variable , rank , Global . config [ \"precision\" ]) for j in range ( len ( pre_ranks )): res [ pre_ranks [ j ]] = data [ j ] return res . reshape ( self . pre . geometry ) res = np . zeros (( 1 , x_size * self . pre . geometry [ 1 ])) for y in range ( y_size ): row = np . concatenate ( [ get_rf ( self . post . rank_from_coordinates ( ( y , x ) ) ) for x in range ( x_size ) ], axis = 1 ) res = np . concatenate (( res , row )) return res def connectivity_matrix ( self , fill = 0.0 ): \"\"\" Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations. The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones. If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default. :param fill: value to put in the matrix when there is no connection (default: 0.0). \"\"\" if not self . initialized : Global . _error ( 'The connectivity matrix can only be accessed after compilation' ) # get correct dimensions for dense matrix if isinstance ( self . pre , PopulationView ): size_pre = self . pre . population . size else : size_pre = self . pre . size if isinstance ( self . post , PopulationView ): size_post = self . post . population . size else : size_post = self . post . size # create empty dense matrix with default values res = np . ones (( size_post , size_pre )) * fill # fill row-by-row with real values for rank in self . post_ranks : # row-rank if self . _storage_format == \"dense\" : idx = rank else : idx = self . post_ranks . index ( rank ) # pre-ranks preranks = self . cyInstance . pre_rank ( idx ) # get the values if \"w\" in self . synapse_type . description [ 'local' ] and ( not self . _has_single_weight ()): w = self . cyInstance . get_local_attribute_row ( \"w\" , idx , Global . config [ \"precision\" ]) elif \"w\" in self . synapse_type . description [ 'semiglobal' ]: w = self . cyInstance . get_semiglobal_attribute ( \"w\" , idx , Global . config [ \"precision\" ]) * np . ones ( self . cyInstance . dendrite_size ( idx )) else : w = self . cyInstance . get_global_attribute ( \"w\" , Global . config [ \"precision\" ]) * np . ones ( self . cyInstance . dendrite_size ( idx )) res [ rank , preranks ] = w return res ################################ ## Save/load methods ################################ def _data ( self ): \"Method gathering all info about the projection when calling save()\" if not self . initialized : Global . _error ( 'save_connectivity(): the network has not been compiled yet.' ) desc = {} desc [ 'name' ] = self . name desc [ 'pre' ] = self . pre . name desc [ 'post' ] = self . post . name desc [ 'target' ] = self . target desc [ 'post_ranks' ] = self . post_ranks desc [ 'attributes' ] = self . attributes desc [ 'parameters' ] = self . parameters desc [ 'variables' ] = self . variables desc [ 'delays' ] = self . _get_delay () # Determine if we have varying number of elements per row # based on the pre-synaptic ranks pre_ranks = self . cyInstance . pre_rank_all () dend_size = len ( pre_ranks [ 0 ]) ragged_list = False for i in range ( 1 , len ( pre_ranks )): if len ( pre_ranks [ i ]) != dend_size : ragged_list = True break # Save pre_ranks if ragged_list : desc [ 'pre_ranks' ] = np . array ( self . cyInstance . pre_rank_all (), dtype = object ) else : desc [ 'pre_ranks' ] = np . array ( self . cyInstance . pre_rank_all ()) # Attributes to save attributes = self . attributes if not 'w' in self . attributes : attributes . append ( 'w' ) # Save all attributes for var in attributes : try : ctype = self . _get_attribute_cpp_type ( var ) if var == \"w\" and self . _has_single_weight (): desc [ var ] = self . cyInstance . get_global_attribute ( \"w\" , ctype ) elif var in self . synapse_type . description [ 'local' ]: if ragged_list : desc [ var ] = np . array ( self . cyInstance . get_local_attribute_all ( var , ctype ), dtype = object ) else : desc [ var ] = self . cyInstance . get_local_attribute_all ( var , ctype ) elif var in self . synapse_type . description [ 'semiglobal' ]: desc [ var ] = self . cyInstance . get_semiglobal_attribute_all ( var , ctype ) else : desc [ var ] = self . cyInstance . get_global_attribute ( var , ctype ) # linear array or single constant except : Global . _warning ( 'Can not save the attribute ' + var + ' in the projection.' ) return desc def save ( self , filename ): \"\"\" Saves all information about the projection (connectivity, current value of parameters and variables) into a file. * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. :param filename: file name, may contain relative or absolute path. **Warning:** the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: ```python proj.save('proj1.npz') proj.save('proj1.txt') proj.save('proj1.txt.gz') proj.save('proj1.mat') ``` \"\"\" from ANNarchy.core.IO import _save_data _save_data ( filename , self . _data ()) def load ( self , filename ): \"\"\" Loads the saved state of the projection by `Projection.save()`. Warning: Matlab data can not be loaded. Example: ```python proj.load('proj1.npz') proj.load('proj1.txt') proj.load('proj1.txt.gz') ``` :param filename: the file name with relative or absolute path. \"\"\" from ANNarchy.core.IO import _load_connectivity_data self . _load_proj_data ( _load_connectivity_data ( filename )) def _load_proj_data ( self , desc ): \"\"\" Updates the projection with the stored data set. \"\"\" # Sanity check if desc == None : # _load_proj should have printed an error message return # If it's not saveable there is nothing to load if not self . _saveable : return # Check deprecation if not 'attributes' in desc . keys (): Global . _error ( 'The file was saved using a deprecated version of ANNarchy.' ) return if 'dendrites' in desc : # Saved before 4.5.3 Global . _error ( \"The file was saved using a deprecated version of ANNarchy.\" ) return # If the post ranks and/or pre-ranks have changed, overwrite connectivity_changed = False if 'post_ranks' in desc and not np . all (( desc [ 'post_ranks' ]) == self . post_ranks ): connectivity_changed = True if 'pre_ranks' in desc and not np . all (( desc [ 'pre_ranks' ]) == np . array ( self . cyInstance . pre_rank_all (), dtype = object )): connectivity_changed = True # synaptic weights weights = desc [ \"w\" ] # Delays can be either uniform (int, float) or non-uniform (np.ndarray). # HD (30th May 2022): # Unfortunately, the storage of constants changed over the time. At the # end of this code block, we should have either a single constant or a # numpy nd-array delays = 0 if 'delays' in desc : delays = desc [ 'delays' ] if isinstance ( delays , ( float , int )): # will be handled below pass elif isinstance ( delays , np . ndarray ): # constants are stored as 0-darray if delays . ndim == 0 : # transform into single float delays = float ( delays ) else : # nothing to do as it is numpy nd-array pass else : # ragged list to nd-array delays = np . array ( delays , dtype = object ) # Some patterns like fixed_number_pre/post or fixed_probability change the # connectivity. If this is not the case, we can simply set the values. if connectivity_changed : # (re-)initialize connectivity if isinstance ( delays , ( float , int )): delays = [[ delays ]] # wrapper expects list from list self . cyInstance . init_from_lil ( desc [ 'post_ranks' ], desc [ 'pre_ranks' ], weights , delays ) else : # set weights self . _set_cython_attribute ( \"w\" , weights ) # set delays if there were some self . _set_delay ( delays ) # Other variables for var in desc [ 'attributes' ]: if var == \"w\" : continue # already done try : self . _set_cython_attribute ( var , desc [ var ]) except Exception as e : Global . _print ( e ) Global . _warning ( 'load(): the variable' , var , 'does not exist in the current version of the network, skipping it.' ) continue ################################ ## Structural plasticity ################################ def start_pruning ( self , period = None ): \"\"\" Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). :param period: how often pruning should be evaluated (default: dt, i.e. each step) \"\"\" if not period : period = Global . config [ 'dt' ] if not self . cyInstance : Global . _error ( 'Can not start pruning if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . start_pruning ( int ( period / Global . config [ 'dt' ]), Global . get_current_step ()) except : Global . _error ( \"The synapse does not define a 'pruning' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start pruning connections.\" ) def stop_pruning ( self ): \"\"\" Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). \"\"\" if not self . cyInstance : Global . _error ( 'Can not stop pruning if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . stop_pruning () except : Global . _error ( \"The synapse does not define a 'pruning' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start pruning connections.\" ) def start_creating ( self , period = None ): \"\"\" Starts creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). :param period: how often creating should be evaluated (default: dt, i.e. each step) \"\"\" if not period : period = Global . config [ 'dt' ] if not self . cyInstance : Global . _error ( 'Can not start creating if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . start_creating ( int ( period / Global . config [ 'dt' ]), Global . get_current_step ()) except : Global . _error ( \"The synapse does not define a 'creating' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start creating connections.\" ) def stop_creating ( self ): \"\"\" Stops creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). \"\"\" if not self . cyInstance : Global . _error ( 'Can not stop creating if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . stop_creating () except : Global . _error ( \"The synapse does not define a 'creating' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start creating connections.\" ) ################################ # Paradigm specific functions ################################ def update_launch_config ( self , nb_blocks =- 1 , threads_per_block = 32 ): \"\"\" Since ANNarchy 4.7.2 we allow the adjustment of the CUDA launch config. Parameters: :nb_blocks: number of CUDA blocks which can be 65535 at maximum. If set to -1 the number of launched blocks is computed by ANNarchy. :threads_per_block: number of CUDA threads for one block which can be maximum 1024. \"\"\" if not Global . _check_paradigm ( \"cuda\" ): Global . _warning ( \"Projection.update_launch_config() is intended for usage on CUDA devices\" ) return if self . initialized : self . cyInstance . update_launch_config ( nb_blocks = nb_blocks , threads_per_block = threads_per_block ) else : Global . _error ( \"Projection.update_launch_config() should be called after compile()\" ) ################################ ## Memory Management ################################ def size_in_bytes ( self ): \"\"\" Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked. \"\"\" if self . initialized : return self . cyInstance . size_in_bytes () else : return 0 def _clear ( self ): \"\"\" Deallocates the container within the C++ instance. The population object is not usable anymore after calling this function. Warning: should be only called by the net deconstructor (in the context of parallel_run). \"\"\" if self . initialized : self . cyInstance . clear () self . initialized = False","title":"Projection"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.dendrites","text":"Iteratively returns the dendrites corresponding to this projection.","title":"dendrites"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.nb_synapses","text":"Total number of synapses in the projection.","title":"nb_synapses"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.size","text":"Number of post-synaptic neurons receiving synapses.","title":"size"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.__init__","text":"By default, the synapse only ensures linear synaptic transmission: For rate-coded populations: psp = w * pre.r For spiking populations: g_target += w to modify this behavior one need to provide a Synapse object. Parameters: Name Type Description Default pre pre-synaptic population (either its name or a Population object). required post post-synaptic population (either its name or a Population object). required target type of the connection. required synapse a Synapse instance. None name unique name of the projection (optional, it defaults to proj0 , proj1 , etc). None disable_omp especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to False . True Source code in ANNarchy/core/Projection.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def __init__ ( self , pre , post , target , synapse = None , name = None , disable_omp = True , copied = False ): \"\"\" By default, the synapse only ensures linear synaptic transmission: * For rate-coded populations: ``psp = w * pre.r`` * For spiking populations: ``g_target += w`` to modify this behavior one need to provide a Synapse object. :param pre: pre-synaptic population (either its name or a ``Population`` object). :param post: post-synaptic population (either its name or a ``Population`` object). :param target: type of the connection. :param synapse: a ``Synapse`` instance. :param name: unique name of the projection (optional, it defaults to ``proj0``, ``proj1``, etc). :param disable_omp: especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to `False`. \"\"\" # Check if the network has already been compiled if Global . _network [ 0 ][ 'compiled' ] and not copied : Global . _error ( 'you cannot add a projection after the network has been compiled.' ) # Store the pre and post synaptic populations # the user provide either a string or a population object # in case of string, we need to search for the corresponding object if isinstance ( pre , str ): for pop in Global . _network [ 0 ][ 'populations' ]: if pop . name == pre : self . pre = pop else : self . pre = pre if isinstance ( post , str ): for pop in Global . _network [ 0 ][ 'populations' ]: if pop . name == post : self . post = pop else : self . post = post # Store the arguments if isinstance ( target , list ) and len ( target ) == 1 : self . target = target [ 0 ] else : self . target = target # Add the target(s) to the postsynaptic population if isinstance ( self . target , list ): for _target in self . target : self . post . targets . append ( _target ) else : self . post . targets . append ( self . target ) # check if a synapse description is attached if not synapse : # No synapse attached assume default synapse based on # presynaptic population. if self . pre . neuron_type . type == 'rate' : from ANNarchy.models.Synapses import DefaultRateCodedSynapse self . synapse_type = DefaultRateCodedSynapse () self . synapse_type . type = 'rate' else : from ANNarchy.models.Synapses import DefaultSpikingSynapse self . synapse_type = DefaultSpikingSynapse () self . synapse_type . type = 'spike' elif inspect . isclass ( synapse ): self . synapse_type = synapse () self . synapse_type . type = self . pre . neuron_type . type else : self . synapse_type = copy . deepcopy ( synapse ) self . synapse_type . type = self . pre . neuron_type . type # Disable omp for spiking networks self . disable_omp = disable_omp # Analyse the parameters and variables self . synapse_type . _analyse () # Create a default name self . id = len ( Global . _network [ 0 ][ 'projections' ]) if name : self . name = name else : self . name = 'proj' + str ( self . id ) # Get a list of parameters and variables self . parameters = [] self . init = {} for param in self . synapse_type . description [ 'parameters' ]: self . parameters . append ( param [ 'name' ]) self . init [ param [ 'name' ]] = param [ 'init' ] self . variables = [] for var in self . synapse_type . description [ 'variables' ]: self . variables . append ( var [ 'name' ]) self . init [ var [ 'name' ]] = var [ 'init' ] self . attributes = self . parameters + self . variables # Get a list of user-defined functions self . functions = [ func [ 'name' ] for func in self . synapse_type . description [ 'functions' ]] # Add the population to the global network Global . _network [ 0 ][ 'projections' ] . append ( self ) # Finalize initialization self . initialized = False # Cython instance self . cyInstance = None # Connectivity self . _synapses = None self . _connection_method = None self . _connection_args = None self . _connection_delay = None self . _connector = None self . _lil_connectivity = None # Default configuration for connectivity self . _storage_format = \"lil\" self . _storage_order = \"post_to_pre\" # If a single weight value is used self . _single_constant_weight = False # Are random distribution used for weights/delays self . connector_weight_dist = None self . connector_delay_dist = None # Reporting self . connector_name = \"Specific\" self . connector_description = \"Specific\" # Overwritten by derived classes, to add # additional code self . _specific_template = {} # Set to False by derived classes to prevent saving of # data, e. g. in case of weight-sharing projections self . _saveable = True # To allow case-specific adjustment of parallelization # parameters, e. g. openMP schedule, we introduce a # dictionary read by the ProjectionGenerator. # # Will be overwritten either by inherited classes or # by an omp_config provided to the compile() method. self . _omp_config = { #'psp_schedule': 'schedule(dynamic)' } # If set to true, the code generator is not allowed to # split the matrix. This will be the case for many # SpecificProjections defined by the user or is disabled # globally. if self . synapse_type . type == \"rate\" : # Normally, the split should not be used for rate-coded models # but maybe there are cases where we want to enable it ... self . _no_split_matrix = Global . config [ \"disable_split_matrix\" ] # If the number of elements is too small, the split # might not be efficient. if self . post . size < Global . OMP_MIN_NB_NEURONS : self . _no_split_matrix = True else : # If the number of elements is too small, the split # might not be efficient. if self . post . size < Global . OMP_MIN_NB_NEURONS : self . _no_split_matrix = True else : self . _no_split_matrix = False # In particular for spiking models, the parallelization on the # inner or outer loop can make a performance difference if self . _no_split_matrix : # LIL and CSR are parallelized on inner loop # to prevent cost of atomic operations self . _parallel_pattern = 'inner_loop' else : # splitted matrices are always parallelized on outer loop! self . _parallel_pattern = 'outer_loop' # For dense matrix format: do we use an optimization for population views? if self . synapse_type . type == \"rate\" : # HD (9th Nov. 2022): currently this optimization is only intended for spiking models self . _has_pop_view = False else : # HD (9th Nov. 2022): currently disabled, more testing is required ... self . _has_pop_view = False #isinstance(self.pre, PopulationView) or isinstance(self.post, PopulationView)","title":"__init__()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.connectivity_matrix","text":"Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations. The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones. If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default. Parameters: Name Type Description Default fill value to put in the matrix when there is no connection (default: 0.0). 0.0 Source code in ANNarchy/core/Projection.py 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 def connectivity_matrix ( self , fill = 0.0 ): \"\"\" Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations. The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones. If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default. :param fill: value to put in the matrix when there is no connection (default: 0.0). \"\"\" if not self . initialized : Global . _error ( 'The connectivity matrix can only be accessed after compilation' ) # get correct dimensions for dense matrix if isinstance ( self . pre , PopulationView ): size_pre = self . pre . population . size else : size_pre = self . pre . size if isinstance ( self . post , PopulationView ): size_post = self . post . population . size else : size_post = self . post . size # create empty dense matrix with default values res = np . ones (( size_post , size_pre )) * fill # fill row-by-row with real values for rank in self . post_ranks : # row-rank if self . _storage_format == \"dense\" : idx = rank else : idx = self . post_ranks . index ( rank ) # pre-ranks preranks = self . cyInstance . pre_rank ( idx ) # get the values if \"w\" in self . synapse_type . description [ 'local' ] and ( not self . _has_single_weight ()): w = self . cyInstance . get_local_attribute_row ( \"w\" , idx , Global . config [ \"precision\" ]) elif \"w\" in self . synapse_type . description [ 'semiglobal' ]: w = self . cyInstance . get_semiglobal_attribute ( \"w\" , idx , Global . config [ \"precision\" ]) * np . ones ( self . cyInstance . dendrite_size ( idx )) else : w = self . cyInstance . get_global_attribute ( \"w\" , Global . config [ \"precision\" ]) * np . ones ( self . cyInstance . dendrite_size ( idx )) res [ rank , preranks ] = w return res","title":"connectivity_matrix()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.dendrite","text":"Returns the dendrite of a postsynaptic neuron according to its rank. Parameters: Name Type Description Default post can be either the rank or the coordinates of the post-synaptic neuron. required Source code in ANNarchy/core/Projection.py 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 def dendrite ( self , post ): \"\"\" Returns the dendrite of a postsynaptic neuron according to its rank. :param post: can be either the rank or the coordinates of the post-synaptic neuron. \"\"\" if not self . initialized : Global . _error ( 'dendrites can only be accessed after compilation.' ) if isinstance ( post , int ): rank = post else : rank = self . post . rank_from_coordinates ( post ) if rank in self . post_ranks : return Dendrite ( self , rank , self . post_ranks . index ( rank )) else : Global . _error ( \" The neuron of rank \" + str ( rank ) + \" has no dendrite in this projection.\" , exit = True )","title":"dendrite()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.disable_learning","text":"Disables learning for all synapses of this projection. The effect depends on the rate-coded or spiking nature of the projection: Rate-coded : the updating of all synaptic variables is disabled (including the weights w ). This is equivalent to proj.update = False . Spiking : the updating of the weights w is disabled, but all other variables are updated. This is equivalent to proj.plasticity = False . This method is useful when performing some tests on a trained network without messing with the learned weights. Source code in ANNarchy/core/Projection.py 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 def disable_learning ( self , update = None ): \"\"\" Disables learning for all synapses of this projection. The effect depends on the rate-coded or spiking nature of the projection: * **Rate-coded**: the updating of all synaptic variables is disabled (including the weights ``w``). This is equivalent to ``proj.update = False``. * **Spiking**: the updating of the weights ``w`` is disabled, but all other variables are updated. This is equivalent to ``proj.plasticity = False``. This method is useful when performing some tests on a trained network without messing with the learned weights. \"\"\" try : if self . synapse_type . type == 'rate' : self . cyInstance . _set_update ( False ) else : self . cyInstance . _set_plasticity ( False ) except : Global . _warning ( 'disabling learning is only possible after compile().' )","title":"disable_learning()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.enable_learning","text":"Enables learning for all the synapses of this projection. For example, providing the following parameters at time 10 ms: enable_learning ( period = 10. , offset = 5. ) would call the updating methods at times 15, 25, 35, etc... The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of dt . Parameters: Name Type Description Default period determines how often the synaptic variables will be updated. None offset determines the offset at which the synaptic variables will be updated relative to the current time. None Source code in ANNarchy/core/Projection.py 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 def enable_learning ( self , period = None , offset = None ): \"\"\" Enables learning for all the synapses of this projection. For example, providing the following parameters at time 10 ms: ```python enable_learning(period=10., offset=5.) ``` would call the updating methods at times 15, 25, 35, etc... The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of ``dt``. :param period: determines how often the synaptic variables will be updated. :param offset: determines the offset at which the synaptic variables will be updated relative to the current time. \"\"\" # Check arguments if not period is None and not offset is None : if offset >= period : Global . _error ( 'enable_learning(): the offset must be smaller than the period.' ) if period is None and not offset is None : Global . _error ( 'enable_learning(): if you define an offset, you have to define a period.' ) try : self . cyInstance . _set_update ( True ) self . cyInstance . _set_plasticity ( True ) if period != None : self . cyInstance . _set_update_period ( int ( period / Global . config [ 'dt' ])) else : self . cyInstance . _set_update_period ( int ( 1 )) period = Global . config [ 'dt' ] if offset != None : relative_offset = Global . get_time () % period + offset self . cyInstance . _set_update_offset ( int ( int ( relative_offset % period ) / Global . config [ 'dt' ])) else : self . cyInstance . _set_update_offset ( int ( 0 )) except : Global . _warning ( 'Enable_learning() is only possible after compile()' )","title":"enable_learning()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.get","text":"Returns a list of parameters/variables values for each dendrite in the projection. The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it. Parameters: Name Type Description Default name the name of the parameter or variable required Source code in ANNarchy/core/Projection.py 696 697 698 699 700 701 702 703 704 def get ( self , name ): \"\"\" Returns a list of parameters/variables values for each dendrite in the projection. The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it. :param name: the name of the parameter or variable \"\"\" return self . __getattr__ ( name )","title":"get()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.load","text":"Loads the saved state of the projection by Projection.save() . Warning: Matlab data can not be loaded. Example: proj . load ( 'proj1.npz' ) proj . load ( 'proj1.txt' ) proj . load ( 'proj1.txt.gz' ) Parameters: Name Type Description Default filename the file name with relative or absolute path. required Source code in ANNarchy/core/Projection.py 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 def load ( self , filename ): \"\"\" Loads the saved state of the projection by `Projection.save()`. Warning: Matlab data can not be loaded. Example: ```python proj.load('proj1.npz') proj.load('proj1.txt') proj.load('proj1.txt.gz') ``` :param filename: the file name with relative or absolute path. \"\"\" from ANNarchy.core.IO import _load_connectivity_data self . _load_proj_data ( _load_connectivity_data ( filename ))","title":"load()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.nb_efferent_synapses","text":"Number of efferent connections. Intended only for spiking models. Source code in ANNarchy/core/Projection.py 623 624 625 626 627 628 def nb_efferent_synapses ( self ): \"Number of efferent connections. Intended only for spiking models.\" if self . synapse_type . type == \"rate\" : Global . _error ( \"Projection.nb_efferent_synapses() is not available for rate-coded projections.\" ) return self . cyInstance . nb_efferent_synapses ()","title":"nb_efferent_synapses()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.nb_synapses_per_dendrite","text":"Total number of synapses for each dendrite as a list. Source code in ANNarchy/core/Projection.py 616 617 618 619 620 621 def nb_synapses_per_dendrite ( self ): \"Total number of synapses for each dendrite as a list.\" if self . cyInstance is None : Global . _warning ( \"Access 'nb_synapses_per_dendrite' attribute of a Projection is only valid after compile()\" ) return [] return [ self . cyInstance . dendrite_size ( n ) for n in range ( self . size )]","title":"nb_synapses_per_dendrite()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.receptive_fields","text":"Gathers all receptive fields within this projection. Parameters: Name Type Description Default variable name of the variable 'w' in_post_geometry if False, the data will be plotted as square grid. (default = True) True Source code in ANNarchy/core/Projection.py 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 def receptive_fields ( self , variable = 'w' , in_post_geometry = True ): \"\"\" Gathers all receptive fields within this projection. :param variable: name of the variable :param in_post_geometry: if False, the data will be plotted as square grid. (default = True) \"\"\" if in_post_geometry : x_size = self . post . geometry [ 1 ] y_size = self . post . geometry [ 0 ] else : x_size = int ( math . floor ( math . sqrt ( self . post . size )) ) y_size = int ( math . ceil ( math . sqrt ( self . post . size )) ) def get_rf ( rank ): # TODO: IMPROVE res = np . zeros ( self . pre . size ) for n in range ( len ( self . post_ranks )): if self . post_ranks [ n ] == n : pre_ranks = self . cyInstance . pre_rank ( n ) data = self . cyInstance . get_local_attribute_row ( variable , rank , Global . config [ \"precision\" ]) for j in range ( len ( pre_ranks )): res [ pre_ranks [ j ]] = data [ j ] return res . reshape ( self . pre . geometry ) res = np . zeros (( 1 , x_size * self . pre . geometry [ 1 ])) for y in range ( y_size ): row = np . concatenate ( [ get_rf ( self . post . rank_from_coordinates ( ( y , x ) ) ) for x in range ( x_size ) ], axis = 1 ) res = np . concatenate (( res , row )) return res","title":"receptive_fields()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.reset","text":"Resets all parameters and variables of the projection to the value they had before the call to compile. Note: Only parameters and variables are reinitialized, not the connectivity structure (including the weights and delays). The parameter synapses will be used in a future release to also reinitialize the connectivity structure. Parameters: Name Type Description Default attributes list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. -1 Source code in ANNarchy/core/Projection.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 def reset ( self , attributes =- 1 , synapses = False ): \"\"\" Resets all parameters and variables of the projection to the value they had before the call to compile. **Note:** Only parameters and variables are reinitialized, not the connectivity structure (including the weights and delays). The parameter ``synapses`` will be used in a future release to also reinitialize the connectivity structure. :param attributes: list of attributes (parameter or variable) which should be reinitialized. Default: all attributes. \"\"\" if attributes == - 1 : attributes = self . attributes if synapses : # destroy the previous C++ content self . _clear () # call the init connectivity again self . _connect ( None ) self . initialized = True for var in attributes : # Skip w if var == 'w' : continue # check it exists if not var in self . attributes : Global . _warning ( \"Projection.reset():\" , var , \"is not an attribute of the population, won't reset.\" ) continue # Set the value try : self . __setattr__ ( var , self . init [ var ]) except Exception as e : Global . _print ( e ) Global . _warning ( \"Projection.reset(): something went wrong while resetting\" , var )","title":"reset()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.save","text":"Saves all information about the projection (connectivity, current value of parameters and variables) into a file. If the file name is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. Otherwise, the data will be pickled into a simple binary text file using pickle. Parameters: Name Type Description Default filename file name, may contain relative or absolute path. Warning: the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: python proj.save('proj1.npz') proj.save('proj1.txt') proj.save('proj1.txt.gz') proj.save('proj1.mat') required Source code in ANNarchy/core/Projection.py 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 def save ( self , filename ): \"\"\" Saves all information about the projection (connectivity, current value of parameters and variables) into a file. * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. :param filename: file name, may contain relative or absolute path. **Warning:** the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose. Example: ```python proj.save('proj1.npz') proj.save('proj1.txt') proj.save('proj1.txt.gz') proj.save('proj1.mat') ``` \"\"\" from ANNarchy.core.IO import _save_data _save_data ( filename , self . _data ())","title":"save()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.save_connectivity","text":"Saves the connectivity of the projection into a file. Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables. The generated data can be used to create a projection in another network: proj . connect_from_file ( filename ) If the file name is '.npz', the data will be saved and compressed using np.savez_compressed (recommended). If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. Otherwise, the data will be pickled into a simple binary text file using pickle. Parameters: Name Type Description Default filename file name, may contain relative or absolute path. required Source code in ANNarchy/core/Projection.py 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 def save_connectivity ( self , filename ): \"\"\" Saves the connectivity of the projection into a file. Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables. The generated data can be used to create a projection in another network: ```python proj.connect_from_file(filename) ``` * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended). * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip. * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed. * Otherwise, the data will be pickled into a simple binary text file using pickle. :param filename: file name, may contain relative or absolute path. \"\"\" # Check that the network is compiled if not self . initialized : Global . _error ( 'save_connectivity(): the network has not been compiled yet.' ) return # Check if the repertory exist ( path , fname ) = os . path . split ( filename ) if not path == '' : if not os . path . isdir ( path ): Global . _print ( 'Creating folder' , path ) os . mkdir ( path ) extension = os . path . splitext ( fname )[ 1 ] # Gathering the data data = { 'name' : self . name , 'post_ranks' : self . post_ranks , 'pre_ranks' : np . array ( self . cyInstance . pre_rank_all (), dtype = object ), 'w' : np . array ( self . w , dtype = object ), 'delay' : np . array ( self . cyInstance . get_delay (), dtype = object ) if hasattr ( self . cyInstance , 'get_delay' ) else None , 'max_delay' : self . max_delay , 'uniform_delay' : self . uniform_delay , 'size' : self . size , 'nb_synapses' : self . cyInstance . nb_synapses () } # Save the data if extension == '.gz' : Global . _print ( \"Saving connectivity in gunzipped binary format...\" ) try : import gzip except : Global . _error ( 'gzip is not installed.' ) return with gzip . open ( filename , mode = 'wb' ) as w_file : try : pickle . dump ( data , w_file , protocol = pickle . HIGHEST_PROTOCOL ) except Exception as e : Global . _print ( 'Error while saving in gzipped binary format.' ) Global . _print ( e ) return elif extension == '.npz' : Global . _print ( \"Saving connectivity in Numpy format...\" ) np . savez_compressed ( filename , ** data ) elif extension == '.mat' : Global . _print ( \"Saving connectivity in Matlab format...\" ) if data [ 'delay' ] is None : data [ 'delay' ] = 0 try : import scipy.io as sio sio . savemat ( filename , data ) except Exception as e : Global . _error ( 'Error while saving in Matlab format.' ) Global . _print ( e ) return else : Global . _print ( \"Saving connectivity in text format...\" ) # save in Pythons pickle format with open ( filename , mode = 'wb' ) as w_file : try : pickle . dump ( data , w_file , protocol = pickle . HIGHEST_PROTOCOL ) except Exception as e : Global . _print ( 'Error while saving in text format.' ) Global . _print ( e ) return return","title":"save_connectivity()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.set","text":"Sets the parameters/variables values for each dendrite in the projection. For parameters, you can provide: a single value, which will be the same for all dendrites. a list or 1D numpy array of the same length as the number of actual dendrites (self.size). For variables, you can provide: a single value, which will be the same for all synapses of all dendrites. a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value. Warning: it is not possible to set different values to each synapse using this method. One should iterate over the dendrites: for dendrite in proj . dendrites : dendrite . w = np . ones ( dendrite . size ) Parameters: Name Type Description Default value a dictionary with the name of the parameter/variable as key. required Source code in ANNarchy/core/Projection.py 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 def set ( self , value ): \"\"\" Sets the parameters/variables values for each dendrite in the projection. For parameters, you can provide: * a single value, which will be the same for all dendrites. * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). For variables, you can provide: * a single value, which will be the same for all synapses of all dendrites. * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value. **Warning:** it is not possible to set different values to each synapse using this method. One should iterate over the dendrites: ```python for dendrite in proj.dendrites: dendrite.w = np.ones(dendrite.size) ``` :param value: a dictionary with the name of the parameter/variable as key. \"\"\" for name , val in value . items (): self . __setattr__ ( name , val )","title":"set()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.size_in_bytes","text":"Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked. Source code in ANNarchy/core/Projection.py 1521 1522 1523 1524 1525 1526 1527 1528 def size_in_bytes ( self ): \"\"\" Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked. \"\"\" if self . initialized : return self . cyInstance . size_in_bytes () else : return 0","title":"size_in_bytes()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.start_creating","text":"Starts creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). Parameters: Name Type Description Default period how often creating should be evaluated (default: dt, i.e. each step) None Source code in ANNarchy/core/Projection.py 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 def start_creating ( self , period = None ): \"\"\" Starts creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). :param period: how often creating should be evaluated (default: dt, i.e. each step) \"\"\" if not period : period = Global . config [ 'dt' ] if not self . cyInstance : Global . _error ( 'Can not start creating if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . start_creating ( int ( period / Global . config [ 'dt' ]), Global . get_current_step ()) except : Global . _error ( \"The synapse does not define a 'creating' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start creating connections.\" )","title":"start_creating()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.start_pruning","text":"Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). Parameters: Name Type Description Default period how often pruning should be evaluated (default: dt, i.e. each step) None Source code in ANNarchy/core/Projection.py 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 def start_pruning ( self , period = None ): \"\"\" Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). :param period: how often pruning should be evaluated (default: dt, i.e. each step) \"\"\" if not period : period = Global . config [ 'dt' ] if not self . cyInstance : Global . _error ( 'Can not start pruning if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . start_pruning ( int ( period / Global . config [ 'dt' ]), Global . get_current_step ()) except : Global . _error ( \"The synapse does not define a 'pruning' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start pruning connections.\" )","title":"start_pruning()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.stop_creating","text":"Stops creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). Source code in ANNarchy/core/Projection.py 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 def stop_creating ( self ): \"\"\" Stops creating the synapses in the projection if the synapse defines a 'creating' argument. 'structural_plasticity' must be set to True in setup(). \"\"\" if not self . cyInstance : Global . _error ( 'Can not stop creating if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . stop_creating () except : Global . _error ( \"The synapse does not define a 'creating' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start creating connections.\" )","title":"stop_creating()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.stop_pruning","text":"Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). Source code in ANNarchy/core/Projection.py 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 def stop_pruning ( self ): \"\"\" Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument. 'structural_plasticity' must be set to True in setup(). \"\"\" if not self . cyInstance : Global . _error ( 'Can not stop pruning if the network is not compiled.' ) if Global . config [ 'structural_plasticity' ]: try : self . cyInstance . stop_pruning () except : Global . _error ( \"The synapse does not define a 'pruning' argument.\" ) else : Global . _error ( \"You must set 'structural_plasticity' to True in setup() to start pruning connections.\" )","title":"stop_pruning()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.synapse","text":"Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise. Parameters: Name Type Description Default pre rank of the pre-synaptic neuron. required post rank of the post-synaptic neuron. required Source code in ANNarchy/core/Projection.py 666 667 668 669 670 671 672 673 674 675 676 def synapse ( self , pre , post ): \"\"\" Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise. :param pre: rank of the pre-synaptic neuron. :param post: rank of the post-synaptic neuron. \"\"\" if not isinstance ( pre , int ) or not isinstance ( post , int ): Global . _error ( 'Projection.synapse() only accepts ranks for the pre and post neurons.' ) return self . dendrite ( post ) . synapse ( pre )","title":"synapse()"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.update_launch_config","text":"Since ANNarchy 4.7.2 we allow the adjustment of the CUDA launch config. Parameters: :nb_blocks: number of CUDA blocks which can be 65535 at maximum. If set to -1 the number of launched blocks is computed by ANNarchy. :threads_per_block: number of CUDA threads for one block which can be maximum 1024. Source code in ANNarchy/core/Projection.py 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 def update_launch_config ( self , nb_blocks =- 1 , threads_per_block = 32 ): \"\"\" Since ANNarchy 4.7.2 we allow the adjustment of the CUDA launch config. Parameters: :nb_blocks: number of CUDA blocks which can be 65535 at maximum. If set to -1 the number of launched blocks is computed by ANNarchy. :threads_per_block: number of CUDA threads for one block which can be maximum 1024. \"\"\" if not Global . _check_paradigm ( \"cuda\" ): Global . _warning ( \"Projection.update_launch_config() is intended for usage on CUDA devices\" ) return if self . initialized : self . cyInstance . update_launch_config ( nb_blocks = nb_blocks , threads_per_block = threads_per_block ) else : Global . _error ( \"Projection.update_launch_config() should be called after compile()\" )","title":"update_launch_config()"},{"location":"API/RandomDistribution.html","text":"Random Distributions # Introduction # Random number generators can be used at several places: while initializing parameters or variables, while creating connection patterns, when injecting noise into a neural or synaptic variable. ANNarchy provides several random distribution objects, implementing the following distributions: Uniform DiscreteUniform Normal LogNormal Gamma Exponential Warning DiscreteUniform, Gamma and Exponential distributions are not available if the CUDA paradigm is used. They can be used in the Python code, as a normal object: dist = Uniform ( - 1.0 , 1.0 ) values = dist . get_values ( 100 ) or inside mathematical expressions: tau * dv/dt + v = g_exc + Normal(0.0, 20.0) The Python objects rely on the numpy.random library, while the C++ values are based on the standard library of C++11. The seed of the underlying random number generator (Mersenne twister, mt19937 in C++11) can be set globally, by defining its value in setup() : setup(seed=187348768237) All random distribution objects (Python or C++) will use this seed. By default, the global seed is taken to be time(NULL) . The seed can also be set individually for each RandomDistribution object as a last argument: dist = Uniform(-1.0, 1.0, 36875937346) as well as in a mathematical expression: tau * dv/dt + v = g_exc + Normal(0.0, 20.0, 497536526) Implementation details # ANNarchy uses default implementations for random number generation: STL methods of C++11 for OpenMP and the device API of the curand library for CUDA. As engines we use mt19937 on openMP side and XORWOW on CUDA. The latter is subject to changes in future releases. It may be important to know that the drawing mechanisms differ between openMP and CUDA slightly: openMP: all distribution objects draw the numbers from one source in a single threaded way. CUDA: each distribution object has it own source, the random numbers are drawn in a parallel way. For further details on random numbers on GPUs please refer to the curand documentation: http://docs.nvidia.com/cuda/curand/device-api-overview.html#device-api-overview Uniform # Bases: RandomDistribution Random distribution object using the uniform distribution between min and max . The returned values are floats in the range [min, max]. __doc__ = ' \\n Random distribution object using the uniform distribution between ``min`` and ``max``. \\n\\n The returned values are floats in the range [min, max]. \\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.core.Random' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'RandomDistribution' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init__ ( min , max ) # Parameters: Name Type Description Default min minimum value. required max maximum value. required __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __repr__ () method descriptor # Return repr(self). __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __str__ () method descriptor # Return str(self). __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). get_list_values ( size ) # Returns a list of the given size. get_value () # Returns a single float value. get_values ( shape ) # Returns a Numpy array with the given shape. DiscreteUniform # Bases: RandomDistribution Random distribution object using the discrete uniform distribution between min and max . The returned values are integers in the range [min, max]. __doc__ = ' \\n Random distribution object using the discrete uniform distribution between ``min`` and ``max``. \\n\\n The returned values are integers in the range [min, max]. \\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.core.Random' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'RandomDistribution' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init__ ( min , max ) # Parameters: Name Type Description Default min minimum value. required max maximum value. required __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __repr__ () method descriptor # Return repr(self). __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __str__ () method descriptor # Return str(self). __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). get_list_values ( size ) # Returns a list of the given size. get_value () # Returns a single float value. get_values ( shape ) # Returns a np.ndarray with the given shape. Normal # Bases: RandomDistribution Random distribution instance returning a random value based on a normal (Gaussian) distribution. __doc__ = ' \\n Random distribution instance returning a random value based on a normal (Gaussian) distribution. \\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.core.Random' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'RandomDistribution' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init__ ( mu , sigma , min = None , max = None ) # Parameters: Name Type Description Default mu mean of the distribution. required sigma standard deviation of the distribution. required min minimum value (default: unlimited). None max maximum value (default: unlimited). None __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __repr__ () method descriptor # Return repr(self). __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __str__ () method descriptor # Return str(self). __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). get_list_values ( size ) # Returns a list of the given size. get_value () # Returns a single float value. get_values ( shape ) # Returns a np.ndarray with the given shape LogNormal # Bases: RandomDistribution Random distribution instance returning a random value based on lognormal distribution. __doc__ = ' \\n Random distribution instance returning a random value based on lognormal distribution. \\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.core.Random' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'RandomDistribution' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init__ ( mu , sigma , min = None , max = None ) # Parameters: Name Type Description Default mu mean of the distribution. required sigma standard deviation of the distribution. required min minimum value (default: unlimited). None max maximum value (default: unlimited). None __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __repr__ () method descriptor # Return repr(self). __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __str__ () method descriptor # Return str(self). __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). get_list_values ( size ) # Returns a list of the given size. get_value () # Returns a single float value. get_values ( shape ) # Returns a np.ndarray with the given shape Gamma # Bases: RandomDistribution Random distribution instance returning a random value based on gamma distribution. __doc__ = ' \\n Random distribution instance returning a random value based on gamma distribution. \\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.core.Random' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'RandomDistribution' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init__ ( alpha , beta = 1.0 , seed =- 1 , min = None , max = None ) # Parameters: Name Type Description Default alpha shape of the gamma distribution required beta scale of the gamma distribution 1.0 min minimum value returned (default: unlimited). None max maximum value returned (default: unlimited). None __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __repr__ () method descriptor # Return repr(self). __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __str__ () method descriptor # Return str(self). __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). get_list_values ( size ) # Returns a list of the given size. get_value () # Returns a single float value. get_values ( shape ) # Returns a np.ndarray with the given shape Exponential # Bases: RandomDistribution Random distribution instance returning a random value based on exponential distribution, according the density function: \\[P(x | \\lambda) = \\lambda e^{(-\\lambda x )}\\] __doc__ = ' \\n Random distribution instance returning a random value based on exponential distribution, according the density function: \\n\\n $$P(x | \\\\ lambda) = \\\\ lambda e^{(- \\\\ lambda x )}$$ \\n\\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.core.Random' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'RandomDistribution' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init__ ( Lambda , min = None , max = None ) # Note: Lambda is capitalized, otherwise it would be a reserved Python keyword. Parameters: Name Type Description Default Lambda rate parameter. required min minimum value (default: unlimited). None max maximum value (default: unlimited). None __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __repr__ () method descriptor # Return repr(self). __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __str__ () method descriptor # Return str(self). __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). get_list_values ( size ) # Returns a list of the given size. get_value () # Returns a single float value. get_values ( shape ) # Returns a np.ndarray with the given shape.","title":"Random Distributions"},{"location":"API/RandomDistribution.html#random-distributions","text":"","title":"Random Distributions"},{"location":"API/RandomDistribution.html#introduction","text":"Random number generators can be used at several places: while initializing parameters or variables, while creating connection patterns, when injecting noise into a neural or synaptic variable. ANNarchy provides several random distribution objects, implementing the following distributions: Uniform DiscreteUniform Normal LogNormal Gamma Exponential Warning DiscreteUniform, Gamma and Exponential distributions are not available if the CUDA paradigm is used. They can be used in the Python code, as a normal object: dist = Uniform ( - 1.0 , 1.0 ) values = dist . get_values ( 100 ) or inside mathematical expressions: tau * dv/dt + v = g_exc + Normal(0.0, 20.0) The Python objects rely on the numpy.random library, while the C++ values are based on the standard library of C++11. The seed of the underlying random number generator (Mersenne twister, mt19937 in C++11) can be set globally, by defining its value in setup() : setup(seed=187348768237) All random distribution objects (Python or C++) will use this seed. By default, the global seed is taken to be time(NULL) . The seed can also be set individually for each RandomDistribution object as a last argument: dist = Uniform(-1.0, 1.0, 36875937346) as well as in a mathematical expression: tau * dv/dt + v = g_exc + Normal(0.0, 20.0, 497536526)","title":"Introduction"},{"location":"API/RandomDistribution.html#implementation-details","text":"ANNarchy uses default implementations for random number generation: STL methods of C++11 for OpenMP and the device API of the curand library for CUDA. As engines we use mt19937 on openMP side and XORWOW on CUDA. The latter is subject to changes in future releases. It may be important to know that the drawing mechanisms differ between openMP and CUDA slightly: openMP: all distribution objects draw the numbers from one source in a single threaded way. CUDA: each distribution object has it own source, the random numbers are drawn in a parallel way. For further details on random numbers on GPUs please refer to the curand documentation: http://docs.nvidia.com/cuda/curand/device-api-overview.html#device-api-overview","title":"Implementation details"},{"location":"API/RandomDistribution.html#ANNarchy.Uniform","text":"Bases: RandomDistribution Random distribution object using the uniform distribution between min and max . The returned values are floats in the range [min, max].","title":"Uniform"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__init__","text":"Parameters: Name Type Description Default min minimum value. required max maximum value. required","title":"__init__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__repr__","text":"Return repr(self).","title":"__repr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__str__","text":"Return str(self).","title":"__str__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.get_list_values","text":"Returns a list of the given size.","title":"get_list_values()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.get_value","text":"Returns a single float value.","title":"get_value()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.get_values","text":"Returns a Numpy array with the given shape.","title":"get_values()"},{"location":"API/RandomDistribution.html#ANNarchy.DiscreteUniform","text":"Bases: RandomDistribution Random distribution object using the discrete uniform distribution between min and max . The returned values are integers in the range [min, max].","title":"DiscreteUniform"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__init__","text":"Parameters: Name Type Description Default min minimum value. required max maximum value. required","title":"__init__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__repr__","text":"Return repr(self).","title":"__repr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__str__","text":"Return str(self).","title":"__str__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.get_list_values","text":"Returns a list of the given size.","title":"get_list_values()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.get_value","text":"Returns a single float value.","title":"get_value()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.get_values","text":"Returns a np.ndarray with the given shape.","title":"get_values()"},{"location":"API/RandomDistribution.html#ANNarchy.Normal","text":"Bases: RandomDistribution Random distribution instance returning a random value based on a normal (Gaussian) distribution.","title":"Normal"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__init__","text":"Parameters: Name Type Description Default mu mean of the distribution. required sigma standard deviation of the distribution. required min minimum value (default: unlimited). None max maximum value (default: unlimited). None","title":"__init__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__repr__","text":"Return repr(self).","title":"__repr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__str__","text":"Return str(self).","title":"__str__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.get_list_values","text":"Returns a list of the given size.","title":"get_list_values()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.get_value","text":"Returns a single float value.","title":"get_value()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.get_values","text":"Returns a np.ndarray with the given shape","title":"get_values()"},{"location":"API/RandomDistribution.html#ANNarchy.LogNormal","text":"Bases: RandomDistribution Random distribution instance returning a random value based on lognormal distribution.","title":"LogNormal"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__init__","text":"Parameters: Name Type Description Default mu mean of the distribution. required sigma standard deviation of the distribution. required min minimum value (default: unlimited). None max maximum value (default: unlimited). None","title":"__init__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__repr__","text":"Return repr(self).","title":"__repr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__str__","text":"Return str(self).","title":"__str__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.get_list_values","text":"Returns a list of the given size.","title":"get_list_values()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.get_value","text":"Returns a single float value.","title":"get_value()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.get_values","text":"Returns a np.ndarray with the given shape","title":"get_values()"},{"location":"API/RandomDistribution.html#ANNarchy.Gamma","text":"Bases: RandomDistribution Random distribution instance returning a random value based on gamma distribution.","title":"Gamma"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__init__","text":"Parameters: Name Type Description Default alpha shape of the gamma distribution required beta scale of the gamma distribution 1.0 min minimum value returned (default: unlimited). None max maximum value returned (default: unlimited). None","title":"__init__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__repr__","text":"Return repr(self).","title":"__repr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__str__","text":"Return str(self).","title":"__str__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.get_list_values","text":"Returns a list of the given size.","title":"get_list_values()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.get_value","text":"Returns a single float value.","title":"get_value()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.get_values","text":"Returns a np.ndarray with the given shape","title":"get_values()"},{"location":"API/RandomDistribution.html#ANNarchy.Exponential","text":"Bases: RandomDistribution Random distribution instance returning a random value based on exponential distribution, according the density function: \\[P(x | \\lambda) = \\lambda e^{(-\\lambda x )}\\]","title":"Exponential"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__init__","text":"Note: Lambda is capitalized, otherwise it would be a reserved Python keyword. Parameters: Name Type Description Default Lambda rate parameter. required min minimum value (default: unlimited). None max maximum value (default: unlimited). None","title":"__init__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__repr__","text":"Return repr(self).","title":"__repr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__str__","text":"Return str(self).","title":"__str__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.get_list_values","text":"Returns a list of the given size.","title":"get_list_values()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.get_value","text":"Returns a single float value.","title":"get_value()"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.get_values","text":"Returns a np.ndarray with the given shape.","title":"get_values()"},{"location":"API/SpecificNeuron.html","text":"Built-in neuron types # ANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN ( http://neuralensemble.org/docs/PyNN/reference/neuronmodels.html ). LeakyIntegrator # Bases: Neuron Leaky-integrator rate-coded neuron, optionally noisy. This simple rate-coded neuron defines an internal variable \\(v(t)\\) which integrates the inputs \\(I(t)\\) with a time constant \\(\\tau\\) and a baseline \\(B\\) . An additive noise \\(N(t)\\) can be optionally defined: \\[\\tau \\cdot \\frac{dv(t)}{dt} + v(t) = I(t) + B + N(t)\\] The transfer function is the positive (or rectified linear ReLU) function with a threshold \\(T\\) : \\[r(t) = (v(t) - T)^+\\] By default, the input \\(I(t)\\) to this neuron is \"sum(exc) - sum(inh)\", but this can be changed by setting the sum argument: neuron = LeakyIntegrator ( sum = \"sum('exc')\" ) By default, there is no additive noise, but the noise argument can be passed with a specific distribution: neuron = LeakyIntegrator ( noise = \"Normal(0.0, 1.0)\" ) Parameters: tau = 10.0 : Time constant in ms of the neuron. B = 0.0 : Baseline value for v. T = 0.0 : Threshold for the positive transfer function. Variables: v : internal variable (init = 0.0): tau * dv/dt + v = sum(exc) - sum(inh) + B + N r : firing rate (init = 0.0): r = pos(v - T) The ODE is solved using the exponential Euler method. Equivalent code: LeakyIntegrator = Neuron ( parameters = ''' tau = 10.0 : population B = 0.0 T = 0.0 : population ''' , equations = ''' tau * dv/dt + v = sum(exc) - sum(inh) + B : exponential r = pos(v - T) ''' ) Source code in ANNarchy/models/Neurons.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 class LeakyIntegrator ( Neuron ): r \"\"\" Leaky-integrator rate-coded neuron, optionally noisy. This simple rate-coded neuron defines an internal variable $v(t)$ which integrates the inputs $I(t)$ with a time constant $\\tau$ and a baseline $B$. An additive noise $N(t)$ can be optionally defined: $$\\tau \\cdot \\frac{dv(t)}{dt} + v(t) = I(t) + B + N(t)$$ The transfer function is the positive (or rectified linear ReLU) function with a threshold $T$: $$r(t) = (v(t) - T)^+$$ By default, the input $I(t)$ to this neuron is \"sum(exc) - sum(inh)\", but this can be changed by setting the ``sum`` argument: ```python neuron = LeakyIntegrator(sum=\"sum('exc')\") ``` By default, there is no additive noise, but the ``noise`` argument can be passed with a specific distribution: ```python neuron = LeakyIntegrator(noise=\"Normal(0.0, 1.0)\") ``` Parameters: * tau = 10.0 : Time constant in ms of the neuron. * B = 0.0 : Baseline value for v. * T = 0.0 : Threshold for the positive transfer function. Variables: * v : internal variable (init = 0.0): tau * dv/dt + v = sum(exc) - sum(inh) + B + N * r : firing rate (init = 0.0): r = pos(v - T) The ODE is solved using the exponential Euler method. Equivalent code: ```python LeakyIntegrator = Neuron( parameters=''' tau = 10.0 : population B = 0.0 T = 0.0 : population ''', equations=''' tau * dv/dt + v = sum(exc) - sum(inh) + B : exponential r = pos(v - T) ''' ) ``` \"\"\" # For reporting _instantiated = [] def __init__ ( self , tau = 10.0 , B = 0.0 , T = 0.0 , sum = 'sum(exc) - sum(inh)' , noise = None ): # Create the arguments parameters = \"\"\" tau = %(tau)s : population B = %(B)s T = %(T)s : population \"\"\" % { 'tau' : tau , 'B' : B , 'T' : T } # Equations for the variables if not noise : noise_def = '' else : noise_def = '+ ' + noise equations = \"\"\" tau * dv/dt + v = %(sum)s + B %(noise)s : exponential r = pos(v - T) \"\"\" % { 'sum' : sum , 'noise' : noise_def } Neuron . __init__ ( self , parameters = parameters , equations = equations , name = \"Leaky-Integrator\" , description = \"Leaky-Integrator with positive transfer function and additive noise.\" ) # For reporting self . _instantiated . append ( True ) Izhikevich # Bases: Neuron Izhikevich neuron as proposed in: Izhikevich, E.M. (2003). Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks , 14:6. http://dx.doi.org/10.1109/TNN.2003.820440 The equations are: \\[\\frac{dv}{dt} = 0.04 * v^2 + 5.0 * v + 140.0 - u + I\\] \\[\\frac{du}{dt} = a * (b * v - u)\\] By default, the conductance is \"g_exc - g_inh\", but this can be changed by setting the conductance argument: neuron = Izhikevich ( conductance = 'g_ampa * (1 + g_nmda) - g_gaba' ) The synapses are instantaneous, i.e the corresponding conductance is increased from the synaptic efficiency w at the time step when a spike is received. Parameters: a = 0.02 : Speed of the recovery variable b = 0.2: Scaling of the recovery variable c = -65.0 : Reset potential. d = 8.0 : Increment of the recovery variable after a spike. v_thresh = 30.0 : Spike threshold (mV). i_offset = 0.0 : external current (nA). noise = 0.0 : Amplitude of the normal additive noise. tau_refrac = 0.0 : Duration of refractory period (ms). Variables: I : input current (user-defined conductance/current + external current + normal noise): I = conductance + i_offset + noise * Normal(0.0, 1.0) v : membrane potential in mV (init = c): dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I u : recovery variable (init= b * c): du/dt = a * (b * v - u) Spike emission: v > v_thresh Reset: v = c u += d The ODEs are solved using the explicit Euler method. Equivalent code: Izhikevich = Neuron ( parameters = \"\"\" noise = 0.0 a = 0.02 b = 0.2 c = -65.0 d = 8.0 v_thresh = 30.0 i_offset = 0.0 \"\"\" , equations = \"\"\" I = g_exc - g_inh + noise * Normal(0.0, 1.0) + i_offset dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init = -65.0 du/dt = a * (b*v - u) : init= -13.0 \"\"\" , spike = \"v > v_thresh\" , reset = \"v = c; u += d\" , refractory = 0.0 ) The default parameters are for a regular spiking (RS) neuron derived from the above mentioned article. Source code in ANNarchy/models/Neurons.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 class Izhikevich ( Neuron ): ''' Izhikevich neuron as proposed in: > Izhikevich, E.M. (2003). *Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks*, 14:6. <http://dx.doi.org/10.1109/TNN.2003.820440> The equations are: $$\\\\frac{dv}{dt} = 0.04 * v^2 + 5.0 * v + 140.0 - u + I$$ $$\\\\frac{du}{dt} = a * (b * v - u)$$ By default, the conductance is \"g_exc - g_inh\", but this can be changed by setting the ``conductance`` argument: ```python neuron = Izhikevich(conductance='g_ampa * (1 + g_nmda) - g_gaba') ``` The synapses are instantaneous, i.e the corresponding conductance is increased from the synaptic efficiency w at the time step when a spike is received. Parameters: * a = 0.02 : Speed of the recovery variable * b = 0.2: Scaling of the recovery variable * c = -65.0 : Reset potential. * d = 8.0 : Increment of the recovery variable after a spike. * v_thresh = 30.0 : Spike threshold (mV). * i_offset = 0.0 : external current (nA). * noise = 0.0 : Amplitude of the normal additive noise. * tau_refrac = 0.0 : Duration of refractory period (ms). Variables: * I : input current (user-defined conductance/current + external current + normal noise): I = conductance + i_offset + noise * Normal(0.0, 1.0) * v : membrane potential in mV (init = c): dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I * u : recovery variable (init= b * c): du/dt = a * (b * v - u) Spike emission: v > v_thresh Reset: v = c u += d The ODEs are solved using the explicit Euler method. Equivalent code: ```python Izhikevich = Neuron( parameters = \"\"\" noise = 0.0 a = 0.02 b = 0.2 c = -65.0 d = 8.0 v_thresh = 30.0 i_offset = 0.0 \"\"\", equations = \"\"\" I = g_exc - g_inh + noise * Normal(0.0, 1.0) + i_offset dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init = -65.0 du/dt = a * (b*v - u) : init= -13.0 \"\"\", spike = \"v > v_thresh\", reset = \"v = c; u += d\", refractory = 0.0 ) ``` The default parameters are for a regular spiking (RS) neuron derived from the above mentioned article. ''' # For reporting _instantiated = [] def __init__ ( self , a = 0.02 , b = 0.2 , c =- 65.0 , d = 8.0 , v_thresh = 30.0 , i_offset = 0.0 , noise = 0.0 , tau_refrac = 0.0 , conductance = \"g_exc - g_inh\" ): # Extract which targets are defined in the conductance #import re #targets = re.findall(r'g_([\\w]+)', conductance) # Create the arguments parameters = \"\"\" noise = %(noise)s a = %(a)s b = %(b)s c = %(c)s d = %(d)s v_thresh = %(v_thresh)s i_offset = %(i_offset)s tau_refrac = %(tau_refrac)s \"\"\" % { 'a' : a , 'b' : b , 'c' : c , 'd' : d , 'v_thresh' : v_thresh , 'i_offset' : i_offset , 'noise' : noise , 'tau_refrac' : tau_refrac } # Equations for the variables equations = \"\"\" I = %(conductance)s + noise * Normal(0.0, 1.0) + i_offset dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init = %(c)s du/dt = a * (b*v - u) : init= %(u)s \"\"\" % { 'conductance' : conductance , 'c' : c , 'u' : b * c } spike = \"\"\" v > v_thresh \"\"\" reset = \"\"\" v = c u += d \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Izhikevich\" , description = \"Quadratic integrate-and-fire spiking neuron with adaptation.\" ) # For reporting self . _instantiated . append ( True ) IF_curr_exp # Bases: Neuron IF_curr_exp neuron. Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current. (Separate synaptic currents for excitatory and inhibitory synapses). Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_curr_exp = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 ) Source code in ANNarchy/models/Neurons.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 class IF_curr_exp ( Neuron ): ''' IF_curr_exp neuron. Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current. (Separate synaptic currents for excitatory and inhibitory synapses). Parameters: * v_rest = -65.0 : Resting membrane potential (mV) * cm = 1.0 : Capacity of the membrane (nF) * tau_m = 20.0 : Membrane time constant (ms) * tau_refrac = 0.0 : Duration of refractory period (ms) * tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) * tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) * i_offset = 0.0 : Offset current (nA) * v_reset = -65.0 : Reset potential after a spike (mV) * v_thresh = -50.0 : Spike threshold (mV) Variables: * v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset * g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: ```python IF_curr_exp = Neuron( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\", equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\", spike = \"v > v_thresh\", reset = \"v = v_reset\", refractory = 0.0 ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , v_rest =- 65.0 , cm = 1.0 , tau_m = 20.0 , tau_refrac = 0.0 , tau_syn_E = 5.0 , tau_syn_I = 5.0 , v_thresh =- 50.0 , v_reset =- 65.0 , i_offset = 0.0 ): # Create the arguments parameters = \"\"\" v_rest = %(v_rest)s cm = %(cm)s tau_m = %(tau_m)s tau_refrac = %(tau_refrac)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s v_thresh = %(v_thresh)s v_reset = %(v_reset)s i_offset = %(i_offset)s \"\"\" % { 'v_rest' : v_rest , 'cm' : cm , 'tau_m' : tau_m , 'tau_refrac' : tau_refrac , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'v_thresh' : v_thresh , 'v_reset' : v_reset , 'i_offset' : i_offset } # Equations for the variables equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset : exponential, init= %(v_reset)s tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" % { 'v_reset' : v_reset } spike = \"\"\" v > v_thresh \"\"\" reset = \"\"\" v = v_reset \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Integrate-and-Fire\" , description = \"Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current.\" ) # For reporting self . _instantiated . append ( True ) IF_cond_exp # Bases: Neuron IF_cond_exp neuron. Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance. Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -70.0 : Reversal potential for inhibitory input (mv) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_cond_exp = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -70.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 ) Source code in ANNarchy/models/Neurons.py 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 class IF_cond_exp ( Neuron ): ''' IF_cond_exp neuron. Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance. Parameters: * v_rest = -65.0 : Resting membrane potential (mV) * cm = 1.0 : Capacity of the membrane (nF) * tau_m = 20.0 : Membrane time constant (ms) * tau_refrac = 0.0 : Duration of refractory period (ms) * tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) * tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) * e_rev_E = 0.0 : Reversal potential for excitatory input (mV) * e_rev_I = -70.0 : Reversal potential for inhibitory input (mv) * i_offset = 0.0 : Offset current (nA) * v_reset = -65.0 : Reset potential after a spike (mV) * v_thresh = -50.0 : Spike threshold (mV) Variables: * v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset * g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: ```python IF_cond_exp = Neuron( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -70.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\", equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\", spike = \"v > v_thresh\", reset = \"v = v_reset\", refractory = 0.0 ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , v_rest =- 65.0 , cm = 1.0 , tau_m = 20.0 , tau_refrac = 0.0 , tau_syn_E = 5.0 , tau_syn_I = 5.0 , e_rev_E = 0.0 , e_rev_I = - 70.0 , v_thresh =- 50.0 , v_reset =- 65.0 , i_offset = 0.0 ): # Create the arguments parameters = \"\"\" v_rest = %(v_rest)s cm = %(cm)s tau_m = %(tau_m)s tau_refrac = %(tau_refrac)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s v_thresh = %(v_thresh)s v_reset = %(v_reset)s i_offset = %(i_offset)s e_rev_E = %(e_rev_E)s e_rev_I = %(e_rev_I)s \"\"\" % { 'v_rest' : v_rest , 'cm' : cm , 'tau_m' : tau_m , 'tau_refrac' : tau_refrac , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'v_thresh' : v_thresh , 'v_reset' : v_reset , 'i_offset' : i_offset , 'e_rev_E' : e_rev_E , 'e_rev_I' : e_rev_I } # Equations for the variables equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset : exponential, init= %(v_reset)s tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" % { 'v_reset' : v_reset } spike = \"\"\" v > v_thresh \"\"\" reset = \"\"\" v = v_reset \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Integrate-and-Fire\" , description = \"Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductances.\" ) # For reporting self . _instantiated . append ( True ) IF_curr_alpha # Bases: Neuron IF_curr_alpha neuron. Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents. (Separate synaptic currents for excitatory and inhibitory synapses). The alpha currents are calculated through a system of two linears ODEs. After a spike is received at t_spike, it peaks at t_spike + tau_syn_X, with a maximum equal to the synaptic efficiency. Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc alpha_exc : alpha function of excitatory current (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh alpha_inh : alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_curr_alpha = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 ) Source code in ANNarchy/models/Neurons.py 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 class IF_curr_alpha ( Neuron ): ''' IF_curr_alpha neuron. Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents. (Separate synaptic currents for excitatory and inhibitory synapses). The alpha currents are calculated through a system of two linears ODEs. After a spike is received at t_spike, it peaks at t_spike + tau_syn_X, with a maximum equal to the synaptic efficiency. Parameters: * v_rest = -65.0 : Resting membrane potential (mV) * cm = 1.0 : Capacity of the membrane (nF) * tau_m = 20.0 : Membrane time constant (ms) * tau_refrac = 0.0 : Duration of refractory period (ms) * tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms) * tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms) * i_offset = 0.0 : Offset current (nA) * v_reset = -65.0 : Reset potential after a spike (mV) * v_thresh = -50.0 : Spike threshold (mV) Variables: * v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset * g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * alpha_exc : alpha function of excitatory current (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc * g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh * alpha_inh : alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: ```python IF_curr_alpha = Neuron( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\", equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\", spike = \"v > v_thresh\", reset = \"v = v_reset\", refractory = 0.0 ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , v_rest =- 65.0 , cm = 1.0 , tau_m = 20.0 , tau_refrac = 0.0 , tau_syn_E = 5.0 , tau_syn_I = 5.0 , v_thresh =- 50.0 , v_reset =- 65.0 , i_offset = 0.0 ): # Create the arguments parameters = \"\"\" v_rest = %(v_rest)s cm = %(cm)s tau_m = %(tau_m)s tau_refrac = %(tau_refrac)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s v_thresh = %(v_thresh)s v_reset = %(v_reset)s i_offset = %(i_offset)s \"\"\" % { 'v_rest' : v_rest , 'cm' : cm , 'tau_m' : tau_m , 'tau_refrac' : tau_refrac , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'v_thresh' : v_thresh , 'v_reset' : v_reset , 'i_offset' : i_offset } # Equations for the variables equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset : exponential, init= %(v_reset)s tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" % { 'v_reset' : v_reset } spike = \"\"\" v > v_thresh \"\"\" reset = \"\"\" v = v_reset \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Integrate-and-Fire\" , description = \"Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents.\" ) # For reporting self . _instantiated . append ( True ) IF_cond_alpha # Bases: Neuron IF_cond_exp neuron. Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance. Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -70.0 : Reversal potential for inhibitory input (mv) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset g_exc : excitatory conductance (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc alpha_exc : alpha function of excitatory conductance (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc g_inh : inhibitory conductance (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh alpha_inh : alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_cond_alpha = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -70.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 ) Source code in ANNarchy/models/Neurons.py 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 class IF_cond_alpha ( Neuron ): ''' IF_cond_exp neuron. Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance. Parameters: * v_rest = -65.0 : Resting membrane potential (mV) * cm = 1.0 : Capacity of the membrane (nF) * tau_m = 20.0 : Membrane time constant (ms) * tau_refrac = 0.0 : Duration of refractory period (ms) * tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms) * tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms) * e_rev_E = 0.0 : Reversal potential for excitatory input (mV) * e_rev_I = -70.0 : Reversal potential for inhibitory input (mv) * i_offset = 0.0 : Offset current (nA) * v_reset = -65.0 : Reset potential after a spike (mV) * v_thresh = -50.0 : Spike threshold (mV) Variables: * v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset * g_exc : excitatory conductance (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * alpha_exc : alpha function of excitatory conductance (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc * g_inh : inhibitory conductance (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh * alpha_inh : alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: ```python IF_cond_alpha = Neuron( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -70.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\", equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\", spike = \"v > v_thresh\", reset = \"v = v_reset\", refractory = 0.0 ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , v_rest =- 65.0 , cm = 1.0 , tau_m = 20.0 , tau_refrac = 0.0 , tau_syn_E = 5.0 , tau_syn_I = 5.0 , e_rev_E = 0.0 , e_rev_I = - 70.0 , v_thresh =- 50.0 , v_reset =- 65.0 , i_offset = 0.0 ): # Create the arguments parameters = \"\"\" v_rest = %(v_rest)s cm = %(cm)s tau_m = %(tau_m)s tau_refrac = %(tau_refrac)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s v_thresh = %(v_thresh)s v_reset = %(v_reset)s i_offset = %(i_offset)s e_rev_E = %(e_rev_E)s e_rev_I = %(e_rev_I)s \"\"\" % { 'v_rest' : v_rest , 'cm' : cm , 'tau_m' : tau_m , 'tau_refrac' : tau_refrac , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'v_thresh' : v_thresh , 'v_reset' : v_reset , 'i_offset' : i_offset , 'e_rev_E' : e_rev_E , 'e_rev_I' : e_rev_I } # Equations for the variables equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset : exponential, init= %(v_reset)s tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" % { 'v_reset' : v_reset } spike = \"\"\" v > v_thresh \"\"\" reset = \"\"\" v = v_reset \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Integrate-and-Fire\" , description = \"Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductances.\" ) # For reporting self . _instantiated . append ( True ) HH_cond_exp # Bases: Neuron HH_cond_exp neuron. Single-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub. Parameters: gbar_Na = 20.0 : Maximal conductance of the Sodium current. gbar_K = 6.0 : Maximal conductance of the Potassium current. gleak = 0.01 : Conductance of the leak current (nF) cm = 0.2 : Capacity of the membrane (nF) v_offset = -63.0 : Threshold for the rate constants (mV) e_rev_Na = 50.0 : Reversal potential for the Sodium current (mV) e_rev_K = -90.0 : Reversal potential for the Potassium current (mV) e_rev_leak = -65.0 : Reversal potential for the leak current (mV) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -80.0 : Reversal potential for inhibitory input (mV) tau_syn_E = 0.2 : Decay time of excitatory synaptic current (ms) tau_syn_I = 2.0 : Decay time of inhibitory synaptic current (ms) i_offset = 0.0 : Offset current (nA) v_thresh = 0.0 : Threshold for spike emission Variables: Voltage-dependent rate constants an, bn, am, bm, ah, bh: an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) Activation variables n, m, h (h is initialized to 1.0, n and m to 0.0): dn/dt = an * (1.0 - n) - bn * n dm/dt = am * (1.0 - m) - bm * m dh/dt = ah * (1.0 - h) - bh * h v : membrane potential in mV (init=-65.0): cm * dv/dt = gleak (e_rev_leak -v) + gbar_K * n 4 * (e_rev_K - v) + gbar_Na * m *3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset g_exc : excitatory conductance (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory conductance (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission (the spike is emitted only once when v crosses the threshold from below): v > v_thresh and v(t-1) < v_thresh The ODEs for n, m, h and v are solved using the midpoint method, while the conductances g_exc and g_inh are solved using the exponential Euler method. Equivalent code: HH_cond_exp = Neuron ( parameters = \"\"\" gbar_Na = 20.0 gbar_K = 6.0 gleak = 0.01 cm = 0.2 v_offset = -63.0 e_rev_Na = 50.0 e_rev_K = -90.0 e_rev_leak = -65.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_syn_E = 0.2 tau_syn_I = 2.0 i_offset = 0.0 v_thresh = 0.0 \"\"\" , equations = \"\"\" # Previous membrane potential prev_v = v # Voltage-dependent rate constants an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) # Activation variables dn/dt = an * (1.0 - n) - bn * n : init = 0.0, exponential dm/dt = am * (1.0 - m) - bm * m : init = 0.0, exponential dh/dt = ah * (1.0 - h) - bh * h : init = 1.0, exponential # Membrane equation cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset: exponential, init=-65.0 # Exponentially-decaying conductances tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"(v > v_thresh) and (prev_v <= v_thresh)\" , reset = \"\" ) Source code in ANNarchy/models/Neurons.py 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 class HH_cond_exp ( Neuron ): ''' HH_cond_exp neuron. Single-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub. Parameters: * gbar_Na = 20.0 : Maximal conductance of the Sodium current. * gbar_K = 6.0 : Maximal conductance of the Potassium current. * gleak = 0.01 : Conductance of the leak current (nF) * cm = 0.2 : Capacity of the membrane (nF) * v_offset = -63.0 : Threshold for the rate constants (mV) * e_rev_Na = 50.0 : Reversal potential for the Sodium current (mV) * e_rev_K = -90.0 : Reversal potential for the Potassium current (mV) * e_rev_leak = -65.0 : Reversal potential for the leak current (mV) * e_rev_E = 0.0 : Reversal potential for excitatory input (mV) * e_rev_I = -80.0 : Reversal potential for inhibitory input (mV) * tau_syn_E = 0.2 : Decay time of excitatory synaptic current (ms) * tau_syn_I = 2.0 : Decay time of inhibitory synaptic current (ms) * i_offset = 0.0 : Offset current (nA) * v_thresh = 0.0 : Threshold for spike emission Variables: * Voltage-dependent rate constants an, bn, am, bm, ah, bh: an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) * Activation variables n, m, h (h is initialized to 1.0, n and m to 0.0): dn/dt = an * (1.0 - n) - bn * n dm/dt = am * (1.0 - m) - bm * m dh/dt = ah * (1.0 - h) - bh * h * v : membrane potential in mV (init=-65.0): cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset * g_exc : excitatory conductance (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * g_inh : inhibitory conductance (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission (the spike is emitted only once when v crosses the threshold from below): v > v_thresh and v(t-1) < v_thresh The ODEs for n, m, h and v are solved using the midpoint method, while the conductances g_exc and g_inh are solved using the exponential Euler method. Equivalent code: ```python HH_cond_exp = Neuron( parameters = \"\"\" gbar_Na = 20.0 gbar_K = 6.0 gleak = 0.01 cm = 0.2 v_offset = -63.0 e_rev_Na = 50.0 e_rev_K = -90.0 e_rev_leak = -65.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_syn_E = 0.2 tau_syn_I = 2.0 i_offset = 0.0 v_thresh = 0.0 \"\"\", equations = \"\"\" # Previous membrane potential prev_v = v # Voltage-dependent rate constants an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) # Activation variables dn/dt = an * (1.0 - n) - bn * n : init = 0.0, exponential dm/dt = am * (1.0 - m) - bm * m : init = 0.0, exponential dh/dt = ah * (1.0 - h) - bh * h : init = 1.0, exponential # Membrane equation cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset: exponential, init=-65.0 # Exponentially-decaying conductances tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\", spike = \"(v > v_thresh) and (prev_v <= v_thresh)\", reset = \"\" ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , gbar_Na = 20.0 , gbar_K = 6.0 , gleak = 0.01 , cm = 0.2 , v_offset = - 63.0 , e_rev_Na = 50.0 , e_rev_K = - 90.0 , e_rev_leak = - 65.0 , e_rev_E = 0.0 , e_rev_I = - 80.0 , tau_syn_E = 0.2 , tau_syn_I = 2.0 , i_offset = 0.0 , v_thresh = 0.0 ): parameters = \"\"\" gbar_Na = %(gbar_Na)s gbar_K = %(gbar_K)s gleak = %(gleak)s cm = %(cm)s v_offset = %(v_offset)s e_rev_Na = %(e_rev_Na)s e_rev_K = %(e_rev_K)s e_rev_leak = %(e_rev_leak)s e_rev_E = %(e_rev_E)s e_rev_I = %(e_rev_I)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s i_offset = %(i_offset)s v_thresh = %(v_thresh)s \"\"\" % { 'gbar_Na' : gbar_Na , 'gbar_K' : gbar_K , 'gleak' : gleak , 'cm' : cm , 'v_offset' : v_offset , 'e_rev_Na' : e_rev_Na , 'e_rev_K' : e_rev_K , 'e_rev_leak' : e_rev_leak , 'e_rev_E' : e_rev_E , 'e_rev_I' : e_rev_I , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'i_offset' : i_offset , 'v_thresh' : v_thresh } equations = \"\"\" # Previous membrane potential prev_v = v # Voltage-dependent rate constants an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) # Activation variables dn/dt = an * (1.0 - n) - bn * n : init = 0.0, exponential dm/dt = am * (1.0 - m) - bm * m : init = 0.0, exponential dh/dt = ah * (1.0 - h) - bh * h : init = 1.0, exponential # Membrane equation cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset: exponential, init= %(e_rev_leak)s # Exponentially-decaying conductances tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" % { 'e_rev_leak' : e_rev_leak } spike = \"(v > v_thresh) and (prev_v <= v_thresh)\" reset = \"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , name = \"Hodgkin-Huxley\" , description = \"Single-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents.\" ) # For reporting self . _instantiated . append ( True ) EIF_cond_exp_isfa_ista # Bases: Neuron EIF_cond_exp neuron. Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.) according to: Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642 Parameters: v_rest = -70.6 : Resting membrane potential (mV) cm = 0.281 : Capacity of the membrane (nF) tau_m = 9.3667 : Membrane time constant (ms) tau_refrac = 0.1 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -80.0 : Reversal potential for inhibitory input (mv) tau_w = 144.0 : Time constant of the adaptation variable (ms) a = 4.0 : Scaling of the adaptation variable b = 0.0805 : Increment on the adaptation variable after a spike i_offset = 0.0 : Offset current (nA) delta_T = 2.0 : Speed of the exponential (mV) v_thresh = -50.4 : Spike threshold for the exponential (mV) v_reset = -70.6 : Reset potential after a spike (mV) v_spike = -40.0 : Spike threshold (mV) Variables: I : input current (nA): I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset v : membrane potential in mV (init=-70.6): tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) w : adaptation variable (init=0.0): tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset u += b The ODEs are solved using the explicit Euler method. Equivalent code: EIF_cond_exp_isfa_ista = Neuron ( parameters = \"\"\" v_rest = -70.6 cm = 0.281 tau_m = 9.3667 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_w = 144.0 a = 4.0 b = 0.0805 i_offset = 0.0 delta_T = 2.0 v_thresh = -50.4 v_reset = -70.6 v_spike = -40.0 \"\"\" , equations = \"\"\" I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6 tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"v > v_spike\" , reset = \"\"\" v = v_reset w += b \"\"\" , refractory = 0.1 ) Source code in ANNarchy/models/Neurons.py 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 class EIF_cond_exp_isfa_ista ( Neuron ): ''' EIF_cond_exp neuron. Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.) according to: Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642 Parameters: * v_rest = -70.6 : Resting membrane potential (mV) * cm = 0.281 : Capacity of the membrane (nF) * tau_m = 9.3667 : Membrane time constant (ms) * tau_refrac = 0.1 : Duration of refractory period (ms) * tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) * tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) * e_rev_E = 0.0 : Reversal potential for excitatory input (mV) * e_rev_I = -80.0 : Reversal potential for inhibitory input (mv) * tau_w = 144.0 : Time constant of the adaptation variable (ms) * a = 4.0 : Scaling of the adaptation variable * b = 0.0805 : Increment on the adaptation variable after a spike * i_offset = 0.0 : Offset current (nA) * delta_T = 2.0 : Speed of the exponential (mV) * v_thresh = -50.4 : Spike threshold for the exponential (mV) * v_reset = -70.6 : Reset potential after a spike (mV) * v_spike = -40.0 : Spike threshold (mV) Variables: * I : input current (nA): I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset * v : membrane potential in mV (init=-70.6): tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) * w : adaptation variable (init=0.0): tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w * g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset u += b The ODEs are solved using the explicit Euler method. Equivalent code: ```python EIF_cond_exp_isfa_ista = Neuron( parameters = \"\"\" v_rest = -70.6 cm = 0.281 tau_m = 9.3667 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_w = 144.0 a = 4.0 b = 0.0805 i_offset = 0.0 delta_T = 2.0 v_thresh = -50.4 v_reset = -70.6 v_spike = -40.0 \"\"\", equations = \"\"\" I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6 tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\", spike = \"v > v_spike\", reset = \"\"\" v = v_reset w += b \"\"\", refractory = 0.1 ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , v_rest = - 70.6 , cm = 0.281 , tau_m = 9.3667 , tau_refrac = 0.1 , tau_syn_E = 5.0 , tau_syn_I = 5.0 , e_rev_E = 0.0 , e_rev_I = - 80.0 , tau_w = 144.0 , a = 4.0 , b = 0.0805 , i_offset = 0.0 , delta_T = 2.0 , v_thresh = - 50.4 , v_reset = - 70.6 , v_spike = - 40.0 ): # Create the arguments parameters = \"\"\" v_rest = %(v_rest)s cm = %(cm)s tau_m = %(tau_m)s tau_refrac = %(tau_refrac)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s e_rev_E = %(e_rev_E)s e_rev_I = %(e_rev_I)s tau_w = %(tau_w)s a = %(a)s b = %(b)s i_offset = %(i_offset)s delta_T = %(delta_T)s v_thresh = %(v_thresh)s v_reset = %(v_reset)s v_spike = %(v_spike)s \"\"\" % { 'v_rest' : v_rest , 'cm' : cm , 'tau_m' : tau_m , 'tau_refrac' : tau_refrac , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'e_rev_E' : e_rev_E , 'e_rev_I' : e_rev_I , 'tau_w' : tau_w , 'a' : a , 'b' : b , 'i_offset' : i_offset , 'delta_T' : delta_T , 'v_thresh' : v_thresh , 'v_reset' : v_reset , 'v_spike' : v_spike , } # Equations for the variables equations = \"\"\" I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init= %(v_reset)s tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" % { 'v_reset' : v_reset } spike = \"\"\" v > v_spike \"\"\" reset = \"\"\" v = v_reset w += b \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Adaptive exponential Integrate-and-Fire\" , description = \"Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.).\" ) # For reporting self . _instantiated . append ( True ) EIF_cond_alpha_isfa_ista # Bases: Neuron EIF_cond_alpha neuron. Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.) according to: Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642 Parameters: v_rest = -70.6 : Resting membrane potential (mV) cm = 0.281 : Capacity of the membrane (nF) tau_m = 9.3667 : Membrane time constant (ms) tau_refrac = 0.1 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -80.0 : Reversal potential for inhibitory input (mv) tau_w = 144.0 : Time constant of the adaptation variable (ms) a = 4.0 : Scaling of the adaptation variable b = 0.0805 : Increment on the adaptation variable after a spike i_offset = 0.0 : Offset current (nA) delta_T = 2.0 : Speed of the exponential (mV) v_thresh = -50.4 : Spike threshold for the exponential (mV) v_reset = -70.6 : Reset potential after a spike (mV) v_spike = -40.0 : Spike threshold (mV) Variables: I : input current (nA): I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset v : membrane potential in mV (init=-70.6): tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) w : adaptation variable (init=0.0): tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh alpha_exc : alpha function of excitatory current (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc alpha_inh: alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_spike Reset: v = v_reset u += b The ODEs are solved using the explicit Euler method. Equivalent code: EIF_cond_alpha_isfa_ista = Neuron ( parameters = \"\"\" v_rest = -70.6 cm = 0.281 tau_m = 9.3667 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_w = 144.0 a = 4.0 b = 0.0805 i_offset = 0.0 delta_T = 2.0 v_thresh = -50.4 v_reset = -70.6 v_spike = -40.0 \"\"\" , equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6 tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" , spike = \"v > v_spike\" , reset = \"\"\" v = v_reset w += b \"\"\" , refractory = 0.1 ) Source code in ANNarchy/models/Neurons.py 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 class EIF_cond_alpha_isfa_ista ( Neuron ): ''' EIF_cond_alpha neuron. Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.) according to: Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642 Parameters: * v_rest = -70.6 : Resting membrane potential (mV) * cm = 0.281 : Capacity of the membrane (nF) * tau_m = 9.3667 : Membrane time constant (ms) * tau_refrac = 0.1 : Duration of refractory period (ms) * tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) * tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) * e_rev_E = 0.0 : Reversal potential for excitatory input (mV) * e_rev_I = -80.0 : Reversal potential for inhibitory input (mv) * tau_w = 144.0 : Time constant of the adaptation variable (ms) * a = 4.0 : Scaling of the adaptation variable * b = 0.0805 : Increment on the adaptation variable after a spike * i_offset = 0.0 : Offset current (nA) * delta_T = 2.0 : Speed of the exponential (mV) * v_thresh = -50.4 : Spike threshold for the exponential (mV) * v_reset = -70.6 : Reset potential after a spike (mV) * v_spike = -40.0 : Spike threshold (mV) Variables: * I : input current (nA): I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset * v : membrane potential in mV (init=-70.6): tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) * w : adaptation variable (init=0.0): tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w * g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh * alpha_exc : alpha function of excitatory current (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc * alpha_inh: alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_spike Reset: v = v_reset u += b The ODEs are solved using the explicit Euler method. Equivalent code: ```python EIF_cond_alpha_isfa_ista = Neuron( parameters = \"\"\" v_rest = -70.6 cm = 0.281 tau_m = 9.3667 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_w = 144.0 a = 4.0 b = 0.0805 i_offset = 0.0 delta_T = 2.0 v_thresh = -50.4 v_reset = -70.6 v_spike = -40.0 \"\"\", equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6 tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\", spike = \"v > v_spike\", reset = \"\"\" v = v_reset w += b \"\"\", refractory = 0.1 ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , v_rest = - 70.6 , cm = 0.281 , tau_m = 9.3667 , tau_refrac = 0.1 , tau_syn_E = 5.0 , tau_syn_I = 5.0 , e_rev_E = 0.0 , e_rev_I = - 80.0 , tau_w = 144.0 , a = 4.0 , b = 0.0805 , i_offset = 0.0 , delta_T = 2.0 , v_thresh = - 50.4 , v_reset = - 70.6 , v_spike = - 40.0 ): # Create the arguments parameters = \"\"\" v_rest = %(v_rest)s cm = %(cm)s tau_m = %(tau_m)s tau_refrac = %(tau_refrac)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s e_rev_E = %(e_rev_E)s e_rev_I = %(e_rev_I)s tau_w = %(tau_w)s a = %(a)s b = %(b)s i_offset = %(i_offset)s delta_T = %(delta_T)s v_thresh = %(v_thresh)s v_reset = %(v_reset)s v_spike = %(v_spike)s \"\"\" % { 'v_rest' : v_rest , 'cm' : cm , 'tau_m' : tau_m , 'tau_refrac' : tau_refrac , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'e_rev_E' : e_rev_E , 'e_rev_I' : e_rev_I , 'tau_w' : tau_w , 'a' : a , 'b' : b , 'i_offset' : i_offset , 'delta_T' : delta_T , 'v_thresh' : v_thresh , 'v_reset' : v_reset , 'v_spike' : v_spike , } # Equations for the variables equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init= %(v_reset)s tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" % { 'v_reset' : v_reset } spike = \"\"\" v > v_spike \"\"\" reset = \"\"\" v = v_reset w += b \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Adaptive exponential Integrate-and-Fire\" , description = \"Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.).\" ) # For reporting self . _instantiated . append ( True )","title":"Built-in neuron types"},{"location":"API/SpecificNeuron.html#built-in-neuron-types","text":"ANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN ( http://neuralensemble.org/docs/PyNN/reference/neuronmodels.html ).","title":"Built-in neuron types"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.LeakyIntegrator","text":"Bases: Neuron Leaky-integrator rate-coded neuron, optionally noisy. This simple rate-coded neuron defines an internal variable \\(v(t)\\) which integrates the inputs \\(I(t)\\) with a time constant \\(\\tau\\) and a baseline \\(B\\) . An additive noise \\(N(t)\\) can be optionally defined: \\[\\tau \\cdot \\frac{dv(t)}{dt} + v(t) = I(t) + B + N(t)\\] The transfer function is the positive (or rectified linear ReLU) function with a threshold \\(T\\) : \\[r(t) = (v(t) - T)^+\\] By default, the input \\(I(t)\\) to this neuron is \"sum(exc) - sum(inh)\", but this can be changed by setting the sum argument: neuron = LeakyIntegrator ( sum = \"sum('exc')\" ) By default, there is no additive noise, but the noise argument can be passed with a specific distribution: neuron = LeakyIntegrator ( noise = \"Normal(0.0, 1.0)\" ) Parameters: tau = 10.0 : Time constant in ms of the neuron. B = 0.0 : Baseline value for v. T = 0.0 : Threshold for the positive transfer function. Variables: v : internal variable (init = 0.0): tau * dv/dt + v = sum(exc) - sum(inh) + B + N r : firing rate (init = 0.0): r = pos(v - T) The ODE is solved using the exponential Euler method. Equivalent code: LeakyIntegrator = Neuron ( parameters = ''' tau = 10.0 : population B = 0.0 T = 0.0 : population ''' , equations = ''' tau * dv/dt + v = sum(exc) - sum(inh) + B : exponential r = pos(v - T) ''' ) Source code in ANNarchy/models/Neurons.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 class LeakyIntegrator ( Neuron ): r \"\"\" Leaky-integrator rate-coded neuron, optionally noisy. This simple rate-coded neuron defines an internal variable $v(t)$ which integrates the inputs $I(t)$ with a time constant $\\tau$ and a baseline $B$. An additive noise $N(t)$ can be optionally defined: $$\\tau \\cdot \\frac{dv(t)}{dt} + v(t) = I(t) + B + N(t)$$ The transfer function is the positive (or rectified linear ReLU) function with a threshold $T$: $$r(t) = (v(t) - T)^+$$ By default, the input $I(t)$ to this neuron is \"sum(exc) - sum(inh)\", but this can be changed by setting the ``sum`` argument: ```python neuron = LeakyIntegrator(sum=\"sum('exc')\") ``` By default, there is no additive noise, but the ``noise`` argument can be passed with a specific distribution: ```python neuron = LeakyIntegrator(noise=\"Normal(0.0, 1.0)\") ``` Parameters: * tau = 10.0 : Time constant in ms of the neuron. * B = 0.0 : Baseline value for v. * T = 0.0 : Threshold for the positive transfer function. Variables: * v : internal variable (init = 0.0): tau * dv/dt + v = sum(exc) - sum(inh) + B + N * r : firing rate (init = 0.0): r = pos(v - T) The ODE is solved using the exponential Euler method. Equivalent code: ```python LeakyIntegrator = Neuron( parameters=''' tau = 10.0 : population B = 0.0 T = 0.0 : population ''', equations=''' tau * dv/dt + v = sum(exc) - sum(inh) + B : exponential r = pos(v - T) ''' ) ``` \"\"\" # For reporting _instantiated = [] def __init__ ( self , tau = 10.0 , B = 0.0 , T = 0.0 , sum = 'sum(exc) - sum(inh)' , noise = None ): # Create the arguments parameters = \"\"\" tau = %(tau)s : population B = %(B)s T = %(T)s : population \"\"\" % { 'tau' : tau , 'B' : B , 'T' : T } # Equations for the variables if not noise : noise_def = '' else : noise_def = '+ ' + noise equations = \"\"\" tau * dv/dt + v = %(sum)s + B %(noise)s : exponential r = pos(v - T) \"\"\" % { 'sum' : sum , 'noise' : noise_def } Neuron . __init__ ( self , parameters = parameters , equations = equations , name = \"Leaky-Integrator\" , description = \"Leaky-Integrator with positive transfer function and additive noise.\" ) # For reporting self . _instantiated . append ( True )","title":"LeakyIntegrator"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.Izhikevich","text":"Bases: Neuron Izhikevich neuron as proposed in: Izhikevich, E.M. (2003). Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks , 14:6. http://dx.doi.org/10.1109/TNN.2003.820440 The equations are: \\[\\frac{dv}{dt} = 0.04 * v^2 + 5.0 * v + 140.0 - u + I\\] \\[\\frac{du}{dt} = a * (b * v - u)\\] By default, the conductance is \"g_exc - g_inh\", but this can be changed by setting the conductance argument: neuron = Izhikevich ( conductance = 'g_ampa * (1 + g_nmda) - g_gaba' ) The synapses are instantaneous, i.e the corresponding conductance is increased from the synaptic efficiency w at the time step when a spike is received. Parameters: a = 0.02 : Speed of the recovery variable b = 0.2: Scaling of the recovery variable c = -65.0 : Reset potential. d = 8.0 : Increment of the recovery variable after a spike. v_thresh = 30.0 : Spike threshold (mV). i_offset = 0.0 : external current (nA). noise = 0.0 : Amplitude of the normal additive noise. tau_refrac = 0.0 : Duration of refractory period (ms). Variables: I : input current (user-defined conductance/current + external current + normal noise): I = conductance + i_offset + noise * Normal(0.0, 1.0) v : membrane potential in mV (init = c): dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I u : recovery variable (init= b * c): du/dt = a * (b * v - u) Spike emission: v > v_thresh Reset: v = c u += d The ODEs are solved using the explicit Euler method. Equivalent code: Izhikevich = Neuron ( parameters = \"\"\" noise = 0.0 a = 0.02 b = 0.2 c = -65.0 d = 8.0 v_thresh = 30.0 i_offset = 0.0 \"\"\" , equations = \"\"\" I = g_exc - g_inh + noise * Normal(0.0, 1.0) + i_offset dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init = -65.0 du/dt = a * (b*v - u) : init= -13.0 \"\"\" , spike = \"v > v_thresh\" , reset = \"v = c; u += d\" , refractory = 0.0 ) The default parameters are for a regular spiking (RS) neuron derived from the above mentioned article. Source code in ANNarchy/models/Neurons.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 class Izhikevich ( Neuron ): ''' Izhikevich neuron as proposed in: > Izhikevich, E.M. (2003). *Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks*, 14:6. <http://dx.doi.org/10.1109/TNN.2003.820440> The equations are: $$\\\\frac{dv}{dt} = 0.04 * v^2 + 5.0 * v + 140.0 - u + I$$ $$\\\\frac{du}{dt} = a * (b * v - u)$$ By default, the conductance is \"g_exc - g_inh\", but this can be changed by setting the ``conductance`` argument: ```python neuron = Izhikevich(conductance='g_ampa * (1 + g_nmda) - g_gaba') ``` The synapses are instantaneous, i.e the corresponding conductance is increased from the synaptic efficiency w at the time step when a spike is received. Parameters: * a = 0.02 : Speed of the recovery variable * b = 0.2: Scaling of the recovery variable * c = -65.0 : Reset potential. * d = 8.0 : Increment of the recovery variable after a spike. * v_thresh = 30.0 : Spike threshold (mV). * i_offset = 0.0 : external current (nA). * noise = 0.0 : Amplitude of the normal additive noise. * tau_refrac = 0.0 : Duration of refractory period (ms). Variables: * I : input current (user-defined conductance/current + external current + normal noise): I = conductance + i_offset + noise * Normal(0.0, 1.0) * v : membrane potential in mV (init = c): dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I * u : recovery variable (init= b * c): du/dt = a * (b * v - u) Spike emission: v > v_thresh Reset: v = c u += d The ODEs are solved using the explicit Euler method. Equivalent code: ```python Izhikevich = Neuron( parameters = \"\"\" noise = 0.0 a = 0.02 b = 0.2 c = -65.0 d = 8.0 v_thresh = 30.0 i_offset = 0.0 \"\"\", equations = \"\"\" I = g_exc - g_inh + noise * Normal(0.0, 1.0) + i_offset dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init = -65.0 du/dt = a * (b*v - u) : init= -13.0 \"\"\", spike = \"v > v_thresh\", reset = \"v = c; u += d\", refractory = 0.0 ) ``` The default parameters are for a regular spiking (RS) neuron derived from the above mentioned article. ''' # For reporting _instantiated = [] def __init__ ( self , a = 0.02 , b = 0.2 , c =- 65.0 , d = 8.0 , v_thresh = 30.0 , i_offset = 0.0 , noise = 0.0 , tau_refrac = 0.0 , conductance = \"g_exc - g_inh\" ): # Extract which targets are defined in the conductance #import re #targets = re.findall(r'g_([\\w]+)', conductance) # Create the arguments parameters = \"\"\" noise = %(noise)s a = %(a)s b = %(b)s c = %(c)s d = %(d)s v_thresh = %(v_thresh)s i_offset = %(i_offset)s tau_refrac = %(tau_refrac)s \"\"\" % { 'a' : a , 'b' : b , 'c' : c , 'd' : d , 'v_thresh' : v_thresh , 'i_offset' : i_offset , 'noise' : noise , 'tau_refrac' : tau_refrac } # Equations for the variables equations = \"\"\" I = %(conductance)s + noise * Normal(0.0, 1.0) + i_offset dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init = %(c)s du/dt = a * (b*v - u) : init= %(u)s \"\"\" % { 'conductance' : conductance , 'c' : c , 'u' : b * c } spike = \"\"\" v > v_thresh \"\"\" reset = \"\"\" v = c u += d \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Izhikevich\" , description = \"Quadratic integrate-and-fire spiking neuron with adaptation.\" ) # For reporting self . _instantiated . append ( True )","title":"Izhikevich"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.IF_curr_exp","text":"Bases: Neuron IF_curr_exp neuron. Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current. (Separate synaptic currents for excitatory and inhibitory synapses). Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_curr_exp = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 ) Source code in ANNarchy/models/Neurons.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 class IF_curr_exp ( Neuron ): ''' IF_curr_exp neuron. Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current. (Separate synaptic currents for excitatory and inhibitory synapses). Parameters: * v_rest = -65.0 : Resting membrane potential (mV) * cm = 1.0 : Capacity of the membrane (nF) * tau_m = 20.0 : Membrane time constant (ms) * tau_refrac = 0.0 : Duration of refractory period (ms) * tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) * tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) * i_offset = 0.0 : Offset current (nA) * v_reset = -65.0 : Reset potential after a spike (mV) * v_thresh = -50.0 : Spike threshold (mV) Variables: * v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset * g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: ```python IF_curr_exp = Neuron( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\", equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\", spike = \"v > v_thresh\", reset = \"v = v_reset\", refractory = 0.0 ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , v_rest =- 65.0 , cm = 1.0 , tau_m = 20.0 , tau_refrac = 0.0 , tau_syn_E = 5.0 , tau_syn_I = 5.0 , v_thresh =- 50.0 , v_reset =- 65.0 , i_offset = 0.0 ): # Create the arguments parameters = \"\"\" v_rest = %(v_rest)s cm = %(cm)s tau_m = %(tau_m)s tau_refrac = %(tau_refrac)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s v_thresh = %(v_thresh)s v_reset = %(v_reset)s i_offset = %(i_offset)s \"\"\" % { 'v_rest' : v_rest , 'cm' : cm , 'tau_m' : tau_m , 'tau_refrac' : tau_refrac , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'v_thresh' : v_thresh , 'v_reset' : v_reset , 'i_offset' : i_offset } # Equations for the variables equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc - g_inh + i_offset : exponential, init= %(v_reset)s tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" % { 'v_reset' : v_reset } spike = \"\"\" v > v_thresh \"\"\" reset = \"\"\" v = v_reset \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Integrate-and-Fire\" , description = \"Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current.\" ) # For reporting self . _instantiated . append ( True )","title":"IF_curr_exp"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.IF_cond_exp","text":"Bases: Neuron IF_cond_exp neuron. Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance. Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -70.0 : Reversal potential for inhibitory input (mv) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_cond_exp = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -70.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 ) Source code in ANNarchy/models/Neurons.py 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 class IF_cond_exp ( Neuron ): ''' IF_cond_exp neuron. Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance. Parameters: * v_rest = -65.0 : Resting membrane potential (mV) * cm = 1.0 : Capacity of the membrane (nF) * tau_m = 20.0 : Membrane time constant (ms) * tau_refrac = 0.0 : Duration of refractory period (ms) * tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) * tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) * e_rev_E = 0.0 : Reversal potential for excitatory input (mV) * e_rev_I = -70.0 : Reversal potential for inhibitory input (mv) * i_offset = 0.0 : Offset current (nA) * v_reset = -65.0 : Reset potential after a spike (mV) * v_thresh = -50.0 : Spike threshold (mV) Variables: * v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset * g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: ```python IF_cond_exp = Neuron( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -70.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\", equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\", spike = \"v > v_thresh\", reset = \"v = v_reset\", refractory = 0.0 ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , v_rest =- 65.0 , cm = 1.0 , tau_m = 20.0 , tau_refrac = 0.0 , tau_syn_E = 5.0 , tau_syn_I = 5.0 , e_rev_E = 0.0 , e_rev_I = - 70.0 , v_thresh =- 50.0 , v_reset =- 65.0 , i_offset = 0.0 ): # Create the arguments parameters = \"\"\" v_rest = %(v_rest)s cm = %(cm)s tau_m = %(tau_m)s tau_refrac = %(tau_refrac)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s v_thresh = %(v_thresh)s v_reset = %(v_reset)s i_offset = %(i_offset)s e_rev_E = %(e_rev_E)s e_rev_I = %(e_rev_I)s \"\"\" % { 'v_rest' : v_rest , 'cm' : cm , 'tau_m' : tau_m , 'tau_refrac' : tau_refrac , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'v_thresh' : v_thresh , 'v_reset' : v_reset , 'i_offset' : i_offset , 'e_rev_E' : e_rev_E , 'e_rev_I' : e_rev_I } # Equations for the variables equations = \"\"\" cm * dv/dt = cm/tau_m*(v_rest -v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset : exponential, init= %(v_reset)s tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" % { 'v_reset' : v_reset } spike = \"\"\" v > v_thresh \"\"\" reset = \"\"\" v = v_reset \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Integrate-and-Fire\" , description = \"Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductances.\" ) # For reporting self . _instantiated . append ( True )","title":"IF_cond_exp"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.IF_curr_alpha","text":"Bases: Neuron IF_curr_alpha neuron. Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents. (Separate synaptic currents for excitatory and inhibitory synapses). The alpha currents are calculated through a system of two linears ODEs. After a spike is received at t_spike, it peaks at t_spike + tau_syn_X, with a maximum equal to the synaptic efficiency. Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc alpha_exc : alpha function of excitatory current (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh alpha_inh : alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_curr_alpha = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 ) Source code in ANNarchy/models/Neurons.py 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 class IF_curr_alpha ( Neuron ): ''' IF_curr_alpha neuron. Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents. (Separate synaptic currents for excitatory and inhibitory synapses). The alpha currents are calculated through a system of two linears ODEs. After a spike is received at t_spike, it peaks at t_spike + tau_syn_X, with a maximum equal to the synaptic efficiency. Parameters: * v_rest = -65.0 : Resting membrane potential (mV) * cm = 1.0 : Capacity of the membrane (nF) * tau_m = 20.0 : Membrane time constant (ms) * tau_refrac = 0.0 : Duration of refractory period (ms) * tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms) * tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms) * i_offset = 0.0 : Offset current (nA) * v_reset = -65.0 : Reset potential after a spike (mV) * v_thresh = -50.0 : Spike threshold (mV) Variables: * v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset * g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * alpha_exc : alpha function of excitatory current (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc * g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh * alpha_inh : alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: ```python IF_curr_alpha = Neuron( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\", equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\", spike = \"v > v_thresh\", reset = \"v = v_reset\", refractory = 0.0 ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , v_rest =- 65.0 , cm = 1.0 , tau_m = 20.0 , tau_refrac = 0.0 , tau_syn_E = 5.0 , tau_syn_I = 5.0 , v_thresh =- 50.0 , v_reset =- 65.0 , i_offset = 0.0 ): # Create the arguments parameters = \"\"\" v_rest = %(v_rest)s cm = %(cm)s tau_m = %(tau_m)s tau_refrac = %(tau_refrac)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s v_thresh = %(v_thresh)s v_reset = %(v_reset)s i_offset = %(i_offset)s \"\"\" % { 'v_rest' : v_rest , 'cm' : cm , 'tau_m' : tau_m , 'tau_refrac' : tau_refrac , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'v_thresh' : v_thresh , 'v_reset' : v_reset , 'i_offset' : i_offset } # Equations for the variables equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset : exponential, init= %(v_reset)s tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" % { 'v_reset' : v_reset } spike = \"\"\" v > v_thresh \"\"\" reset = \"\"\" v = v_reset \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Integrate-and-Fire\" , description = \"Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents.\" ) # For reporting self . _instantiated . append ( True )","title":"IF_curr_alpha"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.IF_cond_alpha","text":"Bases: Neuron IF_cond_exp neuron. Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance. Parameters: v_rest = -65.0 : Resting membrane potential (mV) cm = 1.0 : Capacity of the membrane (nF) tau_m = 20.0 : Membrane time constant (ms) tau_refrac = 0.0 : Duration of refractory period (ms) tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -70.0 : Reversal potential for inhibitory input (mv) i_offset = 0.0 : Offset current (nA) v_reset = -65.0 : Reset potential after a spike (mV) v_thresh = -50.0 : Spike threshold (mV) Variables: v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset g_exc : excitatory conductance (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc alpha_exc : alpha function of excitatory conductance (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc g_inh : inhibitory conductance (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh alpha_inh : alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: IF_cond_alpha = Neuron ( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -70.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\" , equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" , spike = \"v > v_thresh\" , reset = \"v = v_reset\" , refractory = 0.0 ) Source code in ANNarchy/models/Neurons.py 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 class IF_cond_alpha ( Neuron ): ''' IF_cond_exp neuron. Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance. Parameters: * v_rest = -65.0 : Resting membrane potential (mV) * cm = 1.0 : Capacity of the membrane (nF) * tau_m = 20.0 : Membrane time constant (ms) * tau_refrac = 0.0 : Duration of refractory period (ms) * tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms) * tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms) * e_rev_E = 0.0 : Reversal potential for excitatory input (mV) * e_rev_I = -70.0 : Reversal potential for inhibitory input (mv) * i_offset = 0.0 : Offset current (nA) * v_reset = -65.0 : Reset potential after a spike (mV) * v_thresh = -50.0 : Spike threshold (mV) Variables: * v : membrane potential in mV (init=-65.0): cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset * g_exc : excitatory conductance (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * alpha_exc : alpha function of excitatory conductance (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc * g_inh : inhibitory conductance (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh * alpha_inh : alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_thresh Reset: v = v_reset The ODEs are solved using the exponential Euler method. Equivalent code: ```python IF_cond_alpha = Neuron( parameters = \"\"\" v_rest = -65.0 cm = 1.0 tau_m = 20.0 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -70.0 v_thresh = -50.0 v_reset = -65.0 i_offset = 0.0 \"\"\", equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0 tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\", spike = \"v > v_thresh\", reset = \"v = v_reset\", refractory = 0.0 ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , v_rest =- 65.0 , cm = 1.0 , tau_m = 20.0 , tau_refrac = 0.0 , tau_syn_E = 5.0 , tau_syn_I = 5.0 , e_rev_E = 0.0 , e_rev_I = - 70.0 , v_thresh =- 50.0 , v_reset =- 65.0 , i_offset = 0.0 ): # Create the arguments parameters = \"\"\" v_rest = %(v_rest)s cm = %(cm)s tau_m = %(tau_m)s tau_refrac = %(tau_refrac)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s v_thresh = %(v_thresh)s v_reset = %(v_reset)s i_offset = %(i_offset)s e_rev_E = %(e_rev_E)s e_rev_I = %(e_rev_I)s \"\"\" % { 'v_rest' : v_rest , 'cm' : cm , 'tau_m' : tau_m , 'tau_refrac' : tau_refrac , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'v_thresh' : v_thresh , 'v_reset' : v_reset , 'i_offset' : i_offset , 'e_rev_E' : e_rev_E , 'e_rev_I' : e_rev_I } # Equations for the variables equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset : exponential, init= %(v_reset)s tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" % { 'v_reset' : v_reset } spike = \"\"\" v > v_thresh \"\"\" reset = \"\"\" v = v_reset \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Integrate-and-Fire\" , description = \"Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductances.\" ) # For reporting self . _instantiated . append ( True )","title":"IF_cond_alpha"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.HH_cond_exp","text":"Bases: Neuron HH_cond_exp neuron. Single-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub. Parameters: gbar_Na = 20.0 : Maximal conductance of the Sodium current. gbar_K = 6.0 : Maximal conductance of the Potassium current. gleak = 0.01 : Conductance of the leak current (nF) cm = 0.2 : Capacity of the membrane (nF) v_offset = -63.0 : Threshold for the rate constants (mV) e_rev_Na = 50.0 : Reversal potential for the Sodium current (mV) e_rev_K = -90.0 : Reversal potential for the Potassium current (mV) e_rev_leak = -65.0 : Reversal potential for the leak current (mV) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -80.0 : Reversal potential for inhibitory input (mV) tau_syn_E = 0.2 : Decay time of excitatory synaptic current (ms) tau_syn_I = 2.0 : Decay time of inhibitory synaptic current (ms) i_offset = 0.0 : Offset current (nA) v_thresh = 0.0 : Threshold for spike emission Variables: Voltage-dependent rate constants an, bn, am, bm, ah, bh: an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) Activation variables n, m, h (h is initialized to 1.0, n and m to 0.0): dn/dt = an * (1.0 - n) - bn * n dm/dt = am * (1.0 - m) - bm * m dh/dt = ah * (1.0 - h) - bh * h v : membrane potential in mV (init=-65.0): cm * dv/dt = gleak (e_rev_leak -v) + gbar_K * n 4 * (e_rev_K - v) + gbar_Na * m *3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset g_exc : excitatory conductance (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory conductance (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission (the spike is emitted only once when v crosses the threshold from below): v > v_thresh and v(t-1) < v_thresh The ODEs for n, m, h and v are solved using the midpoint method, while the conductances g_exc and g_inh are solved using the exponential Euler method. Equivalent code: HH_cond_exp = Neuron ( parameters = \"\"\" gbar_Na = 20.0 gbar_K = 6.0 gleak = 0.01 cm = 0.2 v_offset = -63.0 e_rev_Na = 50.0 e_rev_K = -90.0 e_rev_leak = -65.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_syn_E = 0.2 tau_syn_I = 2.0 i_offset = 0.0 v_thresh = 0.0 \"\"\" , equations = \"\"\" # Previous membrane potential prev_v = v # Voltage-dependent rate constants an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) # Activation variables dn/dt = an * (1.0 - n) - bn * n : init = 0.0, exponential dm/dt = am * (1.0 - m) - bm * m : init = 0.0, exponential dh/dt = ah * (1.0 - h) - bh * h : init = 1.0, exponential # Membrane equation cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset: exponential, init=-65.0 # Exponentially-decaying conductances tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"(v > v_thresh) and (prev_v <= v_thresh)\" , reset = \"\" ) Source code in ANNarchy/models/Neurons.py 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 class HH_cond_exp ( Neuron ): ''' HH_cond_exp neuron. Single-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub. Parameters: * gbar_Na = 20.0 : Maximal conductance of the Sodium current. * gbar_K = 6.0 : Maximal conductance of the Potassium current. * gleak = 0.01 : Conductance of the leak current (nF) * cm = 0.2 : Capacity of the membrane (nF) * v_offset = -63.0 : Threshold for the rate constants (mV) * e_rev_Na = 50.0 : Reversal potential for the Sodium current (mV) * e_rev_K = -90.0 : Reversal potential for the Potassium current (mV) * e_rev_leak = -65.0 : Reversal potential for the leak current (mV) * e_rev_E = 0.0 : Reversal potential for excitatory input (mV) * e_rev_I = -80.0 : Reversal potential for inhibitory input (mV) * tau_syn_E = 0.2 : Decay time of excitatory synaptic current (ms) * tau_syn_I = 2.0 : Decay time of inhibitory synaptic current (ms) * i_offset = 0.0 : Offset current (nA) * v_thresh = 0.0 : Threshold for spike emission Variables: * Voltage-dependent rate constants an, bn, am, bm, ah, bh: an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) * Activation variables n, m, h (h is initialized to 1.0, n and m to 0.0): dn/dt = an * (1.0 - n) - bn * n dm/dt = am * (1.0 - m) - bm * m dh/dt = ah * (1.0 - h) - bh * h * v : membrane potential in mV (init=-65.0): cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset * g_exc : excitatory conductance (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * g_inh : inhibitory conductance (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission (the spike is emitted only once when v crosses the threshold from below): v > v_thresh and v(t-1) < v_thresh The ODEs for n, m, h and v are solved using the midpoint method, while the conductances g_exc and g_inh are solved using the exponential Euler method. Equivalent code: ```python HH_cond_exp = Neuron( parameters = \"\"\" gbar_Na = 20.0 gbar_K = 6.0 gleak = 0.01 cm = 0.2 v_offset = -63.0 e_rev_Na = 50.0 e_rev_K = -90.0 e_rev_leak = -65.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_syn_E = 0.2 tau_syn_I = 2.0 i_offset = 0.0 v_thresh = 0.0 \"\"\", equations = \"\"\" # Previous membrane potential prev_v = v # Voltage-dependent rate constants an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) # Activation variables dn/dt = an * (1.0 - n) - bn * n : init = 0.0, exponential dm/dt = am * (1.0 - m) - bm * m : init = 0.0, exponential dh/dt = ah * (1.0 - h) - bh * h : init = 1.0, exponential # Membrane equation cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset: exponential, init=-65.0 # Exponentially-decaying conductances tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\", spike = \"(v > v_thresh) and (prev_v <= v_thresh)\", reset = \"\" ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , gbar_Na = 20.0 , gbar_K = 6.0 , gleak = 0.01 , cm = 0.2 , v_offset = - 63.0 , e_rev_Na = 50.0 , e_rev_K = - 90.0 , e_rev_leak = - 65.0 , e_rev_E = 0.0 , e_rev_I = - 80.0 , tau_syn_E = 0.2 , tau_syn_I = 2.0 , i_offset = 0.0 , v_thresh = 0.0 ): parameters = \"\"\" gbar_Na = %(gbar_Na)s gbar_K = %(gbar_K)s gleak = %(gleak)s cm = %(cm)s v_offset = %(v_offset)s e_rev_Na = %(e_rev_Na)s e_rev_K = %(e_rev_K)s e_rev_leak = %(e_rev_leak)s e_rev_E = %(e_rev_E)s e_rev_I = %(e_rev_I)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s i_offset = %(i_offset)s v_thresh = %(v_thresh)s \"\"\" % { 'gbar_Na' : gbar_Na , 'gbar_K' : gbar_K , 'gleak' : gleak , 'cm' : cm , 'v_offset' : v_offset , 'e_rev_Na' : e_rev_Na , 'e_rev_K' : e_rev_K , 'e_rev_leak' : e_rev_leak , 'e_rev_E' : e_rev_E , 'e_rev_I' : e_rev_I , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'i_offset' : i_offset , 'v_thresh' : v_thresh } equations = \"\"\" # Previous membrane potential prev_v = v # Voltage-dependent rate constants an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32 * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) bn = 0.5 * exp ((10.0 - v + v_offset)/40.0) bm = 0.28 * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) ) # Activation variables dn/dt = an * (1.0 - n) - bn * n : init = 0.0, exponential dm/dt = am * (1.0 - m) - bm * m : init = 0.0, exponential dh/dt = ah * (1.0 - h) - bh * h : init = 1.0, exponential # Membrane equation cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset: exponential, init= %(e_rev_leak)s # Exponentially-decaying conductances tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" % { 'e_rev_leak' : e_rev_leak } spike = \"(v > v_thresh) and (prev_v <= v_thresh)\" reset = \"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , name = \"Hodgkin-Huxley\" , description = \"Single-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents.\" ) # For reporting self . _instantiated . append ( True )","title":"HH_cond_exp"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.EIF_cond_exp_isfa_ista","text":"Bases: Neuron EIF_cond_exp neuron. Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.) according to: Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642 Parameters: v_rest = -70.6 : Resting membrane potential (mV) cm = 0.281 : Capacity of the membrane (nF) tau_m = 9.3667 : Membrane time constant (ms) tau_refrac = 0.1 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -80.0 : Reversal potential for inhibitory input (mv) tau_w = 144.0 : Time constant of the adaptation variable (ms) a = 4.0 : Scaling of the adaptation variable b = 0.0805 : Increment on the adaptation variable after a spike i_offset = 0.0 : Offset current (nA) delta_T = 2.0 : Speed of the exponential (mV) v_thresh = -50.4 : Spike threshold for the exponential (mV) v_reset = -70.6 : Reset potential after a spike (mV) v_spike = -40.0 : Spike threshold (mV) Variables: I : input current (nA): I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset v : membrane potential in mV (init=-70.6): tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) w : adaptation variable (init=0.0): tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset u += b The ODEs are solved using the explicit Euler method. Equivalent code: EIF_cond_exp_isfa_ista = Neuron ( parameters = \"\"\" v_rest = -70.6 cm = 0.281 tau_m = 9.3667 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_w = 144.0 a = 4.0 b = 0.0805 i_offset = 0.0 delta_T = 2.0 v_thresh = -50.4 v_reset = -70.6 v_spike = -40.0 \"\"\" , equations = \"\"\" I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6 tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" , spike = \"v > v_spike\" , reset = \"\"\" v = v_reset w += b \"\"\" , refractory = 0.1 ) Source code in ANNarchy/models/Neurons.py 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 class EIF_cond_exp_isfa_ista ( Neuron ): ''' EIF_cond_exp neuron. Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.) according to: Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642 Parameters: * v_rest = -70.6 : Resting membrane potential (mV) * cm = 0.281 : Capacity of the membrane (nF) * tau_m = 9.3667 : Membrane time constant (ms) * tau_refrac = 0.1 : Duration of refractory period (ms) * tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) * tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) * e_rev_E = 0.0 : Reversal potential for excitatory input (mV) * e_rev_I = -80.0 : Reversal potential for inhibitory input (mv) * tau_w = 144.0 : Time constant of the adaptation variable (ms) * a = 4.0 : Scaling of the adaptation variable * b = 0.0805 : Increment on the adaptation variable after a spike * i_offset = 0.0 : Offset current (nA) * delta_T = 2.0 : Speed of the exponential (mV) * v_thresh = -50.4 : Spike threshold for the exponential (mV) * v_reset = -70.6 : Reset potential after a spike (mV) * v_spike = -40.0 : Spike threshold (mV) Variables: * I : input current (nA): I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset * v : membrane potential in mV (init=-70.6): tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) * w : adaptation variable (init=0.0): tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w * g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh Spike emission: v > v_thresh Reset: v = v_reset u += b The ODEs are solved using the explicit Euler method. Equivalent code: ```python EIF_cond_exp_isfa_ista = Neuron( parameters = \"\"\" v_rest = -70.6 cm = 0.281 tau_m = 9.3667 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_w = 144.0 a = 4.0 b = 0.0805 i_offset = 0.0 delta_T = 2.0 v_thresh = -50.4 v_reset = -70.6 v_spike = -40.0 \"\"\", equations = \"\"\" I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6 tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\", spike = \"v > v_spike\", reset = \"\"\" v = v_reset w += b \"\"\", refractory = 0.1 ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , v_rest = - 70.6 , cm = 0.281 , tau_m = 9.3667 , tau_refrac = 0.1 , tau_syn_E = 5.0 , tau_syn_I = 5.0 , e_rev_E = 0.0 , e_rev_I = - 80.0 , tau_w = 144.0 , a = 4.0 , b = 0.0805 , i_offset = 0.0 , delta_T = 2.0 , v_thresh = - 50.4 , v_reset = - 70.6 , v_spike = - 40.0 ): # Create the arguments parameters = \"\"\" v_rest = %(v_rest)s cm = %(cm)s tau_m = %(tau_m)s tau_refrac = %(tau_refrac)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s e_rev_E = %(e_rev_E)s e_rev_I = %(e_rev_I)s tau_w = %(tau_w)s a = %(a)s b = %(b)s i_offset = %(i_offset)s delta_T = %(delta_T)s v_thresh = %(v_thresh)s v_reset = %(v_reset)s v_spike = %(v_spike)s \"\"\" % { 'v_rest' : v_rest , 'cm' : cm , 'tau_m' : tau_m , 'tau_refrac' : tau_refrac , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'e_rev_E' : e_rev_E , 'e_rev_I' : e_rev_I , 'tau_w' : tau_w , 'a' : a , 'b' : b , 'i_offset' : i_offset , 'delta_T' : delta_T , 'v_thresh' : v_thresh , 'v_reset' : v_reset , 'v_spike' : v_spike , } # Equations for the variables equations = \"\"\" I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init= %(v_reset)s tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential \"\"\" % { 'v_reset' : v_reset } spike = \"\"\" v > v_spike \"\"\" reset = \"\"\" v = v_reset w += b \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Adaptive exponential Integrate-and-Fire\" , description = \"Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.).\" ) # For reporting self . _instantiated . append ( True )","title":"EIF_cond_exp_isfa_ista"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.EIF_cond_alpha_isfa_ista","text":"Bases: Neuron EIF_cond_alpha neuron. Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.) according to: Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642 Parameters: v_rest = -70.6 : Resting membrane potential (mV) cm = 0.281 : Capacity of the membrane (nF) tau_m = 9.3667 : Membrane time constant (ms) tau_refrac = 0.1 : Duration of refractory period (ms) tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) e_rev_E = 0.0 : Reversal potential for excitatory input (mV) e_rev_I = -80.0 : Reversal potential for inhibitory input (mv) tau_w = 144.0 : Time constant of the adaptation variable (ms) a = 4.0 : Scaling of the adaptation variable b = 0.0805 : Increment on the adaptation variable after a spike i_offset = 0.0 : Offset current (nA) delta_T = 2.0 : Speed of the exponential (mV) v_thresh = -50.4 : Spike threshold for the exponential (mV) v_reset = -70.6 : Reset potential after a spike (mV) v_spike = -40.0 : Spike threshold (mV) Variables: I : input current (nA): I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset v : membrane potential in mV (init=-70.6): tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) w : adaptation variable (init=0.0): tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh alpha_exc : alpha function of excitatory current (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc alpha_inh: alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_spike Reset: v = v_reset u += b The ODEs are solved using the explicit Euler method. Equivalent code: EIF_cond_alpha_isfa_ista = Neuron ( parameters = \"\"\" v_rest = -70.6 cm = 0.281 tau_m = 9.3667 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_w = 144.0 a = 4.0 b = 0.0805 i_offset = 0.0 delta_T = 2.0 v_thresh = -50.4 v_reset = -70.6 v_spike = -40.0 \"\"\" , equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6 tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" , spike = \"v > v_spike\" , reset = \"\"\" v = v_reset w += b \"\"\" , refractory = 0.1 ) Source code in ANNarchy/models/Neurons.py 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 class EIF_cond_alpha_isfa_ista ( Neuron ): ''' EIF_cond_alpha neuron. Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.) according to: Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642 Parameters: * v_rest = -70.6 : Resting membrane potential (mV) * cm = 0.281 : Capacity of the membrane (nF) * tau_m = 9.3667 : Membrane time constant (ms) * tau_refrac = 0.1 : Duration of refractory period (ms) * tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms) * tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms) * e_rev_E = 0.0 : Reversal potential for excitatory input (mV) * e_rev_I = -80.0 : Reversal potential for inhibitory input (mv) * tau_w = 144.0 : Time constant of the adaptation variable (ms) * a = 4.0 : Scaling of the adaptation variable * b = 0.0805 : Increment on the adaptation variable after a spike * i_offset = 0.0 : Offset current (nA) * delta_T = 2.0 : Speed of the exponential (mV) * v_thresh = -50.4 : Spike threshold for the exponential (mV) * v_reset = -70.6 : Reset potential after a spike (mV) * v_spike = -40.0 : Spike threshold (mV) Variables: * I : input current (nA): I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset * v : membrane potential in mV (init=-70.6): tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) * w : adaptation variable (init=0.0): tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w * g_exc : excitatory current (init = 0.0): tau_syn_E * dg_exc/dt = - g_exc * g_inh : inhibitory current (init = 0.0): tau_syn_I * dg_inh/dt = - g_inh * alpha_exc : alpha function of excitatory current (init = 0.0): tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc * alpha_inh: alpha function of inhibitory current (init = 0.0): tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh Spike emission: v > v_spike Reset: v = v_reset u += b The ODEs are solved using the explicit Euler method. Equivalent code: ```python EIF_cond_alpha_isfa_ista = Neuron( parameters = \"\"\" v_rest = -70.6 cm = 0.281 tau_m = 9.3667 tau_syn_E = 5.0 tau_syn_I = 5.0 e_rev_E = 0.0 e_rev_I = -80.0 tau_w = 144.0 a = 4.0 b = 0.0805 i_offset = 0.0 delta_T = 2.0 v_thresh = -50.4 v_reset = -70.6 v_spike = -40.0 \"\"\", equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6 tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\", spike = \"v > v_spike\", reset = \"\"\" v = v_reset w += b \"\"\", refractory = 0.1 ) ``` ''' # For reporting _instantiated = [] def __init__ ( self , v_rest = - 70.6 , cm = 0.281 , tau_m = 9.3667 , tau_refrac = 0.1 , tau_syn_E = 5.0 , tau_syn_I = 5.0 , e_rev_E = 0.0 , e_rev_I = - 80.0 , tau_w = 144.0 , a = 4.0 , b = 0.0805 , i_offset = 0.0 , delta_T = 2.0 , v_thresh = - 50.4 , v_reset = - 70.6 , v_spike = - 40.0 ): # Create the arguments parameters = \"\"\" v_rest = %(v_rest)s cm = %(cm)s tau_m = %(tau_m)s tau_refrac = %(tau_refrac)s tau_syn_E = %(tau_syn_E)s tau_syn_I = %(tau_syn_I)s e_rev_E = %(e_rev_E)s e_rev_I = %(e_rev_I)s tau_w = %(tau_w)s a = %(a)s b = %(b)s i_offset = %(i_offset)s delta_T = %(delta_T)s v_thresh = %(v_thresh)s v_reset = %(v_reset)s v_spike = %(v_spike)s \"\"\" % { 'v_rest' : v_rest , 'cm' : cm , 'tau_m' : tau_m , 'tau_refrac' : tau_refrac , 'tau_syn_E' : tau_syn_E , 'tau_syn_I' : tau_syn_I , 'e_rev_E' : e_rev_E , 'e_rev_I' : e_rev_I , 'tau_w' : tau_w , 'a' : a , 'b' : b , 'i_offset' : i_offset , 'delta_T' : delta_T , 'v_thresh' : v_thresh , 'v_reset' : v_reset , 'v_spike' : v_spike , } # Equations for the variables equations = \"\"\" gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E) gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I) I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset tau_m * dv/dt = (v_rest - v + delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init= %(v_reset)s tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w tau_syn_E * dg_exc/dt = - g_exc : exponential tau_syn_I * dg_inh/dt = - g_inh : exponential tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc : exponential tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh : exponential \"\"\" % { 'v_reset' : v_reset } spike = \"\"\" v > v_spike \"\"\" reset = \"\"\" v = v_reset w += b \"\"\" Neuron . __init__ ( self , parameters = parameters , equations = equations , spike = spike , reset = reset , refractory = 'tau_refrac' , name = \"Adaptive exponential Integrate-and-Fire\" , description = \"Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.).\" ) # For reporting self . _instantiated . append ( True )","title":"EIF_cond_alpha_isfa_ista"},{"location":"API/SpecificPopulation.html","text":"Specific Populations # ANNarchy provides a set of predefined Population objects to ease the definition of standard networks. PoissonPopulation # Bases: SpecificPopulation Population of spiking neurons following a Poisson distribution. Case 1: Input population Each neuron of the population will randomly emit spikes, with a mean firing rate defined by the rates argument. The mean firing rate in Hz can be a fixed value for all neurons: pop = PoissonPopulation ( geometry = 100 , rates = 100.0 ) but it can be modified later as a normal parameter: pop . rates = np . linspace ( 10 , 150 , 100 ) It is also possible to define a temporal equation for the rates, by passing a string to the argument: pop = PoissonPopulation ( geometry = 100 , rates = \"100.0 * (1.0 + sin(2*pi*t/1000.0) )/2.0\" ) The syntax of this equation follows the same structure as neural variables. It is also possible to add parameters to the population which can be used in the equation of rates : pop = PoissonPopulation ( geometry = 100 , parameters = ''' amp = 100.0 frequency = 1.0 ''' , rates = \"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\" ) Note: The preceding definition is fully equivalent to the definition of this neuron: poisson = Neuron ( parameters = ''' amp = 100.0 frequency = 1.0 ''' , equations = ''' rates = amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0 p = Uniform(0.0, 1.0) * 1000.0 / dt ''' , spike = ''' p < rates ''' ) The refractory period can also be set, so that a neuron can not emit two spikes too close from each other. Case 2: Hybrid population If the rates argument is not set, the population can be used as an interface from a rate-coded population. The target argument specifies which incoming projections will be summed to determine the instantaneous firing rate of each neuron. See the example in examples/hybrid/Hybrid.py for a usage. Source code in ANNarchy/core/SpecificPopulation.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 class PoissonPopulation ( SpecificPopulation ): \"\"\" Population of spiking neurons following a Poisson distribution. **Case 1:** Input population Each neuron of the population will randomly emit spikes, with a mean firing rate defined by the *rates* argument. The mean firing rate in Hz can be a fixed value for all neurons: ```python pop = PoissonPopulation(geometry=100, rates=100.0) ``` but it can be modified later as a normal parameter: ```python pop.rates = np.linspace(10, 150, 100) ``` It is also possible to define a temporal equation for the rates, by passing a string to the argument: ```python pop = PoissonPopulation( geometry=100, rates=\"100.0 * (1.0 + sin(2*pi*t/1000.0) )/2.0\" ) ``` The syntax of this equation follows the same structure as neural variables. It is also possible to add parameters to the population which can be used in the equation of `rates`: ```python pop = PoissonPopulation( geometry=100, parameters = ''' amp = 100.0 frequency = 1.0 ''', rates=\"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\" ) ``` **Note:** The preceding definition is fully equivalent to the definition of this neuron: ```python poisson = Neuron( parameters = ''' amp = 100.0 frequency = 1.0 ''', equations = ''' rates = amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0 p = Uniform(0.0, 1.0) * 1000.0 / dt ''', spike = ''' p < rates ''' ) ``` The refractory period can also be set, so that a neuron can not emit two spikes too close from each other. **Case 2:** Hybrid population If the ``rates`` argument is not set, the population can be used as an interface from a rate-coded population. The ``target`` argument specifies which incoming projections will be summed to determine the instantaneous firing rate of each neuron. See the example in ``examples/hybrid/Hybrid.py`` for a usage. \"\"\" def __init__ ( self , geometry , name = None , rates = None , target = None , parameters = None , refractory = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. :param name: unique name of the population (optional). :param rates: mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string). :param target: the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\"). :param parameters: additional parameters which can be used in the *rates* equation. :param refractory: refractory period in ms. \"\"\" if rates is None and target is None : Global . _error ( 'A PoissonPopulation must define either rates or target.' ) self . target = target self . parameters = parameters self . refractory_init = refractory self . rates_init = rates if target is not None : # hybrid population # Create the neuron poisson_neuron = Neuron ( parameters = \"\"\" %(params)s \"\"\" % { 'params' : parameters if parameters else '' }, equations = \"\"\" rates = sum( %(target)s ) p = Uniform(0.0, 1.0) * 1000.0 / dt _sum_ %(target)s = 0.0 \"\"\" % { 'target' : target }, spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Hybrid\" , description = \"Hybrid spiking neuron emitting spikes according to a Poisson distribution at a frequency determined by the weighted sum of inputs.\" ) elif isinstance ( rates , str ): # Create the neuron poisson_neuron = Neuron ( parameters = \"\"\" %(params)s \"\"\" % { 'params' : parameters if parameters else '' }, equations = \"\"\" rates = %(rates)s p = Uniform(0.0, 1.0) * 1000.0 / dt _sum_exc = 0.0 \"\"\" % { 'rates' : rates }, spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) elif isinstance ( rates , np . ndarray ): poisson_neuron = Neuron ( parameters = \"\"\" rates = 10.0 \"\"\" , equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) else : poisson_neuron = Neuron ( parameters = \"\"\" rates = %(rates)s \"\"\" % { 'rates' : rates }, equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = poisson_neuron , name = name , copied = copied ) if isinstance ( rates , np . ndarray ): self . rates = rates def _copy ( self ): \"Returns a copy of the population when creating networks.\" return PoissonPopulation ( self . geometry , name = self . name , rates = self . rates_init , target = self . target , parameters = self . parameters , refractory = self . refractory_init , copied = True ) def _generate_st ( self ): \"\"\" Generate single thread code. We don't need any separate code snippets. All is done during the normal code generation path. \"\"\" pass def _generate_omp ( self ): \"\"\" Generate openMP code. We don't need any separate code snippets. All is done during the normal code generation path. \"\"\" pass def _generate_cuda ( self ): \"\"\" Generate CUDA code. We don't need any separate code snippets. All is done during the normal code generation path. \"\"\" pass __init__ ( geometry , name = None , rates = None , target = None , parameters = None , refractory = None , copied = False ) # Parameters: Name Type Description Default geometry population geometry as tuple. required name unique name of the population (optional). None rates mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string). None target the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\"). None parameters additional parameters which can be used in the rates equation. None refractory refractory period in ms. None Source code in ANNarchy/core/SpecificPopulation.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def __init__ ( self , geometry , name = None , rates = None , target = None , parameters = None , refractory = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. :param name: unique name of the population (optional). :param rates: mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string). :param target: the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\"). :param parameters: additional parameters which can be used in the *rates* equation. :param refractory: refractory period in ms. \"\"\" if rates is None and target is None : Global . _error ( 'A PoissonPopulation must define either rates or target.' ) self . target = target self . parameters = parameters self . refractory_init = refractory self . rates_init = rates if target is not None : # hybrid population # Create the neuron poisson_neuron = Neuron ( parameters = \"\"\" %(params)s \"\"\" % { 'params' : parameters if parameters else '' }, equations = \"\"\" rates = sum( %(target)s ) p = Uniform(0.0, 1.0) * 1000.0 / dt _sum_ %(target)s = 0.0 \"\"\" % { 'target' : target }, spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Hybrid\" , description = \"Hybrid spiking neuron emitting spikes according to a Poisson distribution at a frequency determined by the weighted sum of inputs.\" ) elif isinstance ( rates , str ): # Create the neuron poisson_neuron = Neuron ( parameters = \"\"\" %(params)s \"\"\" % { 'params' : parameters if parameters else '' }, equations = \"\"\" rates = %(rates)s p = Uniform(0.0, 1.0) * 1000.0 / dt _sum_exc = 0.0 \"\"\" % { 'rates' : rates }, spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) elif isinstance ( rates , np . ndarray ): poisson_neuron = Neuron ( parameters = \"\"\" rates = 10.0 \"\"\" , equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) else : poisson_neuron = Neuron ( parameters = \"\"\" rates = %(rates)s \"\"\" % { 'rates' : rates }, equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = poisson_neuron , name = name , copied = copied ) if isinstance ( rates , np . ndarray ): self . rates = rates SpikeSourceArray # Bases: SpecificPopulation Spike source generating spikes at the times given in the spike_times array. Depending on the initial array provided, the population will have one or several neurons, but the geometry can only be one-dimensional. You can later modify the spike_times attribute of the population, but it must have the same number of neurons as the initial one. The spike times are by default relative to the start of a simulation ( ANNarchy.get_time() is 0.0). If you call the reset() method of a SpikeSourceArray , this will set the spike times relative to the current time. You can then repeat a stimulation many times. # 2 neurons firing at 100Hz with a 1 ms delay times = [ [ 10 , 20 , 30 , 40 ], [ 11 , 21 , 31 , 41 ] ] inp = SpikeSourceArray ( spike_times = times ) compile () # Spikes at 10/11, 20/21, etc simulate ( 50 ) # Reset the internal time of the SpikeSourceArray inp . reset () # Spikes at 60/61, 70/71, etc simulate ( 50 ) Source code in ANNarchy/core/SpecificPopulation.py 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 class SpikeSourceArray ( SpecificPopulation ): \"\"\" Spike source generating spikes at the times given in the spike_times array. Depending on the initial array provided, the population will have one or several neurons, but the geometry can only be one-dimensional. You can later modify the spike_times attribute of the population, but it must have the same number of neurons as the initial one. The spike times are by default relative to the start of a simulation (``ANNarchy.get_time()`` is 0.0). If you call the ``reset()`` method of a ``SpikeSourceArray``, this will set the spike times relative to the current time. You can then repeat a stimulation many times. ```python # 2 neurons firing at 100Hz with a 1 ms delay times = [ [ 10, 20, 30, 40], [ 11, 21, 31, 41] ] inp = SpikeSourceArray(spike_times=times) compile() # Spikes at 10/11, 20/21, etc simulate(50) # Reset the internal time of the SpikeSourceArray inp.reset() # Spikes at 60/61, 70/71, etc simulate(50) ``` \"\"\" def __init__ ( self , spike_times , name = None , copied = False ): \"\"\" :param spike_times: a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt. :param name: optional name for the population. \"\"\" if not isinstance ( spike_times , list ): Global . _error ( 'In a SpikeSourceArray, spike_times must be a Python list.' ) if isinstance ( spike_times [ 0 ], list ): # several neurons nb_neurons = len ( spike_times ) else : # a single Neuron nb_neurons = 1 spike_times = [ spike_times ] # Create a fake neuron just to be sure the description has the correct parameters neuron = Neuron ( parameters = \"\" , equations = \"\" , spike = \" t == 0\" , reset = \"\" , name = \"Spike source\" , description = \"Spike source array.\" ) SpecificPopulation . __init__ ( self , geometry = nb_neurons , neuron = neuron , name = name , copied = copied ) self . init [ 'spike_times' ] = spike_times def _copy ( self ): \"Returns a copy of the population when creating networks.\" return SpikeSourceArray ( self . init [ 'spike_times' ], self . name , copied = True ) def _sort_spikes ( self , spike_times ): \"Sort, unify the spikes and transform them into steps.\" return [ sorted ( list ( set ([ round ( t / Global . config [ 'dt' ]) for t in neur_times ]))) for neur_times in spike_times ] def _generate_st ( self ): \"\"\" Code generation for single-thread. \"\"\" self . _generate_omp () def _generate_omp ( self ): \"\"\" Code generation for openMP paradigm. \"\"\" # Add possible targets for target in self . targets : tpl = { 'name' : 'g_ %(target)s ' % { 'target' : target }, 'locality' : 'local' , 'eq' : '' , 'bounds' : {}, 'flags' : [], 'ctype' : Global . config [ 'precision' ], 'init' : 0.0 , 'transformed_eq' : '' , 'pre_loop' : {}, 'cpp' : '' , 'switch' : '' , 'untouched' : {}, 'method' : 'exponential' , 'dependencies' : [] } self . neuron_type . description [ 'variables' ] . append ( tpl ) self . neuron_type . description [ 'local' ] . append ( 'g_' + target ) self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameter spike_times // std::vector< %(float_prec)s > r ; std::vector< std::vector< long int > > spike_times ; std::vector< long int > next_spike ; std::vector< int > idx_next_spike; long int _t; // Recompute the spike times void recompute_spike_times(){ std::fill(next_spike.begin(), next_spike.end(), -10000); std::fill(idx_next_spike.begin(), idx_next_spike.end(), 0); for(int i=0; i< size; i++){ if(!spike_times[i].empty()){ int idx = 0; // Find the first spike time which is not in the past while(spike_times[i][idx] < _t){ idx++; } // Set the next spike if(idx < spike_times[i].size()) next_spike[i] = spike_times[i][idx]; else next_spike[i] = -10000; } } } \"\"\" % { 'float_prec' : Global . config [ 'precision' ] } #self._specific_template['access_parameters_variables'] = \"\" self . _specific_template [ 'init_additional' ] = \"\"\" _t = 0; next_spike = std::vector<long int>(size, -10000); idx_next_spike = std::vector<int>(size, 0); this->recompute_spike_times(); \"\"\" self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; this->recompute_spike_times(); \"\"\" if Global . config [ \"num_threads\" ] == 1 : self . _specific_template [ 'update_variables' ] = \"\"\" if(_active){ spiked.clear(); for(int i = 0; i < size; i++){ // Emit spike if( _t == next_spike[i] ){ last_spike[i] = _t; idx_next_spike[i]++ ; if(idx_next_spike[i] < spike_times[i].size()){ next_spike[i] = spike_times[i][idx_next_spike[i]]; } spiked.push_back(i); } } _t++; } \"\"\" else : self . _specific_template [ 'update_variables' ] = \"\"\" if(_active){ #pragma omp single { spiked.clear(); } #pragma omp for for(int i = 0; i < size; i++){ // Emit spike if( _t == next_spike[i] ){ last_spike[i] = _t; idx_next_spike[i]++ ; if(idx_next_spike[i] < spike_times[i].size()){ next_spike[i] = spike_times[i][idx_next_spike[i]]; } #pragma omp critical spiked.push_back(i); } } #pragma omp single { _t++; } } \"\"\" self . _specific_template [ 'test_spike_cond' ] = \"\" self . _specific_template [ 'export_additional' ] = \"\"\" vector[vector[long]] spike_times void recompute_spike_times() \"\"\" self . _specific_template [ 'wrapper_args' ] = \"size, times, delay\" self . _specific_template [ 'wrapper_init' ] = \"\"\" pop %(id)s .spike_times = times pop %(id)s .set_size(size) pop %(id)s .set_max_delay(delay)\"\"\" % { 'id' : self . id } self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Local parameter spike_times cpdef get_spike_times(self): return pop %(id)s .spike_times cpdef set_spike_times(self, value): pop %(id)s .spike_times = value pop %(id)s .recompute_spike_times() \"\"\" % { 'id' : self . id } def _generate_cuda ( self ): \"\"\" Code generation for the CUDA paradigm. As the spike time generation is not a very compute intensive step but requires dynamic data structures, we don't implement it on the CUDA devices for now. Consequently, we use the CPU side implementation and transfer after computation the results to the GPU. \"\"\" self . _generate_st () # attach transfer of spiked array to gpu # IMPORTANT: the outside transfer is necessary. # Otherwise, previous spike counts will be not reseted. self . _specific_template [ 'update_variables' ] += \"\"\" if ( _active ) { // Update Spike Count on GPU spike_count = spiked.size(); cudaMemcpy( gpu_spike_count, &spike_count, sizeof(unsigned int), cudaMemcpyHostToDevice); // Transfer generated spikes to GPU if( spike_count > 0 ) { cudaMemcpy( gpu_spiked, spiked.data(), spike_count * sizeof(int), cudaMemcpyHostToDevice); } } \"\"\" self . _specific_template [ 'update_variable_body' ] = \"\" self . _specific_template [ 'update_variable_header' ] = \"\" self . _specific_template [ 'update_variable_call' ] = \"\"\" // host side update of neurons pop %(id)s .update(); \"\"\" % { 'id' : self . id } def _instantiate ( self , module ): # Create the Cython instance self . cyInstance = getattr ( module , self . class_name + '_wrapper' )( self . size , self . init [ 'spike_times' ], self . max_delay ) def __setattr__ ( self , name , value ): if name == 'spike_times' : if not isinstance ( value [ 0 ], list ): # several neurons value = [ value ] if not len ( value ) == self . size : Global . _error ( 'SpikeSourceArray: the size of the spike_times attribute must match the number of neurons in the population.' ) self . init [ 'spike_times' ] = value # when reset is called if self . initialized : self . cyInstance . set_spike_times ( self . _sort_spikes ( value )) else : Population . __setattr__ ( self , name , value ) def __getattr__ ( self , name ): if name == 'spike_times' : if self . initialized : return [ [ Global . config [ 'dt' ] * time for time in neur ] for neur in self . cyInstance . get_spike_times ()] else : return self . init [ 'spike_times' ] else : return Population . __getattribute__ ( self , name ) __init__ ( spike_times , name = None , copied = False ) # Parameters: Name Type Description Default spike_times a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt. required name optional name for the population. None Source code in ANNarchy/core/SpecificPopulation.py 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 def __init__ ( self , spike_times , name = None , copied = False ): \"\"\" :param spike_times: a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt. :param name: optional name for the population. \"\"\" if not isinstance ( spike_times , list ): Global . _error ( 'In a SpikeSourceArray, spike_times must be a Python list.' ) if isinstance ( spike_times [ 0 ], list ): # several neurons nb_neurons = len ( spike_times ) else : # a single Neuron nb_neurons = 1 spike_times = [ spike_times ] # Create a fake neuron just to be sure the description has the correct parameters neuron = Neuron ( parameters = \"\" , equations = \"\" , spike = \" t == 0\" , reset = \"\" , name = \"Spike source\" , description = \"Spike source array.\" ) SpecificPopulation . __init__ ( self , geometry = nb_neurons , neuron = neuron , name = name , copied = copied ) self . init [ 'spike_times' ] = spike_times TimedArray # Bases: SpecificPopulation Data structure holding sequential inputs for a rate-coded network. The input values are stored in the (recordable) attribute r , without any further processing. You will need to connect this population to another one using the connect_one_to_one() method. By default, the firing rate of this population will iterate over the different values step by step: inputs = np . array ( [ [ 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 ] ] ) inp = TimedArray ( rates = inputs ) pop = Population ( 10 , ... ) proj = Projection ( inp , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) compile () simulate ( 10. ) This creates a population of 10 neurons whose activity will change during the first 10*dt milliseconds of the simulation. After that delay, the last input will be kept (i.e. 1 for the last neuron). If you want the TimedArray to \"loop\" over the different input vectors, you can specify a period for the inputs: inp = TimedArray ( rates = inputs , period = 10. ) If the period is smaller than the length of the rates, the last inputs will not be set. If you do not want the inputs to be set at every step, but every 10 ms for example, youcan use the schedule argument: inp = TimedArray ( rates = inputs , schedule = 10. ) The input [1, 0, 0,...] will stay for 10 ms, then[0, 1, 0, ...] for the next 10 ms, etc... If you need a less regular schedule, you can specify it as a list of times: inp = TimedArray ( rates = inputs , schedule = [ 10. , 20. , 50. , 60. , 100. , 110. ]) The first input is set at t = 10 ms (r = 0.0 in the first 10 ms), the second at t = 20 ms, the third at t = 50 ms, etc. If you specify less times than in the array of rates, the last ones will be ignored. Scheduling can be combined with periodic cycling. Note that you can use the reset() method to manually reinitialize the TimedArray, times becoming relative to that call: simulate ( 100. ) # ten inputs are shown with a schedule of 10 ms inp . reset () simulate ( 100. ) # the same ten inputs are presented again. Source code in ANNarchy/core/SpecificPopulation.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 class TimedArray ( SpecificPopulation ): \"\"\" Data structure holding sequential inputs for a rate-coded network. The input values are stored in the (recordable) attribute `r`, without any further processing. You will need to connect this population to another one using the ``connect_one_to_one()`` method. By default, the firing rate of this population will iterate over the different values step by step: ```python inputs = np.array( [ [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1] ] ) inp = TimedArray(rates=inputs) pop = Population(10, ...) proj = Projection(inp, pop, 'exc') proj.connect_one_to_one(1.0) compile() simulate(10.) ``` This creates a population of 10 neurons whose activity will change during the first 10*dt milliseconds of the simulation. After that delay, the last input will be kept (i.e. 1 for the last neuron). If you want the TimedArray to \"loop\" over the different input vectors, you can specify a period for the inputs: ```python inp = TimedArray(rates=inputs, period=10.) ``` If the period is smaller than the length of the rates, the last inputs will not be set. If you do not want the inputs to be set at every step, but every 10 ms for example, youcan use the ``schedule`` argument: ```python inp = TimedArray(rates=inputs, schedule=10.) ``` The input [1, 0, 0,...] will stay for 10 ms, then[0, 1, 0, ...] for the next 10 ms, etc... If you need a less regular schedule, you can specify it as a list of times: ```python inp = TimedArray(rates=inputs, schedule=[10., 20., 50., 60., 100., 110.]) ``` The first input is set at t = 10 ms (r = 0.0 in the first 10 ms), the second at t = 20 ms, the third at t = 50 ms, etc. If you specify less times than in the array of rates, the last ones will be ignored. Scheduling can be combined with periodic cycling. Note that you can use the ``reset()`` method to manually reinitialize the TimedArray, times becoming relative to that call: ```python simulate(100.) # ten inputs are shown with a schedule of 10 ms inp.reset() simulate(100.) # the same ten inputs are presented again. ``` \"\"\" def __init__ ( self , rates , schedule = 0. , period = - 1. , name = None , copied = False ): \"\"\" :param rates: array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population. :param schedule: either a single value or a list of time points where inputs should be set. Default: every timestep. :param period: time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.). \"\"\" neuron = Neuron ( parameters = \"\" , equations = \" r = 0.0\" , name = \"Timed Array\" , description = \"Timed array source.\" ) # Geometry of the population geometry = rates . shape [ 1 :] # Check the schedule if isinstance ( schedule , ( int , float )): if float ( schedule ) <= 0.0 : schedule = Global . config [ 'dt' ] schedule = [ float ( schedule * i ) for i in range ( rates . shape [ 0 ])] if len ( schedule ) > rates . shape [ 0 ]: Global . _error ( 'TimedArray: the length of the schedule parameter cannot exceed the first dimension of the rates parameter.' ) if len ( schedule ) < rates . shape [ 0 ]: Global . _warning ( 'TimedArray: the length of the schedule parameter is smaller than the first dimension of the rates parameter (more data than time points). Make sure it is what you expect.' ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = neuron , name = name , copied = copied ) self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'period' ] = period def _copy ( self ): \"Returns a copy of the population when creating networks.\" return TimedArray ( self . init [ 'rates' ] , self . init [ 'schedule' ], self . init [ 'period' ], self . name , copied = True ) def _generate_st ( self ): \"\"\" adjust code templates for the specific population for single thread and openMP. \"\"\" self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameters of a TimedArray std::vector< int > _schedule; // List of times where new inputs should be set std::vector< std::vector< %(float_prec)s > > _buffer; // buffer holding the data int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameters of a TimedArray void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_buffer(std::vector< std::vector< %(float_prec)s > > buffer) { _buffer = buffer; r = _buffer[0]; } std::vector< std::vector< %(float_prec)s > > get_buffer() { return _buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // Initialize counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters of a TimedArray void set_schedule(vector[int]) vector[int] get_schedule() void set_buffer(vector[vector[ %(float_prec)s ]]) vector[vector[ %(float_prec)s ]] get_buffer() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; _block = 0; r.clear(); r = std::vector< %(float_prec)s >(size, 0.0); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters of a TimedArray cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_rates( self, buffer ): pop %(id)s .set_buffer( buffer ) cpdef np.ndarray get_rates( self ): return np.array(pop %(id)s .get_buffer( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_period(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id } self . _specific_template [ 'update_variables' ] = \"\"\" if(_active){ //std::cout << _t << \" \" << _block<< \" \" << _schedule[_block] << std::endl; // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data r = _buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if (_block == _schedule.size()){ _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if(_period > -1 && (_t == _period-1)){ // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } \"\"\" self . _specific_template [ 'size_in_bytes' ] = \"\"\" // schedule size_in_bytes += _schedule.capacity() * sizeof(int); // buffer size_in_bytes += _buffer.capacity() * sizeof(std::vector< %(float_prec)s >); for( auto it = _buffer.begin(); it != _buffer.end(); it++ ) size_in_bytes += it->capacity() * sizeof( %(float_prec)s ); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} def _generate_omp ( self ): \"\"\" adjust code templates for the specific population for single thread and openMP. \"\"\" self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameters of a TimedArray std::vector< int > _schedule; // List of times where new inputs should be set std::vector< std::vector< %(float_prec)s > > _buffer; // buffer holding the data int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameters of a TimedArray void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_buffer(std::vector< std::vector< %(float_prec)s > > buffer) { _buffer = buffer; r = _buffer[0]; } std::vector< std::vector< %(float_prec)s > > get_buffer() { return _buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // Initialize counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters of a TimedArray void set_schedule(vector[int]) vector[int] get_schedule() void set_buffer(vector[vector[ %(float_prec)s ]]) vector[vector[ %(float_prec)s ]] get_buffer() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; _block = 0; r.clear(); r = std::vector< %(float_prec)s >(size, 0.0); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters of a TimedArray cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_rates( self, buffer ): pop %(id)s .set_buffer( buffer ) cpdef np.ndarray get_rates( self ): return np.array(pop %(id)s .get_buffer( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_period(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id } self . _specific_template [ 'update_variables' ] = \"\"\" if(_active){ #pragma omp single { // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data r = _buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if (_block == _schedule.size()){ _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if(_period > -1 && (_t == _period-1)){ // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } } \"\"\" self . _specific_template [ 'size_in_bytes' ] = \"\"\" // schedule size_in_bytes += _schedule.capacity() * sizeof(int); // buffer size_in_bytes += _buffer.capacity() * sizeof(std::vector< %(float_prec)s >); for( auto it = _buffer.begin(); it != _buffer.end(); it++ ) size_in_bytes += it->capacity() * sizeof( %(float_prec)s ); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} def _generate_cuda ( self ): \"\"\" adjust code templates for the specific population for single thread and CUDA. \"\"\" # HD (18. Nov 2016) # I suppress the code generation for allocating the variable r on gpu, as # well as memory transfer codes. This is only possible as no other variables # allowed in TimedArray. self . _specific_template [ 'init_parameters_variables' ] = \"\" self . _specific_template [ 'host_device_transfer' ] = \"\" self . _specific_template [ 'device_host_transfer' ] = \"\" # # Code for handling the buffer and schedule parameters self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameter timed array std::vector< int > _schedule; std::vector< %(float_prec)s * > gpu_buffer; int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameter timed array void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_buffer(std::vector< std::vector< %(float_prec)s > > buffer) { if ( gpu_buffer.empty() ) { gpu_buffer = std::vector< %(float_prec)s * >(buffer.size(), nullptr); // allocate gpu arrays for(int i = 0; i < buffer.size(); i++) { cudaMalloc((void**)&gpu_buffer[i], buffer[i].size()*sizeof( %(float_prec)s )); } } auto host_it = buffer.begin(); auto dev_it = gpu_buffer.begin(); for (; host_it < buffer.end(); host_it++, dev_it++) { cudaMemcpy( *dev_it, host_it->data(), host_it->size()*sizeof( %(float_prec)s ), cudaMemcpyHostToDevice); } gpu_r = gpu_buffer[0]; } std::vector< std::vector< %(float_prec)s > > get_buffer() { std::vector< std::vector< %(float_prec)s > > buffer = std::vector< std::vector< %(float_prec)s > >( gpu_buffer.size(), std::vector< %(float_prec)s >(size,0.0) ); auto host_it = buffer.begin(); auto dev_it = gpu_buffer.begin(); for (; host_it < buffer.end(); host_it++, dev_it++) { cudaMemcpy( host_it->data(), *dev_it, size*sizeof( %(float_prec)s ), cudaMemcpyDeviceToHost ); } return buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'reset_additional' ] = \"\"\" // counters _t = 0; _block = 0; gpu_r = gpu_buffer[0]; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters timed array void set_schedule(vector[int]) vector[int] get_schedule() void set_buffer(vector[vector[ %(float_prec)s ]]) vector[vector[ %(float_prec)s ]] get_buffer() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters timed array cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_rates( self, buffer ): pop %(id)s .set_buffer( buffer ) cpdef np.ndarray get_rates( self ): return np.array(pop %(id)s .get_buffer( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_periodic(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'update_variables' ] = \"\"\" if(_active) { // std::cout << _t << \" \" << _block<< \" \" << _schedule[_block] << std::endl; // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data gpu_r = gpu_buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if ( _block == _schedule.size() ) { _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if( (_period > -1) && (_t == _period-1) ) { // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } \"\"\" self . _specific_template [ 'update_variable_body' ] = \"\" self . _specific_template [ 'update_variable_header' ] = \"\" self . _specific_template [ 'update_variable_call' ] = \"\"\" // host side update of neurons pop %(id)s .update(); \"\"\" % { 'id' : self . id } self . _specific_template [ 'size_in_bytes' ] = \"//TODO: \" def _instantiate ( self , module ): # Create the Cython instance self . cyInstance = getattr ( module , self . class_name + '_wrapper' )( self . size , self . max_delay ) def __setattr__ ( self , name , value ): if name == 'schedule' : if self . initialized : self . cyInstance . set_schedule ( np . array ( value ) / Global . config [ 'dt' ] ) else : self . init [ 'schedule' ] = value elif name == 'rates' : if self . initialized : if len ( value . shape ) > 2 : # we need to flatten the provided data flat_values = value . reshape ( ( value . shape [ 0 ], self . size ) ) self . cyInstance . set_rates ( flat_values ) else : self . cyInstance . set_rates ( value ) else : self . init [ 'rates' ] = value elif name == \"period\" : if self . initialized : self . cyInstance . set_period ( int ( value / Global . config [ 'dt' ])) else : self . init [ 'period' ] = value else : Population . __setattr__ ( self , name , value ) def __getattr__ ( self , name ): if name == 'schedule' : if self . initialized : return Global . config [ 'dt' ] * self . cyInstance . get_schedule () else : return self . init [ 'schedule' ] elif name == 'rates' : if self . initialized : if len ( self . geometry ) > 1 : # unflatten the data flat_values = self . cyInstance . get_rates () values = np . zeros ( tuple ( [ len ( self . schedule )] + list ( self . geometry ) ) ) for x in range ( len ( self . schedule )): values [ x ] = np . reshape ( flat_values [ x ], self . geometry ) return values else : return self . cyInstance . get_rates () else : return self . init [ 'rates' ] elif name == 'period' : if self . initialized : return self . cyInstance . get_period () * Global . config [ 'dt' ] else : return self . init [ 'period' ] else : return Population . __getattribute__ ( self , name ) __init__ ( rates , schedule = 0.0 , period =- 1.0 , name = None , copied = False ) # Parameters: Name Type Description Default rates array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population. required schedule either a single value or a list of time points where inputs should be set. Default: every timestep. 0.0 period time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.). -1.0 Source code in ANNarchy/core/SpecificPopulation.py 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def __init__ ( self , rates , schedule = 0. , period = - 1. , name = None , copied = False ): \"\"\" :param rates: array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population. :param schedule: either a single value or a list of time points where inputs should be set. Default: every timestep. :param period: time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.). \"\"\" neuron = Neuron ( parameters = \"\" , equations = \" r = 0.0\" , name = \"Timed Array\" , description = \"Timed array source.\" ) # Geometry of the population geometry = rates . shape [ 1 :] # Check the schedule if isinstance ( schedule , ( int , float )): if float ( schedule ) <= 0.0 : schedule = Global . config [ 'dt' ] schedule = [ float ( schedule * i ) for i in range ( rates . shape [ 0 ])] if len ( schedule ) > rates . shape [ 0 ]: Global . _error ( 'TimedArray: the length of the schedule parameter cannot exceed the first dimension of the rates parameter.' ) if len ( schedule ) < rates . shape [ 0 ]: Global . _warning ( 'TimedArray: the length of the schedule parameter is smaller than the first dimension of the rates parameter (more data than time points). Make sure it is what you expect.' ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = neuron , name = name , copied = copied ) self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'period' ] = period HomogeneousCorrelatedSpikeTrains # Bases: SpecificPopulation Population of spiking neurons following a homogeneous distribution with correlated spike trains. The method describing the generation of homogeneous correlated spike trains is described in: Brette, R. (2009). Generation of correlated spike trains. Neural Computation 21(1). http://romainbrette.fr/WordPress3/wp-content/uploads/2014/06/Brette2008NC.pdf The implementation is based on the one provided by Brian http://briansimulator.org . To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation: \\[\\frac{dx}{dt} = \\frac{(\\mu - x)}{\\tau} + \\sigma \\, \\frac{\\xi}{\\sqrt{\\tau}}\\] where \\(\\xi\\) is a random variable. In short, \\(x\\) will randomly vary around mu over time, with an amplitude determined by sigma and a speed determined by tau. This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process. To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates , the desired correlation strength corr and the time constant tau . See Brette's paper for details. In short, you should only define the parameters rates , corr and tau , and let the class compute mu and sigma for you. Changing rates , corr or tau after initialization automatically recomputes mu and sigma. Example: from ANNarchy import * setup ( dt = 0.1 ) pop_corr = HomogeneousCorrelatedSpikeTrains ( 200 , rates = 10. , corr = 0.3 , tau = 10. ) compile () simulate ( 1000. ) pop_corr . rates = 30. simulate ( 1000. ) Alternatively, a schedule can be provided to change automatically the value of rates and corr (but not tau ) at the required times (as in TimedArray or TimedPoissonPopulation): from ANNarchy import * setup ( dt = 0.1 ) pop_corr = HomogeneousCorrelatedSpikeTrains ( geometry = 200 , rates = [ 10. , 30. ], corr = [ 0.3 , 0.5 ], tau = 10. , schedule = [ 0. , 1000. ] ) compile () simulate ( 2000. ) Even when using a schedule, corr accepts a single constant value. The first value of schedule must be 0. period specifies when the schedule \"loops\" back to its initial value. Source code in ANNarchy/core/SpecificPopulation.py 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 class HomogeneousCorrelatedSpikeTrains ( SpecificPopulation ): \"\"\" Population of spiking neurons following a homogeneous distribution with correlated spike trains. The method describing the generation of homogeneous correlated spike trains is described in: > Brette, R. (2009). Generation of correlated spike trains. Neural Computation 21(1). <http://romainbrette.fr/WordPress3/wp-content/uploads/2014/06/Brette2008NC.pdf> The implementation is based on the one provided by Brian <http://briansimulator.org>. To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation: $$\\\\frac{dx}{dt} = \\\\frac{(\\\\mu - x)}{\\\\tau} + \\\\sigma \\\\, \\\\frac{\\\\xi}{\\\\sqrt{\\\\tau}}$$ where $\\\\xi$ is a random variable. In short, $x$ will randomly vary around mu over time, with an amplitude determined by sigma and a speed determined by tau. This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process. To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate **rates**, the desired correlation strength **corr** and the time constant **tau**. See Brette's paper for details. In short, you should only define the parameters ``rates``, ``corr`` and ``tau``, and let the class compute mu and sigma for you. Changing ``rates``, ``corr`` or ``tau`` after initialization automatically recomputes mu and sigma. Example: ```python from ANNarchy import * setup(dt=0.1) pop_corr = HomogeneousCorrelatedSpikeTrains(200, rates=10., corr=0.3, tau=10.) compile() simulate(1000.) pop_corr.rates=30. simulate(1000.) ``` Alternatively, a schedule can be provided to change automatically the value of `rates` and ``corr`` (but not ``tau``) at the required times (as in TimedArray or TimedPoissonPopulation): ```python from ANNarchy import * setup(dt=0.1) pop_corr = HomogeneousCorrelatedSpikeTrains( geometry=200, rates= [10., 30.], corr=[0.3, 0.5], tau=10., schedule=[0., 1000.] ) compile() simulate(2000.) ``` Even when using a schedule, ``corr`` accepts a single constant value. The first value of ``schedule`` must be 0. ``period`` specifies when the schedule \"loops\" back to its initial value. \"\"\" def __init__ ( self , geometry , rates , corr , tau , schedule = None , period =- 1. , name = None , refractory = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. :param rates: rate in Hz of the population (must be a positive float or a list) :param corr: total correlation strength (float in [0, 1], or a list) :param tau: correlation time constant in ms. :param schedule: list of times where new values of ``rates``and ``corr``will be used to computre mu and sigma. :param period: time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.) :param name: unique name of the population (optional). :param refractory: refractory period in ms (careful: may break the correlation) \"\"\" if schedule is not None : self . _has_schedule = True # Rates if not isinstance ( rates , ( list , np . ndarray )): Global . _error ( \"TimedHomogeneousCorrelatedSpikeTrains: the rates argument must be a list or a numpy array.\" ) rates = np . array ( rates ) # Schedule schedule = np . array ( schedule ) nb_schedules = rates . shape [ 0 ] if nb_schedules != schedule . size : Global . _error ( \"TimedHomogeneousCorrelatedSpikeTrains: the length of rates must be the same length as for schedule.\" ) # corr corr = np . array ( corr ) if corr . size == 1 : corr = np . full ( nb_schedules , corr ) else : self . _has_schedule = False rates = np . array ([ float ( rates )]) schedule = np . array ([ 0.0 ]) corr = np . array ([ corr ]) # Store refractory self . refractory_init = refractory # Correction of mu and sigma mu_list , sigma_list = self . _correction ( rates , corr , tau ) self . rates = rates self . corr = corr self . tau = tau # Create the neuron corr_neuron = Neuron ( parameters = \"\"\" tau = %(tau)s : population mu = %(mu)s : population sigma = %(sigma)s : population \"\"\" % { 'tau' : tau , 'mu' : mu_list [ 0 ], 'sigma' : sigma_list [ 0 ]}, equations = \"\"\" x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) : population, init= %(mu)s p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" % { 'mu' : mu_list [ 0 ]}, spike = \"p < x\" , refractory = refractory , name = \"HomogeneousCorrelated\" , description = \"Homogeneous correlated spike trains.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = corr_neuron , name = name , copied = copied ) # Initial values self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'corr' ] = corr self . init [ 'tau' ] = tau self . init [ 'period' ] = period if self . _has_schedule : self . init [ 'mu' ] = mu_list self . init [ 'sigma' ] = sigma_list else : self . init [ 'mu' ] = mu_list [ 0 ] self . init [ 'sigma' ] = sigma_list [ 0 ] def _copy ( self ): \"Returns a copy of the population when creating networks.\" return HomogeneousCorrelatedSpikeTrains ( geometry = self . geometry , rates = self . init [ 'rates' ], corr = self . init [ 'corr' ], tau = self . init [ 'tau' ], schedule = self . init [ 'schedule' ], period = self . init [ 'period' ], name = self . name , refractory = self . refractory_init , copied = True ) def _correction ( self , rates , corr , tau ): # Correction of mu and sigma mu_list = [] sigma_list = [] for i in range ( len ( rates )): if isinstance ( corr , list ): c = corr [ i ] else : c = float ( corr ) mu , sigma = _rectify ( rates [ i ], c , tau ) mu_list . append ( mu ) sigma_list . append ( sigma ) return mu_list , sigma_list def _generate_st ( self ): \"\"\" adjust code templates for the specific population for single thread. \"\"\" self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameters of a HomogeneousCorrelatedSpikeTrains std::vector< int > _schedule; // List of times where new inputs should be set std::vector< %(float_prec)s > _mu; // buffer holding the data std::vector< %(float_prec)s > _sigma; // buffer holding the data int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameters of a HomogeneousCorrelatedSpikeTrains void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_mu_list(std::vector< %(float_prec)s > buffer) { _mu = buffer; mu = _mu[0]; } std::vector< %(float_prec)s > get_mu_list() { return _mu; } void set_sigma_list(std::vector< %(float_prec)s > buffer) { _sigma = buffer; sigma = _sigma[0]; } std::vector< %(float_prec)s > get_sigma_list() { return _sigma; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // Initialize counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters of a HomogeneousCorrelatedSpikeTrains void set_schedule(vector[int]) vector[int] get_schedule() void set_mu_list(vector[ %(float_prec)s ]) vector[ %(float_prec)s ] get_mu_list() void set_sigma_list(vector[ %(float_prec)s ]) vector[ %(float_prec)s ] get_sigma_list() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; _block = 0; r.clear(); r = std::vector< %(float_prec)s >(size, 0.0); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters of a HomogeneousCorrelatedSpikeTrains cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_mu_list( self, buffer ): pop %(id)s .set_mu_list( buffer ) cpdef np.ndarray get_mu_list( self ): return np.array(pop %(id)s .get_mu_list( )) cpdef set_sigma_list( self, buffer ): pop %(id)s .set_sigma_list( buffer ) cpdef np.ndarray get_sigma_list( self ): return np.array(pop %(id)s .get_sigma_list( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_period(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id } scheduling_block = \"\"\" if(_active){ // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data mu = _mu[_block]; sigma = _sigma[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if (_block == _schedule.size()){ _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if(_period > -1 && (_t == _period-1)){ // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } \"\"\" update_block = \"\"\" if( _active ) { spiked.clear(); // x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) x += dt*(mu - x)/tau + rand_0*sigma*sqrt(dt/tau); %(float_prec)s _step = 1000.0/dt; #pragma omp simd for(int i = 0; i < size; i++){ // p = Uniform(0.0, 1.0) * 1000.0 / dt p[i] = _step*rand_1[i]; } } // active \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} if self . _has_schedule : self . _specific_template [ 'update_variables' ] = scheduling_block + update_block else : self . _specific_template [ 'update_variables' ] = update_block self . _specific_template [ 'size_in_bytes' ] = \"\"\" // schedule size_in_bytes += _schedule.capacity() * sizeof(int); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} def _generate_omp ( self ): \"\"\" adjust code templates for the specific population for openMP. \"\"\" self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameters of a HomogeneousCorrelatedSpikeTrains std::vector< int > _schedule; // List of times where new inputs should be set std::vector< %(float_prec)s > _mu; // buffer holding the data std::vector< %(float_prec)s > _sigma; // buffer holding the data int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameters of a HomogeneousCorrelatedSpikeTrains void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_mu_list(std::vector< %(float_prec)s > buffer) { _mu = buffer; mu = _mu[0]; } std::vector< %(float_prec)s > get_mu_list() { return _mu; } void set_sigma_list(std::vector< %(float_prec)s > buffer) { _sigma = buffer; sigma = _sigma[0]; } std::vector< %(float_prec)s > get_sigma_list() { return _sigma; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // Initialize counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters of a HomogeneousCorrelatedSpikeTrains void set_schedule(vector[int]) vector[int] get_schedule() void set_mu_list(vector[ %(float_prec)s ]) vector[ %(float_prec)s ] get_mu_list() void set_sigma_list(vector[ %(float_prec)s ]) vector[ %(float_prec)s ] get_sigma_list() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; _block = 0; r.clear(); r = std::vector< %(float_prec)s >(size, 0.0); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters of a HomogeneousCorrelatedSpikeTrains cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_mu_list( self, buffer ): pop %(id)s .set_mu_list( buffer ) cpdef np.ndarray get_mu_list( self ): return np.array(pop %(id)s .get_mu_list( )) cpdef set_sigma_list( self, buffer ): pop %(id)s .set_sigma_list( buffer ) cpdef np.ndarray get_sigma_list( self ): return np.array(pop %(id)s .get_sigma_list( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_period(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id } scheduling_block = \"\"\" if(_active){ // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data mu = _mu[_block]; sigma = _sigma[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if (_block == _schedule.size()){ _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if(_period > -1 && (_t == _period-1)){ // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } \"\"\" update_block = \"\"\" if( _active ) { #pragma omp single { spiked.clear(); // x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) x += dt*(mu - x)/tau + rand_0*sigma*sqrt(dt/tau); %(float_prec)s _step = 1000.0/dt; #pragma omp simd for(int i = 0; i < size; i++){ // p = Uniform(0.0, 1.0) * 1000.0 / dt p[i] = _step*rand_1[i]; } } } // active \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} if self . _has_schedule : self . _specific_template [ 'update_variables' ] = scheduling_block + update_block else : self . _specific_template [ 'update_variables' ] = update_block self . _specific_template [ 'size_in_bytes' ] = \"\"\" // schedule size_in_bytes += _schedule.capacity() * sizeof(int); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} def _generate_cuda ( self ): \"\"\" Code generation if the CUDA paradigm is set. \"\"\" # # Code for handling the buffer and schedule parameters self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameter HomogeneousCorrelatedSpikeTrains std::vector< int > _schedule; std::vector< %(float_prec)s > mu_buffer; // buffer std::vector< %(float_prec)s > sigma_buffer; // buffer int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameter HomogeneousCorrelatedSpikeTrains void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_mu_list(std::vector< %(float_prec)s > buffer) { mu_buffer = buffer; } void set_sigma_list(std::vector< %(float_prec)s > buffer) { sigma_buffer = buffer; } std::vector< %(float_prec)s > get_mu_list() { return mu_buffer; } std::vector< %(float_prec)s > get_sigma_list() { return sigma_buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ], 'id' : self . id } self . _specific_template [ 'init_additional' ] = \"\"\" // counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'reset_additional' ] = \"\"\" // counters _t = 0; _block = 0; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters timed array void set_schedule(vector[int]) vector[int] get_schedule() void set_mu_list(vector[ %(float_prec)s ]) vector[ %(float_prec)s ] get_mu_list() void set_sigma_list(vector[ %(float_prec)s ]) vector[ %(float_prec)s ] get_sigma_list() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters timed array cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_mu_list( self, buffer ): pop %(id)s .set_mu_list( buffer ) cpdef np.ndarray get_mu_list( self ): return np.array(pop %(id)s .get_mu_list( )) cpdef set_sigma_list( self, buffer ): pop %(id)s .set_sigma_list( buffer ) cpdef np.ndarray get_sigma_list( self ): return np.array(pop %(id)s .get_sigma_list( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_periodic(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id , 'float_prec' : Global . config [ 'precision' ] } if not self . _has_schedule : # we can use the normal code generation for GPU kernels pass else : self . _specific_template [ 'update_variables' ] = \"\"\" if(_active) { // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data mu = mu_buffer[_block]; sigma = sigma_buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if ( _block == _schedule.size() ) { _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if( (_period > -1) && (_t == _period-1) ) { // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } \"\"\" self . _specific_template [ 'update_variable_body' ] = \"\"\" // Updating global variables of population %(id)s __global__ void cuPop %(id)s _global_step( const long int t, const double dt, const double tau, double mu, double* x, curandState* rand_0, double sigma ) { // x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) x[0] += dt*(mu - x[0])/tau + curand_normal_double( &rand_0[0] )*sigma*sqrt(dt/tau); } // Updating local variables of population %(id)s __global__ void cuPop %(id)s _local_step( const long int t, const double dt, curandState* rand_1, double* x, unsigned int* num_events, int* spiked, long int* last_spike ) { int i = threadIdx.x + blockDim.x * blockIdx.x; %(float_prec)s step = 1000.0/dt; while ( i < %(size)s ) { // p = Uniform(0.0, 1.0) * 1000.0 / dt %(float_prec)s p = curand_uniform_double( &rand_1[i] ) * step; if (p < x[0]) { int pos = atomicAdd ( num_events, 1); spiked[pos] = i; last_spike[i] = t; } i += blockDim.x; } __syncthreads(); } \"\"\" % { 'id' : self . id , 'size' : self . size , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'update_variable_header' ] = \"\"\"__global__ void cuPop %(id)s _global_step( const long int t, const double dt, const double tau, double mu, double* x, curandState* rand_0, double sigma ); __global__ void cuPop %(id)s _local_step( const long int t, const double dt, curandState* rand_1, double* x, unsigned int* num_events, int* spiked, long int* last_spike ); \"\"\" % { 'id' : self . id } # Please notice, that the GPU kernels can be launched only with one block. Otherwise, the # atomicAdd which is called inside the kernel is not working correct (HD: April 1st, 2021) self . _specific_template [ 'update_variable_call' ] = \"\"\" if (pop %(id)s ._active) { // Update the scheduling pop %(id)s .update(); // Reset old events clear_num_events<<< 1, 1, 0, pop %(id)s .stream >>>(pop %(id)s .gpu_spike_count); #ifdef _DEBUG cudaError_t err_clear_num_events_ %(id)s = cudaGetLastError(); if (err_clear_num_events_ %(id)s != cudaSuccess) std::cout << \"pop %(id)s _spike_gather: \" << cudaGetErrorString(err_clear_num_events_ %(id)s ) << std::endl; #endif // compute the value of x based on mu/sigma cuPop %(id)s _global_step<<< 1, 1, 0, pop %(id)s .stream >>>( t, dt, pop %(id)s .tau, pop %(id)s .mu, pop %(id)s .gpu_x, pop %(id)s .gpu_rand_0, pop %(id)s .sigma ); #ifdef _DEBUG cudaError_t err_pop %(id)s _global_step = cudaGetLastError(); if( err_pop %(id)s _global_step != cudaSuccess) { std::cout << \"pop %(id)s _step: \" << cudaGetErrorString(err_pop %(id)s _global_step) << std::endl; exit(0); } #endif // Generate new spike events cuPop %(id)s _local_step<<< 1, pop %(id)s ._threads_per_block, 0, pop %(id)s .stream >>>( t, dt, pop %(id)s .gpu_rand_1, pop %(id)s .gpu_x, pop %(id)s .gpu_spike_count, pop %(id)s .gpu_spiked, pop %(id)s .gpu_last_spike ); #ifdef _DEBUG cudaError_t err_pop_spike_gather_ %(id)s = cudaGetLastError(); if(err_pop_spike_gather_ %(id)s != cudaSuccess) { std::cout << \"pop %(id)s _spike_gather: \" << cudaGetErrorString(err_pop_spike_gather_ %(id)s ) << std::endl; exit(0); } #endif // transfer back the spike counter (needed by record) cudaMemcpy( &pop %(id)s .spike_count, pop %(id)s .gpu_spike_count, sizeof(unsigned int), cudaMemcpyDeviceToHost); #ifdef _DEBUG cudaError_t err_pop %(id)s _async_copy = cudaGetLastError(); if ( err_pop %(id)s _async_copy != cudaSuccess ) { std::cout << \"record_spike_count: \" << cudaGetErrorString(err_pop %(id)s _async_copy) << std::endl; exit(0); } #endif // transfer back the spiked array (needed by record) if (pop %(id)s .spike_count > 0) { cudaMemcpy( pop %(id)s .spiked.data(), pop %(id)s .gpu_spiked, pop %(id)s .spike_count*sizeof(int), cudaMemcpyDeviceToHost); #ifdef _DEBUG cudaError_t err_pop %(id)s _async_copy2 = cudaGetLastError(); if ( err_pop %(id)s _async_copy2 != cudaSuccess ) { std::cout << \"record_spike: \" << cudaGetErrorString(err_pop %(id)s _async_copy2) << std::endl; exit(0); } #endif } } \"\"\" % { 'id' : self . id } self . _specific_template [ 'size_in_bytes' ] = \"//TODO: \" def _instantiate ( self , module ): # Create the Cython instance self . cyInstance = getattr ( module , self . class_name + '_wrapper' )( self . size , self . max_delay ) def __setattr__ ( self , name , value ): if not hasattr ( self , 'initialized' ): Population . __setattr__ ( self , name , value ) elif name == 'schedule' : if self . initialized : self . cyInstance . set_schedule ( np . array ( value ) / Global . config [ 'dt' ] ) else : self . init [ 'schedule' ] = value elif name == 'mu' : if self . initialized : if self . _has_schedule : self . cyInstance . set_mu_list ( value ) else : self . cyInstance . set_global_attribute ( \"mu\" , value , Global . config [ \"precision\" ] ) else : self . init [ 'mu' ] = value elif name == 'sigma' : if self . initialized : if self . _has_schedule : self . cyInstance . set_sigma_list ( value ) else : self . cyInstance . set_global_attribute ( \"sigma\" , value , Global . config [ \"precision\" ] ) else : self . init [ 'sigma' ] = value elif name == \"period\" : if self . initialized : self . cyInstance . set_period ( int ( value / Global . config [ 'dt' ])) else : self . init [ 'period' ] = value elif name == 'rates' : if self . _has_schedule : value = np . array ( value ) if not value . size == self . schedule . size : Global . _error ( \"HomogeneousCorrelatedSpikeTrains: rates must have the same length as schedule.\" ) else : value = np . array ([ float ( value )]) if self . initialized : Population . __setattr__ ( self , name , value ) # Correction of mu and sigma everytime r, c or tau is changed try : mu , sigma = self . _correction ( self . rates , self . corr , self . tau ) if self . _has_schedule : self . mu = mu self . sigma = sigma else : self . mu = mu [ 0 ] self . sigma = sigma [ 0 ] except Exception as e : print ( e ) else : self . init [ name ] = value Population . __setattr__ ( self , name , value ) elif name == 'corr' : if self . _has_schedule : if not isinstance ( value , ( list , np . ndarray )): value = np . full (( self . schedule . size , ), value ) else : value = np . array ( value ) if not value . size == self . schedule . size : Global . _error ( \"HomogeneousCorrelatedSpikeTrains: corr must have the same length as schedule.\" ) else : value = np . array ([ float ( value )]) if self . initialized : Population . __setattr__ ( self , name , value ) try : # Correction of mu and sigma everytime r, c or tau is changed mu , sigma = self . _correction ( self . rates , self . corr , self . tau ) if self . _has_schedule : self . mu = mu self . sigma = sigma else : self . mu = mu [ 0 ] self . sigma = sigma [ 0 ] except Exception as e : print ( e ) else : self . init [ name ] = value Population . __setattr__ ( self , name , value ) elif name == 'tau' : if self . initialized : Population . __setattr__ ( self , name , value ) # Correction of mu and sigma everytime r, c or tau is changed mu , sigma = self . _correction ( self . rates , self . corr , self . tau ) if self . _has_schedule : self . mu = mu self . sigma = sigma else : self . mu = mu [ 0 ] self . sigma = sigma [ 0 ] else : self . init [ name ] = value Population . __setattr__ ( self , name , value ) else : Population . __setattr__ ( self , name , value ) def __getattr__ ( self , name ): if name == 'schedule' : if self . initialized : if self . _has_schedule : return Global . config [ 'dt' ] * self . cyInstance . get_schedule () else : return np . array ([ 0.0 ]) else : return self . init [ 'schedule' ] elif name == 'mu' : if self . initialized : if self . _has_schedule : return self . cyInstance . get_mu_list () else : return self . cyInstance . get_global_attribute ( \"mu\" , Global . config [ \"precision\" ] ) else : return self . init [ 'mu' ] elif name == 'sigma' : if self . initialized : if self . _has_schedule : return self . cyInstance . get_sigma_list () else : return self . cyInstance . get_global_attribute ( \"sigma\" , Global . config [ \"precision\" ] ) else : return self . init [ 'sigma' ] elif name == 'tau' : if self . initialized : return self . cyInstance . get_global_attribute ( \"tau\" , Global . config [ \"precision\" ] ) else : return self . init [ 'tau' ] elif name == 'period' : if self . initialized : return self . cyInstance . get_period () * Global . config [ 'dt' ] else : return self . init [ 'period' ] else : return Population . __getattribute__ ( self , name ) __init__ ( geometry , rates , corr , tau , schedule = None , period =- 1.0 , name = None , refractory = None , copied = False ) # Parameters: Name Type Description Default geometry population geometry as tuple. required rates rate in Hz of the population (must be a positive float or a list) required corr total correlation strength (float in [0, 1], or a list) required tau correlation time constant in ms. required schedule list of times where new values of rates and corr will be used to computre mu and sigma. None period time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.) -1.0 name unique name of the population (optional). None refractory refractory period in ms (careful: may break the correlation) None Source code in ANNarchy/core/SpecificPopulation.py 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 def __init__ ( self , geometry , rates , corr , tau , schedule = None , period =- 1. , name = None , refractory = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. :param rates: rate in Hz of the population (must be a positive float or a list) :param corr: total correlation strength (float in [0, 1], or a list) :param tau: correlation time constant in ms. :param schedule: list of times where new values of ``rates``and ``corr``will be used to computre mu and sigma. :param period: time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.) :param name: unique name of the population (optional). :param refractory: refractory period in ms (careful: may break the correlation) \"\"\" if schedule is not None : self . _has_schedule = True # Rates if not isinstance ( rates , ( list , np . ndarray )): Global . _error ( \"TimedHomogeneousCorrelatedSpikeTrains: the rates argument must be a list or a numpy array.\" ) rates = np . array ( rates ) # Schedule schedule = np . array ( schedule ) nb_schedules = rates . shape [ 0 ] if nb_schedules != schedule . size : Global . _error ( \"TimedHomogeneousCorrelatedSpikeTrains: the length of rates must be the same length as for schedule.\" ) # corr corr = np . array ( corr ) if corr . size == 1 : corr = np . full ( nb_schedules , corr ) else : self . _has_schedule = False rates = np . array ([ float ( rates )]) schedule = np . array ([ 0.0 ]) corr = np . array ([ corr ]) # Store refractory self . refractory_init = refractory # Correction of mu and sigma mu_list , sigma_list = self . _correction ( rates , corr , tau ) self . rates = rates self . corr = corr self . tau = tau # Create the neuron corr_neuron = Neuron ( parameters = \"\"\" tau = %(tau)s : population mu = %(mu)s : population sigma = %(sigma)s : population \"\"\" % { 'tau' : tau , 'mu' : mu_list [ 0 ], 'sigma' : sigma_list [ 0 ]}, equations = \"\"\" x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) : population, init= %(mu)s p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" % { 'mu' : mu_list [ 0 ]}, spike = \"p < x\" , refractory = refractory , name = \"HomogeneousCorrelated\" , description = \"Homogeneous correlated spike trains.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = corr_neuron , name = name , copied = copied ) # Initial values self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'corr' ] = corr self . init [ 'tau' ] = tau self . init [ 'period' ] = period if self . _has_schedule : self . init [ 'mu' ] = mu_list self . init [ 'sigma' ] = sigma_list else : self . init [ 'mu' ] = mu_list [ 0 ] self . init [ 'sigma' ] = sigma_list [ 0 ] TimedPoissonPopulation # Bases: SpecificPopulation Poisson population whose rate vary with the provided schedule. Example: inp = TimedPoissonPopulation ( geometry = 100 , rates = [ 10. , 20. , 100. , 20. , 5. ], schedule = [ 0. , 100. , 200. , 500. , 600. ], ) This creates a population of 100 Poisson neurons whose rate will be: 10 Hz during the first 100 ms. 20 HZ during the next 100 ms. 100 Hz during the next 300 ms. 20 Hz during the next 100 ms. 5 Hz until the end of the simulation. If you want the TimedPoissonPopulation to \"loop\" over the schedule, you can specify a period: inp = TimedPoissonPopulation ( geometry = 100 , rates = [ 10. , 20. , 100. , 20. , 5. ], schedule = [ 0. , 100. , 200. , 500. , 600. ], period = 1000. , ) Here the rate will become 10Hz again every 1 second of simulation. If the period is smaller than the schedule, the remaining rates will not be set. Note that you can use the reset() method to manually reinitialize the schedule, times becoming relative to that call: simulate ( 1200. ) # Should switch to 100 Hz due to the period of 1000. inp . reset () simulate ( 1000. ) # Starts at 10 Hz again. The rates were here global to the population. If you want each neuron to have a different rate, rates must have additional dimensions corresponding to the geometry of the population. inp = TimedPoissonPopulation ( geometry = 100 , rates = [ [ 10. + 0.05 * i for i in range ( 100 )], [ 20. + 0.05 * i for i in range ( 100 )], ], schedule = [ 0. , 100. ], period = 1000. , ) Source code in ANNarchy/core/SpecificPopulation.py 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 class TimedPoissonPopulation ( SpecificPopulation ): \"\"\" Poisson population whose rate vary with the provided schedule. Example: ```python inp = TimedPoissonPopulation( geometry = 100, rates = [10., 20., 100., 20., 5.], schedule = [0., 100., 200., 500., 600.], ) ``` This creates a population of 100 Poisson neurons whose rate will be: * 10 Hz during the first 100 ms. * 20 HZ during the next 100 ms. * 100 Hz during the next 300 ms. * 20 Hz during the next 100 ms. * 5 Hz until the end of the simulation. If you want the TimedPoissonPopulation to \"loop\" over the schedule, you can specify a period: ```python inp = TimedPoissonPopulation( geometry = 100, rates = [10., 20., 100., 20., 5.], schedule = [0., 100., 200., 500., 600.], period = 1000., ) ``` Here the rate will become 10Hz again every 1 second of simulation. If the period is smaller than the schedule, the remaining rates will not be set. Note that you can use the ``reset()`` method to manually reinitialize the schedule, times becoming relative to that call: ```python simulate(1200.) # Should switch to 100 Hz due to the period of 1000. inp.reset() simulate(1000.) # Starts at 10 Hz again. ``` The rates were here global to the population. If you want each neuron to have a different rate, ``rates`` must have additional dimensions corresponding to the geometry of the population. ```python inp = TimedPoissonPopulation( geometry = 100, rates = [ [10. + 0.05*i for i in range(100)], [20. + 0.05*i for i in range(100)], ], schedule = [0., 100.], period = 1000., ) ``` \"\"\" def __init__ ( self , geometry , rates , schedule , period = - 1. , name = None , copied = False ): \"\"\" :param rates: array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match the geometry of the population. :param schedule: list of times (in ms) where the firing rate should change. :param period: time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.). \"\"\" neuron = Neuron ( parameters = \"\"\" proba = 1.0 \"\"\" , equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < proba \"\"\" , name = \"TimedPoisson\" , description = \"Spiking neuron following a Poisson distribution.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = neuron , name = name , copied = copied ) # Check arguments try : rates = np . array ( rates ) except : Global . _error ( \"TimedPoissonPopulation: the rates argument must be a numpy array.\" ) schedule = np . array ( schedule ) nb_schedules = rates . shape [ 0 ] if nb_schedules != schedule . size : Global . _error ( \"TimedPoissonPopulation: the first axis of the rates argument must be the same length as schedule.\" ) if rates . ndim == 1 : # One rate for the whole population rates = np . array ([ np . full ( self . size , rates [ i ]) for i in range ( nb_schedules )]) # Initial values self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'period' ] = period def _copy ( self ): \"Returns a copy of the population when creating networks.\" return TimedPoissonPopulation ( self . geometry , self . init [ 'rates' ] , self . init [ 'schedule' ], self . init [ 'period' ], self . name , copied = True ) def _generate_st ( self ): \"\"\" adjust code templates for the specific population for single thread. \"\"\" self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameters of a TimedPoissonPopulation std::vector< int > _schedule; // List of times where new inputs should be set std::vector< std::vector< %(float_prec)s > > _buffer; // buffer holding the data int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameters of a TimedPoissonPopulation void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_buffer(std::vector< std::vector< %(float_prec)s > > buffer) { _buffer = buffer; r = _buffer[0]; } std::vector< std::vector< %(float_prec)s > > get_buffer() { return _buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // Initialize counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters of a TimedPoissonPopulation void set_schedule(vector[int]) vector[int] get_schedule() void set_buffer(vector[vector[ %(float_prec)s ]]) vector[vector[ %(float_prec)s ]] get_buffer() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; _block = 0; r.clear(); r = std::vector< %(float_prec)s >(size, 0.0); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters of a TimedArray cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_rates( self, buffer ): pop %(id)s .set_buffer( buffer ) cpdef np.ndarray get_rates( self ): return np.array(pop %(id)s .get_buffer( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_period(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id } self . _specific_template [ 'update_variables' ] = \"\"\" if(_active){ //std::cout << _t << \" \" << _block<< \" \" << _schedule[_block] << std::endl; // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data proba = _buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if (_block == _schedule.size()){ _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if(_period > -1 && (_t == _period-1)){ // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } if( _active ) { spiked.clear(); // Updating local variables %(float_prec)s step = 1000.0/dt; #pragma omp simd for(int i = 0; i < size; i++){ // p = Uniform(0.0, 1.0) * 1000.0 / dt p[i] = step*rand_0[i]; } } // active \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'size_in_bytes' ] = \"\"\" // schedule size_in_bytes += _schedule.capacity() * sizeof(int); // buffer size_in_bytes += _buffer.capacity() * sizeof(std::vector< %(float_prec)s >); for( auto it = _buffer.begin(); it != _buffer.end(); it++ ) size_in_bytes += it->capacity() * sizeof( %(float_prec)s ); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} def _generate_omp ( self ): \"\"\" adjust code templates for the specific population for openMP. \"\"\" self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameters of a TimedPoissonPopulation std::vector< int > _schedule; // List of times where new inputs should be set std::vector< std::vector< %(float_prec)s > > _buffer; // buffer holding the data int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameters of a TimedPoissonPopulation void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_buffer(std::vector< std::vector< %(float_prec)s > > buffer) { _buffer = buffer; r = _buffer[0]; } std::vector< std::vector< %(float_prec)s > > get_buffer() { return _buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // Initialize counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters of a TimedPoissonPopulation void set_schedule(vector[int]) vector[int] get_schedule() void set_buffer(vector[vector[ %(float_prec)s ]]) vector[vector[ %(float_prec)s ]] get_buffer() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; _block = 0; r.clear(); r = std::vector< %(float_prec)s >(size, 0.0); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters of a TimedArray cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_rates( self, buffer ): pop %(id)s .set_buffer( buffer ) cpdef np.ndarray get_rates( self ): return np.array(pop %(id)s .get_buffer( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_period(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id } self . _specific_template [ 'update_variables' ] = \"\"\" if(_active){ #pragma omp single { //std::cout << _t << \" \" << _block<< \" \" << _schedule[_block] << std::endl; // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data proba = _buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if (_block == _schedule.size()){ _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if(_period > -1 && (_t == _period-1)){ // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } } if( _active ) { spiked.clear(); // Updating local variables %(float_prec)s step = 1000.0/dt; #pragma omp for simd for(int i = 0; i < size; i++){ // p = Uniform(0.0, 1.0) * 1000.0 / dt p[i] = step*rand_0[i]; } } // active \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'size_in_bytes' ] = \"\"\" // schedule size_in_bytes += _schedule.capacity() * sizeof(int); // buffer size_in_bytes += _buffer.capacity() * sizeof(std::vector< %(float_prec)s >); for( auto it = _buffer.begin(); it != _buffer.end(); it++ ) size_in_bytes += it->capacity() * sizeof( %(float_prec)s ); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} def _generate_cuda ( self ): \"\"\" Code generation if the CUDA paradigm is set. \"\"\" # I suppress the code generation for allocating the variable r on gpu, as # well as memory transfer codes. This is only possible as no other variables # allowed in TimedArray. self . _specific_template [ 'init_parameters_variables' ] = \"\"\" // Random numbers cudaMalloc((void**)&gpu_rand_0, size * sizeof(curandState)); init_curand_states( size, gpu_rand_0, global_seed ); \"\"\" self . _specific_template [ 'host_device_transfer' ] = \"\" self . _specific_template [ 'device_host_transfer' ] = \"\" # # Code for handling the buffer and schedule parameters self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameter timed array std::vector< int > _schedule; std::vector< %(float_prec)s * > gpu_buffer; int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameter timed array void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_buffer(std::vector< std::vector< %(float_prec)s > > buffer) { if ( gpu_buffer.empty() ) { gpu_buffer = std::vector< %(float_prec)s * >(buffer.size(), nullptr); // allocate gpu arrays for(int i = 0; i < buffer.size(); i++) { cudaMalloc((void**)&gpu_buffer[i], buffer[i].size()*sizeof( %(float_prec)s )); } } auto host_it = buffer.begin(); auto dev_it = gpu_buffer.begin(); for( ; host_it < buffer.end(); host_it++, dev_it++ ) { cudaMemcpy( *dev_it, host_it->data(), host_it->size()*sizeof( %(float_prec)s ), cudaMemcpyHostToDevice); } gpu_proba = gpu_buffer[0]; } std::vector< std::vector< %(float_prec)s > > get_buffer() { std::vector< std::vector< %(float_prec)s > > buffer = std::vector< std::vector< %(float_prec)s > >( gpu_buffer.size(), std::vector< %(float_prec)s >(size,0.0) ); auto host_it = buffer.begin(); auto dev_it = gpu_buffer.begin(); for( ; host_it < buffer.end(); host_it++, dev_it++ ) { cudaMemcpy( host_it->data(), *dev_it, size*sizeof( %(float_prec)s ), cudaMemcpyDeviceToHost ); } return buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'reset_additional' ] = \"\"\" // counters _t = 0; _block = 0; gpu_proba = gpu_buffer[0]; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters timed array void set_schedule(vector[int]) vector[int] get_schedule() void set_buffer(vector[vector[ %(float_prec)s ]]) vector[vector[ %(float_prec)s ]] get_buffer() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters timed array cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_rates( self, buffer ): pop %(id)s .set_buffer( buffer ) cpdef np.ndarray get_rates( self ): return np.array(pop %(id)s .get_buffer( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_periodic(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'update_variables' ] = \"\"\" if(_active) { // std::cout << _t << \" \" << _block<< \" \" << _schedule[_block] << std::endl; // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data gpu_proba = gpu_buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if ( _block == _schedule.size() ) { _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if( (_period > -1) && (_t == _period-1) ) { // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } \"\"\" self . _specific_template [ 'update_variable_body' ] = \"\"\" __global__ void cuPop %(id)s _local_step( const long int t, const double dt, curandState* rand_0, double* proba, unsigned int* num_events, int* spiked, long int* last_spike ) { int i = threadIdx.x + blockDim.x * blockIdx.x; %(float_prec)s step = 1000.0/dt; while ( i < %(size)s ) { // p = Uniform(0.0, 1.0) * 1000.0 / dt %(float_prec)s p = curand_uniform_double( &rand_0[i] ) * step; if (p < proba[i]) { int pos = atomicAdd ( num_events, 1); spiked[pos] = i; last_spike[i] = t; } i += blockDim.x; } __syncthreads(); } \"\"\" % { 'id' : self . id , 'size' : self . size , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'update_variable_header' ] = \"__global__ void cuPop %(id)s _local_step( const long int t, const double dt, curandState* rand_0, double* proba, unsigned int* num_events, int* spiked, long int* last_spike );\" % { 'id' : self . id } # Please notice, that the GPU kernels can be launched only with one block. Otherwise, the # atomicAdd which is called inside the kernel is not working correct (HD: April 1st, 2021) self . _specific_template [ 'update_variable_call' ] = \"\"\" // host side update of neurons pop %(id)s .update(); // Reset old events clear_num_events<<< 1, 1, 0, pop %(id)s .stream >>>(pop %(id)s .gpu_spike_count); #ifdef _DEBUG cudaError_t err_clear_num_events_ %(id)s = cudaGetLastError(); if(err_clear_num_events_ %(id)s != cudaSuccess) std::cout << \"pop %(id)s _spike_gather: \" << cudaGetErrorString(err_clear_num_events_ %(id)s ) << std::endl; #endif // Compute current events cuPop %(id)s _local_step<<< 1, pop %(id)s ._threads_per_block, 0, pop %(id)s .stream >>>( t, dt, pop %(id)s .gpu_rand_0, pop %(id)s .gpu_proba, pop %(id)s .gpu_spike_count, pop %(id)s .gpu_spiked, pop %(id)s .gpu_last_spike ); #ifdef _DEBUG cudaError_t err_pop_spike_gather_ %(id)s = cudaGetLastError(); if(err_pop_spike_gather_ %(id)s != cudaSuccess) std::cout << \"pop %(id)s _spike_gather: \" << cudaGetErrorString(err_pop_spike_gather_ %(id)s ) << std::endl; #endif // transfer back the spike counter (needed by record) cudaMemcpyAsync( &pop %(id)s .spike_count, pop %(id)s .gpu_spike_count, sizeof(unsigned int), cudaMemcpyDeviceToHost, pop %(id)s .stream ); #ifdef _DEBUG cudaError_t err = cudaGetLastError(); if ( err != cudaSuccess ) std::cout << \"record_spike_count: \" << cudaGetErrorString(err) << std::endl; #endif // transfer back the spiked array (needed by record) cudaMemcpyAsync( pop %(id)s .spiked.data(), pop %(id)s .gpu_spiked, pop %(id)s .spike_count*sizeof(int), cudaMemcpyDeviceToHost, pop %(id)s .stream ); #ifdef _DEBUG err = cudaGetLastError(); if ( err != cudaSuccess ) std::cout << \"record_spike: \" << cudaGetErrorString(err) << std::endl; #endif \"\"\" % { 'id' : self . id } self . _specific_template [ 'size_in_bytes' ] = \"//TODO: \" def _instantiate ( self , module ): # Create the Cython instance self . cyInstance = getattr ( module , self . class_name + '_wrapper' )( self . size , self . max_delay ) def __setattr__ ( self , name , value ): if name == 'schedule' : if self . initialized : self . cyInstance . set_schedule ( np . array ( value ) / Global . config [ 'dt' ] ) else : self . init [ 'schedule' ] = value elif name == 'rates' : if self . initialized : value = np . array ( value ) print ( value . shape ) if value . shape [ 0 ] != self . schedule . shape [ 0 ]: Global . _error ( \"TimedPoissonPopulation: the first dimension of rates must match the schedule.\" ) if value . ndim > 2 : # we need to flatten the provided data values = value . reshape ( ( value . shape [ 0 ], self . size ) ) self . cyInstance . set_rates ( values ) elif value . ndim == 2 : if value . shape [ 1 ] != self . size : if value . shape [ 1 ] == 1 : value = np . array ([ np . full ( self . size , value [ i ]) for i in range ( value . shape [ 0 ])]) else : Global . _error ( \"TimedPoissonPopulation: the second dimension of rates must match the number of neurons.\" ) self . cyInstance . set_rates ( value ) elif value . ndim == 1 : value = np . array ([ np . full ( self . size , value [ i ]) for i in range ( value . shape [ 0 ])]) self . cyInstance . set_rates ( value ) else : self . init [ 'rates' ] = value elif name == \"period\" : if self . initialized : self . cyInstance . set_period ( int ( value / Global . config [ 'dt' ])) else : self . init [ 'period' ] = value else : Population . __setattr__ ( self , name , value ) def __getattr__ ( self , name ): if name == 'schedule' : if self . initialized : return Global . config [ 'dt' ] * self . cyInstance . get_schedule () else : return self . init [ 'schedule' ] elif name == 'rates' : if self . initialized : if len ( self . geometry ) > 1 : # unflatten the data flat_values = self . cyInstance . get_rates () values = np . zeros ( tuple ( [ len ( self . schedule )] + list ( self . geometry ) ) ) for x in range ( len ( self . schedule )): values [ x ] = np . reshape ( flat_values [ x ], self . geometry ) return values else : return self . cyInstance . get_rates () else : return self . init [ 'rates' ] elif name == 'period' : if self . initialized : return self . cyInstance . get_period () * Global . config [ 'dt' ] else : return self . init [ 'period' ] else : return Population . __getattribute__ ( self , name ) __init__ ( geometry , rates , schedule , period =- 1.0 , name = None , copied = False ) # Parameters: Name Type Description Default rates array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match the geometry of the population. required schedule list of times (in ms) where the firing rate should change. required period time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.). -1.0 Source code in ANNarchy/core/SpecificPopulation.py 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 def __init__ ( self , geometry , rates , schedule , period = - 1. , name = None , copied = False ): \"\"\" :param rates: array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match the geometry of the population. :param schedule: list of times (in ms) where the firing rate should change. :param period: time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.). \"\"\" neuron = Neuron ( parameters = \"\"\" proba = 1.0 \"\"\" , equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < proba \"\"\" , name = \"TimedPoisson\" , description = \"Spiking neuron following a Poisson distribution.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = neuron , name = name , copied = copied ) # Check arguments try : rates = np . array ( rates ) except : Global . _error ( \"TimedPoissonPopulation: the rates argument must be a numpy array.\" ) schedule = np . array ( schedule ) nb_schedules = rates . shape [ 0 ] if nb_schedules != schedule . size : Global . _error ( \"TimedPoissonPopulation: the first axis of the rates argument must be the same length as schedule.\" ) if rates . ndim == 1 : # One rate for the whole population rates = np . array ([ np . full ( self . size , rates [ i ]) for i in range ( nb_schedules )]) # Initial values self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'period' ] = period ImagePopulation # Bases: Population Specific rate-coded Population allowing to represent images (png, jpg...) as the firing rate of a population (each neuron represents one pixel). This extension requires the Python Image Library (pip install Pillow). Usage: from ANNarchy import * from ANNarchy.extensions.image import ImagePopulation pop = ImagePopulation ( geometry = ( 480 , 640 )) pop . set_image ( 'image.jpg' ) About the geometry: If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image). If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color). If the third dimension is 3, each will correspond to the RGB values of the pixels. Warning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population. Source code in ANNarchy/extensions/image/ImagePopulation.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 class ImagePopulation ( Population ): \"\"\" Specific rate-coded Population allowing to represent images (png, jpg...) as the firing rate of a population (each neuron represents one pixel). This extension requires the Python Image Library (pip install Pillow). Usage: ```python from ANNarchy import * from ANNarchy.extensions.image import ImagePopulation pop = ImagePopulation(geometry=(480, 640)) pop.set_image('image.jpg') ``` About the geometry: * If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image). * If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color). If the third dimension is 3, each will correspond to the RGB values of the pixels. **Warning:** due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population. \"\"\" def __init__ ( self , geometry , name = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation. :param name: unique name of the population (optional). \"\"\" # Check geometry if isinstance ( geometry , int ) or len ( geometry ) == 1 : Global . _error ( 'The geometry of an ImagePopulation should be 2D (grayscale) or 3D (color).' ) if len ( geometry ) == 3 and ( geometry [ 2 ] != 3 and geometry [ 2 ] != 1 ): Global . _error ( 'The third dimension of an ImagePopulation should be either 1 (grayscale) or 3 (color).' ) if len ( geometry ) == 3 and geometry [ 2 ] == 1 : geometry = ( int ( geometry [ 0 ]), int ( geometry [ 1 ])) # Create the population Population . __init__ ( self , geometry = geometry , name = name , neuron = Neuron ( parameters = \"r = 0.0\" ), copied = copied ) def _copy ( self ): \"Returns a copy of the population when creating networks. Internal use only.\" return ImagePopulation ( geometry = self . geometry , name = self . name , copied = True ) def set_image ( self , image_name ): \"\"\" Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population. If the image has a different size from the population, it will be resized. \"\"\" try : im = Image . open ( image_name ) except : # image does not exist Global . _error ( 'The image ' + image_name + ' does not exist.' ) # Resize the image if needed ( width , height ) = ( self . geometry [ 1 ], self . geometry [ 0 ]) if im . size != ( width , height ): Global . _warning ( 'The image ' + image_name + ' does not have the same size ' + str ( im . size ) + ' as the population ' + str (( width , height )) + '. It will be resized.' ) im = im . resize (( width , height )) # Check if only the luminance should be extracted if self . dimension == 2 or self . geometry [ 2 ] == 1 : im = im . convert ( \"L\" ) # Set the rate of the population self . r = np . array ( im ) . reshape ( self . size ) / 255. __init__ ( geometry , name = None , copied = False ) # Parameters: Name Type Description Default geometry population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation. required name unique name of the population (optional). None Source code in ANNarchy/extensions/image/ImagePopulation.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , geometry , name = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation. :param name: unique name of the population (optional). \"\"\" # Check geometry if isinstance ( geometry , int ) or len ( geometry ) == 1 : Global . _error ( 'The geometry of an ImagePopulation should be 2D (grayscale) or 3D (color).' ) if len ( geometry ) == 3 and ( geometry [ 2 ] != 3 and geometry [ 2 ] != 1 ): Global . _error ( 'The third dimension of an ImagePopulation should be either 1 (grayscale) or 3 (color).' ) if len ( geometry ) == 3 and geometry [ 2 ] == 1 : geometry = ( int ( geometry [ 0 ]), int ( geometry [ 1 ])) # Create the population Population . __init__ ( self , geometry = geometry , name = name , neuron = Neuron ( parameters = \"r = 0.0\" ), copied = copied ) set_image ( image_name ) # Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population. If the image has a different size from the population, it will be resized. Source code in ANNarchy/extensions/image/ImagePopulation.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def set_image ( self , image_name ): \"\"\" Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population. If the image has a different size from the population, it will be resized. \"\"\" try : im = Image . open ( image_name ) except : # image does not exist Global . _error ( 'The image ' + image_name + ' does not exist.' ) # Resize the image if needed ( width , height ) = ( self . geometry [ 1 ], self . geometry [ 0 ]) if im . size != ( width , height ): Global . _warning ( 'The image ' + image_name + ' does not have the same size ' + str ( im . size ) + ' as the population ' + str (( width , height )) + '. It will be resized.' ) im = im . resize (( width , height )) # Check if only the luminance should be extracted if self . dimension == 2 or self . geometry [ 2 ] == 1 : im = im . convert ( \"L\" ) # Set the rate of the population self . r = np . array ( im ) . reshape ( self . size ) / 255. VideoPopulation # Bases: ImagePopulation Specific rate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel). This extension requires the C++ library OpenCV >= 4.0 (apt-get/yum install opencv). pkg-config opencv4 --cflags --libs should not return an error. vtk might additionally have to be installed. Usage: from ANNarchy import * from ANNarchy.extensions.image import VideoPopulation pop = VideoPopulation ( geometry = ( 480 , 640 )) compile () pop . start_camera ( 0 ) while ( True ): pop . grab_image () simulate ( 10.0 ) About the geometry: If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image). If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color). If the third dimension is 3, each will correspond to the RGB values of the pixels. Warning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population. Source code in ANNarchy/extensions/image/ImagePopulation.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 class VideoPopulation ( ImagePopulation ): \"\"\" Specific rate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel). This extension requires the C++ library OpenCV >= 4.0 (apt-get/yum install opencv). ``pkg-config opencv4 --cflags --libs`` should not return an error. `vtk` might additionally have to be installed. Usage: ```python from ANNarchy import * from ANNarchy.extensions.image import VideoPopulation pop = VideoPopulation(geometry=(480, 640)) compile() pop.start_camera(0) while(True): pop.grab_image() simulate(10.0) ``` About the geometry: * If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image). * If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color). If the third dimension is 3, each will correspond to the RGB values of the pixels. **Warning:** due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population. \"\"\" def __init__ ( self , geometry , opencv_version = \"4\" , name = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized. :param opencv_version: OpenCV version (default=4). :param name: unique name of the population (optional). \"\"\" # Create the population ImagePopulation . __init__ ( self , geometry = geometry , name = name , copied = copied ) self . opencv_version = opencv_version def _copy ( self ): \"Returns a copy of the population when creating networks. Internal use only.\" return VideoPopulation ( geometry = self . geometry , name = self . name , copied = True ) def _generate ( self ): \"Code generation\" # Add corresponding libs to the Makefile extra_libs . append ( '`pkg-config opencv' + str ( self . opencv_version ) + ' --cflags --libs`' ) # Include opencv self . _specific_template [ 'include_additional' ] = \"\"\"#include <opencv2/opencv.hpp> using namespace cv; \"\"\" # Class for the camera device self . _specific_template [ 'struct_additional' ] = \"\"\" // VideoPopulation class CameraDeviceCPP : public cv::VideoCapture { public: CameraDeviceCPP (int id, int width, int height, int depth) : cv::VideoCapture(id){ width_ = width; height_ = height; depth_ = depth; img_ = std::vector< %(float_prec)s >(width*height*depth, 0.0); } std::vector< %(float_prec)s > GrabImage(){ if(isOpened()){ // Read a new frame from the video Mat frame; read(frame); // Resize the image Mat resized_frame; resize(frame, resized_frame, Size(width_, height_) ); // If depth=1, only luminance if(depth_==1){ // Convert to luminance cvtColor(resized_frame, resized_frame, COLOR_BGR2GRAY); for(int i = 0; i < resized_frame.rows; i++){ for(int j = 0; j < resized_frame.cols; j++){ this->img_[j+width_*i] = float(resized_frame.at<uchar>(i, j))/255.0; } } } else{ //BGR for(int i = 0; i < resized_frame.rows; i++){ for(int j = 0; j < resized_frame.cols; j++){ Vec3b intensity = resized_frame.at<Vec3b>(i, j); this->img_[(j+width_*i)*3 + 0] = %(float_prec)s (intensity.val[2])/255.0; this->img_[(j+width_*i)*3 + 1] = %(float_prec)s (intensity.val[1])/255.0; this->img_[(j+width_*i)*3 + 2] = %(float_prec)s (intensity.val[0])/255.0; } } } } return this->img_; }; protected: // Width and height of the image, depth_ is 1 (grayscale) or 3 (RGB) int width_, height_, depth_; // Vector of floats for the returned image std::vector< %(float_prec)s > img_; }; \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'declare_additional' ] = \"\"\" // Camera CameraDeviceCPP* camera_; void StartCamera(int id, int width, int height, int depth){ camera_ = new CameraDeviceCPP(id, width, height, depth); if(!camera_->isOpened()){ std::cout << \"Error: could not open the camera.\" << std::endl; } }; void GrabImage(){ if(camera_->isOpened()){ r = camera_->GrabImage(); } }; void ReleaseCamera(){ camera_->release(); }; \"\"\" self . _specific_template [ 'update_variables' ] = \"\" self . _specific_template [ 'export_additional' ] = \"\"\" void StartCamera(int id, int width, int height, int depth) void GrabImage() void ReleaseCamera() \"\"\" self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # CameraDevice def start_camera(self, int id, int width, int height, int depth): pop %(id)s .StartCamera(id, width, height, depth) def grab_image(self): pop %(id)s .GrabImage() def release_camera(self): pop %(id)s .ReleaseCamera() \"\"\" % { 'id' : self . id } def start_camera ( self , camera_port = 0 ): \"\"\" Starts the webcam with the corresponding device (default = 0). On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc. \"\"\" self . cyInstance . start_camera ( camera_port , self . geometry [ 1 ], self . geometry [ 0 ], 3 if self . dimension == 3 else 1 ) def grab_image ( self ): \"\"\" Grabs one image from the camera and feeds it into the population. The camera must be first started with: pop.start_camera(0) \"\"\" self . cyInstance . grab_image () def release ( self ): \"\"\" Releases the camera: pop.release() \"\"\" self . cyInstance . release_camera () __init__ ( geometry , opencv_version = '4' , name = None , copied = False ) # Parameters: Name Type Description Default geometry population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized. required opencv_version OpenCV version (default=4). '4' name unique name of the population (optional). None Source code in ANNarchy/extensions/image/ImagePopulation.py 121 122 123 124 125 126 127 128 129 130 def __init__ ( self , geometry , opencv_version = \"4\" , name = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized. :param opencv_version: OpenCV version (default=4). :param name: unique name of the population (optional). \"\"\" # Create the population ImagePopulation . __init__ ( self , geometry = geometry , name = name , copied = copied ) self . opencv_version = opencv_version grab_image () # Grabs one image from the camera and feeds it into the population. The camera must be first started with: pop.start_camera(0) Source code in ANNarchy/extensions/image/ImagePopulation.py 246 247 248 249 250 251 252 253 254 def grab_image ( self ): \"\"\" Grabs one image from the camera and feeds it into the population. The camera must be first started with: pop.start_camera(0) \"\"\" self . cyInstance . grab_image () release () # Releases the camera: pop.release() Source code in ANNarchy/extensions/image/ImagePopulation.py 256 257 258 259 260 261 262 def release ( self ): \"\"\" Releases the camera: pop.release() \"\"\" self . cyInstance . release_camera () start_camera ( camera_port = 0 ) # Starts the webcam with the corresponding device (default = 0). On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc. Source code in ANNarchy/extensions/image/ImagePopulation.py 237 238 239 240 241 242 243 244 def start_camera ( self , camera_port = 0 ): \"\"\" Starts the webcam with the corresponding device (default = 0). On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc. \"\"\" self . cyInstance . start_camera ( camera_port , self . geometry [ 1 ], self . geometry [ 0 ], 3 if self . dimension == 3 else 1 )","title":"Specific Populations"},{"location":"API/SpecificPopulation.html#specific-populations","text":"ANNarchy provides a set of predefined Population objects to ease the definition of standard networks.","title":"Specific Populations"},{"location":"API/SpecificPopulation.html#ANNarchy.PoissonPopulation","text":"Bases: SpecificPopulation Population of spiking neurons following a Poisson distribution. Case 1: Input population Each neuron of the population will randomly emit spikes, with a mean firing rate defined by the rates argument. The mean firing rate in Hz can be a fixed value for all neurons: pop = PoissonPopulation ( geometry = 100 , rates = 100.0 ) but it can be modified later as a normal parameter: pop . rates = np . linspace ( 10 , 150 , 100 ) It is also possible to define a temporal equation for the rates, by passing a string to the argument: pop = PoissonPopulation ( geometry = 100 , rates = \"100.0 * (1.0 + sin(2*pi*t/1000.0) )/2.0\" ) The syntax of this equation follows the same structure as neural variables. It is also possible to add parameters to the population which can be used in the equation of rates : pop = PoissonPopulation ( geometry = 100 , parameters = ''' amp = 100.0 frequency = 1.0 ''' , rates = \"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\" ) Note: The preceding definition is fully equivalent to the definition of this neuron: poisson = Neuron ( parameters = ''' amp = 100.0 frequency = 1.0 ''' , equations = ''' rates = amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0 p = Uniform(0.0, 1.0) * 1000.0 / dt ''' , spike = ''' p < rates ''' ) The refractory period can also be set, so that a neuron can not emit two spikes too close from each other. Case 2: Hybrid population If the rates argument is not set, the population can be used as an interface from a rate-coded population. The target argument specifies which incoming projections will be summed to determine the instantaneous firing rate of each neuron. See the example in examples/hybrid/Hybrid.py for a usage. Source code in ANNarchy/core/SpecificPopulation.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 class PoissonPopulation ( SpecificPopulation ): \"\"\" Population of spiking neurons following a Poisson distribution. **Case 1:** Input population Each neuron of the population will randomly emit spikes, with a mean firing rate defined by the *rates* argument. The mean firing rate in Hz can be a fixed value for all neurons: ```python pop = PoissonPopulation(geometry=100, rates=100.0) ``` but it can be modified later as a normal parameter: ```python pop.rates = np.linspace(10, 150, 100) ``` It is also possible to define a temporal equation for the rates, by passing a string to the argument: ```python pop = PoissonPopulation( geometry=100, rates=\"100.0 * (1.0 + sin(2*pi*t/1000.0) )/2.0\" ) ``` The syntax of this equation follows the same structure as neural variables. It is also possible to add parameters to the population which can be used in the equation of `rates`: ```python pop = PoissonPopulation( geometry=100, parameters = ''' amp = 100.0 frequency = 1.0 ''', rates=\"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\" ) ``` **Note:** The preceding definition is fully equivalent to the definition of this neuron: ```python poisson = Neuron( parameters = ''' amp = 100.0 frequency = 1.0 ''', equations = ''' rates = amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0 p = Uniform(0.0, 1.0) * 1000.0 / dt ''', spike = ''' p < rates ''' ) ``` The refractory period can also be set, so that a neuron can not emit two spikes too close from each other. **Case 2:** Hybrid population If the ``rates`` argument is not set, the population can be used as an interface from a rate-coded population. The ``target`` argument specifies which incoming projections will be summed to determine the instantaneous firing rate of each neuron. See the example in ``examples/hybrid/Hybrid.py`` for a usage. \"\"\" def __init__ ( self , geometry , name = None , rates = None , target = None , parameters = None , refractory = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. :param name: unique name of the population (optional). :param rates: mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string). :param target: the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\"). :param parameters: additional parameters which can be used in the *rates* equation. :param refractory: refractory period in ms. \"\"\" if rates is None and target is None : Global . _error ( 'A PoissonPopulation must define either rates or target.' ) self . target = target self . parameters = parameters self . refractory_init = refractory self . rates_init = rates if target is not None : # hybrid population # Create the neuron poisson_neuron = Neuron ( parameters = \"\"\" %(params)s \"\"\" % { 'params' : parameters if parameters else '' }, equations = \"\"\" rates = sum( %(target)s ) p = Uniform(0.0, 1.0) * 1000.0 / dt _sum_ %(target)s = 0.0 \"\"\" % { 'target' : target }, spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Hybrid\" , description = \"Hybrid spiking neuron emitting spikes according to a Poisson distribution at a frequency determined by the weighted sum of inputs.\" ) elif isinstance ( rates , str ): # Create the neuron poisson_neuron = Neuron ( parameters = \"\"\" %(params)s \"\"\" % { 'params' : parameters if parameters else '' }, equations = \"\"\" rates = %(rates)s p = Uniform(0.0, 1.0) * 1000.0 / dt _sum_exc = 0.0 \"\"\" % { 'rates' : rates }, spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) elif isinstance ( rates , np . ndarray ): poisson_neuron = Neuron ( parameters = \"\"\" rates = 10.0 \"\"\" , equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) else : poisson_neuron = Neuron ( parameters = \"\"\" rates = %(rates)s \"\"\" % { 'rates' : rates }, equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = poisson_neuron , name = name , copied = copied ) if isinstance ( rates , np . ndarray ): self . rates = rates def _copy ( self ): \"Returns a copy of the population when creating networks.\" return PoissonPopulation ( self . geometry , name = self . name , rates = self . rates_init , target = self . target , parameters = self . parameters , refractory = self . refractory_init , copied = True ) def _generate_st ( self ): \"\"\" Generate single thread code. We don't need any separate code snippets. All is done during the normal code generation path. \"\"\" pass def _generate_omp ( self ): \"\"\" Generate openMP code. We don't need any separate code snippets. All is done during the normal code generation path. \"\"\" pass def _generate_cuda ( self ): \"\"\" Generate CUDA code. We don't need any separate code snippets. All is done during the normal code generation path. \"\"\" pass","title":"PoissonPopulation"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.PoissonPopulation.__init__","text":"Parameters: Name Type Description Default geometry population geometry as tuple. required name unique name of the population (optional). None rates mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string). None target the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\"). None parameters additional parameters which can be used in the rates equation. None refractory refractory period in ms. None Source code in ANNarchy/core/SpecificPopulation.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def __init__ ( self , geometry , name = None , rates = None , target = None , parameters = None , refractory = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. :param name: unique name of the population (optional). :param rates: mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string). :param target: the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\"). :param parameters: additional parameters which can be used in the *rates* equation. :param refractory: refractory period in ms. \"\"\" if rates is None and target is None : Global . _error ( 'A PoissonPopulation must define either rates or target.' ) self . target = target self . parameters = parameters self . refractory_init = refractory self . rates_init = rates if target is not None : # hybrid population # Create the neuron poisson_neuron = Neuron ( parameters = \"\"\" %(params)s \"\"\" % { 'params' : parameters if parameters else '' }, equations = \"\"\" rates = sum( %(target)s ) p = Uniform(0.0, 1.0) * 1000.0 / dt _sum_ %(target)s = 0.0 \"\"\" % { 'target' : target }, spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Hybrid\" , description = \"Hybrid spiking neuron emitting spikes according to a Poisson distribution at a frequency determined by the weighted sum of inputs.\" ) elif isinstance ( rates , str ): # Create the neuron poisson_neuron = Neuron ( parameters = \"\"\" %(params)s \"\"\" % { 'params' : parameters if parameters else '' }, equations = \"\"\" rates = %(rates)s p = Uniform(0.0, 1.0) * 1000.0 / dt _sum_exc = 0.0 \"\"\" % { 'rates' : rates }, spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) elif isinstance ( rates , np . ndarray ): poisson_neuron = Neuron ( parameters = \"\"\" rates = 10.0 \"\"\" , equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) else : poisson_neuron = Neuron ( parameters = \"\"\" rates = %(rates)s \"\"\" % { 'rates' : rates }, equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < rates \"\"\" , refractory = refractory , name = \"Poisson\" , description = \"Spiking neuron with spikes emitted according to a Poisson distribution.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = poisson_neuron , name = name , copied = copied ) if isinstance ( rates , np . ndarray ): self . rates = rates","title":"__init__()"},{"location":"API/SpecificPopulation.html#ANNarchy.SpikeSourceArray","text":"Bases: SpecificPopulation Spike source generating spikes at the times given in the spike_times array. Depending on the initial array provided, the population will have one or several neurons, but the geometry can only be one-dimensional. You can later modify the spike_times attribute of the population, but it must have the same number of neurons as the initial one. The spike times are by default relative to the start of a simulation ( ANNarchy.get_time() is 0.0). If you call the reset() method of a SpikeSourceArray , this will set the spike times relative to the current time. You can then repeat a stimulation many times. # 2 neurons firing at 100Hz with a 1 ms delay times = [ [ 10 , 20 , 30 , 40 ], [ 11 , 21 , 31 , 41 ] ] inp = SpikeSourceArray ( spike_times = times ) compile () # Spikes at 10/11, 20/21, etc simulate ( 50 ) # Reset the internal time of the SpikeSourceArray inp . reset () # Spikes at 60/61, 70/71, etc simulate ( 50 ) Source code in ANNarchy/core/SpecificPopulation.py 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 class SpikeSourceArray ( SpecificPopulation ): \"\"\" Spike source generating spikes at the times given in the spike_times array. Depending on the initial array provided, the population will have one or several neurons, but the geometry can only be one-dimensional. You can later modify the spike_times attribute of the population, but it must have the same number of neurons as the initial one. The spike times are by default relative to the start of a simulation (``ANNarchy.get_time()`` is 0.0). If you call the ``reset()`` method of a ``SpikeSourceArray``, this will set the spike times relative to the current time. You can then repeat a stimulation many times. ```python # 2 neurons firing at 100Hz with a 1 ms delay times = [ [ 10, 20, 30, 40], [ 11, 21, 31, 41] ] inp = SpikeSourceArray(spike_times=times) compile() # Spikes at 10/11, 20/21, etc simulate(50) # Reset the internal time of the SpikeSourceArray inp.reset() # Spikes at 60/61, 70/71, etc simulate(50) ``` \"\"\" def __init__ ( self , spike_times , name = None , copied = False ): \"\"\" :param spike_times: a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt. :param name: optional name for the population. \"\"\" if not isinstance ( spike_times , list ): Global . _error ( 'In a SpikeSourceArray, spike_times must be a Python list.' ) if isinstance ( spike_times [ 0 ], list ): # several neurons nb_neurons = len ( spike_times ) else : # a single Neuron nb_neurons = 1 spike_times = [ spike_times ] # Create a fake neuron just to be sure the description has the correct parameters neuron = Neuron ( parameters = \"\" , equations = \"\" , spike = \" t == 0\" , reset = \"\" , name = \"Spike source\" , description = \"Spike source array.\" ) SpecificPopulation . __init__ ( self , geometry = nb_neurons , neuron = neuron , name = name , copied = copied ) self . init [ 'spike_times' ] = spike_times def _copy ( self ): \"Returns a copy of the population when creating networks.\" return SpikeSourceArray ( self . init [ 'spike_times' ], self . name , copied = True ) def _sort_spikes ( self , spike_times ): \"Sort, unify the spikes and transform them into steps.\" return [ sorted ( list ( set ([ round ( t / Global . config [ 'dt' ]) for t in neur_times ]))) for neur_times in spike_times ] def _generate_st ( self ): \"\"\" Code generation for single-thread. \"\"\" self . _generate_omp () def _generate_omp ( self ): \"\"\" Code generation for openMP paradigm. \"\"\" # Add possible targets for target in self . targets : tpl = { 'name' : 'g_ %(target)s ' % { 'target' : target }, 'locality' : 'local' , 'eq' : '' , 'bounds' : {}, 'flags' : [], 'ctype' : Global . config [ 'precision' ], 'init' : 0.0 , 'transformed_eq' : '' , 'pre_loop' : {}, 'cpp' : '' , 'switch' : '' , 'untouched' : {}, 'method' : 'exponential' , 'dependencies' : [] } self . neuron_type . description [ 'variables' ] . append ( tpl ) self . neuron_type . description [ 'local' ] . append ( 'g_' + target ) self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameter spike_times // std::vector< %(float_prec)s > r ; std::vector< std::vector< long int > > spike_times ; std::vector< long int > next_spike ; std::vector< int > idx_next_spike; long int _t; // Recompute the spike times void recompute_spike_times(){ std::fill(next_spike.begin(), next_spike.end(), -10000); std::fill(idx_next_spike.begin(), idx_next_spike.end(), 0); for(int i=0; i< size; i++){ if(!spike_times[i].empty()){ int idx = 0; // Find the first spike time which is not in the past while(spike_times[i][idx] < _t){ idx++; } // Set the next spike if(idx < spike_times[i].size()) next_spike[i] = spike_times[i][idx]; else next_spike[i] = -10000; } } } \"\"\" % { 'float_prec' : Global . config [ 'precision' ] } #self._specific_template['access_parameters_variables'] = \"\" self . _specific_template [ 'init_additional' ] = \"\"\" _t = 0; next_spike = std::vector<long int>(size, -10000); idx_next_spike = std::vector<int>(size, 0); this->recompute_spike_times(); \"\"\" self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; this->recompute_spike_times(); \"\"\" if Global . config [ \"num_threads\" ] == 1 : self . _specific_template [ 'update_variables' ] = \"\"\" if(_active){ spiked.clear(); for(int i = 0; i < size; i++){ // Emit spike if( _t == next_spike[i] ){ last_spike[i] = _t; idx_next_spike[i]++ ; if(idx_next_spike[i] < spike_times[i].size()){ next_spike[i] = spike_times[i][idx_next_spike[i]]; } spiked.push_back(i); } } _t++; } \"\"\" else : self . _specific_template [ 'update_variables' ] = \"\"\" if(_active){ #pragma omp single { spiked.clear(); } #pragma omp for for(int i = 0; i < size; i++){ // Emit spike if( _t == next_spike[i] ){ last_spike[i] = _t; idx_next_spike[i]++ ; if(idx_next_spike[i] < spike_times[i].size()){ next_spike[i] = spike_times[i][idx_next_spike[i]]; } #pragma omp critical spiked.push_back(i); } } #pragma omp single { _t++; } } \"\"\" self . _specific_template [ 'test_spike_cond' ] = \"\" self . _specific_template [ 'export_additional' ] = \"\"\" vector[vector[long]] spike_times void recompute_spike_times() \"\"\" self . _specific_template [ 'wrapper_args' ] = \"size, times, delay\" self . _specific_template [ 'wrapper_init' ] = \"\"\" pop %(id)s .spike_times = times pop %(id)s .set_size(size) pop %(id)s .set_max_delay(delay)\"\"\" % { 'id' : self . id } self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Local parameter spike_times cpdef get_spike_times(self): return pop %(id)s .spike_times cpdef set_spike_times(self, value): pop %(id)s .spike_times = value pop %(id)s .recompute_spike_times() \"\"\" % { 'id' : self . id } def _generate_cuda ( self ): \"\"\" Code generation for the CUDA paradigm. As the spike time generation is not a very compute intensive step but requires dynamic data structures, we don't implement it on the CUDA devices for now. Consequently, we use the CPU side implementation and transfer after computation the results to the GPU. \"\"\" self . _generate_st () # attach transfer of spiked array to gpu # IMPORTANT: the outside transfer is necessary. # Otherwise, previous spike counts will be not reseted. self . _specific_template [ 'update_variables' ] += \"\"\" if ( _active ) { // Update Spike Count on GPU spike_count = spiked.size(); cudaMemcpy( gpu_spike_count, &spike_count, sizeof(unsigned int), cudaMemcpyHostToDevice); // Transfer generated spikes to GPU if( spike_count > 0 ) { cudaMemcpy( gpu_spiked, spiked.data(), spike_count * sizeof(int), cudaMemcpyHostToDevice); } } \"\"\" self . _specific_template [ 'update_variable_body' ] = \"\" self . _specific_template [ 'update_variable_header' ] = \"\" self . _specific_template [ 'update_variable_call' ] = \"\"\" // host side update of neurons pop %(id)s .update(); \"\"\" % { 'id' : self . id } def _instantiate ( self , module ): # Create the Cython instance self . cyInstance = getattr ( module , self . class_name + '_wrapper' )( self . size , self . init [ 'spike_times' ], self . max_delay ) def __setattr__ ( self , name , value ): if name == 'spike_times' : if not isinstance ( value [ 0 ], list ): # several neurons value = [ value ] if not len ( value ) == self . size : Global . _error ( 'SpikeSourceArray: the size of the spike_times attribute must match the number of neurons in the population.' ) self . init [ 'spike_times' ] = value # when reset is called if self . initialized : self . cyInstance . set_spike_times ( self . _sort_spikes ( value )) else : Population . __setattr__ ( self , name , value ) def __getattr__ ( self , name ): if name == 'spike_times' : if self . initialized : return [ [ Global . config [ 'dt' ] * time for time in neur ] for neur in self . cyInstance . get_spike_times ()] else : return self . init [ 'spike_times' ] else : return Population . __getattribute__ ( self , name )","title":"SpikeSourceArray"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.SpikeSourceArray.__init__","text":"Parameters: Name Type Description Default spike_times a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt. required name optional name for the population. None Source code in ANNarchy/core/SpecificPopulation.py 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 def __init__ ( self , spike_times , name = None , copied = False ): \"\"\" :param spike_times: a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt. :param name: optional name for the population. \"\"\" if not isinstance ( spike_times , list ): Global . _error ( 'In a SpikeSourceArray, spike_times must be a Python list.' ) if isinstance ( spike_times [ 0 ], list ): # several neurons nb_neurons = len ( spike_times ) else : # a single Neuron nb_neurons = 1 spike_times = [ spike_times ] # Create a fake neuron just to be sure the description has the correct parameters neuron = Neuron ( parameters = \"\" , equations = \"\" , spike = \" t == 0\" , reset = \"\" , name = \"Spike source\" , description = \"Spike source array.\" ) SpecificPopulation . __init__ ( self , geometry = nb_neurons , neuron = neuron , name = name , copied = copied ) self . init [ 'spike_times' ] = spike_times","title":"__init__()"},{"location":"API/SpecificPopulation.html#ANNarchy.TimedArray","text":"Bases: SpecificPopulation Data structure holding sequential inputs for a rate-coded network. The input values are stored in the (recordable) attribute r , without any further processing. You will need to connect this population to another one using the connect_one_to_one() method. By default, the firing rate of this population will iterate over the different values step by step: inputs = np . array ( [ [ 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 ] ] ) inp = TimedArray ( rates = inputs ) pop = Population ( 10 , ... ) proj = Projection ( inp , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) compile () simulate ( 10. ) This creates a population of 10 neurons whose activity will change during the first 10*dt milliseconds of the simulation. After that delay, the last input will be kept (i.e. 1 for the last neuron). If you want the TimedArray to \"loop\" over the different input vectors, you can specify a period for the inputs: inp = TimedArray ( rates = inputs , period = 10. ) If the period is smaller than the length of the rates, the last inputs will not be set. If you do not want the inputs to be set at every step, but every 10 ms for example, youcan use the schedule argument: inp = TimedArray ( rates = inputs , schedule = 10. ) The input [1, 0, 0,...] will stay for 10 ms, then[0, 1, 0, ...] for the next 10 ms, etc... If you need a less regular schedule, you can specify it as a list of times: inp = TimedArray ( rates = inputs , schedule = [ 10. , 20. , 50. , 60. , 100. , 110. ]) The first input is set at t = 10 ms (r = 0.0 in the first 10 ms), the second at t = 20 ms, the third at t = 50 ms, etc. If you specify less times than in the array of rates, the last ones will be ignored. Scheduling can be combined with periodic cycling. Note that you can use the reset() method to manually reinitialize the TimedArray, times becoming relative to that call: simulate ( 100. ) # ten inputs are shown with a schedule of 10 ms inp . reset () simulate ( 100. ) # the same ten inputs are presented again. Source code in ANNarchy/core/SpecificPopulation.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 class TimedArray ( SpecificPopulation ): \"\"\" Data structure holding sequential inputs for a rate-coded network. The input values are stored in the (recordable) attribute `r`, without any further processing. You will need to connect this population to another one using the ``connect_one_to_one()`` method. By default, the firing rate of this population will iterate over the different values step by step: ```python inputs = np.array( [ [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1] ] ) inp = TimedArray(rates=inputs) pop = Population(10, ...) proj = Projection(inp, pop, 'exc') proj.connect_one_to_one(1.0) compile() simulate(10.) ``` This creates a population of 10 neurons whose activity will change during the first 10*dt milliseconds of the simulation. After that delay, the last input will be kept (i.e. 1 for the last neuron). If you want the TimedArray to \"loop\" over the different input vectors, you can specify a period for the inputs: ```python inp = TimedArray(rates=inputs, period=10.) ``` If the period is smaller than the length of the rates, the last inputs will not be set. If you do not want the inputs to be set at every step, but every 10 ms for example, youcan use the ``schedule`` argument: ```python inp = TimedArray(rates=inputs, schedule=10.) ``` The input [1, 0, 0,...] will stay for 10 ms, then[0, 1, 0, ...] for the next 10 ms, etc... If you need a less regular schedule, you can specify it as a list of times: ```python inp = TimedArray(rates=inputs, schedule=[10., 20., 50., 60., 100., 110.]) ``` The first input is set at t = 10 ms (r = 0.0 in the first 10 ms), the second at t = 20 ms, the third at t = 50 ms, etc. If you specify less times than in the array of rates, the last ones will be ignored. Scheduling can be combined with periodic cycling. Note that you can use the ``reset()`` method to manually reinitialize the TimedArray, times becoming relative to that call: ```python simulate(100.) # ten inputs are shown with a schedule of 10 ms inp.reset() simulate(100.) # the same ten inputs are presented again. ``` \"\"\" def __init__ ( self , rates , schedule = 0. , period = - 1. , name = None , copied = False ): \"\"\" :param rates: array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population. :param schedule: either a single value or a list of time points where inputs should be set. Default: every timestep. :param period: time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.). \"\"\" neuron = Neuron ( parameters = \"\" , equations = \" r = 0.0\" , name = \"Timed Array\" , description = \"Timed array source.\" ) # Geometry of the population geometry = rates . shape [ 1 :] # Check the schedule if isinstance ( schedule , ( int , float )): if float ( schedule ) <= 0.0 : schedule = Global . config [ 'dt' ] schedule = [ float ( schedule * i ) for i in range ( rates . shape [ 0 ])] if len ( schedule ) > rates . shape [ 0 ]: Global . _error ( 'TimedArray: the length of the schedule parameter cannot exceed the first dimension of the rates parameter.' ) if len ( schedule ) < rates . shape [ 0 ]: Global . _warning ( 'TimedArray: the length of the schedule parameter is smaller than the first dimension of the rates parameter (more data than time points). Make sure it is what you expect.' ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = neuron , name = name , copied = copied ) self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'period' ] = period def _copy ( self ): \"Returns a copy of the population when creating networks.\" return TimedArray ( self . init [ 'rates' ] , self . init [ 'schedule' ], self . init [ 'period' ], self . name , copied = True ) def _generate_st ( self ): \"\"\" adjust code templates for the specific population for single thread and openMP. \"\"\" self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameters of a TimedArray std::vector< int > _schedule; // List of times where new inputs should be set std::vector< std::vector< %(float_prec)s > > _buffer; // buffer holding the data int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameters of a TimedArray void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_buffer(std::vector< std::vector< %(float_prec)s > > buffer) { _buffer = buffer; r = _buffer[0]; } std::vector< std::vector< %(float_prec)s > > get_buffer() { return _buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // Initialize counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters of a TimedArray void set_schedule(vector[int]) vector[int] get_schedule() void set_buffer(vector[vector[ %(float_prec)s ]]) vector[vector[ %(float_prec)s ]] get_buffer() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; _block = 0; r.clear(); r = std::vector< %(float_prec)s >(size, 0.0); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters of a TimedArray cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_rates( self, buffer ): pop %(id)s .set_buffer( buffer ) cpdef np.ndarray get_rates( self ): return np.array(pop %(id)s .get_buffer( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_period(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id } self . _specific_template [ 'update_variables' ] = \"\"\" if(_active){ //std::cout << _t << \" \" << _block<< \" \" << _schedule[_block] << std::endl; // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data r = _buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if (_block == _schedule.size()){ _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if(_period > -1 && (_t == _period-1)){ // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } \"\"\" self . _specific_template [ 'size_in_bytes' ] = \"\"\" // schedule size_in_bytes += _schedule.capacity() * sizeof(int); // buffer size_in_bytes += _buffer.capacity() * sizeof(std::vector< %(float_prec)s >); for( auto it = _buffer.begin(); it != _buffer.end(); it++ ) size_in_bytes += it->capacity() * sizeof( %(float_prec)s ); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} def _generate_omp ( self ): \"\"\" adjust code templates for the specific population for single thread and openMP. \"\"\" self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameters of a TimedArray std::vector< int > _schedule; // List of times where new inputs should be set std::vector< std::vector< %(float_prec)s > > _buffer; // buffer holding the data int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameters of a TimedArray void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_buffer(std::vector< std::vector< %(float_prec)s > > buffer) { _buffer = buffer; r = _buffer[0]; } std::vector< std::vector< %(float_prec)s > > get_buffer() { return _buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // Initialize counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters of a TimedArray void set_schedule(vector[int]) vector[int] get_schedule() void set_buffer(vector[vector[ %(float_prec)s ]]) vector[vector[ %(float_prec)s ]] get_buffer() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; _block = 0; r.clear(); r = std::vector< %(float_prec)s >(size, 0.0); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters of a TimedArray cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_rates( self, buffer ): pop %(id)s .set_buffer( buffer ) cpdef np.ndarray get_rates( self ): return np.array(pop %(id)s .get_buffer( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_period(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id } self . _specific_template [ 'update_variables' ] = \"\"\" if(_active){ #pragma omp single { // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data r = _buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if (_block == _schedule.size()){ _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if(_period > -1 && (_t == _period-1)){ // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } } \"\"\" self . _specific_template [ 'size_in_bytes' ] = \"\"\" // schedule size_in_bytes += _schedule.capacity() * sizeof(int); // buffer size_in_bytes += _buffer.capacity() * sizeof(std::vector< %(float_prec)s >); for( auto it = _buffer.begin(); it != _buffer.end(); it++ ) size_in_bytes += it->capacity() * sizeof( %(float_prec)s ); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} def _generate_cuda ( self ): \"\"\" adjust code templates for the specific population for single thread and CUDA. \"\"\" # HD (18. Nov 2016) # I suppress the code generation for allocating the variable r on gpu, as # well as memory transfer codes. This is only possible as no other variables # allowed in TimedArray. self . _specific_template [ 'init_parameters_variables' ] = \"\" self . _specific_template [ 'host_device_transfer' ] = \"\" self . _specific_template [ 'device_host_transfer' ] = \"\" # # Code for handling the buffer and schedule parameters self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameter timed array std::vector< int > _schedule; std::vector< %(float_prec)s * > gpu_buffer; int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameter timed array void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_buffer(std::vector< std::vector< %(float_prec)s > > buffer) { if ( gpu_buffer.empty() ) { gpu_buffer = std::vector< %(float_prec)s * >(buffer.size(), nullptr); // allocate gpu arrays for(int i = 0; i < buffer.size(); i++) { cudaMalloc((void**)&gpu_buffer[i], buffer[i].size()*sizeof( %(float_prec)s )); } } auto host_it = buffer.begin(); auto dev_it = gpu_buffer.begin(); for (; host_it < buffer.end(); host_it++, dev_it++) { cudaMemcpy( *dev_it, host_it->data(), host_it->size()*sizeof( %(float_prec)s ), cudaMemcpyHostToDevice); } gpu_r = gpu_buffer[0]; } std::vector< std::vector< %(float_prec)s > > get_buffer() { std::vector< std::vector< %(float_prec)s > > buffer = std::vector< std::vector< %(float_prec)s > >( gpu_buffer.size(), std::vector< %(float_prec)s >(size,0.0) ); auto host_it = buffer.begin(); auto dev_it = gpu_buffer.begin(); for (; host_it < buffer.end(); host_it++, dev_it++) { cudaMemcpy( host_it->data(), *dev_it, size*sizeof( %(float_prec)s ), cudaMemcpyDeviceToHost ); } return buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'reset_additional' ] = \"\"\" // counters _t = 0; _block = 0; gpu_r = gpu_buffer[0]; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters timed array void set_schedule(vector[int]) vector[int] get_schedule() void set_buffer(vector[vector[ %(float_prec)s ]]) vector[vector[ %(float_prec)s ]] get_buffer() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters timed array cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_rates( self, buffer ): pop %(id)s .set_buffer( buffer ) cpdef np.ndarray get_rates( self ): return np.array(pop %(id)s .get_buffer( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_periodic(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'update_variables' ] = \"\"\" if(_active) { // std::cout << _t << \" \" << _block<< \" \" << _schedule[_block] << std::endl; // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data gpu_r = gpu_buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if ( _block == _schedule.size() ) { _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if( (_period > -1) && (_t == _period-1) ) { // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } \"\"\" self . _specific_template [ 'update_variable_body' ] = \"\" self . _specific_template [ 'update_variable_header' ] = \"\" self . _specific_template [ 'update_variable_call' ] = \"\"\" // host side update of neurons pop %(id)s .update(); \"\"\" % { 'id' : self . id } self . _specific_template [ 'size_in_bytes' ] = \"//TODO: \" def _instantiate ( self , module ): # Create the Cython instance self . cyInstance = getattr ( module , self . class_name + '_wrapper' )( self . size , self . max_delay ) def __setattr__ ( self , name , value ): if name == 'schedule' : if self . initialized : self . cyInstance . set_schedule ( np . array ( value ) / Global . config [ 'dt' ] ) else : self . init [ 'schedule' ] = value elif name == 'rates' : if self . initialized : if len ( value . shape ) > 2 : # we need to flatten the provided data flat_values = value . reshape ( ( value . shape [ 0 ], self . size ) ) self . cyInstance . set_rates ( flat_values ) else : self . cyInstance . set_rates ( value ) else : self . init [ 'rates' ] = value elif name == \"period\" : if self . initialized : self . cyInstance . set_period ( int ( value / Global . config [ 'dt' ])) else : self . init [ 'period' ] = value else : Population . __setattr__ ( self , name , value ) def __getattr__ ( self , name ): if name == 'schedule' : if self . initialized : return Global . config [ 'dt' ] * self . cyInstance . get_schedule () else : return self . init [ 'schedule' ] elif name == 'rates' : if self . initialized : if len ( self . geometry ) > 1 : # unflatten the data flat_values = self . cyInstance . get_rates () values = np . zeros ( tuple ( [ len ( self . schedule )] + list ( self . geometry ) ) ) for x in range ( len ( self . schedule )): values [ x ] = np . reshape ( flat_values [ x ], self . geometry ) return values else : return self . cyInstance . get_rates () else : return self . init [ 'rates' ] elif name == 'period' : if self . initialized : return self . cyInstance . get_period () * Global . config [ 'dt' ] else : return self . init [ 'period' ] else : return Population . __getattribute__ ( self , name )","title":"TimedArray"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.TimedArray.__init__","text":"Parameters: Name Type Description Default rates array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population. required schedule either a single value or a list of time points where inputs should be set. Default: every timestep. 0.0 period time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.). -1.0 Source code in ANNarchy/core/SpecificPopulation.py 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def __init__ ( self , rates , schedule = 0. , period = - 1. , name = None , copied = False ): \"\"\" :param rates: array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population. :param schedule: either a single value or a list of time points where inputs should be set. Default: every timestep. :param period: time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.). \"\"\" neuron = Neuron ( parameters = \"\" , equations = \" r = 0.0\" , name = \"Timed Array\" , description = \"Timed array source.\" ) # Geometry of the population geometry = rates . shape [ 1 :] # Check the schedule if isinstance ( schedule , ( int , float )): if float ( schedule ) <= 0.0 : schedule = Global . config [ 'dt' ] schedule = [ float ( schedule * i ) for i in range ( rates . shape [ 0 ])] if len ( schedule ) > rates . shape [ 0 ]: Global . _error ( 'TimedArray: the length of the schedule parameter cannot exceed the first dimension of the rates parameter.' ) if len ( schedule ) < rates . shape [ 0 ]: Global . _warning ( 'TimedArray: the length of the schedule parameter is smaller than the first dimension of the rates parameter (more data than time points). Make sure it is what you expect.' ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = neuron , name = name , copied = copied ) self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'period' ] = period","title":"__init__()"},{"location":"API/SpecificPopulation.html#ANNarchy.HomogeneousCorrelatedSpikeTrains","text":"Bases: SpecificPopulation Population of spiking neurons following a homogeneous distribution with correlated spike trains. The method describing the generation of homogeneous correlated spike trains is described in: Brette, R. (2009). Generation of correlated spike trains. Neural Computation 21(1). http://romainbrette.fr/WordPress3/wp-content/uploads/2014/06/Brette2008NC.pdf The implementation is based on the one provided by Brian http://briansimulator.org . To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation: \\[\\frac{dx}{dt} = \\frac{(\\mu - x)}{\\tau} + \\sigma \\, \\frac{\\xi}{\\sqrt{\\tau}}\\] where \\(\\xi\\) is a random variable. In short, \\(x\\) will randomly vary around mu over time, with an amplitude determined by sigma and a speed determined by tau. This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process. To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates , the desired correlation strength corr and the time constant tau . See Brette's paper for details. In short, you should only define the parameters rates , corr and tau , and let the class compute mu and sigma for you. Changing rates , corr or tau after initialization automatically recomputes mu and sigma. Example: from ANNarchy import * setup ( dt = 0.1 ) pop_corr = HomogeneousCorrelatedSpikeTrains ( 200 , rates = 10. , corr = 0.3 , tau = 10. ) compile () simulate ( 1000. ) pop_corr . rates = 30. simulate ( 1000. ) Alternatively, a schedule can be provided to change automatically the value of rates and corr (but not tau ) at the required times (as in TimedArray or TimedPoissonPopulation): from ANNarchy import * setup ( dt = 0.1 ) pop_corr = HomogeneousCorrelatedSpikeTrains ( geometry = 200 , rates = [ 10. , 30. ], corr = [ 0.3 , 0.5 ], tau = 10. , schedule = [ 0. , 1000. ] ) compile () simulate ( 2000. ) Even when using a schedule, corr accepts a single constant value. The first value of schedule must be 0. period specifies when the schedule \"loops\" back to its initial value. Source code in ANNarchy/core/SpecificPopulation.py 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 class HomogeneousCorrelatedSpikeTrains ( SpecificPopulation ): \"\"\" Population of spiking neurons following a homogeneous distribution with correlated spike trains. The method describing the generation of homogeneous correlated spike trains is described in: > Brette, R. (2009). Generation of correlated spike trains. Neural Computation 21(1). <http://romainbrette.fr/WordPress3/wp-content/uploads/2014/06/Brette2008NC.pdf> The implementation is based on the one provided by Brian <http://briansimulator.org>. To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation: $$\\\\frac{dx}{dt} = \\\\frac{(\\\\mu - x)}{\\\\tau} + \\\\sigma \\\\, \\\\frac{\\\\xi}{\\\\sqrt{\\\\tau}}$$ where $\\\\xi$ is a random variable. In short, $x$ will randomly vary around mu over time, with an amplitude determined by sigma and a speed determined by tau. This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process. To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate **rates**, the desired correlation strength **corr** and the time constant **tau**. See Brette's paper for details. In short, you should only define the parameters ``rates``, ``corr`` and ``tau``, and let the class compute mu and sigma for you. Changing ``rates``, ``corr`` or ``tau`` after initialization automatically recomputes mu and sigma. Example: ```python from ANNarchy import * setup(dt=0.1) pop_corr = HomogeneousCorrelatedSpikeTrains(200, rates=10., corr=0.3, tau=10.) compile() simulate(1000.) pop_corr.rates=30. simulate(1000.) ``` Alternatively, a schedule can be provided to change automatically the value of `rates` and ``corr`` (but not ``tau``) at the required times (as in TimedArray or TimedPoissonPopulation): ```python from ANNarchy import * setup(dt=0.1) pop_corr = HomogeneousCorrelatedSpikeTrains( geometry=200, rates= [10., 30.], corr=[0.3, 0.5], tau=10., schedule=[0., 1000.] ) compile() simulate(2000.) ``` Even when using a schedule, ``corr`` accepts a single constant value. The first value of ``schedule`` must be 0. ``period`` specifies when the schedule \"loops\" back to its initial value. \"\"\" def __init__ ( self , geometry , rates , corr , tau , schedule = None , period =- 1. , name = None , refractory = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. :param rates: rate in Hz of the population (must be a positive float or a list) :param corr: total correlation strength (float in [0, 1], or a list) :param tau: correlation time constant in ms. :param schedule: list of times where new values of ``rates``and ``corr``will be used to computre mu and sigma. :param period: time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.) :param name: unique name of the population (optional). :param refractory: refractory period in ms (careful: may break the correlation) \"\"\" if schedule is not None : self . _has_schedule = True # Rates if not isinstance ( rates , ( list , np . ndarray )): Global . _error ( \"TimedHomogeneousCorrelatedSpikeTrains: the rates argument must be a list or a numpy array.\" ) rates = np . array ( rates ) # Schedule schedule = np . array ( schedule ) nb_schedules = rates . shape [ 0 ] if nb_schedules != schedule . size : Global . _error ( \"TimedHomogeneousCorrelatedSpikeTrains: the length of rates must be the same length as for schedule.\" ) # corr corr = np . array ( corr ) if corr . size == 1 : corr = np . full ( nb_schedules , corr ) else : self . _has_schedule = False rates = np . array ([ float ( rates )]) schedule = np . array ([ 0.0 ]) corr = np . array ([ corr ]) # Store refractory self . refractory_init = refractory # Correction of mu and sigma mu_list , sigma_list = self . _correction ( rates , corr , tau ) self . rates = rates self . corr = corr self . tau = tau # Create the neuron corr_neuron = Neuron ( parameters = \"\"\" tau = %(tau)s : population mu = %(mu)s : population sigma = %(sigma)s : population \"\"\" % { 'tau' : tau , 'mu' : mu_list [ 0 ], 'sigma' : sigma_list [ 0 ]}, equations = \"\"\" x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) : population, init= %(mu)s p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" % { 'mu' : mu_list [ 0 ]}, spike = \"p < x\" , refractory = refractory , name = \"HomogeneousCorrelated\" , description = \"Homogeneous correlated spike trains.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = corr_neuron , name = name , copied = copied ) # Initial values self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'corr' ] = corr self . init [ 'tau' ] = tau self . init [ 'period' ] = period if self . _has_schedule : self . init [ 'mu' ] = mu_list self . init [ 'sigma' ] = sigma_list else : self . init [ 'mu' ] = mu_list [ 0 ] self . init [ 'sigma' ] = sigma_list [ 0 ] def _copy ( self ): \"Returns a copy of the population when creating networks.\" return HomogeneousCorrelatedSpikeTrains ( geometry = self . geometry , rates = self . init [ 'rates' ], corr = self . init [ 'corr' ], tau = self . init [ 'tau' ], schedule = self . init [ 'schedule' ], period = self . init [ 'period' ], name = self . name , refractory = self . refractory_init , copied = True ) def _correction ( self , rates , corr , tau ): # Correction of mu and sigma mu_list = [] sigma_list = [] for i in range ( len ( rates )): if isinstance ( corr , list ): c = corr [ i ] else : c = float ( corr ) mu , sigma = _rectify ( rates [ i ], c , tau ) mu_list . append ( mu ) sigma_list . append ( sigma ) return mu_list , sigma_list def _generate_st ( self ): \"\"\" adjust code templates for the specific population for single thread. \"\"\" self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameters of a HomogeneousCorrelatedSpikeTrains std::vector< int > _schedule; // List of times where new inputs should be set std::vector< %(float_prec)s > _mu; // buffer holding the data std::vector< %(float_prec)s > _sigma; // buffer holding the data int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameters of a HomogeneousCorrelatedSpikeTrains void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_mu_list(std::vector< %(float_prec)s > buffer) { _mu = buffer; mu = _mu[0]; } std::vector< %(float_prec)s > get_mu_list() { return _mu; } void set_sigma_list(std::vector< %(float_prec)s > buffer) { _sigma = buffer; sigma = _sigma[0]; } std::vector< %(float_prec)s > get_sigma_list() { return _sigma; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // Initialize counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters of a HomogeneousCorrelatedSpikeTrains void set_schedule(vector[int]) vector[int] get_schedule() void set_mu_list(vector[ %(float_prec)s ]) vector[ %(float_prec)s ] get_mu_list() void set_sigma_list(vector[ %(float_prec)s ]) vector[ %(float_prec)s ] get_sigma_list() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; _block = 0; r.clear(); r = std::vector< %(float_prec)s >(size, 0.0); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters of a HomogeneousCorrelatedSpikeTrains cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_mu_list( self, buffer ): pop %(id)s .set_mu_list( buffer ) cpdef np.ndarray get_mu_list( self ): return np.array(pop %(id)s .get_mu_list( )) cpdef set_sigma_list( self, buffer ): pop %(id)s .set_sigma_list( buffer ) cpdef np.ndarray get_sigma_list( self ): return np.array(pop %(id)s .get_sigma_list( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_period(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id } scheduling_block = \"\"\" if(_active){ // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data mu = _mu[_block]; sigma = _sigma[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if (_block == _schedule.size()){ _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if(_period > -1 && (_t == _period-1)){ // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } \"\"\" update_block = \"\"\" if( _active ) { spiked.clear(); // x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) x += dt*(mu - x)/tau + rand_0*sigma*sqrt(dt/tau); %(float_prec)s _step = 1000.0/dt; #pragma omp simd for(int i = 0; i < size; i++){ // p = Uniform(0.0, 1.0) * 1000.0 / dt p[i] = _step*rand_1[i]; } } // active \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} if self . _has_schedule : self . _specific_template [ 'update_variables' ] = scheduling_block + update_block else : self . _specific_template [ 'update_variables' ] = update_block self . _specific_template [ 'size_in_bytes' ] = \"\"\" // schedule size_in_bytes += _schedule.capacity() * sizeof(int); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} def _generate_omp ( self ): \"\"\" adjust code templates for the specific population for openMP. \"\"\" self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameters of a HomogeneousCorrelatedSpikeTrains std::vector< int > _schedule; // List of times where new inputs should be set std::vector< %(float_prec)s > _mu; // buffer holding the data std::vector< %(float_prec)s > _sigma; // buffer holding the data int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameters of a HomogeneousCorrelatedSpikeTrains void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_mu_list(std::vector< %(float_prec)s > buffer) { _mu = buffer; mu = _mu[0]; } std::vector< %(float_prec)s > get_mu_list() { return _mu; } void set_sigma_list(std::vector< %(float_prec)s > buffer) { _sigma = buffer; sigma = _sigma[0]; } std::vector< %(float_prec)s > get_sigma_list() { return _sigma; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // Initialize counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters of a HomogeneousCorrelatedSpikeTrains void set_schedule(vector[int]) vector[int] get_schedule() void set_mu_list(vector[ %(float_prec)s ]) vector[ %(float_prec)s ] get_mu_list() void set_sigma_list(vector[ %(float_prec)s ]) vector[ %(float_prec)s ] get_sigma_list() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; _block = 0; r.clear(); r = std::vector< %(float_prec)s >(size, 0.0); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters of a HomogeneousCorrelatedSpikeTrains cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_mu_list( self, buffer ): pop %(id)s .set_mu_list( buffer ) cpdef np.ndarray get_mu_list( self ): return np.array(pop %(id)s .get_mu_list( )) cpdef set_sigma_list( self, buffer ): pop %(id)s .set_sigma_list( buffer ) cpdef np.ndarray get_sigma_list( self ): return np.array(pop %(id)s .get_sigma_list( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_period(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id } scheduling_block = \"\"\" if(_active){ // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data mu = _mu[_block]; sigma = _sigma[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if (_block == _schedule.size()){ _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if(_period > -1 && (_t == _period-1)){ // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } \"\"\" update_block = \"\"\" if( _active ) { #pragma omp single { spiked.clear(); // x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) x += dt*(mu - x)/tau + rand_0*sigma*sqrt(dt/tau); %(float_prec)s _step = 1000.0/dt; #pragma omp simd for(int i = 0; i < size; i++){ // p = Uniform(0.0, 1.0) * 1000.0 / dt p[i] = _step*rand_1[i]; } } } // active \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} if self . _has_schedule : self . _specific_template [ 'update_variables' ] = scheduling_block + update_block else : self . _specific_template [ 'update_variables' ] = update_block self . _specific_template [ 'size_in_bytes' ] = \"\"\" // schedule size_in_bytes += _schedule.capacity() * sizeof(int); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} def _generate_cuda ( self ): \"\"\" Code generation if the CUDA paradigm is set. \"\"\" # # Code for handling the buffer and schedule parameters self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameter HomogeneousCorrelatedSpikeTrains std::vector< int > _schedule; std::vector< %(float_prec)s > mu_buffer; // buffer std::vector< %(float_prec)s > sigma_buffer; // buffer int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameter HomogeneousCorrelatedSpikeTrains void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_mu_list(std::vector< %(float_prec)s > buffer) { mu_buffer = buffer; } void set_sigma_list(std::vector< %(float_prec)s > buffer) { sigma_buffer = buffer; } std::vector< %(float_prec)s > get_mu_list() { return mu_buffer; } std::vector< %(float_prec)s > get_sigma_list() { return sigma_buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ], 'id' : self . id } self . _specific_template [ 'init_additional' ] = \"\"\" // counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'reset_additional' ] = \"\"\" // counters _t = 0; _block = 0; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters timed array void set_schedule(vector[int]) vector[int] get_schedule() void set_mu_list(vector[ %(float_prec)s ]) vector[ %(float_prec)s ] get_mu_list() void set_sigma_list(vector[ %(float_prec)s ]) vector[ %(float_prec)s ] get_sigma_list() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters timed array cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_mu_list( self, buffer ): pop %(id)s .set_mu_list( buffer ) cpdef np.ndarray get_mu_list( self ): return np.array(pop %(id)s .get_mu_list( )) cpdef set_sigma_list( self, buffer ): pop %(id)s .set_sigma_list( buffer ) cpdef np.ndarray get_sigma_list( self ): return np.array(pop %(id)s .get_sigma_list( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_periodic(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id , 'float_prec' : Global . config [ 'precision' ] } if not self . _has_schedule : # we can use the normal code generation for GPU kernels pass else : self . _specific_template [ 'update_variables' ] = \"\"\" if(_active) { // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data mu = mu_buffer[_block]; sigma = sigma_buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if ( _block == _schedule.size() ) { _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if( (_period > -1) && (_t == _period-1) ) { // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } \"\"\" self . _specific_template [ 'update_variable_body' ] = \"\"\" // Updating global variables of population %(id)s __global__ void cuPop %(id)s _global_step( const long int t, const double dt, const double tau, double mu, double* x, curandState* rand_0, double sigma ) { // x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) x[0] += dt*(mu - x[0])/tau + curand_normal_double( &rand_0[0] )*sigma*sqrt(dt/tau); } // Updating local variables of population %(id)s __global__ void cuPop %(id)s _local_step( const long int t, const double dt, curandState* rand_1, double* x, unsigned int* num_events, int* spiked, long int* last_spike ) { int i = threadIdx.x + blockDim.x * blockIdx.x; %(float_prec)s step = 1000.0/dt; while ( i < %(size)s ) { // p = Uniform(0.0, 1.0) * 1000.0 / dt %(float_prec)s p = curand_uniform_double( &rand_1[i] ) * step; if (p < x[0]) { int pos = atomicAdd ( num_events, 1); spiked[pos] = i; last_spike[i] = t; } i += blockDim.x; } __syncthreads(); } \"\"\" % { 'id' : self . id , 'size' : self . size , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'update_variable_header' ] = \"\"\"__global__ void cuPop %(id)s _global_step( const long int t, const double dt, const double tau, double mu, double* x, curandState* rand_0, double sigma ); __global__ void cuPop %(id)s _local_step( const long int t, const double dt, curandState* rand_1, double* x, unsigned int* num_events, int* spiked, long int* last_spike ); \"\"\" % { 'id' : self . id } # Please notice, that the GPU kernels can be launched only with one block. Otherwise, the # atomicAdd which is called inside the kernel is not working correct (HD: April 1st, 2021) self . _specific_template [ 'update_variable_call' ] = \"\"\" if (pop %(id)s ._active) { // Update the scheduling pop %(id)s .update(); // Reset old events clear_num_events<<< 1, 1, 0, pop %(id)s .stream >>>(pop %(id)s .gpu_spike_count); #ifdef _DEBUG cudaError_t err_clear_num_events_ %(id)s = cudaGetLastError(); if (err_clear_num_events_ %(id)s != cudaSuccess) std::cout << \"pop %(id)s _spike_gather: \" << cudaGetErrorString(err_clear_num_events_ %(id)s ) << std::endl; #endif // compute the value of x based on mu/sigma cuPop %(id)s _global_step<<< 1, 1, 0, pop %(id)s .stream >>>( t, dt, pop %(id)s .tau, pop %(id)s .mu, pop %(id)s .gpu_x, pop %(id)s .gpu_rand_0, pop %(id)s .sigma ); #ifdef _DEBUG cudaError_t err_pop %(id)s _global_step = cudaGetLastError(); if( err_pop %(id)s _global_step != cudaSuccess) { std::cout << \"pop %(id)s _step: \" << cudaGetErrorString(err_pop %(id)s _global_step) << std::endl; exit(0); } #endif // Generate new spike events cuPop %(id)s _local_step<<< 1, pop %(id)s ._threads_per_block, 0, pop %(id)s .stream >>>( t, dt, pop %(id)s .gpu_rand_1, pop %(id)s .gpu_x, pop %(id)s .gpu_spike_count, pop %(id)s .gpu_spiked, pop %(id)s .gpu_last_spike ); #ifdef _DEBUG cudaError_t err_pop_spike_gather_ %(id)s = cudaGetLastError(); if(err_pop_spike_gather_ %(id)s != cudaSuccess) { std::cout << \"pop %(id)s _spike_gather: \" << cudaGetErrorString(err_pop_spike_gather_ %(id)s ) << std::endl; exit(0); } #endif // transfer back the spike counter (needed by record) cudaMemcpy( &pop %(id)s .spike_count, pop %(id)s .gpu_spike_count, sizeof(unsigned int), cudaMemcpyDeviceToHost); #ifdef _DEBUG cudaError_t err_pop %(id)s _async_copy = cudaGetLastError(); if ( err_pop %(id)s _async_copy != cudaSuccess ) { std::cout << \"record_spike_count: \" << cudaGetErrorString(err_pop %(id)s _async_copy) << std::endl; exit(0); } #endif // transfer back the spiked array (needed by record) if (pop %(id)s .spike_count > 0) { cudaMemcpy( pop %(id)s .spiked.data(), pop %(id)s .gpu_spiked, pop %(id)s .spike_count*sizeof(int), cudaMemcpyDeviceToHost); #ifdef _DEBUG cudaError_t err_pop %(id)s _async_copy2 = cudaGetLastError(); if ( err_pop %(id)s _async_copy2 != cudaSuccess ) { std::cout << \"record_spike: \" << cudaGetErrorString(err_pop %(id)s _async_copy2) << std::endl; exit(0); } #endif } } \"\"\" % { 'id' : self . id } self . _specific_template [ 'size_in_bytes' ] = \"//TODO: \" def _instantiate ( self , module ): # Create the Cython instance self . cyInstance = getattr ( module , self . class_name + '_wrapper' )( self . size , self . max_delay ) def __setattr__ ( self , name , value ): if not hasattr ( self , 'initialized' ): Population . __setattr__ ( self , name , value ) elif name == 'schedule' : if self . initialized : self . cyInstance . set_schedule ( np . array ( value ) / Global . config [ 'dt' ] ) else : self . init [ 'schedule' ] = value elif name == 'mu' : if self . initialized : if self . _has_schedule : self . cyInstance . set_mu_list ( value ) else : self . cyInstance . set_global_attribute ( \"mu\" , value , Global . config [ \"precision\" ] ) else : self . init [ 'mu' ] = value elif name == 'sigma' : if self . initialized : if self . _has_schedule : self . cyInstance . set_sigma_list ( value ) else : self . cyInstance . set_global_attribute ( \"sigma\" , value , Global . config [ \"precision\" ] ) else : self . init [ 'sigma' ] = value elif name == \"period\" : if self . initialized : self . cyInstance . set_period ( int ( value / Global . config [ 'dt' ])) else : self . init [ 'period' ] = value elif name == 'rates' : if self . _has_schedule : value = np . array ( value ) if not value . size == self . schedule . size : Global . _error ( \"HomogeneousCorrelatedSpikeTrains: rates must have the same length as schedule.\" ) else : value = np . array ([ float ( value )]) if self . initialized : Population . __setattr__ ( self , name , value ) # Correction of mu and sigma everytime r, c or tau is changed try : mu , sigma = self . _correction ( self . rates , self . corr , self . tau ) if self . _has_schedule : self . mu = mu self . sigma = sigma else : self . mu = mu [ 0 ] self . sigma = sigma [ 0 ] except Exception as e : print ( e ) else : self . init [ name ] = value Population . __setattr__ ( self , name , value ) elif name == 'corr' : if self . _has_schedule : if not isinstance ( value , ( list , np . ndarray )): value = np . full (( self . schedule . size , ), value ) else : value = np . array ( value ) if not value . size == self . schedule . size : Global . _error ( \"HomogeneousCorrelatedSpikeTrains: corr must have the same length as schedule.\" ) else : value = np . array ([ float ( value )]) if self . initialized : Population . __setattr__ ( self , name , value ) try : # Correction of mu and sigma everytime r, c or tau is changed mu , sigma = self . _correction ( self . rates , self . corr , self . tau ) if self . _has_schedule : self . mu = mu self . sigma = sigma else : self . mu = mu [ 0 ] self . sigma = sigma [ 0 ] except Exception as e : print ( e ) else : self . init [ name ] = value Population . __setattr__ ( self , name , value ) elif name == 'tau' : if self . initialized : Population . __setattr__ ( self , name , value ) # Correction of mu and sigma everytime r, c or tau is changed mu , sigma = self . _correction ( self . rates , self . corr , self . tau ) if self . _has_schedule : self . mu = mu self . sigma = sigma else : self . mu = mu [ 0 ] self . sigma = sigma [ 0 ] else : self . init [ name ] = value Population . __setattr__ ( self , name , value ) else : Population . __setattr__ ( self , name , value ) def __getattr__ ( self , name ): if name == 'schedule' : if self . initialized : if self . _has_schedule : return Global . config [ 'dt' ] * self . cyInstance . get_schedule () else : return np . array ([ 0.0 ]) else : return self . init [ 'schedule' ] elif name == 'mu' : if self . initialized : if self . _has_schedule : return self . cyInstance . get_mu_list () else : return self . cyInstance . get_global_attribute ( \"mu\" , Global . config [ \"precision\" ] ) else : return self . init [ 'mu' ] elif name == 'sigma' : if self . initialized : if self . _has_schedule : return self . cyInstance . get_sigma_list () else : return self . cyInstance . get_global_attribute ( \"sigma\" , Global . config [ \"precision\" ] ) else : return self . init [ 'sigma' ] elif name == 'tau' : if self . initialized : return self . cyInstance . get_global_attribute ( \"tau\" , Global . config [ \"precision\" ] ) else : return self . init [ 'tau' ] elif name == 'period' : if self . initialized : return self . cyInstance . get_period () * Global . config [ 'dt' ] else : return self . init [ 'period' ] else : return Population . __getattribute__ ( self , name )","title":"HomogeneousCorrelatedSpikeTrains"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.HomogeneousCorrelatedSpikeTrains.__init__","text":"Parameters: Name Type Description Default geometry population geometry as tuple. required rates rate in Hz of the population (must be a positive float or a list) required corr total correlation strength (float in [0, 1], or a list) required tau correlation time constant in ms. required schedule list of times where new values of rates and corr will be used to computre mu and sigma. None period time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.) -1.0 name unique name of the population (optional). None refractory refractory period in ms (careful: may break the correlation) None Source code in ANNarchy/core/SpecificPopulation.py 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 def __init__ ( self , geometry , rates , corr , tau , schedule = None , period =- 1. , name = None , refractory = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. :param rates: rate in Hz of the population (must be a positive float or a list) :param corr: total correlation strength (float in [0, 1], or a list) :param tau: correlation time constant in ms. :param schedule: list of times where new values of ``rates``and ``corr``will be used to computre mu and sigma. :param period: time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.) :param name: unique name of the population (optional). :param refractory: refractory period in ms (careful: may break the correlation) \"\"\" if schedule is not None : self . _has_schedule = True # Rates if not isinstance ( rates , ( list , np . ndarray )): Global . _error ( \"TimedHomogeneousCorrelatedSpikeTrains: the rates argument must be a list or a numpy array.\" ) rates = np . array ( rates ) # Schedule schedule = np . array ( schedule ) nb_schedules = rates . shape [ 0 ] if nb_schedules != schedule . size : Global . _error ( \"TimedHomogeneousCorrelatedSpikeTrains: the length of rates must be the same length as for schedule.\" ) # corr corr = np . array ( corr ) if corr . size == 1 : corr = np . full ( nb_schedules , corr ) else : self . _has_schedule = False rates = np . array ([ float ( rates )]) schedule = np . array ([ 0.0 ]) corr = np . array ([ corr ]) # Store refractory self . refractory_init = refractory # Correction of mu and sigma mu_list , sigma_list = self . _correction ( rates , corr , tau ) self . rates = rates self . corr = corr self . tau = tau # Create the neuron corr_neuron = Neuron ( parameters = \"\"\" tau = %(tau)s : population mu = %(mu)s : population sigma = %(sigma)s : population \"\"\" % { 'tau' : tau , 'mu' : mu_list [ 0 ], 'sigma' : sigma_list [ 0 ]}, equations = \"\"\" x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) : population, init= %(mu)s p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" % { 'mu' : mu_list [ 0 ]}, spike = \"p < x\" , refractory = refractory , name = \"HomogeneousCorrelated\" , description = \"Homogeneous correlated spike trains.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = corr_neuron , name = name , copied = copied ) # Initial values self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'corr' ] = corr self . init [ 'tau' ] = tau self . init [ 'period' ] = period if self . _has_schedule : self . init [ 'mu' ] = mu_list self . init [ 'sigma' ] = sigma_list else : self . init [ 'mu' ] = mu_list [ 0 ] self . init [ 'sigma' ] = sigma_list [ 0 ]","title":"__init__()"},{"location":"API/SpecificPopulation.html#ANNarchy.TimedPoissonPopulation","text":"Bases: SpecificPopulation Poisson population whose rate vary with the provided schedule. Example: inp = TimedPoissonPopulation ( geometry = 100 , rates = [ 10. , 20. , 100. , 20. , 5. ], schedule = [ 0. , 100. , 200. , 500. , 600. ], ) This creates a population of 100 Poisson neurons whose rate will be: 10 Hz during the first 100 ms. 20 HZ during the next 100 ms. 100 Hz during the next 300 ms. 20 Hz during the next 100 ms. 5 Hz until the end of the simulation. If you want the TimedPoissonPopulation to \"loop\" over the schedule, you can specify a period: inp = TimedPoissonPopulation ( geometry = 100 , rates = [ 10. , 20. , 100. , 20. , 5. ], schedule = [ 0. , 100. , 200. , 500. , 600. ], period = 1000. , ) Here the rate will become 10Hz again every 1 second of simulation. If the period is smaller than the schedule, the remaining rates will not be set. Note that you can use the reset() method to manually reinitialize the schedule, times becoming relative to that call: simulate ( 1200. ) # Should switch to 100 Hz due to the period of 1000. inp . reset () simulate ( 1000. ) # Starts at 10 Hz again. The rates were here global to the population. If you want each neuron to have a different rate, rates must have additional dimensions corresponding to the geometry of the population. inp = TimedPoissonPopulation ( geometry = 100 , rates = [ [ 10. + 0.05 * i for i in range ( 100 )], [ 20. + 0.05 * i for i in range ( 100 )], ], schedule = [ 0. , 100. ], period = 1000. , ) Source code in ANNarchy/core/SpecificPopulation.py 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 class TimedPoissonPopulation ( SpecificPopulation ): \"\"\" Poisson population whose rate vary with the provided schedule. Example: ```python inp = TimedPoissonPopulation( geometry = 100, rates = [10., 20., 100., 20., 5.], schedule = [0., 100., 200., 500., 600.], ) ``` This creates a population of 100 Poisson neurons whose rate will be: * 10 Hz during the first 100 ms. * 20 HZ during the next 100 ms. * 100 Hz during the next 300 ms. * 20 Hz during the next 100 ms. * 5 Hz until the end of the simulation. If you want the TimedPoissonPopulation to \"loop\" over the schedule, you can specify a period: ```python inp = TimedPoissonPopulation( geometry = 100, rates = [10., 20., 100., 20., 5.], schedule = [0., 100., 200., 500., 600.], period = 1000., ) ``` Here the rate will become 10Hz again every 1 second of simulation. If the period is smaller than the schedule, the remaining rates will not be set. Note that you can use the ``reset()`` method to manually reinitialize the schedule, times becoming relative to that call: ```python simulate(1200.) # Should switch to 100 Hz due to the period of 1000. inp.reset() simulate(1000.) # Starts at 10 Hz again. ``` The rates were here global to the population. If you want each neuron to have a different rate, ``rates`` must have additional dimensions corresponding to the geometry of the population. ```python inp = TimedPoissonPopulation( geometry = 100, rates = [ [10. + 0.05*i for i in range(100)], [20. + 0.05*i for i in range(100)], ], schedule = [0., 100.], period = 1000., ) ``` \"\"\" def __init__ ( self , geometry , rates , schedule , period = - 1. , name = None , copied = False ): \"\"\" :param rates: array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match the geometry of the population. :param schedule: list of times (in ms) where the firing rate should change. :param period: time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.). \"\"\" neuron = Neuron ( parameters = \"\"\" proba = 1.0 \"\"\" , equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < proba \"\"\" , name = \"TimedPoisson\" , description = \"Spiking neuron following a Poisson distribution.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = neuron , name = name , copied = copied ) # Check arguments try : rates = np . array ( rates ) except : Global . _error ( \"TimedPoissonPopulation: the rates argument must be a numpy array.\" ) schedule = np . array ( schedule ) nb_schedules = rates . shape [ 0 ] if nb_schedules != schedule . size : Global . _error ( \"TimedPoissonPopulation: the first axis of the rates argument must be the same length as schedule.\" ) if rates . ndim == 1 : # One rate for the whole population rates = np . array ([ np . full ( self . size , rates [ i ]) for i in range ( nb_schedules )]) # Initial values self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'period' ] = period def _copy ( self ): \"Returns a copy of the population when creating networks.\" return TimedPoissonPopulation ( self . geometry , self . init [ 'rates' ] , self . init [ 'schedule' ], self . init [ 'period' ], self . name , copied = True ) def _generate_st ( self ): \"\"\" adjust code templates for the specific population for single thread. \"\"\" self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameters of a TimedPoissonPopulation std::vector< int > _schedule; // List of times where new inputs should be set std::vector< std::vector< %(float_prec)s > > _buffer; // buffer holding the data int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameters of a TimedPoissonPopulation void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_buffer(std::vector< std::vector< %(float_prec)s > > buffer) { _buffer = buffer; r = _buffer[0]; } std::vector< std::vector< %(float_prec)s > > get_buffer() { return _buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // Initialize counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters of a TimedPoissonPopulation void set_schedule(vector[int]) vector[int] get_schedule() void set_buffer(vector[vector[ %(float_prec)s ]]) vector[vector[ %(float_prec)s ]] get_buffer() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; _block = 0; r.clear(); r = std::vector< %(float_prec)s >(size, 0.0); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters of a TimedArray cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_rates( self, buffer ): pop %(id)s .set_buffer( buffer ) cpdef np.ndarray get_rates( self ): return np.array(pop %(id)s .get_buffer( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_period(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id } self . _specific_template [ 'update_variables' ] = \"\"\" if(_active){ //std::cout << _t << \" \" << _block<< \" \" << _schedule[_block] << std::endl; // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data proba = _buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if (_block == _schedule.size()){ _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if(_period > -1 && (_t == _period-1)){ // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } if( _active ) { spiked.clear(); // Updating local variables %(float_prec)s step = 1000.0/dt; #pragma omp simd for(int i = 0; i < size; i++){ // p = Uniform(0.0, 1.0) * 1000.0 / dt p[i] = step*rand_0[i]; } } // active \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'size_in_bytes' ] = \"\"\" // schedule size_in_bytes += _schedule.capacity() * sizeof(int); // buffer size_in_bytes += _buffer.capacity() * sizeof(std::vector< %(float_prec)s >); for( auto it = _buffer.begin(); it != _buffer.end(); it++ ) size_in_bytes += it->capacity() * sizeof( %(float_prec)s ); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} def _generate_omp ( self ): \"\"\" adjust code templates for the specific population for openMP. \"\"\" self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameters of a TimedPoissonPopulation std::vector< int > _schedule; // List of times where new inputs should be set std::vector< std::vector< %(float_prec)s > > _buffer; // buffer holding the data int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameters of a TimedPoissonPopulation void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_buffer(std::vector< std::vector< %(float_prec)s > > buffer) { _buffer = buffer; r = _buffer[0]; } std::vector< std::vector< %(float_prec)s > > get_buffer() { return _buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // Initialize counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters of a TimedPoissonPopulation void set_schedule(vector[int]) vector[int] get_schedule() void set_buffer(vector[vector[ %(float_prec)s ]]) vector[vector[ %(float_prec)s ]] get_buffer() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'reset_additional' ] = \"\"\" _t = 0; _block = 0; r.clear(); r = std::vector< %(float_prec)s >(size, 0.0); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters of a TimedArray cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_rates( self, buffer ): pop %(id)s .set_buffer( buffer ) cpdef np.ndarray get_rates( self ): return np.array(pop %(id)s .get_buffer( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_period(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id } self . _specific_template [ 'update_variables' ] = \"\"\" if(_active){ #pragma omp single { //std::cout << _t << \" \" << _block<< \" \" << _schedule[_block] << std::endl; // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data proba = _buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if (_block == _schedule.size()){ _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if(_period > -1 && (_t == _period-1)){ // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } } if( _active ) { spiked.clear(); // Updating local variables %(float_prec)s step = 1000.0/dt; #pragma omp for simd for(int i = 0; i < size; i++){ // p = Uniform(0.0, 1.0) * 1000.0 / dt p[i] = step*rand_0[i]; } } // active \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'size_in_bytes' ] = \"\"\" // schedule size_in_bytes += _schedule.capacity() * sizeof(int); // buffer size_in_bytes += _buffer.capacity() * sizeof(std::vector< %(float_prec)s >); for( auto it = _buffer.begin(); it != _buffer.end(); it++ ) size_in_bytes += it->capacity() * sizeof( %(float_prec)s ); \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} def _generate_cuda ( self ): \"\"\" Code generation if the CUDA paradigm is set. \"\"\" # I suppress the code generation for allocating the variable r on gpu, as # well as memory transfer codes. This is only possible as no other variables # allowed in TimedArray. self . _specific_template [ 'init_parameters_variables' ] = \"\"\" // Random numbers cudaMalloc((void**)&gpu_rand_0, size * sizeof(curandState)); init_curand_states( size, gpu_rand_0, global_seed ); \"\"\" self . _specific_template [ 'host_device_transfer' ] = \"\" self . _specific_template [ 'device_host_transfer' ] = \"\" # # Code for handling the buffer and schedule parameters self . _specific_template [ 'declare_additional' ] = \"\"\" // Custom local parameter timed array std::vector< int > _schedule; std::vector< %(float_prec)s * > gpu_buffer; int _period; // Period of cycling long int _t; // Internal time int _block; // Internal block when inputs are set not at each step \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'access_additional' ] = \"\"\" // Custom local parameter timed array void set_schedule(std::vector<int> schedule) { _schedule = schedule; } std::vector<int> get_schedule() { return _schedule; } void set_buffer(std::vector< std::vector< %(float_prec)s > > buffer) { if ( gpu_buffer.empty() ) { gpu_buffer = std::vector< %(float_prec)s * >(buffer.size(), nullptr); // allocate gpu arrays for(int i = 0; i < buffer.size(); i++) { cudaMalloc((void**)&gpu_buffer[i], buffer[i].size()*sizeof( %(float_prec)s )); } } auto host_it = buffer.begin(); auto dev_it = gpu_buffer.begin(); for( ; host_it < buffer.end(); host_it++, dev_it++ ) { cudaMemcpy( *dev_it, host_it->data(), host_it->size()*sizeof( %(float_prec)s ), cudaMemcpyHostToDevice); } gpu_proba = gpu_buffer[0]; } std::vector< std::vector< %(float_prec)s > > get_buffer() { std::vector< std::vector< %(float_prec)s > > buffer = std::vector< std::vector< %(float_prec)s > >( gpu_buffer.size(), std::vector< %(float_prec)s >(size,0.0) ); auto host_it = buffer.begin(); auto dev_it = gpu_buffer.begin(); for( ; host_it < buffer.end(); host_it++, dev_it++ ) { cudaMemcpy( host_it->data(), *dev_it, size*sizeof( %(float_prec)s ), cudaMemcpyDeviceToHost ); } return buffer; } void set_period(int period) { _period = period; } int get_period() { return _period; } \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'init_additional' ] = \"\"\" // counters _t = 0; _block = 0; _period = -1; \"\"\" self . _specific_template [ 'reset_additional' ] = \"\"\" // counters _t = 0; _block = 0; gpu_proba = gpu_buffer[0]; \"\"\" self . _specific_template [ 'export_additional' ] = \"\"\" # Custom local parameters timed array void set_schedule(vector[int]) vector[int] get_schedule() void set_buffer(vector[vector[ %(float_prec)s ]]) vector[vector[ %(float_prec)s ]] get_buffer() void set_period(int) int get_period() \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # Custom local parameters timed array cpdef set_schedule( self, schedule ): pop %(id)s .set_schedule( schedule ) cpdef np.ndarray get_schedule( self ): return np.array(pop %(id)s .get_schedule( )) cpdef set_rates( self, buffer ): pop %(id)s .set_buffer( buffer ) cpdef np.ndarray get_rates( self ): return np.array(pop %(id)s .get_buffer( )) cpdef set_period( self, period ): pop %(id)s .set_period(period) cpdef int get_periodic(self): return pop %(id)s .get_period() \"\"\" % { 'id' : self . id , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'update_variables' ] = \"\"\" if(_active) { // std::cout << _t << \" \" << _block<< \" \" << _schedule[_block] << std::endl; // Check if it is time to set the input if(_t == _schedule[_block]){ // Set the data gpu_proba = gpu_buffer[_block]; // Move to the next block _block++; // If was the last block, go back to the first block if ( _block == _schedule.size() ) { _block = 0; } } // If the timedarray is periodic, check if we arrive at that point if( (_period > -1) && (_t == _period-1) ) { // Reset the counters _block=0; _t = -1; } // Always increment the internal time _t++; } \"\"\" self . _specific_template [ 'update_variable_body' ] = \"\"\" __global__ void cuPop %(id)s _local_step( const long int t, const double dt, curandState* rand_0, double* proba, unsigned int* num_events, int* spiked, long int* last_spike ) { int i = threadIdx.x + blockDim.x * blockIdx.x; %(float_prec)s step = 1000.0/dt; while ( i < %(size)s ) { // p = Uniform(0.0, 1.0) * 1000.0 / dt %(float_prec)s p = curand_uniform_double( &rand_0[i] ) * step; if (p < proba[i]) { int pos = atomicAdd ( num_events, 1); spiked[pos] = i; last_spike[i] = t; } i += blockDim.x; } __syncthreads(); } \"\"\" % { 'id' : self . id , 'size' : self . size , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'update_variable_header' ] = \"__global__ void cuPop %(id)s _local_step( const long int t, const double dt, curandState* rand_0, double* proba, unsigned int* num_events, int* spiked, long int* last_spike );\" % { 'id' : self . id } # Please notice, that the GPU kernels can be launched only with one block. Otherwise, the # atomicAdd which is called inside the kernel is not working correct (HD: April 1st, 2021) self . _specific_template [ 'update_variable_call' ] = \"\"\" // host side update of neurons pop %(id)s .update(); // Reset old events clear_num_events<<< 1, 1, 0, pop %(id)s .stream >>>(pop %(id)s .gpu_spike_count); #ifdef _DEBUG cudaError_t err_clear_num_events_ %(id)s = cudaGetLastError(); if(err_clear_num_events_ %(id)s != cudaSuccess) std::cout << \"pop %(id)s _spike_gather: \" << cudaGetErrorString(err_clear_num_events_ %(id)s ) << std::endl; #endif // Compute current events cuPop %(id)s _local_step<<< 1, pop %(id)s ._threads_per_block, 0, pop %(id)s .stream >>>( t, dt, pop %(id)s .gpu_rand_0, pop %(id)s .gpu_proba, pop %(id)s .gpu_spike_count, pop %(id)s .gpu_spiked, pop %(id)s .gpu_last_spike ); #ifdef _DEBUG cudaError_t err_pop_spike_gather_ %(id)s = cudaGetLastError(); if(err_pop_spike_gather_ %(id)s != cudaSuccess) std::cout << \"pop %(id)s _spike_gather: \" << cudaGetErrorString(err_pop_spike_gather_ %(id)s ) << std::endl; #endif // transfer back the spike counter (needed by record) cudaMemcpyAsync( &pop %(id)s .spike_count, pop %(id)s .gpu_spike_count, sizeof(unsigned int), cudaMemcpyDeviceToHost, pop %(id)s .stream ); #ifdef _DEBUG cudaError_t err = cudaGetLastError(); if ( err != cudaSuccess ) std::cout << \"record_spike_count: \" << cudaGetErrorString(err) << std::endl; #endif // transfer back the spiked array (needed by record) cudaMemcpyAsync( pop %(id)s .spiked.data(), pop %(id)s .gpu_spiked, pop %(id)s .spike_count*sizeof(int), cudaMemcpyDeviceToHost, pop %(id)s .stream ); #ifdef _DEBUG err = cudaGetLastError(); if ( err != cudaSuccess ) std::cout << \"record_spike: \" << cudaGetErrorString(err) << std::endl; #endif \"\"\" % { 'id' : self . id } self . _specific_template [ 'size_in_bytes' ] = \"//TODO: \" def _instantiate ( self , module ): # Create the Cython instance self . cyInstance = getattr ( module , self . class_name + '_wrapper' )( self . size , self . max_delay ) def __setattr__ ( self , name , value ): if name == 'schedule' : if self . initialized : self . cyInstance . set_schedule ( np . array ( value ) / Global . config [ 'dt' ] ) else : self . init [ 'schedule' ] = value elif name == 'rates' : if self . initialized : value = np . array ( value ) print ( value . shape ) if value . shape [ 0 ] != self . schedule . shape [ 0 ]: Global . _error ( \"TimedPoissonPopulation: the first dimension of rates must match the schedule.\" ) if value . ndim > 2 : # we need to flatten the provided data values = value . reshape ( ( value . shape [ 0 ], self . size ) ) self . cyInstance . set_rates ( values ) elif value . ndim == 2 : if value . shape [ 1 ] != self . size : if value . shape [ 1 ] == 1 : value = np . array ([ np . full ( self . size , value [ i ]) for i in range ( value . shape [ 0 ])]) else : Global . _error ( \"TimedPoissonPopulation: the second dimension of rates must match the number of neurons.\" ) self . cyInstance . set_rates ( value ) elif value . ndim == 1 : value = np . array ([ np . full ( self . size , value [ i ]) for i in range ( value . shape [ 0 ])]) self . cyInstance . set_rates ( value ) else : self . init [ 'rates' ] = value elif name == \"period\" : if self . initialized : self . cyInstance . set_period ( int ( value / Global . config [ 'dt' ])) else : self . init [ 'period' ] = value else : Population . __setattr__ ( self , name , value ) def __getattr__ ( self , name ): if name == 'schedule' : if self . initialized : return Global . config [ 'dt' ] * self . cyInstance . get_schedule () else : return self . init [ 'schedule' ] elif name == 'rates' : if self . initialized : if len ( self . geometry ) > 1 : # unflatten the data flat_values = self . cyInstance . get_rates () values = np . zeros ( tuple ( [ len ( self . schedule )] + list ( self . geometry ) ) ) for x in range ( len ( self . schedule )): values [ x ] = np . reshape ( flat_values [ x ], self . geometry ) return values else : return self . cyInstance . get_rates () else : return self . init [ 'rates' ] elif name == 'period' : if self . initialized : return self . cyInstance . get_period () * Global . config [ 'dt' ] else : return self . init [ 'period' ] else : return Population . __getattribute__ ( self , name )","title":"TimedPoissonPopulation"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.TimedPoissonPopulation.__init__","text":"Parameters: Name Type Description Default rates array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match the geometry of the population. required schedule list of times (in ms) where the firing rate should change. required period time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.). -1.0 Source code in ANNarchy/core/SpecificPopulation.py 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 def __init__ ( self , geometry , rates , schedule , period = - 1. , name = None , copied = False ): \"\"\" :param rates: array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match the geometry of the population. :param schedule: list of times (in ms) where the firing rate should change. :param period: time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.). \"\"\" neuron = Neuron ( parameters = \"\"\" proba = 1.0 \"\"\" , equations = \"\"\" p = Uniform(0.0, 1.0) * 1000.0 / dt \"\"\" , spike = \"\"\" p < proba \"\"\" , name = \"TimedPoisson\" , description = \"Spiking neuron following a Poisson distribution.\" ) SpecificPopulation . __init__ ( self , geometry = geometry , neuron = neuron , name = name , copied = copied ) # Check arguments try : rates = np . array ( rates ) except : Global . _error ( \"TimedPoissonPopulation: the rates argument must be a numpy array.\" ) schedule = np . array ( schedule ) nb_schedules = rates . shape [ 0 ] if nb_schedules != schedule . size : Global . _error ( \"TimedPoissonPopulation: the first axis of the rates argument must be the same length as schedule.\" ) if rates . ndim == 1 : # One rate for the whole population rates = np . array ([ np . full ( self . size , rates [ i ]) for i in range ( nb_schedules )]) # Initial values self . init [ 'schedule' ] = schedule self . init [ 'rates' ] = rates self . init [ 'period' ] = period","title":"__init__()"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.ImagePopulation","text":"Bases: Population Specific rate-coded Population allowing to represent images (png, jpg...) as the firing rate of a population (each neuron represents one pixel). This extension requires the Python Image Library (pip install Pillow). Usage: from ANNarchy import * from ANNarchy.extensions.image import ImagePopulation pop = ImagePopulation ( geometry = ( 480 , 640 )) pop . set_image ( 'image.jpg' ) About the geometry: If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image). If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color). If the third dimension is 3, each will correspond to the RGB values of the pixels. Warning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population. Source code in ANNarchy/extensions/image/ImagePopulation.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 class ImagePopulation ( Population ): \"\"\" Specific rate-coded Population allowing to represent images (png, jpg...) as the firing rate of a population (each neuron represents one pixel). This extension requires the Python Image Library (pip install Pillow). Usage: ```python from ANNarchy import * from ANNarchy.extensions.image import ImagePopulation pop = ImagePopulation(geometry=(480, 640)) pop.set_image('image.jpg') ``` About the geometry: * If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image). * If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color). If the third dimension is 3, each will correspond to the RGB values of the pixels. **Warning:** due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population. \"\"\" def __init__ ( self , geometry , name = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation. :param name: unique name of the population (optional). \"\"\" # Check geometry if isinstance ( geometry , int ) or len ( geometry ) == 1 : Global . _error ( 'The geometry of an ImagePopulation should be 2D (grayscale) or 3D (color).' ) if len ( geometry ) == 3 and ( geometry [ 2 ] != 3 and geometry [ 2 ] != 1 ): Global . _error ( 'The third dimension of an ImagePopulation should be either 1 (grayscale) or 3 (color).' ) if len ( geometry ) == 3 and geometry [ 2 ] == 1 : geometry = ( int ( geometry [ 0 ]), int ( geometry [ 1 ])) # Create the population Population . __init__ ( self , geometry = geometry , name = name , neuron = Neuron ( parameters = \"r = 0.0\" ), copied = copied ) def _copy ( self ): \"Returns a copy of the population when creating networks. Internal use only.\" return ImagePopulation ( geometry = self . geometry , name = self . name , copied = True ) def set_image ( self , image_name ): \"\"\" Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population. If the image has a different size from the population, it will be resized. \"\"\" try : im = Image . open ( image_name ) except : # image does not exist Global . _error ( 'The image ' + image_name + ' does not exist.' ) # Resize the image if needed ( width , height ) = ( self . geometry [ 1 ], self . geometry [ 0 ]) if im . size != ( width , height ): Global . _warning ( 'The image ' + image_name + ' does not have the same size ' + str ( im . size ) + ' as the population ' + str (( width , height )) + '. It will be resized.' ) im = im . resize (( width , height )) # Check if only the luminance should be extracted if self . dimension == 2 or self . geometry [ 2 ] == 1 : im = im . convert ( \"L\" ) # Set the rate of the population self . r = np . array ( im ) . reshape ( self . size ) / 255.","title":"ImagePopulation"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.ImagePopulation.__init__","text":"Parameters: Name Type Description Default geometry population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation. required name unique name of the population (optional). None Source code in ANNarchy/extensions/image/ImagePopulation.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , geometry , name = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation. :param name: unique name of the population (optional). \"\"\" # Check geometry if isinstance ( geometry , int ) or len ( geometry ) == 1 : Global . _error ( 'The geometry of an ImagePopulation should be 2D (grayscale) or 3D (color).' ) if len ( geometry ) == 3 and ( geometry [ 2 ] != 3 and geometry [ 2 ] != 1 ): Global . _error ( 'The third dimension of an ImagePopulation should be either 1 (grayscale) or 3 (color).' ) if len ( geometry ) == 3 and geometry [ 2 ] == 1 : geometry = ( int ( geometry [ 0 ]), int ( geometry [ 1 ])) # Create the population Population . __init__ ( self , geometry = geometry , name = name , neuron = Neuron ( parameters = \"r = 0.0\" ), copied = copied )","title":"__init__()"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.ImagePopulation.set_image","text":"Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population. If the image has a different size from the population, it will be resized. Source code in ANNarchy/extensions/image/ImagePopulation.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def set_image ( self , image_name ): \"\"\" Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population. If the image has a different size from the population, it will be resized. \"\"\" try : im = Image . open ( image_name ) except : # image does not exist Global . _error ( 'The image ' + image_name + ' does not exist.' ) # Resize the image if needed ( width , height ) = ( self . geometry [ 1 ], self . geometry [ 0 ]) if im . size != ( width , height ): Global . _warning ( 'The image ' + image_name + ' does not have the same size ' + str ( im . size ) + ' as the population ' + str (( width , height )) + '. It will be resized.' ) im = im . resize (( width , height )) # Check if only the luminance should be extracted if self . dimension == 2 or self . geometry [ 2 ] == 1 : im = im . convert ( \"L\" ) # Set the rate of the population self . r = np . array ( im ) . reshape ( self . size ) / 255.","title":"set_image()"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.VideoPopulation","text":"Bases: ImagePopulation Specific rate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel). This extension requires the C++ library OpenCV >= 4.0 (apt-get/yum install opencv). pkg-config opencv4 --cflags --libs should not return an error. vtk might additionally have to be installed. Usage: from ANNarchy import * from ANNarchy.extensions.image import VideoPopulation pop = VideoPopulation ( geometry = ( 480 , 640 )) compile () pop . start_camera ( 0 ) while ( True ): pop . grab_image () simulate ( 10.0 ) About the geometry: If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image). If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color). If the third dimension is 3, each will correspond to the RGB values of the pixels. Warning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population. Source code in ANNarchy/extensions/image/ImagePopulation.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 class VideoPopulation ( ImagePopulation ): \"\"\" Specific rate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel). This extension requires the C++ library OpenCV >= 4.0 (apt-get/yum install opencv). ``pkg-config opencv4 --cflags --libs`` should not return an error. `vtk` might additionally have to be installed. Usage: ```python from ANNarchy import * from ANNarchy.extensions.image import VideoPopulation pop = VideoPopulation(geometry=(480, 640)) compile() pop.start_camera(0) while(True): pop.grab_image() simulate(10.0) ``` About the geometry: * If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image). * If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color). If the third dimension is 3, each will correspond to the RGB values of the pixels. **Warning:** due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population. \"\"\" def __init__ ( self , geometry , opencv_version = \"4\" , name = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized. :param opencv_version: OpenCV version (default=4). :param name: unique name of the population (optional). \"\"\" # Create the population ImagePopulation . __init__ ( self , geometry = geometry , name = name , copied = copied ) self . opencv_version = opencv_version def _copy ( self ): \"Returns a copy of the population when creating networks. Internal use only.\" return VideoPopulation ( geometry = self . geometry , name = self . name , copied = True ) def _generate ( self ): \"Code generation\" # Add corresponding libs to the Makefile extra_libs . append ( '`pkg-config opencv' + str ( self . opencv_version ) + ' --cflags --libs`' ) # Include opencv self . _specific_template [ 'include_additional' ] = \"\"\"#include <opencv2/opencv.hpp> using namespace cv; \"\"\" # Class for the camera device self . _specific_template [ 'struct_additional' ] = \"\"\" // VideoPopulation class CameraDeviceCPP : public cv::VideoCapture { public: CameraDeviceCPP (int id, int width, int height, int depth) : cv::VideoCapture(id){ width_ = width; height_ = height; depth_ = depth; img_ = std::vector< %(float_prec)s >(width*height*depth, 0.0); } std::vector< %(float_prec)s > GrabImage(){ if(isOpened()){ // Read a new frame from the video Mat frame; read(frame); // Resize the image Mat resized_frame; resize(frame, resized_frame, Size(width_, height_) ); // If depth=1, only luminance if(depth_==1){ // Convert to luminance cvtColor(resized_frame, resized_frame, COLOR_BGR2GRAY); for(int i = 0; i < resized_frame.rows; i++){ for(int j = 0; j < resized_frame.cols; j++){ this->img_[j+width_*i] = float(resized_frame.at<uchar>(i, j))/255.0; } } } else{ //BGR for(int i = 0; i < resized_frame.rows; i++){ for(int j = 0; j < resized_frame.cols; j++){ Vec3b intensity = resized_frame.at<Vec3b>(i, j); this->img_[(j+width_*i)*3 + 0] = %(float_prec)s (intensity.val[2])/255.0; this->img_[(j+width_*i)*3 + 1] = %(float_prec)s (intensity.val[1])/255.0; this->img_[(j+width_*i)*3 + 2] = %(float_prec)s (intensity.val[0])/255.0; } } } } return this->img_; }; protected: // Width and height of the image, depth_ is 1 (grayscale) or 3 (RGB) int width_, height_, depth_; // Vector of floats for the returned image std::vector< %(float_prec)s > img_; }; \"\"\" % { 'float_prec' : Global . config [ 'precision' ]} self . _specific_template [ 'declare_additional' ] = \"\"\" // Camera CameraDeviceCPP* camera_; void StartCamera(int id, int width, int height, int depth){ camera_ = new CameraDeviceCPP(id, width, height, depth); if(!camera_->isOpened()){ std::cout << \"Error: could not open the camera.\" << std::endl; } }; void GrabImage(){ if(camera_->isOpened()){ r = camera_->GrabImage(); } }; void ReleaseCamera(){ camera_->release(); }; \"\"\" self . _specific_template [ 'update_variables' ] = \"\" self . _specific_template [ 'export_additional' ] = \"\"\" void StartCamera(int id, int width, int height, int depth) void GrabImage() void ReleaseCamera() \"\"\" self . _specific_template [ 'wrapper_access_additional' ] = \"\"\" # CameraDevice def start_camera(self, int id, int width, int height, int depth): pop %(id)s .StartCamera(id, width, height, depth) def grab_image(self): pop %(id)s .GrabImage() def release_camera(self): pop %(id)s .ReleaseCamera() \"\"\" % { 'id' : self . id } def start_camera ( self , camera_port = 0 ): \"\"\" Starts the webcam with the corresponding device (default = 0). On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc. \"\"\" self . cyInstance . start_camera ( camera_port , self . geometry [ 1 ], self . geometry [ 0 ], 3 if self . dimension == 3 else 1 ) def grab_image ( self ): \"\"\" Grabs one image from the camera and feeds it into the population. The camera must be first started with: pop.start_camera(0) \"\"\" self . cyInstance . grab_image () def release ( self ): \"\"\" Releases the camera: pop.release() \"\"\" self . cyInstance . release_camera ()","title":"VideoPopulation"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.VideoPopulation.__init__","text":"Parameters: Name Type Description Default geometry population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized. required opencv_version OpenCV version (default=4). '4' name unique name of the population (optional). None Source code in ANNarchy/extensions/image/ImagePopulation.py 121 122 123 124 125 126 127 128 129 130 def __init__ ( self , geometry , opencv_version = \"4\" , name = None , copied = False ): \"\"\" :param geometry: population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized. :param opencv_version: OpenCV version (default=4). :param name: unique name of the population (optional). \"\"\" # Create the population ImagePopulation . __init__ ( self , geometry = geometry , name = name , copied = copied ) self . opencv_version = opencv_version","title":"__init__()"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.VideoPopulation.grab_image","text":"Grabs one image from the camera and feeds it into the population. The camera must be first started with: pop.start_camera(0) Source code in ANNarchy/extensions/image/ImagePopulation.py 246 247 248 249 250 251 252 253 254 def grab_image ( self ): \"\"\" Grabs one image from the camera and feeds it into the population. The camera must be first started with: pop.start_camera(0) \"\"\" self . cyInstance . grab_image ()","title":"grab_image()"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.VideoPopulation.release","text":"Releases the camera: pop.release() Source code in ANNarchy/extensions/image/ImagePopulation.py 256 257 258 259 260 261 262 def release ( self ): \"\"\" Releases the camera: pop.release() \"\"\" self . cyInstance . release_camera ()","title":"release()"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.VideoPopulation.start_camera","text":"Starts the webcam with the corresponding device (default = 0). On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc. Source code in ANNarchy/extensions/image/ImagePopulation.py 237 238 239 240 241 242 243 244 def start_camera ( self , camera_port = 0 ): \"\"\" Starts the webcam with the corresponding device (default = 0). On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc. \"\"\" self . cyInstance . start_camera ( camera_port , self . geometry [ 1 ], self . geometry [ 0 ], 3 if self . dimension == 3 else 1 )","title":"start_camera()"},{"location":"API/SpecificProjection.html","text":"Specific Projections # ANNarchy provides a set of predefined Projection objects to ease the definition of standard networks. CurrentInjection # Bases: SpecificProjection Inject current from a rate-coded population into a spiking population. The pre-synaptic population must be be rate-coded, the post-synaptic one must be spiking, both must have the same size and no plasticity is allowed. For each post-synaptic neuron, the current g_target will be set at each time step to the firing rate r of the pre-synaptic neuron with the same rank. The projection must be connected with connect_current() , which takes no parameter and does not accept delays. It is equivalent to connect_one_to_one(weights=1) . Example: inp = Population ( 100 , Neuron ( equations = \"r = sin(t)\" )) pop = Population ( 100 , Izhikevich ) proj = CurrentInjection ( inp , pop , 'exc' ) proj . connect_current () Source code in ANNarchy/core/SpecificProjection.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 class CurrentInjection ( SpecificProjection ): \"\"\" Inject current from a rate-coded population into a spiking population. The pre-synaptic population must be be rate-coded, the post-synaptic one must be spiking, both must have the same size and no plasticity is allowed. For each post-synaptic neuron, the current ``g_target`` will be set at each time step to the firing rate ``r`` of the pre-synaptic neuron with the same rank. The projection must be connected with ``connect_current()``, which takes no parameter and does not accept delays. It is equivalent to ``connect_one_to_one(weights=1)``. Example: ```python inp = Population(100, Neuron(equations=\"r = sin(t)\")) pop = Population(100, Izhikevich) proj = CurrentInjection(inp, pop, 'exc') proj.connect_current() ``` \"\"\" def __init__ ( self , pre , post , target , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population. :param post: post-synaptic population. :param target: type of the connection. \"\"\" # Instantiate the projection SpecificProjection . __init__ ( self , pre , post , target , None , name , copied ) # Check populations if not self . pre . neuron_type . type == 'rate' : Global . _error ( 'The pre-synaptic population of a CurrentInjection must be rate-coded.' ) if not self . post . neuron_type . type == 'spike' : Global . _error ( 'The post-synaptic population of a CurrentInjection must be spiking.' ) if not self . post . size == self . pre . size : Global . _error ( 'CurrentInjection: The pre- and post-synaptic populations must have the same size.' ) if Global . _check_paradigm ( \"cuda\" ) and ( isinstance ( pre , PopulationView ) or isinstance ( post , PopulationView )): Global . _error ( \"CurrentInjection on GPUs is not allowed for PopulationViews\" ) # Prevent automatic split of matrices self . _no_split_matrix = True def _copy ( self , pre , post ): \"Returns a copy of the population when creating networks. Internal use only.\" return CurrentInjection ( pre = pre , post = post , target = self . target , name = self . name , copied = True ) def _generate_st ( self ): # Generate the code self . _specific_template [ 'psp_code' ] = \"\"\" if (pop %(id_post)s ._active) { for (int i=0; i<post_rank.size(); i++) { pop %(id_post)s .g_ %(target)s [post_rank[i]] += pop %(id_pre)s .r[pre_rank[i][0]]; } } // active \"\"\" % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'target' : self . target } def _generate_omp ( self ): # Generate the code self . _specific_template [ 'psp_code' ] = \"\"\" if (pop %(id_post)s ._active) { #pragma omp for for (int i=0; i<post_rank.size(); i++) { pop %(id_post)s .g_ %(target)s [post_rank[i]] += pop %(id_pre)s .r[pre_rank[i][0]]; } } // active \"\"\" % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'target' : self . target } def _generate_cuda ( self ): \"\"\" Generate the CUDA code. For a first implementation we take a rather simple approach: * We use only one block for the kernel and each thread computes one synapse/post-neuron entry (which is equal as we use one2one). * We use only the pre-synaptic firing rate, no other variables. * We ignore the synaptic weight. \"\"\" ids = { 'id_proj' : self . id , 'id_post' : self . post . id , 'id_pre' : self . pre . id , 'target' : self . target , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'psp_body' ] = \"\"\" __global__ void cu_proj %(id_proj)s _psp(int post_size, %(float_prec)s *pre_r, %(float_prec)s *g_ %(target)s ) { int n = threadIdx.x; while (n < post_size) { g_ %(target)s [n] += pre_r[n]; n += blockDim.x; } } \"\"\" % ids self . _specific_template [ 'psp_header' ] = \"\"\"__global__ void cu_proj %(id_proj)s _psp(int post_size, %(float_prec)s *pre_r, %(float_prec)s *g_ %(target)s );\"\"\" % ids self . _specific_template [ 'psp_call' ] = \"\"\" cu_proj %(id_proj)s _psp<<< 1, 192 >>>( pop %(id_post)s .size, pop %(id_pre)s .gpu_r, pop %(id_post)s .gpu_g_ %(target)s ); \"\"\" % ids def connect_current ( self ): return self . connect_one_to_one ( weights = 1.0 ) __init__ ( pre , post , target , name = None , copied = False ) # Parameters: Name Type Description Default pre pre-synaptic population. required post post-synaptic population. required target type of the connection. required Source code in ANNarchy/core/SpecificProjection.py 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def __init__ ( self , pre , post , target , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population. :param post: post-synaptic population. :param target: type of the connection. \"\"\" # Instantiate the projection SpecificProjection . __init__ ( self , pre , post , target , None , name , copied ) # Check populations if not self . pre . neuron_type . type == 'rate' : Global . _error ( 'The pre-synaptic population of a CurrentInjection must be rate-coded.' ) if not self . post . neuron_type . type == 'spike' : Global . _error ( 'The post-synaptic population of a CurrentInjection must be spiking.' ) if not self . post . size == self . pre . size : Global . _error ( 'CurrentInjection: The pre- and post-synaptic populations must have the same size.' ) if Global . _check_paradigm ( \"cuda\" ) and ( isinstance ( pre , PopulationView ) or isinstance ( post , PopulationView )): Global . _error ( \"CurrentInjection on GPUs is not allowed for PopulationViews\" ) # Prevent automatic split of matrices self . _no_split_matrix = True","title":"Specific Projections"},{"location":"API/SpecificProjection.html#specific-projections","text":"ANNarchy provides a set of predefined Projection objects to ease the definition of standard networks.","title":"Specific Projections"},{"location":"API/SpecificProjection.html#ANNarchy.CurrentInjection","text":"Bases: SpecificProjection Inject current from a rate-coded population into a spiking population. The pre-synaptic population must be be rate-coded, the post-synaptic one must be spiking, both must have the same size and no plasticity is allowed. For each post-synaptic neuron, the current g_target will be set at each time step to the firing rate r of the pre-synaptic neuron with the same rank. The projection must be connected with connect_current() , which takes no parameter and does not accept delays. It is equivalent to connect_one_to_one(weights=1) . Example: inp = Population ( 100 , Neuron ( equations = \"r = sin(t)\" )) pop = Population ( 100 , Izhikevich ) proj = CurrentInjection ( inp , pop , 'exc' ) proj . connect_current () Source code in ANNarchy/core/SpecificProjection.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 class CurrentInjection ( SpecificProjection ): \"\"\" Inject current from a rate-coded population into a spiking population. The pre-synaptic population must be be rate-coded, the post-synaptic one must be spiking, both must have the same size and no plasticity is allowed. For each post-synaptic neuron, the current ``g_target`` will be set at each time step to the firing rate ``r`` of the pre-synaptic neuron with the same rank. The projection must be connected with ``connect_current()``, which takes no parameter and does not accept delays. It is equivalent to ``connect_one_to_one(weights=1)``. Example: ```python inp = Population(100, Neuron(equations=\"r = sin(t)\")) pop = Population(100, Izhikevich) proj = CurrentInjection(inp, pop, 'exc') proj.connect_current() ``` \"\"\" def __init__ ( self , pre , post , target , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population. :param post: post-synaptic population. :param target: type of the connection. \"\"\" # Instantiate the projection SpecificProjection . __init__ ( self , pre , post , target , None , name , copied ) # Check populations if not self . pre . neuron_type . type == 'rate' : Global . _error ( 'The pre-synaptic population of a CurrentInjection must be rate-coded.' ) if not self . post . neuron_type . type == 'spike' : Global . _error ( 'The post-synaptic population of a CurrentInjection must be spiking.' ) if not self . post . size == self . pre . size : Global . _error ( 'CurrentInjection: The pre- and post-synaptic populations must have the same size.' ) if Global . _check_paradigm ( \"cuda\" ) and ( isinstance ( pre , PopulationView ) or isinstance ( post , PopulationView )): Global . _error ( \"CurrentInjection on GPUs is not allowed for PopulationViews\" ) # Prevent automatic split of matrices self . _no_split_matrix = True def _copy ( self , pre , post ): \"Returns a copy of the population when creating networks. Internal use only.\" return CurrentInjection ( pre = pre , post = post , target = self . target , name = self . name , copied = True ) def _generate_st ( self ): # Generate the code self . _specific_template [ 'psp_code' ] = \"\"\" if (pop %(id_post)s ._active) { for (int i=0; i<post_rank.size(); i++) { pop %(id_post)s .g_ %(target)s [post_rank[i]] += pop %(id_pre)s .r[pre_rank[i][0]]; } } // active \"\"\" % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'target' : self . target } def _generate_omp ( self ): # Generate the code self . _specific_template [ 'psp_code' ] = \"\"\" if (pop %(id_post)s ._active) { #pragma omp for for (int i=0; i<post_rank.size(); i++) { pop %(id_post)s .g_ %(target)s [post_rank[i]] += pop %(id_pre)s .r[pre_rank[i][0]]; } } // active \"\"\" % { 'id_pre' : self . pre . id , 'id_post' : self . post . id , 'target' : self . target } def _generate_cuda ( self ): \"\"\" Generate the CUDA code. For a first implementation we take a rather simple approach: * We use only one block for the kernel and each thread computes one synapse/post-neuron entry (which is equal as we use one2one). * We use only the pre-synaptic firing rate, no other variables. * We ignore the synaptic weight. \"\"\" ids = { 'id_proj' : self . id , 'id_post' : self . post . id , 'id_pre' : self . pre . id , 'target' : self . target , 'float_prec' : Global . config [ 'precision' ] } self . _specific_template [ 'psp_body' ] = \"\"\" __global__ void cu_proj %(id_proj)s _psp(int post_size, %(float_prec)s *pre_r, %(float_prec)s *g_ %(target)s ) { int n = threadIdx.x; while (n < post_size) { g_ %(target)s [n] += pre_r[n]; n += blockDim.x; } } \"\"\" % ids self . _specific_template [ 'psp_header' ] = \"\"\"__global__ void cu_proj %(id_proj)s _psp(int post_size, %(float_prec)s *pre_r, %(float_prec)s *g_ %(target)s );\"\"\" % ids self . _specific_template [ 'psp_call' ] = \"\"\" cu_proj %(id_proj)s _psp<<< 1, 192 >>>( pop %(id_post)s .size, pop %(id_pre)s .gpu_r, pop %(id_post)s .gpu_g_ %(target)s ); \"\"\" % ids def connect_current ( self ): return self . connect_one_to_one ( weights = 1.0 )","title":"CurrentInjection"},{"location":"API/SpecificProjection.html#ANNarchy.core.SpecificProjection.CurrentInjection.__init__","text":"Parameters: Name Type Description Default pre pre-synaptic population. required post post-synaptic population. required target type of the connection. required Source code in ANNarchy/core/SpecificProjection.py 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def __init__ ( self , pre , post , target , name = None , copied = False ): \"\"\" :param pre: pre-synaptic population. :param post: post-synaptic population. :param target: type of the connection. \"\"\" # Instantiate the projection SpecificProjection . __init__ ( self , pre , post , target , None , name , copied ) # Check populations if not self . pre . neuron_type . type == 'rate' : Global . _error ( 'The pre-synaptic population of a CurrentInjection must be rate-coded.' ) if not self . post . neuron_type . type == 'spike' : Global . _error ( 'The post-synaptic population of a CurrentInjection must be spiking.' ) if not self . post . size == self . pre . size : Global . _error ( 'CurrentInjection: The pre- and post-synaptic populations must have the same size.' ) if Global . _check_paradigm ( \"cuda\" ) and ( isinstance ( pre , PopulationView ) or isinstance ( post , PopulationView )): Global . _error ( \"CurrentInjection on GPUs is not allowed for PopulationViews\" ) # Prevent automatic split of matrices self . _no_split_matrix = True","title":"__init__()"},{"location":"API/SpecificSynapse.html","text":"Built-in synapse types # ANNarchy provides standard spiking synapse models, similar to the ones defined in PyNN ( http://neuralensemble.org/docs/PyNN/reference/plasticitymodels.html ). Hebb # Bases: Synapse Rate-coded synapse with Hebbian plasticity. Parameters (global) : eta = 0.01 : learning rate. Learning rule : w : weight. dw/dt = eta * pre.r * post.r Equivalent code: Hebb = Synapse ( parameters = \"\"\" eta = 0.01 : projection \"\"\" , equations = \"\"\" dw/dt = eta * pre.r * post.r : min=0.0 \"\"\" ) __doc__ = ' \\n Rate-coded synapse with Hebbian plasticity. \\n\\n **Parameters (global)**: \\n\\n * eta = 0.01 : learning rate. \\n\\n **Learning rule**: \\n\\n * w : weight. \\n\\n ``` \\n dw/dt = eta * pre.r * post.r \\n ``` \\n\\n Equivalent code: \\n\\n ```python \\n Hebb = Synapse( \\n parameters = \"\"\" \\n eta = 0.01 : projection \\n \"\"\", \\n equations = \"\"\" \\n dw/dt = eta * pre.r * post.r : min=0.0 \\n \"\"\" \\n ) \\n ``` \\n\\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.models.Synapses' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'Synapse' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). Oja # Bases: Synapse Rate-coded synapse with regularized Hebbian plasticity (Oja). Parameters (global) : eta = 0.01 : learning rate. alpha = 1.0 : regularization constant. Learning rule : w : weight: dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) Equivalent code: Oja = Synapse ( parameters = \"\"\" eta = 0.01 : projection alpha = 1.0 : projection \"\"\" , equations = \"\"\" dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) : min=0.0 \"\"\" ) __doc__ = ' \\n Rate-coded synapse with regularized Hebbian plasticity (Oja). \\n\\n **Parameters (global)**: \\n\\n * eta = 0.01 : learning rate. \\n\\n * alpha = 1.0 : regularization constant. \\n\\n **Learning rule**: \\n\\n * w : weight: \\n\\n ``` \\n dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) \\n ``` \\n \\n Equivalent code: \\n\\n ```python \\n Oja = Synapse( \\n parameters = \"\"\" \\n eta = 0.01 : projection \\n alpha = 1.0 : projection \\n \"\"\", \\n equations = \"\"\" \\n dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) : min=0.0 \\n \"\"\" \\n ) \\n ``` \\n\\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.models.Synapses' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'Synapse' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). IBCM # Bases: Synapse Rate-coded synapse with Intrator & Cooper (1992) plasticity. Parameters (global) : eta = 0.01 : learning rate. tau = 2000.0 : time constant of the post-synaptic threshold. Learning rule : theta : post-synaptic threshold: tau * dtheta/dt + theta = post.r^2 w : weight: dw/dt = eta * post.r * (post.r - theta) * pre.r Equivalent code: IBCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 2000.0 : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit \"\"\" ) __doc__ = ' \\n Rate-coded synapse with Intrator & Cooper (1992) plasticity. \\n\\n **Parameters (global)**: \\n\\n * eta = 0.01 : learning rate. \\n\\n * tau = 2000.0 : time constant of the post-synaptic threshold. \\n\\n **Learning rule**: \\n\\n * theta : post-synaptic threshold: \\n\\n ``` \\n tau * dtheta/dt + theta = post.r^2 \\n ``` \\n\\n * w : weight: \\n\\n ``` \\n dw/dt = eta * post.r * (post.r - theta) * pre.r \\n ``` \\n \\n Equivalent code: \\n\\n ```python \\n IBCM = Synapse( \\n parameters = \"\"\" \\n eta = 0.01 : projection \\n tau = 2000.0 : projection \\n \"\"\", \\n equations = \"\"\" \\n tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential \\n dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit \\n \"\"\" \\n ) \\n ``` \\n\\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.models.Synapses' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'Synapse' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). STP # Bases: Synapse Synapse exhibiting short-term facilitation and depression, implemented using the model of Tsodyks, Markram et al.: Tsodyks, Uziel and Markram (2000) Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses. Journal of Neuroscience 20:RC50 Note that the time constant of the post-synaptic current is set in the neuron model, not here. Parameters (global) : tau_rec = 100.0 : depression time constant (ms). tau_facil = 0.01 : facilitation time constant (ms). U = 0.5 : use parameter. Variables : x : recovery variable:: dx/dt = (1 - x)/tau_rec u : facilitation variable:: du/dt = (U - u)/tau_facil Both variables are integrated event-driven. Pre-spike events : g_target += w * u * x x *= (1 - u) u += U * (1 - u) Equivalent code: STP = Synapse ( parameters = \"\"\" tau_rec = 100.0 : projection tau_facil = 0.01 : projection U = 0.5 \"\"\" , equations = \"\"\" dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven du/dt = (U - u)/tau_facil : init = 0.5, event-driven \"\"\" , pre_spike = \"\"\" g_target += w * u * x x *= (1 - u) u += U * (1 - u) \"\"\" ) __doc__ = ' \\n Synapse exhibiting short-term facilitation and depression, implemented using the model of Tsodyks, Markram et al.: \\n\\n Tsodyks, Uziel and Markram (2000) Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses. Journal of Neuroscience 20:RC50 \\n\\n Note that the time constant of the post-synaptic current is set in the neuron model, not here. \\n\\n **Parameters (global)**: \\n\\n * tau_rec = 100.0 : depression time constant (ms). \\n * tau_facil = 0.01 : facilitation time constant (ms). \\n * U = 0.5 : use parameter. \\n\\n **Variables**: \\n\\n * x : recovery variable:: \\n\\n ``` \\n dx/dt = (1 - x)/tau_rec \\n ``` \\n\\n * u : facilitation variable:: \\n\\n ``` \\n du/dt = (U - u)/tau_facil \\n ``` \\n\\n Both variables are integrated event-driven. \\n\\n **Pre-spike events**: \\n\\n ``` \\n g_target += w * u * x \\n x *= (1 - u) \\n u += U * (1 - u) \\n ``` \\n \\n Equivalent code: \\n\\n ```python \\n STP = Synapse( \\n parameters = \"\"\" \\n tau_rec = 100.0 : projection \\n tau_facil = 0.01 : projection \\n U = 0.5 \\n \"\"\", \\n equations = \"\"\" \\n dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven \\n du/dt = (U - u)/tau_facil : init = 0.5, event-driven \\n \"\"\", \\n pre_spike=\"\"\" \\n g_target += w * u * x \\n x *= (1 - u) \\n u += U * (1 - u) \\n \"\"\" \\n ) \\n ``` \\n\\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.models.Synapses' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'Synapse' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached). STDP # Bases: Synapse Spike-timing dependent plasticity. This is the online version of the STDP rule. Song, S., and Abbott, L.F. (2001). Cortical development and remapping through spike timing-dependent plasticity. Neuron 32, 339-350. Parameters (global) : tau_plus = 20.0 : time constant of the pre-synaptic trace (ms) tau_minus = 20.0 : time constant of the pre-synaptic trace (ms) A_plus = 0.01 : increase of the pre-synaptic trace after a spike. A_minus = 0.01 : decrease of the post-synaptic trace after a spike. w_min = 0.0 : minimal value of the weight w. w_max = 1.0 : maximal value of the weight w. Variables : x : pre-synaptic trace: tau_plus * dx/dt = -x y: post-synaptic trace: tau_minus * dy/dt = -y Both variables are evaluated event-driven. Pre-spike events : g_target += w x += A_plus * w_max w = clip(w + y, w_min , w_max) Post-spike events :: y -= A_minus * w_max w = clip(w + x, w_min , w_max) Equivalent code: STDP = Synapse ( parameters = \"\"\" tau_plus = 20.0 : projection tau_minus = 20.0 : projection A_plus = 0.01 : projection A_minus = 0.01 : projection w_min = 0.0 : projection w_max = 1.0 : projection \"\"\" , equations = \"\"\" tau_plus * dx/dt = -x : event-driven tau_minus * dy/dt = -y : event-driven \"\"\" , pre_spike = \"\"\" g_target += w x += A_plus * w_max w = clip(w + y, w_min , w_max) \"\"\" , post_spike = \"\"\" y -= A_minus * w_max w = clip(w + x, w_min , w_max) \"\"\" ) __doc__ = ' \\n Spike-timing dependent plasticity. \\n\\n This is the online version of the STDP rule. \\n\\n Song, S., and Abbott, L.F. (2001). Cortical development and remapping through spike timing-dependent plasticity. Neuron 32, 339-350. \\n\\n **Parameters (global)**: \\n\\n * tau_plus = 20.0 : time constant of the pre-synaptic trace (ms) \\n * tau_minus = 20.0 : time constant of the pre-synaptic trace (ms) \\n * A_plus = 0.01 : increase of the pre-synaptic trace after a spike. \\n * A_minus = 0.01 : decrease of the post-synaptic trace after a spike. \\n * w_min = 0.0 : minimal value of the weight w. \\n * w_max = 1.0 : maximal value of the weight w. \\n\\n **Variables**: \\n\\n * x : pre-synaptic trace: \\n\\n ``` \\n tau_plus * dx/dt = -x \\n ``` \\n\\n * y: post-synaptic trace: \\n\\n ``` \\n tau_minus * dy/dt = -y \\n ``` \\n\\n Both variables are evaluated event-driven. \\n\\n **Pre-spike events**: \\n\\n ``` \\n g_target += w \\n\\n x += A_plus * w_max \\n\\n w = clip(w + y, w_min , w_max) \\n ``` \\n\\n **Post-spike events**:: \\n\\n ``` \\n y -= A_minus * w_max \\n \\n w = clip(w + x, w_min , w_max) \\n ``` \\n \\n Equivalent code: \\n\\n ```python \\n\\n STDP = Synapse( \\n parameters = \"\"\" \\n tau_plus = 20.0 : projection \\n tau_minus = 20.0 : projection \\n A_plus = 0.01 : projection \\n A_minus = 0.01 : projection \\n w_min = 0.0 : projection \\n w_max = 1.0 : projection \\n \"\"\", \\n equations = \"\"\" \\n tau_plus * dx/dt = -x : event-driven \\n tau_minus * dy/dt = -y : event-driven \\n \"\"\", \\n pre_spike=\"\"\" \\n g_target += w \\n x += A_plus * w_max \\n w = clip(w + y, w_min , w_max) \\n \"\"\", \\n post_spike=\"\"\" \\n y -= A_minus * w_max \\n w = clip(w + x, w_min , w_max) \\n \"\"\" \\n ) \\n ``` \\n\\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.models.Synapses' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'Synapse' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"Built-in synapse types"},{"location":"API/SpecificSynapse.html#built-in-synapse-types","text":"ANNarchy provides standard spiking synapse models, similar to the ones defined in PyNN ( http://neuralensemble.org/docs/PyNN/reference/plasticitymodels.html ).","title":"Built-in synapse types"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb","text":"Bases: Synapse Rate-coded synapse with Hebbian plasticity. Parameters (global) : eta = 0.01 : learning rate. Learning rule : w : weight. dw/dt = eta * pre.r * post.r Equivalent code: Hebb = Synapse ( parameters = \"\"\" eta = 0.01 : projection \"\"\" , equations = \"\"\" dw/dt = eta * pre.r * post.r : min=0.0 \"\"\" )","title":"Hebb"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Hebb.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja","text":"Bases: Synapse Rate-coded synapse with regularized Hebbian plasticity (Oja). Parameters (global) : eta = 0.01 : learning rate. alpha = 1.0 : regularization constant. Learning rule : w : weight: dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) Equivalent code: Oja = Synapse ( parameters = \"\"\" eta = 0.01 : projection alpha = 1.0 : projection \"\"\" , equations = \"\"\" dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) : min=0.0 \"\"\" )","title":"Oja"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Oja.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM","text":"Bases: Synapse Rate-coded synapse with Intrator & Cooper (1992) plasticity. Parameters (global) : eta = 0.01 : learning rate. tau = 2000.0 : time constant of the post-synaptic threshold. Learning rule : theta : post-synaptic threshold: tau * dtheta/dt + theta = post.r^2 w : weight: dw/dt = eta * post.r * (post.r - theta) * pre.r Equivalent code: IBCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 2000.0 : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit \"\"\" )","title":"IBCM"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.IBCM.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP","text":"Bases: Synapse Synapse exhibiting short-term facilitation and depression, implemented using the model of Tsodyks, Markram et al.: Tsodyks, Uziel and Markram (2000) Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses. Journal of Neuroscience 20:RC50 Note that the time constant of the post-synaptic current is set in the neuron model, not here. Parameters (global) : tau_rec = 100.0 : depression time constant (ms). tau_facil = 0.01 : facilitation time constant (ms). U = 0.5 : use parameter. Variables : x : recovery variable:: dx/dt = (1 - x)/tau_rec u : facilitation variable:: du/dt = (U - u)/tau_facil Both variables are integrated event-driven. Pre-spike events : g_target += w * u * x x *= (1 - u) u += U * (1 - u) Equivalent code: STP = Synapse ( parameters = \"\"\" tau_rec = 100.0 : projection tau_facil = 0.01 : projection U = 0.5 \"\"\" , equations = \"\"\" dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven du/dt = (U - u)/tau_facil : init = 0.5, event-driven \"\"\" , pre_spike = \"\"\" g_target += w * u * x x *= (1 - u) u += U * (1 - u) \"\"\" )","title":"STP"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STP.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP","text":"Bases: Synapse Spike-timing dependent plasticity. This is the online version of the STDP rule. Song, S., and Abbott, L.F. (2001). Cortical development and remapping through spike timing-dependent plasticity. Neuron 32, 339-350. Parameters (global) : tau_plus = 20.0 : time constant of the pre-synaptic trace (ms) tau_minus = 20.0 : time constant of the pre-synaptic trace (ms) A_plus = 0.01 : increase of the pre-synaptic trace after a spike. A_minus = 0.01 : decrease of the post-synaptic trace after a spike. w_min = 0.0 : minimal value of the weight w. w_max = 1.0 : maximal value of the weight w. Variables : x : pre-synaptic trace: tau_plus * dx/dt = -x y: post-synaptic trace: tau_minus * dy/dt = -y Both variables are evaluated event-driven. Pre-spike events : g_target += w x += A_plus * w_max w = clip(w + y, w_min , w_max) Post-spike events :: y -= A_minus * w_max w = clip(w + x, w_min , w_max) Equivalent code: STDP = Synapse ( parameters = \"\"\" tau_plus = 20.0 : projection tau_minus = 20.0 : projection A_plus = 0.01 : projection A_minus = 0.01 : projection w_min = 0.0 : projection w_max = 1.0 : projection \"\"\" , equations = \"\"\" tau_plus * dx/dt = -x : event-driven tau_minus * dy/dt = -y : event-driven \"\"\" , pre_spike = \"\"\" g_target += w x += A_plus * w_max w = clip(w + y, w_min , w_max) \"\"\" , post_spike = \"\"\" y -= A_minus * w_max w = clip(w + x, w_min , w_max) \"\"\" )","title":"STDP"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/SpecificSynapse.html#ANNarchy.models.STDP.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/Synapse.html","text":"Synapse class # The class Synapse is used to describe the behavior of a synapse (parameters, equations...). Synapse # Base class to define a synapse. __doc__ = ' \\n Base class to define a synapse. \\n ' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __module__ = 'ANNarchy.core.Synapse' # str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. __weakref__ = < attribute '__weakref__' of 'Synapse' objects > # list of weak references to the object (if defined) __delattr__ ( name ) method descriptor # Implement delattr(self, name). __dir__ () method descriptor # Default dir() implementation. __eq__ ( value ) method descriptor # Return self==value. __format__ ( format_spec ) method descriptor # Default object formatter. __ge__ ( value ) method descriptor # Return self>=value. __getattribute__ ( name ) method descriptor # Return getattr(self, name). __gt__ ( value ) method descriptor # Return self>value. __hash__ () method descriptor # Return hash(self). __init__ ( parameters = '' , equations = '' , psp = None , operation = 'sum' , pre_spike = None , post_spike = None , pre_axon_spike = None , functions = None , pruning = None , creating = None , name = None , description = None , extra_values = {}) # Parameters: Name Type Description Default parameters parameters of the neuron and their initial value. '' equations equations defining the temporal evolution of variables. '' psp continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r ). Synaptic transmission in spiking synapses occurs in pre_spike . None operation operation (sum, max, min, mean) performed by the post-synaptic neuron on the individual psp (rate-coded only, default=sum). 'sum' pre_spike updating of variables when a pre-synaptic spike is received (spiking only). None post_spike updating of variables when a post-synaptic spike is emitted (spiking only). None pre_axon_spike updating of variables when an axonal spike was emitted (spiking only, default None). The usage of this arguments prevents the application of learning rules. None functions additional functions used in the equations. None name name of the synapse type (used for reporting only). None description short description of the synapse type (used for reporting). None __init_subclass__ () builtin # This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. __le__ ( value ) method descriptor # Return self<=value. __lt__ ( value ) method descriptor # Return self<value. __ne__ ( value ) method descriptor # Return self!=value. __new__ ( * args , ** kwargs ) builtin # Create and return a new object. See help(type) for accurate signature. __reduce__ () method descriptor # Helper for pickle. __reduce_ex__ ( protocol ) method descriptor # Helper for pickle. __setattr__ ( name , value ) method descriptor # Implement setattr(self, name, value). __sizeof__ () method descriptor # Size of object in memory, in bytes. __subclasshook__ () builtin # Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"Synapse class"},{"location":"API/Synapse.html#synapse-class","text":"The class Synapse is used to describe the behavior of a synapse (parameters, equations...).","title":"Synapse class"},{"location":"API/Synapse.html#ANNarchy.Synapse","text":"Base class to define a synapse.","title":"Synapse"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__doc__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__doc__"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__module__","text":"str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"__module__"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__weakref__","text":"list of weak references to the object (if defined)","title":"__weakref__"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__delattr__","text":"Implement delattr(self, name).","title":"__delattr__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__dir__","text":"Default dir() implementation.","title":"__dir__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__eq__","text":"Return self==value.","title":"__eq__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__format__","text":"Default object formatter.","title":"__format__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__ge__","text":"Return self>=value.","title":"__ge__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__getattribute__","text":"Return getattr(self, name).","title":"__getattribute__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__gt__","text":"Return self>value.","title":"__gt__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__hash__","text":"Return hash(self).","title":"__hash__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__init__","text":"Parameters: Name Type Description Default parameters parameters of the neuron and their initial value. '' equations equations defining the temporal evolution of variables. '' psp continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: w*pre.r ). Synaptic transmission in spiking synapses occurs in pre_spike . None operation operation (sum, max, min, mean) performed by the post-synaptic neuron on the individual psp (rate-coded only, default=sum). 'sum' pre_spike updating of variables when a pre-synaptic spike is received (spiking only). None post_spike updating of variables when a post-synaptic spike is emitted (spiking only). None pre_axon_spike updating of variables when an axonal spike was emitted (spiking only, default None). The usage of this arguments prevents the application of learning rules. None functions additional functions used in the equations. None name name of the synapse type (used for reporting only). None description short description of the synapse type (used for reporting). None","title":"__init__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__init_subclass__","text":"This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses.","title":"__init_subclass__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__le__","text":"Return self<=value.","title":"__le__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__lt__","text":"Return self<value.","title":"__lt__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__ne__","text":"Return self!=value.","title":"__ne__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__reduce__","text":"Helper for pickle.","title":"__reduce__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__reduce_ex__","text":"Helper for pickle.","title":"__reduce_ex__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__setattr__","text":"Implement setattr(self, name, value).","title":"__setattr__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__sizeof__","text":"Size of object in memory, in bytes.","title":"__sizeof__()"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__subclasshook__","text":"Abstract classes can override this to customize issubclass(). This is invoked early on by abc.ABCMeta. subclasscheck (). It should return True, False or NotImplemented. If it returns NotImplemented, the normal algorithm is used. Otherwise, it overrides the normal algorithm (and the outcome is cached).","title":"__subclasshook__()"},{"location":"API/Utilities.html","text":"Reporting # report ( filename = './report.tex' , standalone = True , gather_subprojections = False , title = None , author = None , date = None , net_id = 0 ) # Generates a report describing the network. If the filename ends with .tex , the TeX file is generated according to: Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8): e1000456. If the filename ends with .md , a (more complete) Markdown file is generated, which can be converted to pdf or html by pandoc :: pandoc report.md -sSN -V geometry:margin=1in -o report.pdf pandoc report.md -sSN -o report.html Parameters: Name Type Description Default filename name of the file where the report will be written (default: \"./report.tex\") './report.tex' standalone tells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown. True gather_subprojections if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False). False title title of the document (Markdown only) None author author of the document (Markdown only) None date date of the document (Markdown only) None net_id id of the network to be used for reporting (default: 0, everything that was declared) 0 Source code in ANNarchy/parser/report/Report.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def report ( filename = \"./report.tex\" , standalone = True , gather_subprojections = False , title = None , author = None , date = None , net_id = 0 ): \"\"\" Generates a report describing the network. If the filename ends with ``.tex``, the TeX file is generated according to: Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8): e1000456. If the filename ends with ``.md``, a (more complete) Markdown file is generated, which can be converted to pdf or html by ``pandoc``:: pandoc report.md -sSN -V geometry:margin=1in -o report.pdf pandoc report.md -sSN -o report.html :param filename: name of the file where the report will be written (default: \"./report.tex\") :param standalone: tells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown. :param gather_subprojections: if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False). :param title: title of the document (Markdown only) :param author: author of the document (Markdown only) :param date: date of the document (Markdown only) :param net_id: id of the network to be used for reporting (default: 0, everything that was declared) \"\"\" if filename . endswith ( '.tex' ): from .LatexReport import report_latex report_latex ( filename , standalone , gather_subprojections , net_id ) elif filename . endswith ( '.md' ): from .MarkdownReport import report_markdown report_markdown ( filename , standalone , gather_subprojections , title , author , date , net_id ) else : _error ( 'report(): the filename must end with .tex or .md.' )","title":"Reporting"},{"location":"API/Utilities.html#reporting","text":"","title":"Reporting"},{"location":"API/Utilities.html#ANNarchy.report","text":"Generates a report describing the network. If the filename ends with .tex , the TeX file is generated according to: Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8): e1000456. If the filename ends with .md , a (more complete) Markdown file is generated, which can be converted to pdf or html by pandoc :: pandoc report.md -sSN -V geometry:margin=1in -o report.pdf pandoc report.md -sSN -o report.html Parameters: Name Type Description Default filename name of the file where the report will be written (default: \"./report.tex\") './report.tex' standalone tells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown. True gather_subprojections if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False). False title title of the document (Markdown only) None author author of the document (Markdown only) None date date of the document (Markdown only) None net_id id of the network to be used for reporting (default: 0, everything that was declared) 0 Source code in ANNarchy/parser/report/Report.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def report ( filename = \"./report.tex\" , standalone = True , gather_subprojections = False , title = None , author = None , date = None , net_id = 0 ): \"\"\" Generates a report describing the network. If the filename ends with ``.tex``, the TeX file is generated according to: Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8): e1000456. If the filename ends with ``.md``, a (more complete) Markdown file is generated, which can be converted to pdf or html by ``pandoc``:: pandoc report.md -sSN -V geometry:margin=1in -o report.pdf pandoc report.md -sSN -o report.html :param filename: name of the file where the report will be written (default: \"./report.tex\") :param standalone: tells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown. :param gather_subprojections: if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False). :param title: title of the document (Markdown only) :param author: author of the document (Markdown only) :param date: date of the document (Markdown only) :param net_id: id of the network to be used for reporting (default: 0, everything that was declared) \"\"\" if filename . endswith ( '.tex' ): from .LatexReport import report_latex report_latex ( filename , standalone , gather_subprojections , net_id ) elif filename . endswith ( '.md' ): from .MarkdownReport import report_markdown report_markdown ( filename , standalone , gather_subprojections , title , author , date , net_id ) else : _error ( 'report(): the filename must end with .tex or .md.' )","title":"report()"},{"location":"example/BarLearning.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Bar Learning problem # Download the Jupyter notebook : BarLearning.ipynb The bar learning problem describes the process of learning receptive fields on an artificial input pattern. Images consisting of independent bars are used. Those images are generated as following: an 8*8 image can filled randomly by eight horizontal or vertical bars, with a probability of 1/8 for each. These input images are fed into a neural population, whose neurons should learn to extract the independent components of the input distribution, namely single horizontal or vertical bars. Model overview # The model consists of two populations Input and Feature . The size of Input should be chosen to fit the input image size (here 8*8). The number of neurons in the Feature population should be higher than the total number of independent bars (16, we choose here 32 neurons). The Feature population gets excitory connections from Input through an all-to-all connection pattern. The same pattern is used for the inhibitory connections within Feature . Defining the neurons and populations # from ANNarchy import * clear () ANNarchy 4.7 (4.7.2) on darwin (posix). Input population: The input pattern will be clamped into this population by the main loop for every trial, so we need just an empty neuron at this point: InputNeuron = Neuron ( parameters = \"r = 0.0\" ) The trick here is to declare r as a parameter, not a variable: its value will not be computed by the simulator, but only set by external inputs. The Input population can then be created: Input = Population ( geometry = ( 8 , 8 ), neuron = InputNeuron ) Feature population: The neuron type composing this population sums up all the excitory inputs gain from Input and the lateral inhibition within Feature . \\[\\tau \\frac {dr_{j}^{\\text{Feature}}}{dt} + r_{j}^{Feature} = \\sum_{i} w_{ij} \\cdot r_{i}^{\\text{Input}} - \\sum_{k, k \\ne j} w_{kj} * r_{k}^{Feature}\\] could be implemented as the following: LeakyNeuron = Neuron ( parameters = \"\"\" tau = 10.0 : population \"\"\" , equations = \"\"\" tau * dr/dt + r = sum(exc) - sum(inh) : min=0.0 \"\"\" ) The firing rate is restricted to positive values with the min=0.0 flag. The population is created in the following way: Feature = Population ( geometry = ( 8 , 4 ), neuron = LeakyNeuron ) We give it a (8, 4) geometry for visualization only, it does not influence computations at all. Defining the synapse and projections # Both feedforward ( Input \\(\\rightarrow\\) Feature ) and lateral ( Feature \\(\\rightarrow\\) Feature ) projections are learned using the Oja learning rule (a regularized Hebbian learning rule ensuring the sum of all weights coming to a neuron is constant). Only some parameters will differ between the projections. \\[\\tau \\frac{dw_{ij}}{dt} = r_{i} * r_{j} - \\alpha * r_{j}^{2} * w_{ij}\\] where \\(\\alpha\\) is a parameter defining the strength of the regularization, \\(r_i\\) is the pre-synaptic firing rate and \\(r_j\\) the post-synaptic one. The implementation of this synapse type is straightforward: Oja = Synapse ( parameters = \"\"\" tau = 2000.0 : postsynaptic alpha = 8.0 : postsynaptic min_w = 0.0 : postsynaptic \"\"\" , equations = \"\"\" tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=min_w \"\"\" ) For this network we need to create two projections, one excitory between the populations Input and Feature and one inhibitory within the Feature population itself: ff = Projection ( pre = Input , post = Feature , target = 'exc' , synapse = Oja ) ff . connect_all_to_all ( weights = Uniform ( - 0.5 , 0.5 )) lat = Projection ( pre = Feature , post = Feature , target = 'inh' , synapse = Oja ) lat . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) <ANNarchy.core.Projection.Projection at 0x1215ef3d0> The two projections are all-to-all and use the Oja synapse type. They only differ by the parameter alpha (lower in lat ) and the fact that the weights of ff are allowed to be negative (so we set the minimum value to -10.0): ff . min_w = - 10.0 lat . alpha = 0.3 Setting inputs # Once the network is defined, one has to specify how inputs are fed into the Input population. A simple solution is to define a method that sets the firing rate of Input according to the specified probabilities every time it is called, and runs the simulation for 50 ms: def trial (): # Reset the firing rate for all neurons Input . r = 0.0 # Clamp horizontal bars randomly for h in range ( Input . geometry [ 0 ]): if np . random . random () < 1.0 / float ( Input . geometry [ 0 ]): Input [ h , :] . r = 1.0 # Clamp vertical bars randomly for w in range ( Input . geometry [ 1 ]): if np . random . random () < 1.0 / float ( Input . geometry [ 1 ]): Input [:, w ] . r = 1.0 # Simulate for 50ms simulate ( 50. ) # Return firing rates and receptive fields for visualization return Input . r , Feature . r , ff . receptive_fields () One can use here a single value or a Numpy array (e.g. np.zeros(Input.geometry)) ) to reset activity in Input, it does not matter. For all possible horizontal bars, a decision is then made whether the bar should appear or not, in which case the firing rate of the correspondng neurons is set to 1.0: for h in range ( Input . geometry [ 0 ]): if np . random . random () < 1.0 / float ( Input . geometry [ 0 ]): Input [ h , :] . r = 1.0 Input[h, :] is a PopulationView, i.e. a group of neurons defined by the sub-indices (here the row of index h ). Their attributes, such as r , can be accessed as if it were a regular population. The same is done for vertical bars. Running the simulation # Once the method for setting inputs is defined, the simulation can be started. A basic approach would be to define a for loop where the trial() method is called repetitively: compile () for t in range ( 1000 ): input_r , feature_r , weights = trial () Compiling ... OK import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 131 ) plt . imshow ( input_r . T , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . title ( 'Input' ) plt . subplot ( 132 ) plt . imshow ( feature_r . T , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . title ( 'Feature' ) plt . subplot ( 133 ) plt . imshow ( weights . T , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . title ( 'Receptive fields' ) plt . show () In the file BarLearning.py , a visualization class using pyqtgraph is imported from Viz.py , but the user is free to use whatever method he prefers to visualize the result of learning. from Viz import Viewer view = Viewer ( func = trial ) view . run ()","title":"Bar Learning"},{"location":"example/BarLearning.html#bar-learning-problem","text":"Download the Jupyter notebook : BarLearning.ipynb The bar learning problem describes the process of learning receptive fields on an artificial input pattern. Images consisting of independent bars are used. Those images are generated as following: an 8*8 image can filled randomly by eight horizontal or vertical bars, with a probability of 1/8 for each. These input images are fed into a neural population, whose neurons should learn to extract the independent components of the input distribution, namely single horizontal or vertical bars.","title":"Bar Learning problem"},{"location":"example/BarLearning.html#model-overview","text":"The model consists of two populations Input and Feature . The size of Input should be chosen to fit the input image size (here 8*8). The number of neurons in the Feature population should be higher than the total number of independent bars (16, we choose here 32 neurons). The Feature population gets excitory connections from Input through an all-to-all connection pattern. The same pattern is used for the inhibitory connections within Feature .","title":"Model overview"},{"location":"example/BarLearning.html#defining-the-neurons-and-populations","text":"from ANNarchy import * clear () ANNarchy 4.7 (4.7.2) on darwin (posix). Input population: The input pattern will be clamped into this population by the main loop for every trial, so we need just an empty neuron at this point: InputNeuron = Neuron ( parameters = \"r = 0.0\" ) The trick here is to declare r as a parameter, not a variable: its value will not be computed by the simulator, but only set by external inputs. The Input population can then be created: Input = Population ( geometry = ( 8 , 8 ), neuron = InputNeuron ) Feature population: The neuron type composing this population sums up all the excitory inputs gain from Input and the lateral inhibition within Feature . \\[\\tau \\frac {dr_{j}^{\\text{Feature}}}{dt} + r_{j}^{Feature} = \\sum_{i} w_{ij} \\cdot r_{i}^{\\text{Input}} - \\sum_{k, k \\ne j} w_{kj} * r_{k}^{Feature}\\] could be implemented as the following: LeakyNeuron = Neuron ( parameters = \"\"\" tau = 10.0 : population \"\"\" , equations = \"\"\" tau * dr/dt + r = sum(exc) - sum(inh) : min=0.0 \"\"\" ) The firing rate is restricted to positive values with the min=0.0 flag. The population is created in the following way: Feature = Population ( geometry = ( 8 , 4 ), neuron = LeakyNeuron ) We give it a (8, 4) geometry for visualization only, it does not influence computations at all.","title":"Defining the neurons and populations"},{"location":"example/BarLearning.html#defining-the-synapse-and-projections","text":"Both feedforward ( Input \\(\\rightarrow\\) Feature ) and lateral ( Feature \\(\\rightarrow\\) Feature ) projections are learned using the Oja learning rule (a regularized Hebbian learning rule ensuring the sum of all weights coming to a neuron is constant). Only some parameters will differ between the projections. \\[\\tau \\frac{dw_{ij}}{dt} = r_{i} * r_{j} - \\alpha * r_{j}^{2} * w_{ij}\\] where \\(\\alpha\\) is a parameter defining the strength of the regularization, \\(r_i\\) is the pre-synaptic firing rate and \\(r_j\\) the post-synaptic one. The implementation of this synapse type is straightforward: Oja = Synapse ( parameters = \"\"\" tau = 2000.0 : postsynaptic alpha = 8.0 : postsynaptic min_w = 0.0 : postsynaptic \"\"\" , equations = \"\"\" tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=min_w \"\"\" ) For this network we need to create two projections, one excitory between the populations Input and Feature and one inhibitory within the Feature population itself: ff = Projection ( pre = Input , post = Feature , target = 'exc' , synapse = Oja ) ff . connect_all_to_all ( weights = Uniform ( - 0.5 , 0.5 )) lat = Projection ( pre = Feature , post = Feature , target = 'inh' , synapse = Oja ) lat . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) <ANNarchy.core.Projection.Projection at 0x1215ef3d0> The two projections are all-to-all and use the Oja synapse type. They only differ by the parameter alpha (lower in lat ) and the fact that the weights of ff are allowed to be negative (so we set the minimum value to -10.0): ff . min_w = - 10.0 lat . alpha = 0.3","title":"Defining the synapse and projections"},{"location":"example/BarLearning.html#setting-inputs","text":"Once the network is defined, one has to specify how inputs are fed into the Input population. A simple solution is to define a method that sets the firing rate of Input according to the specified probabilities every time it is called, and runs the simulation for 50 ms: def trial (): # Reset the firing rate for all neurons Input . r = 0.0 # Clamp horizontal bars randomly for h in range ( Input . geometry [ 0 ]): if np . random . random () < 1.0 / float ( Input . geometry [ 0 ]): Input [ h , :] . r = 1.0 # Clamp vertical bars randomly for w in range ( Input . geometry [ 1 ]): if np . random . random () < 1.0 / float ( Input . geometry [ 1 ]): Input [:, w ] . r = 1.0 # Simulate for 50ms simulate ( 50. ) # Return firing rates and receptive fields for visualization return Input . r , Feature . r , ff . receptive_fields () One can use here a single value or a Numpy array (e.g. np.zeros(Input.geometry)) ) to reset activity in Input, it does not matter. For all possible horizontal bars, a decision is then made whether the bar should appear or not, in which case the firing rate of the correspondng neurons is set to 1.0: for h in range ( Input . geometry [ 0 ]): if np . random . random () < 1.0 / float ( Input . geometry [ 0 ]): Input [ h , :] . r = 1.0 Input[h, :] is a PopulationView, i.e. a group of neurons defined by the sub-indices (here the row of index h ). Their attributes, such as r , can be accessed as if it were a regular population. The same is done for vertical bars.","title":"Setting inputs"},{"location":"example/BarLearning.html#running-the-simulation","text":"Once the method for setting inputs is defined, the simulation can be started. A basic approach would be to define a for loop where the trial() method is called repetitively: compile () for t in range ( 1000 ): input_r , feature_r , weights = trial () Compiling ... OK import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 131 ) plt . imshow ( input_r . T , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . title ( 'Input' ) plt . subplot ( 132 ) plt . imshow ( feature_r . T , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . title ( 'Feature' ) plt . subplot ( 133 ) plt . imshow ( weights . T , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . title ( 'Receptive fields' ) plt . show () In the file BarLearning.py , a visualization class using pyqtgraph is imported from Viz.py , but the user is free to use whatever method he prefers to visualize the result of learning. from Viz import Viewer view = Viewer ( func = trial ) view . run ()","title":"Running the simulation"},{"location":"example/BasalGanglia.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Logging with tensorboard # Download the Jupyter notebook : BasalGanglia.ipynb The tensorboard extension allows to log various information (scalars, images, etc) during training for visualization using tensorboard . It has to be explicitly imported: from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger import matplotlib.pyplot as plt ANNarchy 4.7 (4.7.2) on darwin (posix). As it is just for demonstration purposes, we will be an extremely simplified model of the basal ganglia learning to solve through reinforcement learning a stimulus-response task with 4 stimuli and 2 responses (left and right). The two first stimuli should be responded with left, the two others with right. stimuli = [ ([ 1 , 0 , 0 , 0 ], 0 ), # A : left ([ 0 , 1 , 0 , 0 ], 0 ), # B : left ([ 0 , 0 , 1 , 0 ], 1 ), # C : right ([ 0 , 0 , 0 , 1 ], 1 ), # D : right ] We keep here the model as simple as possible. It is inspired from the rate-coded model described here: Vitay J, Hamker FH. 2010. A computational model of Basal Ganglia and its role in memory retrieval in rewarded visual memory tasks. Frontiers in computational neuroscience 4. doi:10.3389/fncom.2010.00013 The input population is composed of 4 static neurons to represent the inputs: cortex = Population ( 4 , Neuron ( parameters = \"r=0.0\" )) The cortex projects on the striatum, which is composed of 10 neurons integrating excitatory and inhibitory inputs: msn = Neuron ( parameters = \"tau = 10.0 : population; noise = 0.1 : population\" , equations = \"\"\" tau*dv/dt + v = sum(exc) - sum(inh) + noise * Uniform(-1, 1) r = clip(v, 0.0, 1.0) \"\"\" ) striatum = Population ( 10 , msn ) The striatum projects inhibitorily on GPi, whose neurons are tonically active (high baseline). Normally, GPi would project on the thalamus and back to the cortex, but here we read the output of the network directly in GPi: if the first neuron (corresponding to the left action) is less active than the second neuron, the selected action is left. gp_neuron = Neuron ( parameters = \"tau = 10.0 : population; B = 1.0\" , equations = \"tau*dv/dt + v = B - sum(inh); r= pos(v)\" ) gpi = Population ( 2 , gp_neuron ) Learning occurs at the cortico-striatal synapses, using a reward-modulated Hebbian learning rule, with Oja regularization: corticostriatal = Synapse ( parameters = \"\"\" eta = 0.1 : projection alpha = 0.5 : projection dopamine = 0.0 : projection\"\"\" , equations = \"w += eta*(dopamine * pre.r * post.r - alpha*w*post.r*post.r) : min=0.0\" ) cx_str = Projection ( cortex , striatum , \"exc\" , corticostriatal ) cx_str . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) <ANNarchy.core.Projection.Projection at 0x13600f610> Some lateral competition between the striatal neurons: str_str = Projection ( striatum , striatum , \"inh\" ) str_str . connect_all_to_all ( weights = 0.6 ) <ANNarchy.core.Projection.Projection at 0x13600fca0> One half of the striatal population is connected to the left GPi neuron, the other half to the right neuron: str_gpi1 = Projection ( striatum [: int ( striatum . size / 2 )], gpi [ 0 ], 'inh' ) . connect_all_to_all ( 1.0 ) str_gpi2 = Projection ( striatum [ int ( striatum . size / 2 ):], gpi [ 1 ], 'inh' ) . connect_all_to_all ( 1.0 ) We add a monitor on GPi and compile: m = Monitor ( gpi , 'r' ) compile () Compiling ... OK Each trial is very simple: we get a stimulus x from the stimuli array and a correct response t , reset the network for 40 ms, set the input and simulate for 50 ms, observe the activity in GPi to decide what the answer of the network is, provide reward accordingly to the corticostriatal projection and let learn for 10 ms. Here the \"dopamine\" signal is directly the reward (+1 for success, -1 for failure), not the reward prediction error, but it is just for demonstration. def training_trial ( x , t ): # Delay period cortex . r = 0.0 cx_str . dopamine = 0.0 simulate ( 40.0 ) # Set inputs cortex . r = np . array ( x ) simulate ( 50.0 ) # Read output output = gpi . r answer = np . argmin ( output ) # Provide reward reward = 1.0 if answer == t else - 1.0 cx_str . dopamine = reward simulate ( 10.0 ) # Get recordings data = m . get ( 'r' ) return reward , data The whole training procedure will simply iterate over the four stimuli for 100 trials: for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) We use the Logger class of the tensorboard extension to keep track of various data: with Logger () as logger : for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) # Log data... Note that it would be equivalent to manually close the Logger after training: logger = Logger () for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) # Log data... logger . close () We log here different quantities, just to demonstrate the different methods of the Logger class: The reward received after each trial: logger . add_scalar ( \"Reward\" , reward , trial ) The tag \"Reward\" will be the name of the plot in tensorboard. reward is the value that will be displayed, while trial is the index of the current trial (x-axis). The activity of the two GPi cells at the end of the trial, in separate plots depending on the stimulus: if trial % len ( stimuli ) == 0 : label = \"GPi activity/A\" elif trial % len ( stimuli ) == 1 : label = \"GPi activity/B\" elif trial % len ( stimuli ) == 2 : label = \"GPi activity/C\" elif trial % len ( stimuli ) == 3 : label = \"GPi activity/D\" logger . add_scalars ( label , { \"Left neuron\" : gpi . r [ 0 ], \"Right neuron\" : gpi . r [ 1 ]}, trial ) The four plots will be grouped under the label \"GPi activity\", with a title A, B, C or D. Note that add_scalars() requires a dictionary of values that will plot together. The activity in the striatum as a 2*5 image: logger . add_image ( \"Activity/Striatum\" , striatum . r . reshape (( 2 , 5 )), trial ) The activity should be reshaped to the correct dimensions. Note that activity in the striatum is bounded between 0 and 1, so there is no need for equalization. An histogram of the preference for the stimuli A and B of striatal cells: w = np . array ( cx_str . w ) logger . add_histogram ( \"Cortico-striatal weights/Left - AB/CD\" , np . mean ( w [: 5 , : 2 ] - w [: 5 , 2 :], axis = 1 ), trial ) logger . add_histogram ( \"Cortico-striatal weights/Right - AB/CD\" , np . mean ( w [ 5 :, : 2 ] - w [ 5 :, 2 :], axis = 1 ), trial ) We make here two plots, one for the first 5 striatal cells, the other for the rest. We plot the difference between the mean weights of each cell for the stimuli A and B, and the mean weights for the stimuli C and D. If learning goes well, the first five striatal cells should have stronger weights for A and B than for C and D, as they project to the left GPi cell. A matplotlib figure showing the time course of the two GPi cells (as recorded by the monitor): fig = plt . figure ( figsize = ( 10 , 8 )) plt . plot ( data [:, 0 ], label = \"left\" ) plt . plot ( data [:, 1 ], label = \"right\" ) plt . legend () logger . add_figure ( \"Activity/GPi\" , fig , trial ) Note that the figure will be automatically closed by the logger, no need to call show() . Logging figures is extremely slow, use that feature wisely. By default, the logs are saved in the subfolder runs/ , but this can be changed when creating the Logger: with Logger ( \"/tmp/experiment\" ) as logger : Each run of the network will be saved in this folder. You may want to delete the folder before each run, in order to only visualize the last run: % rm - rf runs with Logger () as logger : for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) # Log received rewards logger . add_scalar ( \"Reward\" , reward , trial ) # Log outputs depending on the task if trial % len ( stimuli ) == 0 : label = \"GPi activity/A\" elif trial % len ( stimuli ) == 1 : label = \"GPi activity/B\" elif trial % len ( stimuli ) == 2 : label = \"GPi activity/C\" elif trial % len ( stimuli ) == 3 : label = \"GPi activity/D\" logger . add_scalars ( label , { \"Left neuron\" : gpi . r [ 0 ], \"Right neuron\" : gpi . r [ 1 ]}, trial ) # Log striatal activity as a 2*5 image logger . add_image ( \"Activity/Striatum\" , striatum . r . reshape (( 2 , 5 )), trial ) # Log histogram of cortico-striatal weights w = np . array ( cx_str . w ) logger . add_histogram ( \"Cortico-striatal weights/Left - AB/CD\" , np . mean ( w [: 5 , : 2 ] - w [: 5 , 2 :], axis = 1 ), trial ) logger . add_histogram ( \"Cortico-striatal weights/Right - AB/CD\" , np . mean ( w [ 5 :, : 2 ] - w [ 5 :, 2 :], axis = 1 ), trial ) # Log matplotlib figure of GPi activity fig = plt . figure ( figsize = ( 10 , 8 )) plt . plot ( data [:, 0 ], label = \"left\" ) plt . plot ( data [:, 1 ], label = \"right\" ) plt . legend () logger . add_figure ( \"Activity/GPi\" , fig , trial ) Logging in runs/Dec14_11-32-14_Juliens-MBP You can now visualize the logged information by running tensorboard in a separate terminal and opening the corresponding page: tensorboard --logdir runs or directly in the notebook if you have the tensorboard extension installed: % load_ext tensorboard % tensorboard -- logdir runs -- samples_per_plugin images = 100 You should see a tensorboard page with four tabs Scalars, Images, Distributions and Histograms: The Reward plot shows that the network successfully learns to solve the task, as it consistently gets rewards of +1 (note that this may vary from run to run, depending on weight initialization): The GPi activity tab shows that the two GPi cells quickly learn to be inhibited for the right stimuli. In the Images tab, the plot for the striatum allows to visualize activity at the end of each rtial, showing that only one cell in the correct subpopulation is active: The matplotlib figure for the GPi activity shows what happens during a trial, especially at the end of the reset period: In the histograms tab, we can see that the left striatal population has acquired a preference (stronger weights) for the stimuli A and B, as the values are positive. The right population has negative values, so the neurons have stronger weights to the stimuli C and D. Note that some neurons in the right population still have stronger weights from A and B, but they are probably inhibited by the left population, so they do not impair performance.","title":"Logging with tensorboard"},{"location":"example/BasalGanglia.html#logging-with-tensorboard","text":"Download the Jupyter notebook : BasalGanglia.ipynb The tensorboard extension allows to log various information (scalars, images, etc) during training for visualization using tensorboard . It has to be explicitly imported: from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger import matplotlib.pyplot as plt ANNarchy 4.7 (4.7.2) on darwin (posix). As it is just for demonstration purposes, we will be an extremely simplified model of the basal ganglia learning to solve through reinforcement learning a stimulus-response task with 4 stimuli and 2 responses (left and right). The two first stimuli should be responded with left, the two others with right. stimuli = [ ([ 1 , 0 , 0 , 0 ], 0 ), # A : left ([ 0 , 1 , 0 , 0 ], 0 ), # B : left ([ 0 , 0 , 1 , 0 ], 1 ), # C : right ([ 0 , 0 , 0 , 1 ], 1 ), # D : right ] We keep here the model as simple as possible. It is inspired from the rate-coded model described here: Vitay J, Hamker FH. 2010. A computational model of Basal Ganglia and its role in memory retrieval in rewarded visual memory tasks. Frontiers in computational neuroscience 4. doi:10.3389/fncom.2010.00013 The input population is composed of 4 static neurons to represent the inputs: cortex = Population ( 4 , Neuron ( parameters = \"r=0.0\" )) The cortex projects on the striatum, which is composed of 10 neurons integrating excitatory and inhibitory inputs: msn = Neuron ( parameters = \"tau = 10.0 : population; noise = 0.1 : population\" , equations = \"\"\" tau*dv/dt + v = sum(exc) - sum(inh) + noise * Uniform(-1, 1) r = clip(v, 0.0, 1.0) \"\"\" ) striatum = Population ( 10 , msn ) The striatum projects inhibitorily on GPi, whose neurons are tonically active (high baseline). Normally, GPi would project on the thalamus and back to the cortex, but here we read the output of the network directly in GPi: if the first neuron (corresponding to the left action) is less active than the second neuron, the selected action is left. gp_neuron = Neuron ( parameters = \"tau = 10.0 : population; B = 1.0\" , equations = \"tau*dv/dt + v = B - sum(inh); r= pos(v)\" ) gpi = Population ( 2 , gp_neuron ) Learning occurs at the cortico-striatal synapses, using a reward-modulated Hebbian learning rule, with Oja regularization: corticostriatal = Synapse ( parameters = \"\"\" eta = 0.1 : projection alpha = 0.5 : projection dopamine = 0.0 : projection\"\"\" , equations = \"w += eta*(dopamine * pre.r * post.r - alpha*w*post.r*post.r) : min=0.0\" ) cx_str = Projection ( cortex , striatum , \"exc\" , corticostriatal ) cx_str . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) <ANNarchy.core.Projection.Projection at 0x13600f610> Some lateral competition between the striatal neurons: str_str = Projection ( striatum , striatum , \"inh\" ) str_str . connect_all_to_all ( weights = 0.6 ) <ANNarchy.core.Projection.Projection at 0x13600fca0> One half of the striatal population is connected to the left GPi neuron, the other half to the right neuron: str_gpi1 = Projection ( striatum [: int ( striatum . size / 2 )], gpi [ 0 ], 'inh' ) . connect_all_to_all ( 1.0 ) str_gpi2 = Projection ( striatum [ int ( striatum . size / 2 ):], gpi [ 1 ], 'inh' ) . connect_all_to_all ( 1.0 ) We add a monitor on GPi and compile: m = Monitor ( gpi , 'r' ) compile () Compiling ... OK Each trial is very simple: we get a stimulus x from the stimuli array and a correct response t , reset the network for 40 ms, set the input and simulate for 50 ms, observe the activity in GPi to decide what the answer of the network is, provide reward accordingly to the corticostriatal projection and let learn for 10 ms. Here the \"dopamine\" signal is directly the reward (+1 for success, -1 for failure), not the reward prediction error, but it is just for demonstration. def training_trial ( x , t ): # Delay period cortex . r = 0.0 cx_str . dopamine = 0.0 simulate ( 40.0 ) # Set inputs cortex . r = np . array ( x ) simulate ( 50.0 ) # Read output output = gpi . r answer = np . argmin ( output ) # Provide reward reward = 1.0 if answer == t else - 1.0 cx_str . dopamine = reward simulate ( 10.0 ) # Get recordings data = m . get ( 'r' ) return reward , data The whole training procedure will simply iterate over the four stimuli for 100 trials: for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) We use the Logger class of the tensorboard extension to keep track of various data: with Logger () as logger : for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) # Log data... Note that it would be equivalent to manually close the Logger after training: logger = Logger () for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) # Log data... logger . close () We log here different quantities, just to demonstrate the different methods of the Logger class: The reward received after each trial: logger . add_scalar ( \"Reward\" , reward , trial ) The tag \"Reward\" will be the name of the plot in tensorboard. reward is the value that will be displayed, while trial is the index of the current trial (x-axis). The activity of the two GPi cells at the end of the trial, in separate plots depending on the stimulus: if trial % len ( stimuli ) == 0 : label = \"GPi activity/A\" elif trial % len ( stimuli ) == 1 : label = \"GPi activity/B\" elif trial % len ( stimuli ) == 2 : label = \"GPi activity/C\" elif trial % len ( stimuli ) == 3 : label = \"GPi activity/D\" logger . add_scalars ( label , { \"Left neuron\" : gpi . r [ 0 ], \"Right neuron\" : gpi . r [ 1 ]}, trial ) The four plots will be grouped under the label \"GPi activity\", with a title A, B, C or D. Note that add_scalars() requires a dictionary of values that will plot together. The activity in the striatum as a 2*5 image: logger . add_image ( \"Activity/Striatum\" , striatum . r . reshape (( 2 , 5 )), trial ) The activity should be reshaped to the correct dimensions. Note that activity in the striatum is bounded between 0 and 1, so there is no need for equalization. An histogram of the preference for the stimuli A and B of striatal cells: w = np . array ( cx_str . w ) logger . add_histogram ( \"Cortico-striatal weights/Left - AB/CD\" , np . mean ( w [: 5 , : 2 ] - w [: 5 , 2 :], axis = 1 ), trial ) logger . add_histogram ( \"Cortico-striatal weights/Right - AB/CD\" , np . mean ( w [ 5 :, : 2 ] - w [ 5 :, 2 :], axis = 1 ), trial ) We make here two plots, one for the first 5 striatal cells, the other for the rest. We plot the difference between the mean weights of each cell for the stimuli A and B, and the mean weights for the stimuli C and D. If learning goes well, the first five striatal cells should have stronger weights for A and B than for C and D, as they project to the left GPi cell. A matplotlib figure showing the time course of the two GPi cells (as recorded by the monitor): fig = plt . figure ( figsize = ( 10 , 8 )) plt . plot ( data [:, 0 ], label = \"left\" ) plt . plot ( data [:, 1 ], label = \"right\" ) plt . legend () logger . add_figure ( \"Activity/GPi\" , fig , trial ) Note that the figure will be automatically closed by the logger, no need to call show() . Logging figures is extremely slow, use that feature wisely. By default, the logs are saved in the subfolder runs/ , but this can be changed when creating the Logger: with Logger ( \"/tmp/experiment\" ) as logger : Each run of the network will be saved in this folder. You may want to delete the folder before each run, in order to only visualize the last run: % rm - rf runs with Logger () as logger : for trial in range ( 100 ): # Get a stimulus x , t = stimuli [ trial % len ( stimuli )] # Perform a trial reward , data = training_trial ( x , t ) # Log received rewards logger . add_scalar ( \"Reward\" , reward , trial ) # Log outputs depending on the task if trial % len ( stimuli ) == 0 : label = \"GPi activity/A\" elif trial % len ( stimuli ) == 1 : label = \"GPi activity/B\" elif trial % len ( stimuli ) == 2 : label = \"GPi activity/C\" elif trial % len ( stimuli ) == 3 : label = \"GPi activity/D\" logger . add_scalars ( label , { \"Left neuron\" : gpi . r [ 0 ], \"Right neuron\" : gpi . r [ 1 ]}, trial ) # Log striatal activity as a 2*5 image logger . add_image ( \"Activity/Striatum\" , striatum . r . reshape (( 2 , 5 )), trial ) # Log histogram of cortico-striatal weights w = np . array ( cx_str . w ) logger . add_histogram ( \"Cortico-striatal weights/Left - AB/CD\" , np . mean ( w [: 5 , : 2 ] - w [: 5 , 2 :], axis = 1 ), trial ) logger . add_histogram ( \"Cortico-striatal weights/Right - AB/CD\" , np . mean ( w [ 5 :, : 2 ] - w [ 5 :, 2 :], axis = 1 ), trial ) # Log matplotlib figure of GPi activity fig = plt . figure ( figsize = ( 10 , 8 )) plt . plot ( data [:, 0 ], label = \"left\" ) plt . plot ( data [:, 1 ], label = \"right\" ) plt . legend () logger . add_figure ( \"Activity/GPi\" , fig , trial ) Logging in runs/Dec14_11-32-14_Juliens-MBP You can now visualize the logged information by running tensorboard in a separate terminal and opening the corresponding page: tensorboard --logdir runs or directly in the notebook if you have the tensorboard extension installed: % load_ext tensorboard % tensorboard -- logdir runs -- samples_per_plugin images = 100 You should see a tensorboard page with four tabs Scalars, Images, Distributions and Histograms: The Reward plot shows that the network successfully learns to solve the task, as it consistently gets rewards of +1 (note that this may vary from run to run, depending on weight initialization): The GPi activity tab shows that the two GPi cells quickly learn to be inhibited for the right stimuli. In the Images tab, the plot for the striatum allows to visualize activity at the end of each rtial, showing that only one cell in the correct subpopulation is active: The matplotlib figure for the GPi activity shows what happens during a trial, especially at the end of the reset period: In the histograms tab, we can see that the left striatal population has acquired a preference (stronger weights) for the stimuli A and B, as the values are positive. The right population has negative values, so the neurons have stronger weights to the stimuli C and D. Note that some neurons in the right population still have stronger weights from A and B, but they are probably inhibited by the left population, so they do not impair performance.","title":"Logging with tensorboard"},{"location":"example/BayesianOptimization.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Hyperparameter optimization # Download the Jupyter notebook : BayesianOptimization.ipynb Most of the work in computational neuroscience is to guess the values of parameters which are not constrained by the biology. The most basic approach is to simply try out different values, run the simulation, reason about why the results are not what you want, change some parameters, run again, etc. It is very easy to get lost in this process and it requires a great deal of intuition about how the model works. If you are able to define an objective function for your model (a single number that tells how well your model performs), you can use search algorithms to find this hyperparameters automatically, at the cost of running your model multiple times. Let's take the example of a rate-coded model depending on two hyperparameters a and b , where is the objective is to have a minimal activity after 1 s of simulation (dummy example): from ANNarchy import * pop = Population ( ... ) ... compile () def run ( a , b ): pop . a = a pop . b = b simulate ( 1000. ) return ( pop . r ) ** 2 Grid search would iterate over all possible values of the parameters to perform the search: min_loss = 1000. for a in np . linspace ( 0.0 , 1.0 , 100 ): for b in np . linspace ( 0.0 , 1.0 , 100 ): loss = run ( a , b ) if loss < min_loss : min_loss = loss a_best = a ; b_best = b If you try 100 values for each parameters, you need 10000 simulations to find your parameters. The number of simulations explodes with the number of free parameters. Moreover, you cannot stop the search before the end, as you could miss the interesting region. Random search samples blindly values for the hyperparameters: min_loss = 1000. for _ in range ( 1000 ): a = np . random . uniform ( 0.0 , 1.0 ) b = np . random . uniform ( 0.0 , 1.0 ) loss = run ( a , b ) if loss < min_loss : min_loss = loss a_best = a ; b_best = b If you are lucky, you may find a good solution quite early in the search, so you can stop it when the loss is below a desired threshold. The main drawback is that the search may spend a lot of time in uninteresting regions: it does not learn anything between two samples. An often much more efficient search method is Bayesian optimization (also called sequential model-based optimization - SMBO). It is a form of random search that updates beliefs on the hyperparameters. In short, if some parameter values do not lead to good values of the objective function in early samples, they will not be used in later samples. The search becomes more and more focused on the interesting regions of the hyperparameter space. As always with Python, there are many libraries for that, including: hyperopt https://github.com/hyperopt/hyperopt optuna https://github.com/pfnet/optuna talos (for keras models) https://github.com/autonomio/talos This notebook demonstrates how to use hyperopt to find some hyperparameters of the COBA models already included in the ANNarchy examples: https://annarchy.readthedocs.io/en/stable/example/COBA.html Additionally, we will use the tensorboard extension to visualize the dependency between the parameters and the objective function. from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger clear () setup ( dt = 0.1 ) ANNarchy 4.7 (4.7.2) on darwin (posix). COBA = Neuron ( parameters = \"\"\" El = -60.0 : population Vr = -60.0 : population Erev_exc = 0.0 : population Erev_inh = -80.0 : population Vt = -50.0 : population tau = 20.0 : population tau_exc = 5.0 : population tau_inh = 10.0 : population I = 20.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I tau_exc * dg_exc/dt = - g_exc tau_inh * dg_inh/dt = - g_inh \"\"\" , spike = \"v > Vt\" , reset = \"v = Vr\" , refractory = 5.0 ) P = Population ( geometry = 4000 , neuron = COBA ) Pe = P [: 3200 ] Pi = P [ 3200 :] P . v = Normal ( - 55.0 , 5.0 ) P . g_exc = Normal ( 4.0 , 1.5 ) P . g_inh = Normal ( 20.0 , 12.0 ) Ce = Projection ( pre = Pe , post = P , target = 'exc' ) Ce . connect_fixed_probability ( weights = 0.6 , probability = 0.02 ) Ci = Projection ( pre = Pi , post = P , target = 'inh' ) Ci . connect_fixed_probability ( weights = 6.7 , probability = 0.02 ) compile () m = Monitor ( P , [ 'spike' ]) Compiling ... OK With the default parameters, the COBA network fires at around 20 Hz: simulate ( 1000.0 ) data = m . get ( 'spike' ) fr = m . mean_fr ( data ) print ( fr ) 19.732999999999997 Let's suppose we now want the network to fire at 30 Hz. Which parameters should we change to obtain that value? Many parameters might influence the firing rate of the network (if not all). Here, we make the assumption that the weight values for the excitatory connections (0.6) and inhibitory ones (6.7) are the most critical ones. Let's start by importing hyperopt (after installing it with pip install hyperopt ): from hyperopt import fmin , tpe , hp , STATUS_OK We define a trial() method taking values for the two hyperparameters as inputs. It starts by resetting the network, sets the excitatory and inhibitory weights to the desired value, simulates for one second, computes the mean firing rate of the population, logs the parameters and finally returns the objective function: the squared error between the recorded firing rate and 30 Hz. logger = Logger () def trial ( args ): # Retrieve the parameters w_exc = args [ 0 ] w_inh = args [ 1 ] # Reset the network reset () # Set the hyperparameters Ce . w = w_exc Ci . w = w_inh # Simulate 1 second simulate ( 1000.0 ) # Retrieve the spike recordings and the membrane potential spikes = m . get ( 'spike' ) # Compute the population firing rate fr = m . mean_fr ( spikes ) # Compute a quadratic loss around 30 Hz loss = 0.001 * ( fr - 30.0 ) ** 2 # Log the parameters logger . add_parameters ({ 'w_exc' : w_exc , 'w_inh' : w_inh }, { 'loss' : loss , 'firing_rate' : fr }) return { 'loss' : loss , 'status' : STATUS_OK , # -- store other results like this 'fr' : fr , } Logging in runs/Dec14_11-34-31_Juliens-MBP We can check that the default parameters indeed lead to a firing rate of 20 Hz: trial ([ 0.6 , 6.7 ]) {'loss': 0.10541128900000007, 'status': 'ok', 'fr': 19.732999999999997} We can now use hyperopt to find the hyperparameters making the network fire at 30 Hz. The fmin() function takes: fn : the objective function for a set of parameters. space : the search space for the hyperparameters (the prior). algo : which algorithm to use, either tpe.suggest or random.suggest max_evals : number of samples (simulations) to make. Here, we will sample the excitatory weights between 0.1 and 1, the inhibitory ones between 1 and 10. Of course, the smaller the range, the better. Refer to the doc of hyperopt for other sampling priors. best = fmin ( fn = trial , space = [ hp . uniform ( 'w_exc' , 0.1 , 1.0 ), hp . uniform ( 'w_inh' , 1.0 , 10.0 ) ], algo = tpe . suggest , max_evals = 100 ) print ( best ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:19<00:00, 5.02trial/s, best loss: 9.0150062499999e-05] {'w_exc': 0.9916507361791144, 'w_inh': 7.825494383808928} After 100 simulations, hyperopt returns a set of hyperparameter values that make the network fire at 30Hz. We can check that it is true with: trial ([ best [ 'w_exc' ], best [ 'w_inh' ]]) {'loss': 9.0150062499999e-05, 'status': 'ok', 'fr': 30.30025} There are plenty of options to hyperopt (check Trials or the parallel search using MongoDB), but this simple example should get you started. If we start tensorboard in the default directory runs/ , we can additionally visualize how the firing rate depends on w_exc and w_inh in the HPARAMS tab. logger . close () % load_ext tensorboard % tensorboard -- logdir runs","title":"Bayesian optimization"},{"location":"example/BayesianOptimization.html#hyperparameter-optimization","text":"Download the Jupyter notebook : BayesianOptimization.ipynb Most of the work in computational neuroscience is to guess the values of parameters which are not constrained by the biology. The most basic approach is to simply try out different values, run the simulation, reason about why the results are not what you want, change some parameters, run again, etc. It is very easy to get lost in this process and it requires a great deal of intuition about how the model works. If you are able to define an objective function for your model (a single number that tells how well your model performs), you can use search algorithms to find this hyperparameters automatically, at the cost of running your model multiple times. Let's take the example of a rate-coded model depending on two hyperparameters a and b , where is the objective is to have a minimal activity after 1 s of simulation (dummy example): from ANNarchy import * pop = Population ( ... ) ... compile () def run ( a , b ): pop . a = a pop . b = b simulate ( 1000. ) return ( pop . r ) ** 2 Grid search would iterate over all possible values of the parameters to perform the search: min_loss = 1000. for a in np . linspace ( 0.0 , 1.0 , 100 ): for b in np . linspace ( 0.0 , 1.0 , 100 ): loss = run ( a , b ) if loss < min_loss : min_loss = loss a_best = a ; b_best = b If you try 100 values for each parameters, you need 10000 simulations to find your parameters. The number of simulations explodes with the number of free parameters. Moreover, you cannot stop the search before the end, as you could miss the interesting region. Random search samples blindly values for the hyperparameters: min_loss = 1000. for _ in range ( 1000 ): a = np . random . uniform ( 0.0 , 1.0 ) b = np . random . uniform ( 0.0 , 1.0 ) loss = run ( a , b ) if loss < min_loss : min_loss = loss a_best = a ; b_best = b If you are lucky, you may find a good solution quite early in the search, so you can stop it when the loss is below a desired threshold. The main drawback is that the search may spend a lot of time in uninteresting regions: it does not learn anything between two samples. An often much more efficient search method is Bayesian optimization (also called sequential model-based optimization - SMBO). It is a form of random search that updates beliefs on the hyperparameters. In short, if some parameter values do not lead to good values of the objective function in early samples, they will not be used in later samples. The search becomes more and more focused on the interesting regions of the hyperparameter space. As always with Python, there are many libraries for that, including: hyperopt https://github.com/hyperopt/hyperopt optuna https://github.com/pfnet/optuna talos (for keras models) https://github.com/autonomio/talos This notebook demonstrates how to use hyperopt to find some hyperparameters of the COBA models already included in the ANNarchy examples: https://annarchy.readthedocs.io/en/stable/example/COBA.html Additionally, we will use the tensorboard extension to visualize the dependency between the parameters and the objective function. from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger clear () setup ( dt = 0.1 ) ANNarchy 4.7 (4.7.2) on darwin (posix). COBA = Neuron ( parameters = \"\"\" El = -60.0 : population Vr = -60.0 : population Erev_exc = 0.0 : population Erev_inh = -80.0 : population Vt = -50.0 : population tau = 20.0 : population tau_exc = 5.0 : population tau_inh = 10.0 : population I = 20.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I tau_exc * dg_exc/dt = - g_exc tau_inh * dg_inh/dt = - g_inh \"\"\" , spike = \"v > Vt\" , reset = \"v = Vr\" , refractory = 5.0 ) P = Population ( geometry = 4000 , neuron = COBA ) Pe = P [: 3200 ] Pi = P [ 3200 :] P . v = Normal ( - 55.0 , 5.0 ) P . g_exc = Normal ( 4.0 , 1.5 ) P . g_inh = Normal ( 20.0 , 12.0 ) Ce = Projection ( pre = Pe , post = P , target = 'exc' ) Ce . connect_fixed_probability ( weights = 0.6 , probability = 0.02 ) Ci = Projection ( pre = Pi , post = P , target = 'inh' ) Ci . connect_fixed_probability ( weights = 6.7 , probability = 0.02 ) compile () m = Monitor ( P , [ 'spike' ]) Compiling ... OK With the default parameters, the COBA network fires at around 20 Hz: simulate ( 1000.0 ) data = m . get ( 'spike' ) fr = m . mean_fr ( data ) print ( fr ) 19.732999999999997 Let's suppose we now want the network to fire at 30 Hz. Which parameters should we change to obtain that value? Many parameters might influence the firing rate of the network (if not all). Here, we make the assumption that the weight values for the excitatory connections (0.6) and inhibitory ones (6.7) are the most critical ones. Let's start by importing hyperopt (after installing it with pip install hyperopt ): from hyperopt import fmin , tpe , hp , STATUS_OK We define a trial() method taking values for the two hyperparameters as inputs. It starts by resetting the network, sets the excitatory and inhibitory weights to the desired value, simulates for one second, computes the mean firing rate of the population, logs the parameters and finally returns the objective function: the squared error between the recorded firing rate and 30 Hz. logger = Logger () def trial ( args ): # Retrieve the parameters w_exc = args [ 0 ] w_inh = args [ 1 ] # Reset the network reset () # Set the hyperparameters Ce . w = w_exc Ci . w = w_inh # Simulate 1 second simulate ( 1000.0 ) # Retrieve the spike recordings and the membrane potential spikes = m . get ( 'spike' ) # Compute the population firing rate fr = m . mean_fr ( spikes ) # Compute a quadratic loss around 30 Hz loss = 0.001 * ( fr - 30.0 ) ** 2 # Log the parameters logger . add_parameters ({ 'w_exc' : w_exc , 'w_inh' : w_inh }, { 'loss' : loss , 'firing_rate' : fr }) return { 'loss' : loss , 'status' : STATUS_OK , # -- store other results like this 'fr' : fr , } Logging in runs/Dec14_11-34-31_Juliens-MBP We can check that the default parameters indeed lead to a firing rate of 20 Hz: trial ([ 0.6 , 6.7 ]) {'loss': 0.10541128900000007, 'status': 'ok', 'fr': 19.732999999999997} We can now use hyperopt to find the hyperparameters making the network fire at 30 Hz. The fmin() function takes: fn : the objective function for a set of parameters. space : the search space for the hyperparameters (the prior). algo : which algorithm to use, either tpe.suggest or random.suggest max_evals : number of samples (simulations) to make. Here, we will sample the excitatory weights between 0.1 and 1, the inhibitory ones between 1 and 10. Of course, the smaller the range, the better. Refer to the doc of hyperopt for other sampling priors. best = fmin ( fn = trial , space = [ hp . uniform ( 'w_exc' , 0.1 , 1.0 ), hp . uniform ( 'w_inh' , 1.0 , 10.0 ) ], algo = tpe . suggest , max_evals = 100 ) print ( best ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:19<00:00, 5.02trial/s, best loss: 9.0150062499999e-05] {'w_exc': 0.9916507361791144, 'w_inh': 7.825494383808928} After 100 simulations, hyperopt returns a set of hyperparameter values that make the network fire at 30Hz. We can check that it is true with: trial ([ best [ 'w_exc' ], best [ 'w_inh' ]]) {'loss': 9.0150062499999e-05, 'status': 'ok', 'fr': 30.30025} There are plenty of options to hyperopt (check Trials or the parallel search using MongoDB), but this simple example should get you started. If we start tensorboard in the default directory runs/ , we can additionally visualize how the firing rate depends on w_exc and w_inh in the HPARAMS tab. logger . close () % load_ext tensorboard % tensorboard -- logdir runs","title":"Hyperparameter optimization"},{"location":"example/BoldMonitoring.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Recording BOLD signals # Download the Jupyter notebook : BoldMonitoring.ipynb This notebook demonstrates the usage of the BOLD monitoring extension. It has to be explicitly imported: from ANNarchy import * from ANNarchy.extensions.bold import * ANNarchy 4.7 (4.7.2) on darwin (posix). Background # ANNarchy pre-implements some model variants of the BOLD models presented in Stephan et al. (2007) which are variations of the Balloon Model originally designed by Buxton et al. (1998). The four balloon variants model pre-implemented in ANNarchy follow the naming scheme of Stephan et al. (2007). The model name starts with balloon_ followed by either C (i. e. classical coefficient) or R (i. e. revised coefficient) and then followed by either N which means non-linear BOLD equation or L which means linear bold equation. We only provide here the equations without much explanations, for more details please refer to the literature: Buxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855\u2013864. doi:10.1002/mrm.1910390602 Friston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466\u2013477 Buxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220\u2013S233. doi:10.1016/j.neuroimage.2004.07.013 Stephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387\u2013401. doi:10.1016/j.neuroimage.2007.07.040 Maith et al. (2021). A computational model-based analysis of basal ganglia pathway changes in Parkinson\u2019s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278\u2013 2295. doi:10.1111/ejn.14868 Maith et al. (2022). BOLD Monitoring in the Neural Simulator ANNarchy. Frontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966. Single input Balloon model # This script shows a simple example for the application of one of the default models (the balloon_RN model) on a simple network. The balloon_RN model is described by the following equations: \\[ \\tau \\cdot \\frac{ds}{dt} = \\phi \\cdot I_\\text{CBF} - \\kappa \\cdot s - \\gamma \\cdot (f_{in} - 1) \\] \\[ \\frac{df_{in}}{dt} = s \\] \\[ E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} } \\] \\[ \\tau \\cdot \\frac{dv}{dt} = \\frac{1}{\\tau_0} \\cdot (f_{in} - f_{out}) \\] \\[ f_{out} = v^{\\frac{1}{\\alpha}} \\] \\[ \\tau \\cdot \\frac{dq}{dt} = \\frac{1}{\\tau_0} \\cdot ( f_{in} \\cdot \\frac{E}{E_0} - \\frac{q}{v} \\cdot f_{out} ) \\] with revised coefficients and non-linear bold equation: \\[k_1 = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2 = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\\] \\[k_3 = 1 - \\epsilon\\] \\[ BOLD = v_0 \\cdot ( k_1 \\cdot (1-q) + k_2 \\cdot (1 - \\dfrac{q}{v}) + k_3 \\cdot (1 - v) ) \\] There are two important variables in that model: BOLD which is the output of the model and I_CBF which is the input signal, reflecting the mean firing rate of the input populations. As the BOLD model reflects the change of activity levels in the recorded area, we will implement the following experiment. We record from two populations with 100 Izhikevich neurons each. After a short period of time we raise the activity level of one population for a fixed time window. For simplicity, we do not use connections between the neurons but simply increase the noise term in the Izhikevich model. This should lead to a noticeable BOLD signal. Populations # We first create two populations of Izhikevich neurons: clear () pop0 = Population ( 100 , neuron = Izhikevich ) pop1 = Population ( 100 , neuron = Izhikevich ) As we will not have any connections between the neurons, we need to increase the noise to create some baseline activity: # Set noise to create some baseline activity pop0 . noise = 5.0 ; pop1 . noise = 5.0 The mean firing rate of the neurons will be used as an input to the BOLD model. This need to be activated explicitly as the computation of this value is quite expensive. In our example, the mean firing rate should be computed across a time window of 100 ms. The resulting value will be stored in the r attribute of the populations and can be easily recorded. # Compute mean firing rate in Hz on 100ms window pop0 . compute_firing_rate ( window = 100.0 ) pop1 . compute_firing_rate ( window = 100.0 ) # Record the mean firing rate mon_pop0 = Monitor ( pop0 , [ \"r\" ], start = False ) mon_pop1 = Monitor ( pop1 , [ \"r\" ], start = False ) BOLD Monitor definition # The BOLD monitor expects a list of populations which we want to record (in our case pop0 and pop1 ). A BOLD model should be specified, here we take balloon_RN which is the default. We then specify the mapping from the population variable that should influence the BOLD signal, in our case the mean firing rate r , to the input variable of the BOLD model I_CBF . The mean firing rate is normalized over a time window of 2000 ms and we record both input and output signals: m_bold = BoldMonitor ( populations = [ pop0 , pop1 ], # recorded populations bold_model = balloon_RN (), # BOLD model to use (default is balloon_RN) mapping = { 'I_CBF' : 'r' }, # mapping from pop.r to I_CBF normalize_input = 2000 , # time window to compute baseline. recorded_variables = [ \"I_CBF\" , \"BOLD\" ] # variables to be recorded ) Now we can compile and initialize the network: compile () Compiling ... OK Simulation # We first simulate 1 second biological time to ensure that the network reaches a stable firing rate: # Ramp up time simulate ( 1000 ) We then enable the recording of all monitors: # Start recording mon_pop0 . start () mon_pop1 . start () m_bold . start () We simulate for 5 seconds with lower noise and we increase the noise in pop0 for 5 seconds before decreasing it again: # We manipulate the noise for the half of the neurons simulate ( 5000 ) # 5s with low noise pop0 . noise = 7.5 simulate ( 5000 ) # 5s with higher noise (one population) pop0 . noise = 5 simulate ( 10000 ) # 10s with low noise # Retrieve the recordings mean_fr1 = np . mean ( mon_pop0 . get ( \"r\" ), axis = 1 ) mean_fr2 = np . mean ( mon_pop1 . get ( \"r\" ), axis = 1 ) input_data = m_bold . get ( \"I_CBF\" ) bold_data = m_bold . get ( \"BOLD\" ) Evaluation # We can now plot: the mean firing rate in the input populations. the recorded activity I which serves as an input to the BOLD model. the resulting BOLD signal. import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 6 )) grid = plt . GridSpec ( 1 , 3 , left = 0.05 , right = 0.95 ) # mean firing rate ax1 = plt . subplot ( grid [ 0 , 0 ]) ax1 . plot ( mean_fr1 , label = \"pop0\" ) ax1 . plot ( mean_fr2 , label = \"pop1\" ) plt . legend () ax1 . set_ylabel ( \"Average mean firing rate [Hz]\" , fontweight = \"bold\" , fontsize = 18 ) # BOLD input signal ax2 = plt . subplot ( grid [ 0 , 1 ]) ax2 . plot ( input_data ) ax2 . set_ylabel ( \"BOLD input I_CBF\" , fontweight = \"bold\" , fontsize = 18 ) # BOLD output signal ax3 = plt . subplot ( grid [ 0 , 2 ]) ax3 . plot ( bold_data * 100.0 ) ax3 . set_ylabel ( \"BOLD [%]\" , fontweight = \"bold\" , fontsize = 18 ) # x-axis labels as seconds for ax in [ ax1 , ax2 , ax3 ]: ax . set_xticks ( np . arange ( 0 , 21 , 2 ) * 1000 ) ax . set_xticklabels ( np . arange ( 0 , 21 , 2 )) ax . set_xlabel ( \"time [s]\" , fontweight = \"bold\" , fontsize = 18 ) plt . show () Davis model # Let's now demonstrate how to define a custom BOLD model. The default Ballon model is defined by the following code: balloon_RN = BoldModel ( parameters = \"\"\" phi = 1.0 ; kappa = 1/1.54 gamma = 1/2.46 ; E_0 = 0.34 tau = 0.98 ; alpha = 0.33 V_0 = 0.02 ; v_0 = 40.3 TE = 40/1000. ; epsilon = 1.43 r_0 = 25. ; second = 1000.0 \"\"\" , equations = \"\"\" # CBF input I_CBF = sum(I_CBF) ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second df_in/dt = s / second : init=1, min=0.01 # Balloon model E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear BOLD equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) \"\"\" , inputs = [ 'I_CBF' ] ) It is very similar to the interface of a Neuron model, with parameters and equations defined in two multi-line strings. The input signal I_CBF has to be explicitly defined in the inputs argument to help the BOLD monitor create the mapping. To demonstrate how to create a custom BOLD model, let's suppose we want a model that computes both the BOLD signal of the Balloon model and the one of the Davis model: Davis, T. L., Kwong, K. K., Weisskoff, R. M., and Rosen, B. R. (1998). Calibrated functional MRI: mapping the dynamics of oxidative metabolism. Proceedings of the National Academy of Sciences 95, 1834\u20131839 Without going into too many details, the Davis model computes the BOLD signal directly using f_in and E , without introducing a differential equation for the BOLD signal. Its implementation using the BOLD model would be: DavisModel = BoldModel ( parameters = \"\"\" second = 1000.0 phi = 1.0 # Friston et al. (2000) kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 M = 0.149 # Griffeth & Buxton (2011) alpha = 0.14 beta = 0.91 \"\"\" , equations = \"\"\" # CBF-driving input as in Friston et al. (2000) I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 \u200b # Using part of the Balloon model to calculate r (normalized CMRO2) as in Buxton et al. (2004) E = 1 - (1 - E_0)**(1 / f_in) : init=0.34 r = f_in * E / E_0 # Davis model BOLD = M * (1 - f_in**alpha * (r / f_in)**beta) : init=0 \"\"\" , inputs = [ 'I_CBF' ] ) Note that we could simply define two BOLD monitors using different models, but let's create a complex model that does both for the sake of demonstration. Let's first redefine the populations of the previous section: from ANNarchy import * from ANNarchy.extensions.bold import * clear () # Two populations of 100 izhikevich neurons pop0 = Population ( 100 , neuron = Izhikevich ) pop1 = Population ( 100 , neuron = Izhikevich ) # Set noise to create some baseline activity pop0 . noise = 5.0 ; pop1 . noise = 5.0 # Compute mean firing rate in Hz on 100ms window pop0 . compute_firing_rate ( window = 100.0 ) pop1 . compute_firing_rate ( window = 100.0 ) # Create required monitors mon_pop0 = Monitor ( pop0 , [ \"r\" ], start = False ) mon_pop1 = Monitor ( pop1 , [ \"r\" ], start = False ) We can now create a hybrid model computing both the Balloon RN model of Stephan et al. (2007) and the Davis model: balloon_Davis = BoldModel ( parameters = \"\"\" phi = 1.0 ; kappa = 1/1.54 gamma = 1/2.46 ; E_0 = 0.34 tau = 0.98 ; alpha = 0.33 V_0 = 0.02 ; v_0 = 40.3 TE = 40/1000. ; epsilon = 1.43 r_0 = 25. ; second = 1000.0 M = 0.062 ; alpha2 = 0.14 beta = 0.91 \"\"\" , equations = \"\"\" # CBF input I_CBF = sum(I_CBF) ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second df_in/dt = s / second : init=1, min=0.01 # Balloon model E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear BOLD equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) # Davis model r = f_in * E / E_0 : init=1, min=0.01 BOLD_Davis = M * (1 - f_in**alpha2 * (r / f_in)**beta) \"\"\" , inputs = [ 'I_CBF' ] ) We now only need to pass that new object to the BOLD monitor, and specify that we want to record both BOLD and BOLD_Davis : m_bold = BoldMonitor ( populations = [ pop0 , pop1 ], bold_model = balloon_Davis , mapping = { 'I_CBF' : 'r' }, normalize_input = 2000 , recorded_variables = [ \"I_CBF\" , \"BOLD\" , \"BOLD_Davis\" ] ) compile () We run the same simulation protocol and compare the two BOLD signals. Note that the value of M has been modified to give a similar amplitude to both signals: # Ramp up time simulate ( 1000 ) # Start recording mon_pop0 . start () mon_pop1 . start () m_bold . start () # we manipulate the noise for the half of the neurons simulate ( 5000 ) # 5s with low noise pop0 . noise = 7.5 simulate ( 5000 ) # 5s with higher noise (one population) pop0 . noise = 5 simulate ( 10000 ) # 10s with low noise # retrieve the recordings mean_fr1 = np . mean ( mon_pop0 . get ( \"r\" ), axis = 1 ) mean_fr2 = np . mean ( mon_pop1 . get ( \"r\" ), axis = 1 ) If_data = m_bold . get ( \"I_CBF\" ) bold_data = m_bold . get ( \"BOLD\" ) davis_data = m_bold . get ( \"BOLD_Davis\" ) import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 8 )) grid = plt . GridSpec ( 1 , 2 , left = 0.05 , right = 0.95 ) # mean firing rate ax1 = plt . subplot ( grid [ 0 , 0 ]) ax1 . plot ( mean_fr1 , label = \"pop0\" ) ax1 . plot ( mean_fr2 , label = \"pop1\" ) plt . legend () ax1 . set_ylabel ( \"Average firing rate [Hz]\" , fontweight = \"bold\" , fontsize = 18 ) # BOLD input signal as percent ax2 = plt . subplot ( grid [ 0 , 1 ]) ax2 . plot ( bold_data * 100.0 , label = \"Balloon_RN\" ) ax2 . plot ( davis_data * 100.0 , label = \"Davis\" ) plt . legend () ax2 . set_ylabel ( \"BOLD [%]\" , fontweight = \"bold\" , fontsize = 18 ) # x-axis labels as seconds for ax in [ ax1 , ax2 ]: ax . set_xticks ( np . arange ( 0 , 21 , 2 ) * 1000 ) ax . set_xticklabels ( np . arange ( 0 , 21 , 2 )) ax . set_xlabel ( \"time [s]\" , fontweight = \"bold\" , fontsize = 18 ) plt . show ()","title":"BOLD monitoring"},{"location":"example/BoldMonitoring.html#recording-bold-signals","text":"Download the Jupyter notebook : BoldMonitoring.ipynb This notebook demonstrates the usage of the BOLD monitoring extension. It has to be explicitly imported: from ANNarchy import * from ANNarchy.extensions.bold import * ANNarchy 4.7 (4.7.2) on darwin (posix).","title":"Recording BOLD signals"},{"location":"example/BoldMonitoring.html#background","text":"ANNarchy pre-implements some model variants of the BOLD models presented in Stephan et al. (2007) which are variations of the Balloon Model originally designed by Buxton et al. (1998). The four balloon variants model pre-implemented in ANNarchy follow the naming scheme of Stephan et al. (2007). The model name starts with balloon_ followed by either C (i. e. classical coefficient) or R (i. e. revised coefficient) and then followed by either N which means non-linear BOLD equation or L which means linear bold equation. We only provide here the equations without much explanations, for more details please refer to the literature: Buxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855\u2013864. doi:10.1002/mrm.1910390602 Friston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466\u2013477 Buxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220\u2013S233. doi:10.1016/j.neuroimage.2004.07.013 Stephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387\u2013401. doi:10.1016/j.neuroimage.2007.07.040 Maith et al. (2021). A computational model-based analysis of basal ganglia pathway changes in Parkinson\u2019s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278\u2013 2295. doi:10.1111/ejn.14868 Maith et al. (2022). BOLD Monitoring in the Neural Simulator ANNarchy. Frontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966.","title":"Background"},{"location":"example/BoldMonitoring.html#single-input-balloon-model","text":"This script shows a simple example for the application of one of the default models (the balloon_RN model) on a simple network. The balloon_RN model is described by the following equations: \\[ \\tau \\cdot \\frac{ds}{dt} = \\phi \\cdot I_\\text{CBF} - \\kappa \\cdot s - \\gamma \\cdot (f_{in} - 1) \\] \\[ \\frac{df_{in}}{dt} = s \\] \\[ E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} } \\] \\[ \\tau \\cdot \\frac{dv}{dt} = \\frac{1}{\\tau_0} \\cdot (f_{in} - f_{out}) \\] \\[ f_{out} = v^{\\frac{1}{\\alpha}} \\] \\[ \\tau \\cdot \\frac{dq}{dt} = \\frac{1}{\\tau_0} \\cdot ( f_{in} \\cdot \\frac{E}{E_0} - \\frac{q}{v} \\cdot f_{out} ) \\] with revised coefficients and non-linear bold equation: \\[k_1 = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2 = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\\] \\[k_3 = 1 - \\epsilon\\] \\[ BOLD = v_0 \\cdot ( k_1 \\cdot (1-q) + k_2 \\cdot (1 - \\dfrac{q}{v}) + k_3 \\cdot (1 - v) ) \\] There are two important variables in that model: BOLD which is the output of the model and I_CBF which is the input signal, reflecting the mean firing rate of the input populations. As the BOLD model reflects the change of activity levels in the recorded area, we will implement the following experiment. We record from two populations with 100 Izhikevich neurons each. After a short period of time we raise the activity level of one population for a fixed time window. For simplicity, we do not use connections between the neurons but simply increase the noise term in the Izhikevich model. This should lead to a noticeable BOLD signal.","title":"Single input Balloon model"},{"location":"example/BoldMonitoring.html#populations","text":"We first create two populations of Izhikevich neurons: clear () pop0 = Population ( 100 , neuron = Izhikevich ) pop1 = Population ( 100 , neuron = Izhikevich ) As we will not have any connections between the neurons, we need to increase the noise to create some baseline activity: # Set noise to create some baseline activity pop0 . noise = 5.0 ; pop1 . noise = 5.0 The mean firing rate of the neurons will be used as an input to the BOLD model. This need to be activated explicitly as the computation of this value is quite expensive. In our example, the mean firing rate should be computed across a time window of 100 ms. The resulting value will be stored in the r attribute of the populations and can be easily recorded. # Compute mean firing rate in Hz on 100ms window pop0 . compute_firing_rate ( window = 100.0 ) pop1 . compute_firing_rate ( window = 100.0 ) # Record the mean firing rate mon_pop0 = Monitor ( pop0 , [ \"r\" ], start = False ) mon_pop1 = Monitor ( pop1 , [ \"r\" ], start = False )","title":"Populations"},{"location":"example/BoldMonitoring.html#bold-monitor-definition","text":"The BOLD monitor expects a list of populations which we want to record (in our case pop0 and pop1 ). A BOLD model should be specified, here we take balloon_RN which is the default. We then specify the mapping from the population variable that should influence the BOLD signal, in our case the mean firing rate r , to the input variable of the BOLD model I_CBF . The mean firing rate is normalized over a time window of 2000 ms and we record both input and output signals: m_bold = BoldMonitor ( populations = [ pop0 , pop1 ], # recorded populations bold_model = balloon_RN (), # BOLD model to use (default is balloon_RN) mapping = { 'I_CBF' : 'r' }, # mapping from pop.r to I_CBF normalize_input = 2000 , # time window to compute baseline. recorded_variables = [ \"I_CBF\" , \"BOLD\" ] # variables to be recorded ) Now we can compile and initialize the network: compile () Compiling ... OK","title":"BOLD Monitor definition"},{"location":"example/BoldMonitoring.html#simulation","text":"We first simulate 1 second biological time to ensure that the network reaches a stable firing rate: # Ramp up time simulate ( 1000 ) We then enable the recording of all monitors: # Start recording mon_pop0 . start () mon_pop1 . start () m_bold . start () We simulate for 5 seconds with lower noise and we increase the noise in pop0 for 5 seconds before decreasing it again: # We manipulate the noise for the half of the neurons simulate ( 5000 ) # 5s with low noise pop0 . noise = 7.5 simulate ( 5000 ) # 5s with higher noise (one population) pop0 . noise = 5 simulate ( 10000 ) # 10s with low noise # Retrieve the recordings mean_fr1 = np . mean ( mon_pop0 . get ( \"r\" ), axis = 1 ) mean_fr2 = np . mean ( mon_pop1 . get ( \"r\" ), axis = 1 ) input_data = m_bold . get ( \"I_CBF\" ) bold_data = m_bold . get ( \"BOLD\" )","title":"Simulation"},{"location":"example/BoldMonitoring.html#evaluation","text":"We can now plot: the mean firing rate in the input populations. the recorded activity I which serves as an input to the BOLD model. the resulting BOLD signal. import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 6 )) grid = plt . GridSpec ( 1 , 3 , left = 0.05 , right = 0.95 ) # mean firing rate ax1 = plt . subplot ( grid [ 0 , 0 ]) ax1 . plot ( mean_fr1 , label = \"pop0\" ) ax1 . plot ( mean_fr2 , label = \"pop1\" ) plt . legend () ax1 . set_ylabel ( \"Average mean firing rate [Hz]\" , fontweight = \"bold\" , fontsize = 18 ) # BOLD input signal ax2 = plt . subplot ( grid [ 0 , 1 ]) ax2 . plot ( input_data ) ax2 . set_ylabel ( \"BOLD input I_CBF\" , fontweight = \"bold\" , fontsize = 18 ) # BOLD output signal ax3 = plt . subplot ( grid [ 0 , 2 ]) ax3 . plot ( bold_data * 100.0 ) ax3 . set_ylabel ( \"BOLD [%]\" , fontweight = \"bold\" , fontsize = 18 ) # x-axis labels as seconds for ax in [ ax1 , ax2 , ax3 ]: ax . set_xticks ( np . arange ( 0 , 21 , 2 ) * 1000 ) ax . set_xticklabels ( np . arange ( 0 , 21 , 2 )) ax . set_xlabel ( \"time [s]\" , fontweight = \"bold\" , fontsize = 18 ) plt . show ()","title":"Evaluation"},{"location":"example/BoldMonitoring.html#davis-model","text":"Let's now demonstrate how to define a custom BOLD model. The default Ballon model is defined by the following code: balloon_RN = BoldModel ( parameters = \"\"\" phi = 1.0 ; kappa = 1/1.54 gamma = 1/2.46 ; E_0 = 0.34 tau = 0.98 ; alpha = 0.33 V_0 = 0.02 ; v_0 = 40.3 TE = 40/1000. ; epsilon = 1.43 r_0 = 25. ; second = 1000.0 \"\"\" , equations = \"\"\" # CBF input I_CBF = sum(I_CBF) ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second df_in/dt = s / second : init=1, min=0.01 # Balloon model E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear BOLD equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) \"\"\" , inputs = [ 'I_CBF' ] ) It is very similar to the interface of a Neuron model, with parameters and equations defined in two multi-line strings. The input signal I_CBF has to be explicitly defined in the inputs argument to help the BOLD monitor create the mapping. To demonstrate how to create a custom BOLD model, let's suppose we want a model that computes both the BOLD signal of the Balloon model and the one of the Davis model: Davis, T. L., Kwong, K. K., Weisskoff, R. M., and Rosen, B. R. (1998). Calibrated functional MRI: mapping the dynamics of oxidative metabolism. Proceedings of the National Academy of Sciences 95, 1834\u20131839 Without going into too many details, the Davis model computes the BOLD signal directly using f_in and E , without introducing a differential equation for the BOLD signal. Its implementation using the BOLD model would be: DavisModel = BoldModel ( parameters = \"\"\" second = 1000.0 phi = 1.0 # Friston et al. (2000) kappa = 1/1.54 gamma = 1/2.46 E_0 = 0.34 M = 0.149 # Griffeth & Buxton (2011) alpha = 0.14 beta = 0.91 \"\"\" , equations = \"\"\" # CBF-driving input as in Friston et al. (2000) I_CBF = sum(I_CBF) : init=0 ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second : init=0 df_in/dt = s / second : init=1, min=0.01 \u200b # Using part of the Balloon model to calculate r (normalized CMRO2) as in Buxton et al. (2004) E = 1 - (1 - E_0)**(1 / f_in) : init=0.34 r = f_in * E / E_0 # Davis model BOLD = M * (1 - f_in**alpha * (r / f_in)**beta) : init=0 \"\"\" , inputs = [ 'I_CBF' ] ) Note that we could simply define two BOLD monitors using different models, but let's create a complex model that does both for the sake of demonstration. Let's first redefine the populations of the previous section: from ANNarchy import * from ANNarchy.extensions.bold import * clear () # Two populations of 100 izhikevich neurons pop0 = Population ( 100 , neuron = Izhikevich ) pop1 = Population ( 100 , neuron = Izhikevich ) # Set noise to create some baseline activity pop0 . noise = 5.0 ; pop1 . noise = 5.0 # Compute mean firing rate in Hz on 100ms window pop0 . compute_firing_rate ( window = 100.0 ) pop1 . compute_firing_rate ( window = 100.0 ) # Create required monitors mon_pop0 = Monitor ( pop0 , [ \"r\" ], start = False ) mon_pop1 = Monitor ( pop1 , [ \"r\" ], start = False ) We can now create a hybrid model computing both the Balloon RN model of Stephan et al. (2007) and the Davis model: balloon_Davis = BoldModel ( parameters = \"\"\" phi = 1.0 ; kappa = 1/1.54 gamma = 1/2.46 ; E_0 = 0.34 tau = 0.98 ; alpha = 0.33 V_0 = 0.02 ; v_0 = 40.3 TE = 40/1000. ; epsilon = 1.43 r_0 = 25. ; second = 1000.0 M = 0.062 ; alpha2 = 0.14 beta = 0.91 \"\"\" , equations = \"\"\" # CBF input I_CBF = sum(I_CBF) ds/dt = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second df_in/dt = s / second : init=1, min=0.01 # Balloon model E = 1 - (1 - E_0)**(1 / f_in) : init=0.3424 dq/dt = (f_in * E / E_0 - (q / v) * f_out)/(tau*second) : init=1, min=0.01 dv/dt = (f_in - f_out)/(tau*second) : init=1, min=0.01 f_out = v**(1 / alpha) : init=1, min=0.01 # Revised coefficients k_1 = 4.3 * v_0 * E_0 * TE k_2 = epsilon * r_0 * E_0 * TE k_3 = 1.0 - epsilon # Non-linear BOLD equation BOLD = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) # Davis model r = f_in * E / E_0 : init=1, min=0.01 BOLD_Davis = M * (1 - f_in**alpha2 * (r / f_in)**beta) \"\"\" , inputs = [ 'I_CBF' ] ) We now only need to pass that new object to the BOLD monitor, and specify that we want to record both BOLD and BOLD_Davis : m_bold = BoldMonitor ( populations = [ pop0 , pop1 ], bold_model = balloon_Davis , mapping = { 'I_CBF' : 'r' }, normalize_input = 2000 , recorded_variables = [ \"I_CBF\" , \"BOLD\" , \"BOLD_Davis\" ] ) compile () We run the same simulation protocol and compare the two BOLD signals. Note that the value of M has been modified to give a similar amplitude to both signals: # Ramp up time simulate ( 1000 ) # Start recording mon_pop0 . start () mon_pop1 . start () m_bold . start () # we manipulate the noise for the half of the neurons simulate ( 5000 ) # 5s with low noise pop0 . noise = 7.5 simulate ( 5000 ) # 5s with higher noise (one population) pop0 . noise = 5 simulate ( 10000 ) # 10s with low noise # retrieve the recordings mean_fr1 = np . mean ( mon_pop0 . get ( \"r\" ), axis = 1 ) mean_fr2 = np . mean ( mon_pop1 . get ( \"r\" ), axis = 1 ) If_data = m_bold . get ( \"I_CBF\" ) bold_data = m_bold . get ( \"BOLD\" ) davis_data = m_bold . get ( \"BOLD_Davis\" ) import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 8 )) grid = plt . GridSpec ( 1 , 2 , left = 0.05 , right = 0.95 ) # mean firing rate ax1 = plt . subplot ( grid [ 0 , 0 ]) ax1 . plot ( mean_fr1 , label = \"pop0\" ) ax1 . plot ( mean_fr2 , label = \"pop1\" ) plt . legend () ax1 . set_ylabel ( \"Average firing rate [Hz]\" , fontweight = \"bold\" , fontsize = 18 ) # BOLD input signal as percent ax2 = plt . subplot ( grid [ 0 , 1 ]) ax2 . plot ( bold_data * 100.0 , label = \"Balloon_RN\" ) ax2 . plot ( davis_data * 100.0 , label = \"Davis\" ) plt . legend () ax2 . set_ylabel ( \"BOLD [%]\" , fontweight = \"bold\" , fontsize = 18 ) # x-axis labels as seconds for ax in [ ax1 , ax2 ]: ax . set_xticks ( np . arange ( 0 , 21 , 2 ) * 1000 ) ax . set_xticklabels ( np . arange ( 0 , 21 , 2 )) ax . set_xlabel ( \"time [s]\" , fontweight = \"bold\" , fontsize = 18 ) plt . show ()","title":"Davis model"},{"location":"example/COBA.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); COBA and CUBA networks # Download the Jupyter notebook : COBA.ipynb The scripts COBA.py and CUBA.py in examples/vogels_abbott reproduce the two first benchmarks used in: Brette, R., Rudolph, M., Carnevale, T., Hines, M., Beeman, D., Bower, J. M., et al. (2007), Simulation of networks of spiking neurons: a review of tools and strategies., J. Comput. Neurosci., 23, 3, 349\u201398 Both are based on the balanced network proposed by: Vogels, T. P. and Abbott, L. F. (2005), Signal propagation and logic gating in networks of integrate-and-fire neurons., J. Neurosci., 25, 46, 10786\u201395 The network is composed of 4000 neurons (3200 excitatory and 800 inhibitory), reciprocally connected with a probability of 0.02 (sparse connection). The CUBA network uses a current-based integrate-and-fire neuron model: \\[\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) - g_\\text{inh} (t)\\] while the COBA model uses conductance-based IF neurons: \\[\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) * (E_\\text{exc}) - v(t)) + g_\\text{inh} (t) * (E_\\text{inh}) - v(t)) + I(t)\\] Apart from the neuron model and synaptic weights, both networks are equal, so we'll focus on the COBA network here. The discretization step has to be set to 0.1 ms: from ANNarchy import * setup ( dt = 0.1 ) ANNarchy 4.7 (4.7.2) on darwin (posix). Neuron definition # COBA = Neuron ( parameters = \"\"\" El = -60.0 : population Vr = -60.0 : population Erev_exc = 0.0 : population Erev_inh = -80.0 : population Vt = -50.0 : population tau = 20.0 : population tau_exc = 5.0 : population tau_inh = 10.0 : population I = 20.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I tau_exc * dg_exc/dt = - g_exc tau_inh * dg_inh/dt = - g_inh \"\"\" , spike = \"v > Vt\" , reset = \"v = Vr\" , refractory = 5.0 ) CUBA = Neuron ( parameters = \"\"\" El = -49.0 : population Vr = -60.0 : population Vt = -50.0 : population tau_m = 20.0 : population tau_exc = 5.0 : population tau_inh = 10.0 : population \"\"\" , equations = \"\"\" tau_m * dv/dt = (El - v) + g_exc + g_inh tau_exc * dg_exc/dt = - g_exc tau_inh * dg_inh/dt = - g_inh \"\"\" , spike = \"v > Vt\" , reset = \"v = Vr\" , refractory = 5.0 ) The neurons define exponentially-decreasing conductance g_exc and g_inh for the excitatory and inhibitory conductances/currents, respectively. They also define a refractory period of 5 ms. Population # P = Population ( geometry = 4000 , neuron = COBA ) Pe = P [: 3200 ] Pi = P [ 3200 :] We create a population of 4000 COBA neurons, and assign the 3200 first ones to the excitatory population and the 800 last ones to the inhibitory population. It would have been equivalent to declare two separate populations as: Pe = Population ( geometry = 3200 , neuron = COBA ) Pi = Population ( geometry = 800 , neuron = COBA ) but splitting a global population allows to apply methods to all neurons, for example when recording all spikes with a single monitor, or when initializing populations parameters uniformly: P . v = Normal ( - 55.0 , 5.0 ) P . g_exc = Normal ( 4.0 , 1.5 ) P . g_inh = Normal ( 20.0 , 12.0 ) Connections # The neurons are randomly connected with a probability of 0.02. Excitatory neurons project on all other neurons with the target \"exc\" and a weight of 0.6, while the inhibitory neurons have the target \"inh\" and a weight of 6.7. Ce = Projection ( pre = Pe , post = P , target = 'exc' ) Ce . connect_fixed_probability ( weights = 0.6 , probability = 0.02 ) Ci = Projection ( pre = Pi , post = P , target = 'inh' ) Ci . connect_fixed_probability ( weights = 6.7 , probability = 0.02 ) <ANNarchy.core.Projection.Projection at 0x11290e640> compile () Compiling ... OK Simulation # We first define a monitor to record the spikes emitted in the whole population: m = Monitor ( P , [ 'spike' ]) We can then simulate for 1 second: simulate ( 1000. ) We retrieve the recorded spikes from the monitor: data = m . get ( 'spike' ) and compute a raster plot from the data: t , n = m . raster_plot ( data ) t and n are lists representing for each spike emitted during the simulation the time at which it was emitted and the index the neuron which fired. The length of this list represents the total number of spikes in the popultion, so we can compute the population mean firing rate: print ( 'Mean firing rate in the population: ' + str ( len ( t ) / 4000. ) + 'Hz' ) Mean firing rate in the population: 18.77925Hz Finally, we can show the raster plot with pylab: import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 12 )) plt . plot ( t , n , '.' , markersize = 0.5 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show ()","title":"COBA"},{"location":"example/COBA.html#coba-and-cuba-networks","text":"Download the Jupyter notebook : COBA.ipynb The scripts COBA.py and CUBA.py in examples/vogels_abbott reproduce the two first benchmarks used in: Brette, R., Rudolph, M., Carnevale, T., Hines, M., Beeman, D., Bower, J. M., et al. (2007), Simulation of networks of spiking neurons: a review of tools and strategies., J. Comput. Neurosci., 23, 3, 349\u201398 Both are based on the balanced network proposed by: Vogels, T. P. and Abbott, L. F. (2005), Signal propagation and logic gating in networks of integrate-and-fire neurons., J. Neurosci., 25, 46, 10786\u201395 The network is composed of 4000 neurons (3200 excitatory and 800 inhibitory), reciprocally connected with a probability of 0.02 (sparse connection). The CUBA network uses a current-based integrate-and-fire neuron model: \\[\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) - g_\\text{inh} (t)\\] while the COBA model uses conductance-based IF neurons: \\[\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) * (E_\\text{exc}) - v(t)) + g_\\text{inh} (t) * (E_\\text{inh}) - v(t)) + I(t)\\] Apart from the neuron model and synaptic weights, both networks are equal, so we'll focus on the COBA network here. The discretization step has to be set to 0.1 ms: from ANNarchy import * setup ( dt = 0.1 ) ANNarchy 4.7 (4.7.2) on darwin (posix).","title":"COBA and CUBA networks"},{"location":"example/COBA.html#neuron-definition","text":"COBA = Neuron ( parameters = \"\"\" El = -60.0 : population Vr = -60.0 : population Erev_exc = 0.0 : population Erev_inh = -80.0 : population Vt = -50.0 : population tau = 20.0 : population tau_exc = 5.0 : population tau_inh = 10.0 : population I = 20.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I tau_exc * dg_exc/dt = - g_exc tau_inh * dg_inh/dt = - g_inh \"\"\" , spike = \"v > Vt\" , reset = \"v = Vr\" , refractory = 5.0 ) CUBA = Neuron ( parameters = \"\"\" El = -49.0 : population Vr = -60.0 : population Vt = -50.0 : population tau_m = 20.0 : population tau_exc = 5.0 : population tau_inh = 10.0 : population \"\"\" , equations = \"\"\" tau_m * dv/dt = (El - v) + g_exc + g_inh tau_exc * dg_exc/dt = - g_exc tau_inh * dg_inh/dt = - g_inh \"\"\" , spike = \"v > Vt\" , reset = \"v = Vr\" , refractory = 5.0 ) The neurons define exponentially-decreasing conductance g_exc and g_inh for the excitatory and inhibitory conductances/currents, respectively. They also define a refractory period of 5 ms.","title":"Neuron definition"},{"location":"example/COBA.html#population","text":"P = Population ( geometry = 4000 , neuron = COBA ) Pe = P [: 3200 ] Pi = P [ 3200 :] We create a population of 4000 COBA neurons, and assign the 3200 first ones to the excitatory population and the 800 last ones to the inhibitory population. It would have been equivalent to declare two separate populations as: Pe = Population ( geometry = 3200 , neuron = COBA ) Pi = Population ( geometry = 800 , neuron = COBA ) but splitting a global population allows to apply methods to all neurons, for example when recording all spikes with a single monitor, or when initializing populations parameters uniformly: P . v = Normal ( - 55.0 , 5.0 ) P . g_exc = Normal ( 4.0 , 1.5 ) P . g_inh = Normal ( 20.0 , 12.0 )","title":"Population"},{"location":"example/COBA.html#connections","text":"The neurons are randomly connected with a probability of 0.02. Excitatory neurons project on all other neurons with the target \"exc\" and a weight of 0.6, while the inhibitory neurons have the target \"inh\" and a weight of 6.7. Ce = Projection ( pre = Pe , post = P , target = 'exc' ) Ce . connect_fixed_probability ( weights = 0.6 , probability = 0.02 ) Ci = Projection ( pre = Pi , post = P , target = 'inh' ) Ci . connect_fixed_probability ( weights = 6.7 , probability = 0.02 ) <ANNarchy.core.Projection.Projection at 0x11290e640> compile () Compiling ... OK","title":"Connections"},{"location":"example/COBA.html#simulation","text":"We first define a monitor to record the spikes emitted in the whole population: m = Monitor ( P , [ 'spike' ]) We can then simulate for 1 second: simulate ( 1000. ) We retrieve the recorded spikes from the monitor: data = m . get ( 'spike' ) and compute a raster plot from the data: t , n = m . raster_plot ( data ) t and n are lists representing for each spike emitted during the simulation the time at which it was emitted and the index the neuron which fired. The length of this list represents the total number of spikes in the popultion, so we can compute the population mean firing rate: print ( 'Mean firing rate in the population: ' + str ( len ( t ) / 4000. ) + 'Hz' ) Mean firing rate in the population: 18.77925Hz Finally, we can show the raster plot with pylab: import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 12 )) plt . plot ( t , n , '.' , markersize = 0.5 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show ()","title":"Simulation"},{"location":"example/GapJunctions.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Gap Junctions # Download the Jupyter notebook : GapJunctions.ipynb A simple network with gap junctions. This is a reimplementation of the Brian example: http://brian2.readthedocs.org/en/2.0b3/examples/synapses.gapjunctions.html from ANNarchy import * clear () setup ( dt = 0.1 ) neuron = Neuron ( parameters = \"v0 = 1.05: population; tau = 10.0: population\" , equations = \"tau*dv/dt = v0 - v + g_gap\" , spike = \"v > 1.\" , reset = \"v = 0.\" ) gap_junction = Synapse ( psp = \"w * (pre.v - post.v)\" ) pop = Population ( 10 , neuron ) pop . v = np . linspace ( 0. , 1. , 10 ) proj = Projection ( pop , pop , 'gap' , gap_junction ) proj . connect_all_to_all ( 0.02 ) trace = Monitor ( pop [ 0 ] + pop [ 5 ], 'v' ) compile () simulate ( 500. ) data = trace . get ( 'v' ) import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . plot ( data [:, 0 ]) plt . plot ( data [:, 1 ]) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( 'v' ) plt . show () ANNarchy 4.7 (4.7.2) on darwin (posix). Compiling ... OK","title":"Gap junctions"},{"location":"example/GapJunctions.html#gap-junctions","text":"Download the Jupyter notebook : GapJunctions.ipynb A simple network with gap junctions. This is a reimplementation of the Brian example: http://brian2.readthedocs.org/en/2.0b3/examples/synapses.gapjunctions.html from ANNarchy import * clear () setup ( dt = 0.1 ) neuron = Neuron ( parameters = \"v0 = 1.05: population; tau = 10.0: population\" , equations = \"tau*dv/dt = v0 - v + g_gap\" , spike = \"v > 1.\" , reset = \"v = 0.\" ) gap_junction = Synapse ( psp = \"w * (pre.v - post.v)\" ) pop = Population ( 10 , neuron ) pop . v = np . linspace ( 0. , 1. , 10 ) proj = Projection ( pop , pop , 'gap' , gap_junction ) proj . connect_all_to_all ( 0.02 ) trace = Monitor ( pop [ 0 ] + pop [ 5 ], 'v' ) compile () simulate ( 500. ) data = trace . get ( 'v' ) import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . plot ( data [:, 0 ]) plt . plot ( data [:, 1 ]) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( 'v' ) plt . show () ANNarchy 4.7 (4.7.2) on darwin (posix). Compiling ... OK","title":"Gap Junctions"},{"location":"example/HodgkinHuxley.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Hodgkin Huxley neuron # Download the Jupyter notebook : HodgkinHuxley.ipynb Simple Hodgkin-Huxley neuron. from ANNarchy import * clear () dt = 0.01 setup ( dt = dt ) HH = Neuron ( parameters = \"\"\" C = 1.0 # Capacitance VL = -59.387 # Leak voltage VK = -82.0 # Potassium reversal voltage VNa = 45.0 # Sodium reveral voltage gK = 36.0 # Maximal Potassium conductance gNa = 120.0 # Maximal Sodium conductance gL = 0.3 # Leak conductance vt = 30.0 # Threshold for spike emission I = 0.0 # External current \"\"\" , equations = \"\"\" # Previous membrane potential prev_V = V # Voltage-dependency parameters an = 0.01 * (V + 60.0) / (1.0 - exp(-0.1* (V + 60.0) ) ) am = 0.1 * (V + 45.0) / (1.0 - exp (- 0.1 * ( V + 45.0 ))) ah = 0.07 * exp(- 0.05 * ( V + 70.0 )) bn = 0.125 * exp (- 0.0125 * (V + 70.0)) bm = 4.0 * exp (- (V + 70.0) / 80.0) bh = 1.0/(1.0 + exp (- 0.1 * ( V + 40.0 )) ) # Alpha/Beta functions dn/dt = an * (1.0 - n) - bn * n : init = 0.3, midpoint dm/dt = am * (1.0 - m) - bm * m : init = 0.0, midpoint dh/dt = ah * (1.0 - h) - bh * h : init = 0.6, midpoint # Membrane equation C * dV/dt = gL * (VL - V ) + gK * n**4 * (VK - V) + gNa * m**3 * h * (VNa - V) + I : midpoint \"\"\" , spike = \"\"\" # Spike is emitted when the membrane potential crosses the threshold from below (V > vt) and (prev_V <= vt) \"\"\" , reset = \"\"\" # Nothing to do, it is built-in... \"\"\" ) pop = Population ( neuron = HH , geometry = 1 ) pop . V = - 50.0 compile () m = Monitor ( pop , [ 'spike' , 'V' , 'n' , 'm' , 'h' ]) # Preparation simulate ( 100.0 ) # Current impulse for 1 ms pop . I = 200.0 simulate ( 1.0 ) # Reset pop . I = 0.0 simulate ( 100.0 ) data = m . get () tstart = int ( 90.0 / dt ) tstop = int ( 120.0 / dt ) import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 2 , 2 , 1 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'V' ][ tstart : tstop , 0 ]) plt . title ( 'V' ) plt . subplot ( 2 , 2 , 2 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'n' ][ tstart : tstop , 0 ]) plt . title ( 'n' ) plt . ylim (( 0.0 , 1.0 )) plt . subplot ( 2 , 2 , 3 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'm' ][ tstart : tstop , 0 ]) plt . title ( 'm' ) plt . ylim (( 0.0 , 1.0 )) plt . subplot ( 2 , 2 , 4 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'h' ][ tstart : tstop , 0 ]) plt . title ( 'h' ) plt . ylim (( 0.0 , 1.0 )) plt . show () ANNarchy 4.7 (4.7.2) on darwin (posix). Compiling ... OK","title":"Hodgkin-Huxley"},{"location":"example/HodgkinHuxley.html#hodgkin-huxley-neuron","text":"Download the Jupyter notebook : HodgkinHuxley.ipynb Simple Hodgkin-Huxley neuron. from ANNarchy import * clear () dt = 0.01 setup ( dt = dt ) HH = Neuron ( parameters = \"\"\" C = 1.0 # Capacitance VL = -59.387 # Leak voltage VK = -82.0 # Potassium reversal voltage VNa = 45.0 # Sodium reveral voltage gK = 36.0 # Maximal Potassium conductance gNa = 120.0 # Maximal Sodium conductance gL = 0.3 # Leak conductance vt = 30.0 # Threshold for spike emission I = 0.0 # External current \"\"\" , equations = \"\"\" # Previous membrane potential prev_V = V # Voltage-dependency parameters an = 0.01 * (V + 60.0) / (1.0 - exp(-0.1* (V + 60.0) ) ) am = 0.1 * (V + 45.0) / (1.0 - exp (- 0.1 * ( V + 45.0 ))) ah = 0.07 * exp(- 0.05 * ( V + 70.0 )) bn = 0.125 * exp (- 0.0125 * (V + 70.0)) bm = 4.0 * exp (- (V + 70.0) / 80.0) bh = 1.0/(1.0 + exp (- 0.1 * ( V + 40.0 )) ) # Alpha/Beta functions dn/dt = an * (1.0 - n) - bn * n : init = 0.3, midpoint dm/dt = am * (1.0 - m) - bm * m : init = 0.0, midpoint dh/dt = ah * (1.0 - h) - bh * h : init = 0.6, midpoint # Membrane equation C * dV/dt = gL * (VL - V ) + gK * n**4 * (VK - V) + gNa * m**3 * h * (VNa - V) + I : midpoint \"\"\" , spike = \"\"\" # Spike is emitted when the membrane potential crosses the threshold from below (V > vt) and (prev_V <= vt) \"\"\" , reset = \"\"\" # Nothing to do, it is built-in... \"\"\" ) pop = Population ( neuron = HH , geometry = 1 ) pop . V = - 50.0 compile () m = Monitor ( pop , [ 'spike' , 'V' , 'n' , 'm' , 'h' ]) # Preparation simulate ( 100.0 ) # Current impulse for 1 ms pop . I = 200.0 simulate ( 1.0 ) # Reset pop . I = 0.0 simulate ( 100.0 ) data = m . get () tstart = int ( 90.0 / dt ) tstop = int ( 120.0 / dt ) import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 2 , 2 , 1 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'V' ][ tstart : tstop , 0 ]) plt . title ( 'V' ) plt . subplot ( 2 , 2 , 2 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'n' ][ tstart : tstop , 0 ]) plt . title ( 'n' ) plt . ylim (( 0.0 , 1.0 )) plt . subplot ( 2 , 2 , 3 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'm' ][ tstart : tstop , 0 ]) plt . title ( 'm' ) plt . ylim (( 0.0 , 1.0 )) plt . subplot ( 2 , 2 , 4 ) plt . plot ( 90.0 + dt * np . arange ( tstop - tstart ), data [ 'h' ][ tstart : tstop , 0 ]) plt . title ( 'h' ) plt . ylim (( 0.0 , 1.0 )) plt . show () ANNarchy 4.7 (4.7.2) on darwin (posix). Compiling ... OK","title":"Hodgkin Huxley neuron"},{"location":"example/Hybrid.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Hybrid network # Download the Jupyter notebook : Hybrid.ipynb Simple example showing hybrid spike/rate-coded networks. Reproduces Fig.4 of (Vitay, Dinkelbach and Hamker, 2015) from ANNarchy import * clear () setup ( dt = 0.1 ) # Rate-coded input neuron input_neuron = Neuron ( parameters = \"baseline = 0.0\" , equations = \"r = baseline\" ) # Rate-coded output neuron simple_neuron = Neuron ( equations = \"r = sum(exc)\" ) # Rate-coded population for input pop1 = Population ( geometry = 1 , neuron = input_neuron ) # Poisson Population to encode pop2 = PoissonPopulation ( geometry = 1000 , target = \"exc\" ) proj = Projection ( pop1 , pop2 , 'exc' ) . connect_all_to_all ( weights = 1. ) # Rate-coded population to decode pop3 = Population ( geometry = 1000 , neuron = simple_neuron ) proj = DecodingProjection ( pop2 , pop3 , 'exc' , window = 10.0 ) def diagonal ( pre , post , weights ): \"\"\" Simple connector pattern to progressively connect each post-synaptic neuron to a growing number of pre-synaptic neurons. \"\"\" lil = CSR () for rk_post in range ( post . size ): lil . add ( rk_post , range (( rk_post + 1 )), [ weights ], [ 0 ] ) return lil proj . connect_with_func ( method = diagonal , weights = 1. ) compile () # Monitors m1 = Monitor ( pop1 , 'r' ) m2 = Monitor ( pop2 , 'spike' ) m3 = Monitor ( pop3 , 'r' ) # Simulate duration = 250. # 0 Hz pop1 . baseline = 0.0 simulate ( duration ) # 10 Hz pop1 . baseline = 10.0 simulate ( duration ) # 50 Hz pop1 . baseline = 50.0 simulate ( duration ) # 100 Hz pop1 . baseline = 100.0 simulate ( duration ) # Get recordings data1 = m1 . get () data2 = m2 . get () data3 = m3 . get () # Raster plot of the spiking population t , n = m2 . raster_plot ( data2 [ 'spike' ]) # Variance of the the decoded firing rate data_10 = data3 [ 'r' ][ int ( 1.0 * duration / dt ()): int ( 2 * duration / dt ()), :] data_50 = data3 [ 'r' ][ int ( 2.0 * duration / dt ()): int ( 3 * duration / dt ()), :] data_100 = data3 [ 'r' ][ int ( 3.0 * duration / dt ()): int ( 4 * duration / dt ()), :] var_10 = np . mean ( np . abs (( data_10 - 10. ) / 10. ), axis = 0 ) var_50 = np . mean ( np . abs (( data_50 - 50. ) / 50. ), axis = 0 ) var_100 = np . mean ( np . abs (( data_100 - 100. ) / 100. ), axis = 0 ) ### Plot the results import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 3 , 1 , 1 ) plt . plot ( t , n , '.' , markersize = 0.5 ) plt . title ( 'a) Raster plot' ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neurons' ) plt . xlim (( 0 , 4 * duration )) plt . subplot ( 3 , 1 , 2 ) plt . plot ( np . arange ( 0 , 4 * duration , 0.1 ), data1 [ 'r' ][:, 0 ], label = 'Original firing rate' ) plt . plot ( np . arange ( 0 , 4 * duration , 0.1 ), data3 [ 'r' ][:, 999 ], label = 'Decoded firing rate' ) plt . legend ( frameon = False , loc = 2 ) plt . title ( 'b) Decoded firing rate' ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( 'Activity (Hz)' ) plt . subplot ( 3 , 1 , 3 ) plt . plot ( var_10 , label = '10 Hz' ) plt . plot ( var_50 , label = '50 Hz' ) plt . plot ( var_100 , label = '100 Hz' ) plt . legend ( frameon = False ) plt . title ( 'c) Precision' ) plt . xlabel ( '# neurons used for decoding' ) plt . ylabel ( 'Normalized error' ) plt . ylim (( 0 , 1 )) plt . show () ANNarchy 4.7 (4.7.2) on darwin (posix). Compiling ... OK","title":"Hybrid"},{"location":"example/Hybrid.html#hybrid-network","text":"Download the Jupyter notebook : Hybrid.ipynb Simple example showing hybrid spike/rate-coded networks. Reproduces Fig.4 of (Vitay, Dinkelbach and Hamker, 2015) from ANNarchy import * clear () setup ( dt = 0.1 ) # Rate-coded input neuron input_neuron = Neuron ( parameters = \"baseline = 0.0\" , equations = \"r = baseline\" ) # Rate-coded output neuron simple_neuron = Neuron ( equations = \"r = sum(exc)\" ) # Rate-coded population for input pop1 = Population ( geometry = 1 , neuron = input_neuron ) # Poisson Population to encode pop2 = PoissonPopulation ( geometry = 1000 , target = \"exc\" ) proj = Projection ( pop1 , pop2 , 'exc' ) . connect_all_to_all ( weights = 1. ) # Rate-coded population to decode pop3 = Population ( geometry = 1000 , neuron = simple_neuron ) proj = DecodingProjection ( pop2 , pop3 , 'exc' , window = 10.0 ) def diagonal ( pre , post , weights ): \"\"\" Simple connector pattern to progressively connect each post-synaptic neuron to a growing number of pre-synaptic neurons. \"\"\" lil = CSR () for rk_post in range ( post . size ): lil . add ( rk_post , range (( rk_post + 1 )), [ weights ], [ 0 ] ) return lil proj . connect_with_func ( method = diagonal , weights = 1. ) compile () # Monitors m1 = Monitor ( pop1 , 'r' ) m2 = Monitor ( pop2 , 'spike' ) m3 = Monitor ( pop3 , 'r' ) # Simulate duration = 250. # 0 Hz pop1 . baseline = 0.0 simulate ( duration ) # 10 Hz pop1 . baseline = 10.0 simulate ( duration ) # 50 Hz pop1 . baseline = 50.0 simulate ( duration ) # 100 Hz pop1 . baseline = 100.0 simulate ( duration ) # Get recordings data1 = m1 . get () data2 = m2 . get () data3 = m3 . get () # Raster plot of the spiking population t , n = m2 . raster_plot ( data2 [ 'spike' ]) # Variance of the the decoded firing rate data_10 = data3 [ 'r' ][ int ( 1.0 * duration / dt ()): int ( 2 * duration / dt ()), :] data_50 = data3 [ 'r' ][ int ( 2.0 * duration / dt ()): int ( 3 * duration / dt ()), :] data_100 = data3 [ 'r' ][ int ( 3.0 * duration / dt ()): int ( 4 * duration / dt ()), :] var_10 = np . mean ( np . abs (( data_10 - 10. ) / 10. ), axis = 0 ) var_50 = np . mean ( np . abs (( data_50 - 50. ) / 50. ), axis = 0 ) var_100 = np . mean ( np . abs (( data_100 - 100. ) / 100. ), axis = 0 ) ### Plot the results import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 3 , 1 , 1 ) plt . plot ( t , n , '.' , markersize = 0.5 ) plt . title ( 'a) Raster plot' ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neurons' ) plt . xlim (( 0 , 4 * duration )) plt . subplot ( 3 , 1 , 2 ) plt . plot ( np . arange ( 0 , 4 * duration , 0.1 ), data1 [ 'r' ][:, 0 ], label = 'Original firing rate' ) plt . plot ( np . arange ( 0 , 4 * duration , 0.1 ), data3 [ 'r' ][:, 999 ], label = 'Decoded firing rate' ) plt . legend ( frameon = False , loc = 2 ) plt . title ( 'b) Decoded firing rate' ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( 'Activity (Hz)' ) plt . subplot ( 3 , 1 , 3 ) plt . plot ( var_10 , label = '10 Hz' ) plt . plot ( var_50 , label = '50 Hz' ) plt . plot ( var_100 , label = '100 Hz' ) plt . legend ( frameon = False ) plt . title ( 'c) Precision' ) plt . xlabel ( '# neurons used for decoding' ) plt . ylabel ( 'Normalized error' ) plt . ylim (( 0 , 1 )) plt . show () ANNarchy 4.7 (4.7.2) on darwin (posix). Compiling ... OK","title":"Hybrid network"},{"location":"example/Image.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Convolutions amd pooling # Download the Jupyter notebook : Image.ipynb This simple example in examples/image demonstrates how to load images directly into the firing rates of a population and apply basic linear filters on it. It relies on the ANNarchy extensions image and convolution which must be explicitly imported: from ANNarchy import * from ANNarchy.extensions.image import * from ANNarchy.extensions.convolution import Convolution , Pooling clear () ANNarchy 4.7 (4.7.2) on darwin (posix). ANNarchy.extensions.image depends on the Python bindings of OpenCV, they must be installed before running the script. We first create an ImagePopulation that will load images: image = ImagePopulation ( geometry = ( 480 , 640 , 3 )) Its geometry specifies the size of the images that can be loaded, here 640x480 RGB images. Note the geometry must be of the form (height, width, channels), where channels is 1 for grayscale images and 3 for color images. The next step is to reduce the size of the image, what can be done by using the Pooling class of the convolution extension. We define a dummy artificial neuron, whose firing rate r will simply be the sum of excitatory connections /ensured to be positive, but this should always be the case). We then create a smaller population pooled with this neuron type, and connect it to the ImagePopulation using mean-pooling: # Simple ANN LinearNeuron = Neuron ( equations = \"r=sum(exc): min=0.0\" ) # Subsampling population pooled = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Mean-pooling projection pool_proj = Pooling ( pre = image , post = pooled , target = 'exc' , operation = 'mean' ) pool_proj . connect_pooling () <ANNarchy.extensions.convolution.Pooling.Pooling at 0x103bf0c10> The pooled population reduces the size of the image by a factor ten (defined by the size of the population) by averaging the pixels values over 10x10 regions ( operation is set to 'mean' , but one could use 'max' or 'min' ). The connect_pooling() connector creates the \"fake\" connection pattern (as no weights are involved). Let's apply now a 3x3 box filter on each channel of the pooled population: # Smoothing population smoothed = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Box filter projection box_filter = np . ones (( 3 , 3 , 1 )) / 9. smooth_proj = Convolution ( pre = pooled , post = smoothed , target = 'exc' ) smooth_proj . connect_filter ( weights = box_filter ) <ANNarchy.extensions.convolution.Convolve.Convolution at 0x116a6db80> To perform a convolution operation on the population (or more precisely a cross-correlation), we call the connect_filter() connector method of the Convolution projection. It requires to define a kernel ( weights ) that will be convolved over the input population. Here we use a simple box filter, but any filter can be used. As the pooled population has three dimensions and we want to smooth the activities per color channel, we need to define a (3, 3, 1) kernel. If we wanted to smooth also over the color channels, we could have used a (3, 3) filter: the resulting population would have the shape (48, 64). We now apply a bank of three filters, each selective to a particular color (red/green/blue). This filters do not have a spatial extent (1x1 convolution), but sum over the third dimension (the color channels): # Convolution population filtered = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Red/Green/Blue filter bank filter_bank = np . array ([ [[ [ 2.0 , - 1.0 , - 1.0 ] ]] , # Red filter [[ [ - 1.0 , 2.0 , - 1.0 ] ]] , # Blue filter [[ [ - 1.0 , - 1.0 , 2.0 ] ]] # Green filter ]) filter_proj = Convolution ( pre = smoothed , post = filtered , target = 'exc' ) filter_proj . connect_filters ( weights = filter_bank ) <ANNarchy.extensions.convolution.Convolve.Convolution at 0x103bd75e0> Each of the three filter has the shape (1, 1, 3). The result of each convolution would then be (48, 64), but as there are three filters, the output population is (48, 64, 3). The last dimension does not correspond to the number of color channels, but to the number of filters in the bank: if you add a filter, the population will have to be (48, 64, 4). Banks of filters require to use connect_filters() instead of connect_filter() . compile () Compiling ... OK After compilation, we can load an image into the input population: image . set_image ( 'test.jpg' ) To see the result, we need to simulate for four time steps (4 milliseconds, as dt=1.0 ). Step 1: The image population loads the image. Step 2: The pooled population subsamples the image. Step 3: The smoothed population filters the pooled image. Step 4: The bank of filters are applied by filtered . simulate ( 4.0 ) import matplotlib.pyplot as plt fig = plt . figure ( figsize = ( 20.0 , 20.0 )) plt . subplot ( 532 ) plt . imshow ( image . r ) plt . title ( 'Original' ) plt . subplot ( 534 ) plt . imshow ( image . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'image R' ) plt . subplot ( 535 ) plt . imshow ( image . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'image G' ) plt . subplot ( 536 ) plt . imshow ( image . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'image B' ) plt . subplot ( 537 ) plt . imshow ( pooled . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'pooled R' ) plt . subplot ( 538 ) plt . imshow ( pooled . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'pooled G' ) plt . subplot ( 539 ) plt . imshow ( pooled . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'pooled B' ) plt . subplot ( 5 , 3 , 10 ) plt . imshow ( smoothed . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'smoothed R' ) plt . subplot ( 5 , 3 , 11 ) plt . imshow ( smoothed . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'smoothed G' ) plt . subplot ( 5 , 3 , 12 ) plt . imshow ( smoothed . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'smoothed B' ) plt . subplot ( 5 , 3 , 13 ) plt . imshow ( filtered . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'filtered R' ) plt . subplot ( 5 , 3 , 14 ) plt . imshow ( filtered . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'filtered G' ) plt . subplot ( 5 , 3 , 15 ) plt . imshow ( filtered . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'filtered B' ) plt . show ()","title":"Image"},{"location":"example/Image.html#convolutions-amd-pooling","text":"Download the Jupyter notebook : Image.ipynb This simple example in examples/image demonstrates how to load images directly into the firing rates of a population and apply basic linear filters on it. It relies on the ANNarchy extensions image and convolution which must be explicitly imported: from ANNarchy import * from ANNarchy.extensions.image import * from ANNarchy.extensions.convolution import Convolution , Pooling clear () ANNarchy 4.7 (4.7.2) on darwin (posix). ANNarchy.extensions.image depends on the Python bindings of OpenCV, they must be installed before running the script. We first create an ImagePopulation that will load images: image = ImagePopulation ( geometry = ( 480 , 640 , 3 )) Its geometry specifies the size of the images that can be loaded, here 640x480 RGB images. Note the geometry must be of the form (height, width, channels), where channels is 1 for grayscale images and 3 for color images. The next step is to reduce the size of the image, what can be done by using the Pooling class of the convolution extension. We define a dummy artificial neuron, whose firing rate r will simply be the sum of excitatory connections /ensured to be positive, but this should always be the case). We then create a smaller population pooled with this neuron type, and connect it to the ImagePopulation using mean-pooling: # Simple ANN LinearNeuron = Neuron ( equations = \"r=sum(exc): min=0.0\" ) # Subsampling population pooled = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Mean-pooling projection pool_proj = Pooling ( pre = image , post = pooled , target = 'exc' , operation = 'mean' ) pool_proj . connect_pooling () <ANNarchy.extensions.convolution.Pooling.Pooling at 0x103bf0c10> The pooled population reduces the size of the image by a factor ten (defined by the size of the population) by averaging the pixels values over 10x10 regions ( operation is set to 'mean' , but one could use 'max' or 'min' ). The connect_pooling() connector creates the \"fake\" connection pattern (as no weights are involved). Let's apply now a 3x3 box filter on each channel of the pooled population: # Smoothing population smoothed = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Box filter projection box_filter = np . ones (( 3 , 3 , 1 )) / 9. smooth_proj = Convolution ( pre = pooled , post = smoothed , target = 'exc' ) smooth_proj . connect_filter ( weights = box_filter ) <ANNarchy.extensions.convolution.Convolve.Convolution at 0x116a6db80> To perform a convolution operation on the population (or more precisely a cross-correlation), we call the connect_filter() connector method of the Convolution projection. It requires to define a kernel ( weights ) that will be convolved over the input population. Here we use a simple box filter, but any filter can be used. As the pooled population has three dimensions and we want to smooth the activities per color channel, we need to define a (3, 3, 1) kernel. If we wanted to smooth also over the color channels, we could have used a (3, 3) filter: the resulting population would have the shape (48, 64). We now apply a bank of three filters, each selective to a particular color (red/green/blue). This filters do not have a spatial extent (1x1 convolution), but sum over the third dimension (the color channels): # Convolution population filtered = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Red/Green/Blue filter bank filter_bank = np . array ([ [[ [ 2.0 , - 1.0 , - 1.0 ] ]] , # Red filter [[ [ - 1.0 , 2.0 , - 1.0 ] ]] , # Blue filter [[ [ - 1.0 , - 1.0 , 2.0 ] ]] # Green filter ]) filter_proj = Convolution ( pre = smoothed , post = filtered , target = 'exc' ) filter_proj . connect_filters ( weights = filter_bank ) <ANNarchy.extensions.convolution.Convolve.Convolution at 0x103bd75e0> Each of the three filter has the shape (1, 1, 3). The result of each convolution would then be (48, 64), but as there are three filters, the output population is (48, 64, 3). The last dimension does not correspond to the number of color channels, but to the number of filters in the bank: if you add a filter, the population will have to be (48, 64, 4). Banks of filters require to use connect_filters() instead of connect_filter() . compile () Compiling ... OK After compilation, we can load an image into the input population: image . set_image ( 'test.jpg' ) To see the result, we need to simulate for four time steps (4 milliseconds, as dt=1.0 ). Step 1: The image population loads the image. Step 2: The pooled population subsamples the image. Step 3: The smoothed population filters the pooled image. Step 4: The bank of filters are applied by filtered . simulate ( 4.0 ) import matplotlib.pyplot as plt fig = plt . figure ( figsize = ( 20.0 , 20.0 )) plt . subplot ( 532 ) plt . imshow ( image . r ) plt . title ( 'Original' ) plt . subplot ( 534 ) plt . imshow ( image . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'image R' ) plt . subplot ( 535 ) plt . imshow ( image . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'image G' ) plt . subplot ( 536 ) plt . imshow ( image . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'image B' ) plt . subplot ( 537 ) plt . imshow ( pooled . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'pooled R' ) plt . subplot ( 538 ) plt . imshow ( pooled . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'pooled G' ) plt . subplot ( 539 ) plt . imshow ( pooled . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'pooled B' ) plt . subplot ( 5 , 3 , 10 ) plt . imshow ( smoothed . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'smoothed R' ) plt . subplot ( 5 , 3 , 11 ) plt . imshow ( smoothed . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'smoothed G' ) plt . subplot ( 5 , 3 , 12 ) plt . imshow ( smoothed . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'smoothed B' ) plt . subplot ( 5 , 3 , 13 ) plt . imshow ( filtered . r [:,:, 0 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'filtered R' ) plt . subplot ( 5 , 3 , 14 ) plt . imshow ( filtered . r [:,:, 1 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'filtered G' ) plt . subplot ( 5 , 3 , 15 ) plt . imshow ( filtered . r [:,:, 2 ], cmap = 'gray' , interpolation = 'nearest' , vmin = 0.0 , vmax = 1.0 ) plt . title ( 'filtered B' ) plt . show ()","title":"Convolutions amd pooling"},{"location":"example/Izhikevich.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Izhikevich's pulse-coupled network # Download the Jupyter notebook : Izhikevich.ipynb This script reproduces the simple pulse-coupled network proposed by Eugene Izhikevich in the article: Izhikevich, E.M. (2003). Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks, 14:6 . The original Matlab code is provided below: % Created by Eugene M. Izhikevich, February 25, 2003 % Excitatory neurons Inhibitory neurons Ne = 800 ; Ni = 200 ; re = rand ( Ne , 1 ); ri = rand ( Ni , 1 ); a = [ 0.02 * ones ( Ne , 1 ); 0.02 + 0.08 * ri ]; b = [ 0.2 * ones ( Ne , 1 ); 0.25 - 0.05 * ri ]; c = [ - 65 + 15 * re .^ 2 ; - 65 * ones ( Ni , 1 )]; d = [ 8 - 6 * re .^ 2 ; 2 * ones ( Ni , 1 )]; S = [ 0.5 * rand ( Ne + Ni , Ne ), - rand ( Ne + Ni , Ni )]; v = - 65 * ones ( Ne + Ni , 1 ); % Initial values of v u = b .* v ; % Initial values of u firings = []; % spike timings for t = 1 : 1000 % simulation of 1000 ms I = [ 5 * randn ( Ne , 1 ); 2 * randn ( Ni , 1 )]; % thalamic input fired = find ( v >= 30 ); % indices of spikes firings = [ firings ; t + 0 * fired , fired ]; v ( fired ) = c ( fired ); u ( fired ) = u ( fired ) + d ( fired ); I = I + sum ( S (:, fired ), 2 ); v = v + 0.5 * ( 0.04 * v .^ 2 + 5 * v + 140 - u + I ); % step 0.5 ms v = v + 0.5 * ( 0.04 * v .^ 2 + 5 * v + 140 - u + I ); % for numerical u = u + a .* ( b .* v - u ); % stability end ; plot ( firings (:, 1 ), firings (:, 2 ), \u2019 . \u2019 ) Neuron type # The network is composed of parameterized quadratic integrate-and-fire neurons, known as Izhikevich neurons. They are simply defined by the following equations: \\[ \\frac{dv}{dt} = 0.04 \\, v^2 + 5 \\, v + 140 - u + I \\] \\[ \\frac{du}{dt} = a \\, (b \\, v - u) \\] The spiking mechanism is defined by: if v > 30.0: emit_spike() v = c u = u + d v is the membrane potential, u is the membrane recovery variable and a , b , c , d are parameters allowing to reproduce many types of neural firing. I is the input voltage to a neuron at each time t . For the desired network, it is the sum of a random value taken from a normal distribution with mean 0.0 and variance 1.0 (multiplied by a scaling factor) and the net effect of incoming spikes (excitatory and inhibitory). Implementing such a neuron in ANNarchy is straightforward: Izhikevich = Neuron ( parameters = \"\"\" noise = 5.0 a = 0.02 b = 0.2 c = -65.0 d = 2.0 v_thresh = 30.0 \"\"\" , equations = \"\"\" I = g_exc - g_inh + noise * Normal(0.0, 1.0) dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I du/dt = a * (b*v - u) \"\"\" , spike = \"\"\" v >= v_thresh \"\"\" , reset = \"\"\" v = c u += d \"\"\" ) The parameters a , b , c , d as well as the noise amplitude noise are declared in the parameters argument, as their value is constant during the simulation. noise is declared as the same throughout the population with the population flag. The equations for v and u are direct translations of their mathematical counterparts. Note the use of dx/dt for the time derivative and ^2 for the square function. The input voltage I is defined as the sum of: the total conductance of excitatory synapses g_exc , the total conductance of inhibitory synapses -g_inh (in this example, we consider all weights to be positive, so we need to invert g_inh in order to model inhibitory synapses), a random number taken from the normal distribution \\(N(0,1)\\) and multiplied by the noise scale noise . In the pulse-coupled network, synapses are considered as instantaneous, i.e. a pre-synaptic spikes increases immediately the post-synaptic conductance proportionally to the weight of the synapse, but does not leave further trace. As this is the default behavior in ANNarchy, nothing has to be specified in the neuron's equations. The spike argument specifies the condition for when a spike should be emitted (here the membrane potential v should be greater than v_thresh ). The reset argument specifies the changes to neural variables that should occur after a spike is emitted: here, the membrane potential is reset to the resting potential c and the membrane recovery variable u is increased from d . The Izhikevich neuron is already defined in ANNarchy, so we will use it directly. Defining the populations # We start by defining a population of 1000 Izhikevich neurons and split it into 800 excitatory neurons and 200 inhibitory ones: from ANNarchy import * clear () pop = Population ( geometry = 1000 , neuron = Izhikevich ) Exc = pop [: 800 ] Inh = pop [ 800 :] ANNarchy 4.7 (4.7.2) on darwin (posix). Exc and Inh are subsets of pop , which have the same properties as a population. We can then set parameters differently for each population: re = np . random . random ( 800 ) ; ri = np . random . random ( 200 ) Exc . noise = 5.0 ; Inh . noise = 2.0 Exc . a = 0.02 ; Inh . a = 0.02 + 0.08 * ri Exc . b = 0.2 ; Inh . b = 0.25 - 0.05 * ri Exc . c = - 65.0 + 15.0 * re ** 2 ; Inh . c = - 65.0 Exc . d = 8.0 - 6.0 * re ** 2 ; Inh . d = 2.0 Exc . v = - 65.0 ; Inh . v = - 65.0 Exc . u = Exc . v * Exc . b ; Inh . u = Inh . v * Inh . b Defining the projections # We can now define the connections within the network: The excitatory neurons are connected to all neurons with a weight randomly chosen in [0, 0.5] The inhibitory neurons are connected to all neurons with a weight randomly chosen in [0, 1] exc_proj = Projection ( pre = Exc , post = pop , target = 'exc' ) exc_proj . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) inh_proj = Projection ( pre = Inh , post = pop , target = 'inh' ) inh_proj . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) <ANNarchy.core.Projection.Projection at 0x1184bf310> The network is now ready, we can compile: compile () Compiling ... OK Running the simulation # We start by monitoring the spikes and membrane potential in the whole population: M = Monitor ( pop , [ 'spike' , 'v' ]) We run the simulation for 1000 milliseconds: simulate ( 1000.0 , measure_time = True ) Simulating 1.0 seconds of the network took 0.049089908599853516 seconds. We retrieve the recordings, generate a raster plot and the population firing rate: spikes = M . get ( 'spike' ) v = M . get ( 'v' ) t , n = M . raster_plot ( spikes ) fr = M . histogram ( spikes ) We plot: The raster plot of population The evolution of the membrane potential of a single excitatory neuron The population firing rate import matplotlib.pyplot as plt fig = plt . figure ( figsize = ( 12 , 12 )) # First plot: raster plot plt . subplot ( 311 ) plt . plot ( t , n , 'b.' ) plt . title ( 'Raster plot' ) # Second plot: membrane potential of a single excitatory cell plt . subplot ( 312 ) plt . plot ( v [:, 15 ]) # for example plt . title ( 'Membrane potential' ) # Third plot: number of spikes per step in the population. plt . subplot ( 313 ) plt . plot ( fr ) plt . title ( 'Number of spikes' ) plt . xlabel ( 'Time (ms)' ) plt . tight_layout () plt . show ()","title":"Izhikevich"},{"location":"example/Izhikevich.html#izhikevichs-pulse-coupled-network","text":"Download the Jupyter notebook : Izhikevich.ipynb This script reproduces the simple pulse-coupled network proposed by Eugene Izhikevich in the article: Izhikevich, E.M. (2003). Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks, 14:6 . The original Matlab code is provided below: % Created by Eugene M. Izhikevich, February 25, 2003 % Excitatory neurons Inhibitory neurons Ne = 800 ; Ni = 200 ; re = rand ( Ne , 1 ); ri = rand ( Ni , 1 ); a = [ 0.02 * ones ( Ne , 1 ); 0.02 + 0.08 * ri ]; b = [ 0.2 * ones ( Ne , 1 ); 0.25 - 0.05 * ri ]; c = [ - 65 + 15 * re .^ 2 ; - 65 * ones ( Ni , 1 )]; d = [ 8 - 6 * re .^ 2 ; 2 * ones ( Ni , 1 )]; S = [ 0.5 * rand ( Ne + Ni , Ne ), - rand ( Ne + Ni , Ni )]; v = - 65 * ones ( Ne + Ni , 1 ); % Initial values of v u = b .* v ; % Initial values of u firings = []; % spike timings for t = 1 : 1000 % simulation of 1000 ms I = [ 5 * randn ( Ne , 1 ); 2 * randn ( Ni , 1 )]; % thalamic input fired = find ( v >= 30 ); % indices of spikes firings = [ firings ; t + 0 * fired , fired ]; v ( fired ) = c ( fired ); u ( fired ) = u ( fired ) + d ( fired ); I = I + sum ( S (:, fired ), 2 ); v = v + 0.5 * ( 0.04 * v .^ 2 + 5 * v + 140 - u + I ); % step 0.5 ms v = v + 0.5 * ( 0.04 * v .^ 2 + 5 * v + 140 - u + I ); % for numerical u = u + a .* ( b .* v - u ); % stability end ; plot ( firings (:, 1 ), firings (:, 2 ), \u2019 . \u2019 )","title":"Izhikevich's pulse-coupled network"},{"location":"example/Izhikevich.html#neuron-type","text":"The network is composed of parameterized quadratic integrate-and-fire neurons, known as Izhikevich neurons. They are simply defined by the following equations: \\[ \\frac{dv}{dt} = 0.04 \\, v^2 + 5 \\, v + 140 - u + I \\] \\[ \\frac{du}{dt} = a \\, (b \\, v - u) \\] The spiking mechanism is defined by: if v > 30.0: emit_spike() v = c u = u + d v is the membrane potential, u is the membrane recovery variable and a , b , c , d are parameters allowing to reproduce many types of neural firing. I is the input voltage to a neuron at each time t . For the desired network, it is the sum of a random value taken from a normal distribution with mean 0.0 and variance 1.0 (multiplied by a scaling factor) and the net effect of incoming spikes (excitatory and inhibitory). Implementing such a neuron in ANNarchy is straightforward: Izhikevich = Neuron ( parameters = \"\"\" noise = 5.0 a = 0.02 b = 0.2 c = -65.0 d = 2.0 v_thresh = 30.0 \"\"\" , equations = \"\"\" I = g_exc - g_inh + noise * Normal(0.0, 1.0) dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I du/dt = a * (b*v - u) \"\"\" , spike = \"\"\" v >= v_thresh \"\"\" , reset = \"\"\" v = c u += d \"\"\" ) The parameters a , b , c , d as well as the noise amplitude noise are declared in the parameters argument, as their value is constant during the simulation. noise is declared as the same throughout the population with the population flag. The equations for v and u are direct translations of their mathematical counterparts. Note the use of dx/dt for the time derivative and ^2 for the square function. The input voltage I is defined as the sum of: the total conductance of excitatory synapses g_exc , the total conductance of inhibitory synapses -g_inh (in this example, we consider all weights to be positive, so we need to invert g_inh in order to model inhibitory synapses), a random number taken from the normal distribution \\(N(0,1)\\) and multiplied by the noise scale noise . In the pulse-coupled network, synapses are considered as instantaneous, i.e. a pre-synaptic spikes increases immediately the post-synaptic conductance proportionally to the weight of the synapse, but does not leave further trace. As this is the default behavior in ANNarchy, nothing has to be specified in the neuron's equations. The spike argument specifies the condition for when a spike should be emitted (here the membrane potential v should be greater than v_thresh ). The reset argument specifies the changes to neural variables that should occur after a spike is emitted: here, the membrane potential is reset to the resting potential c and the membrane recovery variable u is increased from d . The Izhikevich neuron is already defined in ANNarchy, so we will use it directly.","title":"Neuron type"},{"location":"example/Izhikevich.html#defining-the-populations","text":"We start by defining a population of 1000 Izhikevich neurons and split it into 800 excitatory neurons and 200 inhibitory ones: from ANNarchy import * clear () pop = Population ( geometry = 1000 , neuron = Izhikevich ) Exc = pop [: 800 ] Inh = pop [ 800 :] ANNarchy 4.7 (4.7.2) on darwin (posix). Exc and Inh are subsets of pop , which have the same properties as a population. We can then set parameters differently for each population: re = np . random . random ( 800 ) ; ri = np . random . random ( 200 ) Exc . noise = 5.0 ; Inh . noise = 2.0 Exc . a = 0.02 ; Inh . a = 0.02 + 0.08 * ri Exc . b = 0.2 ; Inh . b = 0.25 - 0.05 * ri Exc . c = - 65.0 + 15.0 * re ** 2 ; Inh . c = - 65.0 Exc . d = 8.0 - 6.0 * re ** 2 ; Inh . d = 2.0 Exc . v = - 65.0 ; Inh . v = - 65.0 Exc . u = Exc . v * Exc . b ; Inh . u = Inh . v * Inh . b","title":"Defining the populations"},{"location":"example/Izhikevich.html#defining-the-projections","text":"We can now define the connections within the network: The excitatory neurons are connected to all neurons with a weight randomly chosen in [0, 0.5] The inhibitory neurons are connected to all neurons with a weight randomly chosen in [0, 1] exc_proj = Projection ( pre = Exc , post = pop , target = 'exc' ) exc_proj . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) inh_proj = Projection ( pre = Inh , post = pop , target = 'inh' ) inh_proj . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) <ANNarchy.core.Projection.Projection at 0x1184bf310> The network is now ready, we can compile: compile () Compiling ... OK","title":"Defining the projections"},{"location":"example/Izhikevich.html#running-the-simulation","text":"We start by monitoring the spikes and membrane potential in the whole population: M = Monitor ( pop , [ 'spike' , 'v' ]) We run the simulation for 1000 milliseconds: simulate ( 1000.0 , measure_time = True ) Simulating 1.0 seconds of the network took 0.049089908599853516 seconds. We retrieve the recordings, generate a raster plot and the population firing rate: spikes = M . get ( 'spike' ) v = M . get ( 'v' ) t , n = M . raster_plot ( spikes ) fr = M . histogram ( spikes ) We plot: The raster plot of population The evolution of the membrane potential of a single excitatory neuron The population firing rate import matplotlib.pyplot as plt fig = plt . figure ( figsize = ( 12 , 12 )) # First plot: raster plot plt . subplot ( 311 ) plt . plot ( t , n , 'b.' ) plt . title ( 'Raster plot' ) # Second plot: membrane potential of a single excitatory cell plt . subplot ( 312 ) plt . plot ( v [:, 15 ]) # for example plt . title ( 'Membrane potential' ) # Third plot: number of spikes per step in the population. plt . subplot ( 313 ) plt . plot ( fr ) plt . title ( 'Number of spikes' ) plt . xlabel ( 'Time (ms)' ) plt . tight_layout () plt . show ()","title":"Running the simulation"},{"location":"example/List.html","text":"List of notebooks # This section provides a list of the sample models provided in the examples/ directory of the source code. The Jupyter notebooks can be downloaded from: https://github.com/ANNarchy/ANNarchy.github.io/tree/master/docs/example Rate-coded networks # Neural Field : a simple model using neural field recurrent networks. This is a very simple rate-coded model without learning. Bar learning : an implementation of the bar learning problem, illustrating synaptic plasticity in rate-coded networks. Structural Plasticity : a dummy example using structural plasticity. Spiking networks # Simple networks Izhikevich : an implementation of the simple pulse-coupled network described in (Izhikevich, 2003). It shows how to build a simple spiking network without synaptic plasticity. Gap Junctions : an example using gap junctions. HodgkinHuxley : a single Hodgkin-Huxley neuron. Complex networks COBA : an implementation of the balanced network described in (Vogels and Abbott, 2005). It shows how to build a simple spiking network using integrate-and-fire neurons and sparse connectivity. STP : an example of short-term plasticity based on the model of Tsodyks, Uziel and Markram (2000). With synaptic plasticity STDP : a simple example using spike-timing dependent plasticity (STDP). Ramp : an example of homeostatic STDP based on the model of Carlson, Richert, Dutt and Krichmar (2013) Hybrid networks # Hybrid networks : a simple hybrid network with both rate-coded and spiking sub-parts. Extensions # Image and Webcam : shows how to use the ImagePopulation and VideoPopulation classes of the image extension to clamp directly images and video streams into a rate-coded network. Also demonstrates the convolution extension. Parallel simulations : shows how to call parallel_run to run several networks in parallel. Bayesian optimization : a demo showing how to use hyperopt to search for hyperparameters of a model. Logging with tensorboard : a simple basal ganglia model to show how to use the tensorboard extension. BOLD monitoring : a showcase of the bold extension allowing to record BOLD signals fron a network.","title":"List of notebooks"},{"location":"example/List.html#list-of-notebooks","text":"This section provides a list of the sample models provided in the examples/ directory of the source code. The Jupyter notebooks can be downloaded from: https://github.com/ANNarchy/ANNarchy.github.io/tree/master/docs/example","title":"List of notebooks"},{"location":"example/List.html#rate-coded-networks","text":"Neural Field : a simple model using neural field recurrent networks. This is a very simple rate-coded model without learning. Bar learning : an implementation of the bar learning problem, illustrating synaptic plasticity in rate-coded networks. Structural Plasticity : a dummy example using structural plasticity.","title":"Rate-coded networks"},{"location":"example/List.html#spiking-networks","text":"Simple networks Izhikevich : an implementation of the simple pulse-coupled network described in (Izhikevich, 2003). It shows how to build a simple spiking network without synaptic plasticity. Gap Junctions : an example using gap junctions. HodgkinHuxley : a single Hodgkin-Huxley neuron. Complex networks COBA : an implementation of the balanced network described in (Vogels and Abbott, 2005). It shows how to build a simple spiking network using integrate-and-fire neurons and sparse connectivity. STP : an example of short-term plasticity based on the model of Tsodyks, Uziel and Markram (2000). With synaptic plasticity STDP : a simple example using spike-timing dependent plasticity (STDP). Ramp : an example of homeostatic STDP based on the model of Carlson, Richert, Dutt and Krichmar (2013)","title":"Spiking networks"},{"location":"example/List.html#hybrid-networks","text":"Hybrid networks : a simple hybrid network with both rate-coded and spiking sub-parts.","title":"Hybrid networks"},{"location":"example/List.html#extensions","text":"Image and Webcam : shows how to use the ImagePopulation and VideoPopulation classes of the image extension to clamp directly images and video streams into a rate-coded network. Also demonstrates the convolution extension. Parallel simulations : shows how to call parallel_run to run several networks in parallel. Bayesian optimization : a demo showing how to use hyperopt to search for hyperparameters of a model. Logging with tensorboard : a simple basal ganglia model to show how to use the tensorboard extension. BOLD monitoring : a showcase of the bold extension allowing to record BOLD signals fron a network.","title":"Extensions"},{"location":"example/MultipleNetworks.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Parallel simulations # Download the Jupyter notebook : MultipleNetworks.ipynb This example demonstrates the use of parallel_run() to simulate the same network multiple times in parallel. We start by creating the Izhikevich pulse-coupled network defined in Izhikevich.ipynb . from ANNarchy import * clear () # Create the whole population P = Population ( geometry = 1000 , neuron = Izhikevich ) # Create the excitatory population Exc = P [: 800 ] re = np . random . random ( 800 ) Exc . noise = 5.0 Exc . a = 0.02 Exc . b = 0.2 Exc . c = - 65.0 + 15.0 * re ** 2 Exc . d = 8.0 - 6.0 * re ** 2 Exc . v = - 65.0 Exc . u = Exc . v * Exc . b # Create the Inh population Inh = P [ 800 :] ri = np . random . random ( 200 ) Inh . noise = 2.0 Inh . a = 0.02 + 0.08 * ri Inh . b = 0.25 - 0.05 * ri Inh . c = - 65.0 Inh . d = 2.0 Inh . v = - 65.0 Inh . u = Inh . v * Inh . b # Create the projections proj_exc = Projection ( Exc , P , 'exc' ) proj_inh = Projection ( Inh , P , 'inh' ) proj_exc . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) proj_inh . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) # Create a spike monitor M = Monitor ( P , 'spike' ) compile () ANNarchy 4.7 (4.7.2) on darwin (posix). We define a simulation method that re-initializes the network, runs a simulation and returns a raster plot. The simulation method must take an index as first argument and a Network instance as second one. def run_network ( idx , net ): # Retrieve subpopulations P_local = net . get ( P ) Exc = P_local [: 800 ] Inh = P_local [ 800 :] # Randomize initialization re = np . random . random ( 800 ) Exc . c = - 65.0 + 15.0 * re ** 2 Exc . d = 8.0 - 6.0 * re ** 2 ri = np . random . random ( 200 ) Inh . noise = 2.0 Inh . a = 0.02 + 0.08 * ri Inh . b = 0.25 - 0.05 * ri Inh . u = Inh . v * Inh . b # Simulate net . simulate ( 1000. ) # Recordings t , n = net . get ( M ) . raster_plot () return t , n parallel_run() uses the multiprocessing module to start parallel processes. On Linux, it should work directly, but there is an issue on OSX. Since Python 3.8, the 'spawn' method is the default way to start processes, but it does not work on MacOS. The following cell should fix the issue, but it should only be ran once. import platform if platform . system () == \"Darwin\" : import multiprocessing as mp mp . set_start_method ( 'fork' ) We can now call parallel_run() to simulate 8 identical but differently initialized networks. The first call runs the simulations sequentially, while the second is in parallel. We finally plot the raster plots of the two first simulations. # Run four identical simulations sequentially vals = parallel_run ( method = run_network , number = 8 , measure_time = True , sequential = True ) # Run four identical simulations in parallel vals = parallel_run ( method = run_network , number = 8 , measure_time = True ) # Data analysis t1 , n1 = vals [ 0 ] t2 , n2 = vals [ 1 ] import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 8 )) plt . subplot ( 121 ) plt . plot ( t1 , n1 , '.' ) plt . subplot ( 122 ) plt . plot ( t2 , n2 , '.' ) plt . show () Running 8 networks sequentially took: 1.3284821510314941 Running 8 networks in parallel took: 0.4362208843231201","title":"Parallel simulations"},{"location":"example/MultipleNetworks.html#parallel-simulations","text":"Download the Jupyter notebook : MultipleNetworks.ipynb This example demonstrates the use of parallel_run() to simulate the same network multiple times in parallel. We start by creating the Izhikevich pulse-coupled network defined in Izhikevich.ipynb . from ANNarchy import * clear () # Create the whole population P = Population ( geometry = 1000 , neuron = Izhikevich ) # Create the excitatory population Exc = P [: 800 ] re = np . random . random ( 800 ) Exc . noise = 5.0 Exc . a = 0.02 Exc . b = 0.2 Exc . c = - 65.0 + 15.0 * re ** 2 Exc . d = 8.0 - 6.0 * re ** 2 Exc . v = - 65.0 Exc . u = Exc . v * Exc . b # Create the Inh population Inh = P [ 800 :] ri = np . random . random ( 200 ) Inh . noise = 2.0 Inh . a = 0.02 + 0.08 * ri Inh . b = 0.25 - 0.05 * ri Inh . c = - 65.0 Inh . d = 2.0 Inh . v = - 65.0 Inh . u = Inh . v * Inh . b # Create the projections proj_exc = Projection ( Exc , P , 'exc' ) proj_inh = Projection ( Inh , P , 'inh' ) proj_exc . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) proj_inh . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) # Create a spike monitor M = Monitor ( P , 'spike' ) compile () ANNarchy 4.7 (4.7.2) on darwin (posix). We define a simulation method that re-initializes the network, runs a simulation and returns a raster plot. The simulation method must take an index as first argument and a Network instance as second one. def run_network ( idx , net ): # Retrieve subpopulations P_local = net . get ( P ) Exc = P_local [: 800 ] Inh = P_local [ 800 :] # Randomize initialization re = np . random . random ( 800 ) Exc . c = - 65.0 + 15.0 * re ** 2 Exc . d = 8.0 - 6.0 * re ** 2 ri = np . random . random ( 200 ) Inh . noise = 2.0 Inh . a = 0.02 + 0.08 * ri Inh . b = 0.25 - 0.05 * ri Inh . u = Inh . v * Inh . b # Simulate net . simulate ( 1000. ) # Recordings t , n = net . get ( M ) . raster_plot () return t , n parallel_run() uses the multiprocessing module to start parallel processes. On Linux, it should work directly, but there is an issue on OSX. Since Python 3.8, the 'spawn' method is the default way to start processes, but it does not work on MacOS. The following cell should fix the issue, but it should only be ran once. import platform if platform . system () == \"Darwin\" : import multiprocessing as mp mp . set_start_method ( 'fork' ) We can now call parallel_run() to simulate 8 identical but differently initialized networks. The first call runs the simulations sequentially, while the second is in parallel. We finally plot the raster plots of the two first simulations. # Run four identical simulations sequentially vals = parallel_run ( method = run_network , number = 8 , measure_time = True , sequential = True ) # Run four identical simulations in parallel vals = parallel_run ( method = run_network , number = 8 , measure_time = True ) # Data analysis t1 , n1 = vals [ 0 ] t2 , n2 = vals [ 1 ] import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 8 )) plt . subplot ( 121 ) plt . plot ( t1 , n1 , '.' ) plt . subplot ( 122 ) plt . plot ( t2 , n2 , '.' ) plt . show () Running 8 networks sequentially took: 1.3284821510314941 Running 8 networks in parallel took: 0.4362208843231201","title":"Parallel simulations"},{"location":"example/NeuralField.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Neural Field # Download the Jupyter notebook : NeuralField.ipynb The folder examples/neural_field contains a simple rate-coded model using Neural Fields . It consists of two 2D populations inp and focus , with one-to-one connections between inp and focus , and Difference-of-Gaussians (DoG) lateral connections within focus . Model overview # Each population consists of N*N neurons, with N=20. The inp population is solely used to represent inputs for focus . The firing rate of each neuron is defined by a simple equation: \\[r_i(t) = (\\text{baseline}_i(t) + \\eta(t))^+\\] where \\(r_i(t)\\) is the instantaneous firing rate, \\(\\text{baseline}_i(t)\\) its baseline activity, \\(\\eta(t)\\) an additive noise uniformly taken in \\([-0.5, 0.5]\\) and \\(()^+\\) the positive function. The focus population implements a discretized neural field, with neurons following the ODE: \\[\\tau \\frac{d r_i(t)}{dt} + r_i(t) = r^\\text{input}_i(t) + \\sum_{j=1}^{N} w_{j, i} \\cdot r_j(t) + \\eta(t)\\] where \\(r_i(t)\\) is the neuron's firing rate, \\(\\tau\\) a time constant and \\(w_{j, i}\\) the weight value (synaptic efficiency) of the synapse between the neurons j and i. \\(f()\\) is a semi-linear function, ensuring the firing rate is bounded between 0 and 1. Each neuron in focus takes inputs from the neuron of inp which has the same position, leading to a one_to_one connection pattern. The lateral connections within focus follow a difference-of-Gaussians ( dog ) connection pattern, with the connection weights \\(w_{i,j}\\) depending on the normalized euclidian distance between the neurons in the N*N population: \\[w_{j, i} = A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_+^2}) - A^- \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_-^2})\\] If i and j have coordinates \\((x_i, y_i)\\) and \\((x_j, y_j)\\) in the N*N space, the distance between them is computed as: \\[d(i, j)^2 = (\\frac{x_i - x_j}{N})^2 + (\\frac{y_i - y_j}{N})^2\\] Inputs are given to the network by changing the baseline of inp neurons. This example clamps one or several gaussian profiles (called \"bubbles\") with an additive noise, moving along a circular path at a certain speed (launch the example to understand this sentence...). Importing ANNarchy # The beginning of the script solely consists of importing the ANNarchy library: from ANNarchy import * clear () ANNarchy 4.7 (4.7.2) on darwin (posix). If you want to run the simulation on your graphic card instead of CPU, simply uncomment the following line: #setup(paradigm=\"cuda\") The setup() method allows to configure ANNarchy to run in different modes, such as chosing the parallel framework (omp or cuda), setting the simulation step dt , the numerical method method or the seed of the random number generators. Defining the neurons # Input neuron # InputNeuron = Neuron ( parameters = \"\"\" baseline = 0.0 \"\"\" , equations = \"\"\" r = pos(baseline + Uniform(-0.5, 0.5)) \"\"\" ) Defining the input neuron is straightforward. InputNeuron is here an instance of Neuron , whose only parameter is baseline (initialized to 0.0, but it does not matter here as it will be set externally). The firing rate of each neuron, r , is updated at every time step as the positive part ( pos() ) of the sum of the baseline and a random number taken from a uniform distribution between -0.5 and 0.5. Neural Field neuron # NeuralFieldNeuron = Neuron ( parameters = \"\"\" tau = 10.0 : population \"\"\" , equations = \"\"\" tau * dr/dt + r = sum(exc) + sum(inh) + Uniform(-0.5, 0.5) : min=0.0, max=1.0 \"\"\" ) The second neuron we need is a bit more complex, as it is governed by an ODE and considers inputs from other neurons. It also has a non-linear activation function, which is linear when the firing rate is between 0.0 and 1.0, and constant otherwise. tau is a population-wise parameter, whose value will be the same for all neuron of the population. r is the firing rate of he neuron, whose dynamics are governed by a first-order linear ODE, integrating the sums of excitatory and inhibitory inputs with noise. As explained in the manual for rate-coded neurons, sum(exc) retrieves the weighted sum of pre-synaptic firing rates for the synapses having the connection type exc , here the one_to_one connections between inp and focus . sum(inh) does the same for inh type connections, here the lateral connections within focus . The firing rate is restricted to the range [0, 1] by setting the min and max accordingly in the flags section (everything after the : ). This means that after evaluating the ODE and getting a new value for r , its value will be clamped if it outside these values. One can define both min and max , only one, or none. Creating the populations # The two populations have a geometry of (20, 20), therefore 400 neurons each. They are created simply by instantiating the Population class: N = 20 inp = Population ( geometry = ( N , N ), neuron = InputNeuron , name = 'Input' ) focus = Population ( geometry = ( N , N ), neuron = NeuralFieldNeuron , name = 'Focus' ) The populations can be assigned a unique name (here 'Input' and 'Focus') in order to be be able to retrieve them if the references inp and focus are lost. They are given a 2D geometry and associated to the corresponding Neuron instance. Creating the projections # The first projection is a one-to-one projection from Input to Focus with the type 'exc'. This connection pattern pattern is possible because the two populations have the same geometry. The weights are initialized to 1.0, and this value will not change with time (no learning), so it is not necessary to define a synapse type: ff = Projection ( pre = inp , post = focus , target = 'exc' ) ff . connect_one_to_one ( weights = 1.0 , delays = 20.0 ) <ANNarchy.core.Projection.Projection at 0x124256700> The references to the pre- and post-synaptic population (or their names), as well as the target type, are passed to the constructor of Projection . The connector method connect_one_to_one() is immediately applied to the Projection, defining how many synapses will be created. The weights are initialized uniformly to 1.0. The second projection is a difference of gaussians (DoG) for the lateral connections within 'focus'. The connector method is already provided by ANNarchy, so there is nothing more to do than to call it with the right parameters: lat = Projection ( pre = focus , post = focus , target = 'inh' ) lat . connect_dog ( amp_pos = 0.2 , sigma_pos = 0.1 , amp_neg = 0.1 , sigma_neg = 0.7 ) <ANNarchy.core.Projection.Projection at 0x124256af0> Compiling the network and simulating # Once the populations and projections are created, the network is ready to be generated, compiled and simulated. Compilation is simply done by calling compile() : compile () Compiling ... OK This generates optimized C++ code from the neurons' definition and network structure, compiles it with gcc/clang and instantiates all objects, particularly the synapses. If some errors were made in the neuron definition, they will be signaled at this point. Hint: The call to compile() is mandatory in any script. After it is called, populations and projections can not be added anymore. Once the compilation is successful, the network can be simulated by calling simulate() : simulate ( 1000.0 ) # simulate for 1 second As no input has been fed into the network, calling simulate() now won't lead to anything interesting. The next step is to clamp inputs into the input population's baseline. Setting inputs # Pure Python approach # In this example, we consider as input a moving bubble of activity rotating along a circle in the input space in 5 seconds. A naive way of setting such inputs would be to access population attributes (namely inp.baseline ) in a tight loop in Python: angle = 0.0 x , y = np . meshgrid ( np . linspace ( 0 , 19 , 20 ), np . linspace ( 0 , 19 , 20 )) # Main loop while True : # Update the angle angle += 1.0 / 5000.0 # Compute the center of the bubble cx = 10.0 * ( 1.0 + 0.5 * np . cos ( 2.0 * np . pi * angle ) ) cy = 10.0 * ( 1.0 + 0.5 * np . sin ( 2.0 * np . pi * angle ) ) # Clamp the bubble into pop.baseline inp . baseline = ( np . exp ( - (( x - cx ) ** 2 + ( y - cy ) ** 2 ) / 8.0 )) # Simulate for 1 ms step () angle represents the angle made by the bubble with respect to the center of the input population. x and y are Numpy arrays representing the X- and Y- coordinates of neurons in the input population. At each iteration of the simulation (i.e. every millisecond of simulation, the bubble is slightly rotated ( angle is incremented) so as to make a complete revolution in 5 seconds (5000 steps). cx and cy represent the coordinates of the center of the bubble in neural coordinates according to the new value of the angle. A Gaussian profile (in the form of a Numpy array) is then clamped into the baseline of inp using the distance between each neuron of the population ( x and y ) and the center of the bubble. Last, a single simulation step is performed using step() , before the whole process starts again until the user quits. step() is equivalent to simulate(1) , although a little bit faster as it does not check anything. Although this approach works, you would observe that it is very slow: the computation of the bubble and its feeding into InputPop takes much more time than the call to step() . The interest of using a parallel simulator disappears. This is due to the fact that Python is knowingly bad at performing tight loops because of its interpreted nature. If the while loop were compiled from C code, the computation would be much more efficient. This is what Cython brings you. Cython approach # Generalities on Cython The Cython approach requires to write Cython-specific code in a .pyx file, generate the corresponding C code with Python access methods, compile it and later import it into your Python code. Happily, the Cython syntax is very close to Python. In the most basic approach, it is simply Python code with a couple of type declarations. Instead of: bar = 1 foo = np . ones (( 10 , 10 )) you would write in Cython: cdef int bar = 1 cdef np . ndarray foo = np . ones (( 10 , 10 )) By specifing the type of a variable (which can not be changed later contrary to Python), you help Cython generate optimized C code, what can lead in some cases to speedups up to 100x. The rest of the syntax (indentation, for loops, if...) is the same as in Python. You can als import any Python module in your Cython code. Some modules (importantly Numpy) even provide a Cython interface where the equivalent Cython code can be directly imported (so it becomes very fast to use). The whole compilation procedure is very easy. One particularly simple approach is to use the pyximport module shipped with Cython. Let us suppose you wrote a dummy() method in a Cython file named TestModule.pyx . All you need to use this method in your python code is to write: import pyximport ; pyximport . install () from TestModule import dummy dummy () pyximport takes care of the compilation process (but emits quite a lot of warnings that can be ignored), and allows to import TestModule as if it were a regular Python module. Please refer to the Cython documentation to know more. Moving bubbles in Cython The file BubbleWorld.pyx defines a World class able to rotate the bubble for a specified duration. import numpy as np cimport numpy as np At the beginning of the file, numpy is imported once as a normal Python module with import , and once as a Cython module with cimport . This allows our Cython module to access directly the internal representations of Numpy without going through the Python interpreter. We can then define a World class taking as parameters: the population which will be used as input (here Input ), several arguments such as radius , sigma and period which allow to parameterize the behavior of the rotating bubble, func which is the Python method that will be called at each time step, i.e.e the step() method of ANNarchy. cdef class World : \" Environment class allowing to clamp a rotating bubble into the baseline of a population.\" cdef pop # Input population cdef func # Function to call cdef float angle # Current angle cdef float radius # Radius of the circle cdef float sigma # Width of the bubble cdef float period # Number of steps needed to make one revolution cdef np . ndarray xx , yy # indices cdef float cx , cy , midw , midh cdef np . ndarray data def __cinit__ ( self , population , radius , sigma , period , func ): \" Constructor\" self . pop = population self . func = func self . angle = 0.0 self . radius = radius self . sigma = sigma self . period = period cdef np . ndarray x = np . linspace ( 0 , self . pop . geometry [ 0 ] - 1 , self . pop . geometry [ 0 ]) cdef np . ndarray y = np . linspace ( 0 , self . pop . geometry [ 1 ] - 1 , self . pop . geometry [ 1 ]) self . xx , self . yy = np . meshgrid ( x , y ) self . midw = self . pop . geometry [ 0 ] / 2 self . midh = self . pop . geometry [ 1 ] / 2 def rotate ( self , int duration ): \" Rotates the bubble for the given duration\" cdef int t for t in xrange ( duration ): # Update the angle self . angle += 1.0 / self . period # Compute the center of the bubble self . cx = self . midw * ( 1.0 + self . radius * np . cos ( 2.0 * np . pi * self . angle ) ) self . cy = self . midh * ( 1.0 + self . radius * np . sin ( 2.0 * np . pi * self . angle ) ) # Create the bubble self . data = ( np . exp ( - (( self . xx - self . cx ) ** 2 + ( self . yy - self . cy ) ** 2 ) / 2.0 / self . sigma ** 2 )) # Clamp the bubble into pop.baseline self . pop . baseline = self . data # Simulate for 1 step self . func () Although this tutorial won't go into much detail, you can note the following: The data given to or initialized in the constructor are previously declared (with their type) as attributes of the class. This way, Cython knows at the compilation time which operations are possible on them, which amount of memory to allocate and so on, resulting in a more efficient implementation. The input population ( self.pop ) can be accessed as a normal Python object. In particular, self.pop.geometry is used in the constructor to initialize the meshgrid. The method rotate() performs the simulation for the given duration (in steps, not milliseconds). Its content is relatively similar to the Python version. Running the simulation Once the environment has been defined, the simulation can be executed. The following code, to be placed after the network definition, performs a simulation of the network, taking inputs from BubbleWorld.pyx , during 2 seconds: # Create the environment import pyximport ; pyximport . install ( setup_args = { 'include_dirs' : np . get_include ()}) from BubbleWorld import World world = World ( population = inp , radius = 0.5 , sigma = 2.0 , period = 5000.0 , func = step ) # Simulate for 2 seconds with inputs world . rotate ( 2000 ) Visualizing the network # The preceding code performs correctly the intended simulation, but nothing is visualized. The user has all freedom to visualize his network the way he prefers (for example through animated Matplotlib figures): import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 121 ) plt . imshow ( inp . r , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . subplot ( 122 ) plt . imshow ( focus . r , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . show () However, Matplotlib animations are rather slow, and visualizing the network at each time step would take more time than running the simulation. The provided example takes advantage of the PyQtGraph library (www.pyqtgraph.org) to visualize efficiently activity in the network using OpenGL. The following class and method is defined in Viz.py : # Visualizer using PyQtGraph try : from pyqtgraph.Qt import QtGui , QtCore import pyqtgraph as pg except : print ( 'PyQtGraph is not installed on your system, can not visualize the network.' ) exit ( 0 ) try : import pyqtgraph.opengl as gl except : print ( 'OpenGL is not installed on your system, can not visualize the network.' ) exit ( 0 ) import numpy as np class GLViewer ( object ): \" Class to visualize the network activity using PyQtGraph and openGL.\" def __init__ ( self , populations , func , update_rate ): # Parameters self . populations = populations self . func = func self . update_rate = update_rate # Window self . win = gl . GLViewWidget () self . win . show () self . win . setCameraPosition ( distance = 40 ) # Prepare the plots self . plots = [] shift = 0 for pop in self . populations : p = gl . GLSurfacePlotItem ( x = np . linspace ( 0 , pop . geometry [ 0 ] - 1 , pop . geometry [ 0 ]), y = np . linspace ( 0 , pop . geometry [ 1 ] - 1 , pop . geometry [ 1 ]), shader = 'heightColor' , computeNormals = False , smooth = False ) p . translate ( shift , - 10 , - 1 ) self . win . addItem ( p ) self . plots . append ( p ) shift -= 25 def scale ( self , data ): \" Colors are shown in the range [-1, 1] per default.\" return 1.8 * data - 0.9 def update ( self ): \"Callback\" # Simulate for 200ms self . func ( self . update_rate ) # Refresh the GUI for i in range ( len ( self . populations )): self . plots [ i ] . setData ( z = self . scale ( self . populations [ i ] . r )) # Listen to mouse/keyboard events QtGui . QApplication . processEvents () def run ( self ): \"Inifinite loop\" timer = QtCore . QTimer () timer . timeout . connect ( self . update ) timer . start ( 0 ) QtGui . QApplication . instance () . exec_ () def loop_bubbles ( populations , func , update_rate ): \"Launches the GL GUI and rotates the bubble infinitely.\" # Create the GUI using PyQtGraph app = QtGui . QApplication ([]) viewer = GLViewer ( populations , func , update_rate ) # Start the simulation forever viewer . run () We leave out again the details about this class (please look at the examples and tutorials on the PyQtGraph website to understand it). It allows to open a PyQtGraph window and display the firing rate of both Input and Focus populations using OpenGL. The run() method is an endless loop calling regularly the update() method. The update() method calls first World.rotate(200) and waits for its completion before reactualizing the display. The reason is that refreshing the display can only be done sequentially with the simulation, and calling it too often would impair the simulation time. Once this class has been defined, the simulation can be run endlessly by importing the Viz module: # Launch the GUI and run the simulation from Viz import loop_bubbles loop_bubbles ( populations = [ inp , focus ], func = world . rotate , update_rate = 200 )","title":"Neural Field"},{"location":"example/NeuralField.html#neural-field","text":"Download the Jupyter notebook : NeuralField.ipynb The folder examples/neural_field contains a simple rate-coded model using Neural Fields . It consists of two 2D populations inp and focus , with one-to-one connections between inp and focus , and Difference-of-Gaussians (DoG) lateral connections within focus .","title":"Neural Field"},{"location":"example/NeuralField.html#model-overview","text":"Each population consists of N*N neurons, with N=20. The inp population is solely used to represent inputs for focus . The firing rate of each neuron is defined by a simple equation: \\[r_i(t) = (\\text{baseline}_i(t) + \\eta(t))^+\\] where \\(r_i(t)\\) is the instantaneous firing rate, \\(\\text{baseline}_i(t)\\) its baseline activity, \\(\\eta(t)\\) an additive noise uniformly taken in \\([-0.5, 0.5]\\) and \\(()^+\\) the positive function. The focus population implements a discretized neural field, with neurons following the ODE: \\[\\tau \\frac{d r_i(t)}{dt} + r_i(t) = r^\\text{input}_i(t) + \\sum_{j=1}^{N} w_{j, i} \\cdot r_j(t) + \\eta(t)\\] where \\(r_i(t)\\) is the neuron's firing rate, \\(\\tau\\) a time constant and \\(w_{j, i}\\) the weight value (synaptic efficiency) of the synapse between the neurons j and i. \\(f()\\) is a semi-linear function, ensuring the firing rate is bounded between 0 and 1. Each neuron in focus takes inputs from the neuron of inp which has the same position, leading to a one_to_one connection pattern. The lateral connections within focus follow a difference-of-Gaussians ( dog ) connection pattern, with the connection weights \\(w_{i,j}\\) depending on the normalized euclidian distance between the neurons in the N*N population: \\[w_{j, i} = A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_+^2}) - A^- \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_-^2})\\] If i and j have coordinates \\((x_i, y_i)\\) and \\((x_j, y_j)\\) in the N*N space, the distance between them is computed as: \\[d(i, j)^2 = (\\frac{x_i - x_j}{N})^2 + (\\frac{y_i - y_j}{N})^2\\] Inputs are given to the network by changing the baseline of inp neurons. This example clamps one or several gaussian profiles (called \"bubbles\") with an additive noise, moving along a circular path at a certain speed (launch the example to understand this sentence...).","title":"Model overview"},{"location":"example/NeuralField.html#importing-annarchy","text":"The beginning of the script solely consists of importing the ANNarchy library: from ANNarchy import * clear () ANNarchy 4.7 (4.7.2) on darwin (posix). If you want to run the simulation on your graphic card instead of CPU, simply uncomment the following line: #setup(paradigm=\"cuda\") The setup() method allows to configure ANNarchy to run in different modes, such as chosing the parallel framework (omp or cuda), setting the simulation step dt , the numerical method method or the seed of the random number generators.","title":"Importing ANNarchy"},{"location":"example/NeuralField.html#defining-the-neurons","text":"","title":"Defining the neurons"},{"location":"example/NeuralField.html#input-neuron","text":"InputNeuron = Neuron ( parameters = \"\"\" baseline = 0.0 \"\"\" , equations = \"\"\" r = pos(baseline + Uniform(-0.5, 0.5)) \"\"\" ) Defining the input neuron is straightforward. InputNeuron is here an instance of Neuron , whose only parameter is baseline (initialized to 0.0, but it does not matter here as it will be set externally). The firing rate of each neuron, r , is updated at every time step as the positive part ( pos() ) of the sum of the baseline and a random number taken from a uniform distribution between -0.5 and 0.5.","title":"Input neuron"},{"location":"example/NeuralField.html#neural-field-neuron","text":"NeuralFieldNeuron = Neuron ( parameters = \"\"\" tau = 10.0 : population \"\"\" , equations = \"\"\" tau * dr/dt + r = sum(exc) + sum(inh) + Uniform(-0.5, 0.5) : min=0.0, max=1.0 \"\"\" ) The second neuron we need is a bit more complex, as it is governed by an ODE and considers inputs from other neurons. It also has a non-linear activation function, which is linear when the firing rate is between 0.0 and 1.0, and constant otherwise. tau is a population-wise parameter, whose value will be the same for all neuron of the population. r is the firing rate of he neuron, whose dynamics are governed by a first-order linear ODE, integrating the sums of excitatory and inhibitory inputs with noise. As explained in the manual for rate-coded neurons, sum(exc) retrieves the weighted sum of pre-synaptic firing rates for the synapses having the connection type exc , here the one_to_one connections between inp and focus . sum(inh) does the same for inh type connections, here the lateral connections within focus . The firing rate is restricted to the range [0, 1] by setting the min and max accordingly in the flags section (everything after the : ). This means that after evaluating the ODE and getting a new value for r , its value will be clamped if it outside these values. One can define both min and max , only one, or none.","title":"Neural Field neuron"},{"location":"example/NeuralField.html#creating-the-populations","text":"The two populations have a geometry of (20, 20), therefore 400 neurons each. They are created simply by instantiating the Population class: N = 20 inp = Population ( geometry = ( N , N ), neuron = InputNeuron , name = 'Input' ) focus = Population ( geometry = ( N , N ), neuron = NeuralFieldNeuron , name = 'Focus' ) The populations can be assigned a unique name (here 'Input' and 'Focus') in order to be be able to retrieve them if the references inp and focus are lost. They are given a 2D geometry and associated to the corresponding Neuron instance.","title":"Creating the populations"},{"location":"example/NeuralField.html#creating-the-projections","text":"The first projection is a one-to-one projection from Input to Focus with the type 'exc'. This connection pattern pattern is possible because the two populations have the same geometry. The weights are initialized to 1.0, and this value will not change with time (no learning), so it is not necessary to define a synapse type: ff = Projection ( pre = inp , post = focus , target = 'exc' ) ff . connect_one_to_one ( weights = 1.0 , delays = 20.0 ) <ANNarchy.core.Projection.Projection at 0x124256700> The references to the pre- and post-synaptic population (or their names), as well as the target type, are passed to the constructor of Projection . The connector method connect_one_to_one() is immediately applied to the Projection, defining how many synapses will be created. The weights are initialized uniformly to 1.0. The second projection is a difference of gaussians (DoG) for the lateral connections within 'focus'. The connector method is already provided by ANNarchy, so there is nothing more to do than to call it with the right parameters: lat = Projection ( pre = focus , post = focus , target = 'inh' ) lat . connect_dog ( amp_pos = 0.2 , sigma_pos = 0.1 , amp_neg = 0.1 , sigma_neg = 0.7 ) <ANNarchy.core.Projection.Projection at 0x124256af0>","title":"Creating the projections"},{"location":"example/NeuralField.html#compiling-the-network-and-simulating","text":"Once the populations and projections are created, the network is ready to be generated, compiled and simulated. Compilation is simply done by calling compile() : compile () Compiling ... OK This generates optimized C++ code from the neurons' definition and network structure, compiles it with gcc/clang and instantiates all objects, particularly the synapses. If some errors were made in the neuron definition, they will be signaled at this point. Hint: The call to compile() is mandatory in any script. After it is called, populations and projections can not be added anymore. Once the compilation is successful, the network can be simulated by calling simulate() : simulate ( 1000.0 ) # simulate for 1 second As no input has been fed into the network, calling simulate() now won't lead to anything interesting. The next step is to clamp inputs into the input population's baseline.","title":"Compiling the network and simulating"},{"location":"example/NeuralField.html#setting-inputs","text":"","title":"Setting inputs"},{"location":"example/NeuralField.html#pure-python-approach","text":"In this example, we consider as input a moving bubble of activity rotating along a circle in the input space in 5 seconds. A naive way of setting such inputs would be to access population attributes (namely inp.baseline ) in a tight loop in Python: angle = 0.0 x , y = np . meshgrid ( np . linspace ( 0 , 19 , 20 ), np . linspace ( 0 , 19 , 20 )) # Main loop while True : # Update the angle angle += 1.0 / 5000.0 # Compute the center of the bubble cx = 10.0 * ( 1.0 + 0.5 * np . cos ( 2.0 * np . pi * angle ) ) cy = 10.0 * ( 1.0 + 0.5 * np . sin ( 2.0 * np . pi * angle ) ) # Clamp the bubble into pop.baseline inp . baseline = ( np . exp ( - (( x - cx ) ** 2 + ( y - cy ) ** 2 ) / 8.0 )) # Simulate for 1 ms step () angle represents the angle made by the bubble with respect to the center of the input population. x and y are Numpy arrays representing the X- and Y- coordinates of neurons in the input population. At each iteration of the simulation (i.e. every millisecond of simulation, the bubble is slightly rotated ( angle is incremented) so as to make a complete revolution in 5 seconds (5000 steps). cx and cy represent the coordinates of the center of the bubble in neural coordinates according to the new value of the angle. A Gaussian profile (in the form of a Numpy array) is then clamped into the baseline of inp using the distance between each neuron of the population ( x and y ) and the center of the bubble. Last, a single simulation step is performed using step() , before the whole process starts again until the user quits. step() is equivalent to simulate(1) , although a little bit faster as it does not check anything. Although this approach works, you would observe that it is very slow: the computation of the bubble and its feeding into InputPop takes much more time than the call to step() . The interest of using a parallel simulator disappears. This is due to the fact that Python is knowingly bad at performing tight loops because of its interpreted nature. If the while loop were compiled from C code, the computation would be much more efficient. This is what Cython brings you.","title":"Pure Python approach"},{"location":"example/NeuralField.html#cython-approach","text":"Generalities on Cython The Cython approach requires to write Cython-specific code in a .pyx file, generate the corresponding C code with Python access methods, compile it and later import it into your Python code. Happily, the Cython syntax is very close to Python. In the most basic approach, it is simply Python code with a couple of type declarations. Instead of: bar = 1 foo = np . ones (( 10 , 10 )) you would write in Cython: cdef int bar = 1 cdef np . ndarray foo = np . ones (( 10 , 10 )) By specifing the type of a variable (which can not be changed later contrary to Python), you help Cython generate optimized C code, what can lead in some cases to speedups up to 100x. The rest of the syntax (indentation, for loops, if...) is the same as in Python. You can als import any Python module in your Cython code. Some modules (importantly Numpy) even provide a Cython interface where the equivalent Cython code can be directly imported (so it becomes very fast to use). The whole compilation procedure is very easy. One particularly simple approach is to use the pyximport module shipped with Cython. Let us suppose you wrote a dummy() method in a Cython file named TestModule.pyx . All you need to use this method in your python code is to write: import pyximport ; pyximport . install () from TestModule import dummy dummy () pyximport takes care of the compilation process (but emits quite a lot of warnings that can be ignored), and allows to import TestModule as if it were a regular Python module. Please refer to the Cython documentation to know more. Moving bubbles in Cython The file BubbleWorld.pyx defines a World class able to rotate the bubble for a specified duration. import numpy as np cimport numpy as np At the beginning of the file, numpy is imported once as a normal Python module with import , and once as a Cython module with cimport . This allows our Cython module to access directly the internal representations of Numpy without going through the Python interpreter. We can then define a World class taking as parameters: the population which will be used as input (here Input ), several arguments such as radius , sigma and period which allow to parameterize the behavior of the rotating bubble, func which is the Python method that will be called at each time step, i.e.e the step() method of ANNarchy. cdef class World : \" Environment class allowing to clamp a rotating bubble into the baseline of a population.\" cdef pop # Input population cdef func # Function to call cdef float angle # Current angle cdef float radius # Radius of the circle cdef float sigma # Width of the bubble cdef float period # Number of steps needed to make one revolution cdef np . ndarray xx , yy # indices cdef float cx , cy , midw , midh cdef np . ndarray data def __cinit__ ( self , population , radius , sigma , period , func ): \" Constructor\" self . pop = population self . func = func self . angle = 0.0 self . radius = radius self . sigma = sigma self . period = period cdef np . ndarray x = np . linspace ( 0 , self . pop . geometry [ 0 ] - 1 , self . pop . geometry [ 0 ]) cdef np . ndarray y = np . linspace ( 0 , self . pop . geometry [ 1 ] - 1 , self . pop . geometry [ 1 ]) self . xx , self . yy = np . meshgrid ( x , y ) self . midw = self . pop . geometry [ 0 ] / 2 self . midh = self . pop . geometry [ 1 ] / 2 def rotate ( self , int duration ): \" Rotates the bubble for the given duration\" cdef int t for t in xrange ( duration ): # Update the angle self . angle += 1.0 / self . period # Compute the center of the bubble self . cx = self . midw * ( 1.0 + self . radius * np . cos ( 2.0 * np . pi * self . angle ) ) self . cy = self . midh * ( 1.0 + self . radius * np . sin ( 2.0 * np . pi * self . angle ) ) # Create the bubble self . data = ( np . exp ( - (( self . xx - self . cx ) ** 2 + ( self . yy - self . cy ) ** 2 ) / 2.0 / self . sigma ** 2 )) # Clamp the bubble into pop.baseline self . pop . baseline = self . data # Simulate for 1 step self . func () Although this tutorial won't go into much detail, you can note the following: The data given to or initialized in the constructor are previously declared (with their type) as attributes of the class. This way, Cython knows at the compilation time which operations are possible on them, which amount of memory to allocate and so on, resulting in a more efficient implementation. The input population ( self.pop ) can be accessed as a normal Python object. In particular, self.pop.geometry is used in the constructor to initialize the meshgrid. The method rotate() performs the simulation for the given duration (in steps, not milliseconds). Its content is relatively similar to the Python version. Running the simulation Once the environment has been defined, the simulation can be executed. The following code, to be placed after the network definition, performs a simulation of the network, taking inputs from BubbleWorld.pyx , during 2 seconds: # Create the environment import pyximport ; pyximport . install ( setup_args = { 'include_dirs' : np . get_include ()}) from BubbleWorld import World world = World ( population = inp , radius = 0.5 , sigma = 2.0 , period = 5000.0 , func = step ) # Simulate for 2 seconds with inputs world . rotate ( 2000 )","title":"Cython approach"},{"location":"example/NeuralField.html#visualizing-the-network","text":"The preceding code performs correctly the intended simulation, but nothing is visualized. The user has all freedom to visualize his network the way he prefers (for example through animated Matplotlib figures): import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 121 ) plt . imshow ( inp . r , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . subplot ( 122 ) plt . imshow ( focus . r , interpolation = 'nearest' , cmap = plt . cm . gray ) plt . show () However, Matplotlib animations are rather slow, and visualizing the network at each time step would take more time than running the simulation. The provided example takes advantage of the PyQtGraph library (www.pyqtgraph.org) to visualize efficiently activity in the network using OpenGL. The following class and method is defined in Viz.py : # Visualizer using PyQtGraph try : from pyqtgraph.Qt import QtGui , QtCore import pyqtgraph as pg except : print ( 'PyQtGraph is not installed on your system, can not visualize the network.' ) exit ( 0 ) try : import pyqtgraph.opengl as gl except : print ( 'OpenGL is not installed on your system, can not visualize the network.' ) exit ( 0 ) import numpy as np class GLViewer ( object ): \" Class to visualize the network activity using PyQtGraph and openGL.\" def __init__ ( self , populations , func , update_rate ): # Parameters self . populations = populations self . func = func self . update_rate = update_rate # Window self . win = gl . GLViewWidget () self . win . show () self . win . setCameraPosition ( distance = 40 ) # Prepare the plots self . plots = [] shift = 0 for pop in self . populations : p = gl . GLSurfacePlotItem ( x = np . linspace ( 0 , pop . geometry [ 0 ] - 1 , pop . geometry [ 0 ]), y = np . linspace ( 0 , pop . geometry [ 1 ] - 1 , pop . geometry [ 1 ]), shader = 'heightColor' , computeNormals = False , smooth = False ) p . translate ( shift , - 10 , - 1 ) self . win . addItem ( p ) self . plots . append ( p ) shift -= 25 def scale ( self , data ): \" Colors are shown in the range [-1, 1] per default.\" return 1.8 * data - 0.9 def update ( self ): \"Callback\" # Simulate for 200ms self . func ( self . update_rate ) # Refresh the GUI for i in range ( len ( self . populations )): self . plots [ i ] . setData ( z = self . scale ( self . populations [ i ] . r )) # Listen to mouse/keyboard events QtGui . QApplication . processEvents () def run ( self ): \"Inifinite loop\" timer = QtCore . QTimer () timer . timeout . connect ( self . update ) timer . start ( 0 ) QtGui . QApplication . instance () . exec_ () def loop_bubbles ( populations , func , update_rate ): \"Launches the GL GUI and rotates the bubble infinitely.\" # Create the GUI using PyQtGraph app = QtGui . QApplication ([]) viewer = GLViewer ( populations , func , update_rate ) # Start the simulation forever viewer . run () We leave out again the details about this class (please look at the examples and tutorials on the PyQtGraph website to understand it). It allows to open a PyQtGraph window and display the firing rate of both Input and Focus populations using OpenGL. The run() method is an endless loop calling regularly the update() method. The update() method calls first World.rotate(200) and waits for its completion before reactualizing the display. The reason is that refreshing the display can only be done sequentially with the simulation, and calling it too often would impair the simulation time. Once this class has been defined, the simulation can be run endlessly by importing the Viz module: # Launch the GUI and run the simulation from Viz import loop_bubbles loop_bubbles ( populations = [ inp , focus ], func = world . rotate , update_rate = 200 )","title":"Visualizing the network"},{"location":"example/Ramp.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Homeostatic STDP # Download the Jupyter notebook : Ramp.ipynb This example in examples/homeostatic_stdp is a reimplementation of the mechanism described in: Carlson, K.D.; Richert, M.; Dutt, N.; Krichmar, J.L., \"Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks,\" in Neural Networks (IJCNN), The 2013 International Joint Conference on , vol., no., pp.1-8, 4-9 Aug. 2013. doi: 10.1109/IJCNN.2013.6706961 It is based on the corresponding Carlsim tutorial: http://www.socsci.uci.edu/~jkrichma/CARLsim/doc/tut3_plasticity.html This noteboob focuses on the simple \"Ramp\" experiment ( Ramp.py ), but the principle is similar for the self-organizing receptive fileds (SORF) one ( SORF.py ). from ANNarchy import * clear () ANNarchy 4.7 (4.7.2) on darwin (posix). The network uses regular-spiking Izhikevich neurons (see the Izhikevich notebook), but using exponentially-decaying conductances and NMDA synapses: RSNeuron = Neuron ( parameters = \"\"\" a = 0.02 : population b = 0.2 : population c = -65. : population d = 8. : population tau_ampa = 5. : population tau_nmda = 150. : population vrev = 0.0 : population \"\"\" , equations = \"\"\" # Inputs I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v) # Membrane potential and recovery variable are solved using the midpoint method for stability dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint du/dt = a * (b*v - u) : init=-13., midpoint # AMPA and NMDA conductances tau_ampa * dg_ampa/dt = -g_ampa : exponential tau_nmda * dg_nmda/dt = -g_nmda : exponential \"\"\" , spike = \"\"\" v >= 30. \"\"\" , reset = \"\"\" v = c u += d \"\"\" , functions = \"\"\" nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2) \"\"\" ) The main particularity about NMDA synaptic models is that a single synaptic connection influences two conductances: 1) The AMPA conductance, which primarily drives the post-synaptic neuron: \\[ I_\\text{AMPA} = g_\\text{AMPA} \\times (V_\\text{rev} - V) \\] 2) The NMDA conductance, which is non-linearly dependent on the membrane potential: \\[ I_\\text{NMDA} = g_\\text{NMDA} \\times \\frac{(\\frac{V - V_\\text{NMDA}}{\\sigma})^2}{1 + (\\frac{V - V_\\text{NMDA}}{\\sigma})^2} \\times (V_\\text{rev} - V) \\] In short, the NMDA conductance only increases if the post-synaptic neuron is already depolarized. The nmda function is defined in the functions argument for readability. The parameters \\(V_\\text{NMDA} =-80 \\text{mV}\\) and \\(\\sigma = 60 \\text{mV}\\) are here hardcoded in the equation, but they could be defined as global parameters. The AMPA and NMDA conductances are exponentially decreasing with different time constants: \\[ \\tau_\\text{AMPA} \\frac{dg_\\text{AMPA}(t)}{dt} + g_\\text{AMPA}(t) = 0 $$ $$ \\tau_\\text{NMDA} \\frac{dg_\\text{NMDA}(t)}{dt} + g_\\text{NMDA}(t) = 0 \\] Another thing to notice in this neuron model is that the differential equations for the membrane potential and recovery variable are solved concurrently using the midpoint numerical method for stability: the semi-implicit method initially proposed by Izhikevich would fail. The input of the network is a population of 100 Poisson neurons, whose firing rate vary linearly from 0.2 to 20 Hz: # Input population inp = PoissonPopulation ( 100 , rates = np . linspace ( 0.2 , 20. , 100 )) We will consider two RS neurons, one learning inputs from the Poisson population using the regular STDP, the other learning using the proposed homeostatic STDP: # RS neuron without homeostatic mechanism pop1 = Population ( 1 , RSNeuron ) # RS neuron with homeostatic mechanism pop2 = Population ( 1 , RSNeuron ) The regular STDP used in the article is a nearest-neighbour variant, which integrates LTP and LTD traces triggered after each pre- or post-synaptic spikes, respectively. Contrary to the STDP synapse provided by ANNarchy, weight changes occur at each each time step: In a post-pre interval, weight changes follow the LTP trace, In a pre-post interval, weight changes follow the LTD trace. The weights are clipped between 0 and \\(w_\\text{max}\\) . nearest_neighbour_stdp = Synapse ( parameters = \"\"\" tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_max = 0.03 : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Nearest-neighbour w += if t_post >= t_pre: ltp else: - ltd : min=0.0, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" ) The homeostatic STDP rule proposed by Carlson et al. is more complex. It has a regular STDP part (the nearest-neighbour variant above) and a homeostatic regularization part, ensuring that the post-synaptic firing rate \\(R\\) does not exceed a target firing rate \\(R_\\text{target}\\) = 35 Hz. The firing rate of a spiking neuron can be automatically computed by ANNarchy (see later). It is then accessible as the variable r of the neuron (as if it were a regular rate-coded neuron). The homeostatic STDP rule is defined by: \\[ \\Delta w = K \\, (\\alpha \\, (1 - \\frac{R}{R_\\text{target}}) \\, w + \\beta \\, \\text{stdp} ) \\] where stdp is the regular STDP weight change, and \\(K\\) is a firing rate-dependent learning rate: \\[ K = \\frac{R}{ T \\, (1 + |1 - \\gamma \\, \\frac{R}{R_\\text{target}}|}) \\] with \\(T\\) being the window over which the mean firing rate is computed (5 seconds) and \\(\\alpha\\) , \\(\\beta\\) , \\(\\gamma\\) are parameters. homeo_stdp = Synapse ( parameters = \"\"\" # STDP tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_min = 0.0 : projection w_max = 0.03 : projection # Homeostatic regulation alpha = 0.1 : projection beta = 1.0 : projection gamma = 50. : projection Rtarget = 35. : projection T = 5000. : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Homeostatic values R = post.r : postsynaptic K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic # Nearest-neighbour stdp = if t_post >= t_pre: ltp else: - ltd w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" ) This rule necessitates that the post-synaptic neurons compute their average firing rate over a 5 seconds window. This has to be explicitely enabled, as it would be computationally too expensive to allow it by default: pop1 . compute_firing_rate ( 5000. ) pop2 . compute_firing_rate ( 5000. ) We can now fully connect the input population to the two neurons with random weights: # Projection without homeostatic mechanism proj1 = Projection ( inp , pop1 , [ 'ampa' , 'nmda' ], synapse = nearest_neighbour_stdp ) proj1 . connect_all_to_all ( Uniform ( 0.01 , 0.03 )) # Projection with homeostatic mechanism proj2 = Projection ( inp , pop2 , [ 'ampa' , 'nmda' ], synapse = homeo_stdp ) proj2 . connect_all_to_all ( weights = Uniform ( 0.01 , 0.03 )) <ANNarchy.core.Projection.Projection at 0x13512dee0> Note that the same weights will target both AMPA and NMDA conductances in the post-synaptic neurons. By default, the argument target of Projection should be a string, but you can also pass a list of strings to reach several conductances with the same weights. We can now compileand simulate for 1000 seconds while recording the relevat information: compile () # Record m1 = Monitor ( pop1 , 'r' ) m2 = Monitor ( pop2 , 'r' ) m3 = Monitor ( proj1 [ 0 ], 'w' , period = 1000. ) m4 = Monitor ( proj2 [ 0 ], 'w' , period = 1000. ) # Simulate T = 1000 # 1000s simulate ( T * 1000. , True ) # Get the data data1 = m1 . get ( 'r' ) data2 = m2 . get ( 'r' ) data3 = m3 . get ( 'w' ) data4 = m4 . get ( 'w' ) print ( 'Mean Firing Rate without homeostasis:' , np . mean ( data1 [:, 0 ])) print ( 'Mean Firing Rate with homeostasis:' , np . mean ( data2 [:, 0 ])) Compiling ... OK Simulating 1000.0 seconds of the network took 1.585179090499878 seconds. Mean Firing Rate without homeostasis: 55.649554600000016 Mean Firing Rate with homeostasis: 35.2732598 import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 311 ) plt . plot ( np . linspace ( 0 , T , len ( data1 [:, 0 ])), data1 [:, 0 ], 'r-' , label = \"Without homeostasis\" ) plt . plot ( np . linspace ( 0 , T , len ( data2 [:, 0 ])), data2 [:, 0 ], 'b-' , label = \"With homeostasis\" ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'Firing rate (Hz)' ) plt . subplot ( 312 ) plt . plot ( data3 [ - 1 , :], 'r-' ) plt . plot ( data4 [ - 1 , :], 'bx' ) axes = plt . gca () axes . set_ylim ([ 0. , 0.035 ]) plt . xlabel ( '# neuron' ) plt . ylabel ( 'Weights after 1000s' ) plt . subplot ( 313 ) plt . imshow ( data4 . T , aspect = 'auto' , cmap = 'hot' ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( '# neuron' ) plt . show () We see that without homeostasis, the post-synaptic neuron reaches quickly a firing of 55 Hz, with all weights saturating at their maximum value 0.03. This is true even for inputs as low as 0.2Hz. Meanwhile, with homeostasis, the post-synaptic neuron gets a firing rate of 35 Hz (its desired value), and the weights from the input population are proportional to the underlying activity.","title":"Ramp"},{"location":"example/Ramp.html#homeostatic-stdp","text":"Download the Jupyter notebook : Ramp.ipynb This example in examples/homeostatic_stdp is a reimplementation of the mechanism described in: Carlson, K.D.; Richert, M.; Dutt, N.; Krichmar, J.L., \"Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks,\" in Neural Networks (IJCNN), The 2013 International Joint Conference on , vol., no., pp.1-8, 4-9 Aug. 2013. doi: 10.1109/IJCNN.2013.6706961 It is based on the corresponding Carlsim tutorial: http://www.socsci.uci.edu/~jkrichma/CARLsim/doc/tut3_plasticity.html This noteboob focuses on the simple \"Ramp\" experiment ( Ramp.py ), but the principle is similar for the self-organizing receptive fileds (SORF) one ( SORF.py ). from ANNarchy import * clear () ANNarchy 4.7 (4.7.2) on darwin (posix). The network uses regular-spiking Izhikevich neurons (see the Izhikevich notebook), but using exponentially-decaying conductances and NMDA synapses: RSNeuron = Neuron ( parameters = \"\"\" a = 0.02 : population b = 0.2 : population c = -65. : population d = 8. : population tau_ampa = 5. : population tau_nmda = 150. : population vrev = 0.0 : population \"\"\" , equations = \"\"\" # Inputs I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v) # Membrane potential and recovery variable are solved using the midpoint method for stability dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint du/dt = a * (b*v - u) : init=-13., midpoint # AMPA and NMDA conductances tau_ampa * dg_ampa/dt = -g_ampa : exponential tau_nmda * dg_nmda/dt = -g_nmda : exponential \"\"\" , spike = \"\"\" v >= 30. \"\"\" , reset = \"\"\" v = c u += d \"\"\" , functions = \"\"\" nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2) \"\"\" ) The main particularity about NMDA synaptic models is that a single synaptic connection influences two conductances: 1) The AMPA conductance, which primarily drives the post-synaptic neuron: \\[ I_\\text{AMPA} = g_\\text{AMPA} \\times (V_\\text{rev} - V) \\] 2) The NMDA conductance, which is non-linearly dependent on the membrane potential: \\[ I_\\text{NMDA} = g_\\text{NMDA} \\times \\frac{(\\frac{V - V_\\text{NMDA}}{\\sigma})^2}{1 + (\\frac{V - V_\\text{NMDA}}{\\sigma})^2} \\times (V_\\text{rev} - V) \\] In short, the NMDA conductance only increases if the post-synaptic neuron is already depolarized. The nmda function is defined in the functions argument for readability. The parameters \\(V_\\text{NMDA} =-80 \\text{mV}\\) and \\(\\sigma = 60 \\text{mV}\\) are here hardcoded in the equation, but they could be defined as global parameters. The AMPA and NMDA conductances are exponentially decreasing with different time constants: \\[ \\tau_\\text{AMPA} \\frac{dg_\\text{AMPA}(t)}{dt} + g_\\text{AMPA}(t) = 0 $$ $$ \\tau_\\text{NMDA} \\frac{dg_\\text{NMDA}(t)}{dt} + g_\\text{NMDA}(t) = 0 \\] Another thing to notice in this neuron model is that the differential equations for the membrane potential and recovery variable are solved concurrently using the midpoint numerical method for stability: the semi-implicit method initially proposed by Izhikevich would fail. The input of the network is a population of 100 Poisson neurons, whose firing rate vary linearly from 0.2 to 20 Hz: # Input population inp = PoissonPopulation ( 100 , rates = np . linspace ( 0.2 , 20. , 100 )) We will consider two RS neurons, one learning inputs from the Poisson population using the regular STDP, the other learning using the proposed homeostatic STDP: # RS neuron without homeostatic mechanism pop1 = Population ( 1 , RSNeuron ) # RS neuron with homeostatic mechanism pop2 = Population ( 1 , RSNeuron ) The regular STDP used in the article is a nearest-neighbour variant, which integrates LTP and LTD traces triggered after each pre- or post-synaptic spikes, respectively. Contrary to the STDP synapse provided by ANNarchy, weight changes occur at each each time step: In a post-pre interval, weight changes follow the LTP trace, In a pre-post interval, weight changes follow the LTD trace. The weights are clipped between 0 and \\(w_\\text{max}\\) . nearest_neighbour_stdp = Synapse ( parameters = \"\"\" tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_max = 0.03 : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Nearest-neighbour w += if t_post >= t_pre: ltp else: - ltd : min=0.0, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" ) The homeostatic STDP rule proposed by Carlson et al. is more complex. It has a regular STDP part (the nearest-neighbour variant above) and a homeostatic regularization part, ensuring that the post-synaptic firing rate \\(R\\) does not exceed a target firing rate \\(R_\\text{target}\\) = 35 Hz. The firing rate of a spiking neuron can be automatically computed by ANNarchy (see later). It is then accessible as the variable r of the neuron (as if it were a regular rate-coded neuron). The homeostatic STDP rule is defined by: \\[ \\Delta w = K \\, (\\alpha \\, (1 - \\frac{R}{R_\\text{target}}) \\, w + \\beta \\, \\text{stdp} ) \\] where stdp is the regular STDP weight change, and \\(K\\) is a firing rate-dependent learning rate: \\[ K = \\frac{R}{ T \\, (1 + |1 - \\gamma \\, \\frac{R}{R_\\text{target}}|}) \\] with \\(T\\) being the window over which the mean firing rate is computed (5 seconds) and \\(\\alpha\\) , \\(\\beta\\) , \\(\\gamma\\) are parameters. homeo_stdp = Synapse ( parameters = \"\"\" # STDP tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_min = 0.0 : projection w_max = 0.03 : projection # Homeostatic regulation alpha = 0.1 : projection beta = 1.0 : projection gamma = 50. : projection Rtarget = 35. : projection T = 5000. : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Homeostatic values R = post.r : postsynaptic K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic # Nearest-neighbour stdp = if t_post >= t_pre: ltp else: - ltd w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" ) This rule necessitates that the post-synaptic neurons compute their average firing rate over a 5 seconds window. This has to be explicitely enabled, as it would be computationally too expensive to allow it by default: pop1 . compute_firing_rate ( 5000. ) pop2 . compute_firing_rate ( 5000. ) We can now fully connect the input population to the two neurons with random weights: # Projection without homeostatic mechanism proj1 = Projection ( inp , pop1 , [ 'ampa' , 'nmda' ], synapse = nearest_neighbour_stdp ) proj1 . connect_all_to_all ( Uniform ( 0.01 , 0.03 )) # Projection with homeostatic mechanism proj2 = Projection ( inp , pop2 , [ 'ampa' , 'nmda' ], synapse = homeo_stdp ) proj2 . connect_all_to_all ( weights = Uniform ( 0.01 , 0.03 )) <ANNarchy.core.Projection.Projection at 0x13512dee0> Note that the same weights will target both AMPA and NMDA conductances in the post-synaptic neurons. By default, the argument target of Projection should be a string, but you can also pass a list of strings to reach several conductances with the same weights. We can now compileand simulate for 1000 seconds while recording the relevat information: compile () # Record m1 = Monitor ( pop1 , 'r' ) m2 = Monitor ( pop2 , 'r' ) m3 = Monitor ( proj1 [ 0 ], 'w' , period = 1000. ) m4 = Monitor ( proj2 [ 0 ], 'w' , period = 1000. ) # Simulate T = 1000 # 1000s simulate ( T * 1000. , True ) # Get the data data1 = m1 . get ( 'r' ) data2 = m2 . get ( 'r' ) data3 = m3 . get ( 'w' ) data4 = m4 . get ( 'w' ) print ( 'Mean Firing Rate without homeostasis:' , np . mean ( data1 [:, 0 ])) print ( 'Mean Firing Rate with homeostasis:' , np . mean ( data2 [:, 0 ])) Compiling ... OK Simulating 1000.0 seconds of the network took 1.585179090499878 seconds. Mean Firing Rate without homeostasis: 55.649554600000016 Mean Firing Rate with homeostasis: 35.2732598 import matplotlib.pyplot as plt plt . figure ( figsize = ( 15 , 10 )) plt . subplot ( 311 ) plt . plot ( np . linspace ( 0 , T , len ( data1 [:, 0 ])), data1 [:, 0 ], 'r-' , label = \"Without homeostasis\" ) plt . plot ( np . linspace ( 0 , T , len ( data2 [:, 0 ])), data2 [:, 0 ], 'b-' , label = \"With homeostasis\" ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'Firing rate (Hz)' ) plt . subplot ( 312 ) plt . plot ( data3 [ - 1 , :], 'r-' ) plt . plot ( data4 [ - 1 , :], 'bx' ) axes = plt . gca () axes . set_ylim ([ 0. , 0.035 ]) plt . xlabel ( '# neuron' ) plt . ylabel ( 'Weights after 1000s' ) plt . subplot ( 313 ) plt . imshow ( data4 . T , aspect = 'auto' , cmap = 'hot' ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( '# neuron' ) plt . show () We see that without homeostasis, the post-synaptic neuron reaches quickly a firing of 55 Hz, with all weights saturating at their maximum value 0.03. This is true even for inputs as low as 0.2Hz. Meanwhile, with homeostasis, the post-synaptic neuron gets a firing rate of 35 Hz (its desired value), and the weights from the input population are proportional to the underlying activity.","title":"Homeostatic STDP"},{"location":"example/STP.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Short-term Plasticity and Synchrony # Download the Jupyter notebook : STP.ipynb Implementation of the recurrent network proposed in: Tsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses, The Journal of Neuroscience, 20(50). This example in examples/tsodyks_markram shows how to define short-term plasticity (STP). from ANNarchy import * clear () dt = 0.25 setup ( dt = dt ) ANNarchy 4.7 (4.7.2) on darwin (posix). This network uses simple leaky integrate-and-fire (LIF) neurons: LIF = Neuron ( parameters = \"\"\" tau = 30.0 : population I = 15.0 tau_I = 3.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = -v + g_exc - g_inh + I : init=13.5 tau_I * dg_exc/dt = -g_exc tau_I * dg_inh/dt = -g_inh \"\"\" , spike = \"v > 15.0\" , reset = \"v = 13.5\" , refractory = 3.0 ) P = Population ( geometry = 500 , neuron = LIF ) P . I = np . sort ( Uniform ( 14.625 , 15.375 ) . get_values ( 500 )) P . v = Uniform ( 0.0 , 15.0 ) Exc = P [: 400 ] Inh = P [ 400 :] Short-term plasticity can be defined by dynamical changes of synaptic efficiency, based on pre- or post-synaptic activity. We define a STP synapse, whose post-pynaptic potential (psp, define by g_target ) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u : STP = Synapse ( parameters = \"\"\" w=0.0 tau_rec = 1.0 tau_facil = 1.0 U = 0.1 \"\"\" , equations = \"\"\" dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven du/dt = (U - u)/tau_facil : init = 0.1, event-driven \"\"\" , pre_spike = \"\"\" g_target += w * u * x x *= (1 - u) u += U * (1 - u) \"\"\" ) Creating the projection between the excitatory and inhibitory is straightforward when the right parameters are chosen: # Parameters for the synapses Aee = 1.8 Aei = 5.4 Aie = 7.2 Aii = 7.2 Uee = 0.5 Uei = 0.5 Uie = 0.04 Uii = 0.04 tau_rec_ee = 800.0 tau_rec_ei = 800.0 tau_rec_ie = 100.0 tau_rec_ii = 100.0 tau_facil_ie = 1000.0 tau_facil_ii = 1000.0 # Create projections proj_ee = Projection ( pre = Exc , post = Exc , target = 'exc' , synapse = STP ) proj_ee . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aee , ( Aee / 2.0 ), min = 0.2 * Aee , max = 2.0 * Aee )) proj_ee . U = Normal ( Uee , ( Uee / 2.0 ), min = 0.1 , max = 0.9 ) proj_ee . tau_rec = Normal ( tau_rec_ee , ( tau_rec_ee / 2.0 ), min = 5.0 ) proj_ee . tau_facil = dt # Cannot be 0! proj_ei = Projection ( pre = Inh , post = Exc , target = 'inh' , synapse = STP ) proj_ei . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aei , ( Aei / 2.0 ), min = 0.2 * Aei , max = 2.0 * Aei )) proj_ei . U = Normal ( Uei , ( Uei / 2.0 ), min = 0.1 , max = 0.9 ) proj_ei . tau_rec = Normal ( tau_rec_ei , ( tau_rec_ei / 2.0 ), min = 5.0 ) proj_ei . tau_facil = dt # Cannot be 0! proj_ie = Projection ( pre = Exc , post = Inh , target = 'exc' , synapse = STP ) proj_ie . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aie , ( Aie / 2.0 ), min = 0.2 * Aie , max = 2.0 * Aie )) proj_ie . U = Normal ( Uie , ( Uie / 2.0 ), min = 0.001 , max = 0.07 ) proj_ie . tau_rec = Normal ( tau_rec_ie , ( tau_rec_ie / 2.0 ), min = 5.0 ) proj_ie . tau_facil = Normal ( tau_facil_ie , ( tau_facil_ie / 2.0 ), min = 5.0 ) proj_ii = Projection ( pre = Inh , post = Inh , target = 'inh' , synapse = STP ) proj_ii . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aii , ( Aii / 2.0 ), min = 0.2 * Aii , max = 2.0 * Aii )) proj_ii . U = Normal ( Uii , ( Uii / 2.0 ), min = 0.001 , max = 0.07 ) proj_ii . tau_rec = Normal ( tau_rec_ii , ( tau_rec_ii / 2.0 ), min = 5.0 ) proj_ii . tau_facil = Normal ( tau_facil_ii , ( tau_facil_ii / 2.0 ), min = 5.0 ) We compile and simulate for 10 seconds: compile () # Record Me = Monitor ( Exc , 'spike' ) Mi = Monitor ( Inh , 'spike' ) # Simulate duration = 10000.0 simulate ( duration , measure_time = True ) Compiling ... OK Simulating 10.0 seconds of the network took 0.09651398658752441 seconds. We retrieve the recordings and plot them: # Retrieve recordings data_exc = Me . get () data_inh = Mi . get () te , ne = Me . raster_plot ( data_exc [ 'spike' ]) ti , ni = Mi . raster_plot ( data_inh [ 'spike' ]) # Histogram of the exc population h = Me . histogram ( data_exc [ 'spike' ], bins = 1.0 ) # Mean firing rate of each excitatory neuron rates = [] for neur in data_exc [ 'spike' ] . keys (): rates . append ( len ( data_exc [ 'spike' ][ neur ]) / duration * 1000.0 ) # Plot import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 3 , 1 , 1 ) plt . plot ( te , ne , 'b.' , markersize = 1.0 ) plt . plot ( ti , ni , 'b.' , markersize = 1.0 ) plt . xlim (( 0 , duration )); plt . ylim (( 0 , 500 )) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . subplot ( 3 , 1 , 2 ) plt . plot ( h / 400. ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( 'Net activity' ) plt . subplot ( 3 , 1 , 3 ) plt . plot ( sorted ( rates )) plt . ylabel ( 'Spikes / sec' ) plt . xlabel ( '# neuron' ) plt . show ()","title":"STP"},{"location":"example/STP.html#short-term-plasticity-and-synchrony","text":"Download the Jupyter notebook : STP.ipynb Implementation of the recurrent network proposed in: Tsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses, The Journal of Neuroscience, 20(50). This example in examples/tsodyks_markram shows how to define short-term plasticity (STP). from ANNarchy import * clear () dt = 0.25 setup ( dt = dt ) ANNarchy 4.7 (4.7.2) on darwin (posix). This network uses simple leaky integrate-and-fire (LIF) neurons: LIF = Neuron ( parameters = \"\"\" tau = 30.0 : population I = 15.0 tau_I = 3.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = -v + g_exc - g_inh + I : init=13.5 tau_I * dg_exc/dt = -g_exc tau_I * dg_inh/dt = -g_inh \"\"\" , spike = \"v > 15.0\" , reset = \"v = 13.5\" , refractory = 3.0 ) P = Population ( geometry = 500 , neuron = LIF ) P . I = np . sort ( Uniform ( 14.625 , 15.375 ) . get_values ( 500 )) P . v = Uniform ( 0.0 , 15.0 ) Exc = P [: 400 ] Inh = P [ 400 :] Short-term plasticity can be defined by dynamical changes of synaptic efficiency, based on pre- or post-synaptic activity. We define a STP synapse, whose post-pynaptic potential (psp, define by g_target ) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u : STP = Synapse ( parameters = \"\"\" w=0.0 tau_rec = 1.0 tau_facil = 1.0 U = 0.1 \"\"\" , equations = \"\"\" dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven du/dt = (U - u)/tau_facil : init = 0.1, event-driven \"\"\" , pre_spike = \"\"\" g_target += w * u * x x *= (1 - u) u += U * (1 - u) \"\"\" ) Creating the projection between the excitatory and inhibitory is straightforward when the right parameters are chosen: # Parameters for the synapses Aee = 1.8 Aei = 5.4 Aie = 7.2 Aii = 7.2 Uee = 0.5 Uei = 0.5 Uie = 0.04 Uii = 0.04 tau_rec_ee = 800.0 tau_rec_ei = 800.0 tau_rec_ie = 100.0 tau_rec_ii = 100.0 tau_facil_ie = 1000.0 tau_facil_ii = 1000.0 # Create projections proj_ee = Projection ( pre = Exc , post = Exc , target = 'exc' , synapse = STP ) proj_ee . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aee , ( Aee / 2.0 ), min = 0.2 * Aee , max = 2.0 * Aee )) proj_ee . U = Normal ( Uee , ( Uee / 2.0 ), min = 0.1 , max = 0.9 ) proj_ee . tau_rec = Normal ( tau_rec_ee , ( tau_rec_ee / 2.0 ), min = 5.0 ) proj_ee . tau_facil = dt # Cannot be 0! proj_ei = Projection ( pre = Inh , post = Exc , target = 'inh' , synapse = STP ) proj_ei . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aei , ( Aei / 2.0 ), min = 0.2 * Aei , max = 2.0 * Aei )) proj_ei . U = Normal ( Uei , ( Uei / 2.0 ), min = 0.1 , max = 0.9 ) proj_ei . tau_rec = Normal ( tau_rec_ei , ( tau_rec_ei / 2.0 ), min = 5.0 ) proj_ei . tau_facil = dt # Cannot be 0! proj_ie = Projection ( pre = Exc , post = Inh , target = 'exc' , synapse = STP ) proj_ie . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aie , ( Aie / 2.0 ), min = 0.2 * Aie , max = 2.0 * Aie )) proj_ie . U = Normal ( Uie , ( Uie / 2.0 ), min = 0.001 , max = 0.07 ) proj_ie . tau_rec = Normal ( tau_rec_ie , ( tau_rec_ie / 2.0 ), min = 5.0 ) proj_ie . tau_facil = Normal ( tau_facil_ie , ( tau_facil_ie / 2.0 ), min = 5.0 ) proj_ii = Projection ( pre = Inh , post = Inh , target = 'inh' , synapse = STP ) proj_ii . connect_fixed_probability ( probability = 0.1 , weights = Normal ( Aii , ( Aii / 2.0 ), min = 0.2 * Aii , max = 2.0 * Aii )) proj_ii . U = Normal ( Uii , ( Uii / 2.0 ), min = 0.001 , max = 0.07 ) proj_ii . tau_rec = Normal ( tau_rec_ii , ( tau_rec_ii / 2.0 ), min = 5.0 ) proj_ii . tau_facil = Normal ( tau_facil_ii , ( tau_facil_ii / 2.0 ), min = 5.0 ) We compile and simulate for 10 seconds: compile () # Record Me = Monitor ( Exc , 'spike' ) Mi = Monitor ( Inh , 'spike' ) # Simulate duration = 10000.0 simulate ( duration , measure_time = True ) Compiling ... OK Simulating 10.0 seconds of the network took 0.09651398658752441 seconds. We retrieve the recordings and plot them: # Retrieve recordings data_exc = Me . get () data_inh = Mi . get () te , ne = Me . raster_plot ( data_exc [ 'spike' ]) ti , ni = Mi . raster_plot ( data_inh [ 'spike' ]) # Histogram of the exc population h = Me . histogram ( data_exc [ 'spike' ], bins = 1.0 ) # Mean firing rate of each excitatory neuron rates = [] for neur in data_exc [ 'spike' ] . keys (): rates . append ( len ( data_exc [ 'spike' ][ neur ]) / duration * 1000.0 ) # Plot import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 3 , 1 , 1 ) plt . plot ( te , ne , 'b.' , markersize = 1.0 ) plt . plot ( ti , ni , 'b.' , markersize = 1.0 ) plt . xlim (( 0 , duration )); plt . ylim (( 0 , 500 )) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . subplot ( 3 , 1 , 2 ) plt . plot ( h / 400. ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( 'Net activity' ) plt . subplot ( 3 , 1 , 3 ) plt . plot ( sorted ( rates )) plt . ylabel ( 'Spikes / sec' ) plt . xlabel ( '# neuron' ) plt . show ()","title":"Short-term Plasticity and Synchrony"},{"location":"example/SimpleSTDP.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Simple STDP # Download the Jupyter notebook : SimpleSTDP.ipynb A simple model showing the STDP learning on a single neuron. Model adapted from Song, Miller and Abbott (2000) and Song and Abbott (2001) Code adapted from the Brian example: http://brian.readthedocs.org/en/1.4.1/examples-plasticity_STDP1.html from ANNarchy import * # Parameters F = 15.0 # Poisson distribution at 15 Hz N = 1000 # 1000 Poisson inputs gmax = 0.01 # Maximum weight duration = 100000.0 # Simulation for 100 seconds # Definition of the neuron IF = Neuron ( parameters = \"\"\" tau_m = 10.0 tau_e = 5.0 vt = -54.0 vr = -60.0 El = -74.0 Ee = 0.0 \"\"\" , equations = \"\"\" tau_m * dv/dt = El - v + g_exc * (Ee - vr) : init = -60.0 tau_e * dg_exc/dt = - g_exc \"\"\" , spike = \"\"\" v > vt \"\"\" , reset = \"\"\" v = vr \"\"\" ) # Input population Input = PoissonPopulation ( name = 'Input' , geometry = N , rates = F ) # Output neuron Output = Population ( name = 'Output' , geometry = 1 , neuron = IF ) # Projection learned using STDP proj = Projection ( pre = Input , post = Output , target = 'exc' , synapse = STDP ( tau_plus = 20.0 , tau_minus = 20.0 , A_plus = 0.01 , A_minus = 0.0105 , w_max = 0.01 ) ) proj . connect_all_to_all ( weights = Uniform ( 0.0 , gmax )) # Compile the network compile () # Start recording Mi = Monitor ( Input , 'spike' ) Mo = Monitor ( Output , 'spike' ) # Start the simulation print ( 'Start the simulation' ) simulate ( duration , measure_time = True ) # Retrieve the recordings input_spikes = Mi . get ( 'spike' ) output_spikes = Mo . get ( 'spike' ) # Compute the mean firing rates during the simulation print ( 'Mean firing rate in the input population: ' + str ( Mi . mean_fr ( input_spikes )) ) print ( 'Mean firing rate of the output neuron: ' + str ( Mo . mean_fr ( output_spikes )) ) # Compute the instantaneous firing rate of the output neuron output_rate = Mo . smoothed_rate ( output_spikes , 100.0 ) # Receptive field after simulation weights = proj . w [ 0 ] import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 3 , 1 , 1 ) plt . plot ( output_rate [ 0 , :]) plt . subplot ( 3 , 1 , 2 ) plt . plot ( weights , '.' ) plt . subplot ( 3 , 1 , 3 ) plt . hist ( weights , bins = 20 ) plt . show () ANNarchy 4.7 (4.7.2) on darwin (posix). Compiling ... OK Start the simulation Simulating 100.0 seconds of the network took 1.0213088989257812 seconds. Mean firing rate in the input population: 15.0002 Mean firing rate of the output neuron: 28.139999999999997","title":"STDP"},{"location":"example/SimpleSTDP.html#simple-stdp","text":"Download the Jupyter notebook : SimpleSTDP.ipynb A simple model showing the STDP learning on a single neuron. Model adapted from Song, Miller and Abbott (2000) and Song and Abbott (2001) Code adapted from the Brian example: http://brian.readthedocs.org/en/1.4.1/examples-plasticity_STDP1.html from ANNarchy import * # Parameters F = 15.0 # Poisson distribution at 15 Hz N = 1000 # 1000 Poisson inputs gmax = 0.01 # Maximum weight duration = 100000.0 # Simulation for 100 seconds # Definition of the neuron IF = Neuron ( parameters = \"\"\" tau_m = 10.0 tau_e = 5.0 vt = -54.0 vr = -60.0 El = -74.0 Ee = 0.0 \"\"\" , equations = \"\"\" tau_m * dv/dt = El - v + g_exc * (Ee - vr) : init = -60.0 tau_e * dg_exc/dt = - g_exc \"\"\" , spike = \"\"\" v > vt \"\"\" , reset = \"\"\" v = vr \"\"\" ) # Input population Input = PoissonPopulation ( name = 'Input' , geometry = N , rates = F ) # Output neuron Output = Population ( name = 'Output' , geometry = 1 , neuron = IF ) # Projection learned using STDP proj = Projection ( pre = Input , post = Output , target = 'exc' , synapse = STDP ( tau_plus = 20.0 , tau_minus = 20.0 , A_plus = 0.01 , A_minus = 0.0105 , w_max = 0.01 ) ) proj . connect_all_to_all ( weights = Uniform ( 0.0 , gmax )) # Compile the network compile () # Start recording Mi = Monitor ( Input , 'spike' ) Mo = Monitor ( Output , 'spike' ) # Start the simulation print ( 'Start the simulation' ) simulate ( duration , measure_time = True ) # Retrieve the recordings input_spikes = Mi . get ( 'spike' ) output_spikes = Mo . get ( 'spike' ) # Compute the mean firing rates during the simulation print ( 'Mean firing rate in the input population: ' + str ( Mi . mean_fr ( input_spikes )) ) print ( 'Mean firing rate of the output neuron: ' + str ( Mo . mean_fr ( output_spikes )) ) # Compute the instantaneous firing rate of the output neuron output_rate = Mo . smoothed_rate ( output_spikes , 100.0 ) # Receptive field after simulation weights = proj . w [ 0 ] import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 3 , 1 , 1 ) plt . plot ( output_rate [ 0 , :]) plt . subplot ( 3 , 1 , 2 ) plt . plot ( weights , '.' ) plt . subplot ( 3 , 1 , 3 ) plt . hist ( weights , bins = 20 ) plt . show () ANNarchy 4.7 (4.7.2) on darwin (posix). Compiling ... OK Start the simulation Simulating 100.0 seconds of the network took 1.0213088989257812 seconds. Mean firing rate in the input population: 15.0002 Mean firing rate of the output neuron: 28.139999999999997","title":"Simple STDP"},{"location":"example/StructuralPlasticity.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Structural plasticity # Download the Jupyter notebook : StructuralPlasticity.ipynb As simple example showing how to use structural plasticity (creation/pruning of synapses) in a rate-coded network (spiking networks work similarly). First, the structural plasticity mechanisms must be allowed in setup() : from ANNarchy import * clear () # Compulsory to allow structural plasticity setup ( structural_plasticity = True ) ANNarchy 4.7 (4.7.2) on darwin (posix). We define a leaky integrator rate-coded neuron and a small population: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 : population baseline = 0.0 \"\"\" , equations = \"\"\" tau * dr/dt + r = baseline + sum(exc) : min=0.0 \"\"\" ) pop = Population ( 100 , LeakyIntegratorNeuron ) Structural plasticity has two components: creation of synapses and pruning (removal) under certain conditions. These conditions are defined in the synapse type itself in the pruning and creating arguments: StructuralPlasticSynapse = Synapse ( parameters = \" T = 10000 : int, projection \" , equations = \"\"\" age = if pre.r * post.r > 1.0 : 0 else : age + 1 : init = 0, int\"\"\" , pruning = \"age > T : proba = 0.2\" , creating = \"pre.r * post.r > 1.0 : proba = 0.1, w = 0.01\" , ) proj = Projection ( pop , pop , 'exc' , StructuralPlasticSynapse ) proj . connect_fixed_probability ( weights = 0.01 , probability = 0.1 ) <ANNarchy.core.Projection.Projection at 0x1069b8f70> These conditions must be boolean values, which when True may trigger the creation/pruning of a synapse. The flag proba gives the probability by which the synapse will actually be created/pruned. When creating is True , a synapse that did not exist will be created with the provided probability. Its weight will take the value provided by the flag w (0.01), the other variables take their default value. When pruning is True , a synapse that exists will be deleted with the given probability. The pruning condition can depend on any pre-synaptic, post-synaptic or synaptic variable. The creating condition can only depend on pre- or post-synaptic conditions, as the synapse does not exist yet. Apart from these two fields, the synapse is a regular synapse, one could also define synaptic plasticity mechanisms and so on. We finally create a sparse projection within the population, with 10% connectivity. compile () Compiling ... OK The creation and pruning have to be explicitly started before a simulation, as they are very expensive computationally. The period argument states how often the conditions will be checked (avoid using dt ): proj . start_creating ( period = 100.0 ) proj . start_pruning ( period = 100.0 ) To see the effect of structural plasticity, one alternatively activates one half of the population by setting a high baseline (mimicking corrrelated inputs). As neurons in one half will be activated at the same time, they will create synapses between each other. Between the two halves, the neurons are never co-activated, so the existing synapses will slowly die out. # Save the initial connectivity matrix initial_weights = proj . connectivity_matrix () # Let structural plasticity over several trials num_trials = 100 for trial in range ( num_trials ): # Activate the first subpopulation pop [: 50 ] . baseline = 1.0 # Simulate for 1s simulate ( 1000. ) # Reset the population pop . baseline = 0.0 simulate ( 100. ) # Activate the second subpopulation pop [ 50 :] . baseline = 1.0 # Simulate for 1s simulate ( 1000. ) # Reset the population pop . baseline = 0.0 simulate ( 100. ) # Inspect the final connectivity matrix final_weights = proj . connectivity_matrix () We can check the effect of structural plasticity by looking at the connectivity matrix before and after the stimulation: import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 121 ) plt . imshow ( initial_weights ) plt . title ( 'Connectivity matrix before' ) plt . subplot ( 122 ) plt . imshow ( final_weights ) plt . title ( 'Connectivity matrix after' ) plt . show ()","title":"Structural plasticity"},{"location":"example/StructuralPlasticity.html#structural-plasticity","text":"Download the Jupyter notebook : StructuralPlasticity.ipynb As simple example showing how to use structural plasticity (creation/pruning of synapses) in a rate-coded network (spiking networks work similarly). First, the structural plasticity mechanisms must be allowed in setup() : from ANNarchy import * clear () # Compulsory to allow structural plasticity setup ( structural_plasticity = True ) ANNarchy 4.7 (4.7.2) on darwin (posix). We define a leaky integrator rate-coded neuron and a small population: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 : population baseline = 0.0 \"\"\" , equations = \"\"\" tau * dr/dt + r = baseline + sum(exc) : min=0.0 \"\"\" ) pop = Population ( 100 , LeakyIntegratorNeuron ) Structural plasticity has two components: creation of synapses and pruning (removal) under certain conditions. These conditions are defined in the synapse type itself in the pruning and creating arguments: StructuralPlasticSynapse = Synapse ( parameters = \" T = 10000 : int, projection \" , equations = \"\"\" age = if pre.r * post.r > 1.0 : 0 else : age + 1 : init = 0, int\"\"\" , pruning = \"age > T : proba = 0.2\" , creating = \"pre.r * post.r > 1.0 : proba = 0.1, w = 0.01\" , ) proj = Projection ( pop , pop , 'exc' , StructuralPlasticSynapse ) proj . connect_fixed_probability ( weights = 0.01 , probability = 0.1 ) <ANNarchy.core.Projection.Projection at 0x1069b8f70> These conditions must be boolean values, which when True may trigger the creation/pruning of a synapse. The flag proba gives the probability by which the synapse will actually be created/pruned. When creating is True , a synapse that did not exist will be created with the provided probability. Its weight will take the value provided by the flag w (0.01), the other variables take their default value. When pruning is True , a synapse that exists will be deleted with the given probability. The pruning condition can depend on any pre-synaptic, post-synaptic or synaptic variable. The creating condition can only depend on pre- or post-synaptic conditions, as the synapse does not exist yet. Apart from these two fields, the synapse is a regular synapse, one could also define synaptic plasticity mechanisms and so on. We finally create a sparse projection within the population, with 10% connectivity. compile () Compiling ... OK The creation and pruning have to be explicitly started before a simulation, as they are very expensive computationally. The period argument states how often the conditions will be checked (avoid using dt ): proj . start_creating ( period = 100.0 ) proj . start_pruning ( period = 100.0 ) To see the effect of structural plasticity, one alternatively activates one half of the population by setting a high baseline (mimicking corrrelated inputs). As neurons in one half will be activated at the same time, they will create synapses between each other. Between the two halves, the neurons are never co-activated, so the existing synapses will slowly die out. # Save the initial connectivity matrix initial_weights = proj . connectivity_matrix () # Let structural plasticity over several trials num_trials = 100 for trial in range ( num_trials ): # Activate the first subpopulation pop [: 50 ] . baseline = 1.0 # Simulate for 1s simulate ( 1000. ) # Reset the population pop . baseline = 0.0 simulate ( 100. ) # Activate the second subpopulation pop [ 50 :] . baseline = 1.0 # Simulate for 1s simulate ( 1000. ) # Reset the population pop . baseline = 0.0 simulate ( 100. ) # Inspect the final connectivity matrix final_weights = proj . connectivity_matrix () We can check the effect of structural plasticity by looking at the connectivity matrix before and after the stimulation: import matplotlib.pyplot as plt plt . figure ( figsize = ( 20 , 15 )) plt . subplot ( 121 ) plt . imshow ( initial_weights ) plt . title ( 'Connectivity matrix before' ) plt . subplot ( 122 ) plt . imshow ( final_weights ) plt . title ( 'Connectivity matrix after' ) plt . show ()","title":"Structural plasticity"},{"location":"example/Webcam.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Webcam # Download the Jupyter notebook : Webcam.ipynb The script examples/image/Webcam.py applies a red filter on the input from the webcam, and isolates one mode using a dynamical neural field. Most of the concepts are similar to the Image Processing example. The VideoPopulation object also requires the Python bindings to OpenCV. from ANNarchy import * from ANNarchy.extensions.image import * from ANNarchy.extensions.convolution import Convolution , Pooling clear () ANNarchy 4.7 (4.7.2) on darwin (posix). # Definition of the neurons LinearNeuron = Neuron ( equations = \"r=sum(exc): min=0.0\" ) DNF = Neuron ( parameters = \"tau=10.0\" , equations = \"tau*dr/dt + r = sum(exc) + sum(inh): min=0.0, max=1.0\" ) # Population getting the video stream width = 640 height = 480 video = VideoPopulation ( geometry = ( height , width , 3 )) # Subsampling population pooled = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Mean-pooling projection pool_proj = Pooling ( pre = video , post = pooled , target = 'exc' , operation = 'mean' ) pool_proj . connect_pooling () # Define a red filter with no spatial extent red_filter = [[ [ 2.0 , - 1.0 , - 1.0 ] ]] # Create a population of DNF neurons downscaling the image with a factor 10 dnf = Population ( geometry = ( 48 , 64 ), neuron = DNF ) # Create the convolution using the red filter ff = Convolution ( pre = pooled , post = dnf , target = 'exc' ) ff . connect_filter ( weights = red_filter ) # Create difference of Gaussians lateral connections for denoising/competition lat = Projection ( pre = dnf , post = dnf , target = 'inh' ) lat . connect_dog ( amp_pos = 0.2 , sigma_pos = 0.1 , amp_neg = 0.1 , sigma_neg = 0.7 ) <ANNarchy.core.Projection.Projection at 0x103fc0fa0> The VideoPopulation acquires images from the webcam: here the webcam should be able to deliver 640x480 colored images. The corresponding population is then subsampled with a factor 10, and a red filter is applied on it. This feeds a DNF (see the Neural Field\" example) which selects the region with the highest density. compile () Compiling ... OK We can now start the camera 0 ( /dev/video0 , adapt it to your machine): video . start_camera ( 0 ) A simple GUI based on PyQtGraph allows to display the input and output of the network: try : from pyqtgraph.Qt import QtGui , QtCore import pyqtgraph as pg except : print ( 'PyQtGraph is not installed, can not visualize the network.' ) exit ( 0 ) # Wrapping class class Viewer ( object ): \" Class to visualize the network activity using PyQtGraph.\" def __init__ ( self , video , result ): self . video = video self . result = result app = pg . mkQApp () self . win = pg . GraphicsWindow ( title = \"Live webcam\" ) self . win . resize ( 640 , 480 ) box = self . win . addViewBox ( lockAspect = True ) box . invertY () self . vis = pg . ImageItem () box . addItem ( self . vis ) box = self . win . addViewBox ( lockAspect = True ) box . invertY () self . res = pg . ImageItem () box . addItem ( self . res ) self . win . show () self . lastUpdate = pg . ptime . time () self . avgFps = 0.0 def update ( self ): # Set the input self . video . grab_image () # Simulate for 10 ms with a new input simulate ( 5.0 ) # Refresh the GUI self . vis . setImage ( np . swapaxes ( self . video . r , 0 , 1 )) self . res . setImage ( np . swapaxes ( self . result . r , 0 , 1 )) # Listen to mouse/keyboard events QtGui . QApplication . processEvents () # FPS now = pg . ptime . time () fps = 1.0 / ( now - self . lastUpdate ) self . lastUpdate = now self . avgFps = self . avgFps * 0.8 + fps * 0.2 # print(self.avgFps) def run ( self ): timer = QtCore . QTimer () timer . timeout . connect ( self . update ) timer . start ( 0 ) QtGui . QApplication . instance () . exec_ () timer . stop () # Start the GUI view = Viewer ( video , dnf ) view . run () video . release ()","title":"Webcam"},{"location":"example/Webcam.html#webcam","text":"Download the Jupyter notebook : Webcam.ipynb The script examples/image/Webcam.py applies a red filter on the input from the webcam, and isolates one mode using a dynamical neural field. Most of the concepts are similar to the Image Processing example. The VideoPopulation object also requires the Python bindings to OpenCV. from ANNarchy import * from ANNarchy.extensions.image import * from ANNarchy.extensions.convolution import Convolution , Pooling clear () ANNarchy 4.7 (4.7.2) on darwin (posix). # Definition of the neurons LinearNeuron = Neuron ( equations = \"r=sum(exc): min=0.0\" ) DNF = Neuron ( parameters = \"tau=10.0\" , equations = \"tau*dr/dt + r = sum(exc) + sum(inh): min=0.0, max=1.0\" ) # Population getting the video stream width = 640 height = 480 video = VideoPopulation ( geometry = ( height , width , 3 )) # Subsampling population pooled = Population ( geometry = ( 48 , 64 , 3 ), neuron = LinearNeuron ) # Mean-pooling projection pool_proj = Pooling ( pre = video , post = pooled , target = 'exc' , operation = 'mean' ) pool_proj . connect_pooling () # Define a red filter with no spatial extent red_filter = [[ [ 2.0 , - 1.0 , - 1.0 ] ]] # Create a population of DNF neurons downscaling the image with a factor 10 dnf = Population ( geometry = ( 48 , 64 ), neuron = DNF ) # Create the convolution using the red filter ff = Convolution ( pre = pooled , post = dnf , target = 'exc' ) ff . connect_filter ( weights = red_filter ) # Create difference of Gaussians lateral connections for denoising/competition lat = Projection ( pre = dnf , post = dnf , target = 'inh' ) lat . connect_dog ( amp_pos = 0.2 , sigma_pos = 0.1 , amp_neg = 0.1 , sigma_neg = 0.7 ) <ANNarchy.core.Projection.Projection at 0x103fc0fa0> The VideoPopulation acquires images from the webcam: here the webcam should be able to deliver 640x480 colored images. The corresponding population is then subsampled with a factor 10, and a red filter is applied on it. This feeds a DNF (see the Neural Field\" example) which selects the region with the highest density. compile () Compiling ... OK We can now start the camera 0 ( /dev/video0 , adapt it to your machine): video . start_camera ( 0 ) A simple GUI based on PyQtGraph allows to display the input and output of the network: try : from pyqtgraph.Qt import QtGui , QtCore import pyqtgraph as pg except : print ( 'PyQtGraph is not installed, can not visualize the network.' ) exit ( 0 ) # Wrapping class class Viewer ( object ): \" Class to visualize the network activity using PyQtGraph.\" def __init__ ( self , video , result ): self . video = video self . result = result app = pg . mkQApp () self . win = pg . GraphicsWindow ( title = \"Live webcam\" ) self . win . resize ( 640 , 480 ) box = self . win . addViewBox ( lockAspect = True ) box . invertY () self . vis = pg . ImageItem () box . addItem ( self . vis ) box = self . win . addViewBox ( lockAspect = True ) box . invertY () self . res = pg . ImageItem () box . addItem ( self . res ) self . win . show () self . lastUpdate = pg . ptime . time () self . avgFps = 0.0 def update ( self ): # Set the input self . video . grab_image () # Simulate for 10 ms with a new input simulate ( 5.0 ) # Refresh the GUI self . vis . setImage ( np . swapaxes ( self . video . r , 0 , 1 )) self . res . setImage ( np . swapaxes ( self . result . r , 0 , 1 )) # Listen to mouse/keyboard events QtGui . QApplication . processEvents () # FPS now = pg . ptime . time () fps = 1.0 / ( now - self . lastUpdate ) self . lastUpdate = now self . avgFps = self . avgFps * 0.8 + fps * 0.2 # print(self.avgFps) def run ( self ): timer = QtCore . QTimer () timer . timeout . connect ( self . update ) timer . start ( 0 ) QtGui . QApplication . instance () . exec_ () timer . stop () # Start the GUI view = Viewer ( video , dnf ) view . run () video . release ()","title":"Webcam"},{"location":"manual/Configuration.html","text":"Configuration # The setup() function can be used at the beginning of a script to configure the numerical behavior of ANNarchy. Setting the discretization step # An important value for the simulation is the discretization step dt . Its default value is 1 ms, which is usually fine for rate-coded networks, but may be too high for spiking networks, as the equations are stiffer. Taken too high, it can lead to high numerical errors. Too low, and the simulation will take an unnecessary amount of time. To set the discretization step, just pass the desired value to setup() at the beginning of the script, or at any rate before the call to compile() : setup(dt=0.1) Changing its value after calling compile() will not have any effect. Setting the seed of the random number generators # By default, the random number generators are seeded with time(NULL) , so each simulation will be different. If you want to have deterministic simulations, you simply need to provide a fixed seed to setup() : setup(seed=62756) Note that this also sets the seed of Numpy, so you can also reproduce random initialization values produced by numpy.random . Note Using the same seed with the OpenMP and CUDA backends will not lead to the same sequences of numbers! Cleaning the compilation directory # When calling compile() for the first time, a subfolder annarchy/ will be created in the current directory, where the generated code will be compiled. The first compilation may last a couple of seconds, but further modifications to the script are much faster. If no modification to the network has been made except for parameter values, it will not be recompiled, sparing us this overhead. ANNarchy tracks the changes in the script and re-generates the corresponding code. In some cases (a new version of ANNarchy has been installed, bugs), it may be necessary to perform a fresh compilation of the network. You can either delete the annarchy/ subfolder and restart the script: $ rm -rf annarchy/ $ python MyNetwork.py or pass the --clean flag to Python: $ python MyNetwork.py --clean Selecting the compiler # ANNarchy requires a C++ compiler. On GNU/Linux, the default choice is g++ , while on MacOS it is clang++ . You can change the compiler (and its flags) to use either during the call to compile() in your script: compile(compiler=\"clang++\", compiler_flags=\"-march=native -O3\") or globally by modifying the configuration file located at ~/.config/ANNarchy/annarchy.json : { \"openmp\" : { \"compiler\" : \"clang++\" , \"flags\" : \"-march=native -O3\" } } Be careful with the flags: for example, the optimization level -O3 does not obligatorily produce faster code. But this is the case for most models, therefore it is the default in the ANNarchy 4.7.x releases. Even more caution is required when using the -ffast-math flag. It can increase the performance, in particular in combination with SIMD. However, the application of -ffast-math enables a set of optimizations which might violate IEEE 754 compliance (which might be okay in many cases, but it is important that the user verifies the result). For more details, see the g++ documentation: https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html Note In rare cases, it may occur that the CPU architecture is not detectable for the used g++ compiler (e.g. Intel's Tigerlake and g++ <= 9.4). This will result in a compiler error which can be fixed by removing the '-march=native' flag. To get access to AVX-512 SIMD instructions, you need to add -mavx512f instead, as well as -ftree-vectorize if -O3 is not already used. Parallel computing with OpenMP # The default paradigm for an ANNarchy simulation is through openMP, which distributes automatically the computations over the available CPU cores. By default, OpenMP would use all the available cores for your simulation, even if it is not optimal: small networks in particular tend to run faster with a small amount of cores (for the provided example with Neural Fields, it is for example 2). For this reason, the OMP_NUM_THREADS environment variable has no effect in ANNarchy. You can control the number of cores by passing the -j flag to the Python command: user@machine:~$ python NeuralField.py -j2 It is the responsability of the user to find out which number of cores is optimal for his network, by comparing simulation times. When this optimal number is found, it can be hard-coded in the script by setting the num_threads argument to ANNarchy.setup() : from ANNarchy import * setup ( num_threads = 2 ) Parallel computing with CUDA # To run your network on GPUs, you need to declare to ANNarchy that you want to use CUDA. One way to do so is to pass the --gpu flag to the command line: user@machine:~$ python NeuralField.py --gpu You can also set the paradigm argument of ANNarchy.setup() to make it permanent: from ANNarchy import * setup ( paradigm = \"cuda\" ) If there are multiple GPUs on your machine, you can select the ID of the device by specifying it to the --gpu flag on the command line: user@machine:~$ python NeuralField.py --gpu=2 You can also pass the cuda_config dictionary argument to compile() : compile ( cuda_config = { 'device' : 2 }) The default GPU is defined in the configuration file ~/.config/ANNarchy/annarchy.json (0 unless you modify it). { \"cuda\" : { \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } Hint As the current implementation is a development version, some of the features provided by ANNarchy are not supported yet with CUDA: weight sharing (convolutions), non-uniform synaptic delays, structural plasticity, spiking neurons: a) with mean firing rate and b) continous integration of inputs, SpikeSourceArray .","title":"Configuration"},{"location":"manual/Configuration.html#configuration","text":"The setup() function can be used at the beginning of a script to configure the numerical behavior of ANNarchy.","title":"Configuration"},{"location":"manual/Configuration.html#setting-the-discretization-step","text":"An important value for the simulation is the discretization step dt . Its default value is 1 ms, which is usually fine for rate-coded networks, but may be too high for spiking networks, as the equations are stiffer. Taken too high, it can lead to high numerical errors. Too low, and the simulation will take an unnecessary amount of time. To set the discretization step, just pass the desired value to setup() at the beginning of the script, or at any rate before the call to compile() : setup(dt=0.1) Changing its value after calling compile() will not have any effect.","title":"Setting the discretization step"},{"location":"manual/Configuration.html#setting-the-seed-of-the-random-number-generators","text":"By default, the random number generators are seeded with time(NULL) , so each simulation will be different. If you want to have deterministic simulations, you simply need to provide a fixed seed to setup() : setup(seed=62756) Note that this also sets the seed of Numpy, so you can also reproduce random initialization values produced by numpy.random . Note Using the same seed with the OpenMP and CUDA backends will not lead to the same sequences of numbers!","title":"Setting the seed of the random number generators"},{"location":"manual/Configuration.html#cleaning-the-compilation-directory","text":"When calling compile() for the first time, a subfolder annarchy/ will be created in the current directory, where the generated code will be compiled. The first compilation may last a couple of seconds, but further modifications to the script are much faster. If no modification to the network has been made except for parameter values, it will not be recompiled, sparing us this overhead. ANNarchy tracks the changes in the script and re-generates the corresponding code. In some cases (a new version of ANNarchy has been installed, bugs), it may be necessary to perform a fresh compilation of the network. You can either delete the annarchy/ subfolder and restart the script: $ rm -rf annarchy/ $ python MyNetwork.py or pass the --clean flag to Python: $ python MyNetwork.py --clean","title":"Cleaning the compilation directory"},{"location":"manual/Configuration.html#selecting-the-compiler","text":"ANNarchy requires a C++ compiler. On GNU/Linux, the default choice is g++ , while on MacOS it is clang++ . You can change the compiler (and its flags) to use either during the call to compile() in your script: compile(compiler=\"clang++\", compiler_flags=\"-march=native -O3\") or globally by modifying the configuration file located at ~/.config/ANNarchy/annarchy.json : { \"openmp\" : { \"compiler\" : \"clang++\" , \"flags\" : \"-march=native -O3\" } } Be careful with the flags: for example, the optimization level -O3 does not obligatorily produce faster code. But this is the case for most models, therefore it is the default in the ANNarchy 4.7.x releases. Even more caution is required when using the -ffast-math flag. It can increase the performance, in particular in combination with SIMD. However, the application of -ffast-math enables a set of optimizations which might violate IEEE 754 compliance (which might be okay in many cases, but it is important that the user verifies the result). For more details, see the g++ documentation: https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html Note In rare cases, it may occur that the CPU architecture is not detectable for the used g++ compiler (e.g. Intel's Tigerlake and g++ <= 9.4). This will result in a compiler error which can be fixed by removing the '-march=native' flag. To get access to AVX-512 SIMD instructions, you need to add -mavx512f instead, as well as -ftree-vectorize if -O3 is not already used.","title":"Selecting the compiler"},{"location":"manual/Configuration.html#parallel-computing-with-openmp","text":"The default paradigm for an ANNarchy simulation is through openMP, which distributes automatically the computations over the available CPU cores. By default, OpenMP would use all the available cores for your simulation, even if it is not optimal: small networks in particular tend to run faster with a small amount of cores (for the provided example with Neural Fields, it is for example 2). For this reason, the OMP_NUM_THREADS environment variable has no effect in ANNarchy. You can control the number of cores by passing the -j flag to the Python command: user@machine:~$ python NeuralField.py -j2 It is the responsability of the user to find out which number of cores is optimal for his network, by comparing simulation times. When this optimal number is found, it can be hard-coded in the script by setting the num_threads argument to ANNarchy.setup() : from ANNarchy import * setup ( num_threads = 2 )","title":"Parallel computing with OpenMP"},{"location":"manual/Configuration.html#parallel-computing-with-cuda","text":"To run your network on GPUs, you need to declare to ANNarchy that you want to use CUDA. One way to do so is to pass the --gpu flag to the command line: user@machine:~$ python NeuralField.py --gpu You can also set the paradigm argument of ANNarchy.setup() to make it permanent: from ANNarchy import * setup ( paradigm = \"cuda\" ) If there are multiple GPUs on your machine, you can select the ID of the device by specifying it to the --gpu flag on the command line: user@machine:~$ python NeuralField.py --gpu=2 You can also pass the cuda_config dictionary argument to compile() : compile ( cuda_config = { 'device' : 2 }) The default GPU is defined in the configuration file ~/.config/ANNarchy/annarchy.json (0 unless you modify it). { \"cuda\" : { \"device\" : 0 , \"path\" : \"/usr/local/cuda\" } } Hint As the current implementation is a development version, some of the features provided by ANNarchy are not supported yet with CUDA: weight sharing (convolutions), non-uniform synaptic delays, structural plasticity, spiking neurons: a) with mean firing rate and b) continous integration of inputs, SpikeSourceArray .","title":"Parallel computing with CUDA"},{"location":"manual/Connector.html","text":"Connectivity # There are basically four methods to instantiate projections: By using a built-in connector method. By using a saved projection. By loading dense or sparse matrices. By defining a custom connector method. Available connector methods # For further detailed information about these connectors, please refer to the library reference Projections . connect_all_to_all # All neurons of the post-synaptic population form connections with all neurons of the pre-synaptic population (dense connectivity). Self-connections are avoided by default, but the parameter allow_self_connections can be set to True : proj . connect_all_to_all ( weights = 1.0 , delays = 2.0 , allow_self_connections = False ) The weights and delays arguments accept both single float values (all synapses will take this initial value), as well as random objects allowing to randomly select the initial values for different synapses: proj . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 )) connect_one_to_one # A neuron of the post-synaptic population forms a connection with only one neuron of the pre-synaptic population. The order depends on the ranks: neuron 0 is connected with neuron 0 and so on. It is advised that the pre- and post-populations have the same size/geometry, especially when using population views. pop1 = Population (( 20 , 20 ), Neuron ( parameters = \"r=0.0\" )) pop2 = Population (( 10 , 10 ), Neuron ( equations = \"r=sum(exc)\" )) proj = Projection ( pop1 [ 5 : 15 , 5 : 15 ], pop2 , 'exc' ) proj . connect_one_to_one ( weights = 1.0 ) Weights and delays also accept random distributions. Below is a graphical representation of the difference between all_to_all and one_to_one : connect_gaussian # A neuron of the post-synaptic population forms a connection with a limited region of the pre-synaptic population, centered around the neuron with the same normalized position. Weight values are initialized using a Gaussian function, with a maximal value amp for the neuron of same position and decreasing with distance (standard deviation sigma ): \\[w(x, y) = A \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma^2})\\] where \\((x, y)\\) is the position of the pre-synaptic neuron (normalized to \\([0, 1]^d\\) ) and \\((x_c, y_c)\\) is the position of the post-synaptic neuron (normalized to \\([0, 1]^d\\) ). A = amp, sigma = \\(\\sigma\\) . In order to void creating useless synapses, the parameter limit can be set to restrict the creation of synapses to the cases where the value of the weight would be superior to limit*abs(amp) . Default is 0.01 (1%). Self-connections are avoided by default (parameter allow_self_connections ). The two populations must have the same number of dimensions, but the number of neurons can vary as the positions of each neuron are normalized in \\([0, 1]^d\\) : proj . connect_gaussian ( amp = 1.0 , sigma = 0.2 , limit = 0.001 ) connect_dog # The same as connect_gaussian , except weight values are computed using a Difference-of-Gaussians (DoG), usually positive in the center, negative a bit further away and small at long distances. \\[\\begin{aligned} w(x, y) &= A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_+^2}) \\\\ &- A^- \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_-^2}) \\\\ \\end{aligned}\\] Weights smaller than limit * abs(amp_pos - amp_neg) are not created and self-connections are avoided by default (parameter allow_self_connections ): proj . connect_dog ( amp_pos = 1.0 , sigma_pos = 0.2 , amp_neg = 0.3 , sigma_neg = 0.7 , limit = 0.001 ) The following figure shows the example of a neuron of coordinates (10, 10) in the post-synaptic population, which is connected through the gaussian (left) and dog (right) projections to a population of geometry 30*30. The X and Y axis denote the coordinates of the pre-synaptic neurons, while the Z axis is the weight value. connect_fixed_number_pre # Each neuron in the post-synaptic population receives connections from a fixed number of neurons of the pre-synaptic population chosen randomly. It may happen that two post-synaptic neurons are connected to the same pre-synaptic neuron and that some pre-synaptic neurons are connected to nothing: proj . connect_fixed_number_pre ( number = 20 , weights = 1.0 ) weights and delays can also take a random object. connect_fixed_number_post # Each neuron in the pre-synaptic population sends a connection to a fixed number of neurons of the post-synaptic population chosen randomly. It may happen that two pre-synaptic neurons are connected to the same post-synaptic neuron and that some post-synaptic neurons receive no connection at all: proj . connect_fixed_number_post ( number = 20 , weights = 1.0 ) The following figure shows the fixed_number_pre (left) and fixed_number_post projections between two populations of 4 neurons, with number=2 . In fixed_number_pre , each post-synaptic neuron receives exactly 2 connections, while in fixed_number_post , each pre-synaptic neuron send exactly two connections: connect_fixed_probability # For each post-synaptic neuron, there is a fixed probability that it forms a connection with a neuron of the pre-synaptic population. It is basically a all_to_all projection, except some synapses are not created, making the projection sparser: proj . connect_fixed_probability ( probability = 0.2 , weights = 1.0 ) Important If a single value is used for the weights argument of connect_all_to_all , connect_one_to_one , connect_fixed_probability , connect_fixed_number_pre and connect_fixed_number_post , and the default synapse is used (no synaptic plasticity), ANNarchy will generate a single weight value for all the synapses of the projection, not one per synapse. This allows to save a lot of memory and improve performance. However, if you wish to manually change the weights of some of the synapses after the creation, you need to force the creation of one value per synapse by setting force_multiple_weights=True in the call to the connector. Saved connectivity # It is also possible to build a connection pattern using data saved during a precedent simulation. This is useful when: pre-learning is done in another context; a connector method for static synapses is particularly slow (e.g. DoG), but loading the result from a file is faster. The connectivity of a projection can be saved (after compile() ) using: proj . save_connectivity ( filename = 'proj.npz' ) The filename can used relative or absolute paths. The data is saved in a binary format: Compressed Numpy format when the filename ends with .npz . Compressed binary file format when the filename ends with .gz . Binary file format otherwise. It can then be used to instantiate another projection: proj . connect_from_file ( filename = 'proj.npz' ) Only the connectivity (which neurons are connected), the weights and delays are loaded. Other synaptic variables are left untouched. The pre- and post-synaptic population must have the same size during saving and loading. From connectivity matrices # One can also create connections using Numpy dense matrices or Scipy sparse matrices. connect_from_matrix # This method accepts a Numpy array to define the weights of the projection (and optionally the delays). By default, the matrix should have the size (post.size, pre.size) , so that the first index represents a post-synaptic neuron and the second the pre-synaptic neurons. If your matrix is defined in the reversed order, you can either transpose it or set the pre_post argument to True . This method is useful for dense connectivity matrices (all-to-all). If you do not want to create some synapses, the weight value should be set to None . The following code creates a synfire chain inside a population of 100 neurons: N = 100 proj = Projection ( pop , pop , 'exc' ) # Initialize an empty connectivity matrix w = np . array ([[ None ] * N ] * N ) # Connect each post-synaptic neuron to its predecessor for i in range ( N ): w [ i , ( i - 1 ) % N ] = 1.0 # Create the connections proj . connect_from_matrix ( w ) Connectivity matrices can not work with multi-dimensional coordinates, only ranks are used. Population views can be used in the projection, but the connection matrix must have the corresponding size: proj = Projection ( pop [ 10 : 20 ], pop [ 50 : 60 ], 'exc' ) # Create the connectivity matrix w = np . ones (( 10 , 10 )) # Create the connections proj . connect_from_matrix ( w ) connect_from_sparse # For sparse connection matrices, the Numpy array format may have a huge memory overhead if most of its values are None. It is possible to use Scipy sparse matrices in that case. The previous synfire chain example becomes: from scipy.sparse import lil_matrix proj = Projection ( pop , pop , 'exc' ) w = lil_matrix (( N , N )) for i in range ( N ): w [ i , ( i + 1 ) % N ] = 1.0 proj . connect_from_sparse ( w ) Note Contrary to connect_from_matrix() , the first index of the sparse matrix represents the pre-synaptic neurons, not the post-synaptic ones. This is for compatibility with other neural simulators. connect_from_sparse() accepts lil_matrix , csr_matrix and csc_matrix objects, although lil_matrix should be preferred for its simplicity of element access. User-defined patterns # This section describes the creation of user-specific connection patterns in ANNarchy, if the available patterns are not enough. A connection pattern is simply implemented as a method returning a LILConnectivity (list-of-list) object containing all the necessary information to create the synapses. A connector method must take on the first position the pre-synaptic population (or a subset of it) and on the second one the post-synaptic population. Other arguments are free, but should be passed when creating the projection. probabilistic_pattern ( pre , post , < other arguments > ) As an example, we will recreate the fixed_probability connector method, building synapses with a given probability. For this new pattern we need a weight value (common for all synapses) and a probability value as additional arguments. We consider that no delay is introduced in the synaptic transmission.. from ANNarchy import * def probabilistic_pattern ( pre , post , weight , probability ): synapses = LILConnectivity () ... pattern code comes here ... return synapses fixed_probability in Python # The connector method needs to return a LILConnectivity object storing the connectivity. For each post-synaptic neuron receiving synapses, a list of pre-synaptic ranks, weight values and delays must be added to the structure. If you use 2D or 3D populations you need to transform the coordinates into ranks with the rank_from_coordinates function. import random from ANNarchy import * def probabilistic_pattern ( pre , post , weight , probability ): # Create a LIL structure for the connectivity matrix synapses = LILConnectivity () # For all neurons in the post-synaptic population for post_rank in xrange ( post . size ): # Decide which pre-synaptic neurons should form synapses ranks = [] for pre_rank in xrange ( pre . size ): if random . random () < probability : ranks . append ( pre_rank ) # Create weights and delays arrays of the same size values = [ weight for i in xrange ( len ( ranks )) ] delays = [ 0 for i in xrange ( len ( ranks )) ] # Add this information to the LIL matrix synapses . add ( post_rank , ranks , values , delays ) return synapses The first for - loop iterates over all post-synaptic neurons in the projection. The inner for - loop decides for each of these neurons if a synapse with a pre-synaptic neuron should be created, based on the value probability provided as argument to the function. The lists values and delays are then created with the same size as ranks (important!), and filled with the desired value. All this information is then fed into the LIL matrix using the add(post_rank, ranks, values, delays) method. Note Building such connectivity matrices in Python can be extremely slow, as Python is not made for tight nested loops. If the construction of your network lasts too long, you should definitely write this function in Cython . Warning The add() should be only called once per post-synaptic neuron! If not, ANNarchy will have to reorder its internal representations and this will be really slow. Usage of the pattern To use the pattern within a projection you provide the pattern method to the connect_with_func method of Projection proj = Projection ( pre = pop1 , post = pop2 , target = 'inh' ) proj . connect_with_func ( method = probabilistic_pattern , weight = 1.0 , probability = 0.3 ) method is the method you just wrote. Extra arguments (other than pre and post ) should be passed with the same name. fixed_probability in Cython # For this example, we will create a Cython file CustomPatterns.pyx in the same directory as the script. Its content should be relatively similar to the Python version, except some type definitions: # distutils: language = c++ import random import ANNarchy cimport ANNarchy . core . cython_ext . Connector as Connector def probabilistic_pattern ( pre , post , weight , probability ): # Typedefs cdef Connector . LILConnectivity synapses cdef int post_rank , pre_rank cdef list ranks , values , delays # Create a LILConnectivity structure for the connectivity matrix synapses = Connector . LILConnectivity () # For all neurons in the post-synaptic population for post_rank in xrange ( post . size ): # Decide which pre-synaptic neurons should form synapses ranks = [] for pre_rank in xrange ( pre . size ): if random . random () < probability : ranks . append ( pre_rank ) # Create weights and delays arrays of the same size values = [ weight for i in xrange ( len ( ranks )) ] delays = [ 0 for i in xrange ( len ( ranks )) ] # Add this information to the LILConnectivity matrix synapses . add ( post_rank , ranks , values , delays ) return synapses The only differences with the Python code are: The module Connector where the LILConnectivity connection matrix class is defined should be cimported with: cimport ANNarchy . core . cython_ext . Connector as Connector Data structures should be declared with cdef at the beginning of the method: # Typedefs cdef Connector . LILConnectivity synapses cdef int post_rank , pre_rank cdef list ranks , values , delays To allow Cython to compile this file, we also need to provide with a kind of \\\"Makefile\\\" specifying that the code should be generated in C++, not C. This file should have the same name as the Cython file but end with .pyxbld , here : CustomPatterns.pyxbld . from distutils.extension import Extension import ANNarchy def make_ext ( modname , pyxfilename ): return Extension ( name = modname , sources = [ pyxfilename ], include_dirs = ANNarchy . include_path (), extra_compile_args = [ '-std=c++11' ], language = \"c++\" ) Note This .pyxbld is generic, you don't need to modify anything, except its name. Now you can import the method probabilistic_pattern() into your Python code using the pyximport module of Cython and build the Projection normally: import pyximport ; pyximport . install () from CustomPatterns import probabilistic_pattern proj . connect_with_func ( method = probabilistic_pattern , weight = 1.0 , probability = 0.3 ) Writing the connector in Cython can bring speedups up to 100x compared to Python if the projection has a lot of synapses.","title":"Connectivity"},{"location":"manual/Connector.html#connectivity","text":"There are basically four methods to instantiate projections: By using a built-in connector method. By using a saved projection. By loading dense or sparse matrices. By defining a custom connector method.","title":"Connectivity"},{"location":"manual/Connector.html#available-connector-methods","text":"For further detailed information about these connectors, please refer to the library reference Projections .","title":"Available connector methods"},{"location":"manual/Connector.html#connect_all_to_all","text":"All neurons of the post-synaptic population form connections with all neurons of the pre-synaptic population (dense connectivity). Self-connections are avoided by default, but the parameter allow_self_connections can be set to True : proj . connect_all_to_all ( weights = 1.0 , delays = 2.0 , allow_self_connections = False ) The weights and delays arguments accept both single float values (all synapses will take this initial value), as well as random objects allowing to randomly select the initial values for different synapses: proj . connect_all_to_all ( weights = Uniform ( 0.0 , 0.5 ))","title":"connect_all_to_all"},{"location":"manual/Connector.html#connect_one_to_one","text":"A neuron of the post-synaptic population forms a connection with only one neuron of the pre-synaptic population. The order depends on the ranks: neuron 0 is connected with neuron 0 and so on. It is advised that the pre- and post-populations have the same size/geometry, especially when using population views. pop1 = Population (( 20 , 20 ), Neuron ( parameters = \"r=0.0\" )) pop2 = Population (( 10 , 10 ), Neuron ( equations = \"r=sum(exc)\" )) proj = Projection ( pop1 [ 5 : 15 , 5 : 15 ], pop2 , 'exc' ) proj . connect_one_to_one ( weights = 1.0 ) Weights and delays also accept random distributions. Below is a graphical representation of the difference between all_to_all and one_to_one :","title":"connect_one_to_one"},{"location":"manual/Connector.html#connect_gaussian","text":"A neuron of the post-synaptic population forms a connection with a limited region of the pre-synaptic population, centered around the neuron with the same normalized position. Weight values are initialized using a Gaussian function, with a maximal value amp for the neuron of same position and decreasing with distance (standard deviation sigma ): \\[w(x, y) = A \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma^2})\\] where \\((x, y)\\) is the position of the pre-synaptic neuron (normalized to \\([0, 1]^d\\) ) and \\((x_c, y_c)\\) is the position of the post-synaptic neuron (normalized to \\([0, 1]^d\\) ). A = amp, sigma = \\(\\sigma\\) . In order to void creating useless synapses, the parameter limit can be set to restrict the creation of synapses to the cases where the value of the weight would be superior to limit*abs(amp) . Default is 0.01 (1%). Self-connections are avoided by default (parameter allow_self_connections ). The two populations must have the same number of dimensions, but the number of neurons can vary as the positions of each neuron are normalized in \\([0, 1]^d\\) : proj . connect_gaussian ( amp = 1.0 , sigma = 0.2 , limit = 0.001 )","title":"connect_gaussian"},{"location":"manual/Connector.html#connect_dog","text":"The same as connect_gaussian , except weight values are computed using a Difference-of-Gaussians (DoG), usually positive in the center, negative a bit further away and small at long distances. \\[\\begin{aligned} w(x, y) &= A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_+^2}) \\\\ &- A^- \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_-^2}) \\\\ \\end{aligned}\\] Weights smaller than limit * abs(amp_pos - amp_neg) are not created and self-connections are avoided by default (parameter allow_self_connections ): proj . connect_dog ( amp_pos = 1.0 , sigma_pos = 0.2 , amp_neg = 0.3 , sigma_neg = 0.7 , limit = 0.001 ) The following figure shows the example of a neuron of coordinates (10, 10) in the post-synaptic population, which is connected through the gaussian (left) and dog (right) projections to a population of geometry 30*30. The X and Y axis denote the coordinates of the pre-synaptic neurons, while the Z axis is the weight value.","title":"connect_dog"},{"location":"manual/Connector.html#connect_fixed_number_pre","text":"Each neuron in the post-synaptic population receives connections from a fixed number of neurons of the pre-synaptic population chosen randomly. It may happen that two post-synaptic neurons are connected to the same pre-synaptic neuron and that some pre-synaptic neurons are connected to nothing: proj . connect_fixed_number_pre ( number = 20 , weights = 1.0 ) weights and delays can also take a random object.","title":"connect_fixed_number_pre"},{"location":"manual/Connector.html#connect_fixed_number_post","text":"Each neuron in the pre-synaptic population sends a connection to a fixed number of neurons of the post-synaptic population chosen randomly. It may happen that two pre-synaptic neurons are connected to the same post-synaptic neuron and that some post-synaptic neurons receive no connection at all: proj . connect_fixed_number_post ( number = 20 , weights = 1.0 ) The following figure shows the fixed_number_pre (left) and fixed_number_post projections between two populations of 4 neurons, with number=2 . In fixed_number_pre , each post-synaptic neuron receives exactly 2 connections, while in fixed_number_post , each pre-synaptic neuron send exactly two connections:","title":"connect_fixed_number_post"},{"location":"manual/Connector.html#connect_fixed_probability","text":"For each post-synaptic neuron, there is a fixed probability that it forms a connection with a neuron of the pre-synaptic population. It is basically a all_to_all projection, except some synapses are not created, making the projection sparser: proj . connect_fixed_probability ( probability = 0.2 , weights = 1.0 ) Important If a single value is used for the weights argument of connect_all_to_all , connect_one_to_one , connect_fixed_probability , connect_fixed_number_pre and connect_fixed_number_post , and the default synapse is used (no synaptic plasticity), ANNarchy will generate a single weight value for all the synapses of the projection, not one per synapse. This allows to save a lot of memory and improve performance. However, if you wish to manually change the weights of some of the synapses after the creation, you need to force the creation of one value per synapse by setting force_multiple_weights=True in the call to the connector.","title":"connect_fixed_probability"},{"location":"manual/Connector.html#saved-connectivity","text":"It is also possible to build a connection pattern using data saved during a precedent simulation. This is useful when: pre-learning is done in another context; a connector method for static synapses is particularly slow (e.g. DoG), but loading the result from a file is faster. The connectivity of a projection can be saved (after compile() ) using: proj . save_connectivity ( filename = 'proj.npz' ) The filename can used relative or absolute paths. The data is saved in a binary format: Compressed Numpy format when the filename ends with .npz . Compressed binary file format when the filename ends with .gz . Binary file format otherwise. It can then be used to instantiate another projection: proj . connect_from_file ( filename = 'proj.npz' ) Only the connectivity (which neurons are connected), the weights and delays are loaded. Other synaptic variables are left untouched. The pre- and post-synaptic population must have the same size during saving and loading.","title":"Saved connectivity"},{"location":"manual/Connector.html#from-connectivity-matrices","text":"One can also create connections using Numpy dense matrices or Scipy sparse matrices.","title":"From connectivity matrices"},{"location":"manual/Connector.html#connect_from_matrix","text":"This method accepts a Numpy array to define the weights of the projection (and optionally the delays). By default, the matrix should have the size (post.size, pre.size) , so that the first index represents a post-synaptic neuron and the second the pre-synaptic neurons. If your matrix is defined in the reversed order, you can either transpose it or set the pre_post argument to True . This method is useful for dense connectivity matrices (all-to-all). If you do not want to create some synapses, the weight value should be set to None . The following code creates a synfire chain inside a population of 100 neurons: N = 100 proj = Projection ( pop , pop , 'exc' ) # Initialize an empty connectivity matrix w = np . array ([[ None ] * N ] * N ) # Connect each post-synaptic neuron to its predecessor for i in range ( N ): w [ i , ( i - 1 ) % N ] = 1.0 # Create the connections proj . connect_from_matrix ( w ) Connectivity matrices can not work with multi-dimensional coordinates, only ranks are used. Population views can be used in the projection, but the connection matrix must have the corresponding size: proj = Projection ( pop [ 10 : 20 ], pop [ 50 : 60 ], 'exc' ) # Create the connectivity matrix w = np . ones (( 10 , 10 )) # Create the connections proj . connect_from_matrix ( w )","title":"connect_from_matrix"},{"location":"manual/Connector.html#connect_from_sparse","text":"For sparse connection matrices, the Numpy array format may have a huge memory overhead if most of its values are None. It is possible to use Scipy sparse matrices in that case. The previous synfire chain example becomes: from scipy.sparse import lil_matrix proj = Projection ( pop , pop , 'exc' ) w = lil_matrix (( N , N )) for i in range ( N ): w [ i , ( i + 1 ) % N ] = 1.0 proj . connect_from_sparse ( w ) Note Contrary to connect_from_matrix() , the first index of the sparse matrix represents the pre-synaptic neurons, not the post-synaptic ones. This is for compatibility with other neural simulators. connect_from_sparse() accepts lil_matrix , csr_matrix and csc_matrix objects, although lil_matrix should be preferred for its simplicity of element access.","title":"connect_from_sparse"},{"location":"manual/Connector.html#user-defined-patterns","text":"This section describes the creation of user-specific connection patterns in ANNarchy, if the available patterns are not enough. A connection pattern is simply implemented as a method returning a LILConnectivity (list-of-list) object containing all the necessary information to create the synapses. A connector method must take on the first position the pre-synaptic population (or a subset of it) and on the second one the post-synaptic population. Other arguments are free, but should be passed when creating the projection. probabilistic_pattern ( pre , post , < other arguments > ) As an example, we will recreate the fixed_probability connector method, building synapses with a given probability. For this new pattern we need a weight value (common for all synapses) and a probability value as additional arguments. We consider that no delay is introduced in the synaptic transmission.. from ANNarchy import * def probabilistic_pattern ( pre , post , weight , probability ): synapses = LILConnectivity () ... pattern code comes here ... return synapses","title":"User-defined patterns"},{"location":"manual/Connector.html#fixed_probability-in-python","text":"The connector method needs to return a LILConnectivity object storing the connectivity. For each post-synaptic neuron receiving synapses, a list of pre-synaptic ranks, weight values and delays must be added to the structure. If you use 2D or 3D populations you need to transform the coordinates into ranks with the rank_from_coordinates function. import random from ANNarchy import * def probabilistic_pattern ( pre , post , weight , probability ): # Create a LIL structure for the connectivity matrix synapses = LILConnectivity () # For all neurons in the post-synaptic population for post_rank in xrange ( post . size ): # Decide which pre-synaptic neurons should form synapses ranks = [] for pre_rank in xrange ( pre . size ): if random . random () < probability : ranks . append ( pre_rank ) # Create weights and delays arrays of the same size values = [ weight for i in xrange ( len ( ranks )) ] delays = [ 0 for i in xrange ( len ( ranks )) ] # Add this information to the LIL matrix synapses . add ( post_rank , ranks , values , delays ) return synapses The first for - loop iterates over all post-synaptic neurons in the projection. The inner for - loop decides for each of these neurons if a synapse with a pre-synaptic neuron should be created, based on the value probability provided as argument to the function. The lists values and delays are then created with the same size as ranks (important!), and filled with the desired value. All this information is then fed into the LIL matrix using the add(post_rank, ranks, values, delays) method. Note Building such connectivity matrices in Python can be extremely slow, as Python is not made for tight nested loops. If the construction of your network lasts too long, you should definitely write this function in Cython . Warning The add() should be only called once per post-synaptic neuron! If not, ANNarchy will have to reorder its internal representations and this will be really slow. Usage of the pattern To use the pattern within a projection you provide the pattern method to the connect_with_func method of Projection proj = Projection ( pre = pop1 , post = pop2 , target = 'inh' ) proj . connect_with_func ( method = probabilistic_pattern , weight = 1.0 , probability = 0.3 ) method is the method you just wrote. Extra arguments (other than pre and post ) should be passed with the same name.","title":"fixed_probability in Python"},{"location":"manual/Connector.html#fixed_probability-in-cython","text":"For this example, we will create a Cython file CustomPatterns.pyx in the same directory as the script. Its content should be relatively similar to the Python version, except some type definitions: # distutils: language = c++ import random import ANNarchy cimport ANNarchy . core . cython_ext . Connector as Connector def probabilistic_pattern ( pre , post , weight , probability ): # Typedefs cdef Connector . LILConnectivity synapses cdef int post_rank , pre_rank cdef list ranks , values , delays # Create a LILConnectivity structure for the connectivity matrix synapses = Connector . LILConnectivity () # For all neurons in the post-synaptic population for post_rank in xrange ( post . size ): # Decide which pre-synaptic neurons should form synapses ranks = [] for pre_rank in xrange ( pre . size ): if random . random () < probability : ranks . append ( pre_rank ) # Create weights and delays arrays of the same size values = [ weight for i in xrange ( len ( ranks )) ] delays = [ 0 for i in xrange ( len ( ranks )) ] # Add this information to the LILConnectivity matrix synapses . add ( post_rank , ranks , values , delays ) return synapses The only differences with the Python code are: The module Connector where the LILConnectivity connection matrix class is defined should be cimported with: cimport ANNarchy . core . cython_ext . Connector as Connector Data structures should be declared with cdef at the beginning of the method: # Typedefs cdef Connector . LILConnectivity synapses cdef int post_rank , pre_rank cdef list ranks , values , delays To allow Cython to compile this file, we also need to provide with a kind of \\\"Makefile\\\" specifying that the code should be generated in C++, not C. This file should have the same name as the Cython file but end with .pyxbld , here : CustomPatterns.pyxbld . from distutils.extension import Extension import ANNarchy def make_ext ( modname , pyxfilename ): return Extension ( name = modname , sources = [ pyxfilename ], include_dirs = ANNarchy . include_path (), extra_compile_args = [ '-std=c++11' ], language = \"c++\" ) Note This .pyxbld is generic, you don't need to modify anything, except its name. Now you can import the method probabilistic_pattern() into your Python code using the pyximport module of Cython and build the Projection normally: import pyximport ; pyximport . install () from CustomPatterns import probabilistic_pattern proj . connect_with_func ( method = probabilistic_pattern , weight = 1.0 , probability = 0.3 ) Writing the connector in Cython can bring speedups up to 100x compared to Python if the projection has a lot of synapses.","title":"fixed_probability in Cython"},{"location":"manual/ConvolutionalNetworks.html","text":"Convolution and pooling # Projections use by default a set of weights per post-synaptic neuron. Some networks, including convolutional networks, define a single operation (convolution or pooling) to be applied systematically on all pre-synaptic neurons. In such cases, it would be a waste of resources to allocate weights for each post-synaptic neuron. The extension convolution (see its API ) allows to implement such specific projections. It has to be imported explicitly at the beginning of the script: from ANNarchy import * from ANNarchy.extensions.convolution import * Warning Shared weights are only implemented for rate-coded networks. The only possible backend is currently OpenMP, CUDA will be implemented later. Simple convolutions # The simplest case of convolution is when the pre- and post-synaptic population have the same number of dimensions, for example: pre = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) Contrary to normal projections, the geometry of the populations (number of dimensions and neurons in each dimension) has a great influence on the operation to be performed. In particular the number of dimensions will define how the convolution will be applied. If for example the pre-synaptic population represents an 2D image, you may want to apply a vertical edge detector to it and get the result in the post-synaptic population. Such a filter can be defined by the following Numpy array: vertical_filter = np . array ( [ [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ] ] ) With 2 dimensions, the convolution operation (or more exactly, cross-correlation ) with a 3*3 filter is defined for all neurons in the post-synaptic population by: \\[\\text{post}[i, j] = \\sum_{c_i=-1}^1 \\sum_{c_j=-1}^1 \\text{filter}[c_i][c_j] \\cdot \\text{pre}[i + c_i, j + c_j]\\] Such a convolution is achieved by creating a Convolution object and using the connect_filter() method to create the connection pattern: proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = vertical_filter ) Each neuron of the post-synaptic population will then receive in sum('exc') (or whatever target name is used) the convolution between the kernel and a sub-region of the pre-syanptic population. ANNarchy defines the convolution operation for populations having 1, 2, 3, or 4 dimensions. Several options can be passed to the convolve() method: padding defines the value of the pre-synaptic firing rates which will be used when the coordinates are out-of-bounds. By default zero-padding is used, but you can specify another value with this argument. You can also use the 'border' value to repeat the firing rate of the neurons on the border (for example, if the filter tries to reach a neuron of coordinates (-1, -1), the firing rate of the neuron (0, 0) will be used instead). subsampling . In convolutional networks, the convolution operation is often coupled with a reduction in the number of neurons in each dimension. In the example above, the post-synaptic population could be defined with a geometry (50, 50). For each post-synaptic neuron, the coordinates of the center of the applied kernel would be automatically shifted from two pre-synaptic neurons compared to the previous one. However, if the number of neurons in one dimension of the pre-synaptic population is not exactly a multiple of the number of post-synaptic neurons in the same dimension, ANNarchy can not guess what the correct correspondance should be. In this case, you have to specify this mapping by providing to the subsampling argument a list of pre-synaptic coordinates defining the position of the center of the kernel for each post-synaptic neuron. The list is indexed by the rank of the post-synaptic neurons (use the rank_from_coordinates() method) and must have the same size as the population. Each element should be a list of coordinates in the pre-synaptic population's geometry (with as many elements as dimensions). It is possible to provide a Numpy array instead of a list of lists. One can access the coordinates in the pre-synaptic geometry of the center of the filter corresponding to a particular post-synaptic neuron by calling the center() method of Convolution with the rank or coordinates of the post neuron: pre = Population ( geometry = ( 100 , 100 ), neuron = Whatever ) post = Population ( geometry = ( 50 , 50 ), neuron = Whatever ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = vertical_filter ) pre_coordinates = proj . center ( 10 , 10 ) # returns (20, 20) In some cases, the post-synaptic population can have less dimensions than the pre-synaptic one. An example would be when the pre-synaptic population has three dimensions (e.g. (100, 100, 3)), the last representing the R, G and B components of an image. A 3D filter, with 3 components in the last dimension, would result in a (100, 100, 1) post-synaptic population (or any subsampling of it). ANNarchy accepts in this case the use of a 2D population (100, 100), but it will be checked that the number of elements in the last dimension of the filter equals the number of pre-synaptic neurons in the last dimension: pre = Population ( geometry = ( 100 , 100 , 3 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) red_filter = np . array ( [ [ [ 2.0 , - 1.0 , - 1.0 ] ] ] ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = red_filter ) Non-linear convolutions # Convolution uses by default a regular cross-correlation, summing w * pre.r over the extent of the kernel. As for regular synapses, you can change this behavior when creating the projection: the psp argument defines what will be summed. It is w*pre.r by default but can be changed to any combination of w and pre.r , such as w * log(1+pre.r) : proj = Convolution ( pre = pre , post = post , target = 'exc' , psp = 'w*log(1+pre.r)' ) the operation argument allows to change the summation operation. You can set it to 'max' (the maximum value of w*pre.r over the extent of the filter will be returned), 'min' (minimum) or 'mean' (same as 'sum', but normalized by the number of elements in the filter). The default is 'sum': proj = Convolution ( pre = pre , post = post , target = 'exc' , operation = 'max' ) Layer-wise convolutions # It is possible to define kernels with less dimensions than the pre-synaptic population. A 2D filter can for example be applied on each color component independently: pre = Population ( geometry = ( 100 , 100 , 3 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 , 3 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) vertical_filter = np . array ( [ [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ] ] ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = vertical_filter , keep_last_dimension = True ) The important parameter in this case is keep_last_dimension which tells the code generator that the last dimension of the input should not be used for convolution. The important constraint is that the post-synaptic population must have the same number of neurons in the last dimension than the pre-synaptic one (no subsampling is possible by definition). Bank of filters # Convolutional networks often use banks of filters to perform different operations (such as edge detection with various orientations). It is possible to specify this mode of functioning by using the connect_filters() method: pre = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 , 4 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) bank_filters = np . array ( [ [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ] ], [ [ - 1.0 , 0.0 , 1.0 ], [ - 1.0 , 0.0 , 1.0 ], [ - 1.0 , 0.0 , 1.0 ] ], [ [ - 1.0 , - 1.0 , - 1.0 ], [ 0.0 , 0.0 , 0.0 ], [ 1.0 , 1.0 , 1.0 ] ], [ [ 1.0 , 1.0 , 1.0 ], [ 0.0 , 0.0 , 0.0 ], [ - 1.0 , - 1.0 , - 1.0 ] ] ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filters ( weights = bank_filters ) Here the filter has three dimensions. The first one must correspond to each filter. The last dimension of the post-synaptic population must correspond to the total number of filters. It cannot be combined with keep_last_dimension . Note Current limitation : Each filter must have the same size, it is not possible yet to convolve over multiple scales. Pooling # Another form of atypical projection for a neural network is the pooling operation. In max-pooling, each post-synaptic neuron is associated to a region of the pre-synaptic population and responds like the maximum firing rate in this region. This is already possible by defining the operation argument of the synapse type, but it would use instantiated synapses, what would be a waste of memory. The Pooling class allows to define such an operation without defining any synapse: pre = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Pooling ( pre = pre , post = post , target = 'exc' , operation = 'max' ) proj . connect_pooling () The pooling region of a post-synaptic region is automatically determined by comparing the dimensions of the two populations: here each post-synaptic neuron will cover an area of 2*2 neurons. If the number of dimensions do not match, you have to specify the extent argument to pooling() . For example, you can pool completely over one dimension of the pre-synaptic population: pre = Population ( geometry = ( 100 , 100 , 10 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Pooling ( pre = pre , post = post , target = 'exc' , operation = 'max' ) proj . connect_pooling ( extent = ( 2 , 2 , 10 )) Sharing weights with another projection # A different possibility to share weights is between two projections. If your network is composed of populations of the same size, and the projection patterns are identical, it could save some memory to \"share\" the weights of one projection with another, so they are created only once. To this end, you can use the Copy class and pass it an existing projection: pop1 = Population ( geometry = ( 30 , 30 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) pop2 = Population ( geometry = ( 20 , 20 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) pop3 = Population ( geometry = ( 20 , 20 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj1 = Projection ( pop1 , pop2 , 'exc' ) proj1 . connect_gaussian ( amp = 1.0 , sigma = 0.3 , delays = 2.0 ) proj2 = Copy ( pop1 , pop3 , 'exc' ) proj2 . connect_copy ( proj1 ) This only works when the pre- and post-populations of each projection have the same geometry, but they can be different, of course. If the original projection is learnable, the copied projection will see the changes. However, it is not possible for the shared projection to learn on its own. Copy only accepts psp and operation as parameters, which can be different from the original projection. It is only possible to copy regular projections, not other shared projections. The transmission delays will be identical between the two projections.","title":"Convolution and pooling"},{"location":"manual/ConvolutionalNetworks.html#convolution-and-pooling","text":"Projections use by default a set of weights per post-synaptic neuron. Some networks, including convolutional networks, define a single operation (convolution or pooling) to be applied systematically on all pre-synaptic neurons. In such cases, it would be a waste of resources to allocate weights for each post-synaptic neuron. The extension convolution (see its API ) allows to implement such specific projections. It has to be imported explicitly at the beginning of the script: from ANNarchy import * from ANNarchy.extensions.convolution import * Warning Shared weights are only implemented for rate-coded networks. The only possible backend is currently OpenMP, CUDA will be implemented later.","title":"Convolution and pooling"},{"location":"manual/ConvolutionalNetworks.html#simple-convolutions","text":"The simplest case of convolution is when the pre- and post-synaptic population have the same number of dimensions, for example: pre = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) Contrary to normal projections, the geometry of the populations (number of dimensions and neurons in each dimension) has a great influence on the operation to be performed. In particular the number of dimensions will define how the convolution will be applied. If for example the pre-synaptic population represents an 2D image, you may want to apply a vertical edge detector to it and get the result in the post-synaptic population. Such a filter can be defined by the following Numpy array: vertical_filter = np . array ( [ [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ] ] ) With 2 dimensions, the convolution operation (or more exactly, cross-correlation ) with a 3*3 filter is defined for all neurons in the post-synaptic population by: \\[\\text{post}[i, j] = \\sum_{c_i=-1}^1 \\sum_{c_j=-1}^1 \\text{filter}[c_i][c_j] \\cdot \\text{pre}[i + c_i, j + c_j]\\] Such a convolution is achieved by creating a Convolution object and using the connect_filter() method to create the connection pattern: proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = vertical_filter ) Each neuron of the post-synaptic population will then receive in sum('exc') (or whatever target name is used) the convolution between the kernel and a sub-region of the pre-syanptic population. ANNarchy defines the convolution operation for populations having 1, 2, 3, or 4 dimensions. Several options can be passed to the convolve() method: padding defines the value of the pre-synaptic firing rates which will be used when the coordinates are out-of-bounds. By default zero-padding is used, but you can specify another value with this argument. You can also use the 'border' value to repeat the firing rate of the neurons on the border (for example, if the filter tries to reach a neuron of coordinates (-1, -1), the firing rate of the neuron (0, 0) will be used instead). subsampling . In convolutional networks, the convolution operation is often coupled with a reduction in the number of neurons in each dimension. In the example above, the post-synaptic population could be defined with a geometry (50, 50). For each post-synaptic neuron, the coordinates of the center of the applied kernel would be automatically shifted from two pre-synaptic neurons compared to the previous one. However, if the number of neurons in one dimension of the pre-synaptic population is not exactly a multiple of the number of post-synaptic neurons in the same dimension, ANNarchy can not guess what the correct correspondance should be. In this case, you have to specify this mapping by providing to the subsampling argument a list of pre-synaptic coordinates defining the position of the center of the kernel for each post-synaptic neuron. The list is indexed by the rank of the post-synaptic neurons (use the rank_from_coordinates() method) and must have the same size as the population. Each element should be a list of coordinates in the pre-synaptic population's geometry (with as many elements as dimensions). It is possible to provide a Numpy array instead of a list of lists. One can access the coordinates in the pre-synaptic geometry of the center of the filter corresponding to a particular post-synaptic neuron by calling the center() method of Convolution with the rank or coordinates of the post neuron: pre = Population ( geometry = ( 100 , 100 ), neuron = Whatever ) post = Population ( geometry = ( 50 , 50 ), neuron = Whatever ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = vertical_filter ) pre_coordinates = proj . center ( 10 , 10 ) # returns (20, 20) In some cases, the post-synaptic population can have less dimensions than the pre-synaptic one. An example would be when the pre-synaptic population has three dimensions (e.g. (100, 100, 3)), the last representing the R, G and B components of an image. A 3D filter, with 3 components in the last dimension, would result in a (100, 100, 1) post-synaptic population (or any subsampling of it). ANNarchy accepts in this case the use of a 2D population (100, 100), but it will be checked that the number of elements in the last dimension of the filter equals the number of pre-synaptic neurons in the last dimension: pre = Population ( geometry = ( 100 , 100 , 3 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) red_filter = np . array ( [ [ [ 2.0 , - 1.0 , - 1.0 ] ] ] ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = red_filter )","title":"Simple convolutions"},{"location":"manual/ConvolutionalNetworks.html#non-linear-convolutions","text":"Convolution uses by default a regular cross-correlation, summing w * pre.r over the extent of the kernel. As for regular synapses, you can change this behavior when creating the projection: the psp argument defines what will be summed. It is w*pre.r by default but can be changed to any combination of w and pre.r , such as w * log(1+pre.r) : proj = Convolution ( pre = pre , post = post , target = 'exc' , psp = 'w*log(1+pre.r)' ) the operation argument allows to change the summation operation. You can set it to 'max' (the maximum value of w*pre.r over the extent of the filter will be returned), 'min' (minimum) or 'mean' (same as 'sum', but normalized by the number of elements in the filter). The default is 'sum': proj = Convolution ( pre = pre , post = post , target = 'exc' , operation = 'max' )","title":"Non-linear convolutions"},{"location":"manual/ConvolutionalNetworks.html#layer-wise-convolutions","text":"It is possible to define kernels with less dimensions than the pre-synaptic population. A 2D filter can for example be applied on each color component independently: pre = Population ( geometry = ( 100 , 100 , 3 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 , 3 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) vertical_filter = np . array ( [ [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ] ] ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filter ( weights = vertical_filter , keep_last_dimension = True ) The important parameter in this case is keep_last_dimension which tells the code generator that the last dimension of the input should not be used for convolution. The important constraint is that the post-synaptic population must have the same number of neurons in the last dimension than the pre-synaptic one (no subsampling is possible by definition).","title":"Layer-wise convolutions"},{"location":"manual/ConvolutionalNetworks.html#bank-of-filters","text":"Convolutional networks often use banks of filters to perform different operations (such as edge detection with various orientations). It is possible to specify this mode of functioning by using the connect_filters() method: pre = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 , 4 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) bank_filters = np . array ( [ [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ], [ 1.0 , 0.0 , - 1.0 ] ], [ [ - 1.0 , 0.0 , 1.0 ], [ - 1.0 , 0.0 , 1.0 ], [ - 1.0 , 0.0 , 1.0 ] ], [ [ - 1.0 , - 1.0 , - 1.0 ], [ 0.0 , 0.0 , 0.0 ], [ 1.0 , 1.0 , 1.0 ] ], [ [ 1.0 , 1.0 , 1.0 ], [ 0.0 , 0.0 , 0.0 ], [ - 1.0 , - 1.0 , - 1.0 ] ] ) proj = Convolution ( pre = pre , post = post , target = 'exc' ) proj . connect_filters ( weights = bank_filters ) Here the filter has three dimensions. The first one must correspond to each filter. The last dimension of the post-synaptic population must correspond to the total number of filters. It cannot be combined with keep_last_dimension . Note Current limitation : Each filter must have the same size, it is not possible yet to convolve over multiple scales.","title":"Bank of filters"},{"location":"manual/ConvolutionalNetworks.html#pooling","text":"Another form of atypical projection for a neural network is the pooling operation. In max-pooling, each post-synaptic neuron is associated to a region of the pre-synaptic population and responds like the maximum firing rate in this region. This is already possible by defining the operation argument of the synapse type, but it would use instantiated synapses, what would be a waste of memory. The Pooling class allows to define such an operation without defining any synapse: pre = Population ( geometry = ( 100 , 100 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Pooling ( pre = pre , post = post , target = 'exc' , operation = 'max' ) proj . connect_pooling () The pooling region of a post-synaptic region is automatically determined by comparing the dimensions of the two populations: here each post-synaptic neuron will cover an area of 2*2 neurons. If the number of dimensions do not match, you have to specify the extent argument to pooling() . For example, you can pool completely over one dimension of the pre-synaptic population: pre = Population ( geometry = ( 100 , 100 , 10 ), neuron = Neuron ( parameters = \"r = 0.0\" )) post = Population ( geometry = ( 50 , 50 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj = Pooling ( pre = pre , post = post , target = 'exc' , operation = 'max' ) proj . connect_pooling ( extent = ( 2 , 2 , 10 ))","title":"Pooling"},{"location":"manual/ConvolutionalNetworks.html#sharing-weights-with-another-projection","text":"A different possibility to share weights is between two projections. If your network is composed of populations of the same size, and the projection patterns are identical, it could save some memory to \"share\" the weights of one projection with another, so they are created only once. To this end, you can use the Copy class and pass it an existing projection: pop1 = Population ( geometry = ( 30 , 30 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) pop2 = Population ( geometry = ( 20 , 20 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) pop3 = Population ( geometry = ( 20 , 20 ), neuron = Neuron ( equations = \"r = sum(exc)\" )) proj1 = Projection ( pop1 , pop2 , 'exc' ) proj1 . connect_gaussian ( amp = 1.0 , sigma = 0.3 , delays = 2.0 ) proj2 = Copy ( pop1 , pop3 , 'exc' ) proj2 . connect_copy ( proj1 ) This only works when the pre- and post-populations of each projection have the same geometry, but they can be different, of course. If the original projection is learnable, the copied projection will see the changes. However, it is not possible for the shared projection to learn on its own. Copy only accepts psp and operation as parameters, which can be different from the original projection. It is only possible to copy regular projections, not other shared projections. The transmission delays will be identical between the two projections.","title":"Sharing weights with another projection"},{"location":"manual/Hybrid.html","text":"Hybrid networks # ANNarchy has the possibility to simulate either rate-coded or spiking networks. It is therefore possible to define hybrid networks mixing rate-coded and spiking populations. A typical application would be to define a rate-coded network to process visual inputs, which is used to feed a spiking network for action selection. A dummy example is provided in examples/hybrid . Rate-coded to Spike # Converting a rate-coded population to a spiking network is straightforward. The PoissonPopulation (see its API ) defines a population of spiking neurons emitting spikes following a Poisson distribution: pop = PoissonPopulation ( 1000 , rates = 50. ) In this case, the 1000 neurons emit spikes at a rate of 50 Hz (the rate of individual neurons can be later modified). It is possible to use a weighted sum of rate-coded synapses in order to determine the firing rate of each Poisson neuron. It requires to connect a rate-coded population to the PoissonPopulation with a given target: pop1 = Population ( 4 , Neuron ( parameters = \"r=0.0\" )) pop2 = PoissonPopulation ( 1000 , target = 'exc' ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_number_pre ( weights = 10.0 , number = 1 ) In this example, each of the 4 pre-synaptic neurons \"controls\" the firing rate of one fourth (on average) of the post-synaptic ones. If target is used in the Poisson population, rates will be ignored. The weights determine the scaling of the transmission: a presynaptic rate r of 1.0 generates a firing rate of w Hz in the post-synaptic neurons. Here setting pop1.r = 1.0 will make the post-synaptic neurons fire at 10 Hz. Spike to Rate-coded # Decoding a spiking population is a harder process, because of the stochastic nature of spike trains. One can take advantage of the fact here that a rate-coded neuron usually represents an ensemble of spiking neurons, so the average firing rate in that ensemble can be more precisely decoded. In order to do so, one needs to connect the spiking population to a rate-coded one with a many-to-one pattern using a DecodingProjection . A DecodingProjection heritates all methods of Projection (including the connection methods) but performs the necessary conversion from spike trains to a instantaneous rate: pop1 = PoissonPopulation ( 1000 , rates = 50.0 ) pop2 = Population ( 1 , Neuron ( equations = \"r=sum(exc)\" )) proj = DecodingProjection ( pop1 , pop2 , 'exc' , window = 10.0 ) proj . connect_all_to_all ( weights = 1.0 ) In this example, the spiking population fires at 50 Hz. The single rate-coded neuron decoding that population will count how many spikes arrived in the last \\(T\\) milliseconds and divide it by the total number of synapses in order to estimate the population firing rate in pop1 . This would be accessed in sum(exc) (or whatever target is used in the projection). Because of its simple definition, it will therefore have its rate r at 50.0 (with some variance due to the stochastic nature of spike trains). The window argument defines the duration in milliseconds of the sliding temporal window used to estimate the firing rate. By default, it is equal to dt , which means spikes are counted in a very narrow period of time, what could lead to very big variations of the decoded firing rate. If the window is too big, it would introduce a noticeable lag for the decoded firing rate if the input varies too quickly. window = 10.0 is usually a good compromise, but this depends on the input firing rate. The weights of the projection define the scaling of the decoded firing rate. If one wants a firing rate of 100 Hz to be represented by r=1.0 , the weights should be set to 0.01. No Synapse model can be used in a DecodingProjection . Warning DecodingProjection is not implemented on CUDA yet.","title":"Hybrid networks"},{"location":"manual/Hybrid.html#hybrid-networks","text":"ANNarchy has the possibility to simulate either rate-coded or spiking networks. It is therefore possible to define hybrid networks mixing rate-coded and spiking populations. A typical application would be to define a rate-coded network to process visual inputs, which is used to feed a spiking network for action selection. A dummy example is provided in examples/hybrid .","title":"Hybrid networks"},{"location":"manual/Hybrid.html#rate-coded-to-spike","text":"Converting a rate-coded population to a spiking network is straightforward. The PoissonPopulation (see its API ) defines a population of spiking neurons emitting spikes following a Poisson distribution: pop = PoissonPopulation ( 1000 , rates = 50. ) In this case, the 1000 neurons emit spikes at a rate of 50 Hz (the rate of individual neurons can be later modified). It is possible to use a weighted sum of rate-coded synapses in order to determine the firing rate of each Poisson neuron. It requires to connect a rate-coded population to the PoissonPopulation with a given target: pop1 = Population ( 4 , Neuron ( parameters = \"r=0.0\" )) pop2 = PoissonPopulation ( 1000 , target = 'exc' ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_number_pre ( weights = 10.0 , number = 1 ) In this example, each of the 4 pre-synaptic neurons \"controls\" the firing rate of one fourth (on average) of the post-synaptic ones. If target is used in the Poisson population, rates will be ignored. The weights determine the scaling of the transmission: a presynaptic rate r of 1.0 generates a firing rate of w Hz in the post-synaptic neurons. Here setting pop1.r = 1.0 will make the post-synaptic neurons fire at 10 Hz.","title":"Rate-coded to Spike"},{"location":"manual/Hybrid.html#spike-to-rate-coded","text":"Decoding a spiking population is a harder process, because of the stochastic nature of spike trains. One can take advantage of the fact here that a rate-coded neuron usually represents an ensemble of spiking neurons, so the average firing rate in that ensemble can be more precisely decoded. In order to do so, one needs to connect the spiking population to a rate-coded one with a many-to-one pattern using a DecodingProjection . A DecodingProjection heritates all methods of Projection (including the connection methods) but performs the necessary conversion from spike trains to a instantaneous rate: pop1 = PoissonPopulation ( 1000 , rates = 50.0 ) pop2 = Population ( 1 , Neuron ( equations = \"r=sum(exc)\" )) proj = DecodingProjection ( pop1 , pop2 , 'exc' , window = 10.0 ) proj . connect_all_to_all ( weights = 1.0 ) In this example, the spiking population fires at 50 Hz. The single rate-coded neuron decoding that population will count how many spikes arrived in the last \\(T\\) milliseconds and divide it by the total number of synapses in order to estimate the population firing rate in pop1 . This would be accessed in sum(exc) (or whatever target is used in the projection). Because of its simple definition, it will therefore have its rate r at 50.0 (with some variance due to the stochastic nature of spike trains). The window argument defines the duration in milliseconds of the sliding temporal window used to estimate the firing rate. By default, it is equal to dt , which means spikes are counted in a very narrow period of time, what could lead to very big variations of the decoded firing rate. If the window is too big, it would introduce a noticeable lag for the decoded firing rate if the input varies too quickly. window = 10.0 is usually a good compromise, but this depends on the input firing rate. The weights of the projection define the scaling of the decoded firing rate. If one wants a firing rate of 100 Hz to be represented by r=1.0 , the weights should be set to 0.01. No Synapse model can be used in a DecodingProjection . Warning DecodingProjection is not implemented on CUDA yet.","title":"Spike to Rate-coded"},{"location":"manual/Inputs.html","text":"Setting inputs # The methods to set inputs to a network depends on whether it is rate-coded or spiking. Specific populations to set inputs are described in the API . Inputs to a rate-coded network # Standard method # The simplest way to define an input population is to use a dummy neuron which simply holds a firing rate r as parameter, and connect it to another population: input_pop = Population ( 10 , Neuron ( parameters = \"r=0.0\" )) pop = Population ( 10 , LeakyIntegrator ) proj = Projection ( input_pop , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) compile () simulate ( 100. ) input_pop . r = 1.0 simulate ( 100. ) The only thing you need to do is to manipulate the numpy array r holded by input_pop , and it will influence the \\\"real\\\" population pop It is important to define r as a parameter of the neuron, not a variable in equations . A variable sees its value updated at each step, so the value you set would be immediately forgotten. Note Using this method necessitates to interact with the input population in the Python script everytime you want to change the inputs. If the inputs change every time step, your simulation will alternate between Python and C++ executions and potentially become very slow. Timed Arrays # If the inputs change frequently, it may be more efficient to store all these values in a TimedArray (doc in the API ). Let's suppose you have a population of 10 neurons which should be activated sequentially over time. You can store the inputs to these neurons in a Numpy array, where the first axis corresponds to time and the second (or more) to the geometry of the population: inputs = np . array ( [ [ 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 ] ] ) inp = TimedArray ( rates = inputs ) pop = Population ( 10 , Neuron ( equations = \"r=sum(exc)\" )) proj = Projection ( inp , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) compile () simulate ( 10. ) With this code, each neuron will be activated in sequence at each time step ( dt=1.0 by default). If you simulate longer than 10 ms, the last input [0, 0, .., 1] will be kept forever. If the rates array has two dimensions, the corresponding population will be 1D. You can pass a multidimensional array to obtain a 2D or 3D population. Presenting a input for only one time step is very short, especially if the population pop uses ODEs to integrate the inputs. You can provide a schedule parameter to the TimedArray to define how long (in ms) an input should be presented: inp = TimedArray ( rates = inputs , schedule = 10. ) Here each input will be kept constant for 10 ms, so the 10 inputs will need 100 ms of simulation to be presented. If you do not want a regular schedule, you can also provide a list of times where inputs should be set: inp = TimedArray ( rates = inputs , schedule = [ 0. , 10. , 30. , 60. , 100. , 150. , 210. , 280. , 360. , 450. ]) The length of the schedule list should be equal or smaller to the number of inputs defined in rates . If this length is smaller (e.g. 7), only the 7 first inputs will be used as inputs. If the length is bigger, it will lead to an error. A TimedArray can be reset to iterate again over the inputs: inp = TimedArray ( rates = inputs , schedule = 10. ) ... compile () simulate ( 100. ) # The ten inputs are shown with a schedule of 10 ms inp . reset () simulate ( 100. ) # The same ten inputs are presented again. The times declared in schedule are therefore relative to the last call to reset() (or to t=0.0 at the beginning). If you want to systematically iterate over the inputs without iterating over simulate() and reset() , you can provide the period argument to the TimedArray to define how often the inputs will be reset: inp = TimedArray ( rates = inputs , schedule = 10. . period = 100. ) ... simulate ( 100000. ) If the period is smaller than the total durations of the inputs, the last inputs will be skipped. The rates , schedule and period can be modified after compilation. The only constraint is that the size of the population (defined in the rates array) must stay the same. Images and Videos # Images A simple utility to directly load an image into the firing rates r of a Population is provided by the ImagePopulation class. This class is not automatically imported with ANNarchy, you need to explicitly import it: from ANNarchy import * from ANNarchy.extensions.image import ImagePopulation inp = ImagePopulation ( geometry = ( 480 , 640 )) inp . set_image ( 'image.jpg' ) Using this class requires that you have the Python Image Library installed ( pip install Pillow ). Any image with a format supported by Pillow can be loaded, see the documentation . The ImagePopulation must be initialized with a geometry corresponding to the desired size of the population. If it differs from the resolution of the image (set with set_image ), the image will be first resized to match the geometry of the population. Note The size of an image is defined as (height, width), so a 640x480 image should be loaded in a (480, 640 population). If the geometry has only two dimensions (480, 640), each neuron will represent the luminance (or brightness) of the corresponding pixel. If the geometry has three dimensions (480, 640, 3), the color channels will additionally be represented (RGB). Any other value than 3 for the third dimension will generate an error. Note The firing rate r of a neuron is 1.0 when the corresponding pixel is white (value 255 as an unsigned integer on 8 bits). Note that the following code is functionally equivalent: from ANNarchy import * from PIL import Image inp = Population ( geometry = ( 480 , 640 ), Neuron ( parameters = \"r=0.0\" )) img = Image . open ( 'image.jpg' ) img = img . convert ( 'L' ) img = img . resize (( 480 , 640 )) / 255. inp . r = np . array ( img ) An example is provided in examples/image/Image.py . Videos The VideoPopulation class allows to retrieve images from a Webcam, using the OpenCV computer vision library, version 4.0 or later. pkg-config opencv4 --cflags --libs should not return an error. vtk might have to be additionally installed. from ANNarchy import * from ANNarchy.extensions.image import VideoPopulation inp = VideoPopulation ( geometry = ( 480 , 640 )) compile () inp . start_camera ( 0 ) while ( True ): inp . grab_image () simulate ( 10.0 ) A geometry must be provided as for ImagePopulations . The camera must be explicitly started after compile() with inp.start_camera(0) . 0 corresponds to the index of your camera, change it if you have multiple cameras. The VideoPopulation can then acquire frames from the camera with inp.grab_image() and store the correponding image in its firing rate r (also scaled between 0.0 and 1.0). An example is provided in examples/image/Webcam.py . Warning VideoPopulation is not available with the CUDA backend. Inputs to a spiking network # Standard method # To control the spiking patterns of a spiking population, the simplest way is to inject current into the corresponding membrane potentials. The built-in neuron types defined by ANNarchy have a i_offset variable that can be used for this purpose: from ANNarchy import * setup ( dt = 0.1 ) pop = Population ( 100 , Izhikevich ) pop . i_offset = np . linspace ( 0.0 , 30.0 , 100 ) m = Monitor ( pop , 'spike' ) compile () simulate ( 100. ) data = m . get ( 'spike' ) t , n = m . raster_plot ( data ) import matplotlib.pyplot as plt plt . plot ( t , n , '.' ) plt . ylim ( 0 , 100 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show () Current injection # If you want the injected current to be time-varying, you can design a rate-coded population of the same size as the spiking population and create a CurrentInjection projection between them: inp = Population(100, Neuron(equations=\"r = sin(t)\")) pop = Population(100, Izhikevich) proj = CurrentInjection(inp, pop, 'exc') proj.connect_current() The current g_exc of a neuron in pop will be set at each time step to the firing rate r of the corresponding neuron in inp (i.e. with the same rank). inp can also be defined as a TimedArray . The connector method should be connect_current() , which accepts no weight value and no delay. SpikeSourceArray # If you want to control precisely the spiking patterns used as inputs, you can provide a list of spike times to a SpikeSourceArray object: from ANNarchy import * setup ( dt = 0.1 ) spike_times = [ [ 10 + i / 10 , 20 + i / 10 , 30 + i / 10 , 40 + i / 10 , 50 + i / 10 , 60 + i / 10 , 70 + i / 10 , 80 + i / 10 , 90 + i / 10 ] for i in range ( 100 ) ] pop = SpikeSourceArray ( spike_times = spike_times ) m = Monitor ( pop , 'spike' ) compile () simulate ( 100. ) data = m . get ( 'spike' ) t , n = m . raster_plot ( data ) import matplotlib.pyplot as plt plt . plot ( t , n , '.' ) plt . ylim ( 0 , 100 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show () The spike_times argument must be a list of lists containing the spike times in ms. Its length defines the number of neurons in the population. It is not possible to define a geometry. If one neuron should not spike at all, just provide an empty list. The different neurons can have a different number of spikes. If you want to repeat the same stimulation, you can reset the SpikeSourceArray, what will set its internal time back to 0.0: simulate ( 100. ) pop . reset () simulate ( 100. ) The spikes times can be changed after compilation, bit it must have the same number of neurons: pop.spike_times = new_spike_times_array An example is provided in examples/pyNN/IF_curr_alpha.py . Warning SpikeSourceArray is not available with the CUDA backend. Poisson population # The PoissonPopulation class allows to create a population of identical spiking neurons, whose spiking patterns vary according to a Poisson distribution: from ANNarchy import * setup ( dt = 0.1 ) pop = PoissonPopulation ( 100 , rates = 30. ) m = Monitor ( pop , 'spike' ) compile () simulate ( 100. ) data = m . get ( 'spike' ) t , n = m . raster_plot ( data ) import matplotlib.pyplot as plt plt . plot ( t , n , '.' ) plt . ylim ( 0 , 100 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show () In this example, each of the 100 neurons fires randomly, with a mean firing rate of 30 Hz (next figure, top-left). It is also possible to specify the mean firing rate individually for each neuron (next figure, top-right): pop = PoissonPopulation ( 100 , rates = np . linspace ( 0.0 , 100.0 , 100 )) The rates attribute can be modified at any time during the simulation, as long as it has the same size as the population. Another possibility is to define a rule for the evolution of the mean firing rate in the population (next figure, bottom-left): pop = PoissonPopulation ( geometry = 100 , parameters = \"\"\" amp = 100.0 frequency = 50.0 \"\"\" , rates = \"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\" ) The rule can only depend on the time t : the corresponding mean firing rate is the same for all neurons in the population. Finally, the rates argument can be replaced by a target, so it can be computed by another rate-coded population (next figure, bottom-right): rates = 10. * np . ones (( 2 , 100 )) rates [ 0 , : 50 ] = 100. rates [ 1 , 50 :] = 100. inp = TimedArray ( rates = rates , schedule = 50. ) pop = PoissonPopulation ( 100 , target = \"exc\" ) proj = Projection ( inp , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) In the code above, we define a TimedArray for 100 neurons, so that half of the neurons fire at 100 Hz, while the others fire at 10 Hz. Every 50 ms, the two halves are swapped. We just need to create a projection with the target \\\"exc\\\" between the TimedArray and the PoissonPopulation (with a one-to-one pattern and weights 1.0 to preserve scaling), and the Poisson population will reflect the firing rates defined by the TimedArray. Homogeneous correlated inputs # HomogeneousCorrelatedSpikeTrains defines spiking neurons following a homogeneous distribution with correlated spike trains. The method describing the generation of homogeneous correlated spike trains is described in: Brette, R. (2009). Generation of correlated spike trains. http://audition.ens.fr/brette/papers/Brette2008NC.html The implementation is based on the one provided by Brian . To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation: \\[\\dfrac{dx}{dt} = \\dfrac{\\mu - x}{\\tau} + \\sigma \\, \\dfrac{\\xi }{\\sqrt{\\tau}}\\] where \\(\\xi\\) is a random variable. Basically, \\(x\\) will randomly vary around $\\mu\u00a7 over time, with an amplitude determined by \\(\\sigma\\) and a speed determined by \\(\\tau\\) . This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process. To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates , the desired correlation strength corr and the time constant tau . See Brette's paper for details. In short, you should only define the parameters rates , corr and tau , and let the class compute mu and sigma for you. Changing rates , corr or tau after initialization automatically recomputes mu and sigma. Example: from ANNarchy import * setup ( dt = 0.1 ) pop_poisson = PoissonPopulation ( 200 , rates = 10. ) pop_corr = HomogeneousCorrelatedSpikeTrains ( 200 , rates = 10. , corr = 0.3 , tau = 10. ) compile () simulate ( 1000. ) pop_poisson . rates = 30. pop_corr . rates = 30. simulate ( 1000. )","title":"Setting inputs"},{"location":"manual/Inputs.html#setting-inputs","text":"The methods to set inputs to a network depends on whether it is rate-coded or spiking. Specific populations to set inputs are described in the API .","title":"Setting inputs"},{"location":"manual/Inputs.html#inputs-to-a-rate-coded-network","text":"","title":"Inputs to a rate-coded network"},{"location":"manual/Inputs.html#standard-method","text":"The simplest way to define an input population is to use a dummy neuron which simply holds a firing rate r as parameter, and connect it to another population: input_pop = Population ( 10 , Neuron ( parameters = \"r=0.0\" )) pop = Population ( 10 , LeakyIntegrator ) proj = Projection ( input_pop , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) compile () simulate ( 100. ) input_pop . r = 1.0 simulate ( 100. ) The only thing you need to do is to manipulate the numpy array r holded by input_pop , and it will influence the \\\"real\\\" population pop It is important to define r as a parameter of the neuron, not a variable in equations . A variable sees its value updated at each step, so the value you set would be immediately forgotten. Note Using this method necessitates to interact with the input population in the Python script everytime you want to change the inputs. If the inputs change every time step, your simulation will alternate between Python and C++ executions and potentially become very slow.","title":"Standard method"},{"location":"manual/Inputs.html#timed-arrays","text":"If the inputs change frequently, it may be more efficient to store all these values in a TimedArray (doc in the API ). Let's suppose you have a population of 10 neurons which should be activated sequentially over time. You can store the inputs to these neurons in a Numpy array, where the first axis corresponds to time and the second (or more) to the geometry of the population: inputs = np . array ( [ [ 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 ] ] ) inp = TimedArray ( rates = inputs ) pop = Population ( 10 , Neuron ( equations = \"r=sum(exc)\" )) proj = Projection ( inp , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) compile () simulate ( 10. ) With this code, each neuron will be activated in sequence at each time step ( dt=1.0 by default). If you simulate longer than 10 ms, the last input [0, 0, .., 1] will be kept forever. If the rates array has two dimensions, the corresponding population will be 1D. You can pass a multidimensional array to obtain a 2D or 3D population. Presenting a input for only one time step is very short, especially if the population pop uses ODEs to integrate the inputs. You can provide a schedule parameter to the TimedArray to define how long (in ms) an input should be presented: inp = TimedArray ( rates = inputs , schedule = 10. ) Here each input will be kept constant for 10 ms, so the 10 inputs will need 100 ms of simulation to be presented. If you do not want a regular schedule, you can also provide a list of times where inputs should be set: inp = TimedArray ( rates = inputs , schedule = [ 0. , 10. , 30. , 60. , 100. , 150. , 210. , 280. , 360. , 450. ]) The length of the schedule list should be equal or smaller to the number of inputs defined in rates . If this length is smaller (e.g. 7), only the 7 first inputs will be used as inputs. If the length is bigger, it will lead to an error. A TimedArray can be reset to iterate again over the inputs: inp = TimedArray ( rates = inputs , schedule = 10. ) ... compile () simulate ( 100. ) # The ten inputs are shown with a schedule of 10 ms inp . reset () simulate ( 100. ) # The same ten inputs are presented again. The times declared in schedule are therefore relative to the last call to reset() (or to t=0.0 at the beginning). If you want to systematically iterate over the inputs without iterating over simulate() and reset() , you can provide the period argument to the TimedArray to define how often the inputs will be reset: inp = TimedArray ( rates = inputs , schedule = 10. . period = 100. ) ... simulate ( 100000. ) If the period is smaller than the total durations of the inputs, the last inputs will be skipped. The rates , schedule and period can be modified after compilation. The only constraint is that the size of the population (defined in the rates array) must stay the same.","title":"Timed Arrays"},{"location":"manual/Inputs.html#images-and-videos","text":"Images A simple utility to directly load an image into the firing rates r of a Population is provided by the ImagePopulation class. This class is not automatically imported with ANNarchy, you need to explicitly import it: from ANNarchy import * from ANNarchy.extensions.image import ImagePopulation inp = ImagePopulation ( geometry = ( 480 , 640 )) inp . set_image ( 'image.jpg' ) Using this class requires that you have the Python Image Library installed ( pip install Pillow ). Any image with a format supported by Pillow can be loaded, see the documentation . The ImagePopulation must be initialized with a geometry corresponding to the desired size of the population. If it differs from the resolution of the image (set with set_image ), the image will be first resized to match the geometry of the population. Note The size of an image is defined as (height, width), so a 640x480 image should be loaded in a (480, 640 population). If the geometry has only two dimensions (480, 640), each neuron will represent the luminance (or brightness) of the corresponding pixel. If the geometry has three dimensions (480, 640, 3), the color channels will additionally be represented (RGB). Any other value than 3 for the third dimension will generate an error. Note The firing rate r of a neuron is 1.0 when the corresponding pixel is white (value 255 as an unsigned integer on 8 bits). Note that the following code is functionally equivalent: from ANNarchy import * from PIL import Image inp = Population ( geometry = ( 480 , 640 ), Neuron ( parameters = \"r=0.0\" )) img = Image . open ( 'image.jpg' ) img = img . convert ( 'L' ) img = img . resize (( 480 , 640 )) / 255. inp . r = np . array ( img ) An example is provided in examples/image/Image.py . Videos The VideoPopulation class allows to retrieve images from a Webcam, using the OpenCV computer vision library, version 4.0 or later. pkg-config opencv4 --cflags --libs should not return an error. vtk might have to be additionally installed. from ANNarchy import * from ANNarchy.extensions.image import VideoPopulation inp = VideoPopulation ( geometry = ( 480 , 640 )) compile () inp . start_camera ( 0 ) while ( True ): inp . grab_image () simulate ( 10.0 ) A geometry must be provided as for ImagePopulations . The camera must be explicitly started after compile() with inp.start_camera(0) . 0 corresponds to the index of your camera, change it if you have multiple cameras. The VideoPopulation can then acquire frames from the camera with inp.grab_image() and store the correponding image in its firing rate r (also scaled between 0.0 and 1.0). An example is provided in examples/image/Webcam.py . Warning VideoPopulation is not available with the CUDA backend.","title":"Images and Videos"},{"location":"manual/Inputs.html#inputs-to-a-spiking-network","text":"","title":"Inputs to a spiking network"},{"location":"manual/Inputs.html#standard-method_1","text":"To control the spiking patterns of a spiking population, the simplest way is to inject current into the corresponding membrane potentials. The built-in neuron types defined by ANNarchy have a i_offset variable that can be used for this purpose: from ANNarchy import * setup ( dt = 0.1 ) pop = Population ( 100 , Izhikevich ) pop . i_offset = np . linspace ( 0.0 , 30.0 , 100 ) m = Monitor ( pop , 'spike' ) compile () simulate ( 100. ) data = m . get ( 'spike' ) t , n = m . raster_plot ( data ) import matplotlib.pyplot as plt plt . plot ( t , n , '.' ) plt . ylim ( 0 , 100 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show ()","title":"Standard method"},{"location":"manual/Inputs.html#current-injection","text":"If you want the injected current to be time-varying, you can design a rate-coded population of the same size as the spiking population and create a CurrentInjection projection between them: inp = Population(100, Neuron(equations=\"r = sin(t)\")) pop = Population(100, Izhikevich) proj = CurrentInjection(inp, pop, 'exc') proj.connect_current() The current g_exc of a neuron in pop will be set at each time step to the firing rate r of the corresponding neuron in inp (i.e. with the same rank). inp can also be defined as a TimedArray . The connector method should be connect_current() , which accepts no weight value and no delay.","title":"Current injection"},{"location":"manual/Inputs.html#spikesourcearray","text":"If you want to control precisely the spiking patterns used as inputs, you can provide a list of spike times to a SpikeSourceArray object: from ANNarchy import * setup ( dt = 0.1 ) spike_times = [ [ 10 + i / 10 , 20 + i / 10 , 30 + i / 10 , 40 + i / 10 , 50 + i / 10 , 60 + i / 10 , 70 + i / 10 , 80 + i / 10 , 90 + i / 10 ] for i in range ( 100 ) ] pop = SpikeSourceArray ( spike_times = spike_times ) m = Monitor ( pop , 'spike' ) compile () simulate ( 100. ) data = m . get ( 'spike' ) t , n = m . raster_plot ( data ) import matplotlib.pyplot as plt plt . plot ( t , n , '.' ) plt . ylim ( 0 , 100 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show () The spike_times argument must be a list of lists containing the spike times in ms. Its length defines the number of neurons in the population. It is not possible to define a geometry. If one neuron should not spike at all, just provide an empty list. The different neurons can have a different number of spikes. If you want to repeat the same stimulation, you can reset the SpikeSourceArray, what will set its internal time back to 0.0: simulate ( 100. ) pop . reset () simulate ( 100. ) The spikes times can be changed after compilation, bit it must have the same number of neurons: pop.spike_times = new_spike_times_array An example is provided in examples/pyNN/IF_curr_alpha.py . Warning SpikeSourceArray is not available with the CUDA backend.","title":"SpikeSourceArray"},{"location":"manual/Inputs.html#poisson-population","text":"The PoissonPopulation class allows to create a population of identical spiking neurons, whose spiking patterns vary according to a Poisson distribution: from ANNarchy import * setup ( dt = 0.1 ) pop = PoissonPopulation ( 100 , rates = 30. ) m = Monitor ( pop , 'spike' ) compile () simulate ( 100. ) data = m . get ( 'spike' ) t , n = m . raster_plot ( data ) import matplotlib.pyplot as plt plt . plot ( t , n , '.' ) plt . ylim ( 0 , 100 ) plt . xlabel ( 'Time (ms)' ) plt . ylabel ( '# neuron' ) plt . show () In this example, each of the 100 neurons fires randomly, with a mean firing rate of 30 Hz (next figure, top-left). It is also possible to specify the mean firing rate individually for each neuron (next figure, top-right): pop = PoissonPopulation ( 100 , rates = np . linspace ( 0.0 , 100.0 , 100 )) The rates attribute can be modified at any time during the simulation, as long as it has the same size as the population. Another possibility is to define a rule for the evolution of the mean firing rate in the population (next figure, bottom-left): pop = PoissonPopulation ( geometry = 100 , parameters = \"\"\" amp = 100.0 frequency = 50.0 \"\"\" , rates = \"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\" ) The rule can only depend on the time t : the corresponding mean firing rate is the same for all neurons in the population. Finally, the rates argument can be replaced by a target, so it can be computed by another rate-coded population (next figure, bottom-right): rates = 10. * np . ones (( 2 , 100 )) rates [ 0 , : 50 ] = 100. rates [ 1 , 50 :] = 100. inp = TimedArray ( rates = rates , schedule = 50. ) pop = PoissonPopulation ( 100 , target = \"exc\" ) proj = Projection ( inp , pop , 'exc' ) proj . connect_one_to_one ( 1.0 ) In the code above, we define a TimedArray for 100 neurons, so that half of the neurons fire at 100 Hz, while the others fire at 10 Hz. Every 50 ms, the two halves are swapped. We just need to create a projection with the target \\\"exc\\\" between the TimedArray and the PoissonPopulation (with a one-to-one pattern and weights 1.0 to preserve scaling), and the Poisson population will reflect the firing rates defined by the TimedArray.","title":"Poisson population"},{"location":"manual/Inputs.html#homogeneous-correlated-inputs","text":"HomogeneousCorrelatedSpikeTrains defines spiking neurons following a homogeneous distribution with correlated spike trains. The method describing the generation of homogeneous correlated spike trains is described in: Brette, R. (2009). Generation of correlated spike trains. http://audition.ens.fr/brette/papers/Brette2008NC.html The implementation is based on the one provided by Brian . To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation: \\[\\dfrac{dx}{dt} = \\dfrac{\\mu - x}{\\tau} + \\sigma \\, \\dfrac{\\xi }{\\sqrt{\\tau}}\\] where \\(\\xi\\) is a random variable. Basically, \\(x\\) will randomly vary around $\\mu\u00a7 over time, with an amplitude determined by \\(\\sigma\\) and a speed determined by \\(\\tau\\) . This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process. To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates , the desired correlation strength corr and the time constant tau . See Brette's paper for details. In short, you should only define the parameters rates , corr and tau , and let the class compute mu and sigma for you. Changing rates , corr or tau after initialization automatically recomputes mu and sigma. Example: from ANNarchy import * setup ( dt = 0.1 ) pop_poisson = PoissonPopulation ( 200 , rates = 10. ) pop_corr = HomogeneousCorrelatedSpikeTrains ( 200 , rates = 10. , corr = 0.3 , tau = 10. ) compile () simulate ( 1000. ) pop_poisson . rates = 30. pop_corr . rates = 30. simulate ( 1000. )","title":"Homogeneous correlated inputs"},{"location":"manual/Logging.html","text":"Logging with tensorboard # The tensorboard extension allows to visualize ANNarchy simulations using tensorboard. It requires the tensorboardX package: pip install tensorboardX as well as tensorboard, of course: pip install tensorboard The Logger class is a thin wrapper around tensorboardX.SummaryWriter , which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io . Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch. The extension has to be imported explicitly: from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger For detailed examples of how to use the extension, refer to the examples Basal Ganglia and Bayesian optimization , which are available as notebooks in the folder examples/tensorboard . Creating the logger # The Logger class has to be closed properly at the end of the script, so it is advised to use a context: with Logger () as logger : logger . add_scalar ( \"Accuracy\" , acc , trial ) You can also make sure to close it: logger = Logger () logger . add_scalar ( \"Accuracy\" , acc , trial ) logger . close () By default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine . You can control these two elements by passing arguments to Logger() : with Logger ( logdir = \"/tmp/annarchy\" , experiment = \"trial1\" ): # logs in /tmp/annarchy/trial1 Launching tensorboard # After (or while) logging data within your simulation, run tensorboard in the terminal by specifying the path to the log directory: tensorboard --logdir runs You will then be asked to open localhost:6006 in your browser and will see a page similar to this: The information logged will be available in the different tabs (scalars, images...). You can also visualize and compare several experiments at the same time. Logging scalars # The add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc. The simplest information to log is a scalar, for example the accuracy at the end of a trial: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) accuracy = ... logger . add_scalar ( \"Accuracy\" , accuracy , trial ) A tag should be given for each plot as the first argument. In the example above, the figure with the accuracy will be labelled \\\"Accuracy\\\" in tensorboard. You can also group plots together with 2-levels tags such as: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) train_accuracy = ... test_accuracy = ... logger . add_scalar ( \"Accuracy/Train\" , train_accuracy , trial ) logger . add_scalar ( \"Accuracy/Test\" , test_accuracy , trial ) The second argument is the scalar, obviously. The third is the index of x-axis of the plot. It can be the index of the trial, the current time or whatever you prefer. If you want to display several scalars on the same plot, you can use the method add_scalars() and provide a dictionary: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) train_accuracy = ... test_accuracy = ... logger . add_scalars ( \"Accuracy\" , { 'train' : train_accuracy , 'test' : test_accuracy }, trial ) Logging images # It is also possible to log images, for example an input image or the firing rate of a 2D population, with the add_image() method: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) img = pop . r . reshape (( 10 , 10 )) logger . add_image ( \"Population/Firing rate\" , img , trial ) The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values must be floats between 0 and 1 or integers between 0 and 255 in order to be displayed correctly. You can either do it yourself, or pass equalize=True to the add_image() : logger . add_image ( \"Population/Firing rate\" , img , trial , equalize = True ) The min/max values in the array are internally used to rescale the image: img = ( img - img . min ()) / ( img . max () - img . min ()) To display several images together, for example the receptive fields of a population, an array of size (number, height, width) or (number, height, width, 3) can be passed to add_images() , where number is the number of images to display: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . reshape ( 100 , 10 , 10 ) # 100 post neurons, 10*10 pre neurons logger . add_images ( \"Projection/Receptive fields\" , weights , trial , equalize = True ) equalize=True applies the same scaling to all images, but you additionally pass equalize_per_image=True to have indepent scalings per image. Logging histograms # Histograms can also be logged, for example to visualize the statistics of weights in a projection: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . flatten () logger . add_histogram ( \"Weight distribution\" , weights , trial ) Logging figures # Matplotlib figures can also be logged: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) fig = plt . figure () plt . plot ( pop . r ) logger . add_figure ( \"Activity\" , fig , trial ) add_figure() will automatically close the figure, no need to call show() . Beware that this is very slow and requires a lot of space. Logging parameters # The previous methods can be called multiple times during a simulation, in order to visualize the changes during learning. add_parameters() is more useful in the context of hyperparameter optimization, where the same network with different parameters is run multiple times. Only once per simulation, typically at the end, you can log the value of some important parameters together with some metrics such as accuracy, error rate or so. This will allow tensorboard to display over multiple runs the relation between the parameters and the metrics in the tab \"HPARAMS\": with Logger () as logger : # ... logger . add_parameters ({ 'learning_rate' : lr , 'tau' : tau }, { 'accuracy' : accuracy }) Refer to Bayesian optimization for an example using the hyperopt library.","title":"Logging with tensorboard"},{"location":"manual/Logging.html#logging-with-tensorboard","text":"The tensorboard extension allows to visualize ANNarchy simulations using tensorboard. It requires the tensorboardX package: pip install tensorboardX as well as tensorboard, of course: pip install tensorboard The Logger class is a thin wrapper around tensorboardX.SummaryWriter , which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io . Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch. The extension has to be imported explicitly: from ANNarchy import * from ANNarchy.extensions.tensorboard import Logger For detailed examples of how to use the extension, refer to the examples Basal Ganglia and Bayesian optimization , which are available as notebooks in the folder examples/tensorboard .","title":"Logging with tensorboard"},{"location":"manual/Logging.html#creating-the-logger","text":"The Logger class has to be closed properly at the end of the script, so it is advised to use a context: with Logger () as logger : logger . add_scalar ( \"Accuracy\" , acc , trial ) You can also make sure to close it: logger = Logger () logger . add_scalar ( \"Accuracy\" , acc , trial ) logger . close () By default, the logs will be written in a subfolder of ./runs/ (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. ./runs/Apr22_12-11-22_machine . You can control these two elements by passing arguments to Logger() : with Logger ( logdir = \"/tmp/annarchy\" , experiment = \"trial1\" ): # logs in /tmp/annarchy/trial1","title":"Creating the logger"},{"location":"manual/Logging.html#launching-tensorboard","text":"After (or while) logging data within your simulation, run tensorboard in the terminal by specifying the path to the log directory: tensorboard --logdir runs You will then be asked to open localhost:6006 in your browser and will see a page similar to this: The information logged will be available in the different tabs (scalars, images...). You can also visualize and compare several experiments at the same time.","title":"Launching tensorboard"},{"location":"manual/Logging.html#logging-scalars","text":"The add_* methods allow you to log various structures, such as scalars, images, histograms, figures, etc. The simplest information to log is a scalar, for example the accuracy at the end of a trial: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) accuracy = ... logger . add_scalar ( \"Accuracy\" , accuracy , trial ) A tag should be given for each plot as the first argument. In the example above, the figure with the accuracy will be labelled \\\"Accuracy\\\" in tensorboard. You can also group plots together with 2-levels tags such as: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) train_accuracy = ... test_accuracy = ... logger . add_scalar ( \"Accuracy/Train\" , train_accuracy , trial ) logger . add_scalar ( \"Accuracy/Test\" , test_accuracy , trial ) The second argument is the scalar, obviously. The third is the index of x-axis of the plot. It can be the index of the trial, the current time or whatever you prefer. If you want to display several scalars on the same plot, you can use the method add_scalars() and provide a dictionary: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) train_accuracy = ... test_accuracy = ... logger . add_scalars ( \"Accuracy\" , { 'train' : train_accuracy , 'test' : test_accuracy }, trial )","title":"Logging scalars"},{"location":"manual/Logging.html#logging-images","text":"It is also possible to log images, for example an input image or the firing rate of a 2D population, with the add_image() method: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) img = pop . r . reshape (( 10 , 10 )) logger . add_image ( \"Population/Firing rate\" , img , trial ) The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values must be floats between 0 and 1 or integers between 0 and 255 in order to be displayed correctly. You can either do it yourself, or pass equalize=True to the add_image() : logger . add_image ( \"Population/Firing rate\" , img , trial , equalize = True ) The min/max values in the array are internally used to rescale the image: img = ( img - img . min ()) / ( img . max () - img . min ()) To display several images together, for example the receptive fields of a population, an array of size (number, height, width) or (number, height, width, 3) can be passed to add_images() , where number is the number of images to display: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . reshape ( 100 , 10 , 10 ) # 100 post neurons, 10*10 pre neurons logger . add_images ( \"Projection/Receptive fields\" , weights , trial , equalize = True ) equalize=True applies the same scaling to all images, but you additionally pass equalize_per_image=True to have indepent scalings per image.","title":"Logging images"},{"location":"manual/Logging.html#logging-histograms","text":"Histograms can also be logged, for example to visualize the statistics of weights in a projection: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) weights = proj . w . flatten () logger . add_histogram ( \"Weight distribution\" , weights , trial )","title":"Logging histograms"},{"location":"manual/Logging.html#logging-figures","text":"Matplotlib figures can also be logged: with Logger () as logger : for trial in range ( 100 ): simulate ( 1000.0 ) fig = plt . figure () plt . plot ( pop . r ) logger . add_figure ( \"Activity\" , fig , trial ) add_figure() will automatically close the figure, no need to call show() . Beware that this is very slow and requires a lot of space.","title":"Logging figures"},{"location":"manual/Logging.html#logging-parameters","text":"The previous methods can be called multiple times during a simulation, in order to visualize the changes during learning. add_parameters() is more useful in the context of hyperparameter optimization, where the same network with different parameters is run multiple times. Only once per simulation, typically at the end, you can log the value of some important parameters together with some metrics such as accuracy, error rate or so. This will allow tensorboard to display over multiple runs the relation between the parameters and the metrics in the tab \"HPARAMS\": with Logger () as logger : # ... logger . add_parameters ({ 'learning_rate' : lr , 'tau' : tau }, { 'accuracy' : accuracy }) Refer to Bayesian optimization for an example using the hyperopt library.","title":"Logging parameters"},{"location":"manual/Network.html","text":"Parallel simulations and networks # A typical ANNarchy script represents a single network of populations and projections. Most of the work in computational neuroscience consists in running the same network again and again, varying some free parameters each time, until the fit to the data is publishable. The reset() allows to return the network to its state before compilation, but this is particularly tedious to implement. In order to run different networks using the same script, the Network object can be used to make copies of existing objects (populations, projections and monitors) and simulate them in parallel using parallel_run() . Let's suppose the following dummy network is defined: pop1 = PoissonPopulation ( 100 , rates = 10.0 ) pop2 = Population ( 100 , Izhikevich ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_probability ( weights = 5.0 , probability = 0.2 ) m = Monitor ( pop2 , 'spike' ) compile () One would like to compare the firing patterns in pop2 when: There is no input to pop2 . The Poisson input is at 10 Hz. The Poisson input is at 20 Hz. Warning Running multiple networks in parallel is not supported on CUDA. Note parallel_run() uses the multiprocessing module to start parallel processes. On Linux, it should work directly, but there is an issue on OSX. Since Python 3.8, the 'spawn' method is the default way to start processes, but it does not work on MacOS. The following code should fix the issue, but it should only be ran once in the script. import platform if platform . system () == \"Darwin\" : import multiprocessing as mp mp . set_start_method ( 'fork' ) Parallel simulation # parallel_run # The most simple method is to create a single network and to iterate over some parameters values to run identical simulations multiple times using parallel_run() : pop1 = PoissonPopulation ( 100 , rates = 10.0 ) pop2 = Population ( 100 , Izhikevich ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_probability ( weights = 5.0 , probability = 0.2 ) m = Monitor ( pop2 , 'spike' ) compile () def simulation ( idx , net ): net . get ( pop1 ) . rates = 10. * idx net . simulate ( 1000. ) return net . get ( m ) . raster_plot () results = parallel_run ( method = simulation , number = 3 ) t1 , n1 = results [ 0 ] t2 , n2 = results [ 1 ] t3 , n3 = results [ 2 ] The simulation() method will be called over three copies of the network (in different processes). The first argument to this method must be an integer corresponding to the index of a network (here idx = [0, 1, 2] ), the second must be a Network object (class Network ../API/Network.md)). Populations, projections and monitors of a network must be accessed with: net . get ( pop1 ) net . get ( pop2 ) net . get ( proj ) net . get ( m ) Networks only work on copies of the corresponding objects at the time they are added to the network. It is no use to modify the rates parameter of pop1 after the network are created: the network is now independent. Only net.get(pop1).rates allows to change rates for the current simulation. Similarly, it is useless to read variables from the original objects when the networks are simulated: they would still have their original values. You do not have access on the internally-created networks after the simulation (they are in a separate memory space). The method must the data you want to analyze (here the raster plot) or write them to disk (in separate files). parallel_run() returns a list of the values returned by the passed method: results = parallel_run ( method = simulation , networks = [ net1 , net2 , net3 ]) t1 , n1 = results [ 0 ] t2 , n2 = results [ 1 ] t3 , n3 = results [ 2 ] If you initialize some variables randomly in the main network, for example: pop2 . v = - 60. + 10. * np . random . random ( 100 ) they will have the same value in all networks, they are not drawn again. You need to perform random initialization on each network inside the simulation method: def simulation ( idx , net ): net . get ( pop1 ) . rates = 10. * idx net . get ( pop2 ) . v = - 60. + 10. * np . random . random ( 100 ) net . simulate ( 1000. ) return net . get ( m ) . raster_plot () Oppositely, connection methods having a random components (e.g. connect_fixed_probability() or using weights=Uniform(0.0, 1.0) ) will be redrawn for each network. Global simulation methods should not be called directly, even with the net_id parameter. The Network class overrides them: net . step () net . simulate () net . simulate_until () net . reset () net . get_time () net . set_time ( t ) net . get_current_step () net . set_current_step ( t ) net . set_seed ( seed ) net . enable_learning () net . disable_learning () net . get_population ( name ) Passing additional arguments # The two first obligatory arguments of the simulation callback are idx , the index of the network in the simulation, and net , the network object. You can of course use other names, but these two arguments will be passed. idx can be used for example to access arrays of parameter values: rates = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 ] def simulation ( idx , net ): net . get ( pop1 ) . rates = rates [ idx ] ... results = parallel_run ( method = simulation , number = len ( rates )) Another option is to provide additional arguments to the simulation callback during the parallel_run() call: def simulation ( idx , net , rates ): net . get ( pop1 ) . rates = rates ... rates = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 ] results = parallel_run ( method = simulation , number = len ( rates ), rates = rates ) These additional arguments must be lists of the same size as the number of networks ( number or len(networks) ). You can use as many additional arguments as you want: def simulation ( idx , net , a , b , c , d ): ... results = parallel_run ( method = simulation , number = 10 , a =... , b =... , c =... , d =... ) In parallel_run() , the arguments can be passed in any order, but they must be named (e.g. , a=list(range(0)), , not , list(range(10)), ). Multiple network instances # One can also create three different Network objects to implement the three conditions: net1 = Network () net1 . add ([ pop2 , m ]) net1 . compile () The network is created empty, and the population pop2 as well as the attached monitor are added to it through the add() method. This method takes a list of objects (populations, projections and monitors). The network has then to be compiled by calling the compile() method specifically on the network. The network can be simulated independently by calling simulate() or simulate_until() on the network. The basic network, with inputs at 10 Hz, can be simulated directly using the normal methods, or copied into a new network: net2 = Network () net2 . add ([ pop1 , pop2 , proj , m ]) net2 . compile () Here, all defined objects are added to the network. It would be easier to pass the everything argument of the Network constructor as True , which has the same effect. We can use this for the third network: net3 = Network ( everything = True ) net3 . get ( pop1 ) . rates = 20.0 net3 . compile () Here, the population pop1 of the third network has to be accessed though the get() method. The data corresponding to pop1 will not be the same as for net3.get(pop1) , only the geometry and neuron models are the same. Once a network is compiled, it can be simulated (but it does not matter if the other networks are also compiled, including the \\\"original\\\" network): net1 . simulate ( 1000. ) net2 . simulate ( 1000. ) net3 . simulate ( 1000. ) Spike recordings have to be accessed per network, through the copies of the monitor m : t1 , n1 = net1 . get ( m ) . raster_plot () t2 , n2 = net2 . get ( m ) . raster_plot () t3 , n3 = net3 . get ( m ) . raster_plot () One can also call the parallel_run() method and pass it a list of networks instead of number : parallel_run ( method = simulation , networks = [ net1 , net2 , net3 ]) This will apply simulation() in parallel on the 3 networks, reducing the total computation time.","title":"Parallel simulations and networks"},{"location":"manual/Network.html#parallel-simulations-and-networks","text":"A typical ANNarchy script represents a single network of populations and projections. Most of the work in computational neuroscience consists in running the same network again and again, varying some free parameters each time, until the fit to the data is publishable. The reset() allows to return the network to its state before compilation, but this is particularly tedious to implement. In order to run different networks using the same script, the Network object can be used to make copies of existing objects (populations, projections and monitors) and simulate them in parallel using parallel_run() . Let's suppose the following dummy network is defined: pop1 = PoissonPopulation ( 100 , rates = 10.0 ) pop2 = Population ( 100 , Izhikevich ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_probability ( weights = 5.0 , probability = 0.2 ) m = Monitor ( pop2 , 'spike' ) compile () One would like to compare the firing patterns in pop2 when: There is no input to pop2 . The Poisson input is at 10 Hz. The Poisson input is at 20 Hz. Warning Running multiple networks in parallel is not supported on CUDA. Note parallel_run() uses the multiprocessing module to start parallel processes. On Linux, it should work directly, but there is an issue on OSX. Since Python 3.8, the 'spawn' method is the default way to start processes, but it does not work on MacOS. The following code should fix the issue, but it should only be ran once in the script. import platform if platform . system () == \"Darwin\" : import multiprocessing as mp mp . set_start_method ( 'fork' )","title":"Parallel simulations and networks"},{"location":"manual/Network.html#parallel-simulation","text":"","title":"Parallel simulation"},{"location":"manual/Network.html#parallel_run","text":"The most simple method is to create a single network and to iterate over some parameters values to run identical simulations multiple times using parallel_run() : pop1 = PoissonPopulation ( 100 , rates = 10.0 ) pop2 = Population ( 100 , Izhikevich ) proj = Projection ( pop1 , pop2 , 'exc' ) proj . connect_fixed_probability ( weights = 5.0 , probability = 0.2 ) m = Monitor ( pop2 , 'spike' ) compile () def simulation ( idx , net ): net . get ( pop1 ) . rates = 10. * idx net . simulate ( 1000. ) return net . get ( m ) . raster_plot () results = parallel_run ( method = simulation , number = 3 ) t1 , n1 = results [ 0 ] t2 , n2 = results [ 1 ] t3 , n3 = results [ 2 ] The simulation() method will be called over three copies of the network (in different processes). The first argument to this method must be an integer corresponding to the index of a network (here idx = [0, 1, 2] ), the second must be a Network object (class Network ../API/Network.md)). Populations, projections and monitors of a network must be accessed with: net . get ( pop1 ) net . get ( pop2 ) net . get ( proj ) net . get ( m ) Networks only work on copies of the corresponding objects at the time they are added to the network. It is no use to modify the rates parameter of pop1 after the network are created: the network is now independent. Only net.get(pop1).rates allows to change rates for the current simulation. Similarly, it is useless to read variables from the original objects when the networks are simulated: they would still have their original values. You do not have access on the internally-created networks after the simulation (they are in a separate memory space). The method must the data you want to analyze (here the raster plot) or write them to disk (in separate files). parallel_run() returns a list of the values returned by the passed method: results = parallel_run ( method = simulation , networks = [ net1 , net2 , net3 ]) t1 , n1 = results [ 0 ] t2 , n2 = results [ 1 ] t3 , n3 = results [ 2 ] If you initialize some variables randomly in the main network, for example: pop2 . v = - 60. + 10. * np . random . random ( 100 ) they will have the same value in all networks, they are not drawn again. You need to perform random initialization on each network inside the simulation method: def simulation ( idx , net ): net . get ( pop1 ) . rates = 10. * idx net . get ( pop2 ) . v = - 60. + 10. * np . random . random ( 100 ) net . simulate ( 1000. ) return net . get ( m ) . raster_plot () Oppositely, connection methods having a random components (e.g. connect_fixed_probability() or using weights=Uniform(0.0, 1.0) ) will be redrawn for each network. Global simulation methods should not be called directly, even with the net_id parameter. The Network class overrides them: net . step () net . simulate () net . simulate_until () net . reset () net . get_time () net . set_time ( t ) net . get_current_step () net . set_current_step ( t ) net . set_seed ( seed ) net . enable_learning () net . disable_learning () net . get_population ( name )","title":"parallel_run"},{"location":"manual/Network.html#passing-additional-arguments","text":"The two first obligatory arguments of the simulation callback are idx , the index of the network in the simulation, and net , the network object. You can of course use other names, but these two arguments will be passed. idx can be used for example to access arrays of parameter values: rates = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 ] def simulation ( idx , net ): net . get ( pop1 ) . rates = rates [ idx ] ... results = parallel_run ( method = simulation , number = len ( rates )) Another option is to provide additional arguments to the simulation callback during the parallel_run() call: def simulation ( idx , net , rates ): net . get ( pop1 ) . rates = rates ... rates = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 ] results = parallel_run ( method = simulation , number = len ( rates ), rates = rates ) These additional arguments must be lists of the same size as the number of networks ( number or len(networks) ). You can use as many additional arguments as you want: def simulation ( idx , net , a , b , c , d ): ... results = parallel_run ( method = simulation , number = 10 , a =... , b =... , c =... , d =... ) In parallel_run() , the arguments can be passed in any order, but they must be named (e.g. , a=list(range(0)), , not , list(range(10)), ).","title":"Passing additional arguments"},{"location":"manual/Network.html#multiple-network-instances","text":"One can also create three different Network objects to implement the three conditions: net1 = Network () net1 . add ([ pop2 , m ]) net1 . compile () The network is created empty, and the population pop2 as well as the attached monitor are added to it through the add() method. This method takes a list of objects (populations, projections and monitors). The network has then to be compiled by calling the compile() method specifically on the network. The network can be simulated independently by calling simulate() or simulate_until() on the network. The basic network, with inputs at 10 Hz, can be simulated directly using the normal methods, or copied into a new network: net2 = Network () net2 . add ([ pop1 , pop2 , proj , m ]) net2 . compile () Here, all defined objects are added to the network. It would be easier to pass the everything argument of the Network constructor as True , which has the same effect. We can use this for the third network: net3 = Network ( everything = True ) net3 . get ( pop1 ) . rates = 20.0 net3 . compile () Here, the population pop1 of the third network has to be accessed though the get() method. The data corresponding to pop1 will not be the same as for net3.get(pop1) , only the geometry and neuron models are the same. Once a network is compiled, it can be simulated (but it does not matter if the other networks are also compiled, including the \\\"original\\\" network): net1 . simulate ( 1000. ) net2 . simulate ( 1000. ) net3 . simulate ( 1000. ) Spike recordings have to be accessed per network, through the copies of the monitor m : t1 , n1 = net1 . get ( m ) . raster_plot () t2 , n2 = net2 . get ( m ) . raster_plot () t3 , n3 = net3 . get ( m ) . raster_plot () One can also call the parallel_run() method and pass it a list of networks instead of number : parallel_run ( method = simulation , networks = [ net1 , net2 , net3 ]) This will apply simulation() in parallel on the 3 networks, reducing the total computation time.","title":"Multiple network instances"},{"location":"manual/Notebooks.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Jupyter / IPython Notebooks # It is possible to use ANNarchy in Jupyter / IPython notebooks. Several examples are provided in the examples directory of the source distribution. There are nevertheless two things to take into account when re-running cells: ANNarchy uses global variables to store the populations and projections # Internally, there is a network manager that stores which population and projection has been declared. It is empty when starting a script, but it can know anything about the Jupyter cells. Here are the main consequences: If you re-run the line from ANNarchy import * , Python will not clear the stored populations and projections (by design, as ANNarchy is already cached) If you re-run a cell creating a population or projection, it will create a new population, not replace the previous one. If you create a new population / projection after a call to compile() in the current kernel, this will lead to an error, as the network is already compiled and new objects cannot (yet) be added on the fly. The solution to these problems is to call the clear() command right after importing ANNarchy. This deletes all created objects and puts ANNarchy in a \" clean \" state: from ANNarchy import * clear () ANNarchy 4.6 (4.6.9b) on linux (posix). When you change something to the parameters of your network, you can re-run this cell, as well as all cells defining your network. You will then be able to call compile() again. This is equivalent to a reset of your network. However, if the structure of your network changes (new populations, changed equations), this will have no effect because of the second issue. Python cannot dynamically reload modules # If you change something in the definition of the neurons or synapses (anything that usually provoke a recompilation), this will have NO effect in the notebook, even if the network is successfully recompiled. Python is unable to reload libraries dynamically at runtime ( ). The C++ core actually running the simulation is imported as a dynamic library, so the result of the recompilation will not be reloaded. The only solution is to restart the kernel.","title":"Notebooks"},{"location":"manual/Notebooks.html#jupyter-ipython-notebooks","text":"It is possible to use ANNarchy in Jupyter / IPython notebooks. Several examples are provided in the examples directory of the source distribution. There are nevertheless two things to take into account when re-running cells:","title":"Jupyter / IPython Notebooks"},{"location":"manual/Notebooks.html#annarchy-uses-global-variables-to-store-the-populations-and-projections","text":"Internally, there is a network manager that stores which population and projection has been declared. It is empty when starting a script, but it can know anything about the Jupyter cells. Here are the main consequences: If you re-run the line from ANNarchy import * , Python will not clear the stored populations and projections (by design, as ANNarchy is already cached) If you re-run a cell creating a population or projection, it will create a new population, not replace the previous one. If you create a new population / projection after a call to compile() in the current kernel, this will lead to an error, as the network is already compiled and new objects cannot (yet) be added on the fly. The solution to these problems is to call the clear() command right after importing ANNarchy. This deletes all created objects and puts ANNarchy in a \" clean \" state: from ANNarchy import * clear () ANNarchy 4.6 (4.6.9b) on linux (posix). When you change something to the parameters of your network, you can re-run this cell, as well as all cells defining your network. You will then be able to call compile() again. This is equivalent to a reset of your network. However, if the structure of your network changes (new populations, changed equations), this will have no effect because of the second issue.","title":"ANNarchy uses global variables to store the populations and projections"},{"location":"manual/Notebooks.html#python-cannot-dynamically-reload-modules","text":"If you change something in the definition of the neurons or synapses (anything that usually provoke a recompilation), this will have NO effect in the notebook, even if the network is successfully recompiled. Python is unable to reload libraries dynamically at runtime ( ). The C++ core actually running the simulation is imported as a dynamic library, so the result of the recompilation will not be reloaded. The only solution is to restart the kernel.","title":"Python cannot dynamically reload modules"},{"location":"manual/NumericalMethods.html","text":"Equations and numerical methods # Numerical methods # First-order ordinary differential equations (ODE) can be solved using different numerical methods. The method can be declared globally in the setup() call and used in all ODEs of the network: from ANNarchy import * setup ( method = 'exponential' ) or specified explicitely for each ODE by specifying a flag: equations = \"\"\" tau * dV/dt + V = A : init = 0.0, exponential \"\"\" If nothing is specified, the explicit Euler method will be used. Different numerical methods are available: Explicit Euler 'explicit' Implicit Euler 'implicit' Exponential Euler 'exponential' Midpoint 'midpoint' Fourth-order Runge-Kutta 'rk4' Event-driven 'event-driven' Each method has advantages/drawbacks in term of numerical error, stability and computational cost. To describe these methods, we will take the example of a system of two linear first-order ODEs: \\[\\frac{dx(t)}{dt} = f(x(t), y(t)) = a_x \\cdot x(t) + b_x \\cdot y(t) + c_x\\] \\[\\frac{dy(t)}{dt} = g(x(t), y(t)) = a_y \\cdot x(t) + b_y \\cdot y(t) + c_y\\] The objective of a numerical method is to approximate the value of \\(x\\) and \\(y\\) at time \\(t+h\\) based on its value at time \\(t\\) , where \\(h\\) is the discretization time step (noted dt in ANNarchy): \\[x(t + h) = F(x(t), y(t))\\] \\[y(t + h) = G(x(t), y(t))\\] At each step of the simulation, the new values for the variables are computed using this update rule and will be used for the following step. The derivative of each variable is usually approximated by: \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h}\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h}\\] The different numerical methods mostly differ in the time at which the functions \\(f\\) and \\(g\\) are evaluated. Explicit Euler method # The explicit (forward) Euler method computes the next value for the variables by estimating their derivative at time \\(t\\) : \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t), y(t))\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t), y(t))\\] so the solution is straightforward to obtain: \\[x(t+h) = x(t) + h \\cdot f(x(t), y(t))\\] \\[y(t+h) = y(t) + h \\cdot g(x(t), y(t))\\] Implicit Euler method # The implicit (backward) Euler method computes the next value for the variables by estimating their derivative at time \\(t + h\\) : \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t+h), y(t+h))\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t+h), y(t+h))\\] This leads to a system of equations which must be solved in order to find the update rule. With the linear equations defined above, we need to solve: \\[\\frac{x(t+h) - x(t)}{h} = a_x \\cdot x(t + h) + b_x \\cdot y(t + h) + c_x\\] \\[\\frac{y(t+h) - y(t)}{h} = a_y \\cdot x(t + h) + b_y \\cdot y(t + h) + c_y\\] what gives something like: \\[x(t+h) = x(t) - h \\cdot \\frac{ \\left(a_{x} x(t) + b_{x} y(t) + c_{x} + h \\left(- a_{x} b_{y} x(t) + a_{y} b_{x} x(t) + b_{x} c_{y} - b_{y} c_{x}\\right)\\right)}{h^{2} \\left(- a_{x} b_{y} + a_{y} b_{x}\\right) + h \\left(a_{x} + b_{y}\\right) - 1}\\] \\[y(t+h) = y(t) -h \\cdot \\frac{ a_{y} \\left(c_{x} h + x(t)\\right) + y(t) \\left(- a_{y} b_{x} h^{2} + \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)\\right) + \\left(a_{x} h - 1\\right) \\left(c_{y} h + y(t)\\right)}{a_{y} b_{x} h^{2} - \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)}\\] ANNarchy relies on Sympy to solve and simplify this system of equations and generate the update rule. Note This method is obviously much more computationally expensive than the explicit Euler method, although more stable. The midpoint method is a better trade-off between complexity and stability than the implicit Euler method. Exponential Euler # The exponential Euler method is particularly stable for single first-order linear equations, of the type: \\[\\tau(t) \\cdot \\frac{dx(t)}{dt} + x(t) = A(t)\\] The update rule is then given by: \\[x(t+h) = x(t) + (1 - \\exp(- \\frac{h}{\\tau(t)}) ) \\cdot (A(t) - x(t))\\] The difference with the explicit Euler method is the step size, which is an exponential function of the ratio \\(\\frac{\\tau}{h}\\) . The accurary of the exponential Euler method on linear first-order ODEs is close to perfect, compared to the other Euler methods. As it is an explicit method, systems of equations are solved very easily with the same rule. When the exponential method is used, ANNarchy first tries to reduce the ODE to its canonical form above (with the time constant being possibly dependent on time or inputs) and then generates the update rule accordingly. For example, the description: tau * dv/dt = (E - v) + g_exc * (Ee - v) + g_inh * (v - Ei) would be first transformed in: (1 + g_exc - g_inh) * dv/dt + v = (E + g_exc * Ee - g_inh * Ei) / (1 + g_exc - g_inh) before being transformed into an update rule, with \\(\\tau(t) = 1 + g_\\text{exc} - g_\\text{inh}\\) : \\[v(t+h) = v(t) + (1 - \\exp(- \\frac{h}{1 + g_\\text{exc} - g_\\text{inh}}) ) \\cdot (\\frac{E + g_\\text{exc} \\cdot E_e - g_\\text{inh} \\cdot E_i}{1 + g_\\text{exc} - g_\\text{inh}} - v(t))\\] The exponential method can only be applied to first-order linear ODEs. Any other form of ODE will be rejected by the parser. Important note: The step size \\(1 - \\exp(- \\frac{h}{\\tau(t)})\\) is computationally expensive because of the exponential function. If the time constant \\(\\tau\\) is a global parameter of the population or projection, ANNarchy can pre-compute the step size outside of the for loop over all neurons/synapses, which leads to huge increases in performance. The exponential method should therefore be reserved to first-order linear ODEs with the same time constant for all neurons/synapses: neuron = Neuron ( parameters = \"tau = 10. : population\" , equations = \"tau * dr/dt + r = sum(exc) : min=0.0, exponential\" ) Midpoint # The midpoint method is a Runge-Kutta method of order 2. It estimates the derivative in the middle of the interval \\(t + \\frac{h}{2}\\) . \\[k_x = f(x(t), y(t))\\] \\[k_y = g(x(t), y(t))\\] \\[x(t+h) = x(t) + h \\cdot f(x(t) + k_x \\cdot \\frac{h}{2}, y(t) + k_y \\cdot \\frac{h}{2})\\] \\[y(t+h) = y(t) + h \\cdot g(x(t) + k_x \\cdot \\frac{h}{2}, y(t) + k_y \\cdot \\frac{h}{2})\\] Runge-Kutta 4 # The fourth-order Runge-Kutta method estimates the derivative at four different points and combines them: \\[ k_1 = f(x(t)) \\] \\[ k_2 = f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_1) \\] \\[ k_3 = f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_2) \\] \\[ k_4 = f(x(t + h) + h \\cdot k_3) \\] \\[ x(t+h) = x(t) + \\frac{h}{6} \\cdot (k_1 + 2 \\cdot k_2 + 2 \\cdot k_3 + k_4 ) \\] Event-driven # Event-driven integration is only available for spiking synapses with variables following linear first-order dynamics. Let's consider the following STDP synapse (see Spiking Synapse for explanations): STDP = Synapse ( parameters = \"\"\" tau_pre = 10.0 : postsynaptic tau_post = 10.0 : postsynaptic \"\"\" , equations = \"\"\" tau_pre * dApre/dt = - Apre : event-driven tau_post * dApost/dt = - Apost : event-driven \"\"\" , pre_spike = \"\"\" g_target += w Apre += cApre w = clip(w + Apost, 0.0 , 1.0) \"\"\" , post_spike = \"\"\" Apost += cApost w = clip(w + Apre, 0.0 , 1.0) \"\"\" ) The value of Apost and Apre is only needed when a pre- or post-synaptic spike occurs at the synapse, so there is no need to integrate the corresponding equations between two such events. First-order linear ODEs have the nice property that their analytical solution is easy to obtain. Let's consider an equation of the form: \\[\\tau \\frac{dv}{dt} = E - v\\] If \\(v\\) has the value \\(V_0\\) at time \\(t\\) , its value at time \\(t + \\Delta t\\) is given by: \\[v(t + \\Delta t) = V_0 \\cdot \\exp(-\\frac{\\Delta t}{\\tau})\\] Note If the synapse defines a psp argument (synaptic transmission is continuous), or if another continuous variable depends on the value of an event-driven one, it is not possible to use event-driven integration. Order of evaluation # The values of variables are stored in a single array in order to save some memory. Special care therefore has to be taken on whether the update of a variable depends on the value of another variable at the previous time step or in the same step. Systems of ODEs # Systems of ODEs are integrated concurrently, which means that the following system: tau*dv/dt = I - v - u tau*du/dt = v - u would be numerized using the explicit Euler method as: v[t+1] = v[t] + dt*(I - v[t] - u[t])/tau u[t+1] = u[t] + dt*(v[t] - u[t])/tau As we use a single array, the generated code is similar to: new_v = v + dt*(I - v - u)/tau new_u = u + dt*(v - u)/tau v = new_v u = new_u This way, we ensure that the interdependent ODEs use the correct value for the other variables. Assignments # When assignments ( = , += ...) are used in an equations field, the order of valuation is different: Assigments occurring before or after a system of ODEs are updated sequentially. Systems of ODEs are updated concurrently. Let's consider the following dummy equations: # Process the inputs Exc = some_function ( sum ( exc )) Inh = another_function ( sum ( inh )) I = Exc - Inh # ODE for the membrane potential, with a recovery variable tau * dv / dt = I - v - u tau * du / dt = v - u # Firing rate is the positive part of v r = pos ( v ) Here, Exc and Inh represent the inputs to the neuron at the current time t . The new values should be immediately available for updating I , whose value should similarly be immediately used in the ODE of v . Similarly, the value of r should be the positive part of the value of v that was just calculated, not at the previous time step. Doing otherwise would introduce a lag in the neuron: changes in sum(exc) at t would be reflected in Exc at t+1 , in I at t+2 , in v at t+3 and finally in r at t+4 . This is generally unwanted. The generated code is therefore equivalent to: # Process the inputs Exc = some_function ( sum ( exc )) Inh = another_function ( sum ( inh )) I = Exc - Inh # ODE for the membrane potential, with a recovery variable new_v = v + dt * ( I - v - u ) / tau new_u = u + dt * ( v - u ) / tau v = new_v u = new_u # Firing rate is the positive part of v r = pos ( v ) One can even define multiple groups of assignments and systems of ODEs: systems of ODEs separated by at least one assignment will be evaluated sequentially (but concurrently inside each system). For example, in: tau*du/dt = v - u I = g_exc - g_inh tau*dk/dt = v - k tau*dv/dt = I - v - u + k u and k are updated using the previous value of v , while v uses the new values of both I and u , but the previous one of k .","title":"Equations and numerical methods"},{"location":"manual/NumericalMethods.html#equations-and-numerical-methods","text":"","title":"Equations and numerical methods"},{"location":"manual/NumericalMethods.html#numerical-methods","text":"First-order ordinary differential equations (ODE) can be solved using different numerical methods. The method can be declared globally in the setup() call and used in all ODEs of the network: from ANNarchy import * setup ( method = 'exponential' ) or specified explicitely for each ODE by specifying a flag: equations = \"\"\" tau * dV/dt + V = A : init = 0.0, exponential \"\"\" If nothing is specified, the explicit Euler method will be used. Different numerical methods are available: Explicit Euler 'explicit' Implicit Euler 'implicit' Exponential Euler 'exponential' Midpoint 'midpoint' Fourth-order Runge-Kutta 'rk4' Event-driven 'event-driven' Each method has advantages/drawbacks in term of numerical error, stability and computational cost. To describe these methods, we will take the example of a system of two linear first-order ODEs: \\[\\frac{dx(t)}{dt} = f(x(t), y(t)) = a_x \\cdot x(t) + b_x \\cdot y(t) + c_x\\] \\[\\frac{dy(t)}{dt} = g(x(t), y(t)) = a_y \\cdot x(t) + b_y \\cdot y(t) + c_y\\] The objective of a numerical method is to approximate the value of \\(x\\) and \\(y\\) at time \\(t+h\\) based on its value at time \\(t\\) , where \\(h\\) is the discretization time step (noted dt in ANNarchy): \\[x(t + h) = F(x(t), y(t))\\] \\[y(t + h) = G(x(t), y(t))\\] At each step of the simulation, the new values for the variables are computed using this update rule and will be used for the following step. The derivative of each variable is usually approximated by: \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h}\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h}\\] The different numerical methods mostly differ in the time at which the functions \\(f\\) and \\(g\\) are evaluated.","title":"Numerical methods"},{"location":"manual/NumericalMethods.html#explicit-euler-method","text":"The explicit (forward) Euler method computes the next value for the variables by estimating their derivative at time \\(t\\) : \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t), y(t))\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t), y(t))\\] so the solution is straightforward to obtain: \\[x(t+h) = x(t) + h \\cdot f(x(t), y(t))\\] \\[y(t+h) = y(t) + h \\cdot g(x(t), y(t))\\]","title":"Explicit Euler method"},{"location":"manual/NumericalMethods.html#implicit-euler-method","text":"The implicit (backward) Euler method computes the next value for the variables by estimating their derivative at time \\(t + h\\) : \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t+h), y(t+h))\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t+h), y(t+h))\\] This leads to a system of equations which must be solved in order to find the update rule. With the linear equations defined above, we need to solve: \\[\\frac{x(t+h) - x(t)}{h} = a_x \\cdot x(t + h) + b_x \\cdot y(t + h) + c_x\\] \\[\\frac{y(t+h) - y(t)}{h} = a_y \\cdot x(t + h) + b_y \\cdot y(t + h) + c_y\\] what gives something like: \\[x(t+h) = x(t) - h \\cdot \\frac{ \\left(a_{x} x(t) + b_{x} y(t) + c_{x} + h \\left(- a_{x} b_{y} x(t) + a_{y} b_{x} x(t) + b_{x} c_{y} - b_{y} c_{x}\\right)\\right)}{h^{2} \\left(- a_{x} b_{y} + a_{y} b_{x}\\right) + h \\left(a_{x} + b_{y}\\right) - 1}\\] \\[y(t+h) = y(t) -h \\cdot \\frac{ a_{y} \\left(c_{x} h + x(t)\\right) + y(t) \\left(- a_{y} b_{x} h^{2} + \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)\\right) + \\left(a_{x} h - 1\\right) \\left(c_{y} h + y(t)\\right)}{a_{y} b_{x} h^{2} - \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)}\\] ANNarchy relies on Sympy to solve and simplify this system of equations and generate the update rule. Note This method is obviously much more computationally expensive than the explicit Euler method, although more stable. The midpoint method is a better trade-off between complexity and stability than the implicit Euler method.","title":"Implicit Euler method"},{"location":"manual/NumericalMethods.html#exponential-euler","text":"The exponential Euler method is particularly stable for single first-order linear equations, of the type: \\[\\tau(t) \\cdot \\frac{dx(t)}{dt} + x(t) = A(t)\\] The update rule is then given by: \\[x(t+h) = x(t) + (1 - \\exp(- \\frac{h}{\\tau(t)}) ) \\cdot (A(t) - x(t))\\] The difference with the explicit Euler method is the step size, which is an exponential function of the ratio \\(\\frac{\\tau}{h}\\) . The accurary of the exponential Euler method on linear first-order ODEs is close to perfect, compared to the other Euler methods. As it is an explicit method, systems of equations are solved very easily with the same rule. When the exponential method is used, ANNarchy first tries to reduce the ODE to its canonical form above (with the time constant being possibly dependent on time or inputs) and then generates the update rule accordingly. For example, the description: tau * dv/dt = (E - v) + g_exc * (Ee - v) + g_inh * (v - Ei) would be first transformed in: (1 + g_exc - g_inh) * dv/dt + v = (E + g_exc * Ee - g_inh * Ei) / (1 + g_exc - g_inh) before being transformed into an update rule, with \\(\\tau(t) = 1 + g_\\text{exc} - g_\\text{inh}\\) : \\[v(t+h) = v(t) + (1 - \\exp(- \\frac{h}{1 + g_\\text{exc} - g_\\text{inh}}) ) \\cdot (\\frac{E + g_\\text{exc} \\cdot E_e - g_\\text{inh} \\cdot E_i}{1 + g_\\text{exc} - g_\\text{inh}} - v(t))\\] The exponential method can only be applied to first-order linear ODEs. Any other form of ODE will be rejected by the parser. Important note: The step size \\(1 - \\exp(- \\frac{h}{\\tau(t)})\\) is computationally expensive because of the exponential function. If the time constant \\(\\tau\\) is a global parameter of the population or projection, ANNarchy can pre-compute the step size outside of the for loop over all neurons/synapses, which leads to huge increases in performance. The exponential method should therefore be reserved to first-order linear ODEs with the same time constant for all neurons/synapses: neuron = Neuron ( parameters = \"tau = 10. : population\" , equations = \"tau * dr/dt + r = sum(exc) : min=0.0, exponential\" )","title":"Exponential Euler"},{"location":"manual/NumericalMethods.html#midpoint","text":"The midpoint method is a Runge-Kutta method of order 2. It estimates the derivative in the middle of the interval \\(t + \\frac{h}{2}\\) . \\[k_x = f(x(t), y(t))\\] \\[k_y = g(x(t), y(t))\\] \\[x(t+h) = x(t) + h \\cdot f(x(t) + k_x \\cdot \\frac{h}{2}, y(t) + k_y \\cdot \\frac{h}{2})\\] \\[y(t+h) = y(t) + h \\cdot g(x(t) + k_x \\cdot \\frac{h}{2}, y(t) + k_y \\cdot \\frac{h}{2})\\]","title":"Midpoint"},{"location":"manual/NumericalMethods.html#runge-kutta-4","text":"The fourth-order Runge-Kutta method estimates the derivative at four different points and combines them: \\[ k_1 = f(x(t)) \\] \\[ k_2 = f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_1) \\] \\[ k_3 = f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_2) \\] \\[ k_4 = f(x(t + h) + h \\cdot k_3) \\] \\[ x(t+h) = x(t) + \\frac{h}{6} \\cdot (k_1 + 2 \\cdot k_2 + 2 \\cdot k_3 + k_4 ) \\]","title":"Runge-Kutta 4"},{"location":"manual/NumericalMethods.html#event-driven","text":"Event-driven integration is only available for spiking synapses with variables following linear first-order dynamics. Let's consider the following STDP synapse (see Spiking Synapse for explanations): STDP = Synapse ( parameters = \"\"\" tau_pre = 10.0 : postsynaptic tau_post = 10.0 : postsynaptic \"\"\" , equations = \"\"\" tau_pre * dApre/dt = - Apre : event-driven tau_post * dApost/dt = - Apost : event-driven \"\"\" , pre_spike = \"\"\" g_target += w Apre += cApre w = clip(w + Apost, 0.0 , 1.0) \"\"\" , post_spike = \"\"\" Apost += cApost w = clip(w + Apre, 0.0 , 1.0) \"\"\" ) The value of Apost and Apre is only needed when a pre- or post-synaptic spike occurs at the synapse, so there is no need to integrate the corresponding equations between two such events. First-order linear ODEs have the nice property that their analytical solution is easy to obtain. Let's consider an equation of the form: \\[\\tau \\frac{dv}{dt} = E - v\\] If \\(v\\) has the value \\(V_0\\) at time \\(t\\) , its value at time \\(t + \\Delta t\\) is given by: \\[v(t + \\Delta t) = V_0 \\cdot \\exp(-\\frac{\\Delta t}{\\tau})\\] Note If the synapse defines a psp argument (synaptic transmission is continuous), or if another continuous variable depends on the value of an event-driven one, it is not possible to use event-driven integration.","title":"Event-driven"},{"location":"manual/NumericalMethods.html#order-of-evaluation","text":"The values of variables are stored in a single array in order to save some memory. Special care therefore has to be taken on whether the update of a variable depends on the value of another variable at the previous time step or in the same step.","title":"Order of evaluation"},{"location":"manual/NumericalMethods.html#systems-of-odes","text":"Systems of ODEs are integrated concurrently, which means that the following system: tau*dv/dt = I - v - u tau*du/dt = v - u would be numerized using the explicit Euler method as: v[t+1] = v[t] + dt*(I - v[t] - u[t])/tau u[t+1] = u[t] + dt*(v[t] - u[t])/tau As we use a single array, the generated code is similar to: new_v = v + dt*(I - v - u)/tau new_u = u + dt*(v - u)/tau v = new_v u = new_u This way, we ensure that the interdependent ODEs use the correct value for the other variables.","title":"Systems of ODEs"},{"location":"manual/NumericalMethods.html#assignments","text":"When assignments ( = , += ...) are used in an equations field, the order of valuation is different: Assigments occurring before or after a system of ODEs are updated sequentially. Systems of ODEs are updated concurrently. Let's consider the following dummy equations: # Process the inputs Exc = some_function ( sum ( exc )) Inh = another_function ( sum ( inh )) I = Exc - Inh # ODE for the membrane potential, with a recovery variable tau * dv / dt = I - v - u tau * du / dt = v - u # Firing rate is the positive part of v r = pos ( v ) Here, Exc and Inh represent the inputs to the neuron at the current time t . The new values should be immediately available for updating I , whose value should similarly be immediately used in the ODE of v . Similarly, the value of r should be the positive part of the value of v that was just calculated, not at the previous time step. Doing otherwise would introduce a lag in the neuron: changes in sum(exc) at t would be reflected in Exc at t+1 , in I at t+2 , in v at t+3 and finally in r at t+4 . This is generally unwanted. The generated code is therefore equivalent to: # Process the inputs Exc = some_function ( sum ( exc )) Inh = another_function ( sum ( inh )) I = Exc - Inh # ODE for the membrane potential, with a recovery variable new_v = v + dt * ( I - v - u ) / tau new_u = u + dt * ( v - u ) / tau v = new_v u = new_u # Firing rate is the positive part of v r = pos ( v ) One can even define multiple groups of assignments and systems of ODEs: systems of ODEs separated by at least one assignment will be evaluated sequentially (but concurrently inside each system). For example, in: tau*du/dt = v - u I = g_exc - g_inh tau*dk/dt = v - k tau*dv/dt = I - v - u + k u and k are updated using the previous value of v , while v uses the new values of both I and u , but the previous one of k .","title":"Assignments"},{"location":"manual/Parser.html","text":"Parser # A Neuron or Synapse type is primarily defined by two sets of values which must be specified in its constructor: Parameters are values such as time constants which are constant during the simulation. They can be the same throughout the population/projection, or take different values. Variables are neuronal variables (for example the membrane potential or firing rate) or synaptic variables (the synaptic efficiency) whose value evolve with time during the simulation. The equation (whether it is an ordinary differential equation or not) ruling their evolution can be described using a specific meta-language. Parameters # Parameters are defined by a multi-string consisting of one or more parameter definitions: parameters = \"\"\" tau = 10.0 eta = 0.5 \"\"\" Each parameter should be defined on a single line, with its name on the left side of the equal sign, and its value on the right side. The given value corresponds to the initial value of the parameter (but it can be changed at any further point of the simulation). As a neuron/synapse type is likely to be reused in different populations/projections, it is good practice to set reasonable initial values in the neuron/synapse type, and eventually adapt them to the corresponding populations/projections later on. Local vs. global parameters By default, a neural parameter will be unique to each neuron (i.e. each neuron instance will hold a copy of the parameter) or synapse. In order to save memory space, one can force ANNarchy to store only one parameter value for a whole population by specifying the population flag after a : symbol following the parameter definition: parameters = \"\"\" tau = 10.0 eta = 0.5 : population \"\"\" In this case, there will be only only one instance of the eta parameter for the whole population. eta is called a global parameter, in opposition to local parameters which are the default. The same is true for synapses, whose parameters are by default unique to each synapse in a given projection. If the post-synaptic flag is passed, the parameter will be common to all synapses of a post-synaptic neuron, but can differ from one post-synaptic neuron to another. If the projection flag is passed, the parameter will be common to all synapses of a projection (e.g. the learning rate). Type of the variable Parameters have floating-point precision by default. If you want to force the parameter to be an integer or boolean, you can also pass the int and bool flags, separated by commas: parameters = \"\"\" tau = 10.0 eta = 1 : population, int \"\"\" Constants Alternatively, it is possible to use constants in the parameter definition (see later): tau_exc = Constant ( 'tau_exc' , 10.0 ) neuron = Neuron ( parameters = \"\"\" tau = tau_exc \"\"\" , ) The advantage of this method is that if a parameter value is \\\"shared\\\" across several neuron/synapse types, you only need to change the value once, instead of in each neuron/synapse definition. Variables # Time-varying variables are also defined using a multi-line description: equations = \"\"\" noise = Uniform(0.0, 0.2) tau * dmp/dt + mp = baseline + sum(exc) + noise r = pos(mp) \"\"\" The evolution of each variable with time can be described through a simple equation or an ordinary differential equation (ODE). ANNarchy provides a simple parser for mathematical expressions, whose role is to translate a high-level description of the equation into an optimized C++ code snippet. The equation for one variable can depend on parameters, other variables (even when declared later) or constants. Variables are updated in the same order as their declaration in the multistring (see Numerical methods , as it influences how ODEs are solved). The declaration of a single variable can extend on multiple lines: equations = \"\"\" noise = Uniform(0.0, 0.2) tau * dmp/dt = baseline - mp + sum(exc) + noise : max = 1.0 rate = pos(mp) \"\"\" As it is only a parser and not a solver, some limitations exist: Simple equations must hold only the name of the variable on the left sign of the equation. Variable definitions such as rate + mp = noise are forbidden, as it would be impossible to guess which variable should be updated. ODEs are more free regarding the left side, but only one variable should hold the gradient: the one which will be updated. The following definitions are equivalent and will lead to the same C++ code: tau * dmp / dt = baseline - mp tau * dmp / dt + mp = baseline tau * dmp / dt + mp - baseline = 0 dmp / dt = ( baseline - mp ) / tau In practice, ODEs are transformed using Sympy into the last form (only the gradient stays on the left) and numerized using the chosen numerical method (see Numerical methods ). Flags # Locality and type Like the parameters, variables also accept the population , postsynaptic and projection flags to define the local/global character of the variable, as well as the int or bool flags for their type. Initial value The initial value of the variable (before the first simulation starts) can also be specified using the init keyword followed by the desired value: equations = \"\"\" tau * dmp/dt + mp = baseline : init = 0.2 \"\"\" It must be a single value (the same for all neurons in the population or all synapses in the projection) and should not depend on other parameters and variables. This initial value can be specifically changed after the Population or Projection objects are created (see Populations ). It is also possible to use constants for the initial value: init_mp = Constant ( 'init_mp' , 0.2 ) neuron = Neuron ( equations = \"\"\" tau * dmp/dt + mp = baseline : init = init_mp \"\"\" , ) Min and Max values of a variable Upper- and lower-bounds can be set using the min and max keywords: equations = \"\"\" tau * dmp/dt + mp = baseline : min = -0.2, max = 1.0 \"\"\" At each step of the simulation, after the update rule is calculated for mp , the new value will be compared to the min and max value, and clamped if necessary. min and max can be single values, constants, parameters, variables or functions of all these: parameters = \"\"\" tau = 10.0 min_mp = -1.0 : population max_mp = 1.0 \"\"\" , equations = \"\"\" variance = Uniform(0.0, 1.0) tau * dmp/dt + mp = sum(exc) : min = min_mp, max = max_mp + variance r = mp : min = 0.0 # Equivalent to r = pos(mp) \"\"\" Numerical method The numerization method for a single ODEs can be explicitely set by specifying a flag: tau * dmp/dt + mp = sum(exc) : exponential The available numerical methods are described in Numerical methods . Summary of allowed flags for variables: init : defines the initialization value at begin of simulation and after a network reset (default: 0.0) min : minimum allowed value (unset by default) max : maximum allowed value (unset by default) population : the attribute is shared by all neurons of a population. postsynaptic : the attribute is shared by all synapses of a post-synaptic neuron. projection : the attribute is shared by all synapses of a projection. explicit , implicit , exponential , midpoint , event-driven : the numerical method to be used. Constants # Global constants can be created by the user and used inside any equation. They must define an unique name and a floating point value: tau = Constant ( 'tau' , 10.0 ) neuron = Neuron ( equations = \"tau * dr/dt + r = sum(exc)\" ) In this example, a Neuron or Synapse does not have to define the parameter tau to use it: it is available everywhere. If the Neuron/Synapse redefines a parameter called tau , the constant is not visible anymore to that object. Constants can be manipulated as normal floats to define complex values: tau = Constant ( 'tau' , 20 ) factor = Constant ( 'factor' , 0.1 ) real_tau = Constant ( 'real_tau' , tau * factor ) neuron = Neuron ( equations = ''' real_tau*dr/dt + r =1.0 ''' ) Note that constants are only global, changing their value impacts all objects using them. Changing the value of a constant can only be done through the set() method (before or after compile() ): tau = Constant ( 'tau' , 20 ) tau . set ( 10.0 ) Allowed vocabulary # The mathematical parser relies heavily on the one provided by SymPy . Numerical values # All parameters and variables use implicitly the floating-point double precision, except when stated otherwise with the int or bool keywords. You can use numerical constants within the equation, noting that they will be automatically converted to this precision: tau * dmp / dt = 1 / pos ( mp ) + 1 The constant \\(\\pi\\) is available under the literal form pi . Operators # Additions (+), substractions (-), multiplications (*), divisions (/) and power functions (\\^) are of course allowed. Gradients are allowed only for the variable currently described. They take the form: dmp / dt = A with a d preceding the variable's name and terminated by /dt (with or without spaces). Gradients must be on the left side of the equation. To update the value of a variable at each time step, the operators = , += , -= , *= , and /= are allowed. Parameters and Variables # Any parameter or variable defined in the same Neuron/Synapse can be used inside an equation. User-defined constants can also be used. Additionally, the following variables are pre-defined: dt : the discretization time step for the simulation. Using this variable, you can define the numerical method by yourself. For example: tau * dmp / dt + mp = baseline with backward Euler would be equivalent to: mp += dt / tau * ( baseline - mp ) t : the time in milliseconds elapsed since the creation of the network. This allows to generate oscillating variables: f = 10.0 # Frequency of 10 Hz phi = pi / 4 # Phase ts = t / 1000.0 # ts is in seconds r = 10.0 * ( sin ( 2 * pi * f * ts + phi ) + 1.0 ) Random number generators # Several random generators are available and can be used within an equation. In the current version are for example available: Uniform(min, max) generates random numbers from a uniform distribution in the range \\([\\text{min}, \\text{max}]\\) . Normal(mu, sigma) generates random numbers from a normal distribution with min mu and standard deviation sigma. See Random DIstributions for more distributions. For example: noise = Uniform ( - 0.5 , 0.5 ) The arguments to the random distributions can be either fixed values or (functions of) global parameters. min_val = - 0.5 : population max_val = 0.5 : population noise = Uniform ( min_val , max_val ) It is not allowed to use local parameters (with different values per neuron) or variables, as the random number generators are initialized only once at network creation (doing otherwise would impair performance too much). If a global parameter is used, changing its value will not affect the generator after compilation. It is therefore better practice to use normalized random generators and scale their outputs: min_val = - 0.5 : population max_val = 0.5 : population noise = min_val + ( max_val - min_val ) * Uniform ( 0.0 , 1.0 ) Mathematical functions # Most mathematical functions of the cmath library are understood by the parser, for example: cos , sin , tan , acos , asin , atan , exp , abs , fabs , sqrt , log , ln The positive and negative parts of a term are also defined, with short and long versions: r = pos ( mp ) r = positive ( mp ) r = neg ( mp ) r = negative ( mp ) A piecewise linear function is also provided (linear when x is between a and b, saturated at a or b otherwise): r = clip ( x , a , b ) For integer variables, the modulo operator is defined: x += 1 : int y = modulo ( x , 10 ) When using the power function ( r = x^2 or r = pow(x, 2) ), the cmath pow(double, int) method is used. For small exponents (quadratic or cubic functions), it can be extremely slow, compared to r = x*x or r = x*x*x . Unfortunately, Sympy transforms automatically r = x*x into r = pow(x, 2) . We therefore advise to use the built-in power(double, int) function instead: r = power ( x , 3 ) These functions must be followed by a set of matching brackets: tau * dmp / dt + mp = exp ( - cos ( 2 * pi * f * t + pi / 4 ) + 1 ) Conditional statements # Python-style It is possible to use Python-style conditional statements as the right term of an equation or ODE. They follow the form: if condition : statement1 else : statement2 For example, to define a piecewise linear function, you can nest different conditionals: r = if mp < 1. : if mp > 0. : mp else : 0. else : 1. which is equivalent to: r = clip ( mp , 0.0 , 1.0 ) The condition can use the following vocabulary: True , False , and , or , not , is , is not , == , != , > , < , >= , <= Note The and , or and not logical operators must be used with parentheses around their terms. Example: var = if ( mp > 0 ) and ( ( noise < 0.1 ) or ( not ( condition )) ): 1.0 else : 0.0 is is equivalent to == , is not is equivalent to != . When a conditional statement is split over multiple lines, the flags must be set after the last line: rate = if mp < 1.0 : if mp < 0.0 : 0.0 else : mp else : 1.0 : init = 0.6 An if a: b else:c statement must be exactly the right term of an equation. It is for example NOT possible to write: r = 1.0 + (if mp> 0.0: mp else: 0.0) + b Ternary operator The ternary operator ite(cond, then, else) (ite stands for if-then-else) is available to ease the combination of conditionals with other terms: r = ite ( mp > 0.0 , mp , 0.0 ) # is exactly the same as: r = if mp > 0.0 : mp else : 0.0 The advantage is that the conditional term is not restricted to the right term of the equation, and can be used multiple times: r = ite ( mp > 0.0 , ite ( mp < 1.0 , mp , 1.0 ), 0.0 ) + ite ( stimulated , 1.0 , 0.0 ) Custom functions # To simplify the writing of equations, custom functions can be defined either globally (usable by all neurons and synapses) or locally (only for the particular type of neuron/synapse) using the same mathematical parser. Global functions can be defined using the add_function() method: add_function ( 'sigmoid(x) = 1.0 / (1.0 + exp(-x))' ) With this declaration, sigmoid() can be used in the declaration of any variable, for example: neuron = Neuron ( equations = \"\"\" r = sigmoid(sum(exc)) \"\"\" ) Functions must be one-liners, i.e. they should have only one return value. They can use as many arguments as needed, but are totally unaware of the context: all the needed information should be passed as an argument (except constants which are visible to the function). The types of the arguments (including the return value) are by default floating-point. If other types should be used, they should be specified at the end of the definition, after the : sign, with the type of the return value first, followed by the type of all arguments separated by commas: add_function ( 'conditional_increment(c, v, t) = if v > t : c + 1 else: c : int, int, float, float' ) After compilation , the function can be called using arbitrary list of values for the arguments using the functions() method and the name of the function: add_function ( 'sigmoid(x) = 1.0 / (1.0 + exp(-x))' ) compile () x = np . linspace ( - 10. , 10. , 1000 ) y = functions ( 'sigmoid' )( x ) You can pass a list or a 1D Numpy array as argument, but not a single value or a multidimensional array. When several arguemnts are passed, they must have the same size. Local functions are specific to a Neuron or Synapse class and can only be used within this context (if they have the same name as global variables, they will override them). They can be passed as a multi-line argument to the constructor of a neuron or synapse (see later): functions == \"\"\" sigmoid(x) = 1.0 / (1.0 + exp(-x)) conditional_increment(c, v, t) = if v > t : c + 1 else: c : int, int, float, float \"\"\"","title":"Parser"},{"location":"manual/Parser.html#parser","text":"A Neuron or Synapse type is primarily defined by two sets of values which must be specified in its constructor: Parameters are values such as time constants which are constant during the simulation. They can be the same throughout the population/projection, or take different values. Variables are neuronal variables (for example the membrane potential or firing rate) or synaptic variables (the synaptic efficiency) whose value evolve with time during the simulation. The equation (whether it is an ordinary differential equation or not) ruling their evolution can be described using a specific meta-language.","title":"Parser"},{"location":"manual/Parser.html#parameters","text":"Parameters are defined by a multi-string consisting of one or more parameter definitions: parameters = \"\"\" tau = 10.0 eta = 0.5 \"\"\" Each parameter should be defined on a single line, with its name on the left side of the equal sign, and its value on the right side. The given value corresponds to the initial value of the parameter (but it can be changed at any further point of the simulation). As a neuron/synapse type is likely to be reused in different populations/projections, it is good practice to set reasonable initial values in the neuron/synapse type, and eventually adapt them to the corresponding populations/projections later on. Local vs. global parameters By default, a neural parameter will be unique to each neuron (i.e. each neuron instance will hold a copy of the parameter) or synapse. In order to save memory space, one can force ANNarchy to store only one parameter value for a whole population by specifying the population flag after a : symbol following the parameter definition: parameters = \"\"\" tau = 10.0 eta = 0.5 : population \"\"\" In this case, there will be only only one instance of the eta parameter for the whole population. eta is called a global parameter, in opposition to local parameters which are the default. The same is true for synapses, whose parameters are by default unique to each synapse in a given projection. If the post-synaptic flag is passed, the parameter will be common to all synapses of a post-synaptic neuron, but can differ from one post-synaptic neuron to another. If the projection flag is passed, the parameter will be common to all synapses of a projection (e.g. the learning rate). Type of the variable Parameters have floating-point precision by default. If you want to force the parameter to be an integer or boolean, you can also pass the int and bool flags, separated by commas: parameters = \"\"\" tau = 10.0 eta = 1 : population, int \"\"\" Constants Alternatively, it is possible to use constants in the parameter definition (see later): tau_exc = Constant ( 'tau_exc' , 10.0 ) neuron = Neuron ( parameters = \"\"\" tau = tau_exc \"\"\" , ) The advantage of this method is that if a parameter value is \\\"shared\\\" across several neuron/synapse types, you only need to change the value once, instead of in each neuron/synapse definition.","title":"Parameters"},{"location":"manual/Parser.html#variables","text":"Time-varying variables are also defined using a multi-line description: equations = \"\"\" noise = Uniform(0.0, 0.2) tau * dmp/dt + mp = baseline + sum(exc) + noise r = pos(mp) \"\"\" The evolution of each variable with time can be described through a simple equation or an ordinary differential equation (ODE). ANNarchy provides a simple parser for mathematical expressions, whose role is to translate a high-level description of the equation into an optimized C++ code snippet. The equation for one variable can depend on parameters, other variables (even when declared later) or constants. Variables are updated in the same order as their declaration in the multistring (see Numerical methods , as it influences how ODEs are solved). The declaration of a single variable can extend on multiple lines: equations = \"\"\" noise = Uniform(0.0, 0.2) tau * dmp/dt = baseline - mp + sum(exc) + noise : max = 1.0 rate = pos(mp) \"\"\" As it is only a parser and not a solver, some limitations exist: Simple equations must hold only the name of the variable on the left sign of the equation. Variable definitions such as rate + mp = noise are forbidden, as it would be impossible to guess which variable should be updated. ODEs are more free regarding the left side, but only one variable should hold the gradient: the one which will be updated. The following definitions are equivalent and will lead to the same C++ code: tau * dmp / dt = baseline - mp tau * dmp / dt + mp = baseline tau * dmp / dt + mp - baseline = 0 dmp / dt = ( baseline - mp ) / tau In practice, ODEs are transformed using Sympy into the last form (only the gradient stays on the left) and numerized using the chosen numerical method (see Numerical methods ).","title":"Variables"},{"location":"manual/Parser.html#flags","text":"Locality and type Like the parameters, variables also accept the population , postsynaptic and projection flags to define the local/global character of the variable, as well as the int or bool flags for their type. Initial value The initial value of the variable (before the first simulation starts) can also be specified using the init keyword followed by the desired value: equations = \"\"\" tau * dmp/dt + mp = baseline : init = 0.2 \"\"\" It must be a single value (the same for all neurons in the population or all synapses in the projection) and should not depend on other parameters and variables. This initial value can be specifically changed after the Population or Projection objects are created (see Populations ). It is also possible to use constants for the initial value: init_mp = Constant ( 'init_mp' , 0.2 ) neuron = Neuron ( equations = \"\"\" tau * dmp/dt + mp = baseline : init = init_mp \"\"\" , ) Min and Max values of a variable Upper- and lower-bounds can be set using the min and max keywords: equations = \"\"\" tau * dmp/dt + mp = baseline : min = -0.2, max = 1.0 \"\"\" At each step of the simulation, after the update rule is calculated for mp , the new value will be compared to the min and max value, and clamped if necessary. min and max can be single values, constants, parameters, variables or functions of all these: parameters = \"\"\" tau = 10.0 min_mp = -1.0 : population max_mp = 1.0 \"\"\" , equations = \"\"\" variance = Uniform(0.0, 1.0) tau * dmp/dt + mp = sum(exc) : min = min_mp, max = max_mp + variance r = mp : min = 0.0 # Equivalent to r = pos(mp) \"\"\" Numerical method The numerization method for a single ODEs can be explicitely set by specifying a flag: tau * dmp/dt + mp = sum(exc) : exponential The available numerical methods are described in Numerical methods . Summary of allowed flags for variables: init : defines the initialization value at begin of simulation and after a network reset (default: 0.0) min : minimum allowed value (unset by default) max : maximum allowed value (unset by default) population : the attribute is shared by all neurons of a population. postsynaptic : the attribute is shared by all synapses of a post-synaptic neuron. projection : the attribute is shared by all synapses of a projection. explicit , implicit , exponential , midpoint , event-driven : the numerical method to be used.","title":"Flags"},{"location":"manual/Parser.html#constants","text":"Global constants can be created by the user and used inside any equation. They must define an unique name and a floating point value: tau = Constant ( 'tau' , 10.0 ) neuron = Neuron ( equations = \"tau * dr/dt + r = sum(exc)\" ) In this example, a Neuron or Synapse does not have to define the parameter tau to use it: it is available everywhere. If the Neuron/Synapse redefines a parameter called tau , the constant is not visible anymore to that object. Constants can be manipulated as normal floats to define complex values: tau = Constant ( 'tau' , 20 ) factor = Constant ( 'factor' , 0.1 ) real_tau = Constant ( 'real_tau' , tau * factor ) neuron = Neuron ( equations = ''' real_tau*dr/dt + r =1.0 ''' ) Note that constants are only global, changing their value impacts all objects using them. Changing the value of a constant can only be done through the set() method (before or after compile() ): tau = Constant ( 'tau' , 20 ) tau . set ( 10.0 )","title":"Constants"},{"location":"manual/Parser.html#allowed-vocabulary","text":"The mathematical parser relies heavily on the one provided by SymPy .","title":"Allowed vocabulary"},{"location":"manual/Parser.html#numerical-values","text":"All parameters and variables use implicitly the floating-point double precision, except when stated otherwise with the int or bool keywords. You can use numerical constants within the equation, noting that they will be automatically converted to this precision: tau * dmp / dt = 1 / pos ( mp ) + 1 The constant \\(\\pi\\) is available under the literal form pi .","title":"Numerical values"},{"location":"manual/Parser.html#operators","text":"Additions (+), substractions (-), multiplications (*), divisions (/) and power functions (\\^) are of course allowed. Gradients are allowed only for the variable currently described. They take the form: dmp / dt = A with a d preceding the variable's name and terminated by /dt (with or without spaces). Gradients must be on the left side of the equation. To update the value of a variable at each time step, the operators = , += , -= , *= , and /= are allowed.","title":"Operators"},{"location":"manual/Parser.html#parameters-and-variables","text":"Any parameter or variable defined in the same Neuron/Synapse can be used inside an equation. User-defined constants can also be used. Additionally, the following variables are pre-defined: dt : the discretization time step for the simulation. Using this variable, you can define the numerical method by yourself. For example: tau * dmp / dt + mp = baseline with backward Euler would be equivalent to: mp += dt / tau * ( baseline - mp ) t : the time in milliseconds elapsed since the creation of the network. This allows to generate oscillating variables: f = 10.0 # Frequency of 10 Hz phi = pi / 4 # Phase ts = t / 1000.0 # ts is in seconds r = 10.0 * ( sin ( 2 * pi * f * ts + phi ) + 1.0 )","title":"Parameters and Variables"},{"location":"manual/Parser.html#random-number-generators","text":"Several random generators are available and can be used within an equation. In the current version are for example available: Uniform(min, max) generates random numbers from a uniform distribution in the range \\([\\text{min}, \\text{max}]\\) . Normal(mu, sigma) generates random numbers from a normal distribution with min mu and standard deviation sigma. See Random DIstributions for more distributions. For example: noise = Uniform ( - 0.5 , 0.5 ) The arguments to the random distributions can be either fixed values or (functions of) global parameters. min_val = - 0.5 : population max_val = 0.5 : population noise = Uniform ( min_val , max_val ) It is not allowed to use local parameters (with different values per neuron) or variables, as the random number generators are initialized only once at network creation (doing otherwise would impair performance too much). If a global parameter is used, changing its value will not affect the generator after compilation. It is therefore better practice to use normalized random generators and scale their outputs: min_val = - 0.5 : population max_val = 0.5 : population noise = min_val + ( max_val - min_val ) * Uniform ( 0.0 , 1.0 )","title":"Random number generators"},{"location":"manual/Parser.html#mathematical-functions","text":"Most mathematical functions of the cmath library are understood by the parser, for example: cos , sin , tan , acos , asin , atan , exp , abs , fabs , sqrt , log , ln The positive and negative parts of a term are also defined, with short and long versions: r = pos ( mp ) r = positive ( mp ) r = neg ( mp ) r = negative ( mp ) A piecewise linear function is also provided (linear when x is between a and b, saturated at a or b otherwise): r = clip ( x , a , b ) For integer variables, the modulo operator is defined: x += 1 : int y = modulo ( x , 10 ) When using the power function ( r = x^2 or r = pow(x, 2) ), the cmath pow(double, int) method is used. For small exponents (quadratic or cubic functions), it can be extremely slow, compared to r = x*x or r = x*x*x . Unfortunately, Sympy transforms automatically r = x*x into r = pow(x, 2) . We therefore advise to use the built-in power(double, int) function instead: r = power ( x , 3 ) These functions must be followed by a set of matching brackets: tau * dmp / dt + mp = exp ( - cos ( 2 * pi * f * t + pi / 4 ) + 1 )","title":"Mathematical functions"},{"location":"manual/Parser.html#conditional-statements","text":"Python-style It is possible to use Python-style conditional statements as the right term of an equation or ODE. They follow the form: if condition : statement1 else : statement2 For example, to define a piecewise linear function, you can nest different conditionals: r = if mp < 1. : if mp > 0. : mp else : 0. else : 1. which is equivalent to: r = clip ( mp , 0.0 , 1.0 ) The condition can use the following vocabulary: True , False , and , or , not , is , is not , == , != , > , < , >= , <= Note The and , or and not logical operators must be used with parentheses around their terms. Example: var = if ( mp > 0 ) and ( ( noise < 0.1 ) or ( not ( condition )) ): 1.0 else : 0.0 is is equivalent to == , is not is equivalent to != . When a conditional statement is split over multiple lines, the flags must be set after the last line: rate = if mp < 1.0 : if mp < 0.0 : 0.0 else : mp else : 1.0 : init = 0.6 An if a: b else:c statement must be exactly the right term of an equation. It is for example NOT possible to write: r = 1.0 + (if mp> 0.0: mp else: 0.0) + b Ternary operator The ternary operator ite(cond, then, else) (ite stands for if-then-else) is available to ease the combination of conditionals with other terms: r = ite ( mp > 0.0 , mp , 0.0 ) # is exactly the same as: r = if mp > 0.0 : mp else : 0.0 The advantage is that the conditional term is not restricted to the right term of the equation, and can be used multiple times: r = ite ( mp > 0.0 , ite ( mp < 1.0 , mp , 1.0 ), 0.0 ) + ite ( stimulated , 1.0 , 0.0 )","title":"Conditional statements"},{"location":"manual/Parser.html#custom-functions","text":"To simplify the writing of equations, custom functions can be defined either globally (usable by all neurons and synapses) or locally (only for the particular type of neuron/synapse) using the same mathematical parser. Global functions can be defined using the add_function() method: add_function ( 'sigmoid(x) = 1.0 / (1.0 + exp(-x))' ) With this declaration, sigmoid() can be used in the declaration of any variable, for example: neuron = Neuron ( equations = \"\"\" r = sigmoid(sum(exc)) \"\"\" ) Functions must be one-liners, i.e. they should have only one return value. They can use as many arguments as needed, but are totally unaware of the context: all the needed information should be passed as an argument (except constants which are visible to the function). The types of the arguments (including the return value) are by default floating-point. If other types should be used, they should be specified at the end of the definition, after the : sign, with the type of the return value first, followed by the type of all arguments separated by commas: add_function ( 'conditional_increment(c, v, t) = if v > t : c + 1 else: c : int, int, float, float' ) After compilation , the function can be called using arbitrary list of values for the arguments using the functions() method and the name of the function: add_function ( 'sigmoid(x) = 1.0 / (1.0 + exp(-x))' ) compile () x = np . linspace ( - 10. , 10. , 1000 ) y = functions ( 'sigmoid' )( x ) You can pass a list or a 1D Numpy array as argument, but not a single value or a multidimensional array. When several arguemnts are passed, they must have the same size. Local functions are specific to a Neuron or Synapse class and can only be used within this context (if they have the same name as global variables, they will override them). They can be passed as a multi-line argument to the constructor of a neuron or synapse (see later): functions == \"\"\" sigmoid(x) = 1.0 / (1.0 + exp(-x)) conditional_increment(c, v, t) = if v > t : c + 1 else: c : int, int, float, float \"\"\"","title":"Custom functions"},{"location":"manual/Populations.html","text":"Populations # Once the Neuron objects have been defined, the populations can be created. Let's suppose we have defined the following rate-coded neuron: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) \"\"\" ) Creating populations # Populations of neurons are created using the Population class: pop1 = Population ( geometry = 100 , neuron = LeakyIntegratorNeuron ) pop2 = Population ( geometry = ( 8 , 8 ), neuron = LeakyIntegratorNeuron , name = \"pop2\" ) The rate-coded or spiking nature of the Neuron instance is irrelevant when creating the Population object. It takes different parameters: geometry defines the number of neurons in the population, as well as its spatial structure (1D/2D/3D or more). For example, a two-dimensional population with 15*10 neurons takes the argument (15, 10) , while a one-dimensional array of 100 neurons would take (100,) or simply 100 . neuron indicates the neuron type to use for this population (which must have been defined before). It requires a Neuron instance. name is an unique string for each population in the network. If name is omitted, an internal name such as pop0 will be given (the number is incremented every time a new population is defined). Although this argument is optional, it is strongly recommended to give an understandable name to each population: if you somehow \\\"lose\\\" the reference to the Population object in some portion of your code, you can always retrieve it using the get_population(name) method. After creation, each population has several attributes defined (corresponding to the parameters and variables of the Neuron type) and is assigned a fixed size ( pop.size corresponding to the total number of neurons, here 100 for pop1 and 64 for pop2 ) and geometry ( pop1.geometry , here (100, ) and (8, 8) ). Geometry and ranks # Each neuron in the population has therefore a set of coordinates (expressed relative to pop1.geometry ) and a rank (from 0 to pop1.size -1 ). The reason is that spatial coordinates are useful for visualization, or when defining a distance-dependent connection pattern, but that ANNarchy internally uses flat arrays for performance reasons. The coordinates use the matrix notation for multi-dimensional arrays, which is also used by Numpy (for a 2D matrix, the first index represents the row, the second the column). You can therefore use safely the reshape() method of Numpy to switch between coordinates-based and rank-based representations of an array. To convert the rank of a neuron to its coordinates (and vice-versa), you can use the ravel_multi_index and unravel_index methods of Numpy, but they can be quite slow. The Population class provides two more efficient methods to do this conversion: coordinates_from_rank returns a tuple representing the coordinates of neuron based on its rank (between 0 and size -1 , otherwise an error is thrown). rank_from_coordinates returns the rank corresponding to the coordinates. For example, with pop2 having a geometry (8, 8) : >>> pop2 . coordinates_from_rank ( 15 ) ( 1 , 7 ) >>> pop2 . rank_from_coordinates (( 4 , 6 )) 38 Population attributes # The value of the parameters and variables of all neurons in a population can be accessed and modified through population attributes. With the previously defined populations, you can list all their parameters and variables with: >>> pop2 . attributes [ 'tau' , 'baseline' , 'mp' , 'r' ] >>> pop2 . parameters [ 'tau' , 'baseline' ] >>> pop2 . variables [ 'r' , 'mp' ] Reading their value is straightforward: >>> pop2 . tau 10.0 >>> pop2 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) Population-wise parameters/variables have a single value for the population, while neuron-specific ones return a NumPy array with the same geometry has the population. Setting their value is also simple: >>> pop2 . tau = 20.0 >>> pop2 . tau 20.0 >>> pop2 . r = 1.0 >>> pop2 . r array ([[ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ]]) >>> pop2 . mp = 0.5 * np . ones ( pop2 . geometry ) array ([[ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ]]) >>> pop2 . r = Uniform ( 0.0 , 1.0 ) array ([[ 0.97931939 , 0.64865327 , 0.29740417 , 0.49352664 , 0.36511704 , 0.59879869 , 0.10835491 , 0.38481751 ], [ 0.07664157 , 0.77532887 , 0.04773084 , 0.75395453 , 0.56072342 , 0.54139054 , 0.28553319 , 0.96159595 ], [ 0.01811468 , 0.30214921 , 0.45321071 , 0.56728733 , 0.24577655 , 0.32798484 , 0.84929103 , 0.63025331 ], [ 0.34168482 , 0.07411291 , 0.6510492 , 0.89025337 , 0.31192464 , 0.59834719 , 0.77102494 , 0.88537967 ], [ 0.41813573 , 0.47395247 , 0.46603402 , 0.45863676 , 0.76628989 , 0.42256749 , 0.18527079 , 0.8322103 ], [ 0.70616692 , 0.73210377 , 0.05255718 , 0.01939817 , 0.24659769 , 0.50349528 , 0.79201573 , 0.19159611 ], [ 0.21246111 , 0.93570727 , 0.68544108 , 0.61158741 , 0.17954022 , 0.90084004 , 0.41286698 , 0.45550662 ], [ 0.14720568 , 0.51426136 , 0.36225438 , 0.06096426 , 0.77209455 , 0.07348683 , 0.43178591 , 0.32451531 ]]) For population-wide attributes, you can only specify a single value (float, int or bool depending on the type of the parameter/variable). For neuron-specific attributes, you can provide either: a single value which will be applied to all neurons of the population. a list or a one-dimensional Numpy array of the same length as the number of neurons in the population. This information is provided by pop1.size . a Numpy array of the same shape as the geometry of the population. This information is provided by pop1.geometry . a random number generator object (Uniform, Normal...). Note If you do not want to use the attributes of Python (for example when doing a loop over unknown attributes), you can also use the get(name) and set(values) methods of Population : pop1 . get ( 'tau' ) pop1 . set ({ 'mp' : 1.0 , 'r' : Uniform ( 0.0 , 1.0 )}) Accessing individual neurons # There exists a purely semantic access to individual neurons of a population. The IndividualNeuron class wraps population data for a specific neuron. It can be accessed through the Population.neuron() method using either the rank of the neuron (from 0 to pop1.size - 1 ) or its coordinates in the population's geometry: >>> print pop2 . neuron ( 2 , 2 ) Neuron of the population pop2 with rank 18 ( coordinates ( 2 , 2 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 The individual neurons can be manipulated individually: >>> my_neuron = pop2 . neuron ( 2 , 2 ) >>> my_neuron . rate = 1.0 >>> print my_neuron Neuron of the population pop2 with rank 18 ( coordinates ( 2 , 2 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 1.0 Warning IndividualNeuron is only a wrapper for ease of use, the real data is stored in arrays for the whole population, so accessing individual neurons is much slower and should be reserved to specific cases (i.e. only from time to time and for a limited set of neurons). Accessing groups of neurons # Individual neurons can be grouped into PopulationView objects, which hold references to different neurons of the same population. One can create population views by \\\"adding\\\" several neurons together: >>> popview = pop2 . neuron ( 2 , 2 ) + pop2 . neuron ( 3 , 3 ) + pop2 . neuron ( 4 , 4 ) >>> popview PopulationView of pop2 Ranks : [ 18 , 27 , 36 ] * Neuron of the population pop2 with rank 18 ( coordinates ( 2 , 2 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 * Neuron of the population pop2 with rank 27 ( coordinates ( 3 , 3 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 * Neuron of the population pop2 with rank 36 ( coordinates ( 4 , 4 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 >>> popview . r = 1.0 >>> pop2 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) One can also use the slice operators to create PopulationViews: >>> popview = pop2 [ 3 , :] >>> popview . r = 1.0 >>> pop2 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) or: >>> popview = pop2 [ 2 : 5 , 4 ] >>> popview . r = 1.0 >>> pop1 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) PopulationView objects can be used to create projections. Warning Contrary to the equivalent in PyNN, PopulationViews in ANNarchy can only group neurons from the same population. Functions # If you have defined a function inside a Neuron definition: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 slope = 1.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = sigmoid(mp, slope) \"\"\" , functions = \"\"\" sigmoid(x, k) = 1.0 / (1.0 + exp(-x*k)) \"\"\" ) you can use this function in Python as if it were a method of the corresponding object: pop = Population ( 1000 , LeakyIntegratorNeuron ) x = np . linspace ( - 1. , 1. , 100 ) k = np . ones ( 100 ) r = pop . sigmoid ( x , k ) You can pass either a list or a 1D Numpy array to each argument ( not a single value, nor a multidimensional array! ). The size of the arrays passed for each argument is arbitrary (it must not match the population's size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.","title":"Populations"},{"location":"manual/Populations.html#populations","text":"Once the Neuron objects have been defined, the populations can be created. Let's suppose we have defined the following rate-coded neuron: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) \"\"\" )","title":"Populations"},{"location":"manual/Populations.html#creating-populations","text":"Populations of neurons are created using the Population class: pop1 = Population ( geometry = 100 , neuron = LeakyIntegratorNeuron ) pop2 = Population ( geometry = ( 8 , 8 ), neuron = LeakyIntegratorNeuron , name = \"pop2\" ) The rate-coded or spiking nature of the Neuron instance is irrelevant when creating the Population object. It takes different parameters: geometry defines the number of neurons in the population, as well as its spatial structure (1D/2D/3D or more). For example, a two-dimensional population with 15*10 neurons takes the argument (15, 10) , while a one-dimensional array of 100 neurons would take (100,) or simply 100 . neuron indicates the neuron type to use for this population (which must have been defined before). It requires a Neuron instance. name is an unique string for each population in the network. If name is omitted, an internal name such as pop0 will be given (the number is incremented every time a new population is defined). Although this argument is optional, it is strongly recommended to give an understandable name to each population: if you somehow \\\"lose\\\" the reference to the Population object in some portion of your code, you can always retrieve it using the get_population(name) method. After creation, each population has several attributes defined (corresponding to the parameters and variables of the Neuron type) and is assigned a fixed size ( pop.size corresponding to the total number of neurons, here 100 for pop1 and 64 for pop2 ) and geometry ( pop1.geometry , here (100, ) and (8, 8) ).","title":"Creating populations"},{"location":"manual/Populations.html#geometry-and-ranks","text":"Each neuron in the population has therefore a set of coordinates (expressed relative to pop1.geometry ) and a rank (from 0 to pop1.size -1 ). The reason is that spatial coordinates are useful for visualization, or when defining a distance-dependent connection pattern, but that ANNarchy internally uses flat arrays for performance reasons. The coordinates use the matrix notation for multi-dimensional arrays, which is also used by Numpy (for a 2D matrix, the first index represents the row, the second the column). You can therefore use safely the reshape() method of Numpy to switch between coordinates-based and rank-based representations of an array. To convert the rank of a neuron to its coordinates (and vice-versa), you can use the ravel_multi_index and unravel_index methods of Numpy, but they can be quite slow. The Population class provides two more efficient methods to do this conversion: coordinates_from_rank returns a tuple representing the coordinates of neuron based on its rank (between 0 and size -1 , otherwise an error is thrown). rank_from_coordinates returns the rank corresponding to the coordinates. For example, with pop2 having a geometry (8, 8) : >>> pop2 . coordinates_from_rank ( 15 ) ( 1 , 7 ) >>> pop2 . rank_from_coordinates (( 4 , 6 )) 38","title":"Geometry and ranks"},{"location":"manual/Populations.html#population-attributes","text":"The value of the parameters and variables of all neurons in a population can be accessed and modified through population attributes. With the previously defined populations, you can list all their parameters and variables with: >>> pop2 . attributes [ 'tau' , 'baseline' , 'mp' , 'r' ] >>> pop2 . parameters [ 'tau' , 'baseline' ] >>> pop2 . variables [ 'r' , 'mp' ] Reading their value is straightforward: >>> pop2 . tau 10.0 >>> pop2 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) Population-wise parameters/variables have a single value for the population, while neuron-specific ones return a NumPy array with the same geometry has the population. Setting their value is also simple: >>> pop2 . tau = 20.0 >>> pop2 . tau 20.0 >>> pop2 . r = 1.0 >>> pop2 . r array ([[ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ]]) >>> pop2 . mp = 0.5 * np . ones ( pop2 . geometry ) array ([[ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [ 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ]]) >>> pop2 . r = Uniform ( 0.0 , 1.0 ) array ([[ 0.97931939 , 0.64865327 , 0.29740417 , 0.49352664 , 0.36511704 , 0.59879869 , 0.10835491 , 0.38481751 ], [ 0.07664157 , 0.77532887 , 0.04773084 , 0.75395453 , 0.56072342 , 0.54139054 , 0.28553319 , 0.96159595 ], [ 0.01811468 , 0.30214921 , 0.45321071 , 0.56728733 , 0.24577655 , 0.32798484 , 0.84929103 , 0.63025331 ], [ 0.34168482 , 0.07411291 , 0.6510492 , 0.89025337 , 0.31192464 , 0.59834719 , 0.77102494 , 0.88537967 ], [ 0.41813573 , 0.47395247 , 0.46603402 , 0.45863676 , 0.76628989 , 0.42256749 , 0.18527079 , 0.8322103 ], [ 0.70616692 , 0.73210377 , 0.05255718 , 0.01939817 , 0.24659769 , 0.50349528 , 0.79201573 , 0.19159611 ], [ 0.21246111 , 0.93570727 , 0.68544108 , 0.61158741 , 0.17954022 , 0.90084004 , 0.41286698 , 0.45550662 ], [ 0.14720568 , 0.51426136 , 0.36225438 , 0.06096426 , 0.77209455 , 0.07348683 , 0.43178591 , 0.32451531 ]]) For population-wide attributes, you can only specify a single value (float, int or bool depending on the type of the parameter/variable). For neuron-specific attributes, you can provide either: a single value which will be applied to all neurons of the population. a list or a one-dimensional Numpy array of the same length as the number of neurons in the population. This information is provided by pop1.size . a Numpy array of the same shape as the geometry of the population. This information is provided by pop1.geometry . a random number generator object (Uniform, Normal...). Note If you do not want to use the attributes of Python (for example when doing a loop over unknown attributes), you can also use the get(name) and set(values) methods of Population : pop1 . get ( 'tau' ) pop1 . set ({ 'mp' : 1.0 , 'r' : Uniform ( 0.0 , 1.0 )})","title":"Population attributes"},{"location":"manual/Populations.html#accessing-individual-neurons","text":"There exists a purely semantic access to individual neurons of a population. The IndividualNeuron class wraps population data for a specific neuron. It can be accessed through the Population.neuron() method using either the rank of the neuron (from 0 to pop1.size - 1 ) or its coordinates in the population's geometry: >>> print pop2 . neuron ( 2 , 2 ) Neuron of the population pop2 with rank 18 ( coordinates ( 2 , 2 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 The individual neurons can be manipulated individually: >>> my_neuron = pop2 . neuron ( 2 , 2 ) >>> my_neuron . rate = 1.0 >>> print my_neuron Neuron of the population pop2 with rank 18 ( coordinates ( 2 , 2 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 1.0 Warning IndividualNeuron is only a wrapper for ease of use, the real data is stored in arrays for the whole population, so accessing individual neurons is much slower and should be reserved to specific cases (i.e. only from time to time and for a limited set of neurons).","title":"Accessing individual neurons"},{"location":"manual/Populations.html#accessing-groups-of-neurons","text":"Individual neurons can be grouped into PopulationView objects, which hold references to different neurons of the same population. One can create population views by \\\"adding\\\" several neurons together: >>> popview = pop2 . neuron ( 2 , 2 ) + pop2 . neuron ( 3 , 3 ) + pop2 . neuron ( 4 , 4 ) >>> popview PopulationView of pop2 Ranks : [ 18 , 27 , 36 ] * Neuron of the population pop2 with rank 18 ( coordinates ( 2 , 2 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 * Neuron of the population pop2 with rank 27 ( coordinates ( 3 , 3 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 * Neuron of the population pop2 with rank 36 ( coordinates ( 4 , 4 )) . Parameters : tau = 10.0 baseline = - 0.2 Variables : mp = 0.0 r = 0.0 >>> popview . r = 1.0 >>> pop2 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) One can also use the slice operators to create PopulationViews: >>> popview = pop2 [ 3 , :] >>> popview . r = 1.0 >>> pop2 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) or: >>> popview = pop2 [ 2 : 5 , 4 ] >>> popview . r = 1.0 >>> pop1 . r array ([[ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) PopulationView objects can be used to create projections. Warning Contrary to the equivalent in PyNN, PopulationViews in ANNarchy can only group neurons from the same population.","title":"Accessing groups of neurons"},{"location":"manual/Populations.html#functions","text":"If you have defined a function inside a Neuron definition: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 slope = 1.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = sigmoid(mp, slope) \"\"\" , functions = \"\"\" sigmoid(x, k) = 1.0 / (1.0 + exp(-x*k)) \"\"\" ) you can use this function in Python as if it were a method of the corresponding object: pop = Population ( 1000 , LeakyIntegratorNeuron ) x = np . linspace ( - 1. , 1. , 100 ) k = np . ones ( 100 ) r = pop . sigmoid ( x , k ) You can pass either a list or a 1D Numpy array to each argument ( not a single value, nor a multidimensional array! ). The size of the arrays passed for each argument is arbitrary (it must not match the population's size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.","title":"Functions"},{"location":"manual/Projections.html","text":"Projections # Declaring the projections # Once the populations are created, one can connect them by creating Projection instances: proj = Projection ( pre = pop1 , post = pop2 , target = \"exc\" , synapse = BCM ) pre is either the name of the pre-synaptic population or the corresponding Population object. post is either the name of the post-synaptic population or the corresponding Population object. target is the type of the connection. synapse is an optional argument requiring a Synapse instance. The post-synaptic neuron type must use sum(exc) in the rate-coded case respectively g_exc in the spiking case, otherwise the projection will be useless. If the synapse argument is omitted, the default synapse will be used: the default rate-coded synapse defines psp = w * pre.r , the default spiking synapse defines g_target += w . Building the projections # Creating the Projection objects only defines the information that two populations are connected. The synapses must be explicitely created by applying a connector method on the Projection object. To this end, ANNarchy already provides a set of predefined connector methods, but the user has also the possibility to define his own (see Connector ). The pattern can be applied either directly at the creation of the Projection: proj = Projection ( pre = pop1 , post = pop2 , target = \"exc\" , synapse = BCM ) . connect_all_to_all ( weights = 1.0 ) or afterwards: proj = Projection ( pre = pop1 , post = pop2 , target = \"exc\" , synapse = BCM ) proj . connect_all_to_all ( weights = 1.0 ) The connector method must be called before the network is compiled. Projection attributes # Let's suppose the BCM synapse is used to create the Projection proj (spiking synapses are accessed similarly): BCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 100. : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0 \"\"\" ) Global attributes # The global parameters and variables of a projection (i.e. defined with the postsynaptic or projection flags) can be accessed directly through attributes. Attributes defined with projection have a single value for the whole population: >>> proj . tau 100 Attributes defined with postsynaptic have one value per post-synaptic neuron, so the result is a vector: >>> proj . theta [ 3.575 , 15.987 , ... , 4.620 ] Post-synaptic variables can be modified by passing: a single value, which will be the same for all post-synaptic neurons. a list of values, with the same size as the number of neurons receiving synapses (for some sparse connectivity patterns, it may not be the same as the size of the population, so no multidimensional array is accepted). After compilation (and therefore creation of the synapses), you can access how many post-synaptic neurons receive actual synapses with: >>> proj . size 4 The list of ranks of the post-synaptic neurons receiving synapses is obtained with: >>> proj . post_ranks [ 0 , 1 , 2 , 3 ] Local attributes # At the projection level Local attributes can also be accessed globally through attributes. It will return a list of lists containing the synapse-specific values. The first index represents the post-synaptic neurons. It has the same length as proj.post_ranks . Beware that if some post-synaptic neurons do not receive any connection, this index will not correspond to the ranksof the post-synaptic population. The second index addresses the pre-synaptic neurons. If the connection is sparse, it also is unrelated to the ranks of the pre-synaptic neurons in their populations. Warning Modifying these lists of lists is error-prone, so this method should be avoided if possible. At the post-synaptic level The local parameters and variables of a projection (synapse-specific) should better be accessed through the Dendrite object, which gathers for a single post-synaptic neuron all synapses belonging to the projection. Beware: As projections are only instantiated after the call to compile() , local attributes of a Projection are only available then. Trying to access them before compilation will lead to an error! Each dendrite stores the parameters and variables of the corresponding synapses as attributes, as populations do for neurons. You can loop over all post-synaptic neurons receiving synapses with the dendrites iterator: for dendrite in proj . dendrites : print dendrite . pre_ranks print dendrite . size print dendrite . tau print dendrite . alpha print dendrite . w dendrite.pre_ranks returns a list of pre-synaptic neuron ranks. dendrite.size returns the number of synapses for the considered post-synaptic neuron. Global parameters/variables return a single value ( dendrite.tau ) and local ones return a list ( dendrite.w ). You can even omit the .dendrites part of the iterator: for dendrite in proj : print dendrite . pre_ranks print dendrite . size print dendrite . tau print dendrite . alpha print dendrite . w You can also access the dendrites individually, either by specifying the rank of the post-synaptic neuron: dendrite = proj . dendrite ( 13 ) print dendrite . w or its coordinates: dendrite = proj . dendrite ( 5 , 5 ) print dendrite . w When using ranks, you can also directly address the projection as an array: dendrite = proj [ 13 ] print dendrite . w Warning You should make sure that the dendrite actually exists before accessing it through its rank, because it is otherwise a None object. Functions # If you have defined a function inside a Synapse definition: BCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 100. : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic dw/dt = eta * BCMRule(pre.r, post.r, theta) : min=0.0 \"\"\" , functions = \"\"\" BCMRule(pre, post, theta) = post * (post - theta) * pre \"\"\" ) you can use this function in Python as if it were a method of the corresponding object: proj = Projection ( pop1 , pop2 , 'exc' , BCM ) . connect_xxx () pre = np . linspace ( 0. , 1. , 100 ) post = np . linspace ( 0. , 1. , 100 ) theta = 0.01 * np . ones ( 100 ) weight_change = proj . BCMRule ( pre , post , theta ) You can pass either a list or a 1D Numpy array to each argument ( not a single value, nor a multidimensional array! ). The size of the arrays passed for each argument is arbitrary (it must not match the projection's size) but you have to make sure that they all have the same size. Errors are not catched, so be careful. Connecting population views # Projections are usually understood as a connectivity pattern between two populations. Complex connectivity patterns have to specifically designed (see Connector ). In some cases, it can be much simpler to connect subsets of neurons directly, using built-in connector methods. To this end, the Projection object also accepts PopulationView objects for the pre and post arguments. Let's suppose we want to connect the (8,8) populations pop1 and pop2 in a all-to-all manner, but only for the (4,4) neurons in the center of these populations. The first step is to create the PopulationView objects using the slice operator: pop1_center = pop1 [ 2 : 7 , 2 : 7 ] pop2_center = pop2 [ 2 : 7 , 2 : 7 ] They can then be simply used to create a projection: proj = Projection ( pre = pop1_center , post = pop2_center , target = \"exc\" , synapse = BCM ) . connect_all_to_all ( weights = 1.0 ) Each neuron of pop2_center will receive synapses from all neurons of pop1_center , and only them. Neurons of pop2 which are not in pop2_center will not receive any synapse. Warning If you define your own connector method and want to use PopulationViews, you will need to iterate over the ranks attribute of the PopulationView object. Specifying delays in synaptic transmission # By default, synaptic transmission is considered to be instantaneous (or more precisely, it takes one simulation step ( dt ) for a newly computed firing rate to be taken into account by post-synaptic neurons). In order to take longer propagation times into account in the transmission of information between two populations, one has the possibility to define synaptic delays for a projection. All the built-in connector methods take an argument delays (default= dt ), which can be a float (in milliseconds) or a random number generator. proj . connect_all_to_all ( weights = 1.0 , delays = 10.0 ) proj . connect_all_to_all ( weights = 1.0 , delays = Uniform ( 1.0 , 10.0 )) If the delay is not a multiple of the simulation time step ( dt = 1.0 by default), it will be rounded to the closest multiple. The same is true for the values returned by a random number generator. Note: Per design, the minimal possible delay is equal to dt : values smaller than dt will be replaced by dt . Negative values do not make any sense and are ignored. Warning Non-uniform delays are not available on CUDA. Controlling projections # Synaptic transmission, update and plasticity It is possible to selectively control synaptic transmission and plasticity at the projection level. The boolean flags transmission , update and plasticity can be set for that purpose: proj.transmission = False proj.update = False proj.plasticity = False If transmission is False , the projection is totally shut down: it does not transmit any information to the post-synaptic population (the corresponding weighted sums or conductances are constantly 0) and all synaptic variables are frozen to their current value (including the synaptic weights w ). If update is False , synaptic transmission occurs normally, but the synaptic variables are not updated. For spiking synapses, this includes traces when they are computed at each step, but not when they are integrated in an event-driven manner (flag event-driven ). Beware: continous synaptic transmission as in NMDA synapses will not work in this mode, as internal variables are not updated. If only plasticity is False , synaptic transmission and synaptic variable updates occur normally, but changes to the synaptic weight w are ignored. Disabling learning Alternatively, one can use the enable_learning() and disable_learning() methods of Projection . The effect of disable_learning() depends on the type of the projection: for rate-coded projections, disable_learning() is equivalent to update=False : no synaptic variables is updated. for spiking projections, it is equivalent to plasticity=False : only the weights are blocked. The reason of this difference is to allow continuous synaptic transmission and computation of traces. Calling enable_learning() without arguments resumes the default learning behaviour. Periodic learning enable_learning() also accepts two arguments period and offset . period defines the interval in ms between two evaluations of the synaptic variables. This can be useful when learning should only occur once at the end of a trial. It is recommended not to use ODEs in the equations in this case, as they are numerized according to a fixed time step. offset defines the time inside the period at which the evaluation should occur. By default, it is 0, so the variable updates will occur at the next step, then after period ms, and so on. Setting it to -1 will shift the update at the end of the period. Note that spiking synapses using online evaluation will not be affected by these parameters, as they are event-driven. Multiple targets # For spiking neurons, it may be desirable that a single synapses activates different currents (or conductances) in the post-synaptic neuron. One example are AMPA/NMDA synapses, where a single spike generates a \\\"classical\\\" AMPA current, plus a voltage-gated slower NMDA current. The following conductance-based Izhikevich is an example: RSNeuron = Neuron ( parameters = \"\"\" a = 0.02 b = 0.2 c = -65. d = 8. tau_ampa = 5. tau_nmda = 150. vrev = 0.0 \"\"\" , equations = \"\"\" I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v) dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init=-65., midpoint du/dt = a * (b*v - u) : init=-13. tau_ampa * dg_ampa/dt = -g_ampa tau_nmda * dg_nmda/dt = -g_nmda \"\"\" , spike = \"\"\" v >= 30. \"\"\" , reset = \"\"\" v = c u += d \"\"\" , functions = \"\"\" nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2) \"\"\" ) However, g_ampa and g_nmda collect by default spikes from different projections, so the weights will not be shared between the \\\"ampa\\\" projection and the \\\"nmda\\\" one. It is therefore possible to specify a list of targets when building a projection, meaning that a single pre-synaptic spike will increase both g_ampa and g_nmda from the same weight: proj = Projection(pop1, pop2, ['ampa', 'nmda'], STDP) An example is provided in examples/homeostatic_stdp/Ramp.py . Warning Multiple targets are not available on CUDA yet.","title":"Projections"},{"location":"manual/Projections.html#projections","text":"","title":"Projections"},{"location":"manual/Projections.html#declaring-the-projections","text":"Once the populations are created, one can connect them by creating Projection instances: proj = Projection ( pre = pop1 , post = pop2 , target = \"exc\" , synapse = BCM ) pre is either the name of the pre-synaptic population or the corresponding Population object. post is either the name of the post-synaptic population or the corresponding Population object. target is the type of the connection. synapse is an optional argument requiring a Synapse instance. The post-synaptic neuron type must use sum(exc) in the rate-coded case respectively g_exc in the spiking case, otherwise the projection will be useless. If the synapse argument is omitted, the default synapse will be used: the default rate-coded synapse defines psp = w * pre.r , the default spiking synapse defines g_target += w .","title":"Declaring the projections"},{"location":"manual/Projections.html#building-the-projections","text":"Creating the Projection objects only defines the information that two populations are connected. The synapses must be explicitely created by applying a connector method on the Projection object. To this end, ANNarchy already provides a set of predefined connector methods, but the user has also the possibility to define his own (see Connector ). The pattern can be applied either directly at the creation of the Projection: proj = Projection ( pre = pop1 , post = pop2 , target = \"exc\" , synapse = BCM ) . connect_all_to_all ( weights = 1.0 ) or afterwards: proj = Projection ( pre = pop1 , post = pop2 , target = \"exc\" , synapse = BCM ) proj . connect_all_to_all ( weights = 1.0 ) The connector method must be called before the network is compiled.","title":"Building the projections"},{"location":"manual/Projections.html#projection-attributes","text":"Let's suppose the BCM synapse is used to create the Projection proj (spiking synapses are accessed similarly): BCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 100. : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0 \"\"\" )","title":"Projection attributes"},{"location":"manual/Projections.html#global-attributes","text":"The global parameters and variables of a projection (i.e. defined with the postsynaptic or projection flags) can be accessed directly through attributes. Attributes defined with projection have a single value for the whole population: >>> proj . tau 100 Attributes defined with postsynaptic have one value per post-synaptic neuron, so the result is a vector: >>> proj . theta [ 3.575 , 15.987 , ... , 4.620 ] Post-synaptic variables can be modified by passing: a single value, which will be the same for all post-synaptic neurons. a list of values, with the same size as the number of neurons receiving synapses (for some sparse connectivity patterns, it may not be the same as the size of the population, so no multidimensional array is accepted). After compilation (and therefore creation of the synapses), you can access how many post-synaptic neurons receive actual synapses with: >>> proj . size 4 The list of ranks of the post-synaptic neurons receiving synapses is obtained with: >>> proj . post_ranks [ 0 , 1 , 2 , 3 ]","title":"Global attributes"},{"location":"manual/Projections.html#local-attributes","text":"At the projection level Local attributes can also be accessed globally through attributes. It will return a list of lists containing the synapse-specific values. The first index represents the post-synaptic neurons. It has the same length as proj.post_ranks . Beware that if some post-synaptic neurons do not receive any connection, this index will not correspond to the ranksof the post-synaptic population. The second index addresses the pre-synaptic neurons. If the connection is sparse, it also is unrelated to the ranks of the pre-synaptic neurons in their populations. Warning Modifying these lists of lists is error-prone, so this method should be avoided if possible. At the post-synaptic level The local parameters and variables of a projection (synapse-specific) should better be accessed through the Dendrite object, which gathers for a single post-synaptic neuron all synapses belonging to the projection. Beware: As projections are only instantiated after the call to compile() , local attributes of a Projection are only available then. Trying to access them before compilation will lead to an error! Each dendrite stores the parameters and variables of the corresponding synapses as attributes, as populations do for neurons. You can loop over all post-synaptic neurons receiving synapses with the dendrites iterator: for dendrite in proj . dendrites : print dendrite . pre_ranks print dendrite . size print dendrite . tau print dendrite . alpha print dendrite . w dendrite.pre_ranks returns a list of pre-synaptic neuron ranks. dendrite.size returns the number of synapses for the considered post-synaptic neuron. Global parameters/variables return a single value ( dendrite.tau ) and local ones return a list ( dendrite.w ). You can even omit the .dendrites part of the iterator: for dendrite in proj : print dendrite . pre_ranks print dendrite . size print dendrite . tau print dendrite . alpha print dendrite . w You can also access the dendrites individually, either by specifying the rank of the post-synaptic neuron: dendrite = proj . dendrite ( 13 ) print dendrite . w or its coordinates: dendrite = proj . dendrite ( 5 , 5 ) print dendrite . w When using ranks, you can also directly address the projection as an array: dendrite = proj [ 13 ] print dendrite . w Warning You should make sure that the dendrite actually exists before accessing it through its rank, because it is otherwise a None object.","title":"Local attributes"},{"location":"manual/Projections.html#functions","text":"If you have defined a function inside a Synapse definition: BCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 100. : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic dw/dt = eta * BCMRule(pre.r, post.r, theta) : min=0.0 \"\"\" , functions = \"\"\" BCMRule(pre, post, theta) = post * (post - theta) * pre \"\"\" ) you can use this function in Python as if it were a method of the corresponding object: proj = Projection ( pop1 , pop2 , 'exc' , BCM ) . connect_xxx () pre = np . linspace ( 0. , 1. , 100 ) post = np . linspace ( 0. , 1. , 100 ) theta = 0.01 * np . ones ( 100 ) weight_change = proj . BCMRule ( pre , post , theta ) You can pass either a list or a 1D Numpy array to each argument ( not a single value, nor a multidimensional array! ). The size of the arrays passed for each argument is arbitrary (it must not match the projection's size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.","title":"Functions"},{"location":"manual/Projections.html#connecting-population-views","text":"Projections are usually understood as a connectivity pattern between two populations. Complex connectivity patterns have to specifically designed (see Connector ). In some cases, it can be much simpler to connect subsets of neurons directly, using built-in connector methods. To this end, the Projection object also accepts PopulationView objects for the pre and post arguments. Let's suppose we want to connect the (8,8) populations pop1 and pop2 in a all-to-all manner, but only for the (4,4) neurons in the center of these populations. The first step is to create the PopulationView objects using the slice operator: pop1_center = pop1 [ 2 : 7 , 2 : 7 ] pop2_center = pop2 [ 2 : 7 , 2 : 7 ] They can then be simply used to create a projection: proj = Projection ( pre = pop1_center , post = pop2_center , target = \"exc\" , synapse = BCM ) . connect_all_to_all ( weights = 1.0 ) Each neuron of pop2_center will receive synapses from all neurons of pop1_center , and only them. Neurons of pop2 which are not in pop2_center will not receive any synapse. Warning If you define your own connector method and want to use PopulationViews, you will need to iterate over the ranks attribute of the PopulationView object.","title":"Connecting population views"},{"location":"manual/Projections.html#specifying-delays-in-synaptic-transmission","text":"By default, synaptic transmission is considered to be instantaneous (or more precisely, it takes one simulation step ( dt ) for a newly computed firing rate to be taken into account by post-synaptic neurons). In order to take longer propagation times into account in the transmission of information between two populations, one has the possibility to define synaptic delays for a projection. All the built-in connector methods take an argument delays (default= dt ), which can be a float (in milliseconds) or a random number generator. proj . connect_all_to_all ( weights = 1.0 , delays = 10.0 ) proj . connect_all_to_all ( weights = 1.0 , delays = Uniform ( 1.0 , 10.0 )) If the delay is not a multiple of the simulation time step ( dt = 1.0 by default), it will be rounded to the closest multiple. The same is true for the values returned by a random number generator. Note: Per design, the minimal possible delay is equal to dt : values smaller than dt will be replaced by dt . Negative values do not make any sense and are ignored. Warning Non-uniform delays are not available on CUDA.","title":"Specifying delays in synaptic transmission"},{"location":"manual/Projections.html#controlling-projections","text":"Synaptic transmission, update and plasticity It is possible to selectively control synaptic transmission and plasticity at the projection level. The boolean flags transmission , update and plasticity can be set for that purpose: proj.transmission = False proj.update = False proj.plasticity = False If transmission is False , the projection is totally shut down: it does not transmit any information to the post-synaptic population (the corresponding weighted sums or conductances are constantly 0) and all synaptic variables are frozen to their current value (including the synaptic weights w ). If update is False , synaptic transmission occurs normally, but the synaptic variables are not updated. For spiking synapses, this includes traces when they are computed at each step, but not when they are integrated in an event-driven manner (flag event-driven ). Beware: continous synaptic transmission as in NMDA synapses will not work in this mode, as internal variables are not updated. If only plasticity is False , synaptic transmission and synaptic variable updates occur normally, but changes to the synaptic weight w are ignored. Disabling learning Alternatively, one can use the enable_learning() and disable_learning() methods of Projection . The effect of disable_learning() depends on the type of the projection: for rate-coded projections, disable_learning() is equivalent to update=False : no synaptic variables is updated. for spiking projections, it is equivalent to plasticity=False : only the weights are blocked. The reason of this difference is to allow continuous synaptic transmission and computation of traces. Calling enable_learning() without arguments resumes the default learning behaviour. Periodic learning enable_learning() also accepts two arguments period and offset . period defines the interval in ms between two evaluations of the synaptic variables. This can be useful when learning should only occur once at the end of a trial. It is recommended not to use ODEs in the equations in this case, as they are numerized according to a fixed time step. offset defines the time inside the period at which the evaluation should occur. By default, it is 0, so the variable updates will occur at the next step, then after period ms, and so on. Setting it to -1 will shift the update at the end of the period. Note that spiking synapses using online evaluation will not be affected by these parameters, as they are event-driven.","title":"Controlling projections"},{"location":"manual/Projections.html#multiple-targets","text":"For spiking neurons, it may be desirable that a single synapses activates different currents (or conductances) in the post-synaptic neuron. One example are AMPA/NMDA synapses, where a single spike generates a \\\"classical\\\" AMPA current, plus a voltage-gated slower NMDA current. The following conductance-based Izhikevich is an example: RSNeuron = Neuron ( parameters = \"\"\" a = 0.02 b = 0.2 c = -65. d = 8. tau_ampa = 5. tau_nmda = 150. vrev = 0.0 \"\"\" , equations = \"\"\" I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v) dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init=-65., midpoint du/dt = a * (b*v - u) : init=-13. tau_ampa * dg_ampa/dt = -g_ampa tau_nmda * dg_nmda/dt = -g_nmda \"\"\" , spike = \"\"\" v >= 30. \"\"\" , reset = \"\"\" v = c u += d \"\"\" , functions = \"\"\" nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2) \"\"\" ) However, g_ampa and g_nmda collect by default spikes from different projections, so the weights will not be shared between the \\\"ampa\\\" projection and the \\\"nmda\\\" one. It is therefore possible to specify a list of targets when building a projection, meaning that a single pre-synaptic spike will increase both g_ampa and g_nmda from the same weight: proj = Projection(pop1, pop2, ['ampa', 'nmda'], STDP) An example is provided in examples/homeostatic_stdp/Ramp.py . Warning Multiple targets are not available on CUDA yet.","title":"Multiple targets"},{"location":"manual/RateNeuron.html","text":"Rate-coded neurons # Defining parameters and variables # Let's consider first a simple rate-coded neuron of the leaky-integrator type, which simply integrates the weighted sum of its excitatory inputs: \\[ \\tau \\frac{d \\text{mp}(t)}{dt} = ( B - \\text{mp}(t) ) + \\sum_{i}^{\\text{exc}} \\text{r}_{i} * w_{i} \\] \\[r(t) = ( \\text{mp}(t) )^+\\] where \\(mp(t)\\) represents the membrane potential of the neuron, \\(\\tau\\) the time constant of the neuron, \\(B\\) its baseline firing rate, \\(\\text{r}(t)\\) its instantaneous firing rate, \\(i\\) an index over all excitatory synapses of this neuron, \\(w_i\\) the efficiency of the synapse with the pre-synaptic neuron of firing rate \\(\\text{r}_{i}\\) . It can be implemented in the ANNarchy framework with: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) \"\"\" ) The only required variable is r , which represents the instantaneous firing rate and will be used to propagate activity in the network. All other parameters and variables are freely decided by the user. Custom functions # Custom functions can also be defined when creating the Neuron type and used inside the equations field: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = sigmoid(mp) \"\"\" , functions = \"\"\" sigmoid(x) = 1.0 / (1.0 + exp(-x)) \"\"\" ) Make sure that the name of the function does not overlap with existing mathematical functions (cos, exp), existing variables (tau, r) or built-in functions (pos, t, dt). Predefined attributes # The ODE can depend on other parameters of the neuron (e.g. r depends on mp ), but not on unknown names. ANNarchy already defines the following variables and parameters for a neuron: variable t : time in milliseconds elapsed since the creation of the network. parameter dt : the discretization step, default is 1 ms. Weighted sum of inputs # The sum(target) term gives a direct access to the weighted sum of all inputs to the neuron having the corresponding target (see Projections to see how the target is defined). These inputs are organized in a data structure called Dendrite . It is possible to modify how weighted sums are computed when creating a rate-coded synapse . Note The connection type, e.g. exc or inh , needs to match with the names used as a target parameter when creating a Projection . If such a projection does not exist when the network is compiled, the weighted sum will be set to 0.0 for all neurons. Using only sum() in the equations sums over all defined targets. For example, if two projections with targets \"exc\" and \"inh\" reach a neuron, sum() is equivalent to sum(exc) + sum(inh) . Inhibitory weights must then be defined as negative. Global operations # One has the possibility to use global operations on the population inside the neuron definition, such as the maximal activity in the population. One only needs to use one of the following operations: min(v) for the minimum: \\(\\min_i v_i\\) , max(v) for the maximum: \\(\\max_i v_i\\) , mean(v) for the mean: \\(\\frac{1}{N} \\sum_i v_i\\) , norm1(v) for the L1-norm: \\(\\frac{1}{N} \\sum_i |v_i|\\) , norm2(v) for the L2-norm: \\(\\frac{1}{N} \\sum_i v_i^2\\) Example where neurons react to their inputs only where they exceed the mean over the population: WTANeuron = Neuron( parameters=\"\"\" tau = 10.0 \"\"\", equations = \"\"\" input = sum(exc) tau * dr/dt + r = pos(input - mean(input)) \"\"\" ) Note The global operations are computed using values at the previous time step (like weighted sums), not in the step currently evaluated. There is therefore implicitely a delay of dt , but it cannot be changed.","title":"Rate-coded neurons"},{"location":"manual/RateNeuron.html#rate-coded-neurons","text":"","title":"Rate-coded neurons"},{"location":"manual/RateNeuron.html#defining-parameters-and-variables","text":"Let's consider first a simple rate-coded neuron of the leaky-integrator type, which simply integrates the weighted sum of its excitatory inputs: \\[ \\tau \\frac{d \\text{mp}(t)}{dt} = ( B - \\text{mp}(t) ) + \\sum_{i}^{\\text{exc}} \\text{r}_{i} * w_{i} \\] \\[r(t) = ( \\text{mp}(t) )^+\\] where \\(mp(t)\\) represents the membrane potential of the neuron, \\(\\tau\\) the time constant of the neuron, \\(B\\) its baseline firing rate, \\(\\text{r}(t)\\) its instantaneous firing rate, \\(i\\) an index over all excitatory synapses of this neuron, \\(w_i\\) the efficiency of the synapse with the pre-synaptic neuron of firing rate \\(\\text{r}_{i}\\) . It can be implemented in the ANNarchy framework with: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) \"\"\" ) The only required variable is r , which represents the instantaneous firing rate and will be used to propagate activity in the network. All other parameters and variables are freely decided by the user.","title":"Defining parameters and variables"},{"location":"manual/RateNeuron.html#custom-functions","text":"Custom functions can also be defined when creating the Neuron type and used inside the equations field: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = sigmoid(mp) \"\"\" , functions = \"\"\" sigmoid(x) = 1.0 / (1.0 + exp(-x)) \"\"\" ) Make sure that the name of the function does not overlap with existing mathematical functions (cos, exp), existing variables (tau, r) or built-in functions (pos, t, dt).","title":"Custom functions"},{"location":"manual/RateNeuron.html#predefined-attributes","text":"The ODE can depend on other parameters of the neuron (e.g. r depends on mp ), but not on unknown names. ANNarchy already defines the following variables and parameters for a neuron: variable t : time in milliseconds elapsed since the creation of the network. parameter dt : the discretization step, default is 1 ms.","title":"Predefined attributes"},{"location":"manual/RateNeuron.html#weighted-sum-of-inputs","text":"The sum(target) term gives a direct access to the weighted sum of all inputs to the neuron having the corresponding target (see Projections to see how the target is defined). These inputs are organized in a data structure called Dendrite . It is possible to modify how weighted sums are computed when creating a rate-coded synapse . Note The connection type, e.g. exc or inh , needs to match with the names used as a target parameter when creating a Projection . If such a projection does not exist when the network is compiled, the weighted sum will be set to 0.0 for all neurons. Using only sum() in the equations sums over all defined targets. For example, if two projections with targets \"exc\" and \"inh\" reach a neuron, sum() is equivalent to sum(exc) + sum(inh) . Inhibitory weights must then be defined as negative.","title":"Weighted sum of inputs"},{"location":"manual/RateNeuron.html#global-operations","text":"One has the possibility to use global operations on the population inside the neuron definition, such as the maximal activity in the population. One only needs to use one of the following operations: min(v) for the minimum: \\(\\min_i v_i\\) , max(v) for the maximum: \\(\\max_i v_i\\) , mean(v) for the mean: \\(\\frac{1}{N} \\sum_i v_i\\) , norm1(v) for the L1-norm: \\(\\frac{1}{N} \\sum_i |v_i|\\) , norm2(v) for the L2-norm: \\(\\frac{1}{N} \\sum_i v_i^2\\) Example where neurons react to their inputs only where they exceed the mean over the population: WTANeuron = Neuron( parameters=\"\"\" tau = 10.0 \"\"\", equations = \"\"\" input = sum(exc) tau * dr/dt + r = pos(input - mean(input)) \"\"\" ) Note The global operations are computed using values at the previous time step (like weighted sums), not in the step currently evaluated. There is therefore implicitely a delay of dt , but it cannot be changed.","title":"Global operations"},{"location":"manual/RateSynapse.html","text":"Rate-coded synapses # As for neurons, you can define the synaptic behavior using a Synapse object. Although the description is local to a synapse, the same ODE will be applied to all synapses of a given Projection from one population to another. The same vocabulary as for neurons is accessible (constants, functions, conditional statements), except that the synapse must distinguish pre-synaptic and post-synaptic parameters/variables. Like r for a rate-coded neuron, one variable is special for a rate-coded synapse: w represents the synaptic efficiency (or the weight of the connection). If an ODE is defined for this variable, this will implement a learning rule. If none is provided, the synapse is non-plastic. The ODEs for synaptic variables follow the same syntax as for neurons. As for neurons, the following variables are already defined: t : time in milliseconds elapsed since the creation of the network. dt : the discretization step is 1.0ms by default. Synaptic plasticity # Learning is possible by modifying the variable w of a single synapse during the simulation. For example, the Oja learning rule (see the example Bar learning ): \\[\\tau \\frac{d w(t)}{dt} = r_\\text{pre} * r_\\text{post} - \\alpha * r_\\text{post}^2 * w(t)\\] could be implemented this way: Oja = Synapse ( parameters = \"\"\" tau = 5000 alpha = 8.0 \"\"\" , equations = \"\"\" tau * dw / dt = pre.r * post.r - alpha * post.r^2 * w \"\"\" ) Note that it is equivalent to define the increment directly if you want to apply the explicit Euler method: equations = \"\"\" w += dt / tau * ( pre.r * post.r - alpha * post.r^2 * w) \"\"\" The same vocabulary as for rate-coded neurons applies. Custom functions can also be defined: Oja = Synapse ( parameters = \"\"\" tau = 5000 alpha = 8.0 \"\"\" , equations = \"\"\" tau * dw / dt = product(pre.r, post.r) - alpha * post.r^2 * w \"\"\" , functions = \"\"\" product(x,y) = x * y \"\"\" , ) Neuron-specific variables # A synapse needs to access neural variables both at the pre- and post-synaptic levels. For the pre-synaptic neuron, biologically realistic synapses should only need its firing rate, but in some cases it may be useful to access other variables as well. In order to use neural variables in a synaptic variable, you have to prefix them with pre. or post. . For example: pre . r , post . baseline , post . mp ... ANNarchy will check before the compilation that the pre- or post-synaptic neuron types indeed define such variables. Note If the projection uses delays, all pre-synaptic variables used in the synapse model will be delayed. Locality # There are 3 levels of locality for a synaptic parameter or variable: synaptic : there is one value per synapse in the projection (default). postsynaptic : there is one value per post-synaptic neuron in the projection. projection : there is only one value for the whole projection. The following BCM learning rule makes use of the three levels of locality: BCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 100. : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0 \"\"\" ) eta and tau are global parameters to the projection: all synapses will use the same value. theta defines one value per post-synaptic neuron: it tracks the average of the post-synaptic firing rate. There is therefore no need to update one value per synapse, so we can use the flag \\\"postsynaptic\\\". Naturally, w is local to each synapse, so no locality flag should be passed. Global operations # Some learning rules require global information about the pre- or post-synaptic population, which is not local to the synapse, such as the mean or maximal activity in the pre-synaptic population. This information can be accessed at the synapse-level. The special functions: min(v) for the minimum: \\(\\min_i v_i\\) , max(v) for the maximum: \\(\\max_i v_i\\) , mean(v) for the mean: \\(\\frac{1}{N} \\sum_i v_i\\) , norm1(v) for the L1-norm: \\(\\frac{1}{N} \\sum_i |v_i|\\) , norm2(v) for the L2-norm: \\(\\frac{1}{N} \\sum_i v_i^2\\) are available for any pre- or post-synaptic variable. For example, some covariance-based learning rules depend on the mean firing in the pre- and post-synaptic populations: \\[\\tau \\frac{d w(t)}{dt} = (r_\\text{pre} - \\hat{r}_\\text{pre} ) * (r_\\text{post} - \\hat{r}_\\text{post} )\\] Using the global operations, such a learning rule is trivial to implement: Covariance = Synapse ( parameters = \"\"\" tau = 5000.0 \"\"\" , equations = \"\"\" tau * dw/dt = (pre.r - mean(pre.r) ) * (post.r - mean(post.r) ) \"\"\" ) Warning Such global operations can become expensive to compute if the populations are too big. The global operations are performed over the whole population, not only the synapses which actually reach the post-synaptic neuron. They can only be applied to a single variable, not a combination or function of them. Defining the post-synaptic potential (psp) # The argument psp of a Synapse object represents the post-synaptic potential evoked by the pre-synaptic neuron. This value is actually summed by the post-synaptic neuron over all other synapses of the same projection in sum(target) . If not defined, it will simply represent the product between the pre-synaptic firing rate ( pre.r ) and the weight value ( w ). The post-synaptic potential of a single synapse is by default: psp = w * pre . r where pre.r is the pre-synaptic firing rate, but you may want to override this behaviour in certain cases. For example, you may want to model a non-linear synapse with a logarithmic term: \\[r_{i} = \\sum_j log \\left( \\frac {( r_{j} * w_{ij} ) + 1 } { ( r_{j} * w_{ij} ) - 1 } \\right)\\] In this case, you can just modify the psp argument of the synapse: NonLinearSynapse = Synapse ( psp = \"\"\" log( (pre.r * w + 1 ) / (pre.r * w - 1) ) \"\"\" ) No further modification has to be done in the post-synaptic neuron, this value will be summed over all pre-synaptic neurons automatically when using sum(target) . Defining the post-synaptic operation # By default, a post-synaptic neuron calling sum(target) will compute the sum over all incoming synapses of their defined psp : \\[\\text{sum(exc)} = \\sum_{i \\in \\text{exc}} \\text{psp}(i) = \\sum_{i \\in \\text{exc}} w_i * \\text{pre}.r_i\\] It is possible to define a different operation performed on the connected synapses, using the operation argument of the synapse: MaxPooling = Synapse ( psp = \"w * pre.r\" , operation = \"max\" ) In this case, sum(target) will represent the maximum value of w * pre.r over all incoming synapses, not their sum. It can be useful when defining pooling operations in a convolutional network, for example. The available operations are: \"sum\" : (default): sum of all incoming psps. \"max\" : maximum of all incoming psps. \"min\" : minimum of all incoming psps. \"mean\" : mean of all incoming psps. Warning These operations are only possible for rate-coded synapses.","title":"Rate-coded synapses"},{"location":"manual/RateSynapse.html#rate-coded-synapses","text":"As for neurons, you can define the synaptic behavior using a Synapse object. Although the description is local to a synapse, the same ODE will be applied to all synapses of a given Projection from one population to another. The same vocabulary as for neurons is accessible (constants, functions, conditional statements), except that the synapse must distinguish pre-synaptic and post-synaptic parameters/variables. Like r for a rate-coded neuron, one variable is special for a rate-coded synapse: w represents the synaptic efficiency (or the weight of the connection). If an ODE is defined for this variable, this will implement a learning rule. If none is provided, the synapse is non-plastic. The ODEs for synaptic variables follow the same syntax as for neurons. As for neurons, the following variables are already defined: t : time in milliseconds elapsed since the creation of the network. dt : the discretization step is 1.0ms by default.","title":"Rate-coded synapses"},{"location":"manual/RateSynapse.html#synaptic-plasticity","text":"Learning is possible by modifying the variable w of a single synapse during the simulation. For example, the Oja learning rule (see the example Bar learning ): \\[\\tau \\frac{d w(t)}{dt} = r_\\text{pre} * r_\\text{post} - \\alpha * r_\\text{post}^2 * w(t)\\] could be implemented this way: Oja = Synapse ( parameters = \"\"\" tau = 5000 alpha = 8.0 \"\"\" , equations = \"\"\" tau * dw / dt = pre.r * post.r - alpha * post.r^2 * w \"\"\" ) Note that it is equivalent to define the increment directly if you want to apply the explicit Euler method: equations = \"\"\" w += dt / tau * ( pre.r * post.r - alpha * post.r^2 * w) \"\"\" The same vocabulary as for rate-coded neurons applies. Custom functions can also be defined: Oja = Synapse ( parameters = \"\"\" tau = 5000 alpha = 8.0 \"\"\" , equations = \"\"\" tau * dw / dt = product(pre.r, post.r) - alpha * post.r^2 * w \"\"\" , functions = \"\"\" product(x,y) = x * y \"\"\" , )","title":"Synaptic plasticity"},{"location":"manual/RateSynapse.html#neuron-specific-variables","text":"A synapse needs to access neural variables both at the pre- and post-synaptic levels. For the pre-synaptic neuron, biologically realistic synapses should only need its firing rate, but in some cases it may be useful to access other variables as well. In order to use neural variables in a synaptic variable, you have to prefix them with pre. or post. . For example: pre . r , post . baseline , post . mp ... ANNarchy will check before the compilation that the pre- or post-synaptic neuron types indeed define such variables. Note If the projection uses delays, all pre-synaptic variables used in the synapse model will be delayed.","title":"Neuron-specific variables"},{"location":"manual/RateSynapse.html#locality","text":"There are 3 levels of locality for a synaptic parameter or variable: synaptic : there is one value per synapse in the projection (default). postsynaptic : there is one value per post-synaptic neuron in the projection. projection : there is only one value for the whole projection. The following BCM learning rule makes use of the three levels of locality: BCM = Synapse ( parameters = \"\"\" eta = 0.01 : projection tau = 100. : projection \"\"\" , equations = \"\"\" tau * dtheta/dt + theta = post.r^2 : postsynaptic dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0 \"\"\" ) eta and tau are global parameters to the projection: all synapses will use the same value. theta defines one value per post-synaptic neuron: it tracks the average of the post-synaptic firing rate. There is therefore no need to update one value per synapse, so we can use the flag \\\"postsynaptic\\\". Naturally, w is local to each synapse, so no locality flag should be passed.","title":"Locality"},{"location":"manual/RateSynapse.html#global-operations","text":"Some learning rules require global information about the pre- or post-synaptic population, which is not local to the synapse, such as the mean or maximal activity in the pre-synaptic population. This information can be accessed at the synapse-level. The special functions: min(v) for the minimum: \\(\\min_i v_i\\) , max(v) for the maximum: \\(\\max_i v_i\\) , mean(v) for the mean: \\(\\frac{1}{N} \\sum_i v_i\\) , norm1(v) for the L1-norm: \\(\\frac{1}{N} \\sum_i |v_i|\\) , norm2(v) for the L2-norm: \\(\\frac{1}{N} \\sum_i v_i^2\\) are available for any pre- or post-synaptic variable. For example, some covariance-based learning rules depend on the mean firing in the pre- and post-synaptic populations: \\[\\tau \\frac{d w(t)}{dt} = (r_\\text{pre} - \\hat{r}_\\text{pre} ) * (r_\\text{post} - \\hat{r}_\\text{post} )\\] Using the global operations, such a learning rule is trivial to implement: Covariance = Synapse ( parameters = \"\"\" tau = 5000.0 \"\"\" , equations = \"\"\" tau * dw/dt = (pre.r - mean(pre.r) ) * (post.r - mean(post.r) ) \"\"\" ) Warning Such global operations can become expensive to compute if the populations are too big. The global operations are performed over the whole population, not only the synapses which actually reach the post-synaptic neuron. They can only be applied to a single variable, not a combination or function of them.","title":"Global operations"},{"location":"manual/RateSynapse.html#defining-the-post-synaptic-potential-psp","text":"The argument psp of a Synapse object represents the post-synaptic potential evoked by the pre-synaptic neuron. This value is actually summed by the post-synaptic neuron over all other synapses of the same projection in sum(target) . If not defined, it will simply represent the product between the pre-synaptic firing rate ( pre.r ) and the weight value ( w ). The post-synaptic potential of a single synapse is by default: psp = w * pre . r where pre.r is the pre-synaptic firing rate, but you may want to override this behaviour in certain cases. For example, you may want to model a non-linear synapse with a logarithmic term: \\[r_{i} = \\sum_j log \\left( \\frac {( r_{j} * w_{ij} ) + 1 } { ( r_{j} * w_{ij} ) - 1 } \\right)\\] In this case, you can just modify the psp argument of the synapse: NonLinearSynapse = Synapse ( psp = \"\"\" log( (pre.r * w + 1 ) / (pre.r * w - 1) ) \"\"\" ) No further modification has to be done in the post-synaptic neuron, this value will be summed over all pre-synaptic neurons automatically when using sum(target) .","title":"Defining the post-synaptic potential (psp)"},{"location":"manual/RateSynapse.html#defining-the-post-synaptic-operation","text":"By default, a post-synaptic neuron calling sum(target) will compute the sum over all incoming synapses of their defined psp : \\[\\text{sum(exc)} = \\sum_{i \\in \\text{exc}} \\text{psp}(i) = \\sum_{i \\in \\text{exc}} w_i * \\text{pre}.r_i\\] It is possible to define a different operation performed on the connected synapses, using the operation argument of the synapse: MaxPooling = Synapse ( psp = \"w * pre.r\" , operation = \"max\" ) In this case, sum(target) will represent the maximum value of w * pre.r over all incoming synapses, not their sum. It can be useful when defining pooling operations in a convolutional network, for example. The available operations are: \"sum\" : (default): sum of all incoming psps. \"max\" : maximum of all incoming psps. \"min\" : minimum of all incoming psps. \"mean\" : mean of all incoming psps. Warning These operations are only possible for rate-coded synapses.","title":"Defining the post-synaptic operation"},{"location":"manual/Recording.html","text":"Recording with Monitors # Between two calls to simulate() , all neural and synaptic variables can be accessed through the generated attributes. The evolution of neural or synaptic variables during a simulation phase can be selectively recorded using Monitor objects. The Monitor object can be created at any time (before or after compile() ) to record any variable of a Population , PopulationView , Dendrite or Projection . Note The value of each variable is stored for every simulation step in the RAM. For huge networks and long simulations, this can very rapidly fill up the available memory and lead to cache defaults, thereby degrading strongly the performance. It is the user's responsability to record only the needed variables and to regularly save the values in a file. Neural variables # The Monitor object takes four arguments: obj : the object to monitor. It can be a population, a population view (a slice of a population or an individual neuron), a dendrite (the synapses of a projection which reach a single post-synaptic neuron) or a projection. variables : a (list of) variable name(s) which should be recorded. They should be variables of the neuron/synapse model of the corresponding object. Although it generally makes no sense, you can also record parameters of an object. By definition a parameter is constant throughout a simulation, but it maybe useful when tracking externally-set inputs, for example. You can know which attributes are recordable by checking the attributes attribute of the object ( pop.attributes or proj.attributes ). period : the period in ms at which recordings should be made. By default, recording is done after each simulation step ( dt ), but this may be overkill in long simulations. start : boolean value stating if the recordings should start immediately after the creation of the monitor (default), or if it should be started later. Some examples: m = Monitor ( pop , 'r' ) # record r in all neurons of pop m = Monitor ( pop , [ 'r' , 'v' ]) # record r and v of all neurons m = Monitor ( pop [: 100 ], 'r' , period = 10.0 ) # record r in the first 100 neurons of pop, every 10 ms m = Monitor ( pop , 'r' , start = False ) # record r in all neurons, but do not start recording Spiking networks additionally allow to record the spike events in a population (see later). You also can record conductances (e.g. g_exc ) and weighted sums of inputs in rate-coded networks ( sum(exc) ) the same way: m = Monitor ( pop , [ 'spike' , 'g_exc' , 'g_inh' ]) m = Monitor ( pop , [ 'r' , 'sum(exc)' , 'sum(inh)' ]) Starting the recordings # If start is set to False , recordings can be started later by calling the start() method: m = Monitor ( pop , 'r' , start = False ) simulate ( 100. ) m . start () simulate ( 100. ) In this case, only the last 100 ms of the simulation will be recorded. Otherwise, recording would start immediately after the creation of the object. Pausing/resuming the recordings # If you are interested in recording only specific periods of the simulation, you can ause and resume recordings: m = Monitor ( pop , 'r' ) simulate ( 100. ) m . pause () simulate ( 1000. ) m . resume () simulate ( 100. ) In this example, only the first and last 100 ms of the simulation are recorded. Retrieving the recordings # The recorded values are obtained through the get() method. If no argument is passed, a dictionary is returned with one element per recorded variable. If the name of a variable is passed (for example get('r') ), the recorded values for this variable are directly returned: m = Monitor ( pop , [ 'r' , 'v' ]) simulate ( 100. ) data = m . get () simulate ( 100. ) r = m . get ( 'r' ) v = m . get ( 'v' ) In the example above, data is a dictionary with two keys 'r' and 'v' , while r and v are directly the recorded arrays. The recorded values are Numpy arrays with two dimensions, the first one representing time , the second one representing the ranks of the recorded neurons. For example, the time course of the firing rate of the neuron of rank 15 is accessed through: data [ 'r' ][:, 15 ] The firing rates of the whole population after 50 ms of simulation are accessed with: data [ 'r' ][ 50 , :] Note Once you call get() , the internal data is erased, so calling it immediately afterwards will return an empty recording data. You need to simulate again in order to retrieve new values. Representation of time The time indices are in simulation steps (integers), not in real time (ms). If dt is different from 1.0, this indices must be multiplied by dt() in order to plot real times: setup ( dt = 0.1 ) # ... m = Monitor ( pop , 'r' ) simulate ( 100. ) r = m . get ( 'r' ) plt . plot ( dt () * np . arange ( 100 ), r [:, 15 ]) If recordings used the pause() and resume() methods, get() returns only one array with all recordings concatenated. You can access the steps at which the recording started or paused with the times() method: m = Monitor ( pop , 'r' ) simulate ( 100. ) m . pause () simulate ( 1000. ) m . resume () simulate ( 100. ) r = m . get ( 'r' ) # A (200, N) Numpy array print ( m . times ()) # {'start': [0, 1100], 'stop': [100, 1200]} Special case for spiking neurons # Any variable defined in the neuron type can be recorded. An exception for spiking neurons is the spike variable itself, which is never explicitely defined in the neuron type but can be recorded: m = Monitor ( pop , [ 'v' , 'spike' ]) Unlike other variables, the binary value of spike is not recorded at each time step, which would lead to very sparse matrices, but only the times (in steps, not milliseconds) at which spikes actually occur. As each neuron fires differently (so each neuron will have recorded spikes of different lengths), get() in this case does not return a Numpy array, but a dictionary associating to each recorded neuron a list of spike times: m = Monitor ( pop , [ 'v' , 'spike' ]) simulate ( 100.0 ) data = m . get ( 'spike' ) print ( data [ 0 ]) # [23, 76, 98] In the example above, the neuron of rank 0 has spiked 3 times (at t = 23, 76 and 98 ms if dt = 1.0 ) during the first 100 ms of the simulation. Raster plots In order to easily display raster plots, the method raster_plot() is provided to transform this data into an easily plottable format: spike_times , ranks = m . raster_plot ( data ) plt . plot ( spike_times , ranks , '.' ) raster_plot() returns two Numpy arrays, whose length is the total number of spikes emitted during the simulation. The first array contains the spike times (\u00edn ms) while the second contains the ranks of the neurons who fired. They can be directly used t produce the raster plot with Matplotlib. An example of the use of raster_plot() can be seen in the Izhikevich pulse network section. Mean firing rate The mean firing rate in the population can be easily calculated using the length of the arrays returned by raster_plot : N = 1000 # number of neurons duration = 500. # duration of the simulation data = m . get ( 'spike' ) spike_times , ranks = m . raster_plot ( data ) print ( 'Mean firing rate:' , len ( spike_times ) / float ( N ) / duration * 1000. , 'Hz.' ) For convenience, this value is returned by the mean_fr() method, which has access to the number of recorded neurons and the duration of the recordings: print ( 'Mean firing rate:' , m . mean_fr ( data ), 'Hz.' ) Firing rates Another useful method is smoothed_rate() . It allows to display the instantaneous firing rate of each neuron based on the spike recordings: rates = m . smoothed_rate ( data ) plt . imshow ( rates , aspect = 'auto' ) For each neuron, it returns an array with the instantaneous firing rate during the whole simulation. The instantaneous firing rate is computed by inverting the inter-spike interval (ISI) between two consecutive spikes, and assigning it to all simulation steps between the two spikes. As this value can be quite fluctuating, a smooth argument in milliseconds can be passed to smoothed_rate() to apply a low-pass filter on the firing rates: rates = m . smoothed_rate ( data , smooth = 200.0 ) plt . imshow ( rates , aspect = 'auto' ) A smoothed firing rate for the whole population is also accessible through population_rate() : fr = m . population_rate ( data , smooth = 200.0 ) Histogram histogram() allows to count the spikes emitted in the whole population during successive bins of the recording duration: histo = m . histogram ( data , bins = 1.0 ) plt . plot ( histo ) bins represents the size of each bin, here 1 ms. By default, the bin size is dt . Note : the methods to analyse the spike patterns are also available outside the monitors. For example if you save the spike recordings into a file using numpy: spikes = m . get ( 'spike' ) np . save ( 'spikes.npy' , spikes ) you can analyze them in a separate file like this: # Load the data spikes = np . load ( 'spikes.npy' ) . item () # Compute the raster plot t , n = raster_plot ( spikes ) # Compute the population firing rate fr = histogram ( spikes , bins = 1. ) # Smoothed firing rate sr = smoothed_rate ( spikes , smooth = 10.0 ) # Population firing rate pr = population_rate ( spikes , smooth = 10.0 ) # Global firing rate mfr = mean_fr ( spikes ) Synaptic variables # Recording of synaptic variables such as weights w during learning is also possible using the monitor object. However, it can very easily lead to important memory consumption. Let's suppose we have a network composed of two populations of 1000 neurons each, fully connected: each neuron of the second population receives 1000 synapses. This makes a total of 1 million synapses for the projection and, supposing the weights w use the double floating precision, requires 4 MB of memory. If you record w during a simulation of 1 second (1000 steps, with dt=1.0 ), the total added memory consumption would already be around 4GB. To avoid fast memory fills, you should either 1) record the projection variables infrequently (by setting the period argument of the Monitor), or 2) selectively record particular dendrites. The corresponding dendrite should be simply passed to the monitor: dendrite = proj . dendrite ( 12 ) # or simply proj[12] m = Monitor ( dendrite , 'w' ) simulate ( 1000.0 ) data = m . get ( 'w' ) The Monitor object has the same start() , pause() , resume() and get() methods as for populations. get() returns also 2D Numpy arrays, the first index being time, the second being the index of the synapse. To know to which pre-synaptic neuron it corresponds, use the pre_ranks attribute of the dendrite: dendrite . pre_ranks # [0, 3, 12, ..] To record a complete projection, simply pass it to the Monitor: m = Monitor ( proj , 'w' , period = 1000. ) simulate ( 10000.0 ) data = m . get ( 'w' ) One last time, do not record all weights of a projection at each time step! Warning Recording synaptic variables with CUDA is not available.","title":"Recording with Monitors"},{"location":"manual/Recording.html#recording-with-monitors","text":"Between two calls to simulate() , all neural and synaptic variables can be accessed through the generated attributes. The evolution of neural or synaptic variables during a simulation phase can be selectively recorded using Monitor objects. The Monitor object can be created at any time (before or after compile() ) to record any variable of a Population , PopulationView , Dendrite or Projection . Note The value of each variable is stored for every simulation step in the RAM. For huge networks and long simulations, this can very rapidly fill up the available memory and lead to cache defaults, thereby degrading strongly the performance. It is the user's responsability to record only the needed variables and to regularly save the values in a file.","title":"Recording with Monitors"},{"location":"manual/Recording.html#neural-variables","text":"The Monitor object takes four arguments: obj : the object to monitor. It can be a population, a population view (a slice of a population or an individual neuron), a dendrite (the synapses of a projection which reach a single post-synaptic neuron) or a projection. variables : a (list of) variable name(s) which should be recorded. They should be variables of the neuron/synapse model of the corresponding object. Although it generally makes no sense, you can also record parameters of an object. By definition a parameter is constant throughout a simulation, but it maybe useful when tracking externally-set inputs, for example. You can know which attributes are recordable by checking the attributes attribute of the object ( pop.attributes or proj.attributes ). period : the period in ms at which recordings should be made. By default, recording is done after each simulation step ( dt ), but this may be overkill in long simulations. start : boolean value stating if the recordings should start immediately after the creation of the monitor (default), or if it should be started later. Some examples: m = Monitor ( pop , 'r' ) # record r in all neurons of pop m = Monitor ( pop , [ 'r' , 'v' ]) # record r and v of all neurons m = Monitor ( pop [: 100 ], 'r' , period = 10.0 ) # record r in the first 100 neurons of pop, every 10 ms m = Monitor ( pop , 'r' , start = False ) # record r in all neurons, but do not start recording Spiking networks additionally allow to record the spike events in a population (see later). You also can record conductances (e.g. g_exc ) and weighted sums of inputs in rate-coded networks ( sum(exc) ) the same way: m = Monitor ( pop , [ 'spike' , 'g_exc' , 'g_inh' ]) m = Monitor ( pop , [ 'r' , 'sum(exc)' , 'sum(inh)' ])","title":"Neural variables"},{"location":"manual/Recording.html#starting-the-recordings","text":"If start is set to False , recordings can be started later by calling the start() method: m = Monitor ( pop , 'r' , start = False ) simulate ( 100. ) m . start () simulate ( 100. ) In this case, only the last 100 ms of the simulation will be recorded. Otherwise, recording would start immediately after the creation of the object.","title":"Starting the recordings"},{"location":"manual/Recording.html#pausingresuming-the-recordings","text":"If you are interested in recording only specific periods of the simulation, you can ause and resume recordings: m = Monitor ( pop , 'r' ) simulate ( 100. ) m . pause () simulate ( 1000. ) m . resume () simulate ( 100. ) In this example, only the first and last 100 ms of the simulation are recorded.","title":"Pausing/resuming the recordings"},{"location":"manual/Recording.html#retrieving-the-recordings","text":"The recorded values are obtained through the get() method. If no argument is passed, a dictionary is returned with one element per recorded variable. If the name of a variable is passed (for example get('r') ), the recorded values for this variable are directly returned: m = Monitor ( pop , [ 'r' , 'v' ]) simulate ( 100. ) data = m . get () simulate ( 100. ) r = m . get ( 'r' ) v = m . get ( 'v' ) In the example above, data is a dictionary with two keys 'r' and 'v' , while r and v are directly the recorded arrays. The recorded values are Numpy arrays with two dimensions, the first one representing time , the second one representing the ranks of the recorded neurons. For example, the time course of the firing rate of the neuron of rank 15 is accessed through: data [ 'r' ][:, 15 ] The firing rates of the whole population after 50 ms of simulation are accessed with: data [ 'r' ][ 50 , :] Note Once you call get() , the internal data is erased, so calling it immediately afterwards will return an empty recording data. You need to simulate again in order to retrieve new values. Representation of time The time indices are in simulation steps (integers), not in real time (ms). If dt is different from 1.0, this indices must be multiplied by dt() in order to plot real times: setup ( dt = 0.1 ) # ... m = Monitor ( pop , 'r' ) simulate ( 100. ) r = m . get ( 'r' ) plt . plot ( dt () * np . arange ( 100 ), r [:, 15 ]) If recordings used the pause() and resume() methods, get() returns only one array with all recordings concatenated. You can access the steps at which the recording started or paused with the times() method: m = Monitor ( pop , 'r' ) simulate ( 100. ) m . pause () simulate ( 1000. ) m . resume () simulate ( 100. ) r = m . get ( 'r' ) # A (200, N) Numpy array print ( m . times ()) # {'start': [0, 1100], 'stop': [100, 1200]}","title":"Retrieving the recordings"},{"location":"manual/Recording.html#special-case-for-spiking-neurons","text":"Any variable defined in the neuron type can be recorded. An exception for spiking neurons is the spike variable itself, which is never explicitely defined in the neuron type but can be recorded: m = Monitor ( pop , [ 'v' , 'spike' ]) Unlike other variables, the binary value of spike is not recorded at each time step, which would lead to very sparse matrices, but only the times (in steps, not milliseconds) at which spikes actually occur. As each neuron fires differently (so each neuron will have recorded spikes of different lengths), get() in this case does not return a Numpy array, but a dictionary associating to each recorded neuron a list of spike times: m = Monitor ( pop , [ 'v' , 'spike' ]) simulate ( 100.0 ) data = m . get ( 'spike' ) print ( data [ 0 ]) # [23, 76, 98] In the example above, the neuron of rank 0 has spiked 3 times (at t = 23, 76 and 98 ms if dt = 1.0 ) during the first 100 ms of the simulation. Raster plots In order to easily display raster plots, the method raster_plot() is provided to transform this data into an easily plottable format: spike_times , ranks = m . raster_plot ( data ) plt . plot ( spike_times , ranks , '.' ) raster_plot() returns two Numpy arrays, whose length is the total number of spikes emitted during the simulation. The first array contains the spike times (\u00edn ms) while the second contains the ranks of the neurons who fired. They can be directly used t produce the raster plot with Matplotlib. An example of the use of raster_plot() can be seen in the Izhikevich pulse network section. Mean firing rate The mean firing rate in the population can be easily calculated using the length of the arrays returned by raster_plot : N = 1000 # number of neurons duration = 500. # duration of the simulation data = m . get ( 'spike' ) spike_times , ranks = m . raster_plot ( data ) print ( 'Mean firing rate:' , len ( spike_times ) / float ( N ) / duration * 1000. , 'Hz.' ) For convenience, this value is returned by the mean_fr() method, which has access to the number of recorded neurons and the duration of the recordings: print ( 'Mean firing rate:' , m . mean_fr ( data ), 'Hz.' ) Firing rates Another useful method is smoothed_rate() . It allows to display the instantaneous firing rate of each neuron based on the spike recordings: rates = m . smoothed_rate ( data ) plt . imshow ( rates , aspect = 'auto' ) For each neuron, it returns an array with the instantaneous firing rate during the whole simulation. The instantaneous firing rate is computed by inverting the inter-spike interval (ISI) between two consecutive spikes, and assigning it to all simulation steps between the two spikes. As this value can be quite fluctuating, a smooth argument in milliseconds can be passed to smoothed_rate() to apply a low-pass filter on the firing rates: rates = m . smoothed_rate ( data , smooth = 200.0 ) plt . imshow ( rates , aspect = 'auto' ) A smoothed firing rate for the whole population is also accessible through population_rate() : fr = m . population_rate ( data , smooth = 200.0 ) Histogram histogram() allows to count the spikes emitted in the whole population during successive bins of the recording duration: histo = m . histogram ( data , bins = 1.0 ) plt . plot ( histo ) bins represents the size of each bin, here 1 ms. By default, the bin size is dt . Note : the methods to analyse the spike patterns are also available outside the monitors. For example if you save the spike recordings into a file using numpy: spikes = m . get ( 'spike' ) np . save ( 'spikes.npy' , spikes ) you can analyze them in a separate file like this: # Load the data spikes = np . load ( 'spikes.npy' ) . item () # Compute the raster plot t , n = raster_plot ( spikes ) # Compute the population firing rate fr = histogram ( spikes , bins = 1. ) # Smoothed firing rate sr = smoothed_rate ( spikes , smooth = 10.0 ) # Population firing rate pr = population_rate ( spikes , smooth = 10.0 ) # Global firing rate mfr = mean_fr ( spikes )","title":"Special case for spiking neurons"},{"location":"manual/Recording.html#synaptic-variables","text":"Recording of synaptic variables such as weights w during learning is also possible using the monitor object. However, it can very easily lead to important memory consumption. Let's suppose we have a network composed of two populations of 1000 neurons each, fully connected: each neuron of the second population receives 1000 synapses. This makes a total of 1 million synapses for the projection and, supposing the weights w use the double floating precision, requires 4 MB of memory. If you record w during a simulation of 1 second (1000 steps, with dt=1.0 ), the total added memory consumption would already be around 4GB. To avoid fast memory fills, you should either 1) record the projection variables infrequently (by setting the period argument of the Monitor), or 2) selectively record particular dendrites. The corresponding dendrite should be simply passed to the monitor: dendrite = proj . dendrite ( 12 ) # or simply proj[12] m = Monitor ( dendrite , 'w' ) simulate ( 1000.0 ) data = m . get ( 'w' ) The Monitor object has the same start() , pause() , resume() and get() methods as for populations. get() returns also 2D Numpy arrays, the first index being time, the second being the index of the synapse. To know to which pre-synaptic neuron it corresponds, use the pre_ranks attribute of the dendrite: dendrite . pre_ranks # [0, 3, 12, ..] To record a complete projection, simply pass it to the Monitor: m = Monitor ( proj , 'w' , period = 1000. ) simulate ( 10000.0 ) data = m . get ( 'w' ) One last time, do not record all weights of a projection at each time step! Warning Recording synaptic variables with CUDA is not available.","title":"Synaptic variables"},{"location":"manual/Reporting.html","text":"Reporting # ANNarchy includes an utility allowing to automatically generate a report based on the current structure of the network: report ( filename = \"model_description.tex\" ) report ( filename = \"model_description.md\" ) If the filename ends with .tex , the LaTeX report will be generated based on the specifications provided in: Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8):e1000456. If the filename ends with .md , the report will be generated in Markdown, so it can later be exported to pdf or html using pandoc . report() accepts several arguments: filename : name of the file where the report will be written (default: \\\"./report.tex\\\") standalone : tells if the generated TeX file should be directly compilable or only includable. Ignored in Markdown. gather_subprojections : if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False). title : title of the document (Markdown only) author : author of the document (Markdown only) date : date of the document (Markdown only) net_id : id of the network to be used for reporting (default: 0, everything that was declared) Content of the TeX file # report() produces a .tex file (by default report.tex in the current directory, but this can be changed by passing the filename argument) which can be directly compiled with pdflatex or integrated into a larger file: pdflatex model_description.tex This report consists of different tables describing several aspects of the model: Summary : A summary of the network, with a list of populations, neuron and synapse models, topologies, etc. This section may have to be adapted, as for example, ANNarchy does not make a distinction between synapse and plasticity models. Populations : A list of populations, with their respective neural models and geometries. Projections : A list of projections, with the pre- and post-synaptic populations, the target, the synapse model if any, and a description of the connection pattern. Neuron models : For each neuron model, a description of its dynamics with equations parsed using SymPy and translated to the LaTeX mathematical language. Synapse models : For each synapse model, a description of its dynamics if any. Parameters : The initial value (before the call to compile() ) of the parameters of each population and projection (if any). Input : Inputs set to the network (has to be filled manually). Measurements : Measurements done in the network (has to be filled manually). Content of the Markdown file # The generated Mardown file is globally similar to the LaTeX one, with additional information that make it more useful for debugging (locality of attributes, type...). The Markown file is readable by design, but it can be translated to many markup languages (html, epub, latex, pdf...) using pandoc . To obtain a pdf from the Markdown file (supposing you have a LaTeX distribution available), just type: pandoc model_description.md -sN -V geometry:margin = 1in -o model_description.pdf The -V argument tells LaTex to use the full page instead of the default booklet format. To obtain a html file, use: pandoc model_description.md -sSN --mathjax -o model_description.html You can omit the -S option if you only want to include the code into a webpage, otherwise it is a standalone file. --mathjax is needed to display mathematical equations using the javascript library MathJax . By default, the html file has no styling, and tables can be very ugly. With a simple css file like this one , the html page looks nicer (feel free to edit): pandoc model_description.md -sSN --mathjax --css = simple.css -o model_description.html If you upload your model to a github-like service (bitbucket, gitlab, gogs...), it could be a good idea to generate the README.md directly with report() . Do not forget to set a title+author+date then. Documenting the network # The report is generated based entirely on the Python script. For it to make sense, the user has to provide the necessary information while defining the network: Populations must be assigned a unique name. If no name is given, generic names such as pop0 or pop1 will be used. If two populations have the same name, the connectivity will be unreadable: pop1 = Population ( geometry = ( 100 , 100 ), neuron = Izhikevich , name = \"Excitatory\" ) pop2 = Population ( geometry = ( 20 , 20 ), neuron = Izhikevich , name = \"Inhibitory\" ) User-defined neuron and synapse models should be assigned a name and description. The name should be relatively short and generic (e.g. \"Izhikevich\", \"BCM learning rule\"), while the description should be more specific. They can contain LaTeX code, but remember to double the \\ which is the escape symbol in Python strings: LIF = Neuron ( parameters = \"\"\" tau = 10.0 \"\"\" , equations = \"\"\" tau * dv/dt + v = g_exc \"\"\" , spike = \"v > 30.0\" , reset = \"v = 0.0\" name = \"LIF\" , description = \"Leaky Integrate-and-Fire spiking neuron with time constant $ \\\\ tau$.\" ) Oja = Synapse ( parameters = \"\"\" eta = 10.0 tau = 10.0 : postsynaptic \"\"\" , equations = \"\"\" tau * dalpha/dt + alpha = pos(post.r - 1.0) : postsynaptic eta * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=0.0 \"\"\" , name = \"Oja learning rule\" , description = \"\"\"Oja learning rule ensuring regularization of the synaptic weights.\"\"\" ) Choose simple parameter and variable names for the description of equations. If a parameter/variable name uses only one character, it will be treated as a mathematical variable in the equations (ex: v becomes \\(v\\) ), otherwise the plain text representation will be used (ugly). If the name corresponds to a greek letter ( alpha , tau , etc.), it will be represented by the corresponding greek letter ( \\(\\alpha\\) , \\(\\tau\\) ). If the name is composed of two terms separated by an underscore ( tau_exc ), a subscript will be used ( \\(\\tau_\\text{exc}\\) ). If more than one underscore is used, the text representation is used instead (LaTeX does not allow multiple subscripts). Example # Let's take the homeostatic STDP ramp example provided in examples/homeostatic_stdp/Ramp.py and add names/descriptions to the objects: from ANNarchy import * # Izhikevich RS neuron RSNeuron = Neuron ( parameters = \"\"\" a = 0.02 : population b = 0.2 : population c = -65. : population d = 8. : population tau_ampa = 5. : population tau_nmda = 150. : population vrev = 0.0 : population \"\"\" , equations = \"\"\" # Inputs I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v) # Midpoint scheme dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint du/dt = a * (b*v - u) : init=-13., midpoint # Izhikevich scheme # new_v = v + 0.5*(0.04 * v^2 + 5.0 * v + 140.0 - u + I) : init=-65. # v = new_v + 0.5*(0.04 * new_v^2 + 5.0 * new_v + 140.0 - u + I) : init=-65. # u += a * (b*v - u) : init=-13. # Conductances tau_ampa * dg_ampa/dt = -g_ampa : exponential tau_nmda * dg_nmda/dt = -g_nmda : exponential \"\"\" , spike = \"\"\" v >= 30. \"\"\" , reset = \"\"\" v = c u += d \"\"\" , functions = \"\"\" nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2) \"\"\" , name = \"Regular-spiking Izhikevich\" , description = \"Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\" ) # Input population inp = PoissonPopulation ( 100 , rates = np . linspace ( 0.2 , 20. , 100 ), name = \"Poisson input\" ) # RS neuron without homeostatic mechanism pop1 = Population ( 1 , RSNeuron , name = \"RS neuron without homeostasis\" ) pop1 . compute_firing_rate ( 5000. ) # RS neuron with homeostatic mechanism pop2 = Population ( 1 , RSNeuron , name = \"RS neuron with homeostasis\" ) pop2 . compute_firing_rate ( 5000. ) # Nearest Neighbour STDP nearest_neighbour_stdp = Synapse ( parameters = \"\"\" tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_max = 0.03 : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Nearest-neighbour w += if t_post >= t_pre: ltp else: - ltd : min=0.0, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" , name = \"Nearest-neighbour STDP\" , description = \"Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\" ) # STDP with homeostatic regulation homeo_stdp = Synapse ( parameters = \"\"\" # STDP tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_min = 0.0 : projection w_max = 0.03 : projection # Homeostatic regulation alpha = 0.1 : projection beta = 1.0 : projection gamma = 50. : projection Rtarget = 35. : projection T = 5000. : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Homeostatic values R = post.r : postsynaptic K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic # Nearest-neighbour stdp = if t_post >= t_pre: ltp else: - ltd w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" , name = \"Nearest-neighbour STDP with homeostasis\" , description = \"Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \" ) # Projection without homeostatic mechanism proj1 = Projection ( inp , pop1 , [ 'ampa' , 'nmda' ], synapse = nearest_neighbour_stdp ) proj1 . connect_all_to_all ( Uniform ( 0.01 , 0.03 )) # Projection with homeostatic mechanism proj2 = Projection ( inp , pop2 , [ 'ampa' , 'nmda' ], synapse = homeo_stdp ) proj2 . connect_all_to_all ( weights = Uniform ( 0.01 , 0.03 )) # Record m1 = Monitor ( pop1 , 'r' ) m2 = Monitor ( pop2 , 'r' ) report ( 'ramp.md' , title = \"Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\" , author = \"Carlson, Richert, Dutt and Krichmar\" , date = \"Neural Networks (IJCNN) 2013\" ) This generates the following Markdown file: --- title: Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks author: Carlson, Richert, Dutt and Krichmar date: Neural Networks (IJCNN) 2013 --- # Structure of the network * ANNarchy 4.6.2b using the default backend. * Numerical step size: 1.0 ms. ## Populations | **Population** | **Size** | **Neuron type** | | ----------------------------- | -------- | -------------------------- | | Poisson input | 100 | Poisson | | RS neuron without homeostasis | 1 | Regular-spiking Izhikevich | | RS neuron with homeostasis | 1 | Regular-spiking Izhikevich | ## Projections | **Source** | **Destination** | **Target** | **Synapse type** | **Pattern** | | ------------- | ----------------------------- | ----------- | --------------------------------------- | --------------------------------------------------------- | | Poisson input | RS neuron without homeostasis | ampa / nmda | Nearest-neighbour STDP | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | | Poisson input | RS neuron with homeostasis | ampa / nmda | Nearest-neighbour STDP with homeostasis | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | ## Monitors | **Object** | **Variables** | **Period** | | ----------------------------- | ------------- | ---------- | | RS neuron without homeostasis | r | 1.0 | | RS neuron with homeostasis | r | 1.0 | # Neuron models ## Regular-spiking Izhikevich Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | -------------------- | ----------------- | -------------- | -------- | | $a$ | 0.02 | per population | double | | $b$ | 0.2 | per population | double | | $c$ | -65.0 | per population | double | | $d$ | 8.0 | per population | double | | $\\tau_{\\text{ampa}}$ | 5.0 | per population | double | | $\\tau_{\\text{nmda}}$ | 150.0 | per population | double | | ${\\text{vrev}}$ | 0.0 | per population | double | **Equations:** * Variable $I$ : per neuron, initial value: 0.0 $$ {I}(t) = {g_{\\text{ampa}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) + {g_{\\text{nmda}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) \\cdot \\operatorname{nmda}{\\left ({v}(t),-80.0,60.0 \\right )} $$ * Variable $v$ : per neuron, initial value: -65.0, midpoint numerical method $$ \\frac{d{v}(t)}{dt} = {I}(t) - {u}(t) + {v}(t) \\cdot \\left(0.04 \\cdot {v}(t) + 5.0\\right) + 140.0 $$ * Variable $u$ : per neuron, initial value: -13.0, midpoint numerical method $$ \\frac{d{u}(t)}{dt} = a \\cdot \\left(b \\cdot {v}(t) - {u}(t)\\right) $$ * Variable $g_{\\text{ampa}}$ : per neuron, initial value: 0.0, exponential numerical method $$ \\frac{d{g_{\\text{ampa}}}(t)}{dt} \\cdot \\tau_{\\text{ampa}} = - {g_{\\text{ampa}}}(t) $$ * Variable $g_{\\text{nmda}}$ : per neuron, initial value: 0.0, exponential numerical method $$ \\frac{d{g_{\\text{nmda}}}(t)}{dt} \\cdot \\tau_{\\text{nmda}} = - {g_{\\text{nmda}}}(t) $$ **Spike emission:** if ${v}(t) \\geq 30.0$ : * Emit a spike a time $t$. * ${v}(t) = c$ * ${u}(t) \\mathrel{+}= d$ **Functions** $${\\text{nmda}}(v, t, s) = \\frac{\\left(- t + v\\right)^{2}}{s^{2} \\cdot \\left(1.0 + \\frac{1}{s^{2}} \\cdot \\left(- t + v\\right)^{2}\\right)}$$ ## Poisson Spiking neuron with spikes emitted according to a Poisson distribution. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | ---------------- | ----------------- | ------------ | -------- | | ${\\text{rates}}$ | 10.0 | per neuron | double | **Equations:** * Variable $p$ : per neuron, initial value: 0.0 $$ {p}(t) = \\frac{1000.0}{\\Delta t} \\cdot \\mathcal{U}{\\left (0.0,1.0 \\right )} $$ **Spike emission:** if ${p}(t) < {\\text{rates}}$ : * Emit a spike a time $t$. # Synapse models ## Nearest-neighbour STDP Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | --------------------- | ----------------- | -------------- | -------- | | $\\tau_{\\text{plus}}$ | 20.0 | per projection | double | | $\\tau_{\\text{minus}}$ | 60.0 | per projection | double | | $A_{\\text{plus}}$ | 0.0002 | per projection | double | | $A_{\\text{minus}}$ | 6.6e-05 | per projection | double | | $w_{\\text{max}}$ | 0.03 | per projection | double | **Equations:** * Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t) $$ * Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t) $$ * Variable $w$ : per synapse, initial value: 0.0, minimum: 0.0, maximum: w_max $$ {w}(t) \\mathrel{+}= \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t _{\\text{pos}} \\geq t_ {\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases} $$ **Pre-synaptic event at $t_\\text{pre} + d$:** $$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$ $${{\\text{ltp}}}(t) = A_{\\text{plus}}$$ **Post-synaptic event at $t_\\text{post}$:** $${{\\text{ltd}}}(t) = A_{\\text{minus}}$$ ## Nearest-neighbour STDP with homeostasis Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | --------------------- | ----------------- | -------------- | -------- | | $\\tau_{\\text{plus}}$ | 20.0 | per projection | double | | $\\tau_{\\text{minus}}$ | 60.0 | per projection | double | | $A_{\\text{plus}}$ | 0.0002 | per projection | double | | $A_{\\text{minus}}$ | 6.6e-05 | per projection | double | | $w_{\\text{min}}$ | 0.0 | per projection | double | | $w_{\\text{max}}$ | 0.03 | per projection | double | | $\\alpha$ | 0.1 | per projection | double | | $\\beta$ | 1.0 | per projection | double | | $\\gamma$ | 50.0 | per projection | double | | ${\\text{Rtarget}}$ | 35.0 | per projection | double | | $T$ | 5000.0 | per projection | double | **Equations:** * Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t) $$ * Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t) $$ * Variable $R$ : per post-synaptic neuron, initial value: 0.0 $$ {R}(t) = {r}^{\\text{post}}(t) $$ * Variable $K$ : per post-synaptic neuron, initial value: 0.0 $$ {K}(t) = \\frac{{R}(t)}{T \\cdot \\left(\\gamma \\cdot \\left|{f}\\right|{\\left (- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1.0 \\right )} + 1.0\\right)} $$ * Variable ${\\text{stdp}}$ : per synapse, initial value: 0.0 $$ {{\\text{stdp}}}(t) = \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t _{\\text{pos}} \\geq t_ {\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases} $$ * Variable $w$ : per synapse, initial value: 0.0, minimum: w _min, maximum: w_ max $$ {w}(t) \\mathrel{+}= {K}(t) \\cdot \\left(\\alpha \\cdot {w}(t) \\cdot \\left(- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1\\right) + \\beta \\cdot {{\\text{stdp}}}(t)\\right) $$ **Pre-synaptic event at $t_\\text{pre} + d$:** $$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$ $${{\\text{ltp}}}(t) = A_{\\text{plus}}$$ **Post-synaptic event at $t_\\text{post}$:** $${{\\text{ltd}}}(t) = A_{\\text{minus}}$$ # Parameters ## Population parameters | **Population** | **Neuron type** | **Name** | **Value** | | ----------------------------- | -------------------------- | -------------------- | ------------- | | Poisson input | Poisson | ${\\text{rates}}$ | $[0.2, 20.0]$ | | RS neuron without homeostasis | Regular-spiking Izhikevich | $a$ | 0.02 | | | | $b$ | 0.2 | | | | $c$ | -65.0 | | | | $d$ | 8.0 | | | | $\\tau_{\\text{ampa}}$ | 5.0 | | | | $\\tau_{\\text{nmda}}$ | 150.0 | | | | ${\\text{vrev}}$ | 0.0 | | RS neuron with homeostasis | Regular-spiking Izhikevich | $a$ | 0.02 | | | | $b$ | 0.2 | | | | $c$ | -65.0 | | | | $d$ | 8.0 | | | | $\\tau_{\\text{ampa}}$ | 5.0 | | | | $\\tau_{\\text{nmda}}$ | 150.0 | | | | ${\\text{vrev}}$ | 0.0 | ## Projection parameters | **Projection** | **Synapse type** | **Name** | **Value** | | ---------------------------------------------------------------------------------- | --------------------------------------- | --------------------- | --------- | | Poisson input $\\rightarrow$ RS neuron without homeostasis with target ampa / nmda | Nearest-neighbour STDP | $\\tau_{\\text{plus}}$ | 20.0 | | | | $\\tau_{\\text{minus}}$ | 60.0 | | | | $A_{\\text{plus}}$ | 0.0002 | | | | $A_{\\text{minus}}$ | 6.6e-05 | | | | $w_{\\text{max}}$ | 0.03 | | Poisson input $\\rightarrow$ RS neuron with homeostasis with target ampa / nmda | Nearest-neighbour STDP with homeostasis | $\\tau_{\\text{plus}}$ | 20.0 | | | | $\\tau_{\\text{minus}}$ | 60.0 | | | | $A_{\\text{plus}}$ | 0.0002 | | | | $A_{\\text{minus}}$ | 6.6e-05 | | | | $w_{\\text{min}}$ | 0.0 | | | | $w_{\\text{max}}$ | 0.03 | | | | $\\alpha$ | 0.1 | | | | $\\beta$ | 1.0 | | | | $\\gamma$ | 50.0 | | | | ${\\text{Rtarget}}$ | 35.0 | | | | $T$ | 5000.0 |","title":"Reporting"},{"location":"manual/Reporting.html#reporting","text":"ANNarchy includes an utility allowing to automatically generate a report based on the current structure of the network: report ( filename = \"model_description.tex\" ) report ( filename = \"model_description.md\" ) If the filename ends with .tex , the LaTeX report will be generated based on the specifications provided in: Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8):e1000456. If the filename ends with .md , the report will be generated in Markdown, so it can later be exported to pdf or html using pandoc . report() accepts several arguments: filename : name of the file where the report will be written (default: \\\"./report.tex\\\") standalone : tells if the generated TeX file should be directly compilable or only includable. Ignored in Markdown. gather_subprojections : if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False). title : title of the document (Markdown only) author : author of the document (Markdown only) date : date of the document (Markdown only) net_id : id of the network to be used for reporting (default: 0, everything that was declared)","title":"Reporting"},{"location":"manual/Reporting.html#content-of-the-tex-file","text":"report() produces a .tex file (by default report.tex in the current directory, but this can be changed by passing the filename argument) which can be directly compiled with pdflatex or integrated into a larger file: pdflatex model_description.tex This report consists of different tables describing several aspects of the model: Summary : A summary of the network, with a list of populations, neuron and synapse models, topologies, etc. This section may have to be adapted, as for example, ANNarchy does not make a distinction between synapse and plasticity models. Populations : A list of populations, with their respective neural models and geometries. Projections : A list of projections, with the pre- and post-synaptic populations, the target, the synapse model if any, and a description of the connection pattern. Neuron models : For each neuron model, a description of its dynamics with equations parsed using SymPy and translated to the LaTeX mathematical language. Synapse models : For each synapse model, a description of its dynamics if any. Parameters : The initial value (before the call to compile() ) of the parameters of each population and projection (if any). Input : Inputs set to the network (has to be filled manually). Measurements : Measurements done in the network (has to be filled manually).","title":"Content of the TeX file"},{"location":"manual/Reporting.html#content-of-the-markdown-file","text":"The generated Mardown file is globally similar to the LaTeX one, with additional information that make it more useful for debugging (locality of attributes, type...). The Markown file is readable by design, but it can be translated to many markup languages (html, epub, latex, pdf...) using pandoc . To obtain a pdf from the Markdown file (supposing you have a LaTeX distribution available), just type: pandoc model_description.md -sN -V geometry:margin = 1in -o model_description.pdf The -V argument tells LaTex to use the full page instead of the default booklet format. To obtain a html file, use: pandoc model_description.md -sSN --mathjax -o model_description.html You can omit the -S option if you only want to include the code into a webpage, otherwise it is a standalone file. --mathjax is needed to display mathematical equations using the javascript library MathJax . By default, the html file has no styling, and tables can be very ugly. With a simple css file like this one , the html page looks nicer (feel free to edit): pandoc model_description.md -sSN --mathjax --css = simple.css -o model_description.html If you upload your model to a github-like service (bitbucket, gitlab, gogs...), it could be a good idea to generate the README.md directly with report() . Do not forget to set a title+author+date then.","title":"Content of the Markdown file"},{"location":"manual/Reporting.html#documenting-the-network","text":"The report is generated based entirely on the Python script. For it to make sense, the user has to provide the necessary information while defining the network: Populations must be assigned a unique name. If no name is given, generic names such as pop0 or pop1 will be used. If two populations have the same name, the connectivity will be unreadable: pop1 = Population ( geometry = ( 100 , 100 ), neuron = Izhikevich , name = \"Excitatory\" ) pop2 = Population ( geometry = ( 20 , 20 ), neuron = Izhikevich , name = \"Inhibitory\" ) User-defined neuron and synapse models should be assigned a name and description. The name should be relatively short and generic (e.g. \"Izhikevich\", \"BCM learning rule\"), while the description should be more specific. They can contain LaTeX code, but remember to double the \\ which is the escape symbol in Python strings: LIF = Neuron ( parameters = \"\"\" tau = 10.0 \"\"\" , equations = \"\"\" tau * dv/dt + v = g_exc \"\"\" , spike = \"v > 30.0\" , reset = \"v = 0.0\" name = \"LIF\" , description = \"Leaky Integrate-and-Fire spiking neuron with time constant $ \\\\ tau$.\" ) Oja = Synapse ( parameters = \"\"\" eta = 10.0 tau = 10.0 : postsynaptic \"\"\" , equations = \"\"\" tau * dalpha/dt + alpha = pos(post.r - 1.0) : postsynaptic eta * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=0.0 \"\"\" , name = \"Oja learning rule\" , description = \"\"\"Oja learning rule ensuring regularization of the synaptic weights.\"\"\" ) Choose simple parameter and variable names for the description of equations. If a parameter/variable name uses only one character, it will be treated as a mathematical variable in the equations (ex: v becomes \\(v\\) ), otherwise the plain text representation will be used (ugly). If the name corresponds to a greek letter ( alpha , tau , etc.), it will be represented by the corresponding greek letter ( \\(\\alpha\\) , \\(\\tau\\) ). If the name is composed of two terms separated by an underscore ( tau_exc ), a subscript will be used ( \\(\\tau_\\text{exc}\\) ). If more than one underscore is used, the text representation is used instead (LaTeX does not allow multiple subscripts).","title":"Documenting the network"},{"location":"manual/Reporting.html#example","text":"Let's take the homeostatic STDP ramp example provided in examples/homeostatic_stdp/Ramp.py and add names/descriptions to the objects: from ANNarchy import * # Izhikevich RS neuron RSNeuron = Neuron ( parameters = \"\"\" a = 0.02 : population b = 0.2 : population c = -65. : population d = 8. : population tau_ampa = 5. : population tau_nmda = 150. : population vrev = 0.0 : population \"\"\" , equations = \"\"\" # Inputs I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v) # Midpoint scheme dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint du/dt = a * (b*v - u) : init=-13., midpoint # Izhikevich scheme # new_v = v + 0.5*(0.04 * v^2 + 5.0 * v + 140.0 - u + I) : init=-65. # v = new_v + 0.5*(0.04 * new_v^2 + 5.0 * new_v + 140.0 - u + I) : init=-65. # u += a * (b*v - u) : init=-13. # Conductances tau_ampa * dg_ampa/dt = -g_ampa : exponential tau_nmda * dg_nmda/dt = -g_nmda : exponential \"\"\" , spike = \"\"\" v >= 30. \"\"\" , reset = \"\"\" v = c u += d \"\"\" , functions = \"\"\" nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2) \"\"\" , name = \"Regular-spiking Izhikevich\" , description = \"Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\" ) # Input population inp = PoissonPopulation ( 100 , rates = np . linspace ( 0.2 , 20. , 100 ), name = \"Poisson input\" ) # RS neuron without homeostatic mechanism pop1 = Population ( 1 , RSNeuron , name = \"RS neuron without homeostasis\" ) pop1 . compute_firing_rate ( 5000. ) # RS neuron with homeostatic mechanism pop2 = Population ( 1 , RSNeuron , name = \"RS neuron with homeostasis\" ) pop2 . compute_firing_rate ( 5000. ) # Nearest Neighbour STDP nearest_neighbour_stdp = Synapse ( parameters = \"\"\" tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_max = 0.03 : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Nearest-neighbour w += if t_post >= t_pre: ltp else: - ltd : min=0.0, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" , name = \"Nearest-neighbour STDP\" , description = \"Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\" ) # STDP with homeostatic regulation homeo_stdp = Synapse ( parameters = \"\"\" # STDP tau_plus = 20. : projection tau_minus = 60. : projection A_plus = 0.0002 : projection A_minus = 0.000066 : projection w_min = 0.0 : projection w_max = 0.03 : projection # Homeostatic regulation alpha = 0.1 : projection beta = 1.0 : projection gamma = 50. : projection Rtarget = 35. : projection T = 5000. : projection \"\"\" , equations = \"\"\" # Traces tau_plus * dltp/dt = -ltp : exponential tau_minus * dltd/dt = -ltd : exponential # Homeostatic values R = post.r : postsynaptic K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic # Nearest-neighbour stdp = if t_post >= t_pre: ltp else: - ltd w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max \"\"\" , pre_spike = \"\"\" g_target += w ltp = A_plus \"\"\" , post_spike = \"\"\" ltd = A_minus \"\"\" , name = \"Nearest-neighbour STDP with homeostasis\" , description = \"Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \" ) # Projection without homeostatic mechanism proj1 = Projection ( inp , pop1 , [ 'ampa' , 'nmda' ], synapse = nearest_neighbour_stdp ) proj1 . connect_all_to_all ( Uniform ( 0.01 , 0.03 )) # Projection with homeostatic mechanism proj2 = Projection ( inp , pop2 , [ 'ampa' , 'nmda' ], synapse = homeo_stdp ) proj2 . connect_all_to_all ( weights = Uniform ( 0.01 , 0.03 )) # Record m1 = Monitor ( pop1 , 'r' ) m2 = Monitor ( pop2 , 'r' ) report ( 'ramp.md' , title = \"Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\" , author = \"Carlson, Richert, Dutt and Krichmar\" , date = \"Neural Networks (IJCNN) 2013\" ) This generates the following Markdown file: --- title: Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks author: Carlson, Richert, Dutt and Krichmar date: Neural Networks (IJCNN) 2013 --- # Structure of the network * ANNarchy 4.6.2b using the default backend. * Numerical step size: 1.0 ms. ## Populations | **Population** | **Size** | **Neuron type** | | ----------------------------- | -------- | -------------------------- | | Poisson input | 100 | Poisson | | RS neuron without homeostasis | 1 | Regular-spiking Izhikevich | | RS neuron with homeostasis | 1 | Regular-spiking Izhikevich | ## Projections | **Source** | **Destination** | **Target** | **Synapse type** | **Pattern** | | ------------- | ----------------------------- | ----------- | --------------------------------------- | --------------------------------------------------------- | | Poisson input | RS neuron without homeostasis | ampa / nmda | Nearest-neighbour STDP | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | | Poisson input | RS neuron with homeostasis | ampa / nmda | Nearest-neighbour STDP with homeostasis | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | ## Monitors | **Object** | **Variables** | **Period** | | ----------------------------- | ------------- | ---------- | | RS neuron without homeostasis | r | 1.0 | | RS neuron with homeostasis | r | 1.0 | # Neuron models ## Regular-spiking Izhikevich Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | -------------------- | ----------------- | -------------- | -------- | | $a$ | 0.02 | per population | double | | $b$ | 0.2 | per population | double | | $c$ | -65.0 | per population | double | | $d$ | 8.0 | per population | double | | $\\tau_{\\text{ampa}}$ | 5.0 | per population | double | | $\\tau_{\\text{nmda}}$ | 150.0 | per population | double | | ${\\text{vrev}}$ | 0.0 | per population | double | **Equations:** * Variable $I$ : per neuron, initial value: 0.0 $$ {I}(t) = {g_{\\text{ampa}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) + {g_{\\text{nmda}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) \\cdot \\operatorname{nmda}{\\left ({v}(t),-80.0,60.0 \\right )} $$ * Variable $v$ : per neuron, initial value: -65.0, midpoint numerical method $$ \\frac{d{v}(t)}{dt} = {I}(t) - {u}(t) + {v}(t) \\cdot \\left(0.04 \\cdot {v}(t) + 5.0\\right) + 140.0 $$ * Variable $u$ : per neuron, initial value: -13.0, midpoint numerical method $$ \\frac{d{u}(t)}{dt} = a \\cdot \\left(b \\cdot {v}(t) - {u}(t)\\right) $$ * Variable $g_{\\text{ampa}}$ : per neuron, initial value: 0.0, exponential numerical method $$ \\frac{d{g_{\\text{ampa}}}(t)}{dt} \\cdot \\tau_{\\text{ampa}} = - {g_{\\text{ampa}}}(t) $$ * Variable $g_{\\text{nmda}}$ : per neuron, initial value: 0.0, exponential numerical method $$ \\frac{d{g_{\\text{nmda}}}(t)}{dt} \\cdot \\tau_{\\text{nmda}} = - {g_{\\text{nmda}}}(t) $$ **Spike emission:** if ${v}(t) \\geq 30.0$ : * Emit a spike a time $t$. * ${v}(t) = c$ * ${u}(t) \\mathrel{+}= d$ **Functions** $${\\text{nmda}}(v, t, s) = \\frac{\\left(- t + v\\right)^{2}}{s^{2} \\cdot \\left(1.0 + \\frac{1}{s^{2}} \\cdot \\left(- t + v\\right)^{2}\\right)}$$ ## Poisson Spiking neuron with spikes emitted according to a Poisson distribution. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | ---------------- | ----------------- | ------------ | -------- | | ${\\text{rates}}$ | 10.0 | per neuron | double | **Equations:** * Variable $p$ : per neuron, initial value: 0.0 $$ {p}(t) = \\frac{1000.0}{\\Delta t} \\cdot \\mathcal{U}{\\left (0.0,1.0 \\right )} $$ **Spike emission:** if ${p}(t) < {\\text{rates}}$ : * Emit a spike a time $t$. # Synapse models ## Nearest-neighbour STDP Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | --------------------- | ----------------- | -------------- | -------- | | $\\tau_{\\text{plus}}$ | 20.0 | per projection | double | | $\\tau_{\\text{minus}}$ | 60.0 | per projection | double | | $A_{\\text{plus}}$ | 0.0002 | per projection | double | | $A_{\\text{minus}}$ | 6.6e-05 | per projection | double | | $w_{\\text{max}}$ | 0.03 | per projection | double | **Equations:** * Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t) $$ * Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t) $$ * Variable $w$ : per synapse, initial value: 0.0, minimum: 0.0, maximum: w_max $$ {w}(t) \\mathrel{+}= \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t _{\\text{pos}} \\geq t_ {\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases} $$ **Pre-synaptic event at $t_\\text{pre} + d$:** $$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$ $${{\\text{ltp}}}(t) = A_{\\text{plus}}$$ **Post-synaptic event at $t_\\text{post}$:** $${{\\text{ltd}}}(t) = A_{\\text{minus}}$$ ## Nearest-neighbour STDP with homeostasis Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. **Parameters:** | **Name** | **Default value** | **Locality** | **Type** | | --------------------- | ----------------- | -------------- | -------- | | $\\tau_{\\text{plus}}$ | 20.0 | per projection | double | | $\\tau_{\\text{minus}}$ | 60.0 | per projection | double | | $A_{\\text{plus}}$ | 0.0002 | per projection | double | | $A_{\\text{minus}}$ | 6.6e-05 | per projection | double | | $w_{\\text{min}}$ | 0.0 | per projection | double | | $w_{\\text{max}}$ | 0.03 | per projection | double | | $\\alpha$ | 0.1 | per projection | double | | $\\beta$ | 1.0 | per projection | double | | $\\gamma$ | 50.0 | per projection | double | | ${\\text{Rtarget}}$ | 35.0 | per projection | double | | $T$ | 5000.0 | per projection | double | **Equations:** * Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t) $$ * Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method $$ \\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t) $$ * Variable $R$ : per post-synaptic neuron, initial value: 0.0 $$ {R}(t) = {r}^{\\text{post}}(t) $$ * Variable $K$ : per post-synaptic neuron, initial value: 0.0 $$ {K}(t) = \\frac{{R}(t)}{T \\cdot \\left(\\gamma \\cdot \\left|{f}\\right|{\\left (- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1.0 \\right )} + 1.0\\right)} $$ * Variable ${\\text{stdp}}$ : per synapse, initial value: 0.0 $$ {{\\text{stdp}}}(t) = \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t _{\\text{pos}} \\geq t_ {\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases} $$ * Variable $w$ : per synapse, initial value: 0.0, minimum: w _min, maximum: w_ max $$ {w}(t) \\mathrel{+}= {K}(t) \\cdot \\left(\\alpha \\cdot {w}(t) \\cdot \\left(- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1\\right) + \\beta \\cdot {{\\text{stdp}}}(t)\\right) $$ **Pre-synaptic event at $t_\\text{pre} + d$:** $$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$ $${{\\text{ltp}}}(t) = A_{\\text{plus}}$$ **Post-synaptic event at $t_\\text{post}$:** $${{\\text{ltd}}}(t) = A_{\\text{minus}}$$ # Parameters ## Population parameters | **Population** | **Neuron type** | **Name** | **Value** | | ----------------------------- | -------------------------- | -------------------- | ------------- | | Poisson input | Poisson | ${\\text{rates}}$ | $[0.2, 20.0]$ | | RS neuron without homeostasis | Regular-spiking Izhikevich | $a$ | 0.02 | | | | $b$ | 0.2 | | | | $c$ | -65.0 | | | | $d$ | 8.0 | | | | $\\tau_{\\text{ampa}}$ | 5.0 | | | | $\\tau_{\\text{nmda}}$ | 150.0 | | | | ${\\text{vrev}}$ | 0.0 | | RS neuron with homeostasis | Regular-spiking Izhikevich | $a$ | 0.02 | | | | $b$ | 0.2 | | | | $c$ | -65.0 | | | | $d$ | 8.0 | | | | $\\tau_{\\text{ampa}}$ | 5.0 | | | | $\\tau_{\\text{nmda}}$ | 150.0 | | | | ${\\text{vrev}}$ | 0.0 | ## Projection parameters | **Projection** | **Synapse type** | **Name** | **Value** | | ---------------------------------------------------------------------------------- | --------------------------------------- | --------------------- | --------- | | Poisson input $\\rightarrow$ RS neuron without homeostasis with target ampa / nmda | Nearest-neighbour STDP | $\\tau_{\\text{plus}}$ | 20.0 | | | | $\\tau_{\\text{minus}}$ | 60.0 | | | | $A_{\\text{plus}}$ | 0.0002 | | | | $A_{\\text{minus}}$ | 6.6e-05 | | | | $w_{\\text{max}}$ | 0.03 | | Poisson input $\\rightarrow$ RS neuron with homeostasis with target ampa / nmda | Nearest-neighbour STDP with homeostasis | $\\tau_{\\text{plus}}$ | 20.0 | | | | $\\tau_{\\text{minus}}$ | 60.0 | | | | $A_{\\text{plus}}$ | 0.0002 | | | | $A_{\\text{minus}}$ | 6.6e-05 | | | | $w_{\\text{min}}$ | 0.0 | | | | $w_{\\text{max}}$ | 0.03 | | | | $\\alpha$ | 0.1 | | | | $\\beta$ | 1.0 | | | | $\\gamma$ | 50.0 | | | | ${\\text{Rtarget}}$ | 35.0 | | | | $T$ | 5000.0 |","title":"Example"},{"location":"manual/Saving.html","text":"Saving and loading a network # For the complete APIs, see IO in the library reference. Global parameters # The global parameters of the network (flagged with population or projection in the Neuron/Synapse definitions) can be saved to and loaded from a JSON file using the functions save_parameters() and load_parameters() : save_parameters ( 'network.json' ) load_parameters ( 'network.json' ) The saved JSON file for a network of two populations of Izhikevich neurons connected with STDP will look like: { \"populations\" : { \"pop0\" : { \"a\" : 0.1 , \"b\" : 0.2 , \"c\" : -65.0 , \"d\" : 8.0 , \"tau_ampa\" : 5.0 , \"tau_nmda\" : 150.0 , \"v_rev\" : 0.0 , \"v_thresh\" : 30.0 }, \"pop1\" : { \"a\" : 0.02 , \"b\" : 0.2 , \"c\" : -65.0 , \"d\" : 8.0 , \"tau_ampa\" : 5.0 , \"tau_nmda\" : 150.0 , \"v_rev\" : 0.0 , \"v_thresh\" : 30.0 } }, \"projections\" : { \"proj0\" : { \"tau_plus\" : 20.0 , \"tau_minus\" : 60.0 , \"A_plus\" : 0.0002 , \"A_minus\" : 6.6e-05 , \"w_max\" : 0.03 } }, \"network\" : {}, } By default, populations and projections have names like pop0 and proj1 . For readability, we advise setting explicit (and unique ) names in their constructor: pop = Population ( 100 , Izhikevich , name = \"PFC_exc\" ) proj = Projection ( pop , pop2 , 'exc' , STDP , name = \"PFC_exc_to_inh\" ) Only global parameters can be saved (no array is allowed in the JSON file). By default, only global parameters will be loaded, except if the global_only argument to load_parameters() is set to False. In that case, even local parameters can be set by the JSON file, but they will all use the same values. If you want to initialize other things than population/projection global parameters, you can define arbitrary values in the \"network\" dictionary: { \"network\" : { \"pop1_r_min\" : 0.1 , \"pop1_r_max\" : 1.3 , }, } load_parameters() will return the corresponding dictionary: params = load_parameters ( 'network.json' ) You can then use them to initialize programmatically non-global parameters or variables: pop1 . r = Uniform ( params [ 'pop1_r_min' ], params [ 'pop1_r_max' ]) Complete state of the network # The state of all variables, including the synaptic weights, can be saved in a text file, compressed binary file or Matlab file using the save() method: save ( 'data.txt' ) save ( 'data.txt.gz' ) save ( 'data.mat' ) Filenames ending with .mat correspond to Matlab files (it requires the installation of Scipy), filenames ending with .gz are compressed using gzip (normally standard to all Python distributions, but may require installation), other extensions are normal text files using cPickle (standard). save() also accepts the populations and projections boolean flags. If True (the default), the neural resp. synaptic variables will be saved. For example, if you only care about synaptic plasticity but not the neural variables, you can set populations to False , and only synaptic variables will be saved. save ( 'data.txt' , populations = False ) Except for the Matlab format, you can also load the state of variables stored in these files once the network is compiled : load ( 'data.txt' ) Warning The structure of the network must of course be the same as when the file was saved: number of populations, neurons and projections. The neuron and synapse types must define the same variables. If a variable was saved but does not exist anymore, it will be skipped. If the variable did not exist, its current value will be kept, what can lead to crashes. load() also accepts the populations and projections boolean flags (for example if you want to load only the synaptic weights but not to restore the neural variables). Populations and projections individually # Population and Projection objects also have save() and load() methods, allowing to save the corresponding information individually: pop1 . save ( 'pop1.npz' ) proj . save ( 'proj.npz' ) pop1 . load ( 'pop1.npz' ) proj . load ( 'proj.npz' ) The allowed file formats are: .npz : compressed Numpy binary format ( np.savez_compressed ), preferred. *.gz : gunzipped binary text file. *.mat : Matlab 7.2. * : binary text file. As before, .mat can only be used for saving, not loading.","title":"Saving and loading a network"},{"location":"manual/Saving.html#saving-and-loading-a-network","text":"For the complete APIs, see IO in the library reference.","title":"Saving and loading a network"},{"location":"manual/Saving.html#global-parameters","text":"The global parameters of the network (flagged with population or projection in the Neuron/Synapse definitions) can be saved to and loaded from a JSON file using the functions save_parameters() and load_parameters() : save_parameters ( 'network.json' ) load_parameters ( 'network.json' ) The saved JSON file for a network of two populations of Izhikevich neurons connected with STDP will look like: { \"populations\" : { \"pop0\" : { \"a\" : 0.1 , \"b\" : 0.2 , \"c\" : -65.0 , \"d\" : 8.0 , \"tau_ampa\" : 5.0 , \"tau_nmda\" : 150.0 , \"v_rev\" : 0.0 , \"v_thresh\" : 30.0 }, \"pop1\" : { \"a\" : 0.02 , \"b\" : 0.2 , \"c\" : -65.0 , \"d\" : 8.0 , \"tau_ampa\" : 5.0 , \"tau_nmda\" : 150.0 , \"v_rev\" : 0.0 , \"v_thresh\" : 30.0 } }, \"projections\" : { \"proj0\" : { \"tau_plus\" : 20.0 , \"tau_minus\" : 60.0 , \"A_plus\" : 0.0002 , \"A_minus\" : 6.6e-05 , \"w_max\" : 0.03 } }, \"network\" : {}, } By default, populations and projections have names like pop0 and proj1 . For readability, we advise setting explicit (and unique ) names in their constructor: pop = Population ( 100 , Izhikevich , name = \"PFC_exc\" ) proj = Projection ( pop , pop2 , 'exc' , STDP , name = \"PFC_exc_to_inh\" ) Only global parameters can be saved (no array is allowed in the JSON file). By default, only global parameters will be loaded, except if the global_only argument to load_parameters() is set to False. In that case, even local parameters can be set by the JSON file, but they will all use the same values. If you want to initialize other things than population/projection global parameters, you can define arbitrary values in the \"network\" dictionary: { \"network\" : { \"pop1_r_min\" : 0.1 , \"pop1_r_max\" : 1.3 , }, } load_parameters() will return the corresponding dictionary: params = load_parameters ( 'network.json' ) You can then use them to initialize programmatically non-global parameters or variables: pop1 . r = Uniform ( params [ 'pop1_r_min' ], params [ 'pop1_r_max' ])","title":"Global parameters"},{"location":"manual/Saving.html#complete-state-of-the-network","text":"The state of all variables, including the synaptic weights, can be saved in a text file, compressed binary file or Matlab file using the save() method: save ( 'data.txt' ) save ( 'data.txt.gz' ) save ( 'data.mat' ) Filenames ending with .mat correspond to Matlab files (it requires the installation of Scipy), filenames ending with .gz are compressed using gzip (normally standard to all Python distributions, but may require installation), other extensions are normal text files using cPickle (standard). save() also accepts the populations and projections boolean flags. If True (the default), the neural resp. synaptic variables will be saved. For example, if you only care about synaptic plasticity but not the neural variables, you can set populations to False , and only synaptic variables will be saved. save ( 'data.txt' , populations = False ) Except for the Matlab format, you can also load the state of variables stored in these files once the network is compiled : load ( 'data.txt' ) Warning The structure of the network must of course be the same as when the file was saved: number of populations, neurons and projections. The neuron and synapse types must define the same variables. If a variable was saved but does not exist anymore, it will be skipped. If the variable did not exist, its current value will be kept, what can lead to crashes. load() also accepts the populations and projections boolean flags (for example if you want to load only the synaptic weights but not to restore the neural variables).","title":"Complete state of the network"},{"location":"manual/Saving.html#populations-and-projections-individually","text":"Population and Projection objects also have save() and load() methods, allowing to save the corresponding information individually: pop1 . save ( 'pop1.npz' ) proj . save ( 'proj.npz' ) pop1 . load ( 'pop1.npz' ) proj . load ( 'proj.npz' ) The allowed file formats are: .npz : compressed Numpy binary format ( np.savez_compressed ), preferred. *.gz : gunzipped binary text file. *.mat : Matlab 7.2. * : binary text file. As before, .mat can only be used for saving, not loading.","title":"Populations and projections individually"},{"location":"manual/Simulation.html","text":"Simulation # Compiling the network # Once all the relevant information has been defined, one needs to actually compile the network, by calling the ANNarchy.compile() method: compile () The optimized C++ code will be generated in the annarchy/ subfolder relative to your script, compiled, the underlying objects created and made available to the Python interface. You can specify the following arguments to compile() : directory : relative path to the directory where files will be generated and compiled (default: annarchy/ ) populations and projections : to compile only a subpart of the network, see Network . compiler : to select which C++ compiler will be used. By default g++ on Linux and clang++ on OS X are used, you can change it here. Note that only these two compilers are supported for now, and that they must be in your $PATH . compiler_flags : to select which flags are passed to the compiler. By default it is -march=native -O2 , but you can fine-tune it here. Beware that -O3 is most often a bad idea! Simulating the network # After the network is correctly compiled, the simulation can be run for the specified duration (in milliseconds) through the ANNarchy.simulate() method: simulate ( 1000.0 ) # Simulate for 1 second The provided duration should be a multiple of dt . If not, the number of simulation steps performed will be approximated. In some cases, you may want to perform only one step of the simulation, instead of specifing the duration. The ANNarchy.step() can then be used. step () # Simulate for 1 step Early-stopping # In some cases, it is desired to stop the simulation whenever a criterion is fulfilled (for example, a neural integrator exceeds a certain threshold), not after a fixed amount of time. There is the possibility to define a stop_condition at the Population level: pop1 = Population ( ... , stop_condition = \"r > 1.0\" ) When calling the simulate_until() method instead of simulate() : t = simulate_until ( max_duration = 1000.0 , populations = pop1 ) the simulation will be stopped whenever the stop_condition of pop1 is met, i.e. when the firing rate of any neuron of pop1 is above 1.0. If the condition is never met, the simulation will last maximally max_duration . The methods returns the effective duration of the simulation (to compute reaction times, for example). The stop_condition can use any logical operation on the parameters and variables of the neuron associated to the population: pop1 = Population ( ... , stop_condition = \"(r > 1.0) and (mp < 2.0)\" ) By default, the simulation stops when at least one neuron in the population fulfills the criterion. If you want to stop the simulation when all neurons fulfill the condition, you can use the flag all after the condition: pop1 = Population ( ... , stop_condition = \"r > 1.0 : all\" ) The flag any is the default behavior and can be omitted. The stop criterion can depend on several populations, by providing a list of populations to the populations argument instead of a single population: t = simulate_until ( max_duration = 1000.0 , populations = [ pop1 , pop2 ]) The simulation will then stop when the criterion is met in both populations at the same time. If you want that the simulation stops when at least one population meets its criterion, you can specify the operator argument: t = simulate_until ( max_duration = 1000.0 , populations = [ pop1 , pop2 ], operator = 'or' ) The default value of operator is a 'and' function between the populations' criteria. Warning Global operations (min, max, mean) are not possible inside the stop_condition . If you need them, store them in a variable in the equations argument of the neuron and use it as the condition: equations = \"\"\" r = ... max_r = max(r) \"\"\" Setting inputs periodically # In most cases, your simulation will be decomposed into a series of fixed-duration trials, where you basically set inputs at the beginning of the trial, run the simulation for a fixed duration, and possibly read out results at the end: # Iterate over 100 trials result = [] for trial in range ( 100 ): # Set inputs to the network pop . I = Uniform ( 0.0 , 1.0 ) # Simulate for 1 second simulate ( 1000. ) # Save the output result . append ( pop . r ) For convenience, we provide the decorator every , which allows to register a python method and call it automatically during the simulation with a fixed period: result = [] @every ( period = 1000. ) def set inputs ( n ): # Set inputs to the network pop . I = Uniform ( 0.0 , 1.0 ) # Save the output of the previous step if n > 0 : result . append ( pop . r ) simulate ( 100 * 1000. ) In this example, set_inputs() will be executed just before the steps corresponding to times t = 0., 1000., 2000., and so on until t = 100000. The method can have any name, but must accept only one argument, the integer n which will be incremented at each call of the method (i.e. it will take the values 0, 1, 2 until 99). This can for example be used to access data in a numpy array: images = np . random . random (( 100 , 640 , 480 )) @every ( period = 1000. ) def set inputs ( n ): # Set inputs to the network pop . I = images [ n , :, :] simulate ( 100 * 1000. ) One can define several methods that will be called in the order of their definition: @every ( period = 1000. ) def set inputs ( n ): pop . I = 1.0 @every ( period = 1000. ) def reset inputs ( n ): pop . I = 0.0 In this example, set_inputs() will be called first, followed by reset_inputs , so pop.I will finally be 0.0. The decorator every accepts an argument offset defining a delay within the period to call the method: @every ( period = 1000. ) def set inputs ( n ): pop . I = 1.0 @every ( period = 1000. , offset = 500. ) def reset inputs ( n ): pop . I = 0.0 In this case, set_inputs() will be called at times 0, 1000, 2000... while reset_inputs() will be called at times 500, 1500, 2500..., allowing to structure a trial more effectively. The offset can be set negative, in which case it will be relative to the end of the trial: @every ( period = 1000. , offset =- 100. ) def reset inputs ( n ): pop . I = 0.0 In this example, the method will be called at times 900, 1900, 2900 and so on. The offset value can not be longer than the period , by definition. If you try to do so, a modulo operation will anyway be applied (i.e. an offset of 1500 with a period of 1000 becomes 500). Finally, the wait argument allows to delay the first call to the method from a fixed interval: @every ( period = 1000. , wait = 5000. ) def reset inputs ( n ): pop . I = 0.0 In this case, the method will be called at times 5000, 6000 and so on. Between two calls to simulate() , the callbacks can be disabled or re-enabled using the following methods: @every ( period = 1000. ) def reset inputs ( n ): pop . I = 0.0 # Simulate with callbacks simulate ( 10000. ) # Disable callbacks disable_callbacks () # Simulate without callbacks simulate ( 10000. ) # Re-enable callbacks enable_callbacks () # Simulate with callbacks simulate ( 10000. ) Note that the period is always relative to the time when simulate() is called, so if no offset is defined, the callbacks will be called before the first step of a simulation, no matter how long the previous simulation lasted. In the current state, it is not possible yet to enable/disable callbacks selectively, it is all or none. Callbacks can only be used with simulate() , not with step() or simulate_until() .","title":"Simulation"},{"location":"manual/Simulation.html#simulation","text":"","title":"Simulation"},{"location":"manual/Simulation.html#compiling-the-network","text":"Once all the relevant information has been defined, one needs to actually compile the network, by calling the ANNarchy.compile() method: compile () The optimized C++ code will be generated in the annarchy/ subfolder relative to your script, compiled, the underlying objects created and made available to the Python interface. You can specify the following arguments to compile() : directory : relative path to the directory where files will be generated and compiled (default: annarchy/ ) populations and projections : to compile only a subpart of the network, see Network . compiler : to select which C++ compiler will be used. By default g++ on Linux and clang++ on OS X are used, you can change it here. Note that only these two compilers are supported for now, and that they must be in your $PATH . compiler_flags : to select which flags are passed to the compiler. By default it is -march=native -O2 , but you can fine-tune it here. Beware that -O3 is most often a bad idea!","title":"Compiling the network"},{"location":"manual/Simulation.html#simulating-the-network","text":"After the network is correctly compiled, the simulation can be run for the specified duration (in milliseconds) through the ANNarchy.simulate() method: simulate ( 1000.0 ) # Simulate for 1 second The provided duration should be a multiple of dt . If not, the number of simulation steps performed will be approximated. In some cases, you may want to perform only one step of the simulation, instead of specifing the duration. The ANNarchy.step() can then be used. step () # Simulate for 1 step","title":"Simulating the network"},{"location":"manual/Simulation.html#early-stopping","text":"In some cases, it is desired to stop the simulation whenever a criterion is fulfilled (for example, a neural integrator exceeds a certain threshold), not after a fixed amount of time. There is the possibility to define a stop_condition at the Population level: pop1 = Population ( ... , stop_condition = \"r > 1.0\" ) When calling the simulate_until() method instead of simulate() : t = simulate_until ( max_duration = 1000.0 , populations = pop1 ) the simulation will be stopped whenever the stop_condition of pop1 is met, i.e. when the firing rate of any neuron of pop1 is above 1.0. If the condition is never met, the simulation will last maximally max_duration . The methods returns the effective duration of the simulation (to compute reaction times, for example). The stop_condition can use any logical operation on the parameters and variables of the neuron associated to the population: pop1 = Population ( ... , stop_condition = \"(r > 1.0) and (mp < 2.0)\" ) By default, the simulation stops when at least one neuron in the population fulfills the criterion. If you want to stop the simulation when all neurons fulfill the condition, you can use the flag all after the condition: pop1 = Population ( ... , stop_condition = \"r > 1.0 : all\" ) The flag any is the default behavior and can be omitted. The stop criterion can depend on several populations, by providing a list of populations to the populations argument instead of a single population: t = simulate_until ( max_duration = 1000.0 , populations = [ pop1 , pop2 ]) The simulation will then stop when the criterion is met in both populations at the same time. If you want that the simulation stops when at least one population meets its criterion, you can specify the operator argument: t = simulate_until ( max_duration = 1000.0 , populations = [ pop1 , pop2 ], operator = 'or' ) The default value of operator is a 'and' function between the populations' criteria. Warning Global operations (min, max, mean) are not possible inside the stop_condition . If you need them, store them in a variable in the equations argument of the neuron and use it as the condition: equations = \"\"\" r = ... max_r = max(r) \"\"\"","title":"Early-stopping"},{"location":"manual/Simulation.html#setting-inputs-periodically","text":"In most cases, your simulation will be decomposed into a series of fixed-duration trials, where you basically set inputs at the beginning of the trial, run the simulation for a fixed duration, and possibly read out results at the end: # Iterate over 100 trials result = [] for trial in range ( 100 ): # Set inputs to the network pop . I = Uniform ( 0.0 , 1.0 ) # Simulate for 1 second simulate ( 1000. ) # Save the output result . append ( pop . r ) For convenience, we provide the decorator every , which allows to register a python method and call it automatically during the simulation with a fixed period: result = [] @every ( period = 1000. ) def set inputs ( n ): # Set inputs to the network pop . I = Uniform ( 0.0 , 1.0 ) # Save the output of the previous step if n > 0 : result . append ( pop . r ) simulate ( 100 * 1000. ) In this example, set_inputs() will be executed just before the steps corresponding to times t = 0., 1000., 2000., and so on until t = 100000. The method can have any name, but must accept only one argument, the integer n which will be incremented at each call of the method (i.e. it will take the values 0, 1, 2 until 99). This can for example be used to access data in a numpy array: images = np . random . random (( 100 , 640 , 480 )) @every ( period = 1000. ) def set inputs ( n ): # Set inputs to the network pop . I = images [ n , :, :] simulate ( 100 * 1000. ) One can define several methods that will be called in the order of their definition: @every ( period = 1000. ) def set inputs ( n ): pop . I = 1.0 @every ( period = 1000. ) def reset inputs ( n ): pop . I = 0.0 In this example, set_inputs() will be called first, followed by reset_inputs , so pop.I will finally be 0.0. The decorator every accepts an argument offset defining a delay within the period to call the method: @every ( period = 1000. ) def set inputs ( n ): pop . I = 1.0 @every ( period = 1000. , offset = 500. ) def reset inputs ( n ): pop . I = 0.0 In this case, set_inputs() will be called at times 0, 1000, 2000... while reset_inputs() will be called at times 500, 1500, 2500..., allowing to structure a trial more effectively. The offset can be set negative, in which case it will be relative to the end of the trial: @every ( period = 1000. , offset =- 100. ) def reset inputs ( n ): pop . I = 0.0 In this example, the method will be called at times 900, 1900, 2900 and so on. The offset value can not be longer than the period , by definition. If you try to do so, a modulo operation will anyway be applied (i.e. an offset of 1500 with a period of 1000 becomes 500). Finally, the wait argument allows to delay the first call to the method from a fixed interval: @every ( period = 1000. , wait = 5000. ) def reset inputs ( n ): pop . I = 0.0 In this case, the method will be called at times 5000, 6000 and so on. Between two calls to simulate() , the callbacks can be disabled or re-enabled using the following methods: @every ( period = 1000. ) def reset inputs ( n ): pop . I = 0.0 # Simulate with callbacks simulate ( 10000. ) # Disable callbacks disable_callbacks () # Simulate without callbacks simulate ( 10000. ) # Re-enable callbacks enable_callbacks () # Simulate with callbacks simulate ( 10000. ) Note that the period is always relative to the time when simulate() is called, so if no offset is defined, the callbacks will be called before the first step of a simulation, no matter how long the previous simulation lasted. In the current state, it is not possible yet to enable/disable callbacks selectively, it is all or none. Callbacks can only be used with simulate() , not with step() or simulate_until() .","title":"Setting inputs periodically"},{"location":"manual/SpikeNeuron.html","text":"Spiking neurons # Contrary to rate-coded neurons, the use of spiking neurons requires the additional definition of a spike condition (the criteria defining the emission of a spike, typically when the membrane potential exceeds a threshold) and reset equations, governing the evolution of all variables after a spike is emitted. Built-in neurons # ANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN ( http://neuralensemble.org/docs/PyNN/reference/neuronmodels.html ). Their definition (parameters, equations) are described in Specific Neurons . The classes can be used directly when creating the populations (no need to instantiate them). Example: pop = Population ( geometry = 1000 , neuron = Izhikevich ) User-defined neurons # Let's consider a simple leaky integrate-and-fire spiking neuron model (LIF) using a voltage-gated excitatory conductance: \\[\\tau \\cdot \\frac{ d v(t) }{ dt } = (E_r - v(t) ) + g_\\text{exc}(t) \\cdot (E_e - v(t) )\\] where \\(v(t)\\) is the membrane potential, \\(\\tau\\) is the membrane time constant (in milliseconds), \\(E_r\\) the resting potential, \\(E_e\\) the target potential for excitatory synapses and \\(g_\\text{exc}(t)\\) the total current induced by excitatory synapses. This neural model can be defined in ANNarchy by: LIF = Neuron ( parameters = \"\"\" tau = 10.0 : population Er = -60.0 : population Ee = 0.0 : population T = -45.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0 \"\"\" , spike = \"v > T\" , reset = \"v = Er\" , refractory = 5.0 ) As for rate-coded neurons, the parameters are defined in the parameters description, here globally for the population. equations contains the description of the ODE followed by the membrane potential. The additional information to provide is: spike : a boolean condition on a single variable (typically the membrane potential) deciding when a spike is emitted. reset : the modifications to the neuron's variables after a spike is emitted (typically, clamping the membrane potential to its reset potential). refractory : optionally a refractory period in ms. Spike condition # The spike condition is a single constraint definition. You may use the different available comparison operators (>, \\<, ==, etc) on a single neuron variable, using as many parameters as you want. The use of assignment statements or ODEs will lead to an error. Conditional statements can be used. Example: parameters = \"\"\" ... T = -45.0 \"\"\" , equations = \"\"\" prev_v = v noise = Uniform (-5.0, 5.0) tau*dv/dt = E - v + g_exc \"\"\" , spike = \"\"\" (v > T + noise) and (prev_v < T + noise) \"\"\" Reset # Here you define the variables which should be set to certain values after a spike occured. Any assignment statements is allowed ( = , += , etc), but the use of ODEs is not possible, as the reset is performed only once at the end of the time step. Example: reset = \"\"\" v = Er u += 0.1 \"\"\" Conductances # Contrary to rate-coded neurons, spiking neurons use conductance variables to encode the received inputs, not weighted sums. In ANNarchy, the conductances are defined by g_ followed by the target name. For example, if a population receives excitatory input (target exc ) from another one, you can access the total conductance provoked by exc spikes with: tau * dv / dt + v = g_exc The dynamics of the conductance can be specified after its usage in the membrane potential equation. The default behaviour for conductances is an instantaneous reset (or infinitely fast exponential decay). In practice, this means that all incoming spikes are summed up (weighted by the synaptic efficiency) at the beginning of a simulation step, and the resulting conductance is reset to 0.0 at the end of the step. This default behaviour is equivalent to : LIF = Neuron ( parameters = \"\"\" ... \"\"\" , equations = \"\"\" tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0 g_exc = 0.0 \"\"\" , spike = \" ... \" , reset = \" ... \" ) Incoming spikes increase g_exc and can provoke a post-synaptic spike at the next step, but leave no trace beyond that point. Most models however use exponentially decaying synapses , where the conductance decays with a short time constant after a spike is received. This behavior should be explicitely specified in the neuron's equations: LIF = Neuron ( parameters = \"\"\" ... \"\"\" , equations = \"\"\" tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0 tau_exc * dg_exc/dt = - g_exc \"\"\" , spike = \" ... \" , reset = \" ... \" ) g_exc is increased by incoming spikes, and slowly decays back to 0.0 until the next spikes arrive. Refractory period # The refractory period in milliseconds is specified by the refractory parameter of Neuron . LIF = Neuron ( parameters = \"\"\" ... \"\"\" , equations = \"\"\" ... \"\"\" , spike = \"\"\" ... \"\"\" , reset = \"\"\" v = c u += d \"\"\" , refractory = 5.0 ) The refractory argument can be a floating value or the name of a parameter/variable (string). If dt = 0.1 , this means that the equations will not be evaluated for 50 consecutive steps after a spike is emitted, except for the conductances (starting with g_ ) which are evaluated normally during the refractory period (the neuron is not \\\"deaf\\\", it only is frozen in a refractory state). refractory becomes an attribute of a spiking Population object, so it can be set specifically for a population even when omitted in the neuron definition: LIF = Neuron ( parameters = \" ... \" , equations = \" ... \" , spike = \" ... \" , reset = \"\"\" v = c u += d \"\"\" ) pop = Population ( geometry = 1000 , neuron = LIF ) pop . refractory = Uniform ( 1.0 , 10.0 ) It can be either a single value, a RandomDistribution object or a Numpy array of the same size/geometry as the population. Instantaneous firing rate # Method 1: ISI Spiking neurons define an additional variable t_last which represents the timestamp (in ms) of the last emitted spike (updated at the end of the reset statement). The time elapsed since the last spike is then t - t_last . This can be used to update the instantaneous firing rate of a neuron, by inverting the inter-spike interval (ISI) during the reset statement following the emission of a spike: neuron = Neuron ( parameters = \"tau = 20.0; tauf = 1000.\" , equations = \"\"\" tau * dv/dt + v = ... tauf * df/dt = -f \"\"\" , spike = \"v > 1.0\" , reset = \"\"\" v = 0.0 f = 1000./(t - t_last) \"\"\" ) Here, a leaky integrator on f is needed to 1) smooth the firing rate and 2) slowly decay to 0 when the neuron stops firing. This method reflects very fast changes in the firing rate, but is also very sensible to noise. Method 2: Window A more stable way to compute the firing rate of a neuron is to count at each time step the number of spikes emitted during a sliding temporal window (of 100 ms or 1s for example). By default, spiking neurons only record the time of the last spike they emitted ( t_last ), so this mechanism has to be explicitely enabled by calling the compute_firing_rate() method of the desired population: pop = Population ( 100 , Izhikevich ) pop . compute_firing_rate ( window = 1000.0 ) The window argument represents the period in milliseconds over which the spikes will be counted. The resulting firing rate (in Hz) will be stored in the local variable r (as for rate-coded neurons), which can be accessed by the neuron itself or by incoming and outgoing synapse ( pre.r and post.r ). If the method has not been called, the variable r of a spiking neuron will be constantly 0.0. Warning The window method is not available on CUDA yet.","title":"Spiking neurons"},{"location":"manual/SpikeNeuron.html#spiking-neurons","text":"Contrary to rate-coded neurons, the use of spiking neurons requires the additional definition of a spike condition (the criteria defining the emission of a spike, typically when the membrane potential exceeds a threshold) and reset equations, governing the evolution of all variables after a spike is emitted.","title":"Spiking neurons"},{"location":"manual/SpikeNeuron.html#built-in-neurons","text":"ANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN ( http://neuralensemble.org/docs/PyNN/reference/neuronmodels.html ). Their definition (parameters, equations) are described in Specific Neurons . The classes can be used directly when creating the populations (no need to instantiate them). Example: pop = Population ( geometry = 1000 , neuron = Izhikevich )","title":"Built-in neurons"},{"location":"manual/SpikeNeuron.html#user-defined-neurons","text":"Let's consider a simple leaky integrate-and-fire spiking neuron model (LIF) using a voltage-gated excitatory conductance: \\[\\tau \\cdot \\frac{ d v(t) }{ dt } = (E_r - v(t) ) + g_\\text{exc}(t) \\cdot (E_e - v(t) )\\] where \\(v(t)\\) is the membrane potential, \\(\\tau\\) is the membrane time constant (in milliseconds), \\(E_r\\) the resting potential, \\(E_e\\) the target potential for excitatory synapses and \\(g_\\text{exc}(t)\\) the total current induced by excitatory synapses. This neural model can be defined in ANNarchy by: LIF = Neuron ( parameters = \"\"\" tau = 10.0 : population Er = -60.0 : population Ee = 0.0 : population T = -45.0 : population \"\"\" , equations = \"\"\" tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0 \"\"\" , spike = \"v > T\" , reset = \"v = Er\" , refractory = 5.0 ) As for rate-coded neurons, the parameters are defined in the parameters description, here globally for the population. equations contains the description of the ODE followed by the membrane potential. The additional information to provide is: spike : a boolean condition on a single variable (typically the membrane potential) deciding when a spike is emitted. reset : the modifications to the neuron's variables after a spike is emitted (typically, clamping the membrane potential to its reset potential). refractory : optionally a refractory period in ms.","title":"User-defined neurons"},{"location":"manual/SpikeNeuron.html#spike-condition","text":"The spike condition is a single constraint definition. You may use the different available comparison operators (>, \\<, ==, etc) on a single neuron variable, using as many parameters as you want. The use of assignment statements or ODEs will lead to an error. Conditional statements can be used. Example: parameters = \"\"\" ... T = -45.0 \"\"\" , equations = \"\"\" prev_v = v noise = Uniform (-5.0, 5.0) tau*dv/dt = E - v + g_exc \"\"\" , spike = \"\"\" (v > T + noise) and (prev_v < T + noise) \"\"\"","title":"Spike condition"},{"location":"manual/SpikeNeuron.html#reset","text":"Here you define the variables which should be set to certain values after a spike occured. Any assignment statements is allowed ( = , += , etc), but the use of ODEs is not possible, as the reset is performed only once at the end of the time step. Example: reset = \"\"\" v = Er u += 0.1 \"\"\"","title":"Reset"},{"location":"manual/SpikeNeuron.html#conductances","text":"Contrary to rate-coded neurons, spiking neurons use conductance variables to encode the received inputs, not weighted sums. In ANNarchy, the conductances are defined by g_ followed by the target name. For example, if a population receives excitatory input (target exc ) from another one, you can access the total conductance provoked by exc spikes with: tau * dv / dt + v = g_exc The dynamics of the conductance can be specified after its usage in the membrane potential equation. The default behaviour for conductances is an instantaneous reset (or infinitely fast exponential decay). In practice, this means that all incoming spikes are summed up (weighted by the synaptic efficiency) at the beginning of a simulation step, and the resulting conductance is reset to 0.0 at the end of the step. This default behaviour is equivalent to : LIF = Neuron ( parameters = \"\"\" ... \"\"\" , equations = \"\"\" tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0 g_exc = 0.0 \"\"\" , spike = \" ... \" , reset = \" ... \" ) Incoming spikes increase g_exc and can provoke a post-synaptic spike at the next step, but leave no trace beyond that point. Most models however use exponentially decaying synapses , where the conductance decays with a short time constant after a spike is received. This behavior should be explicitely specified in the neuron's equations: LIF = Neuron ( parameters = \"\"\" ... \"\"\" , equations = \"\"\" tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0 tau_exc * dg_exc/dt = - g_exc \"\"\" , spike = \" ... \" , reset = \" ... \" ) g_exc is increased by incoming spikes, and slowly decays back to 0.0 until the next spikes arrive.","title":"Conductances"},{"location":"manual/SpikeNeuron.html#refractory-period","text":"The refractory period in milliseconds is specified by the refractory parameter of Neuron . LIF = Neuron ( parameters = \"\"\" ... \"\"\" , equations = \"\"\" ... \"\"\" , spike = \"\"\" ... \"\"\" , reset = \"\"\" v = c u += d \"\"\" , refractory = 5.0 ) The refractory argument can be a floating value or the name of a parameter/variable (string). If dt = 0.1 , this means that the equations will not be evaluated for 50 consecutive steps after a spike is emitted, except for the conductances (starting with g_ ) which are evaluated normally during the refractory period (the neuron is not \\\"deaf\\\", it only is frozen in a refractory state). refractory becomes an attribute of a spiking Population object, so it can be set specifically for a population even when omitted in the neuron definition: LIF = Neuron ( parameters = \" ... \" , equations = \" ... \" , spike = \" ... \" , reset = \"\"\" v = c u += d \"\"\" ) pop = Population ( geometry = 1000 , neuron = LIF ) pop . refractory = Uniform ( 1.0 , 10.0 ) It can be either a single value, a RandomDistribution object or a Numpy array of the same size/geometry as the population.","title":"Refractory period"},{"location":"manual/SpikeNeuron.html#instantaneous-firing-rate","text":"Method 1: ISI Spiking neurons define an additional variable t_last which represents the timestamp (in ms) of the last emitted spike (updated at the end of the reset statement). The time elapsed since the last spike is then t - t_last . This can be used to update the instantaneous firing rate of a neuron, by inverting the inter-spike interval (ISI) during the reset statement following the emission of a spike: neuron = Neuron ( parameters = \"tau = 20.0; tauf = 1000.\" , equations = \"\"\" tau * dv/dt + v = ... tauf * df/dt = -f \"\"\" , spike = \"v > 1.0\" , reset = \"\"\" v = 0.0 f = 1000./(t - t_last) \"\"\" ) Here, a leaky integrator on f is needed to 1) smooth the firing rate and 2) slowly decay to 0 when the neuron stops firing. This method reflects very fast changes in the firing rate, but is also very sensible to noise. Method 2: Window A more stable way to compute the firing rate of a neuron is to count at each time step the number of spikes emitted during a sliding temporal window (of 100 ms or 1s for example). By default, spiking neurons only record the time of the last spike they emitted ( t_last ), so this mechanism has to be explicitely enabled by calling the compute_firing_rate() method of the desired population: pop = Population ( 100 , Izhikevich ) pop . compute_firing_rate ( window = 1000.0 ) The window argument represents the period in milliseconds over which the spikes will be counted. The resulting firing rate (in Hz) will be stored in the local variable r (as for rate-coded neurons), which can be accessed by the neuron itself or by incoming and outgoing synapse ( pre.r and post.r ). If the method has not been called, the variable r of a spiking neuron will be constantly 0.0. Warning The window method is not available on CUDA yet.","title":"Instantaneous firing rate"},{"location":"manual/SpikeSynapse.html","text":"Spiking synapses # Synapses in spiking networks differ from rate-coded synapses in that they are event-driven, i.e. the most important changes occur whenever a pre- or post-synaptic spike is emitted. For this reason, additional arguments have to be passed to the Synapse object. Increase of conductance after a pre-synaptic spike # In the simplest case, a pre-synaptic spike increases a target conductance value in the post-synaptic neuron. The rule defining how this conductance is modified has to be placed in the pre_spike argument of a Synapse object. The default spiking synapse in ANNarchy is equivalent to: DefaultSynapse = Synapse ( parameters = \"w=0.0\" , equations = \"\" , pre_spike = \"\"\" g_target += w \"\"\" ) The only thing it does is to increase the conductance g_target of the post-synaptic neuron (for example g_exc if the target is exc ) every time a pre-synaptic spike arrives at the synapse, proportionally to the synaptic efficiency w of the synapse. Note that w is implicitely defined in all synapses, you will never need to define it explicitely. You can override this default behavior by providing a new Synapse object when building a Projection . For example, you may want to implement a \\\"fatigue\\\" mechanism for the synapse, transciently reducing the synaptic efficiency when the pre-synaptic neuron fires too strongly. One solution would be to decrease a synaptic variable everytime a pre-synaptic spike is received and increase the post-synaptic conductance proportionally to this value. When no spike is received, this trace variable should slowly return to its maximal value. FatigueSynapse = Synapse ( parameters = \"\"\" tau = 1000 : postsynaptic # Time constant of the trace is 1 second dec = 0.05 : postsynaptic # Decrement of the trace \"\"\" , equations = \"\"\" tau * dtrace/dt + trace = 1.0 : min = 0.0 \"\"\" , pre_spike = \"\"\" g_target += w * trace trace -= dec \"\"\" ) Each time a pre-synaptic spike occurs, the post-synaptic conductance is increased from w*trace . As the baseline of trace is 1.0 (as defined in equations ), this means that a \\\"fresh\\\" synapse will use the full synaptic efficiency. However, after each pre-synaptic spike, trace is decreased from dec = 0.05 , meaning that the \\\"real\\\" synaptic efficiency can go down to 0.0 (the minimal value of trace) if the pre-synaptic neuron fires too often. It is important here to restrict trace to positive values with the flags min=0.0 , as it could otherwise transform an excitatory synapse into an inhibitory one. Hint It is obligatory to use the keyword g_target for the post-synaptic conductance. This value relates to the corresponding value in post-synaptic neuron: The target will be replaced with the projection's target (for example exc or inh ). So if you use this synapse in a projection with target = \\'exc\\', the value of g_exc in post-synaptic neuron will be automatically replaced. Synaptic plasticity # In spiking networks, there are usually two methods to implement event-driven synaptic plasticity (see the entry on STDP at Scholarpedia ): by using the difference in spike times between the pre- and post-synaptic neurons; by using online implementations. Using spike-time differences # A Synapse has access to two specific variables: t_pre corresponding to the time of the last pre-synaptic spike in milliseconds. t_post corresponding to the time of the last post-synaptic spike in milliseconds. These times are relative to the creation of the network, so they only make sense when compared to each other or to t . Spike-timing dependent plasticity can for example be implemented the following way: STDP = Synapse ( parameters = \"\"\" tau_pre = 10.0 : projection tau_post = 10.0 : projection cApre = 0.01 : projection cApost = 0.0105 : projection wmax = 0.01 : projection \"\"\" , pre_spike = \"\"\" g_target += w w = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \"\"\" , post_spike = \"\"\" w = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax) \"\"\" ) Every time a pre-synaptic spike arrives at the synapse ( pre_spike ), the post-synaptic conductance is increased from the current value of the synaptic efficiency. g_target += w When a synapse object is defined, this behavior should be explicitely declared. The value w is then decreased using a decreasing exponential function of the time elapsed since the last post-synaptic spike: w = clip ( w - cApost * exp (( t_post - t ) / tau_post ) , 0.0 , wmax ) The clip() global function is there to ensure that w is bounded between 0.0 and wmax . As t >= t_post , the exponential part is smaller than 1.0. The pre_spike argument therefore ensures that the synapse is depressed is a pre-synaptic spike occurs shortly after a post-synaptic one. \\\"Shortly\\\" is quantified by the time constant tau_post , usually in the range of 10 ms. Every time a post-synaptic spike is emitted ( post_spike ), the value w is increased proportionally to the time elapsed since the last pre-synaptic spike: w = clip ( w + cApre * exp (( t_pre - t ) / tau_pre ) , 0.0 , wmax ) This term defines the potentiation of a synapse when a pre-synaptic spike is followed immediately by a post-synaptic one: the inferred causality between the two events should be reinforced. Warning Only the last pre- and post-synaptic spikes are accessible, not the whole history. Only nearest-neighbor spike-interactions are possible using ANNarchy, not temporal all-to-all interactions where the whole spike history is used for learning (see the entry on STDP at Scholarpedia ). Some networks may not work properly when using this simulation mode. For example, whenever the pre-synaptic neurons fires twice in a very short interval and causes a post-synaptic spike, the corresponding weight should be reinforced twice. With the proposed STDP rule, it would be reinforced only once. It is therefore generally advised to use online versions of STDP. Online version # The online version of STDP requires two synaptic traces, which are increased whenever a pre- resp. post-synaptic spike is perceived, and decay with their own dynamics in between. Using the same vocabulary as Brian, such an implementation would be: STDP_online = Synapse ( parameters = \"\"\" tau_pre = 10.0 : projection tau_post = 10.0 : projection cApre = 0.01 : projection cApost = 0.0105 : projection wmax = 0.01 : projection \"\"\" , equations = \"\"\" tau_pre * dApre/dt = - Apre : event-driven tau_post * dApost/dt = - Apost : event-driven \"\"\" , pre_spike = \"\"\" g_target += w Apre += cApre * wmax w = clip(w - Apost, 0.0 , wmax) \"\"\" , post_spike = \"\"\" Apost += cApost * wmax w = clip(w + Apre, 0.0 , wmax) \"\"\" ) The variables Apre and Apost are exponentially decreasing traces of pre- and post-synaptic spikes, as shown by the leaky integration in equations . When a pre-synaptic spike is emitted, Apre is incremented, the conductance level of the post-synaptic neuron g_target too, and the synaptic efficiency is decreased proportionally to Apost (this means that if a post-synaptic spike was emitted shortly before, LTD will strongly be applied, while if it was longer ago, no major change will be observed). When a post-synaptic spike is observed, Apost increases and the synaptic efficiency is increased proportionally to Apre . The effect of this online version is globally the same as the spike timing dependent version, except that the history of pre- and post-synaptic spikes is fully contained in the variables Apre and Apost . The event-driven keyword allows event-driven integration of the variables Apre and Apost . This means the equations are not updated at each time step, but only when a pre- or post-synaptic spike occurs at the synapse. This is only possible because the two variables follow linear first-order ODEs. The event-driven integration method allows to spare a lot of computations if the number of spikes is not too high in the network. Order of evaluation # Three types of updates are potentially executed at every time step: Pre-synaptic events, defined by pre_spike and triggered after each pre-synaptic spike, after a delay of at least dt . Synaptic variables defined by equations . Post-synaptic events, defined by post_spike and triggered after each post-synaptic spike, without delay. These updates are conducted in that order at each time step. First, all spikes emitted in the previous step (or earlier if there are delays) are propagated to the corresponding synapses and influence variables there (especially conductance increases), then all synaptic variables are updated according to their ODE (after the neurons\\' equations are updated), then all neurons which have emitted a spike in the current step modify their synapses. A potential problem arises when a pre-synaptic and a post-synaptic spike are emitted at the same time. STDP-like plasticity rules are usually not defined when the spike time difference is 0, as the two spikes can not be correlated in that case (the pre-spike can not possibly be the cause of the post-spike). By default, both event-driven updates ( pre_spike leading to LTD, post_spike leading to LTP) will be conducted when the spikes are emitted at the same time. This can be problematic for some plastic models, for example the simple_stdp example provided in the source code. To avoid this problem, the flag unless_post can be specified in pre_spike to indicate that the corresponding variable should be updated after each pre-synaptic spike, unless the post-synaptic neuron also fired at the previous time step. Without even-driven integration, the online STDP learning rule would become: STDP_online = Synapse ( parameters = \"\"\" tau_pre = 10.0 : projection tau_post = 10.0 : projection cApre = 0.01 : projection cApost = 0.0105 : projection wmax = 0.01 : projection \"\"\" , equations = \"\"\" tau_pre * dApre/dt = - Apre tau_post * dApost/dt = - Apost \"\"\" , pre_spike = \"\"\" g_target += w Apre += cApre : unless_post w = clip(w - Apost, 0.0 , wmax) : unless_post \"\"\" , post_spike = \"\"\" Apost += cApost w = clip(w + Apre, 0.0 , wmax) \"\"\" ) Continuous synaptic transmission # In some cases, synaptic transmission cannot be described in an event-driven framework. Synapses using the NMDA neurotransmitter are for example often modeled as non-linear synapses. Non-linear synapses can require the post-synaptic conductance to be a sum of synapse-specific variables, as for rate-coded neurons, and not simply incremented when a pre-synaptic spike occurs. NMDA synapses can be represented by two variables \\(x(t)\\) and \\(g(t)\\) following first-order ODEs: \\[\\begin{aligned} \\begin{aligned} \\tau \\cdot \\frac{dx(t)}{dt} &= - x(t) \\\\ \\tau \\cdot \\frac{dg(t)}{dt} &= - g(t) + x(t) \\cdot (1 - g(t)) \\end{aligned} \\end{aligned}\\] When a pre-synaptic spike occurs, \\(x(t)\\) is incremented by the weight \\(w(t)\\) . However, it does not influence directly the post-synaptic neuron, as the output of a synapse is the signal \\(g(t)\\) . The post-synaptic conductance is defined at each time \\(t\\) as the sum over all synapses of the same type of their variable \\(g(t)\\) : \\[g_\\text{exc}(t) = \\sum_{i=1}^{N_\\text{exc}} g_i (t)\\] Such a synapse could be implemented the following way: NMDA = Synapse( parameters = \"\"\" tau = 10.0 : projection \"\"\", equations = \"\"\" tau * dx/dt = -x tau * dg/dt = -g + x * (1 -g) \"\"\", pre_spike = \"x += w\", psp = \"g\" ) The synapse defines a psp argument which means that the output of this synapse is non-linear and the post-synaptic conductance should be summed over this value ( g in this case). It is not possible to use the event-driven integration scheme for such non-linear synapses.","title":"Spiking synapses"},{"location":"manual/SpikeSynapse.html#spiking-synapses","text":"Synapses in spiking networks differ from rate-coded synapses in that they are event-driven, i.e. the most important changes occur whenever a pre- or post-synaptic spike is emitted. For this reason, additional arguments have to be passed to the Synapse object.","title":"Spiking synapses"},{"location":"manual/SpikeSynapse.html#increase-of-conductance-after-a-pre-synaptic-spike","text":"In the simplest case, a pre-synaptic spike increases a target conductance value in the post-synaptic neuron. The rule defining how this conductance is modified has to be placed in the pre_spike argument of a Synapse object. The default spiking synapse in ANNarchy is equivalent to: DefaultSynapse = Synapse ( parameters = \"w=0.0\" , equations = \"\" , pre_spike = \"\"\" g_target += w \"\"\" ) The only thing it does is to increase the conductance g_target of the post-synaptic neuron (for example g_exc if the target is exc ) every time a pre-synaptic spike arrives at the synapse, proportionally to the synaptic efficiency w of the synapse. Note that w is implicitely defined in all synapses, you will never need to define it explicitely. You can override this default behavior by providing a new Synapse object when building a Projection . For example, you may want to implement a \\\"fatigue\\\" mechanism for the synapse, transciently reducing the synaptic efficiency when the pre-synaptic neuron fires too strongly. One solution would be to decrease a synaptic variable everytime a pre-synaptic spike is received and increase the post-synaptic conductance proportionally to this value. When no spike is received, this trace variable should slowly return to its maximal value. FatigueSynapse = Synapse ( parameters = \"\"\" tau = 1000 : postsynaptic # Time constant of the trace is 1 second dec = 0.05 : postsynaptic # Decrement of the trace \"\"\" , equations = \"\"\" tau * dtrace/dt + trace = 1.0 : min = 0.0 \"\"\" , pre_spike = \"\"\" g_target += w * trace trace -= dec \"\"\" ) Each time a pre-synaptic spike occurs, the post-synaptic conductance is increased from w*trace . As the baseline of trace is 1.0 (as defined in equations ), this means that a \\\"fresh\\\" synapse will use the full synaptic efficiency. However, after each pre-synaptic spike, trace is decreased from dec = 0.05 , meaning that the \\\"real\\\" synaptic efficiency can go down to 0.0 (the minimal value of trace) if the pre-synaptic neuron fires too often. It is important here to restrict trace to positive values with the flags min=0.0 , as it could otherwise transform an excitatory synapse into an inhibitory one. Hint It is obligatory to use the keyword g_target for the post-synaptic conductance. This value relates to the corresponding value in post-synaptic neuron: The target will be replaced with the projection's target (for example exc or inh ). So if you use this synapse in a projection with target = \\'exc\\', the value of g_exc in post-synaptic neuron will be automatically replaced.","title":"Increase of conductance after a pre-synaptic spike"},{"location":"manual/SpikeSynapse.html#synaptic-plasticity","text":"In spiking networks, there are usually two methods to implement event-driven synaptic plasticity (see the entry on STDP at Scholarpedia ): by using the difference in spike times between the pre- and post-synaptic neurons; by using online implementations.","title":"Synaptic plasticity"},{"location":"manual/SpikeSynapse.html#using-spike-time-differences","text":"A Synapse has access to two specific variables: t_pre corresponding to the time of the last pre-synaptic spike in milliseconds. t_post corresponding to the time of the last post-synaptic spike in milliseconds. These times are relative to the creation of the network, so they only make sense when compared to each other or to t . Spike-timing dependent plasticity can for example be implemented the following way: STDP = Synapse ( parameters = \"\"\" tau_pre = 10.0 : projection tau_post = 10.0 : projection cApre = 0.01 : projection cApost = 0.0105 : projection wmax = 0.01 : projection \"\"\" , pre_spike = \"\"\" g_target += w w = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \"\"\" , post_spike = \"\"\" w = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax) \"\"\" ) Every time a pre-synaptic spike arrives at the synapse ( pre_spike ), the post-synaptic conductance is increased from the current value of the synaptic efficiency. g_target += w When a synapse object is defined, this behavior should be explicitely declared. The value w is then decreased using a decreasing exponential function of the time elapsed since the last post-synaptic spike: w = clip ( w - cApost * exp (( t_post - t ) / tau_post ) , 0.0 , wmax ) The clip() global function is there to ensure that w is bounded between 0.0 and wmax . As t >= t_post , the exponential part is smaller than 1.0. The pre_spike argument therefore ensures that the synapse is depressed is a pre-synaptic spike occurs shortly after a post-synaptic one. \\\"Shortly\\\" is quantified by the time constant tau_post , usually in the range of 10 ms. Every time a post-synaptic spike is emitted ( post_spike ), the value w is increased proportionally to the time elapsed since the last pre-synaptic spike: w = clip ( w + cApre * exp (( t_pre - t ) / tau_pre ) , 0.0 , wmax ) This term defines the potentiation of a synapse when a pre-synaptic spike is followed immediately by a post-synaptic one: the inferred causality between the two events should be reinforced. Warning Only the last pre- and post-synaptic spikes are accessible, not the whole history. Only nearest-neighbor spike-interactions are possible using ANNarchy, not temporal all-to-all interactions where the whole spike history is used for learning (see the entry on STDP at Scholarpedia ). Some networks may not work properly when using this simulation mode. For example, whenever the pre-synaptic neurons fires twice in a very short interval and causes a post-synaptic spike, the corresponding weight should be reinforced twice. With the proposed STDP rule, it would be reinforced only once. It is therefore generally advised to use online versions of STDP.","title":"Using spike-time differences"},{"location":"manual/SpikeSynapse.html#online-version","text":"The online version of STDP requires two synaptic traces, which are increased whenever a pre- resp. post-synaptic spike is perceived, and decay with their own dynamics in between. Using the same vocabulary as Brian, such an implementation would be: STDP_online = Synapse ( parameters = \"\"\" tau_pre = 10.0 : projection tau_post = 10.0 : projection cApre = 0.01 : projection cApost = 0.0105 : projection wmax = 0.01 : projection \"\"\" , equations = \"\"\" tau_pre * dApre/dt = - Apre : event-driven tau_post * dApost/dt = - Apost : event-driven \"\"\" , pre_spike = \"\"\" g_target += w Apre += cApre * wmax w = clip(w - Apost, 0.0 , wmax) \"\"\" , post_spike = \"\"\" Apost += cApost * wmax w = clip(w + Apre, 0.0 , wmax) \"\"\" ) The variables Apre and Apost are exponentially decreasing traces of pre- and post-synaptic spikes, as shown by the leaky integration in equations . When a pre-synaptic spike is emitted, Apre is incremented, the conductance level of the post-synaptic neuron g_target too, and the synaptic efficiency is decreased proportionally to Apost (this means that if a post-synaptic spike was emitted shortly before, LTD will strongly be applied, while if it was longer ago, no major change will be observed). When a post-synaptic spike is observed, Apost increases and the synaptic efficiency is increased proportionally to Apre . The effect of this online version is globally the same as the spike timing dependent version, except that the history of pre- and post-synaptic spikes is fully contained in the variables Apre and Apost . The event-driven keyword allows event-driven integration of the variables Apre and Apost . This means the equations are not updated at each time step, but only when a pre- or post-synaptic spike occurs at the synapse. This is only possible because the two variables follow linear first-order ODEs. The event-driven integration method allows to spare a lot of computations if the number of spikes is not too high in the network.","title":"Online version"},{"location":"manual/SpikeSynapse.html#order-of-evaluation","text":"Three types of updates are potentially executed at every time step: Pre-synaptic events, defined by pre_spike and triggered after each pre-synaptic spike, after a delay of at least dt . Synaptic variables defined by equations . Post-synaptic events, defined by post_spike and triggered after each post-synaptic spike, without delay. These updates are conducted in that order at each time step. First, all spikes emitted in the previous step (or earlier if there are delays) are propagated to the corresponding synapses and influence variables there (especially conductance increases), then all synaptic variables are updated according to their ODE (after the neurons\\' equations are updated), then all neurons which have emitted a spike in the current step modify their synapses. A potential problem arises when a pre-synaptic and a post-synaptic spike are emitted at the same time. STDP-like plasticity rules are usually not defined when the spike time difference is 0, as the two spikes can not be correlated in that case (the pre-spike can not possibly be the cause of the post-spike). By default, both event-driven updates ( pre_spike leading to LTD, post_spike leading to LTP) will be conducted when the spikes are emitted at the same time. This can be problematic for some plastic models, for example the simple_stdp example provided in the source code. To avoid this problem, the flag unless_post can be specified in pre_spike to indicate that the corresponding variable should be updated after each pre-synaptic spike, unless the post-synaptic neuron also fired at the previous time step. Without even-driven integration, the online STDP learning rule would become: STDP_online = Synapse ( parameters = \"\"\" tau_pre = 10.0 : projection tau_post = 10.0 : projection cApre = 0.01 : projection cApost = 0.0105 : projection wmax = 0.01 : projection \"\"\" , equations = \"\"\" tau_pre * dApre/dt = - Apre tau_post * dApost/dt = - Apost \"\"\" , pre_spike = \"\"\" g_target += w Apre += cApre : unless_post w = clip(w - Apost, 0.0 , wmax) : unless_post \"\"\" , post_spike = \"\"\" Apost += cApost w = clip(w + Apre, 0.0 , wmax) \"\"\" )","title":"Order of evaluation"},{"location":"manual/SpikeSynapse.html#continuous-synaptic-transmission","text":"In some cases, synaptic transmission cannot be described in an event-driven framework. Synapses using the NMDA neurotransmitter are for example often modeled as non-linear synapses. Non-linear synapses can require the post-synaptic conductance to be a sum of synapse-specific variables, as for rate-coded neurons, and not simply incremented when a pre-synaptic spike occurs. NMDA synapses can be represented by two variables \\(x(t)\\) and \\(g(t)\\) following first-order ODEs: \\[\\begin{aligned} \\begin{aligned} \\tau \\cdot \\frac{dx(t)}{dt} &= - x(t) \\\\ \\tau \\cdot \\frac{dg(t)}{dt} &= - g(t) + x(t) \\cdot (1 - g(t)) \\end{aligned} \\end{aligned}\\] When a pre-synaptic spike occurs, \\(x(t)\\) is incremented by the weight \\(w(t)\\) . However, it does not influence directly the post-synaptic neuron, as the output of a synapse is the signal \\(g(t)\\) . The post-synaptic conductance is defined at each time \\(t\\) as the sum over all synapses of the same type of their variable \\(g(t)\\) : \\[g_\\text{exc}(t) = \\sum_{i=1}^{N_\\text{exc}} g_i (t)\\] Such a synapse could be implemented the following way: NMDA = Synapse( parameters = \"\"\" tau = 10.0 : projection \"\"\", equations = \"\"\" tau * dx/dt = -x tau * dg/dt = -g + x * (1 -g) \"\"\", pre_spike = \"x += w\", psp = \"g\" ) The synapse defines a psp argument which means that the output of this synapse is non-linear and the post-synaptic conductance should be summed over this value ( g in this case). It is not possible to use the event-driven integration scheme for such non-linear synapses.","title":"Continuous synaptic transmission"},{"location":"manual/StructuralPlasticity.html","text":"Structural plasticity # ANNarchy supports the dynamic addition/suppression of synapses during the simulation (i.e. after compilation). Warning Structural plasticity is not available with the CUDA backend and will likely never be... Because structural plasticity adds some complexity to the generated code, it has to be enabled before compilation by setting the structural_plasticity flag to True in the call to setup() : setup ( structural_plasticity = True ) If the flag is not set, the following methods will do nothing. There are two possibilities to dynamically create or delete synapses: Externally, using methods at the dendrite level from Python. Internally, by defining conditions for creating/pruning in the synapse description. Dendrite level # Two methods of the Dendrite class are available for creating/deleting synapses: create_synapse() prune_synapse() Creating synapses # Let's suppose that we want to add regularly new synapses between strongly active but not yet connected neurons with a low probability. One could for example define a neuron type with an additional variable averaging the firing rate over a long period of time. LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 tau_mean = 100000.0 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) tau_mean * dmean_r/dt = (r - mean_r) : init = 0.0 \"\"\" ) Two populations are created and connected using a sparse connectivity: pop1 = Population ( 1000 , LeakyIntegratorNeuron ) pop2 = Population ( 1000 , LeakyIntegratorNeuron ) proj = Projection ( pop1 , pop2 , 'exc' , Oja ) proj . connect_fixed_probability ( weights = 1.0 , probability = 0.1 ) After an initial period of simulation, one could add new synapses between strongly active pair of neurons: # For all post-synaptic neurons for post in xrange ( pop2 . size ): # For all pre-synaptic neurons for pre in xrange ( pop1 . size ): # If the neurons are not connected yet if not pre in proj [ post ] . ranks : # If they are both sufficientely active if pop1 [ pre ] . mean_r * pop2 [ post ] . mean_r > 0.7 : # Add a synapse with weight 1.0 and the default delay proj [ post ] . create_synapse ( pre , 1.0 ) create_synapse only allows to specify the value of the weight and the delay. Other syanptic variables will take the value they would have had before compile(). If another value is desired, it should be explicitely set afterwards. Removing synapses # Removing useless synapses (pruning) is also possible. Let's consider a synapse type whose \\\"age\\\" is incremented as long as both pre- and post-synaptic neurons are inactive at the same time: AgingSynapse = Synapse ( equations = \"\"\" age = if pre.r * post.r > 0.0 : 0 else : age + 1 : init = 0, int \"\"\" ) One could periodically track the too \\\"old\\\" synapses and remove them: # Threshold on the age: T = 100000 # For all post-synaptic neurons receiving synapses for post in proj . post_ranks : # For all existing synapses for pre in proj [ post ] . ranks : # If the synapse is too old if proj [ post ][ pre ] . age > T : # Remove it proj [ post ] . prune_synapse ( pre ) Warning This form of structural plasticity is rather slow because: The for loops are in Python, not C++. Implementing this structural plasticity in Cython should already help. The memory allocated for the synapses of a projection may have to be displaced at another location. This can lead to massive transfer of data, slowing the simulation down. It is of course the user's responsability to balance synapse creation/destruction, otherwise projections could become either empty or fully connected on the long-term. Synapse level # Conditions for creating or deleting synapses can also be specified in the synapse description, through the creating or pruning arguments. Thise arguments accept string descriptions of the boolean conditions at which a synapse should be created/deleted, using the same notation as other arguments. Creating synapses # The creation of a synapse must be described by a boolean expression: CreatingSynapse = Synapse ( parameters = \" ... \" , equations = \" ... \" , creating = \"pre.mean_r * post.mean_r > 0.7 : proba = 0.5, w = 1.0\" ) The condition can make use of any pre- or post-synaptic variable, but NOT synaptic variables, as they obviously do not exist yet. Global parameters (defined with the postsynaptic or projection flags) can nevertheless be used. Several flags can be passed to the expression: proba specifies the probability according to which a synapse will be created, if the condition is met. The default is 1.0 (i.e. a synapse will be created whenever the condition is fulfilled). w specifies the value for the weight which will be created (default: 0.0). d specifies the delay (default: the same as all other synapses if the delay is constant in the projection, dt otherwise). Warning Note that the new value for the delay can not exceed the maximal delay in the projection, nor be different from the others if they were all equal. Other synaptic variables will take the default value after creation. Synapse creation is not automatically enabled at the start of the simulation: the Projectiom method start_creating() must be called: proj . start_creating ( period = 100.0 ) This method accepts a period parameter specifying how often the conditions for creating synapses will be checked (in ms). By default they would be checked at each time step ( dt ), what would be too costly. Similarly, the stop_creating() method can be called to stop the creation conditions from being checked. Deleting synapses # Synaptic pruning also rely on a boolean expression: PruningSynapse = Synapse ( parameters = \" T = 100000 : int, projection \" , equations = \"\"\" age = if pre.r * post.r > 0.0 : 0 else : age + 1 : init = 0, int\"\"\" , pruning = \"age > T : proba = 0.5\" ) A synapse type can combine creating and pruning arguments. The pruning argument can rely on synaptic variables (here age ), as the synapse already exist. Only the proba flag can be passed to specify the probability at which the synapse will be deleted if the condition is met. Pruning has to be started/stopped with the start_pruning() and stop_pruning() methods. start_pruning() accepts a period argument.","title":"Structural plasticity"},{"location":"manual/StructuralPlasticity.html#structural-plasticity","text":"ANNarchy supports the dynamic addition/suppression of synapses during the simulation (i.e. after compilation). Warning Structural plasticity is not available with the CUDA backend and will likely never be... Because structural plasticity adds some complexity to the generated code, it has to be enabled before compilation by setting the structural_plasticity flag to True in the call to setup() : setup ( structural_plasticity = True ) If the flag is not set, the following methods will do nothing. There are two possibilities to dynamically create or delete synapses: Externally, using methods at the dendrite level from Python. Internally, by defining conditions for creating/pruning in the synapse description.","title":"Structural plasticity"},{"location":"manual/StructuralPlasticity.html#dendrite-level","text":"Two methods of the Dendrite class are available for creating/deleting synapses: create_synapse() prune_synapse()","title":"Dendrite level"},{"location":"manual/StructuralPlasticity.html#creating-synapses","text":"Let's suppose that we want to add regularly new synapses between strongly active but not yet connected neurons with a low probability. One could for example define a neuron type with an additional variable averaging the firing rate over a long period of time. LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 tau_mean = 100000.0 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) tau_mean * dmean_r/dt = (r - mean_r) : init = 0.0 \"\"\" ) Two populations are created and connected using a sparse connectivity: pop1 = Population ( 1000 , LeakyIntegratorNeuron ) pop2 = Population ( 1000 , LeakyIntegratorNeuron ) proj = Projection ( pop1 , pop2 , 'exc' , Oja ) proj . connect_fixed_probability ( weights = 1.0 , probability = 0.1 ) After an initial period of simulation, one could add new synapses between strongly active pair of neurons: # For all post-synaptic neurons for post in xrange ( pop2 . size ): # For all pre-synaptic neurons for pre in xrange ( pop1 . size ): # If the neurons are not connected yet if not pre in proj [ post ] . ranks : # If they are both sufficientely active if pop1 [ pre ] . mean_r * pop2 [ post ] . mean_r > 0.7 : # Add a synapse with weight 1.0 and the default delay proj [ post ] . create_synapse ( pre , 1.0 ) create_synapse only allows to specify the value of the weight and the delay. Other syanptic variables will take the value they would have had before compile(). If another value is desired, it should be explicitely set afterwards.","title":"Creating synapses"},{"location":"manual/StructuralPlasticity.html#removing-synapses","text":"Removing useless synapses (pruning) is also possible. Let's consider a synapse type whose \\\"age\\\" is incremented as long as both pre- and post-synaptic neurons are inactive at the same time: AgingSynapse = Synapse ( equations = \"\"\" age = if pre.r * post.r > 0.0 : 0 else : age + 1 : init = 0, int \"\"\" ) One could periodically track the too \\\"old\\\" synapses and remove them: # Threshold on the age: T = 100000 # For all post-synaptic neurons receiving synapses for post in proj . post_ranks : # For all existing synapses for pre in proj [ post ] . ranks : # If the synapse is too old if proj [ post ][ pre ] . age > T : # Remove it proj [ post ] . prune_synapse ( pre ) Warning This form of structural plasticity is rather slow because: The for loops are in Python, not C++. Implementing this structural plasticity in Cython should already help. The memory allocated for the synapses of a projection may have to be displaced at another location. This can lead to massive transfer of data, slowing the simulation down. It is of course the user's responsability to balance synapse creation/destruction, otherwise projections could become either empty or fully connected on the long-term.","title":"Removing synapses"},{"location":"manual/StructuralPlasticity.html#synapse-level","text":"Conditions for creating or deleting synapses can also be specified in the synapse description, through the creating or pruning arguments. Thise arguments accept string descriptions of the boolean conditions at which a synapse should be created/deleted, using the same notation as other arguments.","title":"Synapse level"},{"location":"manual/StructuralPlasticity.html#creating-synapses_1","text":"The creation of a synapse must be described by a boolean expression: CreatingSynapse = Synapse ( parameters = \" ... \" , equations = \" ... \" , creating = \"pre.mean_r * post.mean_r > 0.7 : proba = 0.5, w = 1.0\" ) The condition can make use of any pre- or post-synaptic variable, but NOT synaptic variables, as they obviously do not exist yet. Global parameters (defined with the postsynaptic or projection flags) can nevertheless be used. Several flags can be passed to the expression: proba specifies the probability according to which a synapse will be created, if the condition is met. The default is 1.0 (i.e. a synapse will be created whenever the condition is fulfilled). w specifies the value for the weight which will be created (default: 0.0). d specifies the delay (default: the same as all other synapses if the delay is constant in the projection, dt otherwise). Warning Note that the new value for the delay can not exceed the maximal delay in the projection, nor be different from the others if they were all equal. Other synaptic variables will take the default value after creation. Synapse creation is not automatically enabled at the start of the simulation: the Projectiom method start_creating() must be called: proj . start_creating ( period = 100.0 ) This method accepts a period parameter specifying how often the conditions for creating synapses will be checked (in ms). By default they would be checked at each time step ( dt ), what would be too costly. Similarly, the stop_creating() method can be called to stop the creation conditions from being checked.","title":"Creating synapses"},{"location":"manual/StructuralPlasticity.html#deleting-synapses","text":"Synaptic pruning also rely on a boolean expression: PruningSynapse = Synapse ( parameters = \" T = 100000 : int, projection \" , equations = \"\"\" age = if pre.r * post.r > 0.0 : 0 else : age + 1 : init = 0, int\"\"\" , pruning = \"age > T : proba = 0.5\" ) A synapse type can combine creating and pruning arguments. The pruning argument can rely on synaptic variables (here age ), as the synapse already exist. Only the proba flag can be passed to specify the probability at which the synapse will be deleted if the condition is met. Pruning has to be started/stopped with the start_pruning() and stop_pruning() methods. start_pruning() accepts a period argument.","title":"Deleting synapses"},{"location":"manual/Structure.html","text":"General structure # Definition of a neural network # A neural network in ANNarchy is a collection of interconnected Populations . Each population comprises a set of similar artificial Neurons , whose mean-firing rate or spiking behavior is governed by one or many ordinary differential equations (ODE). These ODEs are dependent on the activity of other neurons through Synapses . The connection pattern between two populations is called a Projection . The efficiency of the connections received by a neuron is stored in a connectivity matrix, depending on the type that was assigned to them: excitatory, inhibitory, modulatory... This typed organization of afferent connections also allows to easily apply them different learning rules (Hebbian, three-factor, Oja, BCM, STDP...). To define a neural network and simulate its behavior, you need to define the following information: The number of populations, their geometry (number of neurons, optionally the spatial structure - 1D/2D/3D). For each population, the type of neuron composing it, with all the necessary ODEs. For each projection between two populations, the connection pattern (all-to-all, one-to-one, distance-dependent...), the initial synaptic weights, and optionally the delays in synaptic transmission. For plastic synapses, the ODEs describing the evolution of synaptic weights during the simulation (learning). The interaction of the network with its environment (I/O relationships, rewarded tasks, fitting procedure...) ANNarchy provides a convenient way to define this information in a single Python script. In this manual, we will focus on an simple rate-coded network composed of two interconnected populations pop1 and pop2 , but more complex architectures are of course possible (see the examples in section Examples ). Basic structure of a script # In a script file (e.g. MyNetwork.py ), you first need to import the ANNarchy package: from ANNarchy import * All the necessary objects and class definitions are then imported. The next step is to define the neurons and synapses needed by your network. To keep things simple, we will define a simple neuron model, whose firing rate is determined by the leaky-integration of excitatory inputs: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) \"\"\" ) mp is an internal variable integrating with the time constant tau the weighted sum of excitatory inputs sum(exc) to this neuron plus its baseline activity. r is the instantaneous firing rate of the neuron, defined as the positive part of mp . More details on the difference between parameters and variables, as well as details on the mathematical parser are to be found in the sections Parser and Rate-coded neurons . The synapse type between the two populations will implement a simple Oja learning rule, which is a Hebbian learning rule with an additional regularization term: Oja = Synapse ( parameters = \"\"\" tau = 5000.0 alpha = 8.0 \"\"\" , equations = \"\"\" tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w \"\"\" ) w represents the synaptic efficiency (or weight value). Its evolution over time depends on a time constant tau , the regularization parameter alpha , the pre-synaptic firing rate pre.r and the post-synaptic firing rate post.r . See Rate-coded synapses for more details. Once these objects are defined, the populations can be created (section Populations ). We create here two populations pop1 and pop2 containing 100 neurons each and using the LeakyIntegratorNeuron neural model: pop1 = Population ( name = 'pop1' , geometry = 100 , neuron = LeakyIntegratorNeuron ) pop2 = Population ( name = 'pop2' , geometry = 100 , neuron = LeakyIntegratorNeuron ) We additionally define an excitatory projection between the neurons of pop1 and pop2 , with a target exc and a all_to_all connection pattern (section Projections ). The synaptic weights are initialized randomly between 0.0 and 1.0: proj = Projection ( pre = pop1 , post = pop2 , target = 'exc' , synapse = Oja ) proj . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) Now that the structure of the network is defined, it can be analyzed to generate optimized C++ code in the annarchy/ subfolder and create the objects: compile () The network is now ready to be simulated for the desired amount of time: simulate ( 1000.0 ) # simulate for 1 second It remains to set inputs, record variables and analyze the results, but the structure of the network is already there.","title":"General structure"},{"location":"manual/Structure.html#general-structure","text":"","title":"General structure"},{"location":"manual/Structure.html#definition-of-a-neural-network","text":"A neural network in ANNarchy is a collection of interconnected Populations . Each population comprises a set of similar artificial Neurons , whose mean-firing rate or spiking behavior is governed by one or many ordinary differential equations (ODE). These ODEs are dependent on the activity of other neurons through Synapses . The connection pattern between two populations is called a Projection . The efficiency of the connections received by a neuron is stored in a connectivity matrix, depending on the type that was assigned to them: excitatory, inhibitory, modulatory... This typed organization of afferent connections also allows to easily apply them different learning rules (Hebbian, three-factor, Oja, BCM, STDP...). To define a neural network and simulate its behavior, you need to define the following information: The number of populations, their geometry (number of neurons, optionally the spatial structure - 1D/2D/3D). For each population, the type of neuron composing it, with all the necessary ODEs. For each projection between two populations, the connection pattern (all-to-all, one-to-one, distance-dependent...), the initial synaptic weights, and optionally the delays in synaptic transmission. For plastic synapses, the ODEs describing the evolution of synaptic weights during the simulation (learning). The interaction of the network with its environment (I/O relationships, rewarded tasks, fitting procedure...) ANNarchy provides a convenient way to define this information in a single Python script. In this manual, we will focus on an simple rate-coded network composed of two interconnected populations pop1 and pop2 , but more complex architectures are of course possible (see the examples in section Examples ).","title":"Definition of a neural network"},{"location":"manual/Structure.html#basic-structure-of-a-script","text":"In a script file (e.g. MyNetwork.py ), you first need to import the ANNarchy package: from ANNarchy import * All the necessary objects and class definitions are then imported. The next step is to define the neurons and synapses needed by your network. To keep things simple, we will define a simple neuron model, whose firing rate is determined by the leaky-integration of excitatory inputs: LeakyIntegratorNeuron = Neuron ( parameters = \"\"\" tau = 10.0 baseline = -0.2 \"\"\" , equations = \"\"\" tau * dmp/dt + mp = baseline + sum(exc) r = pos(mp) \"\"\" ) mp is an internal variable integrating with the time constant tau the weighted sum of excitatory inputs sum(exc) to this neuron plus its baseline activity. r is the instantaneous firing rate of the neuron, defined as the positive part of mp . More details on the difference between parameters and variables, as well as details on the mathematical parser are to be found in the sections Parser and Rate-coded neurons . The synapse type between the two populations will implement a simple Oja learning rule, which is a Hebbian learning rule with an additional regularization term: Oja = Synapse ( parameters = \"\"\" tau = 5000.0 alpha = 8.0 \"\"\" , equations = \"\"\" tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w \"\"\" ) w represents the synaptic efficiency (or weight value). Its evolution over time depends on a time constant tau , the regularization parameter alpha , the pre-synaptic firing rate pre.r and the post-synaptic firing rate post.r . See Rate-coded synapses for more details. Once these objects are defined, the populations can be created (section Populations ). We create here two populations pop1 and pop2 containing 100 neurons each and using the LeakyIntegratorNeuron neural model: pop1 = Population ( name = 'pop1' , geometry = 100 , neuron = LeakyIntegratorNeuron ) pop2 = Population ( name = 'pop2' , geometry = 100 , neuron = LeakyIntegratorNeuron ) We additionally define an excitatory projection between the neurons of pop1 and pop2 , with a target exc and a all_to_all connection pattern (section Projections ). The synaptic weights are initialized randomly between 0.0 and 1.0: proj = Projection ( pre = pop1 , post = pop2 , target = 'exc' , synapse = Oja ) proj . connect_all_to_all ( weights = Uniform ( 0.0 , 1.0 )) Now that the structure of the network is defined, it can be analyzed to generate optimized C++ code in the annarchy/ subfolder and create the objects: compile () The network is now ready to be simulated for the desired amount of time: simulate ( 1000.0 ) # simulate for 1 second It remains to set inputs, record variables and analyze the results, but the structure of the network is already there.","title":"Basic structure of a script"}]}