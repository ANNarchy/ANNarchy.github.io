{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Documentation for ANNarchy","text":"<p>ANNarchy (Artificial Neural Networks architect) is a neural simulator designed for distributed rate-coded or spiking neural networks. The core of the library is written in C++ and distributed using openMP or CUDA. It provides an interface in Python for the definition of the networks. It is released under the GNU GPL v2 or later.</p> <p>The source code of ANNarchy is available at:</p> <p>https://github.com/ANNarchy/ANNarchy</p> <p>The documentation is at:</p> <p>https://annarchy.github.io</p> <p>A forum for discussion is set at:</p> <p>https://groups.google.com/forum/#!forum/annarchy</p> <p>Bug reports should be done through the Issue Tracker of ANNarchy on Github.</p> <p>Citation</p> <p>If you use ANNarchy for your research, we would appreciate if you cite the following paper:</p> <p>Vitay J, Dinkelbach H\u00dc and Hamker FH (2015). ANNarchy: a code generation approach to neural simulations on parallel hardware. Frontiers in Neuroinformatics 9:19. doi:10.3389/fninf.2015.00019</p> <pre><code>@article{Vitay2015,\n  title = {{{ANNarchy}}: A Code Generation Approach to Neural Simulations on Parallel Hardware},\n  author = {Vitay, Julien and Dinkelbach, Helge {\\\"U}. and Hamker, Fred H.},\n  year = {2015},\n  journal = {Frontiers in Neuroinformatics},\n  volume = {9},\n  number = {19},\n  doi = {10.3389/fninf.2015.00019},\n  url = {https://www.frontiersin.org/articles/10.3389/fninf.2015.00019},\n  abstract = {Many modern neural simulators focus on the simulation of networks of spiking neurons on parallel hardware. Another important framework in computational neuroscience, rate-coded neural networks, is mostly difficult or impossible to implement using these simulators. We present here the ANNarchy (Artificial Neural Networks architect) neural simulator, which allows to easily define and simulate rate-coded and spiking networks, as well as combinations of both. The interface in Python has been designed to be close to the PyNN interface, while the definition of neuron and synapse models can be specified using an equation-oriented mathematical description similar to the Brian neural simulator. This information is used to generate C++ code that will efficiently perform the simulation on the chosen parallel hardware (multi-core system or graphical processing unit). Several numerical methods are available to transform ordinary differential equations into an efficient C++code. We compare the parallel performance of the simulator to existing solutions.}\n}\n</code></pre>"},{"location":"Installation.html","title":"Installation of ANNarchy","text":"<p>ANNarchy is designed to run on GNU/Linux and OSX. It relies mostly on a C++ compiler (g++ or clang++), Cython (C for Python extension) and Python (Numpy, Sympy) libraries. Installation on Windows is not possible.</p>"},{"location":"Installation.html#download","title":"Download","text":"<p>The source code of ANNarchy can be downloaded on github:</p> <pre><code>git clone https://github.com/ANNarchy/ANNarchy.git\n</code></pre>"},{"location":"Installation.html#installation-on-gnulinux","title":"Installation on GNU/Linux","text":""},{"location":"Installation.html#dependencies","title":"Dependencies","text":"<p>ANNarchy depends on a number of packages that should be easily accessible on recent GNU/Linux distributions. The classical way to install these dependencies is through your package manager, or using full Python distributions such as Anaconda. Older versions of these packages may work but have not been tested.</p> <ul> <li>g++ &gt;= 6.1 ( &gt;= 7.4 recommended )</li> <li>make &gt;= 3.0</li> <li>python &gt;= 3.7 (with the development files, e.g. <code>python-dev</code> or <code>python-devel</code>)</li> <li>cython &gt;= 0.20</li> <li>setuptools &gt;= 40.0</li> <li>numpy &gt;= 1.13</li> <li>sympy &gt;= 1.6</li> <li>scipy &gt;= 0.19</li> <li>matplotlib &gt;= 2.0</li> </ul> <p>Additionally, the following packages are optional but strongly recommended:</p> <ul> <li>pyqtgraph &gt;= 0.9.8 (to visualize some of the provided examples. The OpenGL backend can also be needed).</li> <li>lxml &gt;= 3.0 (to save the networks in .xml format).</li> <li>pandoc &gt;= 2.0 (for reporting).</li> <li>tensorboardX (for the logging extension).</li> </ul> <p>To use the CUDA backend:</p> <ul> <li>the CUDA-SDK is available on the official website (we recommend to use at least a SDK version &gt; 6.x). For further details on installation etc., please consider the corresponding Quickstart guides (Quickstart_8.0 for the SDK 8.x).</li> </ul> <p>ANNarchy works with full Python distributions such as Anaconda, as well as in virtual environments and Jupyter notebooks.</p> <p>Note</p> <p>On a fresh install of Ubuntu 22.04, here are the minimal system packages to install before ANNarchy:</p> <pre><code>sudo apt install build-essential git python3-dev python3-setuptools python3-pip </code></pre> <p>The minimal Python packages can be installed with:</p> <pre><code>pip install numpy scipy matplotlib cython sympy\n</code></pre>"},{"location":"Installation.html#installation","title":"Installation","text":""},{"location":"Installation.html#using-pip","title":"Using pip","text":"<p>Stable releases of ANNarchy are available on PyPi:</p> <pre><code>pip install ANNarchy\n</code></pre> <p>or:</p> <pre><code>pip install ANNarchy --user\n</code></pre> <p>if you do not have administrator permissions. Omit <code>--user</code> in a virtual environment.</p> <p>You may also install directly the latest commit in the <code>master</code> (stable) or <code>develop</code> branches with:</p> <pre><code>pip install git+https://github.com/ANNarchy/ANNarchy.git@master\n</code></pre>"},{"location":"Installation.html#using-the-source-code","title":"Using the source code","text":"<p>Installation of ANNarchy from source is possible using <code>pip</code> in the top-level directory:</p> <pre><code>pip install .\n</code></pre> <p>or in development mode:</p> <pre><code>pip install -e .\n</code></pre> <p>Using <code>python setup.py install</code> is deprecated, but still works.</p>"},{"location":"Installation.html#c-compiler","title":"C++ compiler","text":"<p>By default, ANNarchy will use the GNU C++ compiler <code>g++</code>, which should be in your PATH. If you want to use another compiler (clang++, icc), you can edit the configuration file located at <code>$HOME/.config/ANNarchy/annarchy.json</code> (created during installation) accordingly. By default, it is:</p> <pre><code>{\n\"openmp\": {\n\"compiler\": \"g++\",\n\"flags\": \"-march=native -O2\"\n},\n\"cuda\": {\n\"compiler\": \"nvcc\",\n\"flags\": \"\",\n\"device\": 0,\n\"path\": \"/usr/local/cuda\"\n}\n}\n</code></pre> <p>The (path to the) compiler can be changed in the <code>openmp</code> section (ignore the <code>cuda</code> section if you do not have a GPU).</p> <p>You can also change the compiler flags if you know what you are doing. <code>-O3</code> does not always lead to faster simulation times, but it is worth a shot.</p>"},{"location":"Installation.html#cuda","title":"CUDA","text":"<p>If ANNarchy detects the CUDA SDK during installation, it will prepare the required modules. You need to make sure that the CUDA compiler <code>nvcc</code> is accessible in your path.</p> <p>The main problem with CUDA is that the binaries, headers and libraries are installed at different locations depending on the version: <code>/usr/local/cuda</code>, <code>/usr/local/cuda-7.0</code> or <code>/usr/local/cuda-8.0</code>. There is unfortunately no way for ANNarchy to guess the installation path.</p> <p>A first thing to help ANNarchy find the CUDA libraries is to define the <code>LD_LIBRARY_PATH</code> environment variable and have point at the <code>lib64/</code> subfolder:</p> <pre><code>export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64/:$LD_LIBRARY_PATH\n</code></pre> <p>This should in most cases work if you have only one CUDA installation. Otherwise, it is needed that you indicate where the CUDA libraries are, by modifying the ANNarchy configuration file located at <code>$HOME/.config/ANNarchy/annarchy.json</code>:</p> <pre><code>{\n\"openmp\": {\n\"compiler\": \"g++\",\n\"flags\": \"-march=native -O2\"\n},\n\"cuda\": {\n\"compiler\": \"nvcc\",\n\"flags\": \"\",\n\"device\": 0,\n\"path\": \"/usr/local/cuda\"\n}\n}\n</code></pre> <p>Simply point the <code>['cuda']['path']</code> field to the right location (without <code>lib64/</code>). If the nvcc compiler binary is at a different location, the absolute path to the nvcc can be provided by <code>['cuda']['compiler']</code> field.</p> <p>It can happen that the detection of CUDA fails during installation, as some environment variables are not set. In this case try:</p> <pre><code>env \"PATH=$PATH\" \"LIBRARY_PATH=$LIBRARY_PATH\" pip install .\n</code></pre>"},{"location":"Installation.html#installation-on-macos-x","title":"Installation on MacOS X","text":"<p>Installation on MacOS X is in principle similar to GNU/Linux:</p> <pre><code>pip install ANNarchy\n</code></pre> <p>We advise using a full Python distribution such as Miniforge, which allows to install all dependencies of ANNarchy, rather than using the default python provided by Apple.</p> <p>The main issue if the choice of the C++ compiler:</p>"},{"location":"Installation.html#using-apples-llvmclang","title":"Using Apple's LLVM/clang","text":"<p>If not done already, you should first install the Xcode Command Line Tools, either through Apple's website or through Homebrew (see https://mac.install.guide/commandlinetools/ for a guide) to get the LLVM clang++ compiler.</p> <p>The major drawback is that Apple's clang++ still does not support OpenMP for parallel computing. Any attempt to use OpenMP with ANNarchy using this compiler will crash.</p> <p>If you have a M1 arm64 processor, it might be beneficial to tell clang++ to use optimizations for that hardware. Open the configuration file at <code>$HOME/.config/ANNarchy/annarchy.json</code> and add the following compiler flag (Xcode &gt; 13.0):</p> <pre><code>{\n\"openmp\": {\n\"compiler\": \"clang++\",\n\"flags\": \"-mcpu=apple-m1 -O2\"\n},\n\"cuda\": {\n\"compiler\": \"nvcc\",\n\"flags\": \"\",\n\"device\": 0,\n\"path\": \"/usr/local/cuda\"\n}\n}\n</code></pre>"},{"location":"Installation.html#using-gcc","title":"Using gcc","text":"<p>In order to benefit from OpenMP parallelization, you should install <code>gcc</code>, the GNU C compiler, using Homebrew:</p> <pre><code>brew install gcc\n</code></pre> <p>You will get the command-line C++ compiler with a version number, e.g.:</p> <p><pre><code>g++-11\n</code></pre> The <code>g++</code> executable is a symlink to Apple's clang++, do not use it...</p> <p>You now have to tell ANNarchy which compiler to use, even if it is in your PATH. After installing ANNarchy, a config file is created in <code>$HOME/.config/ANNarchy/annarchy.json</code>. Open it and change the <code>openmp</code> entry to:</p> <pre><code>{\n\"openmp\": {\n\"compiler\": \"g++-11\",\n\"flags\": \"-march=native -O2\"\n},\n\"cuda\": {\n\"compiler\": \"nvcc\",\n\"flags\": \"\",\n\"device\": 0,\n\"path\": \"/usr/local/cuda\"\n}\n}\n</code></pre> <p>Note</p> <p>A potential problem with Anaconda/miniforge is that the compiler will use by default the Python shared library provided by Apple, leading to the following crash when simulating:</p> <pre><code>Fatal Python error: PyThreadState_Get: no current thread\nAbort trap: 6\n</code></pre> <p>The solution is to set the environment variable <code>DYLD_FALLBACK_LIBRARY_PATH</code> to point at the correct library <code>libpython3.6.dylib</code> in your <code>.bash_profile</code>. For a standard Anaconda installation, this should be:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=$HOME/anaconda/lib:$DYLD_FALLBACK_LIBRARY_PATH\n</code></pre> <p>Note</p> <p>The CUDA backend is not available on OS X.</p>"},{"location":"License.html","title":"License","text":""},{"location":"License.html#gnu-general-public-license","title":"GNU GENERAL PUBLIC LICENSE","text":"<p>Version 2, June 1991</p> <pre><code>Copyright (C) 1989, 1991 Free Software Foundation, Inc.  \n51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA\n\nEveryone is permitted to copy and distribute verbatim copies\nof this license document, but changing it is not allowed.\n</code></pre>"},{"location":"License.html#preamble","title":"Preamble","text":"<p>The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Lesser General Public License instead.) You can apply it to your programs, too.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things.</p> <p>To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it.</p> <p>For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.</p> <p>We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software.</p> <p>Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations.</p> <p>Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"License.html#terms-and-conditions-for-copying-distribution-and-modification","title":"TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION","text":"<p>0. This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The \"Program\", below, refers to any such program or work, and a \"work based on the Program\" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term \"modification\".) Each licensee is addressed as \"you\".</p> <p>Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does.</p> <p>1. You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program.</p> <p>You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee.</p> <p>2. You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions:</p> <p>a) You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change.</p> <p>b) You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License.</p> <p>c) If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.)</p> <p>These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it.</p> <p>Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program.</p> <p>In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License.</p> <p>3. You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following:</p> <p>a) Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or,</p> <p>b) Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or,</p> <p>c) Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.)</p> <p>The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable.</p> <p>If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code.</p> <p>4. You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance.</p> <p>5. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it.</p> <p>6. Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License.</p> <p>7. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program.</p> <p>If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances.</p> <p>It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice.</p> <p>This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License.</p> <p>8. If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License.</p> <p>9. The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation.</p> <p>10. If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally.</p> <p>NO WARRANTY</p> <p>11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <p>12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"API/ANNarchy.html","title":"Top-level methods","text":"<p>These methods are directly available in the main namespace when importing ANNarchy:</p> <pre><code>from ANNarchy import *\n</code></pre> <p>Note that numpy is automatically imported as:</p> <pre><code>import numpy as np\n</code></pre>"},{"location":"API/ANNarchy.html#configuration-and-compilation","title":"Configuration and compilation","text":"<p>Contrary to other simulators, ANNarchy is entirely based on code generation. It provides a set of first-level functions to ensure the network is correctly created. It is important to call these functions in the right order.</p>"},{"location":"API/ANNarchy.html#ANNarchy.setup","title":"<code>ANNarchy.setup(**keyValueArgs)</code>","text":"<p>The setup function is used to configure ANNarchy simulation environment. It takes various optional arguments:</p> <ul> <li>dt: simulation step size (default: 1.0 ms).</li> <li>paradigm: parallel framework for code generation. Accepted values: \"openmp\" or \"cuda\" (default: \"openmp\").</li> <li>method: default method to numerize ODEs. Default is the explicit forward Euler method ('explicit').</li> <li>sparse_matrix_format: the default matrix format for projections in ANNarchy (by default: List-In-List for CPUs and Compressed Sparse Row)</li> <li>precision: default floating precision for variables in ANNarchy. Accepted values: \"float\" or \"double\" (default: \"double\")</li> <li>num_threads: number of treads used by openMP (overrides the environment variable <code>OMP_NUM_THREADS</code> when set, default = None).</li> <li>visible_cores: allows a fine-grained control which cores are useable for the created threads (default = [] for no limitation).                  It can be used to limit created openMP threads to a physical socket.</li> <li>structural_plasticity: allows synapses to be dynamically added/removed during the simulation (default: False).</li> <li>seed: the seed (integer) to be used in the random number generators (default = -1 is equivalent to time(NULL)).</li> </ul> <p>The following parameters are mainly for debugging and profiling, and should be ignored by most users:</p> <ul> <li>verbose: shows details about compilation process on console (by default False). Additional some information of the network construction will be shown.</li> <li>suppress_warnings: if True, warnings (e. g. from the mathematical parser) are suppressed.</li> <li>show_time: if True, initialization times are shown. Attention: verbose should be set to True additionally.</li> <li>disable_shared_library_time_offset: by default False. If set to True, the shared library generated by ANNarchy will not be extended by time offset.</li> </ul> <p>Note:</p> <p>This function should be used before any other functions of ANNarchy (including importing a network definition), right after <code>from ANNarchy import *</code>:</p> <pre><code>from ANNarchy import *\nsetup(dt=1.0, method='midpoint', num_threads=2)\n</code></pre>"},{"location":"API/ANNarchy.html#ANNarchy.compile","title":"<code>ANNarchy.compile(directory='annarchy', clean=False, populations=None, projections=None, compiler='default', compiler_flags='default', add_sources='', extra_libs='', cuda_config={'device': 0}, annarchy_json='', silent=False, debug_build=False, profile_enabled=False, net_id=0)</code>","text":"<p>This method uses the network architecture to generate optimized C++ code and compile a shared library that will perform the simulation.</p> <p>The <code>compiler</code>, <code>compiler_flags</code> and part of <code>cuda_config</code> take their default value from the configuration file <code>~/.config/ANNarchy/annarchy.json</code>.</p> <p>The following arguments are for internal development use only:</p> <ul> <li>debug_build: creates a debug version of ANNarchy, which logs the creation of objects and some other data (default: False).</li> <li>profile_enabled: creates a profilable version of ANNarchy, which logs several computation timings (default: False).</li> </ul> <p>Parameters:</p> <ul> <li> directory         \u2013          <p>name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\".</p> </li> <li> clean         \u2013          <p>boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False).</p> </li> <li> populations         \u2013          <p>list of populations which should be compiled. If set to None, all available populations will be used.</p> </li> <li> projections         \u2013          <p>list of projection which should be compiled. If set to None, all available projections will be used.</p> </li> <li> compiler         \u2013          <p>C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++].</p> </li> <li> compiler_flags         \u2013          <p>platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended.</p> </li> <li> cuda_config         \u2013          <p>dictionary defining the CUDA configuration for each population and projection.</p> </li> <li> annarchy_json         \u2013          <p>compiler flags etc can be stored in a .json file normally placed in the home directory (see comment below). With this flag one can directly assign a file location.</p> </li> <li> silent         \u2013          <p>defines if status message like \"Compiling... OK\" should be printed.</p> </li> </ul>"},{"location":"API/ANNarchy.html#ANNarchy.clear","title":"<code>ANNarchy.clear()</code>","text":"<p>Clears all variables (erasing already defined populations, projections, monitors and constants), as if you had just imported ANNarchy.</p> <p>Useful when re-running Jupyter/IPython notebooks multiple times:</p> <pre><code>from ANNarchy import *\nclear()\n</code></pre>"},{"location":"API/ANNarchy.html#simulation","title":"Simulation","text":"<p>Different methods are available to run the simulation:</p>"},{"location":"API/ANNarchy.html#ANNarchy.simulate","title":"<code>ANNarchy.simulate(duration, measure_time=False, progress_bar=False, callbacks=True, net_id=0)</code>","text":"<p>Simulates the network for the given duration in milliseconds. </p> <p>The number of simulation steps is computed relative to the discretization step <code>dt</code> declared in <code>setup()</code> (default: 1ms):</p> <pre><code>simulate(1000.0)\n</code></pre> <p>Parameters:</p> <ul> <li> duration         \u2013          <p>the duration in milliseconds.</p> </li> <li> measure_time         \u2013          <p>defines whether the simulation time should be printed. Default: False.</p> </li> <li> progress_bar         \u2013          <p>defines whether a progress bar should be printed. Default: False</p> </li> <li> callbacks         \u2013          <p>defines if the callback method (decorator <code>every</code> should be called). Default: True.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Simulate.py</code> <pre><code>def simulate(duration, measure_time=False, progress_bar=False, callbacks=True, net_id=0):\n\"\"\"\n    Simulates the network for the given duration in milliseconds. \n\n    The number of simulation steps is computed relative to the discretization step ``dt`` declared in ``setup()`` (default: 1ms):\n\n    ```python\n    simulate(1000.0)\n    ```\n\n    :param duration: the duration in milliseconds.\n    :param measure_time: defines whether the simulation time should be printed. Default: False.\n    :param progress_bar: defines whether a progress bar should be printed. Default: False\n    :param callbacks: defines if the callback method (decorator ``every`` should be called). Default: True.\n    \"\"\"\n    if Global._profiler:\n        t0 = time.time()\n\n    if not _network[net_id]['instance']:\n        _error('simulate(): the network is not compiled yet.')\n\n    # Compute the number of steps\n    nb_steps = ceil(float(duration) / dt())\n\n    if measure_time:\n        tstart = time.time()\n\n    if callbacks and _callbacks_enabled[net_id] and len(_callbacks[net_id]) &gt; 0:\n        _simulate_with_callbacks(duration, net_id)\n    else:\n        _network[net_id]['instance'].pyx_run(nb_steps, progress_bar)\n\n    if measure_time:\n        if net_id &gt; 0:\n            _print('Simulating', duration/1000.0, 'seconds of the network', net_id, 'took', time.time() - tstart, 'seconds.')\n        else:\n            _print('Simulating', duration/1000.0, 'seconds of the network took', time.time() - tstart, 'seconds.')\n\n    # Store the Python and C++ timings. Please note, that the C++ core\n    # measures in ms and Python measures in s\n    if Global._profiler:\n        t1 = time.time()\n        Global._profiler.add_entry( t0, t1, \"simulate\", \"simulate\")\n\n        # network single step\n        overall_avg, _ = Global._profiler._cpp_profiler.get_timing(\"network\", \"step\")\n        Global._profiler.add_entry(overall_avg/1000.0, 100.0, \"overall\", \"cpp core\")\n\n        # single operations for populations\n        for pop in _network[net_id]['populations']:\n            for func in [\"step\", \"rng\", \"delay\", \"spike\"]:\n                avg_time, std_time = Global._profiler._cpp_profiler.get_timing(pop.name, func)\n                Global._profiler.add_entry( avg_time/1000.0, (avg_time/overall_avg)*100.0, pop.name+\"_\"+func, \"cpp core\")\n\n        # single operations for projections\n        for proj in _network[net_id]['projections']:\n            for func in [\"psp\", \"step\", \"post_event\"]:\n                avg_time, std_time = Global._profiler._cpp_profiler.get_timing(proj.name, func)\n                Global._profiler.add_entry( avg_time/1000.0, (avg_time/overall_avg)*100.0, proj.name+\"_\"+func, \"cpp core\")\n\n        monitor_avg, _ = Global._profiler._cpp_profiler.get_timing(\"network\", \"record\")\n        Global._profiler.add_entry( monitor_avg/1000.0, (monitor_avg/overall_avg)*100.0, \"record\", \"cpp core\")\n</code></pre>"},{"location":"API/ANNarchy.html#ANNarchy.simulate_until","title":"<code>ANNarchy.simulate_until(max_duration, population, operator='and', measure_time=False, net_id=0)</code>","text":"<p>Runs the network for the maximal duration in milliseconds. If the <code>stop_condition</code> defined in the population becomes true during the simulation, it is stopped.</p> <p>One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function).</p> <p>Example:</p> <pre><code>pop1 = Population( ..., stop_condition = \"r &gt; 1.0 : any\")\ncompile()\nsimulate_until(max_duration=1000.0. population=pop1)\n</code></pre> <p>Parameters:</p> <ul> <li> max_duration         \u2013          <p>the maximum duration of the simulation in milliseconds.</p> </li> <li> population         \u2013          <p>the (list of) population whose <code>stop_condition</code> should be checked to stop the simulation.</p> </li> <li> operator         \u2013          <p>operator to be used ('and' or 'or') when multiple populations are provided (default: 'and').</p> </li> <li> measure_time         \u2013          <p>defines whether the simulation time should be printed (default=False).</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>the actual duration of the simulation in milliseconds.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Simulate.py</code> <pre><code>def simulate_until(max_duration, population, operator='and', measure_time = False, net_id=0):\n\"\"\"\n    Runs the network for the maximal duration in milliseconds. If the ``stop_condition`` defined in the population becomes true during the simulation, it is stopped.\n\n    One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function).\n\n    Example:\n\n    ```python\n    pop1 = Population( ..., stop_condition = \"r &gt; 1.0 : any\")\n    compile()\n    simulate_until(max_duration=1000.0. population=pop1)\n    ```\n\n    :param max_duration: the maximum duration of the simulation in milliseconds.\n    :param population: the (list of) population whose ``stop_condition`` should be checked to stop the simulation.\n    :param operator: operator to be used ('and' or 'or') when multiple populations are provided (default: 'and').\n    :param measure_time: defines whether the simulation time should be printed (default=False).\n    :return: the actual duration of the simulation in milliseconds.\n    \"\"\"\n    if not _network[net_id]['instance']:\n        _error('simulate_until(): the network is not compiled yet.')\n\n\n    nb_steps = ceil(float(max_duration) / dt())\n    if not isinstance(population, list):\n        population = [population]\n\n\n    if measure_time:\n        tstart = time.time()\n\n    nb = _network[net_id]['instance'].pyx_run_until(nb_steps, [pop.id for pop in population], True if operator=='and' else False)\n\n    sim_time = float(nb) / dt()\n    if measure_time:\n        _print('Simulating', nb/dt()/1000.0, 'seconds of the network took', time.time() - tstart, 'seconds.')\n    return sim_time\n</code></pre>"},{"location":"API/ANNarchy.html#ANNarchy.step","title":"<code>ANNarchy.step(net_id=0)</code>","text":"<p>Performs a single simulation step (duration = <code>dt</code>).</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Simulate.py</code> <pre><code>def step(net_id=0):\n\"\"\"\n    Performs a single simulation step (duration = ``dt``).\n    \"\"\"\n    if not _network[net_id]['instance']:\n        _error('simulate_until(): the network is not compiled yet.')\n\n\n    _network[net_id]['instance'].pyx_step()\n</code></pre>"},{"location":"API/ANNarchy.html#ANNarchy.every","title":"<code>ANNarchy.every</code>","text":"<p>             Bases: <code>object</code></p> <p>Decorator to declare a callback method that will be called periodically during the simulation.</p> <p>Example of setting increasing inputs to a population every 100 ms, with an offset of 90 ms (or -10 ms relative to the period):</p> <pre><code>@every(period=100., offset=-10.)\ndef step_input(n):\n    pop.I = float(n) / 100.\n\nsimulate(10000.)\n</code></pre> <p><code>step_input()</code> will be called at times 90, 190, ..., 9990 ms during the call to <code>simulate()</code>.</p> <p>The method must accept only <code>n</code> as parameter (an integer being 0 the first time the method is called, and incremented afterwards) and can not return anything.</p> <p>The times at which the method is called are relative to the time when <code>simulate()</code> is called (if <code>t</code> is already 150 before calling <code>simulate()</code>, the first call will then be made at <code>t=240</code> with the previous example).</p> <p>If multiple callbacks are defined, they will be called in the order of their declaration if they occur at the same time.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Simulate.py</code> <pre><code>class every(object):\n\"\"\"\n    Decorator to declare a callback method that will be called periodically during the simulation.\n\n    Example of setting increasing inputs to a population every 100 ms, with an offset of 90 ms (or -10 ms relative to the period):\n\n    ```python\n    @every(period=100., offset=-10.)\n    def step_input(n):\n        pop.I = float(n) / 100.\n\n    simulate(10000.)\n    ```\n\n    ``step_input()`` will be called at times 90, 190, ..., 9990 ms during the call to ``simulate()``.\n\n    The method must accept only ``n`` as parameter (an integer being 0 the first time the method is called, and incremented afterwards) and can not return anything.\n\n    The times at which the method is called are relative to the time when ``simulate()`` is called (if ``t`` is already 150 before calling ``simulate()``, the first call will then be made at ``t=240`` with the previous example).\n\n    If multiple callbacks are defined, they will be called in the order of their declaration if they occur at the same time.\n\n    \"\"\"\n\n    def __init__(self, period, offset=0., wait=0.0, net_id=0):\n\"\"\"\n        :param period: interval in ms between two calls to the function. If less than ``dt``, will be called every step.\n        :param offset: by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period.\n        :param wait: allows to wait for a certain amount of time (in ms) before starting to call the method.\n\n        ``wait`` can be combined with ``offset``, so if ``period=100.``, ``offset=50.`` and ``wait=500.``, the first call will be made 550 ms after the call to ``simulate()``\n\n        \"\"\"\n        self.period = max(float(period), dt())\n        self.offset = min(float(offset), self.period)\n        self.wait = max(float(wait), 0.0)\n        _callbacks[net_id].append(self)\n\n    def __call__(self, f):\n\n        # If there are decorator arguments, __call__() is only called\n        # once, as part of the decoration process! You can only give\n        # it a single argument, which is the function object.\n\n        self.func = f\n        return f\n</code></pre>"},{"location":"API/ANNarchy.html#ANNarchy.core.Simulate.every.__init__","title":"<code>__init__(period, offset=0.0, wait=0.0, net_id=0)</code>","text":"<p>Parameters:</p> <ul> <li> period         \u2013          <p>interval in ms between two calls to the function. If less than <code>dt</code>, will be called every step.</p> </li> <li> offset         \u2013          <p>by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period.</p> </li> <li> wait         \u2013          <p>allows to wait for a certain amount of time (in ms) before starting to call the method.  <code>wait</code> can be combined with <code>offset</code>, so if <code>period=100.</code>, <code>offset=50.</code> and <code>wait=500.</code>, the first call will be made 550 ms after the call to <code>simulate()</code></p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Simulate.py</code> <pre><code>def __init__(self, period, offset=0., wait=0.0, net_id=0):\n\"\"\"\n    :param period: interval in ms between two calls to the function. If less than ``dt``, will be called every step.\n    :param offset: by default, the first call to the method will be made at the start of the simulation. The offset delays the call within the period (default: 0.0). Can be negative, in which case it will be counted from the end of the period.\n    :param wait: allows to wait for a certain amount of time (in ms) before starting to call the method.\n\n    ``wait`` can be combined with ``offset``, so if ``period=100.``, ``offset=50.`` and ``wait=500.``, the first call will be made 550 ms after the call to ``simulate()``\n\n    \"\"\"\n    self.period = max(float(period), dt())\n    self.offset = min(float(offset), self.period)\n    self.wait = max(float(wait), 0.0)\n    _callbacks[net_id].append(self)\n</code></pre>"},{"location":"API/ANNarchy.html#ANNarchy.enable_callbacks","title":"<code>ANNarchy.enable_callbacks(net_id=0)</code>","text":"<p>Enables all declared callbacks for the network.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Simulate.py</code> <pre><code>def enable_callbacks(net_id=0):\n\"\"\"\n    Enables all declared callbacks for the network.\n    \"\"\"\n    _callbacks_enabled[net_id] = True\n</code></pre>"},{"location":"API/ANNarchy.html#ANNarchy.disable_callbacks","title":"<code>ANNarchy.disable_callbacks(net_id=0)</code>","text":"<p>Disables all callbacks for the network.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Simulate.py</code> <pre><code>def disable_callbacks(net_id=0):\n\"\"\"\n    Disables all callbacks for the network.\n    \"\"\"\n    _callbacks_enabled[net_id] = False\n</code></pre>"},{"location":"API/ANNarchy.html#ANNarchy.clear_all_callbacks","title":"<code>ANNarchy.clear_all_callbacks(net_id=0)</code>","text":"<p>Clears the list of declared callbacks for the network.</p> <p>Cannot be undone!</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Simulate.py</code> <pre><code>def clear_all_callbacks(net_id=0):\n\"\"\"\n    Clears the list of declared callbacks for the network.\n\n    Cannot be undone!\n    \"\"\"\n    _callbacks[net_id].clear()\n</code></pre>"},{"location":"API/ANNarchy.html#reset-the-network","title":"Reset the network","text":"<p>If you want to run multiple experiments with the same network, or if your experiment setup requires a pre-learning phase, you can reset selectively neural or synaptic variables to their initial values.</p>"},{"location":"API/ANNarchy.html#ANNarchy.reset","title":"<code>ANNarchy.reset(populations=True, projections=False, synapses=False, monitors=True, net_id=0)</code>","text":"<p>Reinitialises the network to its state before the call to compile. The network time will be set to 0ms.</p> <p>All monitors are emptied.</p> <p>Parameters:</p> <ul> <li> populations         \u2013          <p>if True (default), the neural parameters and variables will be reset to their initial value.</p> </li> <li> projections         \u2013          <p>if True, the synaptic parameters and variables (except the connections) will be reset (default=False).</p> </li> <li> synapses         \u2013          <p>if True, the synaptic weights will be erased and recreated (default=False).</p> </li> <li> monitors         \u2013          <p>if True, the monitors will be emptied and reset (default=True).</p> </li> </ul>"},{"location":"API/ANNarchy.html#access-to-populations","title":"Access to populations","text":""},{"location":"API/ANNarchy.html#ANNarchy.get_population","title":"<code>ANNarchy.get_population(name, net_id=0)</code>","text":"<p>Returns the population with the given <code>name</code>.</p> <p>Parameters:</p> <ul> <li> name         \u2013          <p>name of the population.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>The requested <code>Population</code> object if existing, <code>None</code> otherwise.</p> </li> </ul>"},{"location":"API/ANNarchy.html#ANNarchy.get_projection","title":"<code>ANNarchy.get_projection(name, net_id=0)</code>","text":"<p>Returns the projection with the given name.</p> <p>Parameters:</p> <ul> <li> name         \u2013          <p>name of the projection.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>The requested <code>Projection</code> object if existing, <code>None</code> otherwise.</p> </li> </ul>"},{"location":"API/ANNarchy.html#functions","title":"Functions","text":""},{"location":"API/ANNarchy.html#ANNarchy.add_function","title":"<code>ANNarchy.add_function(function)</code>","text":"<p>Defines a global function which can be used by all neurons and synapses.</p> <p>The function must have only one return value and use only the passed arguments.</p> <p>Examples of valid functions:</p> <pre><code>logistic(x) = 1 / (1 + exp(-x))\n\npiecewise(x, a, b) =    if x &lt; a:\n                            a\n                        else:\n                            if x &gt; b :\n                                b\n                            else:\n                                x\n</code></pre> <p>Please refer to the manual to know the allowed mathematical functions.</p>"},{"location":"API/ANNarchy.html#ANNarchy.functions","title":"<code>ANNarchy.functions(name, net_id=0)</code>","text":"<p>Allows to access a global function defined with <code>add_function</code> and use it from Python using arrays after compilation.</p> <p>The name of the function is not added to the global namespace to avoid overloading.</p> <pre><code>add_function(\"logistic(x) = 1. / (1. + exp(-x))\") \n\ncompile()  \n\nresult = functions('logistic')([0., 1., 2., 3., 4.])\n</code></pre> <p>Only lists or 1D Numpy arrays can be passed as arguments, not single values nor multidimensional arrays.</p> <p>When passing several arguments, make sure they have the same size.</p>"},{"location":"API/ANNarchy.html#constants","title":"Constants","text":""},{"location":"API/ANNarchy.html#ANNarchy.Constant","title":"<code>ANNarchy.Constant</code>","text":"<p>             Bases: <code>builtins.float</code></p> <p>Constant parameter that can be used by all neurons and synapses.</p> <p>The class <code>Constant</code> derives from <code>float</code>, so any legal operation on floats (addition, multiplication) can be used.</p> <p>If a Neuron/Synapse defines a parameter with the same name, the constant parameters will not be visible.</p> <p>Example:</p> <pre><code>tau = Constant('tau', 20)\nfactor = Constant('factor', 0.1)\nreal_tau = Constant('real_tau', tau*factor)\n\nneuron = Neuron(\n    equations='''\n        real_tau*dr/dt + r =1.0\n    '''\n)\n</code></pre> <p>The value of the constant can be changed anytime with the <code>set()</code> method. Assignments will have no effect (e.g. <code>tau = 10.0</code> only creates a new float).</p> <p>The value of constants defined as combination of other constants (<code>real_tau</code>) is not updated if the value of these constants changes (changing <code>tau</code> with <code>tau.set(10.0)</code> will not modify the value of <code>real_tau</code>).</p>"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.__init__","title":"<code>__init__(name, value, net_id=0)</code>","text":"<p>Parameters:</p> <ul> <li> name         \u2013          <p>name of the constant (unique), which can be used in equations.</p> </li> <li> value         \u2013          <p>the value of the constant, which must be a float, or a combination of Constants.</p> </li> </ul>"},{"location":"API/ANNarchy.html#ANNarchy.core.Global.Constant.set","title":"<code>set(value)</code>","text":"<p>Changes the value of the constant.</p>"},{"location":"API/ANNarchy.html#learning","title":"Learning","text":""},{"location":"API/ANNarchy.html#ANNarchy.enable_learning","title":"<code>ANNarchy.enable_learning(projections=None, period=None, offset=None, net_id=0)</code>","text":"<p>Enables learning for all projections. Optionally period and offset can be changed for all projections.</p> <p>Parameters:</p> <ul> <li> projections         \u2013          <p>the projections whose learning should be enabled. By default, all the existing projections are enabled.</p> </li> <li> period         \u2013          <p>determines how often the synaptic variables will be updated.</p> </li> <li> offset         \u2013          <p>determines the offset at which the synaptic variables will be updated relative to the current time.</p> </li> </ul>"},{"location":"API/ANNarchy.html#ANNarchy.disable_learning","title":"<code>ANNarchy.disable_learning(projections=None, net_id=0)</code>","text":"<p>Disables learning for all projections.</p> <p>Parameters:</p> <ul> <li> projections         \u2013          <p>the projections whose learning should be disabled. By default, all the existing projections are disabled.</p> </li> </ul>"},{"location":"API/ANNarchy.html#access-to-simulation-times","title":"Access to simulation times","text":""},{"location":"API/ANNarchy.html#ANNarchy.get_time","title":"<code>ANNarchy.get_time(net_id=0)</code>","text":"<p>Returns the current time in ms.</p>"},{"location":"API/ANNarchy.html#ANNarchy.set_time","title":"<code>ANNarchy.set_time(t, net_id=0)</code>","text":"<p>Sets the current time in ms.</p> <p>Warning: can be dangerous for some spiking models.</p>"},{"location":"API/ANNarchy.html#ANNarchy.get_current_step","title":"<code>ANNarchy.get_current_step(net_id=0)</code>","text":"<p>Returns the current simulation step.</p>"},{"location":"API/ANNarchy.html#ANNarchy.set_current_step","title":"<code>ANNarchy.set_current_step(t, net_id=0)</code>","text":"<p>Sets the current simulation step (integer).</p> <p>Warning: can be dangerous for some spiking models.</p>"},{"location":"API/ANNarchy.html#ANNarchy.dt","title":"<code>ANNarchy.dt()</code>","text":"<p>Returns the simulation step size <code>dt</code> used in the simulation.</p>"},{"location":"API/BOLD.html","title":"BOLD monitoring","text":"<p>BOLD monitoring utilities are provided in the module <code>ANNarchy.extensions.bold</code>, which must be explicitly imported:</p> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.bold import BoldMonitor\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldMonitor","title":"<code>ANNarchy.extensions.bold.BoldMonitor</code>","text":""},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor","title":"<code>BoldMonitor</code>","text":"<p>             Bases: <code>object</code></p> <p>Monitors the BOLD signal for several populations using a computational model.</p> <p>The BOLD monitor transforms one or two input population variables (such as the mean firing rate) into a recordable BOLD signal according to a computational model (for example a variation of the Balloon model).</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/BoldMonitor.py</code> <pre><code>class BoldMonitor(object):\n\"\"\"\n    Monitors the BOLD signal for several populations using a computational model.\n\n    The BOLD monitor transforms one or two input population variables (such as the mean firing rate) into a recordable BOLD signal according to a computational model (for example a variation of the Balloon model).\n    \"\"\"\n    def __init__(self,\n        populations=None,\n        bold_model=balloon_RN,\n        mapping={'I_CBF': 'r'},\n        scale_factor=None,\n        normalize_input=None,\n        recorded_variables=None,\n        start=False,\n        net_id=0,\n        copied=False):\n\"\"\"\n        :param populations: list of recorded populations.\n\n        :param bold_model: computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is `balloon_RN`.\n\n        :param mapping: mapping dictionary between the inputs of the BOLD model (`I_CBF` for single inputs, `I_CBF` and `I_CMRO2` for double inputs in the provided examples) and the variables of the input populations. By default, `{'I_CBF': 'r'}` maps the firing rate `r` of the input population(s) to the variable `I_CBF` of the BOLD model. \n\n        :param scale_factor: list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region.\n\n        :param normalize_input: list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time).\n\n        :param recorded_variables: which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples).\n        \"\"\"\n        self.net_id = net_id\n\n        # instantiate if necessary, please note\n        # that population will make a deepcopy on this objects\n        if inspect.isclass(bold_model):\n            bold_model = bold_model()\n\n        # for reporting\n        bold_model._model_instantiated = True\n\n        # The usage of [] as default arguments in the __init__ call lead to strange side effects.\n        # We decided therefore to use None as default and create the lists locally.\n        if populations is None:\n            Global._error(\"Either a population or a list of populations must be provided to the BOLD monitor (populations=...)\")\n        if scale_factor is None:\n            scale_factor = []\n        if normalize_input is None:\n            normalize_input = []\n        if recorded_variables is None:\n            recorded_variables = []\n\n        # argument check\n        if not(isinstance(populations, list)):\n            populations = [populations]\n        if not(isinstance(scale_factor, list)):\n            scale_factor = [scale_factor]*len(populations)\n        if not(isinstance(normalize_input, list)):\n            normalize_input = [normalize_input]*len(populations)\n        if isinstance(recorded_variables, str):\n            recorded_variables = [recorded_variables]\n\n        if len(scale_factor) &gt; 0:\n            if len(populations) != len(scale_factor):\n                Global._error(\"BoldMonitor: Length of scale_factor must be equal to number of populations\")\n\n        if len(normalize_input) &gt; 0:\n            if len(populations) != len(normalize_input):\n                Global._error(\"BoldMonitor: Length of normalize_input must be equal to number of populations\")\n\n        # Check mapping\n        for target, input_var in mapping.items():\n            if not target in bold_model._inputs:\n                Global._error(\"BoldMonitor: the key \" + target + \" of mapping is not part of the BOLD model.\")\n\n        # Check recorded variables\n        if len(recorded_variables) == 0:\n            recorded_variables = bold_model._output\n        else:\n            # Add the output variables (and remove doublons)\n            l1 = bold_model._output\n            l2 = [recorded_variables] if isinstance(recorded_variables, str) else recorded_variables\n            recorded_variables = list(set(l2+l1))\n            recorded_variables.sort()\n\n        if not copied:\n            # Add the container to the object management\n            Global._network[0]['extensions'].append(self)\n            self.id = len(Global._network[self.net_id]['extensions'])\n\n            # create the population\n            self._bold_pop = Population(1, neuron=bold_model, name= bold_model.name )\n            self._bold_pop.enabled = start\n\n            # create the monitor\n            self._monitor = Monitor(self._bold_pop, recorded_variables, start=start)\n\n            # create the projection(s)\n            self._acc_proj = []\n\n            if len(scale_factor) == 0:\n                pop_overall_size = 0\n                for _, pop in enumerate(populations):\n                    pop_overall_size += pop.size\n\n                # the conductance is normalized between [0 .. 1]. This scale factor\n                # should balance different population sizes\n                for _, pop in enumerate(populations):\n                    scale_factor_conductance = float(pop.size)/float(pop_overall_size)\n                    scale_factor.append(scale_factor_conductance)\n\n            if len(normalize_input) == 0:\n                normalize_input = [0] * len(populations)\n                # TODO: can we check if users used NormProjections? If not, this will crash ...\n\n            for target, input_var in mapping.items():\n                for pop, scale, normalize in zip(populations, scale_factor, normalize_input):\n\n                    tmp_proj = AccProjection(pre = pop, post=self._bold_pop, target=target, variable=input_var, scale_factor=scale, normalize_input=normalize)\n\n                    tmp_proj.connect_all_to_all(weights= 1.0)\n\n                    self._acc_proj.append(tmp_proj)\n\n        else:\n            # Add the container to the object management\n            Global._network[net_id]['extensions'].append(self)\n            self.id = len(Global._network[self.net_id]['extensions'])\n\n            # instances are assigned by the copying instance\n            self._bold_pop = None\n            self._monitor = None\n            self._acc_proj = []\n\n        self.name = \"bold_monitor\"\n\n        # store arguments for copy\n        self._populations = populations\n        self._bold_model = bold_model\n        self._mapping = mapping\n        self._scale_factor = scale_factor\n        self._normalize_input = normalize_input\n        self._recorded_variables = recorded_variables\n        self._start = start\n\n        # Finalize initialization\n        self._initialized = True if not copied else False\n\n    #\n    #   MONITOR functions\n    #\n    def start(self):\n\"\"\"\n        Same as `ANNarchy.core.Monitor.start()`\n        \"\"\"\n        self._monitor.start()\n\n        # enable ODEs\n        self._bold_pop.cyInstance.activate(True)\n\n        # check if we have projections with baseline\n        for proj in self._acc_proj:\n            if proj._normalize_input &gt; 0:\n                proj.cyInstance.start(proj._normalize_input/Global.config[\"dt\"])\n\n    def stop(self):\n\"\"\"\n        Same as `ANNarchy.core.Monitor.stop()`\n        \"\"\"\n        self._monitor.stop()\n\n        # enable ODEs\n        self._bold_pop.cyInstance.activate(False)\n\n    def get(self, variable):\n\"\"\"\n        Same as `ANNarchy.core.Monitor.get()`\n        \"\"\"\n        return self._monitor.get(variable)\n\n\n    #\n    #   POPULATION functions i. e. access to model parameter\n    #\n\n    # Method called when accessing an attribute.\n    # We overload the default to allow access to monitor variables.\n    def __getattr__(self, name):\n\n        if name == '_initialized' or not hasattr(self, '_initialized'): # Before the end of the constructor\n            return object.__getattribute__(self, name)\n\n        if self._initialized:\n            if self._bold_pop.initialized == False:\n                Global._error(\"BoldMonitor: attributes can not modified before compile()\")\n\n            if name in self._bold_pop.attributes:\n                return getattr(self._bold_pop, name)\n\n        return object.__getattribute__(self, name)\n\n    # Method called when accessing an attribute.\n    # We overload the default to allow access to monitor variables.\n    def __setattr__(self, name, value):\n\n        if name == '_initialized' or not hasattr(self, '_initialized'): # Before the end of the constructor\n            return object.__setattr__(self, name, value)\n\n        if self._initialized:\n            if self._bold_pop.initialized == False:\n                Global._error(\"BoldMonitor: attributes can not modified before compile()\")\n\n            if name in self._bold_pop.attributes:\n                setattr(self._bold_pop, name, value)\n            else:\n                raise AttributeError(\"the variable '\"+str(name)+ \"' is not an attribute of the bold model.\")\n\n        else:\n            object.__setattr__(self, name, value)\n\n    #\n    # Destruction\n    def _clear(self):\n        pass\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor.__init__","title":"<code>__init__(populations=None, bold_model=balloon_RN, mapping={'I_CBF': 'r'}, scale_factor=None, normalize_input=None, recorded_variables=None, start=False, net_id=0, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> populations         \u2013          <p>list of recorded populations.</p> </li> <li> bold_model         \u2013          <p>computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is <code>balloon_RN</code>.</p> </li> <li> mapping         \u2013          <p>mapping dictionary between the inputs of the BOLD model (<code>I_CBF</code> for single inputs, <code>I_CBF</code> and <code>I_CMRO2</code> for double inputs in the provided examples) and the variables of the input populations. By default, <code>{'I_CBF': 'r'}</code> maps the firing rate <code>r</code> of the input population(s) to the variable <code>I_CBF</code> of the BOLD model.</p> </li> <li> scale_factor         \u2013          <p>list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region.</p> </li> <li> normalize_input         \u2013          <p>list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time).</p> </li> <li> recorded_variables         \u2013          <p>which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples).</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/BoldMonitor.py</code> <pre><code>def __init__(self,\n    populations=None,\n    bold_model=balloon_RN,\n    mapping={'I_CBF': 'r'},\n    scale_factor=None,\n    normalize_input=None,\n    recorded_variables=None,\n    start=False,\n    net_id=0,\n    copied=False):\n\"\"\"\n    :param populations: list of recorded populations.\n\n    :param bold_model: computational model for BOLD signal defined as a BoldModel class/object (see ANNarchy.extensions.bold.PredefinedModels for more predefined examples). Default is `balloon_RN`.\n\n    :param mapping: mapping dictionary between the inputs of the BOLD model (`I_CBF` for single inputs, `I_CBF` and `I_CMRO2` for double inputs in the provided examples) and the variables of the input populations. By default, `{'I_CBF': 'r'}` maps the firing rate `r` of the input population(s) to the variable `I_CBF` of the BOLD model. \n\n    :param scale_factor: list of float values to allow a weighting of signals between populations. By default, the input signal is weighted by the ratio of the population size to all populations within the recorded region.\n\n    :param normalize_input: list of integer values which represent a optional baseline per population. The input signals will require an additional normalization using a baseline value. A value different from 0 represents the time period for determing this baseline in milliseconds (biological time).\n\n    :param recorded_variables: which variables of the BOLD model should be recorded? (by default, the output variable of the BOLD model is added, e.g. [\"BOLD\"] for the provided examples).\n    \"\"\"\n    self.net_id = net_id\n\n    # instantiate if necessary, please note\n    # that population will make a deepcopy on this objects\n    if inspect.isclass(bold_model):\n        bold_model = bold_model()\n\n    # for reporting\n    bold_model._model_instantiated = True\n\n    # The usage of [] as default arguments in the __init__ call lead to strange side effects.\n    # We decided therefore to use None as default and create the lists locally.\n    if populations is None:\n        Global._error(\"Either a population or a list of populations must be provided to the BOLD monitor (populations=...)\")\n    if scale_factor is None:\n        scale_factor = []\n    if normalize_input is None:\n        normalize_input = []\n    if recorded_variables is None:\n        recorded_variables = []\n\n    # argument check\n    if not(isinstance(populations, list)):\n        populations = [populations]\n    if not(isinstance(scale_factor, list)):\n        scale_factor = [scale_factor]*len(populations)\n    if not(isinstance(normalize_input, list)):\n        normalize_input = [normalize_input]*len(populations)\n    if isinstance(recorded_variables, str):\n        recorded_variables = [recorded_variables]\n\n    if len(scale_factor) &gt; 0:\n        if len(populations) != len(scale_factor):\n            Global._error(\"BoldMonitor: Length of scale_factor must be equal to number of populations\")\n\n    if len(normalize_input) &gt; 0:\n        if len(populations) != len(normalize_input):\n            Global._error(\"BoldMonitor: Length of normalize_input must be equal to number of populations\")\n\n    # Check mapping\n    for target, input_var in mapping.items():\n        if not target in bold_model._inputs:\n            Global._error(\"BoldMonitor: the key \" + target + \" of mapping is not part of the BOLD model.\")\n\n    # Check recorded variables\n    if len(recorded_variables) == 0:\n        recorded_variables = bold_model._output\n    else:\n        # Add the output variables (and remove doublons)\n        l1 = bold_model._output\n        l2 = [recorded_variables] if isinstance(recorded_variables, str) else recorded_variables\n        recorded_variables = list(set(l2+l1))\n        recorded_variables.sort()\n\n    if not copied:\n        # Add the container to the object management\n        Global._network[0]['extensions'].append(self)\n        self.id = len(Global._network[self.net_id]['extensions'])\n\n        # create the population\n        self._bold_pop = Population(1, neuron=bold_model, name= bold_model.name )\n        self._bold_pop.enabled = start\n\n        # create the monitor\n        self._monitor = Monitor(self._bold_pop, recorded_variables, start=start)\n\n        # create the projection(s)\n        self._acc_proj = []\n\n        if len(scale_factor) == 0:\n            pop_overall_size = 0\n            for _, pop in enumerate(populations):\n                pop_overall_size += pop.size\n\n            # the conductance is normalized between [0 .. 1]. This scale factor\n            # should balance different population sizes\n            for _, pop in enumerate(populations):\n                scale_factor_conductance = float(pop.size)/float(pop_overall_size)\n                scale_factor.append(scale_factor_conductance)\n\n        if len(normalize_input) == 0:\n            normalize_input = [0] * len(populations)\n            # TODO: can we check if users used NormProjections? If not, this will crash ...\n\n        for target, input_var in mapping.items():\n            for pop, scale, normalize in zip(populations, scale_factor, normalize_input):\n\n                tmp_proj = AccProjection(pre = pop, post=self._bold_pop, target=target, variable=input_var, scale_factor=scale, normalize_input=normalize)\n\n                tmp_proj.connect_all_to_all(weights= 1.0)\n\n                self._acc_proj.append(tmp_proj)\n\n    else:\n        # Add the container to the object management\n        Global._network[net_id]['extensions'].append(self)\n        self.id = len(Global._network[self.net_id]['extensions'])\n\n        # instances are assigned by the copying instance\n        self._bold_pop = None\n        self._monitor = None\n        self._acc_proj = []\n\n    self.name = \"bold_monitor\"\n\n    # store arguments for copy\n    self._populations = populations\n    self._bold_model = bold_model\n    self._mapping = mapping\n    self._scale_factor = scale_factor\n    self._normalize_input = normalize_input\n    self._recorded_variables = recorded_variables\n    self._start = start\n\n    # Finalize initialization\n    self._initialized = True if not copied else False\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor.get","title":"<code>get(variable)</code>","text":"<p>Same as <code>ANNarchy.core.Monitor.get()</code></p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/BoldMonitor.py</code> <pre><code>def get(self, variable):\n\"\"\"\n    Same as `ANNarchy.core.Monitor.get()`\n    \"\"\"\n    return self._monitor.get(variable)\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor.start","title":"<code>start()</code>","text":"<p>Same as <code>ANNarchy.core.Monitor.start()</code></p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/BoldMonitor.py</code> <pre><code>def start(self):\n\"\"\"\n    Same as `ANNarchy.core.Monitor.start()`\n    \"\"\"\n    self._monitor.start()\n\n    # enable ODEs\n    self._bold_pop.cyInstance.activate(True)\n\n    # check if we have projections with baseline\n    for proj in self._acc_proj:\n        if proj._normalize_input &gt; 0:\n            proj.cyInstance.start(proj._normalize_input/Global.config[\"dt\"])\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldMonitor.BoldMonitor.stop","title":"<code>stop()</code>","text":"<p>Same as <code>ANNarchy.core.Monitor.stop()</code></p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/BoldMonitor.py</code> <pre><code>def stop(self):\n\"\"\"\n    Same as `ANNarchy.core.Monitor.stop()`\n    \"\"\"\n    self._monitor.stop()\n\n    # enable ODEs\n    self._bold_pop.cyInstance.activate(False)\n</code></pre>"},{"location":"API/BOLD.html#bold-models","title":"BOLD models","text":"<p>The provided BOLD models follow the Balloon model (Buxton et al., 1998) with the different variations studied in (Stephan et al., 2007). Those models all compute the vascular response to neural activity through a dampened oscillator:</p> \\[     \\frac{ds}{dt} = \\phi \\, I_\\text{CBF} - \\kappa \\, s - \\gamma \\, (f_{in} - 1) \\] \\[     \\frac{df_{in}}{dt} = s \\] <p>This allows to compute the oxygen extraction fraction:</p> \\[     E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} } \\] <p>The (normalized) venous blood volume is computed as:</p> \\[     \\tau_0 \\, \\frac{dv}{dt} = (f_{in} - f_{out}) \\] \\[     f_{out} = v^{\\frac{1}{\\alpha}} \\] <p>The level of deoxyhemoglobin into the venous compartment is computed by:</p> \\[     \\tau_0 \\, \\frac{dq}{dt} = f_{in} \\, \\frac{E}{E_0} - \\frac{q}{v} \\, f_{out} \\] <p>Using the two signals \\(v\\) and \\(q\\), there are two ways to compute the corresponding BOLD signal:</p> <ul> <li>N: Non-linear BOLD equation:</li> </ul> \\[     BOLD = v_0 \\, ( k_1 \\, (1-q) + k_2 \\, (1- \\dfrac{q}{v}) + k_3 \\, (1 - v) ) \\] <ul> <li>L: Linear BOLD equation:</li> </ul> \\[     BOLD = v_0 \\, ((k_1 + k_2) \\, (1 - q) + (k_3 - k_2) \\, (1 - v))  \\] <p>Additionally, the three coefficients \\(k_1\\), \\(k_2\\), \\(k_3\\) can be computed in two different ways:</p> <ul> <li>C: classical coefficients from (Buxton et al., 1998):</li> </ul> \\[k_1            = (1 - v_0) \\, 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2            = 2 \\, E_0\\] \\[k_3            = 1 - \\epsilon\\] <ul> <li>R: revised coefficients from (Obata et al., 2004):</li> </ul> \\[k_1            = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2            = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\\] \\[k_3            = 1 - \\epsilon\\] <p>This makes a total of four different BOLD model (RN, RL, CN, CL) which are provided by the extension. The different parameters can be modified in the constructor. Additionally, we also provide the model that was used in (Maith et al., 2021) and the two-inputs model of (Maith et al, 2022).</p>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldModel","title":"<code>ANNarchy.extensions.bold.BoldModel</code>","text":""},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldModel.BoldModel","title":"<code>BoldModel</code>","text":"<p>             Bases: <code>Neuron</code></p> <p>Base class to define a BOLD model to be used in a BOLD monitor.</p> <p>A BOLD model is quite similar to a regular rate-coded neuron. It gets a weighted sum of inputs with a specific target (e.g. I_CBF) and compute a single output variable (called <code>BOLD</code> in the predefined models, but it could be anything).</p> <p>The main difference is that a BOLD model should also declare which targets are used for the input signal:</p> <pre><code>bold_model = BoldModel(\n    parameters = '''\n        tau = 1000.\n    ''',\n    equations = '''\n        I_CBF = sum(I_CBF)\n        # ...\n        tau * dBOLD/dt = I_CBF - BOLD\n    ''',\n    inputs = \"I_CBF\"\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/BoldModel.py</code> <pre><code>class BoldModel(Neuron):\n\"\"\"\n    Base class to define a BOLD model to be used in a BOLD monitor.\n\n    A BOLD model is quite similar to a regular rate-coded neuron. It gets a weighted sum of inputs with a specific target (e.g. I_CBF) and compute a single output variable (called `BOLD` in the predefined models, but it could be anything).\n\n    The main difference is that a BOLD model should also declare which targets are used for the input signal:\n\n    ```python\n    bold_model = BoldModel(\n        parameters = '''\n            tau = 1000.\n        ''',\n        equations = '''\n            I_CBF = sum(I_CBF)\n            # ...\n            tau * dBOLD/dt = I_CBF - BOLD\n        ''',\n        inputs = \"I_CBF\"\n    )\n    ```\n    \"\"\"\n    def __init__(self, parameters, equations, inputs, output=[\"BOLD\"], name=\"Custom BOLD model\", description=\"\"):\n\"\"\"\n        See ANNarchy.extensions.bold.PredefinedModels.py for some example models.\n\n        :param parameters: parameters of the model and their initial value.\n        :param equations: equations defining the temporal evolution of variables.\n        :param inputs: single variable or list of input signals (e.g. 'I_CBF' or ['I_CBF', 'I_CMRO2']).\n        :param output: output variable of the model (default is 'BOLD').\n        :param name: optional model name.\n        :param description: optional model description.\n        \"\"\"\n        # The processing in BoldMonitor expects lists, but the interface\n        # should allow also single strings (if only one variable is considered)\n        self._inputs = [inputs] if isinstance(inputs, str) else inputs\n        self._output = [output] if isinstance(output, str) else output\n\n        Neuron.__init__(self, parameters=parameters, equations=equations, name=name, description=description)\n\n        self._model_instantiated = False    # activated by BoldMonitor\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.BoldModel.BoldModel.__init__","title":"<code>__init__(parameters, equations, inputs, output=['BOLD'], name='Custom BOLD model', description='')</code>","text":"<p>See ANNarchy.extensions.bold.PredefinedModels.py for some example models.</p> <p>Parameters:</p> <ul> <li> parameters         \u2013          <p>parameters of the model and their initial value.</p> </li> <li> equations         \u2013          <p>equations defining the temporal evolution of variables.</p> </li> <li> inputs         \u2013          <p>single variable or list of input signals (e.g. 'I_CBF' or ['I_CBF', 'I_CMRO2']).</p> </li> <li> output         \u2013          <p>output variable of the model (default is 'BOLD').</p> </li> <li> name         \u2013          <p>optional model name.</p> </li> <li> description         \u2013          <p>optional model description.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/BoldModel.py</code> <pre><code>def __init__(self, parameters, equations, inputs, output=[\"BOLD\"], name=\"Custom BOLD model\", description=\"\"):\n\"\"\"\n    See ANNarchy.extensions.bold.PredefinedModels.py for some example models.\n\n    :param parameters: parameters of the model and their initial value.\n    :param equations: equations defining the temporal evolution of variables.\n    :param inputs: single variable or list of input signals (e.g. 'I_CBF' or ['I_CBF', 'I_CMRO2']).\n    :param output: output variable of the model (default is 'BOLD').\n    :param name: optional model name.\n    :param description: optional model description.\n    \"\"\"\n    # The processing in BoldMonitor expects lists, but the interface\n    # should allow also single strings (if only one variable is considered)\n    self._inputs = [inputs] if isinstance(inputs, str) else inputs\n    self._output = [output] if isinstance(output, str) else output\n\n    Neuron.__init__(self, parameters=parameters, equations=equations, name=name, description=description)\n\n    self._model_instantiated = False    # activated by BoldMonitor\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.balloon_RN","title":"<code>ANNarchy.extensions.bold.balloon_RN</code>","text":"<p>             Bases: <code>BoldModel</code></p> <p>A balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007).</p> <p>Equivalent code:</p> <pre><code>balloon_RN = BoldModel(\n    parameters = '''\n        second    = 1000.0\n        phi       = 1.0\n        kappa     = 1/1.54\n        gamma     = 1/2.46\n        E_0       = 0.34\n        tau       = 0.98\n        alpha     = 0.33\n        V_0       = 0.02\n        v_0       = 40.3\n        TE        = 40/1000.\n        epsilon   = 1.43\n        r_0       = 25.\n    ''',\n    equations = '''\n        # Single input\n        I_CBF          = sum(I_CBF)                                                : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))  : init=0\n    ''',\n    inputs=\"I_CBF\",\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/PredefinedModels.py</code> <pre><code>class balloon_RN(BoldModel):\n\"\"\"\n    A balloon model with revised coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\n\n    Equivalent code:\n\n    ```python\n    balloon_RN = BoldModel(\n        parameters = '''\n            second    = 1000.0\n            phi       = 1.0\n            kappa     = 1/1.54\n            gamma     = 1/2.46\n            E_0       = 0.34\n            tau       = 0.98\n            alpha     = 0.33\n            V_0       = 0.02\n            v_0       = 40.3\n            TE        = 40/1000.\n            epsilon   = 1.43\n            r_0       = 25.\n        ''',\n        equations = '''\n            # Single input\n            I_CBF          = sum(I_CBF)                                                : init=0\n            ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n            df_in/dt       = s / second                                                : init=1, min=0.01\n\n            E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n            dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n            dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n            f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n            # Revised coefficients\n            k_1            = 4.3 * v_0 * E_0 * TE\n            k_2            = epsilon * r_0 * E_0 * TE\n            k_3            = 1.0 - epsilon\n\n            # Non-linear equation\n            BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))  : init=0\n        ''',\n        inputs=\"I_CBF\",\n    )\n    ```\n    \"\"\"\n    def __init__(self,\n            phi       = 1.0,\n            kappa     = 1/1.54,\n            gamma     = 1/2.46,\n            E_0       = 0.34,\n            tau       = 0.98,\n            alpha     = 0.33,\n            V_0       = 0.02,\n            v_0       = 40.3,\n            TE        = 40/1000.,\n            epsilon   = 1.43,\n            r_0       = 25,\n        ):\n\n\"\"\"\n        :param phi:       input coefficient\n        :param kappa:     signal decay\n        :param gamma:     feedback regulation\n        :param E_0:       oxygen extraction fraction at rest\n        :param tau:       time constant (in s!)\n        :param alpha:     vessel stiffness\n        :param V_0:       resting venous blood volume fraction\n        :param v_0:       frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n        :param TE:        echo time\n        :param epsilon:   ratio of intra- and extravascular signal\n        :param r_0:       slope of the relation between the intravascular relaxation rate and oxygen saturation\n        \"\"\"\n        parameters = \"\"\"\n            second    = 1000.0 : population\n            phi       = %(phi)s : population\n            kappa     = %(kappa)s : population\n            gamma     = %(gamma)s : population\n            E_0       = %(E_0)s : population\n            tau       = %(tau)s : population\n            alpha     = %(alpha)s : population\n            V_0       = %(V_0)s : population\n            v_0       = %(v_0)s : population\n            TE        = %(TE)s : population\n            epsilon   = %(epsilon)s : population\n            r_0       = %(r_0)s : population\n        \"\"\" % {\n            'phi': phi,\n            'kappa': kappa,\n            'gamma': gamma,\n            'E_0': E_0,\n            'tau': tau,\n            'alpha': alpha,\n            'V_0': V_0,\n            'v_0': v_0,\n            'TE': TE,\n            'epsilon': epsilon,\n            'r_0': r_0,\n        }\n\n        equations = \"\"\"\n            # Single input\n            I_CBF          = sum(I_CBF)                                                : init=0\n            ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n            df_in/dt       = s / second                                                : init=1, min=0.01\n\n            E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n            dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n            dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n            f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n            # Revised coefficients\n            k_1            = 4.3 * v_0 * E_0 * TE\n            k_2            = epsilon * r_0 * E_0 * TE\n            k_3            = 1.0 - epsilon\n\n            # Non-linear equation\n            BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))  : init=0\n        \"\"\"\n        name = \"BOLD model RN\"\n        description = \"BOLD computation with revised coefficients and non-linear BOLD equation (Stephan et al., 2007).\"\n\n        BoldModel.__init__(self, \n            parameters=parameters, \n            equations=equations,  \n            inputs=\"I_CBF\",\n            output=\"BOLD\",\n            name=name, description=description\n        )\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.PredefinedModels.balloon_RN.__init__","title":"<code>__init__(phi=1.0, kappa=1 / 1.54, gamma=1 / 2.46, E_0=0.34, tau=0.98, alpha=0.33, V_0=0.02, v_0=40.3, TE=40 / 1000.0, epsilon=1.43, r_0=25)</code>","text":"<p>Parameters:</p> <ul> <li> phi         \u2013          <p>input coefficient</p> </li> <li> kappa         \u2013          <p>signal decay</p> </li> <li> gamma         \u2013          <p>feedback regulation</p> </li> <li> E_0         \u2013          <p>oxygen extraction fraction at rest</p> </li> <li> tau         \u2013          <p>time constant (in s!)</p> </li> <li> alpha         \u2013          <p>vessel stiffness</p> </li> <li> V_0         \u2013          <p>resting venous blood volume fraction</p> </li> <li> v_0         \u2013          <p>frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T</p> </li> <li> TE         \u2013          <p>echo time</p> </li> <li> epsilon         \u2013          <p>ratio of intra- and extravascular signal</p> </li> <li> r_0         \u2013          <p>slope of the relation between the intravascular relaxation rate and oxygen saturation</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/PredefinedModels.py</code> <pre><code>def __init__(self,\n        phi       = 1.0,\n        kappa     = 1/1.54,\n        gamma     = 1/2.46,\n        E_0       = 0.34,\n        tau       = 0.98,\n        alpha     = 0.33,\n        V_0       = 0.02,\n        v_0       = 40.3,\n        TE        = 40/1000.,\n        epsilon   = 1.43,\n        r_0       = 25,\n    ):\n\n\"\"\"\n    :param phi:       input coefficient\n    :param kappa:     signal decay\n    :param gamma:     feedback regulation\n    :param E_0:       oxygen extraction fraction at rest\n    :param tau:       time constant (in s!)\n    :param alpha:     vessel stiffness\n    :param V_0:       resting venous blood volume fraction\n    :param v_0:       frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n    :param TE:        echo time\n    :param epsilon:   ratio of intra- and extravascular signal\n    :param r_0:       slope of the relation between the intravascular relaxation rate and oxygen saturation\n    \"\"\"\n    parameters = \"\"\"\n        second    = 1000.0 : population\n        phi       = %(phi)s : population\n        kappa     = %(kappa)s : population\n        gamma     = %(gamma)s : population\n        E_0       = %(E_0)s : population\n        tau       = %(tau)s : population\n        alpha     = %(alpha)s : population\n        V_0       = %(V_0)s : population\n        v_0       = %(v_0)s : population\n        TE        = %(TE)s : population\n        epsilon   = %(epsilon)s : population\n        r_0       = %(r_0)s : population\n    \"\"\" % {\n        'phi': phi,\n        'kappa': kappa,\n        'gamma': gamma,\n        'E_0': E_0,\n        'tau': tau,\n        'alpha': alpha,\n        'V_0': V_0,\n        'v_0': v_0,\n        'TE': TE,\n        'epsilon': epsilon,\n        'r_0': r_0,\n    }\n\n    equations = \"\"\"\n        # Single input\n        I_CBF          = sum(I_CBF)                                                : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))  : init=0\n    \"\"\"\n    name = \"BOLD model RN\"\n    description = \"BOLD computation with revised coefficients and non-linear BOLD equation (Stephan et al., 2007).\"\n\n    BoldModel.__init__(self, \n        parameters=parameters, \n        equations=equations,  \n        inputs=\"I_CBF\",\n        output=\"BOLD\",\n        name=name, description=description\n    )\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.balloon_RL","title":"<code>ANNarchy.extensions.bold.balloon_RL</code>","text":"<p>             Bases: <code>BoldModel</code></p> <p>A balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007).</p> <p>Equivalent code:</p> <pre><code>balloon_RL = BoldModel(\n    parameters = '''\n        second    = 1000.0\n        phi       = 1.0\n        kappa     = 1/1.54\n        gamma     = 1/2.46\n        E_0       = 0.34\n        tau       = 0.98\n        alpha     = 0.33\n        V_0       = 0.02\n        v_0       = 40.3\n        TE        = 40/1000.\n        epsilon   = 1.43\n        r_0       = 25.\n    ''',\n    equations = '''\n        # Single input\n        I_CBF          = sum(I_CBF)                                                : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Linear equation\n        BOLD           = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v))         : init=0\n    ''',\n    inputs=\"I_CBF\",\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/PredefinedModels.py</code> <pre><code>class balloon_RL(BoldModel):\n\"\"\"\n    A balloon model with revised coefficients and linear BOLD equation derived from Stephan et al. (2007).\n\n    Equivalent code:\n\n    ```python\n    balloon_RL = BoldModel(\n        parameters = '''\n            second    = 1000.0\n            phi       = 1.0\n            kappa     = 1/1.54\n            gamma     = 1/2.46\n            E_0       = 0.34\n            tau       = 0.98\n            alpha     = 0.33\n            V_0       = 0.02\n            v_0       = 40.3\n            TE        = 40/1000.\n            epsilon   = 1.43\n            r_0       = 25.\n        ''',\n        equations = '''\n            # Single input\n            I_CBF          = sum(I_CBF)                                                : init=0\n            ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n            df_in/dt       = s / second                                                : init=1, min=0.01\n\n            E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n            dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n            dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n            f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n            # Revised coefficients\n            k_1            = 4.3 * v_0 * E_0 * TE\n            k_2            = epsilon * r_0 * E_0 * TE\n            k_3            = 1.0 - epsilon\n\n            # Linear equation\n            BOLD           = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v))         : init=0\n        ''',\n        inputs=\"I_CBF\",\n    )\n    ```\n    \"\"\"\n    def __init__(self,\n            phi       = 1.0,\n            kappa     = 1/1.54,\n            gamma     = 1/2.46,\n            E_0       = 0.34,\n            tau       = 0.98,\n            alpha     = 0.33,\n            V_0       = 0.02,\n            v_0       = 40.3,\n            TE        = 40/1000.,\n            epsilon   = 1.43,\n            r_0       = 25,\n        ):\n\n\"\"\"\n        :param phi:       input coefficient\n        :param kappa:     signal decay\n        :param gamma:     feedback regulation\n        :param E_0:       oxygen extraction fraction at rest\n        :param tau:       time constant (in s!)\n        :param alpha:     vessel stiffness\n        :param V_0:       resting venous blood volume fraction\n        :param v_0:       frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n        :param TE:        echo time\n        :param epsilon:   ratio of intra- and extravascular signal\n        :param r_0:       slope of the relation between the intravascular relaxation rate and oxygen saturation\n        \"\"\"\n\n        parameters = \"\"\"\n            second    = 1000.0 : population\n            phi       = %(phi)s : population\n            kappa     = %(kappa)s : population\n            gamma     = %(gamma)s : population\n            E_0       = %(E_0)s : population\n            tau       = %(tau)s : population\n            alpha     = %(alpha)s : population\n            V_0       = %(V_0)s : population\n            v_0       = %(v_0)s : population\n            TE        = %(TE)s : population\n            epsilon   = %(epsilon)s : population\n            r_0       = %(r_0)s : population\n        \"\"\" % {\n            'phi': phi,\n            'kappa': kappa,\n            'gamma': gamma,\n            'E_0': E_0,\n            'tau': tau,\n            'alpha': alpha,\n            'V_0': V_0,\n            'v_0': v_0,\n            'TE': TE,\n            'epsilon': epsilon,\n            'r_0': r_0,\n        }\n\n        equations = \"\"\"\n            # Single input\n            I_CBF          = sum(I_CBF)                                                    : init=0\n            ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second         : init=0\n            df_in/dt       = s  / second                                                   : init=1, min=0.01\n\n            E              = 1 - (1 - E_0)**(1 / f_in)                                     : init=0.3424\n            tau*dq/dt      = f_in * E / E_0 - (q / v) * f_out                              : init=1, min=0.01\n            tau*dv/dt      = f_in - f_out                                                  : init=1, min=0.01\n            f_out          = v**(1 / alpha)                                                : init=1, min=0.01\n\n            # Revised coeeficients\n            k_1            = 4.3 * v_0 * E_0 * TE\n            k_2            = epsilon * r_0 * E_0 * TE\n            k_3            = 1 - epsilon\n\n            # Linear equation\n            BOLD           = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v))         : init=0\n        \"\"\"\n        name = \"BOLD model RL\"\n        description = \"BOLD computation with revised coefficients and linear BOLD equation (Stephan et al., 2007).\"\n\n        BoldModel.__init__(self, \n            parameters=parameters, \n            equations=equations,  \n            inputs=\"I_CBF\",\n            output=\"BOLD\",\n            name=name, description=description\n        )\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.PredefinedModels.balloon_RL.__init__","title":"<code>__init__(phi=1.0, kappa=1 / 1.54, gamma=1 / 2.46, E_0=0.34, tau=0.98, alpha=0.33, V_0=0.02, v_0=40.3, TE=40 / 1000.0, epsilon=1.43, r_0=25)</code>","text":"<p>Parameters:</p> <ul> <li> phi         \u2013          <p>input coefficient</p> </li> <li> kappa         \u2013          <p>signal decay</p> </li> <li> gamma         \u2013          <p>feedback regulation</p> </li> <li> E_0         \u2013          <p>oxygen extraction fraction at rest</p> </li> <li> tau         \u2013          <p>time constant (in s!)</p> </li> <li> alpha         \u2013          <p>vessel stiffness</p> </li> <li> V_0         \u2013          <p>resting venous blood volume fraction</p> </li> <li> v_0         \u2013          <p>frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T</p> </li> <li> TE         \u2013          <p>echo time</p> </li> <li> epsilon         \u2013          <p>ratio of intra- and extravascular signal</p> </li> <li> r_0         \u2013          <p>slope of the relation between the intravascular relaxation rate and oxygen saturation</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/PredefinedModels.py</code> <pre><code>def __init__(self,\n        phi       = 1.0,\n        kappa     = 1/1.54,\n        gamma     = 1/2.46,\n        E_0       = 0.34,\n        tau       = 0.98,\n        alpha     = 0.33,\n        V_0       = 0.02,\n        v_0       = 40.3,\n        TE        = 40/1000.,\n        epsilon   = 1.43,\n        r_0       = 25,\n    ):\n\n\"\"\"\n    :param phi:       input coefficient\n    :param kappa:     signal decay\n    :param gamma:     feedback regulation\n    :param E_0:       oxygen extraction fraction at rest\n    :param tau:       time constant (in s!)\n    :param alpha:     vessel stiffness\n    :param V_0:       resting venous blood volume fraction\n    :param v_0:       frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n    :param TE:        echo time\n    :param epsilon:   ratio of intra- and extravascular signal\n    :param r_0:       slope of the relation between the intravascular relaxation rate and oxygen saturation\n    \"\"\"\n\n    parameters = \"\"\"\n        second    = 1000.0 : population\n        phi       = %(phi)s : population\n        kappa     = %(kappa)s : population\n        gamma     = %(gamma)s : population\n        E_0       = %(E_0)s : population\n        tau       = %(tau)s : population\n        alpha     = %(alpha)s : population\n        V_0       = %(V_0)s : population\n        v_0       = %(v_0)s : population\n        TE        = %(TE)s : population\n        epsilon   = %(epsilon)s : population\n        r_0       = %(r_0)s : population\n    \"\"\" % {\n        'phi': phi,\n        'kappa': kappa,\n        'gamma': gamma,\n        'E_0': E_0,\n        'tau': tau,\n        'alpha': alpha,\n        'V_0': V_0,\n        'v_0': v_0,\n        'TE': TE,\n        'epsilon': epsilon,\n        'r_0': r_0,\n    }\n\n    equations = \"\"\"\n        # Single input\n        I_CBF          = sum(I_CBF)                                                    : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second         : init=0\n        df_in/dt       = s  / second                                                   : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                     : init=0.3424\n        tau*dq/dt      = f_in * E / E_0 - (q / v) * f_out                              : init=1, min=0.01\n        tau*dv/dt      = f_in - f_out                                                  : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                                : init=1, min=0.01\n\n        # Revised coeeficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1 - epsilon\n\n        # Linear equation\n        BOLD           = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v))         : init=0\n    \"\"\"\n    name = \"BOLD model RL\"\n    description = \"BOLD computation with revised coefficients and linear BOLD equation (Stephan et al., 2007).\"\n\n    BoldModel.__init__(self, \n        parameters=parameters, \n        equations=equations,  \n        inputs=\"I_CBF\",\n        output=\"BOLD\",\n        name=name, description=description\n    )\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.balloon_CL","title":"<code>ANNarchy.extensions.bold.balloon_CL</code>","text":"<p>             Bases: <code>BoldModel</code></p> <p>A balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007).</p> <p>Equivalent code:</p> <pre><code>balloon_CL = BoldModel(\n    parameters = '''\n        second    = 1000.0\n        phi       = 1.0\n        kappa     = 1/1.54\n        gamma     = 1/2.46\n        E_0       = 0.34\n        tau       = 0.98\n        alpha     = 0.33\n        V_0       = 0.02\n        v_0       = 40.3\n        TE        = 40/1000.\n        epsilon   = 1.43\n    ''',\n    equations = '''\n        # Single input\n        I_CBF          = sum(I_CBF)                                                : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Classic coefficients\n        k_1            = (1 - V_0) * 4.3 * v_0 * E_0 * TE\n        k_2            = 2 * E_0\n        k_3            = 1 - epsilon\n\n        # Linear equation\n        BOLD           = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v))        : init=0\n    ''',\n    inputs=\"I_CBF\",\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/PredefinedModels.py</code> <pre><code>class balloon_CL(BoldModel):\n\"\"\"\n    A balloon model with classical coefficients and linear BOLD equation derived from Stephan et al. (2007).\n\n    Equivalent code:\n\n    ```python\n    balloon_CL = BoldModel(\n        parameters = '''\n            second    = 1000.0\n            phi       = 1.0\n            kappa     = 1/1.54\n            gamma     = 1/2.46\n            E_0       = 0.34\n            tau       = 0.98\n            alpha     = 0.33\n            V_0       = 0.02\n            v_0       = 40.3\n            TE        = 40/1000.\n            epsilon   = 1.43\n        ''',\n        equations = '''\n            # Single input\n            I_CBF          = sum(I_CBF)                                                : init=0\n            ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n            df_in/dt       = s / second                                                : init=1, min=0.01\n\n            E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n            dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n            dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n            f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n            # Classic coefficients\n            k_1            = (1 - V_0) * 4.3 * v_0 * E_0 * TE\n            k_2            = 2 * E_0\n            k_3            = 1 - epsilon\n\n            # Linear equation\n            BOLD           = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v))        : init=0\n        ''',\n        inputs=\"I_CBF\",\n    )\n    ```\n    \"\"\"\n    def __init__(self,\n            phi       = 1.0,\n            kappa     = 1/1.54,\n            gamma     = 1/2.46,\n            E_0       = 0.34,\n            tau       = 0.98,\n            alpha     = 0.33,\n            V_0       = 0.02,\n            v_0       = 40.3,\n            TE        = 40/1000.,\n            epsilon   = 1.43,\n        ):\n\n\"\"\"\n        :param phi:       input coefficient\n        :param kappa:     signal decay\n        :param gamma:     feedback regulation\n        :param E_0:       oxygen extraction fraction at rest\n        :param tau:       time constant (in s!)\n        :param alpha:     vessel stiffness\n        :param V_0:       resting venous blood volume fraction\n        :param v_0:       frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n        :param TE:        echo time\n        :param epsilon:   ratio of intra- and extravascular signal\n        \"\"\"\n        parameters = \"\"\"\n            second    = 1000.0 : population\n            phi       = %(phi)s : population\n            kappa     = %(kappa)s : population\n            gamma     = %(gamma)s : population\n            E_0       = %(E_0)s : population\n            tau       = %(tau)s : population\n            alpha     = %(alpha)s : population\n            V_0       = %(V_0)s : population\n            v_0       = %(v_0)s : population\n            TE        = %(TE)s : population\n            epsilon   = %(epsilon)s : population\n        \"\"\" % {\n            'phi': phi,\n            'kappa': kappa,\n            'gamma': gamma,\n            'E_0': E_0,\n            'tau': tau,\n            'alpha': alpha,\n            'V_0': V_0,\n            'v_0': v_0,\n            'TE': TE,\n            'epsilon': epsilon,\n        }\n\n        equations = \"\"\"\n            # Single input\n            I_CBF          = sum(I_CBF)                                                : init=0\n            ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n            df_in/dt       = s / second                                                : init=1, min=0.01\n\n            E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n            dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n            dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n            f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n            # Classic coefficients\n            k_1            = (1 - V_0) * 4.3 * v_0 * E_0 * TE\n            k_2            = 2 * E_0\n            k_3            = 1 - epsilon\n\n            # Linear equation\n            BOLD           = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v))        : init=0\n        \"\"\"\n        name = \"BoldNeuron CL\"\n        description = \"BOLD computation with classic coefficients and linear BOLD equation (Stephan et al., 2007).\"\n\n        BoldModel.__init__(self, \n            parameters=parameters, \n            equations=equations,  \n            inputs=\"I_CBF\",\n            output=\"BOLD\",\n            name=name, description=description\n        )\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.PredefinedModels.balloon_CL.__init__","title":"<code>__init__(phi=1.0, kappa=1 / 1.54, gamma=1 / 2.46, E_0=0.34, tau=0.98, alpha=0.33, V_0=0.02, v_0=40.3, TE=40 / 1000.0, epsilon=1.43)</code>","text":"<p>Parameters:</p> <ul> <li> phi         \u2013          <p>input coefficient</p> </li> <li> kappa         \u2013          <p>signal decay</p> </li> <li> gamma         \u2013          <p>feedback regulation</p> </li> <li> E_0         \u2013          <p>oxygen extraction fraction at rest</p> </li> <li> tau         \u2013          <p>time constant (in s!)</p> </li> <li> alpha         \u2013          <p>vessel stiffness</p> </li> <li> V_0         \u2013          <p>resting venous blood volume fraction</p> </li> <li> v_0         \u2013          <p>frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T</p> </li> <li> TE         \u2013          <p>echo time</p> </li> <li> epsilon         \u2013          <p>ratio of intra- and extravascular signal</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/PredefinedModels.py</code> <pre><code>def __init__(self,\n        phi       = 1.0,\n        kappa     = 1/1.54,\n        gamma     = 1/2.46,\n        E_0       = 0.34,\n        tau       = 0.98,\n        alpha     = 0.33,\n        V_0       = 0.02,\n        v_0       = 40.3,\n        TE        = 40/1000.,\n        epsilon   = 1.43,\n    ):\n\n\"\"\"\n    :param phi:       input coefficient\n    :param kappa:     signal decay\n    :param gamma:     feedback regulation\n    :param E_0:       oxygen extraction fraction at rest\n    :param tau:       time constant (in s!)\n    :param alpha:     vessel stiffness\n    :param V_0:       resting venous blood volume fraction\n    :param v_0:       frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n    :param TE:        echo time\n    :param epsilon:   ratio of intra- and extravascular signal\n    \"\"\"\n    parameters = \"\"\"\n        second    = 1000.0 : population\n        phi       = %(phi)s : population\n        kappa     = %(kappa)s : population\n        gamma     = %(gamma)s : population\n        E_0       = %(E_0)s : population\n        tau       = %(tau)s : population\n        alpha     = %(alpha)s : population\n        V_0       = %(V_0)s : population\n        v_0       = %(v_0)s : population\n        TE        = %(TE)s : population\n        epsilon   = %(epsilon)s : population\n    \"\"\" % {\n        'phi': phi,\n        'kappa': kappa,\n        'gamma': gamma,\n        'E_0': E_0,\n        'tau': tau,\n        'alpha': alpha,\n        'V_0': V_0,\n        'v_0': v_0,\n        'TE': TE,\n        'epsilon': epsilon,\n    }\n\n    equations = \"\"\"\n        # Single input\n        I_CBF          = sum(I_CBF)                                                : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Classic coefficients\n        k_1            = (1 - V_0) * 4.3 * v_0 * E_0 * TE\n        k_2            = 2 * E_0\n        k_3            = 1 - epsilon\n\n        # Linear equation\n        BOLD           = V_0 * ((k_1 + k_2) * (1 - q) + (k_3 - k_2) * (1 - v))        : init=0\n    \"\"\"\n    name = \"BoldNeuron CL\"\n    description = \"BOLD computation with classic coefficients and linear BOLD equation (Stephan et al., 2007).\"\n\n    BoldModel.__init__(self, \n        parameters=parameters, \n        equations=equations,  \n        inputs=\"I_CBF\",\n        output=\"BOLD\",\n        name=name, description=description\n    )\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.balloon_CN","title":"<code>ANNarchy.extensions.bold.balloon_CN</code>","text":"<p>             Bases: <code>BoldModel</code></p> <p>A balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007).</p> <p>Equivalent code:</p> <pre><code>balloon_CN = BoldModel(\n    parameters = '''\n        second    = 1000.0\n        phi       = 1.0\n        kappa     = 1/1.54\n        gamma     = 1/2.46\n        E_0       = 0.34\n        tau       = 0.98\n        alpha     = 0.33\n        V_0       = 0.02\n        v_0       = 40.3\n        TE        = 40/1000.\n        epsilon   = 1.43\n    ''',\n    equations = '''\n        # Single input\n        I_CBF          = sum(I_CBF)                                                : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Classic coefficients\n        k_1            = (1 - V_0) * 4.3 * v_0 * E_0 * TE\n        k_2            = 2 * E_0\n        k_3            = 1 - epsilon\n\n        # Non-linear equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))  : init=0\n    ''',\n    inputs=\"I_CBF\",\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/PredefinedModels.py</code> <pre><code>class balloon_CN(BoldModel):\n\"\"\"\n    A balloon model with classic coefficients and non-linear BOLD equation derived from Stephan et al. (2007).\n\n    Equivalent code:\n\n    ```python\n    balloon_CN = BoldModel(\n        parameters = '''\n            second    = 1000.0\n            phi       = 1.0\n            kappa     = 1/1.54\n            gamma     = 1/2.46\n            E_0       = 0.34\n            tau       = 0.98\n            alpha     = 0.33\n            V_0       = 0.02\n            v_0       = 40.3\n            TE        = 40/1000.\n            epsilon   = 1.43\n        ''',\n        equations = '''\n            # Single input\n            I_CBF          = sum(I_CBF)                                                : init=0\n            ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n            df_in/dt       = s / second                                                : init=1, min=0.01\n\n            E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n            dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n            dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n            f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n            # Classic coefficients\n            k_1            = (1 - V_0) * 4.3 * v_0 * E_0 * TE\n            k_2            = 2 * E_0\n            k_3            = 1 - epsilon\n\n            # Non-linear equation\n            BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))  : init=0\n        ''',\n        inputs=\"I_CBF\",\n    )\n    ```\n    \"\"\"\n    def __init__(self,\n            phi       = 1.0,\n            kappa     = 1/1.54,\n            gamma     = 1/2.46,\n            E_0       = 0.34,\n            tau       = 0.98,\n            alpha     = 0.33,\n            V_0       = 0.02,\n            v_0       = 40.3,\n            TE        = 40/1000.,\n            epsilon   = 1.43,\n        ):\n\n\"\"\"\n        :param phi:       input coefficient\n        :param kappa:     signal decay\n        :param gamma:     feedback regulation\n        :param E_0:       oxygen extraction fraction at rest\n        :param tau:       time constant (in s!)\n        :param alpha:     vessel stiffness\n        :param V_0:       resting venous blood volume fraction\n        :param v_0:       frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n        :param TE:        echo time\n        :param epsilon:   ratio of intra- and extravascular signal\n        \"\"\"\n        parameters = \"\"\"\n            second    = 1000.0 : population\n            phi       = %(phi)s : population\n            kappa     = %(kappa)s : population\n            gamma     = %(gamma)s : population\n            E_0       = %(E_0)s : population\n            tau       = %(tau)s : population\n            alpha     = %(alpha)s : population\n            V_0       = %(V_0)s : population\n            v_0       = %(v_0)s : population\n            TE        = %(TE)s : population\n            epsilon   = %(epsilon)s : population\n        \"\"\" % {\n            'phi': phi,\n            'kappa': kappa,\n            'gamma': gamma,\n            'E_0': E_0,\n            'tau': tau,\n            'alpha': alpha,\n            'V_0': V_0,\n            'v_0': v_0,\n            'TE': TE,\n            'epsilon': epsilon,\n        }\n\n        equations = \"\"\"\n            # Single input\n            I_CBF          = sum(I_CBF)                                                : init=0\n            ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n            df_in/dt       = s / second                                                : init=1, min=0.01\n\n            E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n            dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n            dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n            f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n            # Classic coefficients\n            k_1            = (1 - V_0) * 4.3 * v_0 * E_0 * TE\n            k_2            = 2 * E_0\n            k_3            = 1 - epsilon\n\n            # Non-linear equation\n            BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))  : init=0\n        \"\"\"\n        name = \"BoldNeuron CN\",\n        description = \"BOLD computation with classic coefficients and non-linear BOLD equation (Stephan et al., 2007).\"\n\n        BoldModel.__init__(self, \n            parameters=parameters, \n            equations=equations,  \n            inputs=\"I_CBF\",\n            output=\"BOLD\",\n            name=name, description=description\n        )\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.PredefinedModels.balloon_CN.__init__","title":"<code>__init__(phi=1.0, kappa=1 / 1.54, gamma=1 / 2.46, E_0=0.34, tau=0.98, alpha=0.33, V_0=0.02, v_0=40.3, TE=40 / 1000.0, epsilon=1.43)</code>","text":"<p>Parameters:</p> <ul> <li> phi         \u2013          <p>input coefficient</p> </li> <li> kappa         \u2013          <p>signal decay</p> </li> <li> gamma         \u2013          <p>feedback regulation</p> </li> <li> E_0         \u2013          <p>oxygen extraction fraction at rest</p> </li> <li> tau         \u2013          <p>time constant (in s!)</p> </li> <li> alpha         \u2013          <p>vessel stiffness</p> </li> <li> V_0         \u2013          <p>resting venous blood volume fraction</p> </li> <li> v_0         \u2013          <p>frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T</p> </li> <li> TE         \u2013          <p>echo time</p> </li> <li> epsilon         \u2013          <p>ratio of intra- and extravascular signal</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/PredefinedModels.py</code> <pre><code>def __init__(self,\n        phi       = 1.0,\n        kappa     = 1/1.54,\n        gamma     = 1/2.46,\n        E_0       = 0.34,\n        tau       = 0.98,\n        alpha     = 0.33,\n        V_0       = 0.02,\n        v_0       = 40.3,\n        TE        = 40/1000.,\n        epsilon   = 1.43,\n    ):\n\n\"\"\"\n    :param phi:       input coefficient\n    :param kappa:     signal decay\n    :param gamma:     feedback regulation\n    :param E_0:       oxygen extraction fraction at rest\n    :param tau:       time constant (in s!)\n    :param alpha:     vessel stiffness\n    :param V_0:       resting venous blood volume fraction\n    :param v_0:       frequency offset at the outer surface of the magnetized vessel for fully deoxygenated blood at 1.5 T\n    :param TE:        echo time\n    :param epsilon:   ratio of intra- and extravascular signal\n    \"\"\"\n    parameters = \"\"\"\n        second    = 1000.0 : population\n        phi       = %(phi)s : population\n        kappa     = %(kappa)s : population\n        gamma     = %(gamma)s : population\n        E_0       = %(E_0)s : population\n        tau       = %(tau)s : population\n        alpha     = %(alpha)s : population\n        V_0       = %(V_0)s : population\n        v_0       = %(v_0)s : population\n        TE        = %(TE)s : population\n        epsilon   = %(epsilon)s : population\n    \"\"\" % {\n        'phi': phi,\n        'kappa': kappa,\n        'gamma': gamma,\n        'E_0': E_0,\n        'tau': tau,\n        'alpha': alpha,\n        'V_0': V_0,\n        'v_0': v_0,\n        'TE': TE,\n        'epsilon': epsilon,\n    }\n\n    equations = \"\"\"\n        # Single input\n        I_CBF          = sum(I_CBF)                                                : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second     : init=0\n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Classic coefficients\n        k_1            = (1 - V_0) * 4.3 * v_0 * E_0 * TE\n        k_2            = 2 * E_0\n        k_3            = 1 - epsilon\n\n        # Non-linear equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))  : init=0\n    \"\"\"\n    name = \"BoldNeuron CN\",\n    description = \"BOLD computation with classic coefficients and non-linear BOLD equation (Stephan et al., 2007).\"\n\n    BoldModel.__init__(self, \n        parameters=parameters, \n        equations=equations,  \n        inputs=\"I_CBF\",\n        output=\"BOLD\",\n        name=name, description=description\n    )\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.balloon_maith2021","title":"<code>ANNarchy.extensions.bold.balloon_maith2021</code>","text":"<p>             Bases: <code>BoldModel</code></p> <p>The balloon model as used in Maith et al. (2021).</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/PredefinedModels.py</code> <pre><code>class balloon_maith2021(BoldModel):\n\"\"\"\n    The balloon model as used in Maith et al. (2021).\n    \"\"\"\n    def __init__(self):\n        \"Constructor\"\n\n        parameters = \"\"\"\n            second    = 1000.0\n            phi   = 1.0\n            kappa = 0.665\n            gamma = 0.412\n            E_0       = 0.3424\n            tau     = 1.0368\n            alpha     = 0.3215\n            V_0       = 0.02\n        \"\"\"\n        equations = \"\"\"\n            I_CBF          = sum(I_CBF)                                                 : init=0\n            ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second      : init=0\n            df_in/dt       = s / second                                                 : init=1, min=0.01\n\n            E              = 1 - (1 - E_0)**(1 / f_in)                                  : init=0.3424\n            dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)            : init=1, min=0.01\n            dv/dt          = (f_in - f_out)/(tau*second)                                : init=1, min=0.01\n            f_out          = v**(1 / alpha)                                             : init=1, min=0.01\n\n            k_1            = 7 * E_0\n            k_2            = 2\n            k_3            = 2 * E_0 - 0.2\n\n            BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0\n        \"\"\"\n        name = \"Maith2021 BOLD model\"\n        description = \"BOLD computation from Maith et al. (2021).\"\n\n        BoldModel.__init__(self, \n            parameters=parameters, \n            equations=equations,  \n            inputs=\"I_CBF\",\n            output=\"BOLD\",\n            name=name, description=description\n        )\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.PredefinedModels.balloon_maith2021.__init__","title":"<code>__init__()</code>","text":"<p>Constructor</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/PredefinedModels.py</code> <pre><code>def __init__(self):\n    \"Constructor\"\n\n    parameters = \"\"\"\n        second    = 1000.0\n        phi   = 1.0\n        kappa = 0.665\n        gamma = 0.412\n        E_0       = 0.3424\n        tau     = 1.0368\n        alpha     = 0.3215\n        V_0       = 0.02\n    \"\"\"\n    equations = \"\"\"\n        I_CBF          = sum(I_CBF)                                                 : init=0\n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second      : init=0\n        df_in/dt       = s / second                                                 : init=1, min=0.01\n\n        E              = 1 - (1 - E_0)**(1 / f_in)                                  : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)            : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                                : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                             : init=1, min=0.01\n\n        k_1            = 7 * E_0\n        k_2            = 2\n        k_3            = 2 * E_0 - 0.2\n\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0\n    \"\"\"\n    name = \"Maith2021 BOLD model\"\n    description = \"BOLD computation from Maith et al. (2021).\"\n\n    BoldModel.__init__(self, \n        parameters=parameters, \n        equations=equations,  \n        inputs=\"I_CBF\",\n        output=\"BOLD\",\n        name=name, description=description\n    )\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.balloon_two_inputs","title":"<code>ANNarchy.extensions.bold.balloon_two_inputs</code>","text":"<p>             Bases: <code>BoldModel</code></p> <p>BOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007).</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/PredefinedModels.py</code> <pre><code>class balloon_two_inputs(BoldModel):\n\"\"\"\n    BOLD model with two input signals (CBF-driving and CMRO2-driving) for the ballon model and non-linear BOLD equation with revised coefficients based on Buxton et al. (2004), Friston et al. (2000) and Stephan et al. (2007).\n    \"\"\"\n    def __init__(self):\n        \"Constructor\"\n\n        # damped harmonic oscillators, gamma-&gt;spring coefficient, kappa-&gt;damping coefficient\n        # CBF --&gt; gamma from Friston\n        # CMRO2 --&gt; faster --&gt; gamma=gamma_CBF*10 (therefore scaling of I_CMRO2 by (gamma_CMRO2 / gamma_CBF) --&gt; if same input (I_CBF==I_CMRO2) CMRO2 and CBF same steady-state)\n        # critical kappa --&gt; kappa**2-4*gamma = 0 --&gt; kappa=sqrt(4*gamma)\n        # CBF underdamped for undershoot --&gt; kappa = 0.6*sqrt(4*gamma)\n        # CMRO2 critical --&gt; kappa = sqrt(4*gamma)\n        # after CBF and CMRO2 standard balloon model with revised coefficients, parameter values = Friston et al. (2000)\n        parameters = \"\"\"\n            kappa_CBF   = 0.7650920556760059\n            gamma_CBF   = 1/2.46\n            kappa_CMRO2 = 4.032389192727559\n            gamma_CMRO2 = 10/2.46\n            phi_CBF     = 1.0\n            phi_CMRO2   = 1.0\n            E_0         = 0.34\n            tau         = 0.98\n            alpha       = 0.33\n            V_0         = 0.02\n            v_0         = 40.3\n            TE          = 40/1000.\n            epsilon     = 1\n            r_0         = 25\n            tau_out1    = 0\n            tau_out2    = 20\n            second      = 1000\n        \"\"\"\n        equations = \"\"\"\n            # CBF input\n            I_CBF               = sum(I_CBF)                                                   : init=0\n            second*ds_CBF/dt    = phi_CBF * I_CBF - kappa_CBF * s_CBF - gamma_CBF * (f_in - 1) : init=0\n            second*df_in/dt     = s_CBF                                                        : init=1, min=0.01\n\n            # CMRO2 input\n            I_CMRO2             = sum(I_CMRO2)                                                 : init=0\n            second*ds_CMRO2/dt  = phi_CMRO2 * I_CMRO2 * (gamma_CMRO2 / gamma_CBF) - kappa_CMRO2 * s_CMRO2 - gamma_CMRO2 * (r - 1) : init=0\n            second*dr/dt        = s_CMRO2                                                      : init=1, min=0.01\n\n            dv                  = f_in - v**(1 / alpha)\n            tau_out             = if dv&gt;0: tau_out1 else: tau_out2\n            f_out               = v**(1/alpha) + tau_out * dv / (tau + tau_out)              : init=1, min=0.01\n\n            dq/dt               = (r - (q / v) * f_out)  / (second*tau)                        : init=1, min=0.01\n            dv/dt               = dv / (tau + tau_out)  / second                             : init=1, min=0.01\n\n            # Revised coefficients\n            k_1                 = 4.3 * v_0 * E_0 * TE\n            k_2                 = epsilon * r_0 * E_0 * TE\n            k_3                 = 1 - epsilon\n\n            # Non-linear equation\n            BOLD                = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))  : init=0\n        \"\"\"\n        name = \"BOLD model with two inputs\"\n        description = \"BOLD model with two inputs (CBF-driving and CMRO2-driving). Combination of neurovascular coupling of Friston et al. (2000) and non-linear Balloon model with revised coefficients (Buxton et al, 1998, Stephan et al, 2007)\"\n\n        BoldModel.__init__(self, \n            parameters=parameters, \n            equations=equations, \n            inputs=['I_CBF', 'I_CMRO2'],\n            output=\"BOLD\",\n            name=name, \n            description=description\n        )\n</code></pre>"},{"location":"API/BOLD.html#ANNarchy.extensions.bold.PredefinedModels.balloon_two_inputs.__init__","title":"<code>__init__()</code>","text":"<p>Constructor</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/bold/PredefinedModels.py</code> <pre><code>def __init__(self):\n    \"Constructor\"\n\n    # damped harmonic oscillators, gamma-&gt;spring coefficient, kappa-&gt;damping coefficient\n    # CBF --&gt; gamma from Friston\n    # CMRO2 --&gt; faster --&gt; gamma=gamma_CBF*10 (therefore scaling of I_CMRO2 by (gamma_CMRO2 / gamma_CBF) --&gt; if same input (I_CBF==I_CMRO2) CMRO2 and CBF same steady-state)\n    # critical kappa --&gt; kappa**2-4*gamma = 0 --&gt; kappa=sqrt(4*gamma)\n    # CBF underdamped for undershoot --&gt; kappa = 0.6*sqrt(4*gamma)\n    # CMRO2 critical --&gt; kappa = sqrt(4*gamma)\n    # after CBF and CMRO2 standard balloon model with revised coefficients, parameter values = Friston et al. (2000)\n    parameters = \"\"\"\n        kappa_CBF   = 0.7650920556760059\n        gamma_CBF   = 1/2.46\n        kappa_CMRO2 = 4.032389192727559\n        gamma_CMRO2 = 10/2.46\n        phi_CBF     = 1.0\n        phi_CMRO2   = 1.0\n        E_0         = 0.34\n        tau         = 0.98\n        alpha       = 0.33\n        V_0         = 0.02\n        v_0         = 40.3\n        TE          = 40/1000.\n        epsilon     = 1\n        r_0         = 25\n        tau_out1    = 0\n        tau_out2    = 20\n        second      = 1000\n    \"\"\"\n    equations = \"\"\"\n        # CBF input\n        I_CBF               = sum(I_CBF)                                                   : init=0\n        second*ds_CBF/dt    = phi_CBF * I_CBF - kappa_CBF * s_CBF - gamma_CBF * (f_in - 1) : init=0\n        second*df_in/dt     = s_CBF                                                        : init=1, min=0.01\n\n        # CMRO2 input\n        I_CMRO2             = sum(I_CMRO2)                                                 : init=0\n        second*ds_CMRO2/dt  = phi_CMRO2 * I_CMRO2 * (gamma_CMRO2 / gamma_CBF) - kappa_CMRO2 * s_CMRO2 - gamma_CMRO2 * (r - 1) : init=0\n        second*dr/dt        = s_CMRO2                                                      : init=1, min=0.01\n\n        dv                  = f_in - v**(1 / alpha)\n        tau_out             = if dv&gt;0: tau_out1 else: tau_out2\n        f_out               = v**(1/alpha) + tau_out * dv / (tau + tau_out)              : init=1, min=0.01\n\n        dq/dt               = (r - (q / v) * f_out)  / (second*tau)                        : init=1, min=0.01\n        dv/dt               = dv / (tau + tau_out)  / second                             : init=1, min=0.01\n\n        # Revised coefficients\n        k_1                 = 4.3 * v_0 * E_0 * TE\n        k_2                 = epsilon * r_0 * E_0 * TE\n        k_3                 = 1 - epsilon\n\n        # Non-linear equation\n        BOLD                = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))  : init=0\n    \"\"\"\n    name = \"BOLD model with two inputs\"\n    description = \"BOLD model with two inputs (CBF-driving and CMRO2-driving). Combination of neurovascular coupling of Friston et al. (2000) and non-linear Balloon model with revised coefficients (Buxton et al, 1998, Stephan et al, 2007)\"\n\n    BoldModel.__init__(self, \n        parameters=parameters, \n        equations=equations, \n        inputs=['I_CBF', 'I_CMRO2'],\n        output=\"BOLD\",\n        name=name, \n        description=description\n    )\n</code></pre>"},{"location":"API/BOLD.html#references","title":"References","text":"<p>Buxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855\u2013864. doi:10.1002/mrm.1910390602</p> <p>Friston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466\u2013477</p> <p>Buxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220\u2013S233. doi:10.1016/j.neuroimage.2004.07.013</p> <p>Stephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387\u2013401. doi:10.1016/j.neuroimage.2007.07.040</p> <p>Maith et al. (2021) A computational model-based analysis of basal ganglia pathway changes in Parkinson\u2019s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278\u2013 2295. doi:10.1111/ejn.14868 </p> <p>Maith et al. (2022) BOLD monitoring in the neural simulator ANNarchy. Frontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966.</p>"},{"location":"API/Convolution.html","title":"Convolution and Pooling","text":"<p>Convolution and pooling operations are provided in the module <code>ANNarchy.extensions.convolution</code>. They must be explicitly imported:</p> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.convolution import *\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolution","title":"<code>ANNarchy.extensions.convolution.Convolution</code>","text":"<p>             Bases: <code>Projection</code></p> <p>Performs a convolution of a weight kernel on the pre-synaptic population.</p> <p>Despite its name, the operation performed is actually a cross-correlation, as is usual in computer vision and convolutional neural networks:</p> \\[g(x) = \\sum_{k=-n}^n h(k) \\, f(x + k)\\] <p>The convolution operation benefits from giving a multi-dimensional geometry to the populations and filters, for example in 2D:</p> <pre><code>inp = Population(geometry=(100, 100), neuron=Neuron(parameters=\"r = 0.0\"))\npop = Population(geometry=(100, 100), neuron=Neuron(equations=\"r = sum(exc)\"))\nproj = Convolution(inp, pop, 'exc')\nproj.connect_filter(\n    [\n        [-1., 0., 1.],\n        [-1., 0., 1.],\n        [-1., 0., 1.]\n    ])\n</code></pre> <p>The maximum number of dimensions for populations and filters is 4, an error is thrown otherwise.</p> <p>Depending on the number of dimensions of the pre- and post-synaptic populations, as well as of the kernel, the convolution is implemented differentely.</p> <p>Method connect_filter()</p> <ul> <li> <p>If the pre- and post-populations have the same dimension as the kernel, the convolution is regular. Example:</p> <p>(100, 100) * (3, 3) -&gt; (100, 100)</p> </li> <li> <p>If the post-population has one dimension less than the pre-synaptic one, the last dimension of the kernel must match the last one of the pre-synaptic population. Example:</p> <p>(100, 100, 3) * (3, 3, 3) -&gt; (100, 100)</p> </li> <li> <p>If the kernel has less dimensions than the two populations, the number of neurons in the last dimension of the populations must be the same. The convolution will be calculated for each feature map in the last dimension. In this case, you must set <code>keep_last_dimension</code> to <code>True</code>. Example:</p> <p>(100, 100, 16) * (3, 3) -&gt; (100, 100, 16)</p> </li> </ul> <p>Method connect_filters()</p> <ul> <li> <p>If the kernel has more dimensions than the pre-synaptic population, this means a bank of different filters will be applied on the pre-synaptic population (like a convolutional layer in a CNN). Attention: the first index of <code>weights</code> corresponds to the different filters, while the result will be accessible in the last dimension of the post-synaptic population. You must set the <code>multiple</code> argument to True. Example:</p> <p>(100, 100) * (16, 3, 3) -&gt; (100, 100, 16)</p> </li> </ul> <p>The convolution always uses padding for elements that would be outside the array (no equivalent of <code>valid</code> in tensorflow). It is 0.0 by default, but can be changed using the <code>padding</code> argument. Setting <code>padding</code> to the string <code>border</code> will repeat the value of the border elements.</p> <p>Sub-sampling will be automatically performed according to the populations' geometry. If these geometries do not match, an error will be thrown. Example:</p> <pre><code>(100, 100) * (3, 3) -&gt; (50, 50)\n</code></pre> <p>You can redefine the sub-sampling by providing a list <code>subsampling</code> as argument, defining for each post-synaptic neuron the coordinates of the pre-synaptic neuron which will be the center of the filter/kernel.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Convolve.py</code> <pre><code>class Convolution(Projection):\n\"\"\"\n    Performs a convolution of a weight kernel on the pre-synaptic population.\n\n    Despite its name, the operation performed is actually a cross-correlation, as is usual in computer vision and convolutional neural networks:\n\n    $$g(x) = \\sum_{k=-n}^n h(k) \\, f(x + k)$$\n\n    The convolution operation benefits from giving a multi-dimensional geometry to the populations and filters, for example in 2D:\n\n    ```python\n    inp = Population(geometry=(100, 100), neuron=Neuron(parameters=\"r = 0.0\"))\n    pop = Population(geometry=(100, 100), neuron=Neuron(equations=\"r = sum(exc)\"))\n    proj = Convolution(inp, pop, 'exc')\n    proj.connect_filter(\n        [\n            [-1., 0., 1.],\n            [-1., 0., 1.],\n            [-1., 0., 1.]\n        ])\n    ```\n\n    The maximum number of dimensions for populations and filters is 4, an error is thrown otherwise.\n\n    Depending on the number of dimensions of the pre- and post-synaptic populations, as well as of the kernel, the convolution is implemented differentely.\n\n    **Method connect_filter()**\n\n    * If the pre- and post-populations have the same dimension as the kernel, the convolution is regular. Example:\n\n        (100, 100) * (3, 3) -&gt; (100, 100)\n\n    * If the post-population has one dimension less than the pre-synaptic one, the last dimension of the kernel must match the last one of the pre-synaptic population. Example:\n\n        (100, 100, 3) * (3, 3, 3) -&gt; (100, 100)\n\n    * If the kernel has less dimensions than the two populations, the number of neurons in the last dimension of the populations must be the same. The convolution will be calculated for each feature map in the last dimension. In this case, you must set ``keep_last_dimension`` to ``True``. Example:\n\n        (100, 100, 16) * (3, 3) -&gt; (100, 100, 16)\n\n    **Method connect_filters()**\n\n    * If the kernel has more dimensions than the pre-synaptic population, this means a bank of different filters will be applied on the pre-synaptic population (like a convolutional layer in a CNN). Attention: the first index of ``weights`` corresponds to the different filters, while the result will be accessible in the last dimension of the post-synaptic population. You must set the ``multiple`` argument to True. Example:\n\n        (100, 100) * (16, 3, 3) -&gt; (100, 100, 16)\n\n    The convolution **always** uses padding for elements that would be outside the array (no equivalent of ``valid`` in tensorflow). It is 0.0 by default, but can be changed using the ``padding`` argument. Setting ``padding`` to the string ``border`` will repeat the value of the border elements.\n\n    Sub-sampling will be automatically performed according to the populations' geometry. If these geometries do not match, an error will be thrown. Example:\n\n        (100, 100) * (3, 3) -&gt; (50, 50)\n\n    You can redefine the sub-sampling by providing a list ``subsampling`` as argument, defining for each post-synaptic neuron the coordinates of the pre-synaptic neuron which will be the center of the filter/kernel.\n    \"\"\"\n\n    def __init__(self, pre, post, target, psp=\"pre.r * w\", operation=\"sum\", name=None, copied=False):\n\"\"\"\n        :param pre: pre-synaptic population (either its name or a ``Population`` object).\n        :param post: post-synaptic population (either its name or a ``Population`` object).\n        :param target: type of the connection\n        :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``).\n        :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum).\n        \"\"\"\n        # Sanity check\n        #if not pre.neuron_type.type == 'rate':\n        #    Global._error('Convolution: only implemented for rate-coded populations.')\n\n        # Create the description, but it will not be used for generation\n        Projection.__init__(\n            self,\n            pre,\n            post,\n            target,\n            synapse=SharedSynapse(psp=psp, operation=operation, name=\"Convolution operation\", description=\"Convoluted kernel over the pre-synaptic population.\"),\n            name=name,\n            copied=copied\n        )\n\n        # Disable saving\n        self._saveable = False\n\n        # For copy\n        self._used_single_filter = False\n        self._used_bank_of_filters = False\n        self.operation = operation\n\n    @property\n    def weights(self):\n        if not self.initialized:\n            return self.init[\"weights\"]\n        else:\n            return np.array(self.cyInstance.get_w())\n\n    @weights.setter\n    def weights(self, value):\n        if not self.initialized:\n            self.init[\"weights\"]=value\n        else:\n            if self.dim_kernel != value.ndim:\n                raise AttributeError(\"Mismatch between filter dimensions\")\n\n            self.cyInstance.set_w(value)\n\n    def connect_filter(self, weights, delays=0.0, keep_last_dimension=False, padding=0.0, subsampling=None):\n\"\"\"\n        Applies a single filter on the pre-synaptic population.\n\n        :param weights: numpy array or list of lists representing the matrix of weights for the filter.\n        :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons.\n        :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.\n        :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0.\n        :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.\n        \"\"\"\n\n        # Process the weights\n        self.weights = np.array(weights)\n\n        # Process the delays\n        self.delays = float(delays)\n        if not isinstance(delays, (int, float)):\n            Global._error('Convolutions can only have constant delays.')\n\n        self.subsampling = subsampling\n        self.keep_last_dimension = keep_last_dimension\n        self.padding = padding\n        self.multiple = False\n\n        # Check dimensions of populations and weight matrix\n        self.dim_kernel = self.weights.ndim\n        self.dim_pre = self.pre.dimension\n        self.dim_post = self.post.dimension\n\n        if self.dim_post &gt; 4:\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: Too many dimensions for the post-synaptic population (maximum 4).')\n\n        if self.dim_pre &gt; 4:\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: Too many dimensions for the pre-synaptic population (maximum 4).')\n\n        if self.dim_kernel &gt; 5  or (not self.multiple and self.dim_kernel &gt; 4):\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: Too many dimensions for the kernel (maximum 4).')\n\n        # Check if the last axes match for parallel convolution (e.g. 3-2-3)\n        if self.dim_kernel &lt; self.dim_pre:\n            if not self.keep_last_dimension:\n                print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n                Global._error('Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.')\n\n            if self.pre.geometry[-1] != self.post.geometry[-1]:\n                print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n                Global._error('Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.')\n\n        # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less.\n        if self.dim_post &lt; self.dim_pre: # OK, but check the last dimension of the kernel has the same size as the post-population\n            if self.weights.shape[-1] != self.pre.geometry[-1]:\n                print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n                Global._error('Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.')\n\n        # Check if it is a bank of filters\n        if self.dim_kernel &gt; self.dim_pre:\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: If the kernel has more dimensions than the pre-synaptic population, you need to use the connect_filters() method.')\n\n\n        # Generate the pre-synaptic coordinates\n        self._generate_pre_coordinates()\n\n        # Finish building the synapses\n        self._create()\n\n        # For copy\n        self._used_single_filter = True\n\n        return self\n\n\n    def connect_filters(self, weights, delays=0.0, keep_last_dimension=False, padding=0.0, subsampling=None):\n\"\"\"\n        Applies a set of different filters on the pre-synaptic population.\n\n        The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters.\n\n\n        :param weights: numpy array or list of lists representing the matrix of weights for the filter.\n        :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons.\n        :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.\n        :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0.\n        :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.\n        \"\"\"\n\n        # Process the weights\n        self.weights = np.array(weights)\n\n        # Process the delays\n        self.delays = float(delays)\n        if not isinstance(delays, (int, float)):\n            Global._error('Convolutions can only have constant delays.')\n\n        self.subsampling = subsampling\n        self.keep_last_dimension = keep_last_dimension\n        self.padding = padding\n        self.multiple = True\n\n        # Check dimensions of populations and weight matrix\n        self.dim_kernel = self.weights.ndim\n        self.dim_pre = self.pre.dimension\n        self.dim_post = self.post.dimension\n\n\n        if self.dim_post &gt; 4:\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: Too many dimensions for the post-synaptic population (maximum 4).')\n\n        if self.dim_pre &gt; 4:\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: Too many dimensions for the pre-synaptic population (maximum 4).')\n\n        if self.dim_kernel &gt; 5  or (not self.multiple and self.dim_kernel &gt; 4):\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: Too many dimensions for the kernel (maximum 4).')\n\n        # Check if the last axes match for parallel convolution (e.g. 3-2-3)\n        if self.dim_kernel &lt; self.dim_pre:\n            if not self.keep_last_dimension:\n                print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n                Global._error('Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.')\n\n            if self.pre.geometry[-1] != self.post.geometry[-1]:\n                print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n                Global._error('Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.')\n\n        # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less.\n        if self.dim_post &lt; self.dim_pre: # OK, but check the last dimension of the kernel has the same size as the post-population\n            if self.weights.shape[-1] != self.pre.geometry[-1]:\n                print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n                Global._error('Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.')\n\n        # The last dimension of the post population must correspond to the number of filters\n        if self.weights.shape[0] != self.post.geometry[-1]:\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: For multiple filters, the last dimension of the post-synaptic population must have as many neurons as there are filters.')\n\n        # Generate the pre-synaptic coordinates\n        self._generate_pre_coordinates_bank()\n\n        # Finish building the synapses\n        self._create()\n\n        # For copy\n        self._used_bank_of_filters = True\n\n        return self\n\n    def _copy(self, pre, post):\n        \"Returns a copy of the projection when creating networks.  Internal use only.\"\n        copied_proj = Convolution(pre=pre, post=post, target=self.target,\n                                  psp=self.synapse_type.psp, operation=self.operation,\n                                  name=self.name, copied=True)\n\n        copied_proj.delays = self.delays\n        copied_proj.weights = self.weights\n\n        copied_proj.subsampling = self.subsampling\n        copied_proj.keep_last_dimension = self.keep_last_dimension\n        copied_proj.padding = self.padding\n        copied_proj.multiple = self.multiple\n        copied_proj.dim_kernel = self.weights.ndim\n        copied_proj.dim_pre = self.pre.dimension\n        copied_proj.dim_post = self.post.dimension\n\n        if self._used_single_filter:\n            copied_proj._generate_pre_coordinates()\n        elif self._used_bank_of_filters:\n            copied_proj._generate_pre_coordinates_bank()\n        else:\n            raise ValueError(\"Either use single filter or bank of filter must be True! (Missing connect?)\")\n\n        copied_proj._create()\n        copied_proj._connection_method = self._connection_method\n        copied_proj._connection_args = self._connection_args\n        copied_proj._connection_delay = self._connection_delay\n        copied_proj._storage_format = self._storage_format\n        return copied_proj\n\n    def _create(self):\n        # create fake LIL object, just for compilation.\n        try:\n            from ANNarchy.core.cython_ext.Connector import LILConnectivity\n        except Exception as e:\n            Global._print(e)\n            Global._error('ANNarchy was not successfully installed.')\n\n        lil = LILConnectivity()\n        lil.max_delay = self.delays\n        lil.uniform_delay = self.delays\n        self.connector_name = \"Convolution\"\n        self.connector_description = \"Convolution\"\n        self._store_connectivity(self._load_from_lil, (lil, ), self.delays, storage_format=\"lil\", storage_order=\"post_to_pre\")\n\n    ################################\n    ### Create connection pattern\n    ################################\n    def _connect(self, module):\n\"\"\"\n        Builds up dendrites either from list or dictionary. Called by instantiate().\n        \"\"\"\n        if not self._connection_method:\n            Global._error('Convolution: The projection between ' + self.pre.name + ' and ' + self.post.name + ' is declared but not connected.')\n\n        # Create the Cython instance\n        proj = getattr(module, 'proj'+str(self.id)+'_wrapper')\n        self.cyInstance = proj(self.weights, self.pre_coordinates)\n\n        # Set delays after instantiation\n        if self.delays &gt; 0.0:\n            self.cyInstance.set_delay(self.delays/Global.config['dt'])\n\n        return True\n\n    def _generate_pre_coordinates(self):\n        \" Returns a list for each post neuron of the corresponding center coordinates.\"\n\n        # Check if the list is already defined:\n        if self.subsampling:\n            try:\n                shape = np.array(self.subsampling).shape\n            except:\n                Global._error('Convolution: The sub-sampling list must have', self.post.size, 'elements of size', self.pre.dimension)\n                return\n            if shape != (self.post.size, self.pre.dimension):\n                Global._error('Convolution: The sub-sampling list must have', self.post.size, 'elements of size', self.pre.dimension)\n                return\n            self.pre_coordinates = self.subsampling\n            return\n\n        # Otherwise create it, possibly with sub-sampling\n        coords = [[] for i in range(self.post.size)]\n\n        # Compute pre-indices\n        idx_range= []\n        for dim in range(self.dim_pre):\n            if dim &lt; self.dim_post:\n                pre_size = int(self.pre.geometry[dim])\n                post_size = int(self.post.geometry[dim])\n                sample = int(pre_size/post_size)\n                if post_size * sample != pre_size:\n                    Global._error('Convolution: The pre-synaptic dimensions must be a multiple of the post-synaptic ones for down-sampling to work.')\n\n                idx_range.append([int((sample-1)/2) + sample * i for i in range(post_size)])\n            else: # extra dimension\n                if self.keep_last_dimension:\n                    idx_range.append(range(self.post.geometry[dim]))\n                else:\n                    idx_range.append([self._center_filter(self.weights.shape[dim])])\n\n        # Generates coordinates TODO: Find a more robust way!\n        if self.dim_pre == 1 :\n            rk = 0\n            for i in idx_range[0]:\n                coords[rk] = [i]\n                rk += 1\n        elif self.dim_pre == 2 :\n            rk = 0\n            for i in idx_range[0]:\n                for j in idx_range[1]:\n                    coords[rk] = [i, j]\n                    rk += 1\n        elif self.dim_pre == 3 :\n            rk = 0\n            for i in idx_range[0]:\n                for j in idx_range[1]:\n                    for k in idx_range[2]:\n                        coords[rk] = [i, j, k]\n                        rk += 1\n        elif self.dim_pre == 4 :\n            rk = 0\n            for i in idx_range[0]:\n                for j in idx_range[1]:\n                    for k in idx_range[2]:\n                        for l in idx_range[3]:\n                            coords[rk] = [i, j, k, l]\n                            rk += 1\n\n        # Save the result\n        self.pre_coordinates = coords\n\n    def _generate_pre_coordinates_bank(self):\n        \" Returns a list for each post neuron of the corresponding center coordinates, when the filter is a bank.\"\n\n        self.nb_filters = self.weights.shape[0]\n        self.dim_single_filter = self.weights.shape[1:]\n\n        # Check if the list is already defined:\n        if self.subsampling:\n            try:\n                shape = np.array(self.subsampling).shape\n            except:\n                Global._error('Convolution: The sub-sampling list must have', self.post.size / self.post.geometry[-1], 'elements of size', self.pre.dimension)\n                return\n            if shape != (self.post.size/ self.post.geometry[-1], self.pre.dimension):\n                Global._error('Convolution: The sub-sampling list must have', self.post.size/ self.post.geometry[-1], 'elements of size', self.pre.dimension)\n                return\n            self.pre_coordinates = [c + [d] for c in self.subsampling  for d  in range(self.nb_filters)]\n            return\n\n        # Otherwise create it, possibly with sub-sampling\n        coords = [[] for i in range(self.post.size)]\n\n        # Compute pre-indices\n        idx_range= []\n        for dim in range(self.dim_pre):\n            if dim &lt; self.dim_post -1:\n                pre_size = self.pre.geometry[dim]\n                post_size = self.post.geometry[dim]\n                sample = int(pre_size/post_size)\n                if post_size * sample != pre_size:\n                    Global._error('Convolution: The pre-synaptic dimensions must be a multiple of the post-synaptic ones for down-sampling to work.')\n\n                idx_range.append([int((sample-1)/2) + sample * i for i in range(post_size)])\n            else: # extra dimension\n                if self.keep_last_dimension:\n                    idx_range.append(range(self.post.geometry[dim]))\n                else:\n                    idx_range.append([self._center_filter(self.weights.shape[dim+1])])\n\n\n        # Generates coordinates TODO: Find a more robust way!\n        if self.dim_pre == 1 :\n            rk = 0\n            for i in idx_range[0]:\n                for d in range(self.nb_filters):\n                    coords[rk] = [i, d]\n                    rk += 1\n        elif self.dim_pre == 2 :\n            rk = 0\n            for i in idx_range[0]:\n                for j in idx_range[1]:\n                    for d in range(self.nb_filters):\n                        coords[rk] = [i, j, d ]\n                        rk += 1\n        elif self.dim_pre == 3 :\n            rk = 0\n            for i in idx_range[0]:\n                for j in idx_range[1]:\n                    for k in idx_range[2]:\n                        for d in range(self.nb_filters):\n                            coords[rk] = [i, j, k, d]\n                            rk += 1\n        elif self.dim_pre == 4 :\n            rk = 0\n            for i in idx_range[0]:\n                for j in idx_range[1]:\n                    for k in idx_range[2]:\n                        for l in idx_range[3]:\n                            for d in range(self.nb_filters):\n                                coords[rk] = [i, j, k, l, d]\n                                rk += 1\n\n        # Save the result\n        self.pre_coordinates = coords\n\n    ################################\n    # Code generation\n    ################################\n    def _generate(self):\n\"\"\"\n        Overrides default code generation. This function is called during the code generation procedure.\n        \"\"\"\n        # Filter definition\n        filter_definition, filter_pyx_definition = self._filter_definition()\n\n        # Convolve_code\n        if not self.multiple:\n            convolve_code, sum_code = self._generate_convolve_code()\n        else:\n            convolve_code, sum_code = self._generate_bank_code()\n\n        if Global._check_paradigm(\"openmp\"):\n            self._generate_omp(filter_definition, filter_pyx_definition, convolve_code, sum_code)\n        elif Global._check_paradigm(\"cuda\"):\n            raise Global.ANNarchyException(\"Convolution is not available on CUDA devices yet.\", True)\n        else:\n            raise NotImplementedError\n\n    def _generate_omp(self, filter_definition, filter_pyx_definition, convolve_code, sum_code, kernel=True):\n\"\"\"\n        OpenMP code generation.\n        \"\"\"\n        # Basic ids\n        base_ids = {\n            'id_proj': self.id,\n            'size_post': self.post.size,\n            'float_prec': Global.config['precision']\n        }\n\n        # Fill the basic definitions\n        conv_dict = deepcopy(convole_template_omp)\n        for key, value in conv_dict.items():\n            value = value % base_ids\n            conv_dict[key] = value\n        self._specific_template.update(conv_dict)\n\n        # Kernel-based method: specify w with the correct dimension\n        if kernel:\n            self._specific_template['declare_parameters_variables'] = tabify(filter_definition.strip(), 1)\n            self._specific_template['export_parameters_variables'] = \"\"\n            self._specific_template['access_parameters_variables'] = \"\"\"\n    // Local parameter w\n%(type_w)s get_w() { return w; }\n    void set_w(%(type_w)s value) { w = value; }\n\"\"\" % {'type_w': filter_definition.replace(' w;', '')}\n            self._specific_template['export_connectivity'] += \"\"\"\n        # Local variable w\n%(type_w)s get_w()\n        void set_w(%(type_w)s)\n\"\"\" % {'type_w': filter_pyx_definition.replace(' w', '')}\n            self._specific_template['wrapper_init_connectivity'] += \"\"\"\n        proj%(id_proj)s.set_w(weights)\n\"\"\" % {'id_proj': self.id}\n\n            self._specific_template['wrapper_access_connectivity'] += \"\"\"\n    # Local variable w\n    def get_w(self):\n        return proj%(id_proj)s.get_w()\n    def set_w(self, value):\n        proj%(id_proj)s.set_w( value )\n    def get_dendrite_w(self, int rank):\n        return proj%(id_proj)s.get_w()\n    def set_dendrite_w(self, int rank, value):\n        proj%(id_proj)s.set_w(value)\n    def get_synapse_w(self, int rank_post, int rank_pre):\n        return 0.0\n    def set_synapse_w(self, int rank_post, int rank_pre, %(float_prec)s value):\n        pass\n\"\"\" % {'id_proj': self.id, 'float_prec': Global.config['precision']}\n\n        # Override the monitor to avoid recording the weights\n        self._specific_template['monitor_class'] = \"\"\n\n        self._specific_template['monitor_export'] = \"\"\n\n        self._specific_template['monitor_wrapper'] = \"\"\n\n        # OMP code\n        omp_code = \"\"\n        if Global.config['num_threads'] &gt; 1:\n            omp_code = \"\"\"\n        #pragma omp for private(sum, rk_pre, coord) %(psp_schedule)s\"\"\" % {'psp_schedule': \"\" if not 'psp_schedule' in self._omp_config.keys() else self._omp_config['psp_schedule']}\n\n        # HD ( 16.10.2015 ):\n        # pre-load delayed firing rate in a local array, so we\n        # prevent multiple accesses to pop%(id_pre)s._delayed_r[delay-1]\n        # wheareas delay is set available as variable\n        # TODO HD: wouldn't it be much better to reduce delay globaly, instead of the substraction here???\n        if self.delays &gt; Global.config['dt']:\n            pre_load_r = \"\"\"\n        // pre-load delayed firing rate\n        auto delayed_r = pop%(id_pre)s._delayed_r[delay-1];\n        \"\"\"% {'id_pre': self.pre.id}\n        else:\n            pre_load_r = \"\"\n\n        # Target variable depends on neuron type\n        target_code = \"_sum_%(target)s\" if self.post.neuron_type.type==\"rate\" else \"g_%(target)s\"\n        target_code %= {'target': self.target}\n\n        # Compute sum\n        wsum =  \"\"\"\n        if ( _transmission &amp;&amp; pop%(id_pre)s._active ) {\n            int* coord;\n\"\"\" + pre_load_r + \"\"\"\n%(omp_code)s\n            for(int i = 0; i &lt; %(size_post)s; i++){\n                coord = pre_coords[i].data();\n\n                // perform the convolution\n\"\"\" + tabify(convolve_code, 1) + \"\"\"\n\n                // store result\n                pop%(id_post)s.%(target)s[i] += \"\"\" + sum_code + \"\"\";\n            } // for\n        } // if\n\"\"\"\n\n        self._specific_template['psp_code'] = wsum % \\\n        {   'id_proj': self.id,\n            'target': target_code,\n            'id_pre': self.pre.id, 'name_pre': self.pre.name, 'size_pre': self.pre.size,\n            'id_post': self.post.id, 'name_post': self.post.name, 'size_post': self.post.size,\n            'omp_code': omp_code,\n            'convolve_code': convolve_code\n        }\n\n        self._specific_template['size_in_bytes'] = \"\"\"\n        // post-ranks\n        size_in_bytes += sizeof(std::vector&lt;int&gt;);\n        size_in_bytes += post_rank.capacity() * sizeof(int);\n\n        // pre-coords\n        size_in_bytes += sizeof(std::vector&lt;std::vector&lt;int&gt;&gt;);\n        size_in_bytes += pre_coords.capacity() * sizeof(std::vector&lt;int&gt;);\n        for (auto it = pre_coords.begin(); it != pre_coords.end(); it++) {\n            size_in_bytes += it-&gt;capacity() * sizeof(int);\n        }\n\n        // filter\n        // TODO:\n\"\"\"\n        self._specific_template['clear'] = \"\"\"\n        // post-ranks\n        post_rank.clear();\n        post_rank.shrink_to_fit();\n\n        // pre-coords\n        for (auto it = pre_coords.begin(); it != pre_coords.end(); it++) {\n            it-&gt;clear();\n            it-&gt;shrink_to_fit();\n        }\n        pre_coords.clear();\n        pre_coords.shrink_to_fit();\n\n        // filter\n        // TODO:\n\"\"\"\n\n    ################################\n    ### Utilities\n    ################################\n    def _center_filter(self, i):\n        return int(i/2) if i%2==1 else int(i/2)-1\n\n    def _filter_definition(self):\n        dim = self.dim_kernel\n        cpp = Global.config['precision']\n        pyx = Global.config['precision']\n        for d in range(dim):\n            cpp = 'std::vector&lt; ' + cpp + ' &gt;'\n            pyx = 'vector[' + pyx + ']'\n        cpp += ' w;'\n        pyx += ' w'\n        return cpp, pyx\n\n    def _coordinates_to_rank(self, name, geometry):\n\n        dim = len(geometry)\n\n        txt = \"\"\n\n        for d in range(dim):\n            if txt == \"\" : # first coordinate is special\n                txt = indices[0] + \"_\" + name\n            else:\n                txt = str(geometry[d]) + '*(' + txt + ') + ' + indices[d]  + '_' + name\n\n        return txt\n\n    def _generate_convolve_code(self):\n\n        # Operation to be performed: sum, max, min, mean\n        operation = self.synapse_type.operation\n\n        # Main code\n        code = tabify(\"sum = 0.0;\\n\", 3)\n\n        # Generate for loops\n        for dim in range(self.dim_kernel):\n            if dim == self.dim_kernel-1:\n                inner_idx = \"\"\n                for i in range(self.dim_kernel-1):\n                    inner_idx += \"[\"+indices[i]+\"_w]\"\n                code += \"auto inner_line = w\"+inner_idx+\".data();\\n\"\n\n            code += tabify(\"\"\"\n            for(int %(index)s_w = 0; %(index)s_w &lt; %(size)s;%(index)s_w++) {\n            \"\"\" % { 'index': indices[dim], 'size': self.weights.shape[dim]}, dim)\n\n            # Compute indices\n            if dim &lt; self.dim_kernel:\n                code += tabify(\n\"\"\"int %(index)s_pre = coord[%(dim)s] %(operator)s (%(index)s_w - %(center)s);\"\"\" %\n                        {\n                            'id_proj': self.id,\n                            'index': indices[dim],\n                            'dim': dim,\n                            'operator': '+' ,\n                            'center': self._center_filter(self.weights.shape[dim])\n                        }, 1)\n            else:\n                code += tabify(\n\"\"\"int %(index)s_pre = coord[%(dim)s];\"\"\" %\n                        {\n                            'id_proj': self.id,\n                            'index': indices[dim],\n                            'dim': dim\n                        }, 1)\n\n            # Check indices\n            if operation in ['sum', 'mean']:\n                if isinstance(self.padding, str): # 'border'\n                        code += tabify(\"\"\"\n                if (%(index)s_pre &lt; 0) %(index)s_pre = 0 ;\n                if (%(index)s_pre &gt; %(max_size)s) %(index)s_pre = %(max_size)s ;\n                \"\"\" % { 'index': indices[dim], 'dim': dim, 'max_size': self.pre.geometry[dim] -1}, dim)\n                else:\n                    code += tabify(\"\"\"\n                if ((%(index)s_pre &lt; 0) || (%(index)s_pre &gt; %(max_size)s)){\n                    sum += %(padding)s;\n                    continue;\n                }\n                \"\"\" % { 'index': indices[dim], 'padding': self.padding, 'max_size': self.pre.geometry[dim] -1}, dim)\n\n            else: # min, max\n                code += \"\"\"\n                if ((%(index)s_pre &lt; 0) || (%(index)s_pre &gt; %(max_size)s)) {\n                    continue;\n                }\n                \"\"\" % { 'index': indices[dim], 'max_size': self.pre.geometry[dim] -1}\n\n        # if True, we need to take the last dimension from coords\n        if self.keep_last_dimension:\n            id_dict = {\n                'index': indices[self.dim_kernel],\n                'dim': self.dim_kernel\n            }\n            code += \"int %(index)s_pre = coord[%(dim)s];\" % id_dict\n\n        # Compute pre-synaptic rank\n        code += tabify(\"\"\"\n                rk_pre = %(value)s;\"\"\" % {'value': self._coordinates_to_rank('pre', self.pre.geometry)}, dim)\n\n        # Compute the increment\n        index = \"\"\n        for dim in range(self.dim_kernel):\n            index += '[' + indices[dim] + '_w]'\n\n        increment = self.synapse_type.description['psp']['cpp'] % {\n            'id_pre': self.pre.id,\n            'id_post': self.post.id,\n            'local_index': index,\n            'global_index': '[i]',\n            'pre_index': '[rk_pre]',\n            'post_index': '[rk_post]',\n            'pre_prefix': 'pop'+str(self.pre.id)+'.',\n            'post_prefix': 'pop'+str(self.post.id)+'.'\n        }\n\n        # Delays\n        if self.delays &gt; Global.config['dt']:\n            increment = increment.replace(\n                'pop%(id_pre)s.r[rk_pre]' % {'id_pre': self.pre.id},\n                'delayed_r[rk_pre]'\n            )\n\n        # Apply the operation\n        if operation == \"sum\":\n            if self.dim_kernel == 1:\n                code += tabify(\"\"\"\n                sum += %(increment)s\"\"\" % {'increment': increment}, dim)\n            else:\n                code += tabify(\"\"\"\n                sum += %(increment)s\"\"\" % {'increment': increment.replace('w'+inner_idx, 'inner_line')}, dim)\n        elif operation == \"max\":\n            code += tabify(\"\"\"\n%(float_prec)s _psp = %(increment)s\n                if(_psp &gt; sum) sum = _psp;\"\"\" % {'increment': increment, 'float_prec': Global.config['precision']}, dim)\n        elif operation == \"min\":\n            code += tabify(\"\"\"\n%(float_prec)s _psp = %(increment)s\n                if(_psp &lt; sum) sum = _psp;\"\"\" % {'increment': increment, 'float_prec': Global.config['precision']}, dim)\n        elif operation == \"mean\":\n            code += tabify(\"\"\"\n                sum += %(increment)s\"\"\" % {'increment': increment}, dim)\n        else:\n            Global._error('Convolution: Operation', operation, 'is not implemented yet for shared projections.')\n\n        # Close for loops\n        for dim in range(self.dim_kernel):\n            code += tabify(\"\"\"\n            }\"\"\", self.dim_kernel-1-dim)\n\n        impl_code = code % {'id_proj': self.id,\n            'target': self.target,\n            'id_pre': self.pre.id,\n            'name_pre': self.pre.name,\n            'size_pre': self.pre.size,\n            'id_post': self.post.id,\n            'name_post': self.post.name,\n            'size_post': self.post.size\n          }\n\n        # sum code\n        self.weights.size\n        if operation == \"mean\":\n            sum_code = \"\"\"sum/%(filter_size)s\"\"\" % {'filter_size': self.weights.size}\n        else:\n            sum_code = \"sum\"\n\n        return impl_code, sum_code\n\n    def _generate_bank_code(self):\n\n        # Operation to be performed: sum, max, min, mean\n        operation = self.synapse_type.operation\n\n        # Main code\n        code = tabify(\"sum = 0.0;\\n\", 3)\n\n        # Generate for loops\n        for dim in range(self.dim_kernel-1):\n            code += tabify(\"\"\"\n            for(int %(index)s_w = 0; %(index)s_w &lt; %(size)s;%(index)s_w++) {\n            \"\"\" % { 'index': indices[dim], 'size': self.weights.shape[dim+1]}, dim)\n\n            # Compute indices\n            if dim &lt; self.dim_kernel:\n                code += tabify(\n\"\"\"int %(index)s_pre = coord[%(dim)s] %(operator)s (%(index)s_w - %(center)s);\"\"\" %\n                    {\n                        'id_proj': self.id,\n                        'index': indices[dim],\n                        'dim': dim,\n                        'operator': '+',\n                        'center': self._center_filter(self.weights.shape[dim+1])\n                    }, 1)\n            else:\n                code += tabify(\n\"\"\"int %(index)s_pre = coord[%(dim)s];\"\"\" %\n                    {\n                        'id_proj': self.id,\n                        'index': indices[dim],\n                        'dim': dim\n                    }, 1)\n\n            # Check indices\n            if operation in ['sum', 'mean']:\n                if isinstance(self.padding, str): # 'border'\n                    code += tabify(\"\"\"\n            if (%(index)s_pre &lt; 0) %(index)s_pre = 0 ;\n            if (%(index)s_pre &gt; %(max_size)s) %(index)s_pre = %(max_size)s ;\n            \"\"\" % { 'index': indices[dim], 'dim': dim, 'max_size': self.pre.geometry[dim] -1}, 1+dim)\n                else:\n                    code += tabify(\"\"\"\n            if ((%(index)s_pre &lt; 0) || (%(index)s_pre &gt; %(max_size)s)) {\n                sum += %(padding)s;\n                continue;\n            }\n            \"\"\" % { 'index': indices[dim], 'padding': self.padding, 'max_size': self.pre.geometry[dim] -1}, 1+dim)\n\n            else: # min, max\n                code += tabify(\"\"\"\n            if ((%(index)s_pre &lt; 0) || (%(index)s_pre &gt; %(max_size)s)){\n                continue;\n            }\n            \"\"\" % { 'index': indices[dim], 'max_size': self.pre.geometry[dim] -1}, 1+dim)\n\n        # Compute pre-synaptic rank\n        code +=tabify(\"\"\"\n            rk_pre = %(value)s;\"\"\" % {'value': self._coordinates_to_rank('pre', self.pre.geometry)}, 1+dim)\n\n        # Compute the increment\n        index = \"[coord[\"+str(self.dim_pre)+\"]]\"\n        for dim in range(self.dim_kernel-1):\n            index += '[' + indices[dim] + '_w]'\n\n        increment = self.synapse_type.description['psp']['cpp'] % {\n            'id_pre': self.pre.id,\n            'id_post': self.post.id,\n            'local_index': index,\n            'global_index': '[i]',\n            'pre_index': '[rk_pre]',\n            'post_index': '[rk_post]',\n            'pre_prefix': 'pop'+str(self.pre.id)+'.',\n            'post_prefix': 'pop'+str(self.post.id)+'.'}\n\n        # Delays\n        if self.delays &gt; Global.config['dt']:\n            increment = increment.replace(\n                'pop%(id_pre)s.r[rk_pre]' % {'id_pre': self.pre.id},\n                'delayed_r[rk_pre]'\n            )\n\n        # Apply the operation\n        if operation == \"sum\":\n            code += tabify(\"\"\"\n            sum += %(increment)s\"\"\" % {'increment': increment}, 1+dim)\n        elif operation == \"max\":\n            code += tabify(\"\"\"\n%(float_prec)s _psp = %(increment)s\n            if(_psp &gt; sum) sum = _psp;\"\"\" % {'increment': increment, 'float_prec': Global.config['precision']}, 1+dim)\n        elif operation == \"min\":\n            code += tabify(\"\"\"\n%(float_prec)s _psp = %(increment)s\n            if(_psp &lt; sum) sum = _psp;\"\"\" % {'increment': increment, 'float_prec': Global.config['precision']}, 1+dim)\n        elif operation == \"mean\":\n            code += tabify(\"\"\"\n            sum += %(increment)s\"\"\" % {'increment': increment}, 1+dim)\n        else:\n            Global._error('SharedProjection: Operation', operation, 'is not implemented yet for shared projections.')\n\n        # Close for loops\n        for dim in range(self.dim_kernel-1):\n            code += tabify(\"\"\"\n        }\"\"\", self.dim_kernel-1-dim)\n\n        impl_code = code % {'id_proj': self.id,\n            'target': self.target,\n            'id_pre': self.pre.id, 'name_pre': self.pre.name, 'size_pre': self.pre.size,\n            'id_post': self.post.id, 'name_post': self.post.name, 'size_post': self.post.size\n        }\n\n        # sum code\n        if operation == \"mean\":\n            sum_code = \"\"\"sum/%(filter_size)s\"\"\" % {'filter_size': self.weights.size}\n        else:\n            sum_code = \"sum\"\n\n        return impl_code, sum_code\n\n    ##############################\n    ## Override useless methods\n    ##############################\n    def _data(self):\n        \"Disable saving.\"\n        desc = {}\n        desc['post_ranks'] = self.post_ranks\n        desc['attributes'] = self.attributes\n        desc['parameters'] = self.parameters\n        desc['variables'] = self.variables\n\n        desc['dendrites'] = []\n        desc['number_of_synapses'] = 0\n        return desc\n\n    def save_connectivity(self, filename):\n        \"Not available.\"\n        Global._warning('Convolutional projections can not be saved.')\n    def save(self, filename):\n        \"Not available.\"\n        Global._warning('Convolutional projections can not be saved.')\n    def load(self, filename):\n        \"Not available.\"\n        Global._warning('Convolutional projections can not be loaded.')\n    def receptive_fields(self, variable = 'w', in_post_geometry = True):\n        \"Not available.\"\n        Global._warning('Convolutional projections can not display receptive fields.')\n    def connectivity_matrix(self, fill=0.0):\n        \"Not available.\"\n        Global._warning('Convolutional projections can not display connectivity matrices.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.__init__","title":"<code>__init__(pre, post, target, psp='pre.r * w', operation='sum', name=None, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> pre         \u2013          <p>pre-synaptic population (either its name or a <code>Population</code> object).</p> </li> <li> post         \u2013          <p>post-synaptic population (either its name or a <code>Population</code> object).</p> </li> <li> target         \u2013          <p>type of the connection</p> </li> <li> psp         \u2013          <p>continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: <code>w*pre.r</code>).</p> </li> <li> operation         \u2013          <p>operation (sum, max, min, mean) performed by the kernel (default: sum).</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Convolve.py</code> <pre><code>def __init__(self, pre, post, target, psp=\"pre.r * w\", operation=\"sum\", name=None, copied=False):\n\"\"\"\n    :param pre: pre-synaptic population (either its name or a ``Population`` object).\n    :param post: post-synaptic population (either its name or a ``Population`` object).\n    :param target: type of the connection\n    :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``).\n    :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum).\n    \"\"\"\n    # Sanity check\n    #if not pre.neuron_type.type == 'rate':\n    #    Global._error('Convolution: only implemented for rate-coded populations.')\n\n    # Create the description, but it will not be used for generation\n    Projection.__init__(\n        self,\n        pre,\n        post,\n        target,\n        synapse=SharedSynapse(psp=psp, operation=operation, name=\"Convolution operation\", description=\"Convoluted kernel over the pre-synaptic population.\"),\n        name=name,\n        copied=copied\n    )\n\n    # Disable saving\n    self._saveable = False\n\n    # For copy\n    self._used_single_filter = False\n    self._used_bank_of_filters = False\n    self.operation = operation\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.connect_filter","title":"<code>connect_filter(weights, delays=0.0, keep_last_dimension=False, padding=0.0, subsampling=None)</code>","text":"<p>Applies a single filter on the pre-synaptic population.</p> <p>Parameters:</p> <ul> <li> weights         \u2013          <p>numpy array or list of lists representing the matrix of weights for the filter.</p> </li> <li> delays         \u2013          <p>delay in synaptic transmission (default: dt). Can only be the same value for all neurons.</p> </li> <li> keep_last_dimension         \u2013          <p>defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.</p> </li> <li> padding         \u2013          <p>value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0.</p> </li> <li> subsampling         \u2013          <p>list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Convolve.py</code> <pre><code>def connect_filter(self, weights, delays=0.0, keep_last_dimension=False, padding=0.0, subsampling=None):\n\"\"\"\n    Applies a single filter on the pre-synaptic population.\n\n    :param weights: numpy array or list of lists representing the matrix of weights for the filter.\n    :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons.\n    :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.\n    :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0.\n    :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.\n    \"\"\"\n\n    # Process the weights\n    self.weights = np.array(weights)\n\n    # Process the delays\n    self.delays = float(delays)\n    if not isinstance(delays, (int, float)):\n        Global._error('Convolutions can only have constant delays.')\n\n    self.subsampling = subsampling\n    self.keep_last_dimension = keep_last_dimension\n    self.padding = padding\n    self.multiple = False\n\n    # Check dimensions of populations and weight matrix\n    self.dim_kernel = self.weights.ndim\n    self.dim_pre = self.pre.dimension\n    self.dim_post = self.post.dimension\n\n    if self.dim_post &gt; 4:\n        print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n        Global._error('Convolution: Too many dimensions for the post-synaptic population (maximum 4).')\n\n    if self.dim_pre &gt; 4:\n        print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n        Global._error('Convolution: Too many dimensions for the pre-synaptic population (maximum 4).')\n\n    if self.dim_kernel &gt; 5  or (not self.multiple and self.dim_kernel &gt; 4):\n        print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n        Global._error('Convolution: Too many dimensions for the kernel (maximum 4).')\n\n    # Check if the last axes match for parallel convolution (e.g. 3-2-3)\n    if self.dim_kernel &lt; self.dim_pre:\n        if not self.keep_last_dimension:\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.')\n\n        if self.pre.geometry[-1] != self.post.geometry[-1]:\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.')\n\n    # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less.\n    if self.dim_post &lt; self.dim_pre: # OK, but check the last dimension of the kernel has the same size as the post-population\n        if self.weights.shape[-1] != self.pre.geometry[-1]:\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.')\n\n    # Check if it is a bank of filters\n    if self.dim_kernel &gt; self.dim_pre:\n        print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n        Global._error('Convolution: If the kernel has more dimensions than the pre-synaptic population, you need to use the connect_filters() method.')\n\n\n    # Generate the pre-synaptic coordinates\n    self._generate_pre_coordinates()\n\n    # Finish building the synapses\n    self._create()\n\n    # For copy\n    self._used_single_filter = True\n\n    return self\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.connect_filters","title":"<code>connect_filters(weights, delays=0.0, keep_last_dimension=False, padding=0.0, subsampling=None)</code>","text":"<p>Applies a set of different filters on the pre-synaptic population.</p> <p>The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters.</p> <p>Parameters:</p> <ul> <li> weights         \u2013          <p>numpy array or list of lists representing the matrix of weights for the filter.</p> </li> <li> delays         \u2013          <p>delay in synaptic transmission (default: dt). Can only be the same value for all neurons.</p> </li> <li> keep_last_dimension         \u2013          <p>defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.</p> </li> <li> padding         \u2013          <p>value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0.</p> </li> <li> subsampling         \u2013          <p>list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Convolve.py</code> <pre><code>def connect_filters(self, weights, delays=0.0, keep_last_dimension=False, padding=0.0, subsampling=None):\n\"\"\"\n    Applies a set of different filters on the pre-synaptic population.\n\n    The weights matrix must have one dimension more than the pre-synaptic populations, and the number of neurons in the last dimension of the post-synaptic population must be equal to the number of filters.\n\n\n    :param weights: numpy array or list of lists representing the matrix of weights for the filter.\n    :param delays: delay in synaptic transmission (default: dt). Can only be the same value for all neurons.\n    :param keep_last_dimension: defines if the last dimension of the pre- and post-synaptic will be convolved in parallel. The weights matrix must have one dimension less than the pre-synaptic population, and the number of neurons in the last dimension of the pre- and post-synaptic populations must match. Default: False.\n    :param padding: value to be used for the rates outside the pre-synaptic population. If it is a floating value, the pre-synaptic population is virtually extended with this value above its boundaries. If it is equal to 'border', the values on the boundaries are repeated. Default: 0.0.\n    :param subsampling: list for each post-synaptic neuron of coordinates in the pre-synaptic population defining the center of the kernel/filter. Default: None.\n    \"\"\"\n\n    # Process the weights\n    self.weights = np.array(weights)\n\n    # Process the delays\n    self.delays = float(delays)\n    if not isinstance(delays, (int, float)):\n        Global._error('Convolutions can only have constant delays.')\n\n    self.subsampling = subsampling\n    self.keep_last_dimension = keep_last_dimension\n    self.padding = padding\n    self.multiple = True\n\n    # Check dimensions of populations and weight matrix\n    self.dim_kernel = self.weights.ndim\n    self.dim_pre = self.pre.dimension\n    self.dim_post = self.post.dimension\n\n\n    if self.dim_post &gt; 4:\n        print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n        Global._error('Convolution: Too many dimensions for the post-synaptic population (maximum 4).')\n\n    if self.dim_pre &gt; 4:\n        print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n        Global._error('Convolution: Too many dimensions for the pre-synaptic population (maximum 4).')\n\n    if self.dim_kernel &gt; 5  or (not self.multiple and self.dim_kernel &gt; 4):\n        print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n        Global._error('Convolution: Too many dimensions for the kernel (maximum 4).')\n\n    # Check if the last axes match for parallel convolution (e.g. 3-2-3)\n    if self.dim_kernel &lt; self.dim_pre:\n        if not self.keep_last_dimension:\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: If the kernel has less dimensions than the pre-synaptic population, you need to set the flag keep_last_dimension to True.')\n\n        if self.pre.geometry[-1] != self.post.geometry[-1]:\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: If the kernel has fewer dimensions than the two populations (keep_last_dimension=True), these must have the same number of neurons in the last dimension.')\n\n    # If the last dim of the kernel matches the last dim of the pre-pop, the last pop can have one dimension less.\n    if self.dim_post &lt; self.dim_pre: # OK, but check the last dimension of the kernel has the same size as the post-population\n        if self.weights.shape[-1] != self.pre.geometry[-1]:\n            print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n            Global._error('Convolution: If the post-synaptic population has less dimensions than the pre-synaptic one, the last dimension of the filter must be equal to the last of the pre-synaptic population.')\n\n    # The last dimension of the post population must correspond to the number of filters\n    if self.weights.shape[0] != self.post.geometry[-1]:\n        print(\"Convolution:\", self.dim_pre, '*', self.dim_kernel, '-&gt;', self.dim_post)\n        Global._error('Convolution: For multiple filters, the last dimension of the post-synaptic population must have as many neurons as there are filters.')\n\n    # Generate the pre-synaptic coordinates\n    self._generate_pre_coordinates_bank()\n\n    # Finish building the synapses\n    self._create()\n\n    # For copy\n    self._used_bank_of_filters = True\n\n    return self\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.connectivity_matrix","title":"<code>connectivity_matrix(fill=0.0)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Convolve.py</code> <pre><code>def connectivity_matrix(self, fill=0.0):\n    \"Not available.\"\n    Global._warning('Convolutional projections can not display connectivity matrices.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.load","title":"<code>load(filename)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Convolve.py</code> <pre><code>def load(self, filename):\n    \"Not available.\"\n    Global._warning('Convolutional projections can not be loaded.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.receptive_fields","title":"<code>receptive_fields(variable='w', in_post_geometry=True)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Convolve.py</code> <pre><code>def receptive_fields(self, variable = 'w', in_post_geometry = True):\n    \"Not available.\"\n    Global._warning('Convolutional projections can not display receptive fields.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.save","title":"<code>save(filename)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Convolve.py</code> <pre><code>def save(self, filename):\n    \"Not available.\"\n    Global._warning('Convolutional projections can not be saved.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Convolve.Convolution.save_connectivity","title":"<code>save_connectivity(filename)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Convolve.py</code> <pre><code>def save_connectivity(self, filename):\n    \"Not available.\"\n    Global._warning('Convolutional projections can not be saved.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling","title":"<code>ANNarchy.extensions.convolution.Pooling</code>","text":""},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling","title":"<code>Pooling</code>","text":"<p>             Bases: <code>Projection</code></p> <p>Performs a pooling operation (e.g. max.pooling) on the pre-synaptic population.</p> <p>Each post-synaptic neuron covers a specific region (<code>extent</code>) of the pre-synaptic population, over which the result of the operation on firing rates will be assigned to sum(target).</p> <p>The extent is automatically computed using the geometry of the populations, but can be specified in the `connect_pooling()`` methods.</p> <p>Example:</p> <pre><code>inp = Population(geometry=(100, 100), neuron=Neuron(parameters=\"r = 0.0\"))\npop = Population(geometry=(50, 50), neuron=Neuron(equations=\"r = sum(exc)\"))\nproj = Pooling(inp, pop, 'exc', operation='max') # max-pooling\nproj.connect_pooling() # extent=(2, 2) is implicit\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Pooling.py</code> <pre><code>class Pooling(Projection):\n\"\"\"\n    Performs a pooling operation (e.g. max.pooling) on the pre-synaptic population.\n\n    Each post-synaptic neuron covers a specific region (``extent``) of the pre-synaptic\n    population, over which the result of the operation on firing rates will be\n    assigned to sum(target).\n\n    The extent is automatically computed using the geometry of the populations, but can be specified in the `connect_pooling()`` methods.\n\n    Example:\n\n    ```python\n    inp = Population(geometry=(100, 100), neuron=Neuron(parameters=\"r = 0.0\"))\n    pop = Population(geometry=(50, 50), neuron=Neuron(equations=\"r = sum(exc)\"))\n    proj = Pooling(inp, pop, 'exc', operation='max') # max-pooling\n    proj.connect_pooling() # extent=(2, 2) is implicit\n    ```\n    \"\"\"\n    def __init__(self, pre, post, target, psp=\"pre.r\", operation=\"max\", name=None, copied=False):\n\"\"\"\n        :param pre: pre-synaptic population (either its name or a ``Population`` object).\n        :param post: post-synaptic population (either its name or a ``Population`` object).\n        :param target: type of the connection\n        :param operation: pooling function to be applied (\"max\", \"min\", \"mean\")\n        \"\"\"\n        # Sanity check\n        #if not pre.neuron_type.type == 'rate':\n        #    Global._error('Pooling: only implemented for rate-coded populations.')\n\n        # Sanity check\n        if not operation in [\"max\", \"mean\", \"min\"]:\n            Global._error(\"Pooling: the operation must be either 'max', 'mean' or 'min'.\")\n        self.operation = operation\n\n        # Store for _copy\n        self.psp = psp\n\n        Projection.__init__(\n            self,\n            pre,\n            post,\n            target,\n            synapse=SharedSynapse(psp=psp, operation=operation, name=\"Pooling operation\", description=operation+\"-pooling operation over the pre-synaptic population.\"),\n            name=name,\n            copied=copied\n        )\n\n        # check dimensions of populations, should not exceed 4\n        self.dim_pre = self.pre.dimension\n        self.dim_post = self.post.dimension\n        if self.dim_post &gt; 4:\n            Global._error('Pooling: Too many dimensions for the post-synaptic population (maximum 4).')\n        if self.dim_pre &gt; 4:\n            Global._error('Pooling: Too many dimensions for the pre-synaptic population (maximum 4).')\n\n        # Disable saving\n        self._saveable = False\n\n\n\n    def connect_pooling(self, extent=None, delays=0.0):\n\"\"\"\n        :param extent: extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g ``(2, 2)``). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None.\n        :param delays: synaptic delay in ms\n        \"\"\"\n\n        # process extent\n        self.extent_init = extent\n        if extent is None:  # compute the extent automatically\n            if self.pre.dimension != self.post.dimension:\n                Global._error(\n                    'Pooling: If you do not provide the extent parameter, the two populations must have the same number of dimensions.')\n\n            extent = list(self.pre.geometry)\n            for dim in range(self.pre.dimension):\n                extent[dim] /= self.post.geometry[dim]\n                if self.pre.geometry[dim] != extent[dim] * self.post.geometry[dim]:\n                    Global._error(\n                        'Pooling: Unable to compute the extent of the pooling area: the number of neurons do not match.')\n\n        elif not isinstance(extent, tuple):\n            Global._error('Pooling: You must provide a tuple for the extent of the pooling operation.')\n\n        self.extent = list(extent)\n        if len(self.extent) &lt; self.pre.dimension:\n            Global._error('Pooling: You must provide a tuple for the extent of the pooling operation.')\n\n        # process delays\n        self.delays = delays\n\n        # Generate the pre-synaptic coordinates\n        self._generate_extent_coordinates()\n\n        # create fake LIL\n        self._create()\n\n        return self\n\n    def _copy(self, pre, post):\n        \"Returns a copy of the projection when creating networks.  Internal use only.\"\n        copied_proj = Pooling(pre=pre, post=post, target=self.target, psp=self.psp,\n                              operation=self.operation, name=self.name, copied=True)\n\n        copied_proj.extent = self.extent\n        copied_proj.delays = self.delays\n\n        copied_proj._generate_extent_coordinates()\n        copied_proj._create()\n\n        copied_proj._connection_method = self._connection_method\n        copied_proj._connection_args = self._connection_args\n        copied_proj._connection_delay = self._connection_delay\n        copied_proj._storage_format = self._storage_format\n        return copied_proj\n\n    def _create(self):\n\"\"\"\n        create fake LIL object, just for compilation process\n\n        :return: no return value\n        \"\"\"\n        try:\n            from ANNarchy.core.cython_ext.Connector import LILConnectivity\n        except Exception as e:\n            Global._print(e)\n            Global._error('ANNarchy was not successfully installed.')\n\n        lil = LILConnectivity()\n        lil.max_delay = self.delays\n        lil.uniform_delay = self.delays\n        self.connector_name = \"Pooling\"\n        self.connector_description = \"Pooling\"\n        self._store_connectivity(self._load_from_lil, (lil, ), self.delays, storage_format=\"lil\", storage_order=\"post_to_pre\")\n\n    def _connect(self, module):\n\"\"\"\n        Builds up dendrites either from list or dictionary. Called by instantiate().\n        \"\"\"\n        if not self._connection_method:\n            Global._error(\n                'Pooling: The projection between ' + self.pre.name + ' and ' + self.post.name + ' is declared but not connected.')\n\n        # Create the Cython instance\n        proj = getattr(module, 'proj' + str(self.id) + '_wrapper')\n        self.cyInstance = proj([], self.pre_coordinates)\n\n        return True\n\n    def _generate_extent_coordinates(self):\n\"\"\"\n        Generates for each post-neuron the position of the top-left corner, where the pooling should be applied.\n\n        :return:  a list for each post neuron of the corresponding top-left coordinates\n        \"\"\"\n        # Generates coordinates TODO: Find a more robust way!\n        coords = [[] for i in range(self.post.size)]\n        if self.dim_pre == 1:\n            rk = 0\n            for i in range(self.post.geometry[0]):\n                coords[rk] = [i * self.extent[0]]\n                rk += 1\n        elif self.dim_pre == 2:\n            rk = 0\n            for i in range(self.post.geometry[0]):\n                if self.dim_post &gt; 1:\n                    for j in range(self.post.geometry[1]):\n                        coords[rk] = [i * self.extent[0], j * self.extent[1]]\n                        rk += 1\n                else: # over the whole second axis\n                    coords[rk] = [i * self.extent[0], 0]\n                    rk += 1\n\n        elif self.dim_pre == 3:\n            rk = 0\n            for i in range(self.post.geometry[0]):\n                for j in range(self.post.geometry[1]):\n                    if self.dim_post &gt; 2:\n                        for k in range(self.post.geometry[2]):\n                            coords[rk] = [i * self.extent[0], j * self.extent[1], k * self.extent[2]]\n                            rk += 1\n                    else: # over the whole third axis\n                        coords[rk] = [i * self.extent[0], j * self.extent[1], 0]\n                        rk += 1\n\n        elif self.dim_pre == 4: # TODO: post has less than 4 dimensions\n            rk = 0\n            for i in range(self.post.geometry[0]):\n                for j in range(self.post.geometry[1]):\n                    for k in range(self.post.geometry[2]):\n                        for l in range(self.post.geometry[3]):\n                            coords[rk] = [i * self.extent[0], j * self.extent[1], k * self.extent[2], l * self.extent[3]]\n                            rk += 1\n        # Save the result\n        self.pre_coordinates = coords\n\n    def _generate(self):\n\"\"\"\n        Overrides the default code generation.\n        \"\"\"\n        # Convolve_code\n        convolve_code, sum_code = self._generate_pooling_code()\n\n        # Generate the code\n        if Global._check_paradigm(\"openmp\"):\n            self._generate_omp(convolve_code, sum_code)\n        elif Global._check_paradigm(\"cuda\"):\n            Global._error(\"Pooling: not available on GPU devices\")\n            #self._generate_cuda(convolve_code, sum_code)\n        else:\n            Global._error(\"Pooling: not implemented for the configured paradigm\")\n\n    def _generate_pooling_code(self):\n\"\"\"\n        Generate loop statements for the desired pooling operation.\n        \"\"\"\n        # Operation to be performed: sum, max, min, mean\n        operation = self.synapse_type.operation\n\n        # Main code\n        # default value for sum in code depends on operation\n        sum_default = \"0.0\"\n        if self.synapse_type.operation == \"min\":\n            sum_default = \"std::numeric_limits&lt;%(float_prec)s&gt;::max()\" % {'float_prec': Global.config['precision']}\n        elif self.synapse_type.operation == \"max\":\n            sum_default = \"std::numeric_limits&lt;%(float_prec)s&gt;::min()\" % {'float_prec': Global.config['precision']}\n\n        code = \"\"\"\n            sum = %(sum_default)s;\n\"\"\" % {'sum_default': sum_default}\n\n        # Generate for loops\n        for dim in range(self.dim_pre):\n            ind_dict = {\n                'index': indices[dim],\n                'size': self.extent[dim]\n            }\n            if self.extent[dim] &gt; 1:\n                code += \"\"\"\n            for(int %(index)s_w = 0; %(index)s_w &lt; %(size)s; %(index)s_w++){\n    \"\"\" % ind_dict\n\n        # Compute indices\n        for dim in range(self.dim_pre):\n            ind_dict = {\n                'index': indices[dim],\n                'dim': dim\n            }\n            if self.extent[dim] &gt; 1:\n                code += \"\"\"\n                int %(index)s_pre = coord[%(dim)s] + %(index)s_w;\"\"\" % ind_dict\n            else:\n                code += \"\"\"\n                int %(index)s_pre = coord[%(dim)s];\"\"\" % ind_dict\n\n        # Check indices\n        for dim in range(self.dim_pre):\n            ind_dict = {\n                'index': indices[dim],\n                'max_size': self.pre.geometry[dim] - 1\n            }\n            code += \"\"\"\n                if ((%(index)s_pre &lt; 0) ||(%(index)s_pre &gt; %(max_size)s)){\n                    continue;\n                }\"\"\" % ind_dict\n\n        # Compute pre-synaptic rank\n        code += \"\"\"\n                rk_pre = %(value)s;\"\"\" % {'value': self._coordinates_to_rank('pre', self.pre.geometry)}\n\n        # Compute the value to pool\n        psp = self.synapse_type.description['psp']['cpp'] % {\n            'id_pre': self.pre.id,\n            'id_post': self.post.id,\n            'local_index': '[i][j]',\n            'global_index': '[i]',\n            'pre_index': '[rk_pre]',\n            'post_index': '[rk_post]',\n            'pre_prefix': 'pop'+str(self.pre.id)+'.',\n            'post_prefix': 'pop'+str(self.post.id)+'.'\n        }\n\n        # Delays\n        if self.delays &gt; Global.config['dt']:\n            psp = psp.replace(\n                'pop%(id_pre)s.r[rk_pre]' % {'id_pre': self.pre.id},\n                'pop%(id_pre)s._delayed_r[%(delay)s][rk_pre]' % {'id_pre': self.pre.id, 'delay': str(int(self.delays/Global.config['dt'])-1)}\n            )\n\n        # Apply the operation\n        if operation == \"max\":\n            code += \"\"\"\n%(float_prec)s _psp = %(psp)s;\n                if(_psp &gt; sum) sum = _psp;\"\"\"\n        elif operation == \"min\":\n            code += \"\"\"\n%(float_prec)s _psp = %(psp)s;\n                if(_psp &lt; sum) sum = _psp;\"\"\"\n        elif operation == \"sum\":\n            code += \"\"\"\n                sum += %(psp)s;\"\"\"\n        elif operation == \"mean\":\n            code += \"\"\"\n                sum += %(psp)s;\"\"\"\n        else:\n            Global._error('SharedProjection: Operation', operation, 'is not implemented yet for shared projections with pooling.')\n\n        # Close for loops\n        for dim in range(self.dim_pre):\n            if self.extent[dim] &gt; 1:\n                code += \"\"\"\n            }\"\"\"\n\n        impl_code = code % {\n            'id_proj': self.id,\n            'target': self.target,\n            'id_pre': self.pre.id, 'name_pre': self.pre.name, 'size_pre': self.pre.size,\n            'id_post': self.post.id, 'name_post': self.post.name, 'size_post': self.post.size,\n            'psp': psp,\n            'float_prec': Global.config['precision']\n        }\n\n        if operation == \"mean\":\n            size = 1\n            for dim in range(self.pre.dimension):\n                size *= self.extent[dim]\n            sum_code = \"sum/\" + str(size)\n        else:\n            sum_code = \"sum\"\n\n        return impl_code, sum_code\n\n    def _generate_omp(self, convolve_code, sum_code):\n\"\"\"\n        Update the ProjectionGenerator._specific_template structure and bypass the standard openMP code generation.\n\n        :param convolve_code:\n        :param sum_code:\n        \"\"\"\n        # default value for sum in code depends on operation\n        sum_default = \"0.0\"\n        if self.synapse_type.operation == \"min\":\n            sum_default = \"std::numeric_limits&lt;%(float_prec)s&gt;::max()\" % {'float_prec': Global.config['precision']}\n        elif self.synapse_type.operation == \"max\":\n            sum_default = \"std::numeric_limits&lt;%(float_prec)s&gt;::min()\" % {'float_prec': Global.config['precision']}\n\n        # Specific template for generation\n        pool_dict = deepcopy(pooling_template_omp)\n        for key, value in pool_dict.items():\n            value = value % {\n                'id_proj': self.id,\n                'size_post': self.post.size,\n                'sum_default': sum_default,\n                'float_prec': Global.config['precision']\n            }\n            pool_dict[key] = value\n        self._specific_template.update(pool_dict)\n\n        # OMP code\n        omp_code = \"\"\n        if Global.config['num_threads'] &gt; 1:\n            omp_code = \"\"\"\n        #pragma omp for private(sum, rk_pre, coord) %(psp_schedule)s\"\"\" % {\n                'psp_schedule': \"\" if not 'psp_schedule' in self._omp_config.keys() else self._omp_config[\n                    'psp_schedule']}\n\n        # HD ( 16.10.2015 ):\n        # pre-load delayed firing rate in a local array, so we\n        # prevent multiple accesses to pop%(id_pre)s._delayed_r[%(delay)s]\n        if self.delays &gt; Global.config['dt']:\n            pre_load_r = \"\"\"\n        // pre-load delayed firing rate\n        auto delayed_r = pop%(id_pre)s._delayed_r[%(delay)s];\n        \"\"\" % {'id_pre': self.pre.id, 'delay': str(int(self.delays / Global.config['dt']) - 1)}\n        else:\n            pre_load_r = \"\"\n\n        # Target variable depends on neuron type\n        target_code = \"_sum_%(target)s\" if self.post.neuron_type.type==\"rate\" else \"g_%(target)s\"\n        target_code %= {'target': self.target}\n\n        # Compute sum\n        wsum = \"\"\"\n        if ( _transmission &amp;&amp; pop%(id_pre)s._active ) {\n        std::vector&lt;int&gt; coord;\n\"\"\" + pre_load_r + \"\"\"\n%(omp_code)s\n        for(int i = 0; i &lt; %(size_post)s; i++){\n            coord = pre_rank[i];\n\"\"\" + convolve_code + \"\"\"\n            pop%(id_post)s.%(target)s[i] += \"\"\" + sum_code + \"\"\";\n        } // for\n        } // if\n\"\"\"\n\n        # Delays\n        self._specific_template['wrapper_init_delay'] = \"\"\n\n        # Dictionary keys\n        psp_dict = {\n            'id_proj': self.id,\n            'target': target_code,\n            'id_pre': self.pre.id, 'name_pre': self.pre.name,\n            'size_pre': self.pre.size,\n            'id_post': self.post.id, 'name_post': self.post.name,\n            'size_post': self.post.size,\n            'omp_code': omp_code,\n            'convolve_code': convolve_code\n        }\n\n        # Psp code\n        self._specific_template['psp_code'] = wsum % psp_dict\n        self._specific_template['size_in_bytes'] = \"\"\"\n        // connectivity\n        size_in_bytes += sizeof(std::vector&lt;int&gt;);\n        size_in_bytes += pre_rank.capacity() * sizeof(int);\n\n        size_in_bytes += sizeof(std::vector&lt;std::vector&lt;int&gt;&gt;);\n        size_in_bytes += pre_rank.capacity() * sizeof(std::vector&lt;int&gt;);\n        for (auto it = pre_rank.begin(); it != pre_rank.end(); it++) {\n            size_in_bytes += it-&gt;capacity() * sizeof(int);\n        }\n\"\"\"\n        self._specific_template['clear'] = \"\"\"\n        // post-ranks\n        post_rank.clear();\n        post_rank.shrink_to_fit();\n\n        // pre-ranks sub-lists\n        for (auto it = pre_rank.begin(); it != pre_rank.end(); it++) {\n            it-&gt;clear();\n            it-&gt;shrink_to_fit();\n        }\n        // pre-ranks top-list\n        pre_rank.clear();\n        pre_rank.shrink_to_fit();\n\"\"\"\n\n    def _generate_cuda(self, convolve_code, sum_code):\n\"\"\"\n        Update the ProjectionGenerator._specific_template structure and bypass the standard CUDA code generation.\n        \"\"\"\n        pool_operation = self.synapse_type.operation\n\n        # default value for sum in code depends on operation\n        sum_default = \"0.0\"\n        if pool_operation == \"min\":\n            sum_default = \"FLT_MAX\"\n        elif pool_operation == \"max\":\n            sum_default = \"FLT_MIN\"\n\n        # operation to perform\n        pool_op_code = cuda_op_code[pool_operation] % {'float_prec': Global.config['precision']}\n\n        # result dictionary with code for\n        # body, call and header\n        pool_template = {}\n        base_ids = {\n            'id_proj': self.id,\n            'id_pre': self.pre.id,\n            'id_post': self.post.id,\n            'target': self.target,\n            'float_prec': Global.config['precision'],\n            'size_post': self.post.size # TODO: population views?\n        }\n\n        # The correct templates depends on both\n        # kernel-geometry and extent\n        if len(self.pre.geometry) == 2:\n            # For small extents, we compute multiple coords within one warp. If one extent can fill alone\n            # a half-warp we switch to the other implementation.\n            if self.extent[0] &lt; 6:\n\n                pool_op_reduce_code = cuda_pooling_code_2d_small_extent['reduce_code'][pool_operation] % {\n                    'float_prec': Global.config['precision'],\n                    'row_extent': int(self.extent[0]),\n                    'col_extent': int(self.extent[1])\n                }\n\n                pool_dict = deepcopy(base_ids)\n                pool_dict.update({\n                    'sum_default': sum_default,\n                    'row_extent': int(self.extent[0]),\n                    'col_extent': int(self.extent[1]),\n                    'row_size': int(self.pre.geometry[0]),\n                    'col_size': int(self.pre.geometry[1]),\n                    'operation': tabify(pool_op_code, 3),\n                    'operation_reduce': pool_op_reduce_code\n                })\n\n                pool_template['psp_body'] = cuda_pooling_code_2d_small_extent['psp_body'] % pool_dict\n                pool_template['psp_header'] = cuda_pooling_code_2d_small_extent['psp_header'] % pool_dict\n                pool_template['psp_call'] = cuda_pooling_code_2d_small_extent['psp_call'] % pool_dict\n\n            else:\n                pool_op_reduce_code = cuda_pooling_code_2d['reduce_code'][pool_operation] % {\n                    'float_prec': Global.config['precision'],\n                    'row_extent': int(self.extent[0]),\n                    'col_extent': int(self.extent[1])\n                }\n\n                pool_dict = deepcopy(base_ids)\n                pool_dict.update({\n                    'sum_default': sum_default,\n                    'row_extent': int(self.extent[0]),\n                    'col_extent': int(self.extent[1]),\n                    'row_size': int(self.pre.geometry[0]),\n                    'col_size': int(self.pre.geometry[1]),\n                    'operation': tabify(pool_op_code, 3),\n                    'operation_reduce': tabify(pool_op_reduce_code, 2)\n                })\n\n                pool_template['psp_body'] = remove_trailing_spaces(cuda_pooling_code_2d['psp_body'] % pool_dict)\n                pool_template['psp_header'] = cuda_pooling_code_2d['psp_header'] % pool_dict\n                pool_template['psp_call'] = cuda_pooling_code_2d['psp_call'] % pool_dict\n\n        elif len(self.pre.geometry) == 3:\n\n            pool_dict = deepcopy(base_ids)\n            pool_dict.update({\n                'sum_default': sum_default,\n                'row_extent': self.extent[0],\n                'col_extent': self.extent[1],\n                'plane_extent': self.extent[2],\n                'row_size': self.pre.geometry[0],\n                'col_size': self.pre.geometry[1],\n                'plane_size': self.pre.geometry[2],\n                'operation': tabify(pool_op_code, 4)\n            })\n\n            pool_template['psp_body'] = remove_trailing_spaces(cuda_pooling_code_3d['psp_body'] % pool_dict)\n            pool_template['psp_header'] = cuda_pooling_code_3d['psp_header'] % pool_dict\n            pool_template['psp_call'] = cuda_pooling_code_3d['psp_header'] % pool_dict\n\n        else:\n            raise NotImplementedError\n\n        # Update psp fields\n        self._specific_template.update(pool_template)\n\n        # Specific template for generation (wrapper, etc)\n        pool_dict = deepcopy(pooling_template_cuda)\n        for key, value in pool_dict.items():\n            value = value % base_ids\n            pool_dict[key] = value\n        self._specific_template.update(pool_dict)\n\n        self._specific_template['wrapper_connector_call'] = \"\"\n        self._specific_template['access_parameters_variables'] = \"\"\n\n        self._specific_template['size_in_bytes'] = \"//TODO:\\n\"\n\n    @staticmethod\n    def _coordinates_to_rank(name, geometry):\n\"\"\"\n        Generate the code for array access, for instance used\n        for pre-synaptic ranks.\n        \"\"\"\n        dim = len(geometry)\n\n        txt = \"\"\n\n        for d in range(dim):\n            if txt == \"\":   # first coordinate is special\n                txt = indices[0] + \"_\" + name\n            else:\n                txt = str(geometry[d]) + '*(' + txt + ') + ' + indices[d] + '_' + name\n\n        return txt\n\n    ##############################\n    ## Override useless methods\n    ##############################\n    def _data(self):\n        \"Disable saving.\"\n        desc = {}\n        desc['post_ranks'] = self.post_ranks\n        desc['attributes'] = self.attributes\n        desc['parameters'] = self.parameters\n        desc['variables'] = self.variables\n\n        desc['dendrites'] = []\n        desc['number_of_synapses'] = 0\n        return desc\n\n    def save_connectivity(self, filename):\n        \"Not available.\"\n        Global._warning('Pooling projections can not be saved.')\n    def save(self, filename):\n        \"Not available.\"\n        Global._warning('Pooling projections can not be saved.')\n    def load(self, filename):\n        \"Not available.\"\n        Global._warning('Pooling projections can not be loaded.')\n    def receptive_fields(self, variable = 'w', in_post_geometry = True):\n        \"Not available.\"\n        Global._warning('Pooling projections can not display receptive fields.')\n    def connectivity_matrix(self, fill=0.0):\n        \"Not available.\"\n        Global._warning('Pooling projections can not display connectivity matrices.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.__init__","title":"<code>__init__(pre, post, target, psp='pre.r', operation='max', name=None, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> pre         \u2013          <p>pre-synaptic population (either its name or a <code>Population</code> object).</p> </li> <li> post         \u2013          <p>post-synaptic population (either its name or a <code>Population</code> object).</p> </li> <li> target         \u2013          <p>type of the connection</p> </li> <li> operation         \u2013          <p>pooling function to be applied (\"max\", \"min\", \"mean\")</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Pooling.py</code> <pre><code>def __init__(self, pre, post, target, psp=\"pre.r\", operation=\"max\", name=None, copied=False):\n\"\"\"\n    :param pre: pre-synaptic population (either its name or a ``Population`` object).\n    :param post: post-synaptic population (either its name or a ``Population`` object).\n    :param target: type of the connection\n    :param operation: pooling function to be applied (\"max\", \"min\", \"mean\")\n    \"\"\"\n    # Sanity check\n    #if not pre.neuron_type.type == 'rate':\n    #    Global._error('Pooling: only implemented for rate-coded populations.')\n\n    # Sanity check\n    if not operation in [\"max\", \"mean\", \"min\"]:\n        Global._error(\"Pooling: the operation must be either 'max', 'mean' or 'min'.\")\n    self.operation = operation\n\n    # Store for _copy\n    self.psp = psp\n\n    Projection.__init__(\n        self,\n        pre,\n        post,\n        target,\n        synapse=SharedSynapse(psp=psp, operation=operation, name=\"Pooling operation\", description=operation+\"-pooling operation over the pre-synaptic population.\"),\n        name=name,\n        copied=copied\n    )\n\n    # check dimensions of populations, should not exceed 4\n    self.dim_pre = self.pre.dimension\n    self.dim_post = self.post.dimension\n    if self.dim_post &gt; 4:\n        Global._error('Pooling: Too many dimensions for the post-synaptic population (maximum 4).')\n    if self.dim_pre &gt; 4:\n        Global._error('Pooling: Too many dimensions for the pre-synaptic population (maximum 4).')\n\n    # Disable saving\n    self._saveable = False\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.connect_pooling","title":"<code>connect_pooling(extent=None, delays=0.0)</code>","text":"<p>Parameters:</p> <ul> <li> extent         \u2013          <p>extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g <code>(2, 2)</code>). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None.</p> </li> <li> delays         \u2013          <p>synaptic delay in ms</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Pooling.py</code> <pre><code>def connect_pooling(self, extent=None, delays=0.0):\n\"\"\"\n    :param extent: extent of the pooling area expressed in the geometry of the pre-synaptic population (e.g ``(2, 2)``). In each dimension, the product of this extent with the number of neurons in the post-synaptic population must be equal to the number of pre-synaptic neurons. Default: None.\n    :param delays: synaptic delay in ms\n    \"\"\"\n\n    # process extent\n    self.extent_init = extent\n    if extent is None:  # compute the extent automatically\n        if self.pre.dimension != self.post.dimension:\n            Global._error(\n                'Pooling: If you do not provide the extent parameter, the two populations must have the same number of dimensions.')\n\n        extent = list(self.pre.geometry)\n        for dim in range(self.pre.dimension):\n            extent[dim] /= self.post.geometry[dim]\n            if self.pre.geometry[dim] != extent[dim] * self.post.geometry[dim]:\n                Global._error(\n                    'Pooling: Unable to compute the extent of the pooling area: the number of neurons do not match.')\n\n    elif not isinstance(extent, tuple):\n        Global._error('Pooling: You must provide a tuple for the extent of the pooling operation.')\n\n    self.extent = list(extent)\n    if len(self.extent) &lt; self.pre.dimension:\n        Global._error('Pooling: You must provide a tuple for the extent of the pooling operation.')\n\n    # process delays\n    self.delays = delays\n\n    # Generate the pre-synaptic coordinates\n    self._generate_extent_coordinates()\n\n    # create fake LIL\n    self._create()\n\n    return self\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.connectivity_matrix","title":"<code>connectivity_matrix(fill=0.0)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Pooling.py</code> <pre><code>def connectivity_matrix(self, fill=0.0):\n    \"Not available.\"\n    Global._warning('Pooling projections can not display connectivity matrices.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.load","title":"<code>load(filename)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Pooling.py</code> <pre><code>def load(self, filename):\n    \"Not available.\"\n    Global._warning('Pooling projections can not be loaded.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.receptive_fields","title":"<code>receptive_fields(variable='w', in_post_geometry=True)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Pooling.py</code> <pre><code>def receptive_fields(self, variable = 'w', in_post_geometry = True):\n    \"Not available.\"\n    Global._warning('Pooling projections can not display receptive fields.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.save","title":"<code>save(filename)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Pooling.py</code> <pre><code>def save(self, filename):\n    \"Not available.\"\n    Global._warning('Pooling projections can not be saved.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Pooling.Pooling.save_connectivity","title":"<code>save_connectivity(filename)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Pooling.py</code> <pre><code>def save_connectivity(self, filename):\n    \"Not available.\"\n    Global._warning('Pooling projections can not be saved.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy","title":"<code>ANNarchy.extensions.convolution.Copy</code>","text":""},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy","title":"<code>Copy</code>","text":"<p>             Bases: <code>Projection</code></p> <p>Creates a virtual projection reusing the weights and delays of an already-defined projection.</p> <p>Although the original projection can be learnable, this one can not. Changes in the original weights will be reflected in this projection. The only possible modifications are <code>psp</code> and <code>operation</code>.</p> <p>The pre- and post-synaptic populations of both projections must have the same geometry.</p> <p>Example:</p> <pre><code>proj = Projection(pop1, pop2, \"exc\")\nproj.connect_fixed_probability(0.1, 0.5)\n\ncopy_proj = Copy(pop1, pop3, \"exc\")\ncopy_proj.connect_copy(proj)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Copy.py</code> <pre><code>class Copy(Projection):\n\"\"\"\n    Creates a virtual projection reusing the weights and delays of an already-defined projection.\n\n    Although the original projection can be learnable, this one can not. Changes in the original weights will be reflected in this projection. The only possible modifications are ``psp`` and ``operation``.\n\n    The pre- and post-synaptic populations of both projections must have the same geometry.\n\n    Example:\n\n    ```python\n    proj = Projection(pop1, pop2, \"exc\")\n    proj.connect_fixed_probability(0.1, 0.5)\n\n    copy_proj = Copy(pop1, pop3, \"exc\")\n    copy_proj.connect_copy(proj)\n    ```\n\n    \"\"\"\n    def __init__(self, pre, post, target, psp=\"pre.r * w\", operation=\"sum\", name=None, copied=False):\n\"\"\"\n        :param pre: pre-synaptic population (either its name or a ``Population`` object).\n        :param post: post-synaptic population (either its name or a ``Population`` object).\n        :param target: type of the connection\n        :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``).\n        :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum).\n        \"\"\"\n\n        # Create the description, but it will not be used for generation\n        Projection.__init__(\n            self,\n            pre=pre,\n            post=post,\n            target=target,\n            synapse = SharedSynapse(psp=psp, operation=operation),\n            name=name,\n            copied=copied\n        )\n\n    def connect_copy(self, projection):\n\"\"\"\n        :param projection: Existing projection to copy.\n        \"\"\"\n        self.projection = projection\n\n        # Sanity checks\n        if not isinstance(self.projection, Projection):\n            Global._error('Copy: You must provide an existing projection to copy().')\n\n        if isinstance(self.projection, (Convolution, Pooling)):\n            Global._error('Copy: You can only copy regular projections, not shared projections.')\n\n        if not self.pre.geometry == self.projection.pre.geometry or not self.post.geometry == self.projection.post.geometry:\n            Global._error('Copy: When copying a projection, the geometries must be the same.')\n\n        # Dummy weights\n        self.weights = None\n        self.pre_coordinates = []\n\n        # Finish building the synapses\n        self._create()\n\n        return self\n\n    def _copy(self, pre, post):\n        \"Returns a copy of the projection when creating networks. Internal use only.\"\n        raise NotImplementedError\n\n    def _create(self):\n        # create fake LIL object, just for compilation.\n        try:\n            from ANNarchy.core.cython_ext.Connector import LILConnectivity\n        except Exception as e:\n            Global._print(e)\n            Global._error('ANNarchy was not successfully installed.')\n\n        lil = LILConnectivity()\n        lil.max_delay = self.delays\n        lil.uniform_delay = self.delays\n        self.connector_name = \"Copy\"\n        self.connector_description = \"Copy projection\"\n        self._store_connectivity(self._load_from_lil, (lil, ), self.delays)\n\n    def _connect(self, module):\n\"\"\"\n        Builds up dendrites either from list or dictionary. Called by instantiate().\n        \"\"\"\n        if not self._connection_method:\n            Global._error('Copy: The projection between ' + self.pre.name + ' and ' + self.post.name + ' is declared but not connected.')\n\n        # Create the Cython instance\n        proj = getattr(module, 'proj'+str(self.id)+'_wrapper')\n        self.cyInstance = proj(self.weights, self.pre_coordinates)\n\n        # Define the list of postsynaptic neurons\n        self.post_ranks = list(range(self.post.size))\n\n        # Set delays after instantiation\n        if self.delays &gt; 0.0:\n            self.cyInstance.set_delay(self.delays/Global.config['dt'])\n\n        return True\n\n    def _generate(self):\n\"\"\"\n        Overrides default code generation. This function is called during the code generation procedure.\n        \"\"\"\n        if Global._check_paradigm(\"openmp\"):\n            self._generate_omp()\n        elif Global._check_paradigm(\"cuda\"):\n            self._generate_cuda()\n        else:\n            raise NotImplementedError\n\n    def generate_omp(self):\n\"\"\"\n        Code generation of CopyProjection object for the openMP paradigm.\n        \"\"\"\n        # Set the projection specific parameters\n        copy_proj_dict = deepcopy(copy_proj_template)\n        for key, value in copy_proj_dict.items():\n            value = value % {\n                'id_proj': self.id,\n                'id_copy': self.projection.id,\n                'float_prec': Global.config['precision']\n            }\n            copy_proj_dict[key] = value\n\n        # Update specific template\n        self._specific_template.update(copy_proj_dict)\n\n        # OMP code if more then one thread\n        if Global.config['num_threads'] &gt; 1:\n            omp_code = '#pragma omp for private(sum)' if self.post.size &gt; Global.OMP_MIN_NB_NEURONS else ''\n        else:\n            omp_code = \"\"\n\n        # PSP\n        psp = self.synapse_type.description['psp']['cpp']  % {\n            'id_pre': self.pre.id,\n            'id_post': self.post.id,\n            'local_index':'[i][j]',\n            'global_index': '[i]',\n            'pre_index': '[pre_rank[i][j]]',\n            'post_index': '[post_rank[i]]',\n            'pre_prefix': 'pop'+str(self.pre.id)+'.',\n            'post_prefix': 'pop'+str(self.post.id)+'.'}\n        psp = psp.replace('rk_pre', 'pre_rank[i][j]').replace(';', '')\n\n        # Take delays into account if any\n        if self.delays &gt; Global.config['dt']:\n            psp = psp.replace(\n                'pop%(id_pre)s.r[rk_pre]' % {'id_pre': self.pre.id},\n                'pop%(id_pre)s._delayed_r[delay-1][rk_pre]' % {'id_pre': self.pre.id}\n                # TODO HD: wouldn't it be much better to reduce delay globaly, instead of the substraction here???\n            )\n\n        # Select template for operation to be performed: sum, max, min, mean\n        try:\n            sum_code = copy_sum_template[self.synapse_type.operation]\n        except KeyError:\n            Global._error(\"CopyProjection: the operation \", self.synapse_type.operation, ' is not available.')\n\n        # Finalize code\n        self.generator['omp']['body_compute_psp'] = sum_code % {\n            'id_proj': self.id, 'target': self.target,\n            'id_pre': self.pre.id, 'name_pre': self.pre.name,\n            'id_post': self.post.id, 'name_post': self.post.name,\n            'id': self.projection.id,\n            'float_prec': Global.config['precision'],\n            'omp_code': omp_code,\n            'psp': psp\n        }\n\n    def _generate_cuda(self):\n\"\"\"\n        Code generation of CopyProjection object for the CUDA paradigm.\n\n        Note: currently not implemented (TODO HD)\n        \"\"\"\n        raise NotImplementedError\n\n    ##############################\n    ## Override useless methods\n    ##############################\n    def _data(self):\n        \"Disable saving.\"\n        desc = {}\n        desc['post_ranks'] = self.post_ranks\n        desc['attributes'] = self.attributes\n        desc['parameters'] = self.parameters\n        desc['variables'] = self.variables\n\n        desc['dendrites'] = []\n        desc['number_of_synapses'] = 0\n        return desc\n\n    def save_connectivity(self, filename):\n        \"Not available.\"\n        Global._warning('Copied projections can not be saved.')\n    def save(self, filename):\n        \"Not available.\"\n        Global._warning('Copied projections can not be saved.')\n    def load(self, filename):\n        \"Not available.\"\n        Global._warning('Copied projections can not be loaded.')\n    def receptive_fields(self, variable = 'w', in_post_geometry = True):\n        \"Not available.\"\n        Global._warning('Copied projections can not display receptive fields.')\n    def connectivity_matrix(self, fill=0.0):\n        \"Not available.\"\n        Global._warning('Copied projections can not display connectivity matrices.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.__init__","title":"<code>__init__(pre, post, target, psp='pre.r * w', operation='sum', name=None, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> pre         \u2013          <p>pre-synaptic population (either its name or a <code>Population</code> object).</p> </li> <li> post         \u2013          <p>post-synaptic population (either its name or a <code>Population</code> object).</p> </li> <li> target         \u2013          <p>type of the connection</p> </li> <li> psp         \u2013          <p>continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: <code>w*pre.r</code>).</p> </li> <li> operation         \u2013          <p>operation (sum, max, min, mean) performed by the kernel (default: sum).</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Copy.py</code> <pre><code>def __init__(self, pre, post, target, psp=\"pre.r * w\", operation=\"sum\", name=None, copied=False):\n\"\"\"\n    :param pre: pre-synaptic population (either its name or a ``Population`` object).\n    :param post: post-synaptic population (either its name or a ``Population`` object).\n    :param target: type of the connection\n    :param psp: continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: ``w*pre.r``).\n    :param operation: operation (sum, max, min, mean) performed by the kernel (default: sum).\n    \"\"\"\n\n    # Create the description, but it will not be used for generation\n    Projection.__init__(\n        self,\n        pre=pre,\n        post=post,\n        target=target,\n        synapse = SharedSynapse(psp=psp, operation=operation),\n        name=name,\n        copied=copied\n    )\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.connect_copy","title":"<code>connect_copy(projection)</code>","text":"<p>Parameters:</p> <ul> <li> projection         \u2013          <p>Existing projection to copy.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Copy.py</code> <pre><code>def connect_copy(self, projection):\n\"\"\"\n    :param projection: Existing projection to copy.\n    \"\"\"\n    self.projection = projection\n\n    # Sanity checks\n    if not isinstance(self.projection, Projection):\n        Global._error('Copy: You must provide an existing projection to copy().')\n\n    if isinstance(self.projection, (Convolution, Pooling)):\n        Global._error('Copy: You can only copy regular projections, not shared projections.')\n\n    if not self.pre.geometry == self.projection.pre.geometry or not self.post.geometry == self.projection.post.geometry:\n        Global._error('Copy: When copying a projection, the geometries must be the same.')\n\n    # Dummy weights\n    self.weights = None\n    self.pre_coordinates = []\n\n    # Finish building the synapses\n    self._create()\n\n    return self\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.connectivity_matrix","title":"<code>connectivity_matrix(fill=0.0)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Copy.py</code> <pre><code>def connectivity_matrix(self, fill=0.0):\n    \"Not available.\"\n    Global._warning('Copied projections can not display connectivity matrices.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.generate_omp","title":"<code>generate_omp()</code>","text":"<p>Code generation of CopyProjection object for the openMP paradigm.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Copy.py</code> <pre><code>def generate_omp(self):\n\"\"\"\n    Code generation of CopyProjection object for the openMP paradigm.\n    \"\"\"\n    # Set the projection specific parameters\n    copy_proj_dict = deepcopy(copy_proj_template)\n    for key, value in copy_proj_dict.items():\n        value = value % {\n            'id_proj': self.id,\n            'id_copy': self.projection.id,\n            'float_prec': Global.config['precision']\n        }\n        copy_proj_dict[key] = value\n\n    # Update specific template\n    self._specific_template.update(copy_proj_dict)\n\n    # OMP code if more then one thread\n    if Global.config['num_threads'] &gt; 1:\n        omp_code = '#pragma omp for private(sum)' if self.post.size &gt; Global.OMP_MIN_NB_NEURONS else ''\n    else:\n        omp_code = \"\"\n\n    # PSP\n    psp = self.synapse_type.description['psp']['cpp']  % {\n        'id_pre': self.pre.id,\n        'id_post': self.post.id,\n        'local_index':'[i][j]',\n        'global_index': '[i]',\n        'pre_index': '[pre_rank[i][j]]',\n        'post_index': '[post_rank[i]]',\n        'pre_prefix': 'pop'+str(self.pre.id)+'.',\n        'post_prefix': 'pop'+str(self.post.id)+'.'}\n    psp = psp.replace('rk_pre', 'pre_rank[i][j]').replace(';', '')\n\n    # Take delays into account if any\n    if self.delays &gt; Global.config['dt']:\n        psp = psp.replace(\n            'pop%(id_pre)s.r[rk_pre]' % {'id_pre': self.pre.id},\n            'pop%(id_pre)s._delayed_r[delay-1][rk_pre]' % {'id_pre': self.pre.id}\n            # TODO HD: wouldn't it be much better to reduce delay globaly, instead of the substraction here???\n        )\n\n    # Select template for operation to be performed: sum, max, min, mean\n    try:\n        sum_code = copy_sum_template[self.synapse_type.operation]\n    except KeyError:\n        Global._error(\"CopyProjection: the operation \", self.synapse_type.operation, ' is not available.')\n\n    # Finalize code\n    self.generator['omp']['body_compute_psp'] = sum_code % {\n        'id_proj': self.id, 'target': self.target,\n        'id_pre': self.pre.id, 'name_pre': self.pre.name,\n        'id_post': self.post.id, 'name_post': self.post.name,\n        'id': self.projection.id,\n        'float_prec': Global.config['precision'],\n        'omp_code': omp_code,\n        'psp': psp\n    }\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.load","title":"<code>load(filename)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Copy.py</code> <pre><code>def load(self, filename):\n    \"Not available.\"\n    Global._warning('Copied projections can not be loaded.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.receptive_fields","title":"<code>receptive_fields(variable='w', in_post_geometry=True)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Copy.py</code> <pre><code>def receptive_fields(self, variable = 'w', in_post_geometry = True):\n    \"Not available.\"\n    Global._warning('Copied projections can not display receptive fields.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.save","title":"<code>save(filename)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Copy.py</code> <pre><code>def save(self, filename):\n    \"Not available.\"\n    Global._warning('Copied projections can not be saved.')\n</code></pre>"},{"location":"API/Convolution.html#ANNarchy.extensions.convolution.Copy.Copy.save_connectivity","title":"<code>save_connectivity(filename)</code>","text":"<p>Not available.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/convolution/Copy.py</code> <pre><code>def save_connectivity(self, filename):\n    \"Not available.\"\n    Global._warning('Copied projections can not be saved.')\n</code></pre>"},{"location":"API/Dendrite.html","title":"Dendrite class","text":"<p>A <code>Dendrite</code> is a sub-group of a <code>Projection</code>, gathering the synapses between the pre-synaptic population and a single post-synaptic neuron.</p>"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite","title":"<code>ANNarchy.core.Dendrite.Dendrite</code>","text":"<p>             Bases: <code>object</code></p> <p>A <code>Dendrite</code> is a sub-group of a <code>Projection</code>, gathering the synapses between the pre-synaptic population and a single post-synaptic neuron.</p> <p>It can not be created directly, only through a call to <code>Projection.dendrite(rank)</code>:</p> <pre><code>dendrite = proj.dendrite(6)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Dendrite.py</code> <pre><code>class Dendrite(object):\n\"\"\"\n    A ``Dendrite`` is a sub-group of a ``Projection``, gathering the synapses between the pre-synaptic population and a single post-synaptic neuron.\n\n    It can not be created directly, only through a call to ``Projection.dendrite(rank)``:\n\n    ```python\n    dendrite = proj.dendrite(6)\n    ```\n    \"\"\"\n    def __init__(self, proj, post_rank, idx):\n\n        self.post_rank = post_rank\n        self.idx = idx\n        self.proj = proj\n        self.pre = proj.pre\n\n        self.target = self.proj.target\n\n        self.attributes = self.proj.attributes\n        self.parameters = self.proj.parameters\n        self.variables = self.proj.variables\n\n    @property\n    def size(self):\n\"\"\"\n        Number of synapses.\n        \"\"\"\n        if self.proj.cyInstance:\n            return self.proj.cyInstance.dendrite_size(self.idx)\n        return 0\n\n    @property\n    def pre_ranks(self):\n\"\"\"\n        List of ranks of pre-synaptic neurons.\n        \"\"\"\n        if self.proj.cyInstance:\n            return self.proj.cyInstance.pre_rank(self.idx)\n        return []\n\n    def __len__(self):\n        # Number of synapses.\n\n        return self.size\n\n    @property\n    def synapses(self):\n\"\"\"\n        Iteratively returns the synapses corresponding to this dendrite.\n        \"\"\"\n        for n in self.pre_ranks:\n            yield IndividualSynapse(self, n)\n\n    def synapse(self, pos):\n\"\"\"\n        Returns the synapse coming from the corresponding presynaptic neuron.\n\n        :param pos: can be either the rank or the coordinates of the presynaptic neuron\n        \"\"\"\n        if isinstance(pos, int):\n            rank = pos\n        else:\n            rank = self.proj.pre.rank_from_coordinates(pos)\n\n        if rank in self.pre_ranks:\n            return IndividualSynapse(self, rank)\n        else:\n            Global._error(\" The neuron of rank \"+ str(rank) + \" has no synapse in this dendrite.\")\n            return None\n\n    # Iterators\n    def __getitem__(self, *args, **kwds):\n        # Returns the synapse of the given position in the presynaptic population.\n        # If only one argument is given, it is a rank. If it is a tuple, it is coordinates.\n\n        if len(args) == 1:\n            return self.synapse(args[0])\n        return self.synapse(args)\n\n    def __iter__(self):\n        # Returns iteratively each synapse in the dendrite in ascending pre-synaptic rank order.\n        for n in self.pre_ranks:\n            yield IndividualSynapse(self, n)\n\n    #########################\n    ### Access to attributes\n    #########################\n    def __getattr__(self, name):\n        # Method called when accessing an attribute.\n        if name == 'proj':\n            return object.__getattribute__(self, name)\n        elif hasattr(self, 'proj'):\n            if name == 'rank': # TODO: remove 'rank' in a future version\n                Global._warning(\"Dendrite.rank: the attribute is deprecated, use Dendrite.pre_ranks instead.\")\n                return self.proj.cyInstance.pre_rank(self.idx)\n\n            elif name == 'pre_rank':\n                return self.proj.cyInstance.pre_rank(self.idx)\n\n            elif name == 'delay':\n                if self.proj.uniform_delay == -1:\n                    return [d*Global.config['dt'] for d in self.proj.cyInstance.get_dendrite_delay(self.idx)]\n                else:\n                    return self.proj.max_delay * Global.config['dt']\n\n            elif name == \"w\" and self.proj._has_single_weight():\n                return self.proj.cyInstance.get_global_attribute(name, Global.config[\"precision\"])\n\n            elif name in self.proj.attributes:\n                # Determine C++ data type\n                ctype = None\n                for var in self.proj.synapse_type.description['variables']+self.proj.synapse_type.description['parameters']:\n                    if var['name'] == name:\n                        ctype = var['ctype']\n\n                if name in self.proj.synapse_type.description['local']:\n                    return self.proj.cyInstance.get_local_attribute_row(name, self.idx, ctype)\n                elif name in self.proj.synapse_type.description['semiglobal']:\n                    return self.proj.cyInstance.get_semiglobal_attribute(name, self.idx, ctype)\n                else:\n                    return self.proj.cyInstance.get_global_attribute(name, ctype)\n            else:\n                return object.__getattribute__(self, name)\n        else:\n            return object.__getattribute__(self, name)\n\n    def __setattr__(self, name, value):\n        # Method called when setting an attribute.\n        if name == 'proj':\n            object.__setattr__(self, 'proj', value)\n        elif name == 'attributes':\n            object.__setattr__(self, 'attributes', value)\n        elif hasattr(self, 'proj'):\n            if name in self.proj.attributes:\n                # Determine C++ data type\n                ctype = None\n                for var in self.proj.synapse_type.description['variables']+self.proj.synapse_type.description['parameters']:\n                    if var['name'] == name:\n                        ctype = var['ctype']\n\n                if name in self.proj.synapse_type.description['local']:\n                    if isinstance(value, (np.ndarray, list)):\n                        self.proj.cyInstance.set_local_attribute_row(name, self.idx, value, ctype)\n                    else:\n                        self.proj.cyInstance.set_local_attribute_row(name, self.idx, value * np.ones(self.size), ctype)\n\n                elif name in self.proj.synapse_type.description['semiglobal']:\n                    self.proj.cyInstance.set_semiglobal_attribute(name, self.idx, value, ctype)\n\n                else:\n                    raise Global._error(\"Projection attributes marked as *projection* should not be updated through dendrites.\")\n            else:\n                object.__setattr__(self, name, value)\n        else:\n            object.__setattr__(self, name, value)\n\n    def set(self, value):\n\"\"\"\n        Sets the value of a parameter/variable of all synapses.\n\n        Example:\n\n        ```python\n        dendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } )\n        ```\n\n        :param value: a dictionary containing the parameter/variable names as keys.\n        \"\"\"\n        for key, value in value.items():\n            # sanity check and then forward to __setattr__\n            if key in self.attributes:\n                setattr(self, key, value)\n            else:\n                Global._error(\"Dendrite has no parameter/variable called\", key)\n\n    def get(self, name):\n\"\"\"\n        Returns the value of a variable/parameter.\n\n        Example:\n\n        ```python\n        dendrite.get('w')\n        ```\n\n        :param name: name of the parameter/variable.\n        \"\"\"\n        if name == 'rank':\n            Global._warning(\"Dendrite.get('rank'): the attribute is deprecated, use Dendrite.pre_ranks instead.\")\n            return self.proj.cyInstance.pre_rank(self.idx)\n        elif name == 'pre_ranks':\n            return self.proj.cyInstance.pre_rank(self.idx)\n        elif name in self.attributes:\n            return getattr(self, name)\n        else:\n            Global._error(\"Dendrite has no parameter/variable called\", name)\n\n\n    #########################\n    ### Formatting\n    #########################\n    def receptive_field(self, variable='w', fill=0.0):\n\"\"\"\n        Returns the given variable as a receptive field.\n\n        A Numpy array of the same geometry as the pre-synaptic population is returned. \n        Non-existing synapses are replaced by zeros (or the value ``fill``).\n\n        :param variable: name of the variable (default = 'w')\n        :param fill: value to use when a synapse does not exist (default: 0.0).\n        \"\"\"\n        values = getattr(self.proj.cyInstance, 'get_dendrite_'+variable)(self.idx)\n        pre_ranks = self.proj.cyInstance.pre_rank( self.idx )\n\n        m = fill * np.ones( self.pre.size )\n        m[pre_ranks] = values\n\n        return m.reshape(self.pre.geometry)\n\n\n    #########################\n    ### Structural plasticity\n    #########################\n    def create_synapse(self, rank, w=0.0, delay=0):\n\"\"\"\n        Creates a synapse for this dendrite with the given pre-synaptic neuron.\n\n        :param rank: rank of the pre-synaptic neuron\n        :param w: synaptic weight (defalt: 0.0).\n        :param delay: synaptic delay (default = dt)\n        \"\"\"\n        if not Global.config['structural_plasticity']:\n            Global._error('\"structural_plasticity\" has not been set to True in setup(), can not add the synapse.')\n            return\n\n        if self.proj.cyInstance.dendrite_index(self.post_rank, rank) != -1:\n            Global._error('the synapse of rank ' + str(rank) + ' already exists.')\n            return\n\n        # Set default values for the additional variables\n        extra_attributes = {}\n        for var in self.proj.synapse_type.description['parameters'] + self.proj.synapse_type.description['variables']:\n            if not var['name'] in ['w', 'delay'] and  var['name'] in self.proj.synapse_type.description['local']:\n                if not isinstance(self.proj.init[var['name']], (int, float, bool)):\n                    init = var['init']\n                else:\n                    init = self.proj.init[var['name']]\n                extra_attributes[var['name']] = init\n\n        try:\n            self.proj.cyInstance.add_synapse(self.post_rank, rank, w, int(delay/Global.config['dt']), **extra_attributes)\n        except Exception as e:\n            Global._print(e)\n\n    def prune_synapse(self, rank):\n\"\"\"\n        Removes the synapse with the given pre-synaptic neuron from the dendrite.\n\n        :param rank: rank of the pre-synaptic neuron\n        \"\"\"\n        if not Global.config['structural_plasticity']:\n            Global._error('\"structural_plasticity\" has not been set to True in setup(), can not remove the synapse.')\n            return\n\n        if not rank in self.pre_ranks:\n            Global._error('the synapse with the pre-synaptic neuron of rank ' + str(rank) + ' did not already exist.')\n            return\n\n        self.proj.cyInstance.remove_synapse(self.post_rank, rank)\n</code></pre>"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.pre_ranks","title":"<code>pre_ranks</code>  <code>property</code>","text":"<p>List of ranks of pre-synaptic neurons.</p>"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.size","title":"<code>size</code>  <code>property</code>","text":"<p>Number of synapses.</p>"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.synapses","title":"<code>synapses</code>  <code>property</code>","text":"<p>Iteratively returns the synapses corresponding to this dendrite.</p>"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.create_synapse","title":"<code>create_synapse(rank, w=0.0, delay=0)</code>","text":"<p>Creates a synapse for this dendrite with the given pre-synaptic neuron.</p> <p>Parameters:</p> <ul> <li> rank         \u2013          <p>rank of the pre-synaptic neuron</p> </li> <li> w         \u2013          <p>synaptic weight (defalt: 0.0).</p> </li> <li> delay         \u2013          <p>synaptic delay (default = dt)</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Dendrite.py</code> <pre><code>def create_synapse(self, rank, w=0.0, delay=0):\n\"\"\"\n    Creates a synapse for this dendrite with the given pre-synaptic neuron.\n\n    :param rank: rank of the pre-synaptic neuron\n    :param w: synaptic weight (defalt: 0.0).\n    :param delay: synaptic delay (default = dt)\n    \"\"\"\n    if not Global.config['structural_plasticity']:\n        Global._error('\"structural_plasticity\" has not been set to True in setup(), can not add the synapse.')\n        return\n\n    if self.proj.cyInstance.dendrite_index(self.post_rank, rank) != -1:\n        Global._error('the synapse of rank ' + str(rank) + ' already exists.')\n        return\n\n    # Set default values for the additional variables\n    extra_attributes = {}\n    for var in self.proj.synapse_type.description['parameters'] + self.proj.synapse_type.description['variables']:\n        if not var['name'] in ['w', 'delay'] and  var['name'] in self.proj.synapse_type.description['local']:\n            if not isinstance(self.proj.init[var['name']], (int, float, bool)):\n                init = var['init']\n            else:\n                init = self.proj.init[var['name']]\n            extra_attributes[var['name']] = init\n\n    try:\n        self.proj.cyInstance.add_synapse(self.post_rank, rank, w, int(delay/Global.config['dt']), **extra_attributes)\n    except Exception as e:\n        Global._print(e)\n</code></pre>"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.get","title":"<code>get(name)</code>","text":"<p>Returns the value of a variable/parameter.</p> <p>Example:</p> <pre><code>dendrite.get('w')\n</code></pre> <p>Parameters:</p> <ul> <li> name         \u2013          <p>name of the parameter/variable.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Dendrite.py</code> <pre><code>def get(self, name):\n\"\"\"\n    Returns the value of a variable/parameter.\n\n    Example:\n\n    ```python\n    dendrite.get('w')\n    ```\n\n    :param name: name of the parameter/variable.\n    \"\"\"\n    if name == 'rank':\n        Global._warning(\"Dendrite.get('rank'): the attribute is deprecated, use Dendrite.pre_ranks instead.\")\n        return self.proj.cyInstance.pre_rank(self.idx)\n    elif name == 'pre_ranks':\n        return self.proj.cyInstance.pre_rank(self.idx)\n    elif name in self.attributes:\n        return getattr(self, name)\n    else:\n        Global._error(\"Dendrite has no parameter/variable called\", name)\n</code></pre>"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.prune_synapse","title":"<code>prune_synapse(rank)</code>","text":"<p>Removes the synapse with the given pre-synaptic neuron from the dendrite.</p> <p>Parameters:</p> <ul> <li> rank         \u2013          <p>rank of the pre-synaptic neuron</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Dendrite.py</code> <pre><code>def prune_synapse(self, rank):\n\"\"\"\n    Removes the synapse with the given pre-synaptic neuron from the dendrite.\n\n    :param rank: rank of the pre-synaptic neuron\n    \"\"\"\n    if not Global.config['structural_plasticity']:\n        Global._error('\"structural_plasticity\" has not been set to True in setup(), can not remove the synapse.')\n        return\n\n    if not rank in self.pre_ranks:\n        Global._error('the synapse with the pre-synaptic neuron of rank ' + str(rank) + ' did not already exist.')\n        return\n\n    self.proj.cyInstance.remove_synapse(self.post_rank, rank)\n</code></pre>"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.receptive_field","title":"<code>receptive_field(variable='w', fill=0.0)</code>","text":"<p>Returns the given variable as a receptive field.</p> <p>A Numpy array of the same geometry as the pre-synaptic population is returned.  Non-existing synapses are replaced by zeros (or the value <code>fill</code>).</p> <p>Parameters:</p> <ul> <li> variable         \u2013          <p>name of the variable (default = 'w')</p> </li> <li> fill         \u2013          <p>value to use when a synapse does not exist (default: 0.0).</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Dendrite.py</code> <pre><code>def receptive_field(self, variable='w', fill=0.0):\n\"\"\"\n    Returns the given variable as a receptive field.\n\n    A Numpy array of the same geometry as the pre-synaptic population is returned. \n    Non-existing synapses are replaced by zeros (or the value ``fill``).\n\n    :param variable: name of the variable (default = 'w')\n    :param fill: value to use when a synapse does not exist (default: 0.0).\n    \"\"\"\n    values = getattr(self.proj.cyInstance, 'get_dendrite_'+variable)(self.idx)\n    pre_ranks = self.proj.cyInstance.pre_rank( self.idx )\n\n    m = fill * np.ones( self.pre.size )\n    m[pre_ranks] = values\n\n    return m.reshape(self.pre.geometry)\n</code></pre>"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.set","title":"<code>set(value)</code>","text":"<p>Sets the value of a parameter/variable of all synapses.</p> <p>Example:</p> <pre><code>dendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } )\n</code></pre> <p>Parameters:</p> <ul> <li> value         \u2013          <p>a dictionary containing the parameter/variable names as keys.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Dendrite.py</code> <pre><code>def set(self, value):\n\"\"\"\n    Sets the value of a parameter/variable of all synapses.\n\n    Example:\n\n    ```python\n    dendrite.set( { 'tau' : 20, 'w'= Uniform(0.0, 1.0) } )\n    ```\n\n    :param value: a dictionary containing the parameter/variable names as keys.\n    \"\"\"\n    for key, value in value.items():\n        # sanity check and then forward to __setattr__\n        if key in self.attributes:\n            setattr(self, key, value)\n        else:\n            Global._error(\"Dendrite has no parameter/variable called\", key)\n</code></pre>"},{"location":"API/Dendrite.html#ANNarchy.core.Dendrite.Dendrite.synapse","title":"<code>synapse(pos)</code>","text":"<p>Returns the synapse coming from the corresponding presynaptic neuron.</p> <p>Parameters:</p> <ul> <li> pos         \u2013          <p>can be either the rank or the coordinates of the presynaptic neuron</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Dendrite.py</code> <pre><code>def synapse(self, pos):\n\"\"\"\n    Returns the synapse coming from the corresponding presynaptic neuron.\n\n    :param pos: can be either the rank or the coordinates of the presynaptic neuron\n    \"\"\"\n    if isinstance(pos, int):\n        rank = pos\n    else:\n        rank = self.proj.pre.rank_from_coordinates(pos)\n\n    if rank in self.pre_ranks:\n        return IndividualSynapse(self, rank)\n    else:\n        Global._error(\" The neuron of rank \"+ str(rank) + \" has no synapse in this dendrite.\")\n        return None\n</code></pre>"},{"location":"API/IO.html","title":"Saving / Loading","text":""},{"location":"API/IO.html#saving-loading-the-state-of-the-network","title":"Saving / loading the state of the network","text":"<p>To save or load the network state you can use the following methods:</p> <p>Please note that these functions are only usable after the call to <code>ANNarchy.compile()</code>.</p>"},{"location":"API/IO.html#ANNarchy.save","title":"<code>ANNarchy.save(filename, populations=True, projections=True, net_id=0)</code>","text":"<p>Save the current network state (parameters and variables) to a file.</p> <ul> <li> <p>If the extension is '.npz', the data will be saved and compressed using <code>np.savez_compressed</code> (recommended).</p> </li> <li> <p>If the extension is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed.</p> </li> <li> <p>If the extension ends with '.gz', the data will be pickled into a binary file and compressed using gzip.</p> </li> <li> <p>Otherwise, the data will be pickled into a simple binary text file using cPickle.</p> </li> </ul> <p>Warning: The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose.</p> <p>Example:</p> <pre><code>save('results/init.npz')\n\nsave('results/init.data')\n\nsave('results/init.txt.gz')\n\nsave('1000_trials.mat')\n</code></pre> <p>Parameters:</p> <ul> <li> filename         \u2013          <p>filename, may contain relative or absolute path.</p> </li> <li> populations         \u2013          <p>if True, population data will be saved (by default True)</p> </li> <li> projections         \u2013          <p>if True, projection data will be saved (by default True)</p> </li> </ul>"},{"location":"API/IO.html#ANNarchy.load","title":"<code>ANNarchy.load(filename, populations=True, projections=True, pickle_encoding=None, net_id=0)</code>","text":"<p>Loads a saved state of the network.</p> <p>Warning: Matlab data can not be loaded.</p> <p>Example:</p> <pre><code>load('results/network.npz')\n</code></pre> <p>Parameters:</p> <ul> <li> filename         \u2013          <p>the filename with relative or absolute path.</p> </li> <li> populations         \u2013          <p>if True, population data will be loaded (by default True)</p> </li> <li> projections         \u2013          <p>if True, projection data will be loaded (by default True)</p> </li> <li> pickle_encoding         \u2013          <p>optional parameter provided to the pickle.load() method. If set to None the default is used.</p> </li> </ul>"},{"location":"API/IO.html#saving-loading-the-parameters-of-the-network","title":"Saving / loading the parameters of the network","text":""},{"location":"API/IO.html#ANNarchy.save_parameters","title":"<code>ANNarchy.save_parameters(filename, net_id=0)</code>","text":"<p>Saves the global parameters of a network (flag <code>population</code> for neurons, <code>projection</code> for synapses) to a JSON file.</p> <p>Parameters:</p> <ul> <li> filename         \u2013          <p>path to the JSON file.</p> </li> <li> net_id         \u2013          <p>ID of the network (default: 0, the global network).</p> </li> </ul>"},{"location":"API/IO.html#ANNarchy.load_parameters","title":"<code>ANNarchy.load_parameters(filename, global_only=True, verbose=False, net_id=0)</code>","text":"<p>Loads the global parameters of a network (flag <code>population</code> for neurons, <code>projection</code> for synapses) from a JSON file.</p> <p>It is advised to generate the JSON file first with <code>save_parameters()</code> and later edit it manually.</p> <p>A strong restriction is that population/projection names cannot change between saving and loading. By default, they take names such as <code>pop0</code> or <code>proj2</code>, we advise setting explicitly a name in their constructor for readability.</p> <p>If you add a parameter name to the JSON file but it does not exist in te neuron/synapse, it will be silently skipped. Enable <code>verbose=True</code> to see which parameters are effectively changed.</p> <p>If you set <code>global_only</code> to True, you will be able to set values for non-global parameters (e.g. synapse-specific), but a single value will be loaded for all. The JSON file cannot contain arrays.</p> <p>If you want to save/load the value of variables after a simulation, please refer to <code>save()</code> or <code>load()</code>.</p> <p>Parameters:</p> <ul> <li> filename         \u2013          <p>path to the JSON file.</p> </li> <li> global_only         \u2013          <p>True if only global parameters (flags <code>population</code> and <code>projection</code>) should be loaded, the other values are ignored. (default: True)</p> </li> <li> verbose         \u2013          <p>True if the old and new values of the parameters should be printed (default: False).</p> </li> <li> net_id         \u2013          <p>ID of the network (default: 0, the global network).</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>a dictionary of additional parameters not related to populations or projections (keyword <code>network</code> in the JSON file).</p> </li> </ul>"},{"location":"API/Logging.html","title":"Logging with tensorboard","text":"<p>Logging utilities based on tensorboard are provided in the module <code>ANNarchy.extensions.tensorboard</code>, which must be explicitely imported:</p> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.tensorboard import Logger\n</code></pre> <p>The main object in that module is the <code>Logger</code> class.</p>"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger","title":"<code>ANNarchy.extensions.tensorboard.Logger</code>","text":""},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger","title":"<code>Logger</code>","text":"<p>             Bases: <code>object</code></p> <p>Logger class to use tensorboard to visualize ANNarchy simulations. Requires the <code>tensorboardX</code> package (pip install tensorboardX). </p> <p>The Logger class is a thin wrapper around tensorboardX.SummaryWriter, which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io/. Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch.</p> <p>The extension has to be imported explictly:</p> <pre><code>from ANNarchy.extensions.tensorboard import Logger\n</code></pre> <p>The <code>Logger</code> class has to be closed properly at the end of the script, so it is advised to use a context:</p> <pre><code>with Logger() as logger:\n    logger.add_scalar(\"Accuracy\", acc, trial)\n</code></pre> <p>You can also make sure to close it:</p> <pre><code>logger = Logger()\nlogger.add_scalar(\"Accuracy\", acc, trial)\nlogger.close()\n</code></pre> <p>By default, the logs will be written in a subfolder of <code>./runs/</code> (which will be created in the current directory).  The subfolder is a combination of the current datetime and of the hostname, e.g. <code>./runs/Apr22_12-11-22_machine</code>.  You can control these two elements by passing arguments to <code>Logger()</code>:</p> <pre><code>with Logger(logdir=\"/tmp/annarchy\", experiment=\"trial1\"): # logs in /tmp/annarchy/trial1\n</code></pre> <p>The <code>add_*</code> methods allow you to log various structures, such as scalars, images, histograms, figures, etc.</p> <p>A tag should be given to each plot. In the example above, the figure with the accuracy will be labelled \"Accuracy\" in tensorboard.  You can also group plots together with tags such as \"Global performance/Accuracy\", \"Global performance/Error rate\", \"Neural activity/Population 1\", etc.</p> <p>After (or while) logging data within your simulation, run <code>tensorboard</code> in the terminal by specifying the log directory:</p> <pre><code>tensorboard --logdir runs\n</code></pre> <p>TensorboardX enqueues the data in memory before writing to disk. You can force flushing with:</p> <pre><code>logger.flush()\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/tensorboard/Logger.py</code> <pre><code>class Logger(object):\n\"\"\"\n    Logger class to use tensorboard to visualize ANNarchy simulations. Requires the `tensorboardX` package (pip install tensorboardX). \n\n    The Logger class is a thin wrapper around tensorboardX.SummaryWriter, which you could also use directly. The doc is available at &lt;https://tensorboardx.readthedocs.io/&gt;. Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch.\n\n    The extension has to be imported explictly:\n\n    ```python\n    from ANNarchy.extensions.tensorboard import Logger\n    ```\n\n    The ``Logger`` class has to be closed properly at the end of the script, so it is advised to use a context:\n\n    ```python\n    with Logger() as logger:\n        logger.add_scalar(\"Accuracy\", acc, trial)\n    ```\n\n    You can also make sure to close it:\n\n    ```python\n    logger = Logger()\n    logger.add_scalar(\"Accuracy\", acc, trial)\n    logger.close()\n    ```\n\n    By default, the logs will be written in a subfolder of ``./runs/`` (which will be created in the current directory). \n    The subfolder is a combination of the current datetime and of the hostname, e.g. ``./runs/Apr22_12-11-22_machine``. \n    You can control these two elements by passing arguments to ``Logger()``:\n\n    ```python\n    with Logger(logdir=\"/tmp/annarchy\", experiment=\"trial1\"): # logs in /tmp/annarchy/trial1\n    ```\n\n    The ``add_*`` methods allow you to log various structures, such as scalars, images, histograms, figures, etc.\n\n    A tag should be given to each plot. In the example above, the figure with the accuracy will be labelled \"Accuracy\" in tensorboard. \n    You can also group plots together with tags such as \"Global performance/Accuracy\", \"Global performance/Error rate\", \"Neural activity/Population 1\", etc.\n\n    After (or while) logging data within your simulation, run `tensorboard` in the terminal by specifying the log directory:\n\n    ```bash\n    tensorboard --logdir runs\n    ```\n\n    TensorboardX enqueues the data in memory before writing to disk. You can force flushing with:\n\n    ```python\n    logger.flush()\n    ```\n\n    \"\"\"\n\n    def __init__(self, logdir=\"runs/\", experiment=None):\n\"\"\"\n        :param logdir: path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\"\n        :param experiment: name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended.\n        \"\"\"\n        self.logdir = logdir\n        self.experiment = experiment\n\n        # Create the logdir if it does not exist\n        if not os.path.exists(self.logdir):\n            os.makedirs(self.logdir)\n\n        if not experiment:\n            current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n            self.currentlogdir=os.path.join(\n                self.logdir, current_time + '_' + socket.gethostname())\n        else:\n            self.currentlogdir = self.logdir + \"/\" + self.experiment\n\n        print(\"Logging in\", self.currentlogdir)\n\n        self._create_summary_writer()\n\n    def _create_summary_writer(self):\n\n         self._summary = SummaryWriter(self.currentlogdir, comment=\"\", purge_step=None, max_queue=10, flush_secs=10, filename_suffix='', write_to_disk=True)\n\n    # Logging methods\n\n    def add_scalar(self, tag, value, step=None):\n\"\"\"\n        Logs a single scalar value, e.g. a success rate at various stages of learning.\n\n        Example:\n\n        ```python\n        with Logger() as logger:\n            for trial in range(100):\n                simulate(1000.0)\n                accuracy = ...\n                logger.add_scalar(\"Accuracy\", accuracy, trial)\n        ```\n\n        :param tag: name of the figure in tensorboard.\n        :param value: value.\n        :param step: time index.\n        \"\"\"\n\n        self._summary.add_scalar(tag=tag, scalar_value=value, global_step=step, walltime=None)\n\n    def add_scalars(self, tag, value, step=None):\n\"\"\"\n        Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities.\n\n        Example:\n\n        ```python\n        with Logger() as logger:\n            for trial in range(100):\n                simulate(1000.0)\n                act1 = pop.r[0]\n                act2 = pop.r[1]\n                logger.add_scalars(\n                    \"Accuracy\", \n                    {'First neuron': act1, 'Second neuron': act2}, \n                    trial)\n        ```\n\n        :param tag: name of the figure in tensorboard.\n        :param value: dictionary of values.\n        :param step: time index.\n        \"\"\"\n\n        self._summary.add_scalars(main_tag=tag, tag_scalar_dict=value, global_step=step, walltime=None)\n\n    def add_image(self, tag, img, step=None, equalize=False):\n\"\"\"\n        Logs an image.\n\n        The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values.\n\n        Example::\n\n        ```python\n        with Logger() as logger:\n            for trial in range(100):\n                simulate(1000.0)\n                img = pop.r.reshape((10, 10))\n                logger.add_image(\"Population / Firing rate\", img, trial, equalize=True)\n        ```\n\n        :param tag: name of the figure in tensorboard.\n        :param img: array for the image.\n        :param step: time index.\n        :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array.\n        \"\"\"\n        if img.ndim ==2:\n            if equalize:  \n                img = img.astype(np.float)              \n                img = (img - img.min())/(img.max() - img.min())\n\n            self._summary.add_image(tag=tag, img_tensor=img, global_step=step, walltime=None, dataformats='HW')\n\n        elif img.ndim == 3:\n            if not img.shape[2] == 3:\n                Global._error(\"Logger.add_image: color images must be of shape (H, W, 3).\")\n\n            if equalize:   \n                img = np.array(img).astype(np.float)         \n                img = (img - img.min())/(img.max() - img.min())\n\n            self._summary.add_image(tag=tag, img_tensor=img, global_step=step, walltime=None, dataformats='HWC')\n\n        else:\n            Global._error(\"Logger.add_image: images must be of shape (H, W) or (H, W, 3).\")\n\n    def add_images(self, tag, img, step=None, equalize=False, equalize_per_image=False):\n\"\"\"\n        Logs a set of images (e.g. receptive fields).\n\n        The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values.\n\n        Example:\n\n        ```python\n        with Logger() as logger:\n            for trial in range(100):\n                simulate(1000.0)\n                weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons\n                logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True)\n        ```\n\n        :param tag: name of the figure in tensorboard.\n        :param img: array for the images.\n        :param step: time index.\n        :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array.\n        :param equalize_per_image: whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False.\n\n        \"\"\"\n        if img.ndim == 3:\n            img = np.expand_dims(img, axis=3)\n\n        if equalize:   \n            img = np.array(img).astype(np.float) \n            if not equalize_per_image:        \n                img = (img - img.min())/(img.max() - img.min())\n            else:\n                for i in range(img.shape[0]):\n                    img[i,...] = (img[i,...] - img[i,...].min())/(img[i,...].max() - img[i,...].min())\n\n        self._summary.add_images(tag=tag, img_tensor=img, global_step=step, walltime=None, dataformats='NHWC')\n\n    def add_parameters(self, params, metrics):\n\"\"\"\n        Logs parameters of a simulation.\n\n        This should be run only once per simulation, generally at the end. \n        This allows to compare different runs of the same network using \n        different parameter values and study how they influence the global output metrics, \n        such as accuracy, error rate, reaction speed, etc.\n\n        Example:\n\n        ```python\n        with Logger() as logger:\n            # ...\n            logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy})\n        ```\n\n        :param params: dictionary of parameters.\n        :param metrics: dictionary of metrics.\n        \"\"\"\n\n        self._summary.add_hparams(params, metrics)\n\n    def add_histogram(self, tag, hist, step=None):\n\"\"\"\n        Logs an histogram.\n\n        Example:\n\n        ```python\n        with Logger() as logger:\n            for trial in range(100):\n                simulate(1000.0)\n                weights= proj.w.flatten()\n                logger.add_histogram(\"Weight distribution\", weights, trial)\n        ```\n\n\n        :param tag: name of the figure in tensorboard.\n        :param hist: a list or 1D numpy array of values.\n        :param step: time index.\n        \"\"\"\n\n        self._summary.add_histogram(tag, hist, step)\n\n    def add_figure(self, tag, figure, step=None, close=True):\n\"\"\"\n        Logs a Matplotlib figure.\n\n        Example:\n\n        ```python\n        with Logger() as logger:\n            for trial in range(100):\n                simulate(1000.0)\n                fig = plt.figure()\n                plt.plot(pop.r)\n                logger.add_figure(\"Activity\", fig, trial)\n        ```\n\n        :param tag: name of the image in tensorboard.\n        :param figure: a list or 1D numpy array of values.\n        :param step: time index.\n        :param close: whether the logger will close the figure when done (default: True).\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n        import matplotlib.backends.backend_agg as plt_backend_agg\n        canvas = plt_backend_agg.FigureCanvasAgg(figure)\n        canvas.draw()\n        data = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n        w, h = figure.canvas.get_width_height()\n        image_hwc = data.reshape([h, w, 4])[:, :, 0:3]\n        image_chw = np.moveaxis(image_hwc, source=2, destination=0)\n        if close:\n            plt.close(figure)\n        self._summary.add_image(tag, image_chw, step)\n\n    # Resource management\n    def flush(self):\n        \"Forces the logged data to be flushed to disk.\"\n        self._summary.flush()\n\n    def close(self):\n        \"Closes the logger.\"\n        self._summary.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n</code></pre>"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.__init__","title":"<code>__init__(logdir='runs/', experiment=None)</code>","text":"<p>Parameters:</p> <ul> <li> logdir         \u2013          <p>path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\"</p> </li> <li> experiment         \u2013          <p>name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/tensorboard/Logger.py</code> <pre><code>def __init__(self, logdir=\"runs/\", experiment=None):\n\"\"\"\n    :param logdir: path (absolute or relative) to the logging directory. Subfolders will be created for each individual run. The default is \"runs/\"\n    :param experiment: name of the subfolder for the current run. By default, it is a combination of the current time and the hostname (e.g. Apr22_12-11-22_machine). If you reuse an experiment name, the data will be appended.\n    \"\"\"\n    self.logdir = logdir\n    self.experiment = experiment\n\n    # Create the logdir if it does not exist\n    if not os.path.exists(self.logdir):\n        os.makedirs(self.logdir)\n\n    if not experiment:\n        current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n        self.currentlogdir=os.path.join(\n            self.logdir, current_time + '_' + socket.gethostname())\n    else:\n        self.currentlogdir = self.logdir + \"/\" + self.experiment\n\n    print(\"Logging in\", self.currentlogdir)\n\n    self._create_summary_writer()\n</code></pre>"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_figure","title":"<code>add_figure(tag, figure, step=None, close=True)</code>","text":"<p>Logs a Matplotlib figure.</p> <p>Example:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        fig = plt.figure()\n        plt.plot(pop.r)\n        logger.add_figure(\"Activity\", fig, trial)\n</code></pre> <p>Parameters:</p> <ul> <li> tag         \u2013          <p>name of the image in tensorboard.</p> </li> <li> figure         \u2013          <p>a list or 1D numpy array of values.</p> </li> <li> step         \u2013          <p>time index.</p> </li> <li> close         \u2013          <p>whether the logger will close the figure when done (default: True).</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/tensorboard/Logger.py</code> <pre><code>def add_figure(self, tag, figure, step=None, close=True):\n\"\"\"\n    Logs a Matplotlib figure.\n\n    Example:\n\n    ```python\n    with Logger() as logger:\n        for trial in range(100):\n            simulate(1000.0)\n            fig = plt.figure()\n            plt.plot(pop.r)\n            logger.add_figure(\"Activity\", fig, trial)\n    ```\n\n    :param tag: name of the image in tensorboard.\n    :param figure: a list or 1D numpy array of values.\n    :param step: time index.\n    :param close: whether the logger will close the figure when done (default: True).\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n    import matplotlib.backends.backend_agg as plt_backend_agg\n    canvas = plt_backend_agg.FigureCanvasAgg(figure)\n    canvas.draw()\n    data = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n    w, h = figure.canvas.get_width_height()\n    image_hwc = data.reshape([h, w, 4])[:, :, 0:3]\n    image_chw = np.moveaxis(image_hwc, source=2, destination=0)\n    if close:\n        plt.close(figure)\n    self._summary.add_image(tag, image_chw, step)\n</code></pre>"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_histogram","title":"<code>add_histogram(tag, hist, step=None)</code>","text":"<p>Logs an histogram.</p> <p>Example:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        weights= proj.w.flatten()\n        logger.add_histogram(\"Weight distribution\", weights, trial)\n</code></pre> <p>Parameters:</p> <ul> <li> tag         \u2013          <p>name of the figure in tensorboard.</p> </li> <li> hist         \u2013          <p>a list or 1D numpy array of values.</p> </li> <li> step         \u2013          <p>time index.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/tensorboard/Logger.py</code> <pre><code>def add_histogram(self, tag, hist, step=None):\n\"\"\"\n    Logs an histogram.\n\n    Example:\n\n    ```python\n    with Logger() as logger:\n        for trial in range(100):\n            simulate(1000.0)\n            weights= proj.w.flatten()\n            logger.add_histogram(\"Weight distribution\", weights, trial)\n    ```\n\n\n    :param tag: name of the figure in tensorboard.\n    :param hist: a list or 1D numpy array of values.\n    :param step: time index.\n    \"\"\"\n\n    self._summary.add_histogram(tag, hist, step)\n</code></pre>"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_image","title":"<code>add_image(tag, img, step=None, equalize=False)</code>","text":"<p>Logs an image.</p> <p>The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter <code>equalize</code> forces the values to be between 0 and 1 by equalizing using the min/max values.</p> <p>Example::</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        img = pop.r.reshape((10, 10))\n        logger.add_image(\"Population / Firing rate\", img, trial, equalize=True)\n</code></pre> <p>Parameters:</p> <ul> <li> tag         \u2013          <p>name of the figure in tensorboard.</p> </li> <li> img         \u2013          <p>array for the image.</p> </li> <li> step         \u2013          <p>time index.</p> </li> <li> equalize         \u2013          <p>rescales the pixels between 0 and 1 using the min and max values of the array.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/tensorboard/Logger.py</code> <pre><code>def add_image(self, tag, img, step=None, equalize=False):\n\"\"\"\n    Logs an image.\n\n    The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values.\n\n    Example::\n\n    ```python\n    with Logger() as logger:\n        for trial in range(100):\n            simulate(1000.0)\n            img = pop.r.reshape((10, 10))\n            logger.add_image(\"Population / Firing rate\", img, trial, equalize=True)\n    ```\n\n    :param tag: name of the figure in tensorboard.\n    :param img: array for the image.\n    :param step: time index.\n    :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array.\n    \"\"\"\n    if img.ndim ==2:\n        if equalize:  \n            img = img.astype(np.float)              \n            img = (img - img.min())/(img.max() - img.min())\n\n        self._summary.add_image(tag=tag, img_tensor=img, global_step=step, walltime=None, dataformats='HW')\n\n    elif img.ndim == 3:\n        if not img.shape[2] == 3:\n            Global._error(\"Logger.add_image: color images must be of shape (H, W, 3).\")\n\n        if equalize:   \n            img = np.array(img).astype(np.float)         \n            img = (img - img.min())/(img.max() - img.min())\n\n        self._summary.add_image(tag=tag, img_tensor=img, global_step=step, walltime=None, dataformats='HWC')\n\n    else:\n        Global._error(\"Logger.add_image: images must be of shape (H, W) or (H, W, 3).\")\n</code></pre>"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_images","title":"<code>add_images(tag, img, step=None, equalize=False, equalize_per_image=False)</code>","text":"<p>Logs a set of images (e.g. receptive fields).</p> <p>The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter <code>equalize</code> forces the values to be between 0 and 1 by equalizing using the min/max values.</p> <p>Example:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons\n        logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True)\n</code></pre> <p>Parameters:</p> <ul> <li> tag         \u2013          <p>name of the figure in tensorboard.</p> </li> <li> img         \u2013          <p>array for the images.</p> </li> <li> step         \u2013          <p>time index.</p> </li> <li> equalize         \u2013          <p>rescales the pixels between 0 and 1 using the min and max values of the array.</p> </li> <li> equalize_per_image         \u2013          <p>whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/tensorboard/Logger.py</code> <pre><code>def add_images(self, tag, img, step=None, equalize=False, equalize_per_image=False):\n\"\"\"\n    Logs a set of images (e.g. receptive fields).\n\n    The numpy array must be of size (number, height, width) for monochrome images or (number, height, width, 3) for colored images. The values should either be integers between 0 and 255 or floats between 0 and 1. The parameter ``equalize`` forces the values to be between 0 and 1 by equalizing using the min/max values.\n\n    Example:\n\n    ```python\n    with Logger() as logger:\n        for trial in range(100):\n            simulate(1000.0)\n            weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons\n            logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True)\n    ```\n\n    :param tag: name of the figure in tensorboard.\n    :param img: array for the images.\n    :param step: time index.\n    :param equalize: rescales the pixels between 0 and 1 using the min and max values of the array.\n    :param equalize_per_image: whether the rescaling should be using the global min/max values of the array, or per image. Has no effect if equalize of False.\n\n    \"\"\"\n    if img.ndim == 3:\n        img = np.expand_dims(img, axis=3)\n\n    if equalize:   \n        img = np.array(img).astype(np.float) \n        if not equalize_per_image:        \n            img = (img - img.min())/(img.max() - img.min())\n        else:\n            for i in range(img.shape[0]):\n                img[i,...] = (img[i,...] - img[i,...].min())/(img[i,...].max() - img[i,...].min())\n\n    self._summary.add_images(tag=tag, img_tensor=img, global_step=step, walltime=None, dataformats='NHWC')\n</code></pre>"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_parameters","title":"<code>add_parameters(params, metrics)</code>","text":"<p>Logs parameters of a simulation.</p> <p>This should be run only once per simulation, generally at the end.  This allows to compare different runs of the same network using  different parameter values and study how they influence the global output metrics,  such as accuracy, error rate, reaction speed, etc.</p> <p>Example:</p> <pre><code>with Logger() as logger:\n    # ...\n    logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy})\n</code></pre> <p>Parameters:</p> <ul> <li> params         \u2013          <p>dictionary of parameters.</p> </li> <li> metrics         \u2013          <p>dictionary of metrics.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/tensorboard/Logger.py</code> <pre><code>def add_parameters(self, params, metrics):\n\"\"\"\n    Logs parameters of a simulation.\n\n    This should be run only once per simulation, generally at the end. \n    This allows to compare different runs of the same network using \n    different parameter values and study how they influence the global output metrics, \n    such as accuracy, error rate, reaction speed, etc.\n\n    Example:\n\n    ```python\n    with Logger() as logger:\n        # ...\n        logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy})\n    ```\n\n    :param params: dictionary of parameters.\n    :param metrics: dictionary of metrics.\n    \"\"\"\n\n    self._summary.add_hparams(params, metrics)\n</code></pre>"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_scalar","title":"<code>add_scalar(tag, value, step=None)</code>","text":"<p>Logs a single scalar value, e.g. a success rate at various stages of learning.</p> <p>Example:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        accuracy = ...\n        logger.add_scalar(\"Accuracy\", accuracy, trial)\n</code></pre> <p>Parameters:</p> <ul> <li> tag         \u2013          <p>name of the figure in tensorboard.</p> </li> <li> value         \u2013          <p>value.</p> </li> <li> step         \u2013          <p>time index.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/tensorboard/Logger.py</code> <pre><code>def add_scalar(self, tag, value, step=None):\n\"\"\"\n    Logs a single scalar value, e.g. a success rate at various stages of learning.\n\n    Example:\n\n    ```python\n    with Logger() as logger:\n        for trial in range(100):\n            simulate(1000.0)\n            accuracy = ...\n            logger.add_scalar(\"Accuracy\", accuracy, trial)\n    ```\n\n    :param tag: name of the figure in tensorboard.\n    :param value: value.\n    :param step: time index.\n    \"\"\"\n\n    self._summary.add_scalar(tag=tag, scalar_value=value, global_step=step, walltime=None)\n</code></pre>"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.add_scalars","title":"<code>add_scalars(tag, value, step=None)</code>","text":"<p>Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities.</p> <p>Example:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        act1 = pop.r[0]\n        act2 = pop.r[1]\n        logger.add_scalars(\n            \"Accuracy\", \n            {'First neuron': act1, 'Second neuron': act2}, \n            trial)\n</code></pre> <p>Parameters:</p> <ul> <li> tag         \u2013          <p>name of the figure in tensorboard.</p> </li> <li> value         \u2013          <p>dictionary of values.</p> </li> <li> step         \u2013          <p>time index.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/tensorboard/Logger.py</code> <pre><code>def add_scalars(self, tag, value, step=None):\n\"\"\"\n    Logs multiple scalar values to be displayed in the same figure, e.g. several metrics or neural activities.\n\n    Example:\n\n    ```python\n    with Logger() as logger:\n        for trial in range(100):\n            simulate(1000.0)\n            act1 = pop.r[0]\n            act2 = pop.r[1]\n            logger.add_scalars(\n                \"Accuracy\", \n                {'First neuron': act1, 'Second neuron': act2}, \n                trial)\n    ```\n\n    :param tag: name of the figure in tensorboard.\n    :param value: dictionary of values.\n    :param step: time index.\n    \"\"\"\n\n    self._summary.add_scalars(main_tag=tag, tag_scalar_dict=value, global_step=step, walltime=None)\n</code></pre>"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.close","title":"<code>close()</code>","text":"<p>Closes the logger.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/tensorboard/Logger.py</code> <pre><code>def close(self):\n    \"Closes the logger.\"\n    self._summary.close()\n</code></pre>"},{"location":"API/Logging.html#ANNarchy.extensions.tensorboard.Logger.Logger.flush","title":"<code>flush()</code>","text":"<p>Forces the logged data to be flushed to disk.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/tensorboard/Logger.py</code> <pre><code>def flush(self):\n    \"Forces the logged data to be flushed to disk.\"\n    self._summary.flush()\n</code></pre>"},{"location":"API/Monitor.html","title":"Monitoring","text":"<p>Recording of neural or synaptic variables during the simulation is possible through a <code>Monitor</code> object.</p>"},{"location":"API/Monitor.html#ANNarchy.Monitor","title":"<code>ANNarchy.Monitor</code>","text":"<p>Monitoring class allowing to record easily parameters or variables from Population, PopulationView, Dendrite or Projection objects.</p> <p>Example:</p> <pre><code>m = Monitor(pop, ['g_exc', 'v', 'spike'], period=10.0)\n</code></pre> <p>It is also possible to record the sum of inputs to each neuron in a rate-coded population:</p> <pre><code>m = Monitor(pop, ['sum(exc)', 'r'])\n</code></pre>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.__init__","title":"<code>__init__(obj, variables=[], period=None, period_offset=None, start=True, net_id=0)</code>","text":"<p>Parameters:</p> <ul> <li> obj         \u2013          <p>object to monitor. Must be a Population, PopulationView, Dendrite or Projection object.</p> </li> <li> variables         \u2013          <p>single variable name or list of variable names to record (default: []).</p> </li> <li> period         \u2013          <p>delay in ms between two recording (default: dt). Not valid for the <code>spike</code> variable of a Population(View).</p> </li> <li> period_offset         \u2013          <p>determine the moment in ms of recording within the period (default 0). Must be smaller than period.</p> </li> <li> start         \u2013          <p>defines if the recording should start immediately (default: True). If not, you should later start the recordings with the <code>start()</code> method.</p> </li> </ul>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.start","title":"<code>start(variables=None, period=None)</code>","text":"<p>Starts recording the variables. </p> <p>It is called automatically after <code>compile()</code> if the flag <code>start</code> was not passed to the constructor.</p> <p>Parameters:</p> <ul> <li> variables         \u2013          <p>single variable name or list of variable names to start recording (default: the <code>variables</code> argument passed to the constructor).</p> </li> <li> period         \u2013          <p>delay in ms between two recording (default: dt). Not valid for the <code>spike</code> variable of a Population(View).</p> </li> </ul>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.stop","title":"<code>stop()</code>","text":"<p>Stops the recording.</p> <p>Warning: This will delete the content of the C++ object and all data not previously retrieved is lost.</p>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.pause","title":"<code>pause()</code>","text":"<p>Pauses the recordings.</p>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.resume","title":"<code>resume()</code>","text":"<p>Resumes the recordings.</p>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.get","title":"<code>get(variables=None, keep=False, reshape=False, force_dict=False)</code>","text":"<p>Returns the recorded variables as a Numpy array (first dimension is time, second is neuron index).</p> <p>If a single variable name is provided, the recorded values for this variable are directly returned. If a list is provided or the argument left empty, a dictionary with all recorded variables is returned.</p> <p>The <code>spike</code> variable of a population will be returned as a dictionary of lists, where the spike times (in steps) for each recorded neurons are returned.</p> <p>Parameters:</p> <ul> <li> variables         \u2013          <p>(list of) variables. By default, a dictionary with all variables is returned.</p> </li> <li> keep         \u2013          <p>defines if the content in memory for each variable should be kept (default: False).</p> </li> <li> reshape         \u2013          <p>transforms the second axis of the array to match the population's geometry (default: False).</p> </li> </ul>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.reset","title":"<code>reset()</code>","text":"<p>Reset the monitor to its initial state.</p>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.histogram","title":"<code>histogram(spikes=None, bins=None)</code>","text":"<p>Returns a histogram for the recorded spikes in the population.</p> <p>Example:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nhisto = m.histogram()\nplt.plot(histo)\n</code></pre> <p>or:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nspikes = m.get('spike')\nhisto = m.histogram(spikes)\nplt.plot(histo)\n</code></pre> <p>Parameters:</p> <ul> <li> spikes         \u2013          <p>the dictionary of spikes returned by <code>get('spike')</code>. If left empty, <code>get('spike')</code> will be called. Beware: this erases the data from memory.</p> </li> <li> bins         \u2013          <p>the bin size in ms (default: dt).</p> </li> </ul>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.mean_fr","title":"<code>mean_fr(spikes=None)</code>","text":"<p>Computes the mean firing rate in the population during the recordings.</p> <p>Example:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nfr = m.mean_fr()\n</code></pre> <p>or:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nspikes = m.get('spike')\nfr = m.mean_fr(spikes)\n</code></pre> <p>Parameters:</p> <ul> <li> spikes         \u2013          <p>the dictionary of spikes returned by <code>get('spike')</code>. If left empty, <code>get('spike')</code> will be called. Beware: this erases the data from memory.</p> </li> </ul>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.population_rate","title":"<code>population_rate(spikes=None, smooth=0.0)</code>","text":"<p>Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons.</p> <p>This method is faster than calling <code>smoothed_rate</code> and then averaging.</p> <p>The first axis is the neuron index, the second is time.</p> <p>If <code>spikes</code> is left empty, <code>get('spike')</code> will be called. Beware: this erases the data from memory.</p> <p>Example:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nr = m.population_rate(smooth=100.)\n</code></pre> <p>Parameters:</p> <ul> <li> spikes         \u2013          <p>the dictionary of spikes returned by <code>get('spike')</code>.</p> </li> <li> smooth         \u2013          <p>smoothing time constant. Default: 0.0 (no smoothing).</p> </li> </ul>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.raster_plot","title":"<code>raster_plot(spikes=None)</code>","text":"<p>Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.</p> <p>Example:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nspike_times, spike_ranks = m.raster_plot()\nplt.plot(spike_times, spike_ranks, '.')\n</code></pre> <p>or:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nspikes = m.get('spike')\nspike_times, spike_ranks = m.raster_plot(spikes)\nplt.plot(spike_times, spike_ranks, '.')\n</code></pre> <p>Parameters:</p> <ul> <li> spikes         \u2013          <p>the dictionary of spikes returned by <code>get('spike')</code>. If left empty, <code>get('spike')</code> will be called. Beware: this erases the data from memory.</p> </li> </ul>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.smoothed_rate","title":"<code>smoothed_rate(spikes=None, smooth=0.0)</code>","text":"<p>Computes the smoothed firing rate of the recorded spiking neurons.</p> <p>The first axis is the neuron index, the second is time.</p> <p>Example:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nr = m.smoothed_rate(smooth=100.)\n</code></pre> <p>Parameters:</p> <ul> <li> spikes         \u2013          <p>the dictionary of spikes returned by <code>get('spike')</code>. If left empty, <code>get('spike')</code> will be called. Beware: this erases the data from memory.</p> </li> <li> smooth         \u2013          <p>smoothing time constant. Default: 0.0 (no smoothing).</p> </li> </ul>"},{"location":"API/Monitor.html#ANNarchy.core.Monitor.Monitor.times","title":"<code>times(variables=None)</code>","text":"<p>Returns the start and stop times (in ms) of the recorded variables. </p> <p>It should only be called after a call to <code>get()</code>, so that it describes when the variables have been recorded.</p> <p>Parameters:</p> <ul> <li> variables         \u2013          <p>(list of) variables. By default, the times for all variables is returned.</p> </li> </ul>"},{"location":"API/Monitor.html#plotting-methods","title":"Plotting methods","text":""},{"location":"API/Monitor.html#ANNarchy.raster_plot","title":"<code>ANNarchy.raster_plot(spikes)</code>","text":"<p>Returns two vectors representing for each recorded spike 1) the spike times and 2) the ranks of the neurons.</p> <p>Example:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nspikes = m.get('spike')\nspike_times, spike_ranks = raster_plot(spikes)\nplt.plot(spike_times, spike_ranks, '.')\n</code></pre> <p>Parameters:</p> <ul> <li> spikes         \u2013          <p>the dictionary of spikes returned by <code>get('spike')</code>.</p> </li> </ul>"},{"location":"API/Monitor.html#ANNarchy.histogram","title":"<code>ANNarchy.histogram(spikes, bins=None)</code>","text":"<p>Returns a histogram for the recorded spikes in the population.</p> <p>Example:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nspikes = m.get('spike')\nhisto = histogram(spikes)\nplt.plot(histo)\n</code></pre> <p>Parameters:</p> <ul> <li> spikes         \u2013          <p>the dictionary of spikes returned by <code>get('spike')</code>.</p> </li> <li> bins         \u2013          <p>the bin size in ms (default: dt).</p> </li> </ul>"},{"location":"API/Monitor.html#ANNarchy.mean_fr","title":"<code>ANNarchy.mean_fr(spikes, duration=None)</code>","text":"<p>Computes the mean firing rate in the population during the recordings.</p> <p>Example:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nspikes = m.get('spike')\nfr = mean_fr(spikes)\n</code></pre> <p>Parameters:</p> <ul> <li> spikes         \u2013          <p>the dictionary of spikes returned by <code>get('spike')</code>.</p> </li> <li> duration         \u2013          <p>duration of the recordings. By default, the mean firing rate is computed between the first and last spikes of the recordings.</p> </li> </ul>"},{"location":"API/Monitor.html#ANNarchy.smoothed_rate","title":"<code>ANNarchy.smoothed_rate(spikes, smooth=0.0)</code>","text":"<p>Computes the smoothed firing rate of the recorded spiking neurons.</p> <p>The first axis is the neuron index, the second is time.</p> <p>Example:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nspikes = m.get('spike')\nr = smoothed_rate(smooth=100.)\n</code></pre> <p>Parameters:</p> <ul> <li> spikes         \u2013          <p>the dictionary of spikes returned by <code>get('spike')</code>. If left empty, <code>get('spike')</code> will be called. Beware: this erases the data from memory.</p> </li> <li> smooth         \u2013          <p>smoothing time constant. Default: 0.0 (no smoothing).</p> </li> </ul>"},{"location":"API/Monitor.html#ANNarchy.population_rate","title":"<code>ANNarchy.population_rate(spikes, smooth=0.0)</code>","text":"<p>Takes the recorded spikes of a population and returns a smoothed firing rate for the population of recorded neurons.</p> <p>This method is faster than calling <code>smoothed_rate</code> and then averaging.</p> <p>The first axis is the neuron index, the second is time.</p> <p>Example:</p> <pre><code>m = Monitor(P[:1000], 'spike')\nsimulate(1000.0)\nspikes = m.get('spike')\nr = population_rate(smooth=100.)\n</code></pre> <p>Parameters:</p> <ul> <li> spikes         \u2013          <p>the dictionary of spikes returned by <code>get('spike')</code>.</p> </li> <li> smooth         \u2013          <p>smoothing time constant. Default: 0.0 (no smoothing).</p> </li> </ul>"},{"location":"API/Network.html","title":"Network class","text":"<p>A <code>Network</code> object holds copies of previously defined populations, projections or monitors in order to simulate them independently.</p> <p>The <code>parallel_run()</code> method can be used to simulate different networks in parallel.</p>"},{"location":"API/Network.html#ANNarchy.core.Network.Network","title":"<code>ANNarchy.core.Network.Network</code>","text":"<p>A network gathers already defined populations, projections and monitors in order to run them independently.</p> <p>This is particularly useful when varying single parameters of a network and comparing the results (see the <code>parallel_run()</code> method).</p> <p>Only objects declared before the creation of the network can be used. Global methods such as <code>simulate()</code> must be used on the network object. The objects must be accessed through the <code>get()</code> method, as the original ones will not be part of the network (a copy is made).</p> <p>Each network must be individually compiled, but it does not matter if the original objects were already compiled.</p> <p>When passing <code>everything=True</code> to the constructor, all populations/projections/monitors already defined at the global level will be added to the network.</p> <p>If not, you can select which object will be added to network with the <code>add()</code> method.</p> <p>Example with <code>everything=True</code>:</p> <pre><code>pop = Population(100, Izhikevich)\nproj = Projection(pop, pop, 'exc')\nproj.connect_all_to_all(1.0)\nm = Monitor(pop, 'spike')\n\ncompile() # Optional\n\nnet = Network(everything=True)\nnet.get(pop).a = 0.02\nnet.compile()\nnet.simulate(1000.)\n\nnet2 = Network(everything=True)\nnet2.get(pop).a = 0.05\nnet2.compile()\nnet2.simulate(1000.)\n\nt, n = net.get(m).raster_plot()\nt2, n2 = net2.get(m).raster_plot()\n</code></pre> <p>Example with <code>everything=False</code> (the default):</p> <pre><code>pop = Population(100, Izhikevich)\nproj1 = Projection(pop, pop, 'exc')\nproj1.connect_all_to_all(1.0)\nproj2 = Projection(pop, pop, 'exc')\nproj2.connect_all_to_all(2.0)\nm = Monitor(pop, 'spike')\n\nnet = Network()\nnet.add([pop, proj1, m])\nnet.compile()\nnet.simulate(1000.)\n\nnet2 = Network()\nnet2.add([pop, proj2, m])\nnet2.compile()\nnet2.simulate(1000.)\n\nt, n = net.get(m).raster_plot()\nt2, n2 = net2.get(m).raster_plot()\n</code></pre>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.__init__","title":"<code>__init__(everything=False)</code>","text":"<p>Parameters:</p> <ul> <li> everything         \u2013          <p>defines if all existing populations and projections should be automatically added (default: False).</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.add","title":"<code>add(objects)</code>","text":"<p>Adds a Population, Projection or Monitor to the network.</p> <p>Parameters:</p> <ul> <li> objects         \u2013          <p>A single object or a list to add to the network.</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.compile","title":"<code>compile(directory='annarchy', clean=False, compiler='default', compiler_flags='default', add_sources='', extra_libs='', cuda_config={'device': 0}, annarchy_json='', silent=False, debug_build=False, profile_enabled=False)</code>","text":"<p>Compiles the network.</p> <p>Parameters:</p> <ul> <li> directory         \u2013          <p>name of the subdirectory where the code will be generated and compiled. Must be a relative path. Default: \"annarchy/\".</p> </li> <li> clean         \u2013          <p>boolean to specifying if the library should be recompiled entirely or only the changes since last compilation (default: False).</p> </li> <li> compiler         \u2013          <p>C++ compiler to use. Default: g++ on GNU/Linux, clang++ on OS X. Valid compilers are [g++, clang++].</p> </li> <li> compiler_flags         \u2013          <p>platform-specific flags to pass to the compiler. Default: \"-march=native -O2\". Warning: -O3 often generates slower code and can cause linking problems, so it is not recommended.</p> </li> <li> cuda_config         \u2013          <p>dictionary defining the CUDA configuration for each population and projection.</p> </li> <li> annarchy_json         \u2013          <p>compiler flags etc are stored in a .json file normally placed in the home directory. With this flag one can directly assign a file location.</p> </li> <li> silent         \u2013          <p>defines if the \"Compiling... OK\" should be printed.</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.disable_learning","title":"<code>disable_learning(projections=None)</code>","text":"<p>Disables learning for all projections.</p> <p>Parameters:</p> <ul> <li> projections         \u2013          <p>the projections whose learning should be disabled. By default, all the existing projections are disabled.</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.enable_learning","title":"<code>enable_learning(projections=None, period=None, offset=None)</code>","text":"<p>Enables learning for all projections.</p> <p>Parameters:</p> <ul> <li> projections         \u2013          <p>the projections whose learning should be enabled. By default, all the existing projections are disabled.</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get","title":"<code>get(obj)</code>","text":"<p>Returns the local Population, Projection or Monitor identical to the provided argument.</p> <p>Example:</p> <pre><code>pop = Population(100, Izhikevich)\nnet = Network()\nnet.add(pop)\nnet.compile()\nnet.simulate(100.)\nprint net.get(pop).v\n</code></pre> <p>Parameters:</p> <ul> <li> obj         \u2013          <p>A single object or a list of objects.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>The corresponding object or list of objects.</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get_current_step","title":"<code>get_current_step()</code>","text":"<p>Returns the current simulation step.</p>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get_population","title":"<code>get_population(name)</code>","text":"<p>Returns the population with the given name.</p> <p>Parameters:</p> <ul> <li> name         \u2013          <p>name of the population</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>The requested <code>Population</code> object if existing, <code>None</code> otherwise.</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get_populations","title":"<code>get_populations()</code>","text":"<p>Returns a list of all declared populations in this network.</p>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get_projection","title":"<code>get_projection(name)</code>","text":"<p>Returns the projection with the given name.</p> <p>Parameters:</p> <ul> <li> name         \u2013          <p>name of the projection</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>The requested <code>Projection</code> object if existing, <code>None</code> otherwise.</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get_projections","title":"<code>get_projections(post=None, pre=None, target=None, suppress_error=False)</code>","text":"<p>Get a list of declared projections for the current network. By default, the method returns all connections within the network.</p> <p>By setting the arguments, post, pre and target one can select a subset.</p> <p>Parameters:</p> <ul> <li> post         \u2013          <p>all returned projections should have this population as post.</p> </li> <li> pre         \u2013          <p>all returned projections should have this population as pre.</p> </li> <li> target         \u2013          <p>all returned projections should have this target.</p> </li> <li> suppress_error         \u2013          <p>by default, ANNarchy throws an error if the list of assigned projections is empty. If this flag is set to True, the error message is suppressed.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>A list of all assigned projections in this network or a subset according to the arguments.</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.get_time","title":"<code>get_time()</code>","text":"<p>Returns the current time in ms.</p>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.load","title":"<code>load(filename, populations=True, projections=True, pickle_encoding=None)</code>","text":"<p>Loads a saved state of the current network by calling ANNarchy.core.IO.load().</p> <p>Parameters:</p> <ul> <li> filename         \u2013          <p>filename, may contain relative or absolute path.</p> </li> <li> populations         \u2013          <p>if True, population data will be saved (by default True)</p> </li> <li> projections         \u2013          <p>if True, projection data will be saved (by default True)</p> </li> <li> pickle_encoding         \u2013          <p>optional parameter provided to the pickle.load() method. If set to None the default is used.</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.reset","title":"<code>reset(populations=True, projections=False, monitors=True, synapses=False)</code>","text":"<p>Reinitialises the network to its state before the call to compile.</p> <p>Parameters:</p> <ul> <li> populations         \u2013          <p>if True (default), the neural parameters and variables will be reset to their initial value.</p> </li> <li> projections         \u2013          <p>if True, the synaptic parameters and variables (except the connections) will be reset (default=False).</p> </li> <li> synapses         \u2013          <p>if True, the synaptic weights will be erased and recreated (default=False).</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.save","title":"<code>save(filename, populations=True, projections=True)</code>","text":"<p>Saves the current network by calling ANNarchy.core.IO.save().</p> <p>Parameters:</p> <ul> <li> filename         \u2013          <p>filename, may contain relative or absolute path.</p> </li> <li> populations         \u2013          <p>if True, population data will be saved (by default True)</p> </li> <li> projections         \u2013          <p>if True, projection data will be saved (by default True)</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.set_current_step","title":"<code>set_current_step(t)</code>","text":"<p>Sets the current simulation step.</p> <p>Warning: can be dangerous for some spiking models.</p>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.set_seed","title":"<code>set_seed(seed, use_seed_seq=True)</code>","text":"<p>Sets the seed of the random number generators for this network.</p>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.set_time","title":"<code>set_time(t, net_id=0)</code>","text":"<p>Sets the current time in ms.</p> <p>Warning: can be dangerous for some spiking models.</p>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.simulate","title":"<code>simulate(duration, measure_time=False)</code>","text":"<p>Runs the network for the given duration in milliseconds. </p> <p>The number of simulation steps is  computed relative to the discretization step <code>dt</code> declared in <code>setup()</code> (default: 1ms):</p> <pre><code>simulate(1000.0)\n</code></pre> <p>Parameters:</p> <ul> <li> duration         \u2013          <p>the duration in milliseconds.</p> </li> <li> measure_time         \u2013          <p>defines whether the simulation time should be printed (default=False).</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.simulate_until","title":"<code>simulate_until(max_duration, population, operator='and', measure_time=False)</code>","text":"<p>Runs the network for the maximal duration in milliseconds. If the <code>stop_condition</code> defined in the population becomes true during the simulation, it is stopped.</p> <p>One can specify several populations. If the stop condition is true for any of the populations, the simulation will stop ('or' function).</p> <p>Example:</p> <pre><code>pop1 = Population( ..., stop_condition = \"r &gt; 1.0 : any\")\ncompile()\nsimulate_until(max_duration=1000.0. population=pop1)\n</code></pre> <p>Parameters:</p> <ul> <li> max_duration         \u2013          <p>the maximum duration of the simulation in milliseconds.</p> </li> <li> population         \u2013          <p>the (list of) population whose <code>stop_condition</code> should be checked to stop the simulation.</p> </li> <li> operator         \u2013          <p>operator to be used ('and' or 'or') when multiple populations are provided (default: 'and').</p> </li> <li> measure_time         \u2013          <p>defines whether the simulation time should be printed (default=False).</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>the actual duration of the simulation in milliseconds.</p> </li> </ul>"},{"location":"API/Network.html#ANNarchy.core.Network.Network.step","title":"<code>step()</code>","text":"<p>Performs a single simulation step (duration = <code>dt</code>).</p>"},{"location":"API/Network.html#ANNarchy.core.Network.parallel_run","title":"<code>ANNarchy.core.Network.parallel_run(method, networks=None, number=0, max_processes=-1, measure_time=False, sequential=False, same_seed=False, annarchy_json='', visible_cores=[], **args)</code>","text":"<p>Allows to run multiple networks in parallel using multiprocessing.</p> <p>If the <code>networks</code> argument is provided as a list of Network objects, the given method will be executed for each of these networks.</p> <p>If <code>number</code> is given instead, the same number of networks will be created and the method is applied.</p> <p>If <code>number</code> is used, the created networks are not returned, you should return what you need to analyse.</p> <p>Example:</p> <pre><code>pop1 = PoissonPopulation(100, rates=10.0)\npop2 = Population(100, Izhikevich)\nproj = Projection(pop1, pop2, 'exc')\nproj.connect_fixed_probability(weights=5.0, probability=0.2)\nm = Monitor(pop2, 'spike')\n\ncompile()\n\ndef simulation(idx, net):\n    net.get(pop1).rates = 10. * idx\n    net.simulate(1000.)\n    return net.get(m).raster_plot()\n\nresults = parallel_run(method=simulation, number = 3)\n\nt1, n1 = results[0]\nt2, n2 = results[1]\nt3, n3 = results[2]\n</code></pre> <p>Parameters:</p> <ul> <li> method         \u2013          <p>a Python method which will be executed for each network. This function must accept an integer as first argument (id of the simulation) and a Network object as second argument.</p> </li> <li> networks         \u2013          <p>a list of networks to simulate in parallel.</p> </li> <li> number         \u2013          <p>the number of identical networks to run in parallel.</p> </li> <li> max_processes         \u2013          <p>maximal number of processes to start concurrently (default: the available number of cores on the machine).</p> </li> <li> measure_time         \u2013          <p>if the total simulation time should be printed out.</p> </li> <li> sequential         \u2013          <p>if True, runs the simulations sequentially instead of in parallel (default: False).</p> </li> <li> same_seed         \u2013          <p>if True, all networks will use the same seed. If not, the seed will be randomly initialized with time(0) for each network (default). It has no influence when the <code>networks</code> argument is set (the seed has to be set individually for each network using <code>net.set_seed()</code>), only when <code>number</code> is used.</p> </li> <li> annarchy.json         \u2013          <p>path to a different configuration file if needed (default \"\").</p> </li> <li> visible_cores         \u2013          <p>a list of CPU core ids to simulate on (must have max_processes entries and max_processes must be != -1)</p> </li> <li> args         \u2013          <p>other named arguments you want to pass to the simulation method.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>a list of the values returned by <code>method</code>.</p> </li> </ul>"},{"location":"API/Neuron.html","title":"Neuron","text":""},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron","title":"<code>ANNarchy.core.Neuron.Neuron</code>","text":"<p>Base class to define a neuron.</p>"},{"location":"API/Neuron.html#ANNarchy.core.Neuron.Neuron.__init__","title":"<code>__init__(parameters='', equations='', spike=None, axon_spike=None, reset=None, axon_reset=None, refractory=None, functions=None, name='', description='', extra_values={})</code>","text":"<p>Parameters:</p> <ul> <li> parameters         \u2013          <p>parameters of the neuron and their initial value.</p> </li> <li> equations         \u2013          <p>equations defining the temporal evolution of variables.</p> </li> <li> functions         \u2013          <p>additional functions used in the variables' equations.</p> </li> <li> spike         \u2013          <p>condition to emit a spike (only for spiking neurons).</p> </li> <li> axon_spike         \u2013          <p>condition to emit an axonal spike (only for spiking neurons and optional). The axonal spike can appear additional to the spike and is independent from refractoriness of a neuron.</p> </li> <li> reset         \u2013          <p>changes to the variables after a spike (only for spiking neurons).</p> </li> <li> axon_reset         \u2013          <p>changes to the variables after an axonal spike (only for spiking neurons).</p> </li> <li> refractory         \u2013          <p>refractory period of a neuron after a spike (only for spiking neurons).</p> </li> <li> name         \u2013          <p>name of the neuron type (used for reporting only).</p> </li> <li> description         \u2013          <p>short description of the neuron type (used for reporting).</p> </li> </ul>"},{"location":"API/Population.html","title":"Population class","text":"<p>A <code>Population</code> object represents a group of identical neurons. It is associated with a geometry (defining the number of neurons and optionally its spatial structure), a neuron type and optionally a name.</p>"},{"location":"API/Population.html#ANNarchy.core.Population.Population","title":"<code>ANNarchy.core.Population.Population</code>","text":"<p>Container for a population of homogeneous neurons.</p>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.neurons","title":"<code>neurons</code>  <code>property</code>","text":"<p>Returns iteratively each neuron in the population.</p> <p>For instance, if you want to iterate over all neurons of a population:</p> <pre><code>for neuron in pop.neurons:\n    neuron.r = 0.0\n</code></pre> <p>Alternatively, one could also benefit from the <code>__iter__</code> special command.  The following code is equivalent:</p> <pre><code>for neuron in pop:\n    neuron.r = 0.0\n</code></pre>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.__init__","title":"<code>__init__(geometry, neuron, name=None, stop_condition=None, storage_order='post_to_pre', copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> geometry         \u2013          <p>population geometry as tuple. If an integer is given, it is the size of the population.</p> </li> <li> neuron         \u2013          <p>instance of <code>ANNarchy.Neuron</code>. It can be user-defined or a built-in model.</p> </li> <li> name         \u2013          <p>unique name of the population (optional, it defaults to <code>pop0</code>, <code>pop1</code>, etc).</p> </li> <li> stop_condition         \u2013          <p>a single condition on a neural variable which can stop the simulation whenever it is true.  Example:  <code>python pop = Population(100, neuron=Izhikevich, name=\"Excitatory population\")</code></p> </li> </ul>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.clear","title":"<code>clear()</code>","text":"<p>Clears all spiking events previously emitted (history of spikes, delayed spikes).</p> <p>Can be useful if you do not want to totally reset a population (i.e. all variables), only to clear the spiking history between two trials.</p> <p>Note: does nothing for rate-coded networks.</p>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.compute_firing_rate","title":"<code>compute_firing_rate(window)</code>","text":"<p>Tells spiking neurons in the population to compute their mean firing rate over the given window and store the values in the variable <code>r</code>.</p> <p>This method has an effect on spiking neurons only.</p> <p>If this method is not called, <code>r</code> will always be 0.0. <code>r</code> can of course be accessed and recorded as any other variable.</p> <p>Parameters:</p> <ul> <li> window         \u2013          <p>window in ms over which the spikes will be counted.</p> </li> </ul>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.coordinates_from_rank","title":"<code>coordinates_from_rank(rank)</code>","text":"<p>Returns the coordinates of a neuron based on its rank.</p> <p>Parameters:</p> <ul> <li> rank         \u2013          <p>rank of the neuron.</p> </li> </ul>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.disable","title":"<code>disable()</code>","text":"<p>Temporarily disables computations in this population (including the projections leading to it).</p> <p>You can re-enable it with the <code>enable()</code> method.</p>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.enable","title":"<code>enable()</code>","text":"<p>(Re)-enables computations in this population, after they were disabled by the <code>disable()</code> method.</p> <p>The status of the population is accessible through the <code>enabled</code> flag.</p>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.get","title":"<code>get(name)</code>","text":"<p>Returns the value of neural variables and parameters.</p> <p>Parameters:</p> <ul> <li> name         \u2013          <p>attribute name as a string.</p> </li> </ul>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.load","title":"<code>load(filename, pickle_encoding=None)</code>","text":"<p>Load the saved state of the population by <code>Population.save()</code>.</p> <p>Warning: Matlab data can not be loaded.</p> <p>Example:</p> <pre><code>pop.load('pop1.npz')\npop.load('pop1.txt')\npop.load('pop1.txt.gz')\n</code></pre> <p>Parameters:</p> <ul> <li> filename         \u2013          <p>the filename with relative or absolute path.</p> </li> </ul>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.neuron","title":"<code>neuron(*coord)</code>","text":"<p>Returns an <code>IndividualNeuron</code> object wrapping the neuron with the provided rank or coordinates.</p>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.normalized_coordinates_from_rank","title":"<code>normalized_coordinates_from_rank(rank, norm=1.0)</code>","text":"<p>Returns normalized coordinates of a neuron based on its rank.  The geometry of the population is mapped to the hypercube \\([0, 1]^d\\)</p> <p>Parameters:</p> <ul> <li> rank         \u2013          <p>rank of the neuron</p> </li> <li> norm         \u2013          <p>norm of the cube (default = 1.0)</p> </li> </ul>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.rank_from_coordinates","title":"<code>rank_from_coordinates(coord)</code>","text":"<p>Returns the rank of a neuron based on coordinates.</p> <p>Parameters:</p> <ul> <li> coord         \u2013          <p>coordinate tuple, can be multidimensional.</p> </li> </ul>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.reset","title":"<code>reset(attributes=-1)</code>","text":"<p>Resets all parameters and variables of the population to the value they had before the call to compile().</p> <p>Parameters:</p> <ul> <li> attributes         \u2013          <p>list of attributes (parameter or variable) which should be reinitialized. Default: all attributes.</p> </li> </ul>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.save","title":"<code>save(filename)</code>","text":"<p>Saves all information about the population (structure, current value of parameters and variables) into a file.</p> <ul> <li> <p>If the file name is '.npz', the data will be saved and compressed using <code>np.savez_compressed</code> (recommended).</p> </li> <li> <p>If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip.</p> </li> <li> <p>If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed.</p> </li> <li> <p>Otherwise, the data will be pickled into a simple binary text file using pickle.</p> </li> </ul> <p>Warning: The '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose.</p> <p>Parameters:</p> <ul> <li> filename         \u2013          <p>filename, may contain relative or absolute path.  Example:  <code>python pop.save('pop1.npz') pop.save('pop1.txt') pop.save('pop1.txt.gz') pop.save('pop1.mat')</code></p> </li> </ul>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.set","title":"<code>set(values)</code>","text":"<p>Sets the value of neural variables and parameters.</p> <p>Example:</p> <pre><code>pop.set({ 'tau' : 20.0, 'r'= np.random.rand((8,8)) } )\n</code></pre> <p>Parameters:</p> <ul> <li> values         \u2013          <p>dictionary of attributes to be updated.</p> </li> </ul>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.size_in_bytes","title":"<code>size_in_bytes()</code>","text":"<p>Returns the size of allocated memory on the C++ side. Please note that this does not contain monitored data and works only if compile() was invoked.</p>"},{"location":"API/Population.html#ANNarchy.core.Population.Population.sum","title":"<code>sum(target)</code>","text":"<p>Returns the array of weighted sums corresponding to the target:</p> <pre><code>excitatory = pop.sum('exc')\n</code></pre> <p>For spiking networks, this is equivalent to accessing the conductances directly:</p> <pre><code>excitatory = pop.g_exc\n</code></pre> <p>If no incoming projection has the given target, the method returns zeros.</p> <p>Note: it is not possible to distinguish the original population when the same target is used.</p> <p>Parameters:</p> <ul> <li> target         \u2013          <p>the desired projection target.</p> </li> </ul>"},{"location":"API/Projection.html","title":"Projection class","text":"<p>The class <code>ANNarchy.Projection</code> defines projections at the population level. A projection is an ensemble of connections (or synapses) between a subset of a population (called the pre-synaptic population) and a subset of another population (the post-synaptic population), with a specific connection type. The pre- and post-synaptic populations may be the same.</p>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection","title":"<code>ANNarchy.core.Projection.Projection</code>","text":"<p>             Bases: <code>object</code></p> <p>Container for all the synapses of the same type between two populations.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>class Projection(object):\n\"\"\"\n    Container for all the synapses of the same type between two populations.\n    \"\"\"\n\n    def __init__(self, pre, post, target, synapse=None, name=None, disable_omp=True, copied=False):\n\"\"\"\n        By default, the synapse only ensures linear synaptic transmission:\n\n        * For rate-coded populations: ``psp = w * pre.r``\n        * For spiking populations: ``g_target += w``\n\n        to modify this behavior one need to provide a Synapse object.\n\n        :param pre: pre-synaptic population (either its name or a ``Population`` object).\n        :param post: post-synaptic population (either its name or a ``Population`` object).\n        :param target: type of the connection.\n        :param synapse: a ``Synapse`` instance.\n        :param name: unique name of the projection (optional, it defaults to ``proj0``, ``proj1``, etc).\n        :param disable_omp: especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to `False`.\n        \"\"\"\n        # Check if the network has already been compiled\n        if Global._network[0]['compiled'] and not copied:\n            Global._error('you cannot add a projection after the network has been compiled.')\n\n        # Store the pre and post synaptic populations\n        # the user provide either a string or a population object\n        # in case of string, we need to search for the corresponding object\n        if isinstance(pre, str):\n            for pop in Global._network[0]['populations']:\n                if pop.name == pre:\n                    self.pre = pop\n        else:\n            self.pre = pre\n\n        if isinstance(post, str):\n            for pop in Global._network[0]['populations']:\n                if pop.name == post:\n                    self.post = pop\n        else:\n            self.post = post\n\n        # Store the arguments\n        if isinstance(target, list) and len(target) == 1:\n            self.target = target[0]\n        else:\n            self.target = target\n\n        # Add the target(s) to the postsynaptic population\n        if isinstance(self.target, list):\n            for _target in self.target:\n                self.post.targets.append(_target)\n        else:\n            self.post.targets.append(self.target)\n\n        # check if a synapse description is attached\n        if not synapse:\n            # No synapse attached assume default synapse based on\n            # presynaptic population.\n            if self.pre.neuron_type.type == 'rate':\n                from ANNarchy.models.Synapses import DefaultRateCodedSynapse\n                self.synapse_type = DefaultRateCodedSynapse()\n                self.synapse_type.type = 'rate'\n            else:\n                from ANNarchy.models.Synapses import DefaultSpikingSynapse\n                self.synapse_type = DefaultSpikingSynapse()\n                self.synapse_type.type = 'spike'\n\n        elif inspect.isclass(synapse):\n            self.synapse_type = synapse()\n            self.synapse_type.type = self.pre.neuron_type.type\n        else:\n            self.synapse_type = copy.deepcopy(synapse)\n            self.synapse_type.type = self.pre.neuron_type.type\n\n        # Disable omp for spiking networks\n        self.disable_omp = disable_omp\n\n        # Analyse the parameters and variables\n        self.synapse_type._analyse()\n\n        # Create a default name\n        self.id = len(Global._network[0]['projections'])\n        if name:\n            self.name = name\n        else:\n            self.name = 'proj'+str(self.id)\n\n        # Container for control/attribute states\n        self.init = {}\n\n        # Control-flow variables\n        self.init[\"transmission\"] = True\n        self.init[\"update\"] = True\n        self.init[\"plasticity\"] = True\n\n        # Get a list of parameters and variables\n        self.parameters = []\n        for param in self.synapse_type.description['parameters']:\n            self.parameters.append(param['name'])\n            self.init[param['name']] = param['init']\n\n        self.variables = []\n        for var in self.synapse_type.description['variables']:\n            self.variables.append(var['name'])\n            self.init[var['name']] = var['init']\n\n        self.attributes = self.parameters + self.variables\n\n        # Get a list of user-defined functions\n        self.functions = [func['name'] for func in self.synapse_type.description['functions']]\n\n        # Add the population to the global network\n        Global._network[0]['projections'].append(self)\n\n        # Finalize initialization\n        self.initialized = False\n\n        # Cython instance\n        self.cyInstance = None\n\n        # Connectivity\n        self._synapses = None\n        self._connection_method = None\n        self._connection_args = None\n        self._connection_delay = None\n        self._connector = None\n        self._lil_connectivity = None\n\n        # Default configuration for connectivity\n        self._storage_format = \"lil\"\n        self._storage_order = \"post_to_pre\"\n\n        # If a single weight value is used\n        self._single_constant_weight = False\n\n        # Are random distribution used for weights/delays\n        self.connector_weight_dist = None\n        self.connector_delay_dist = None\n\n        # Reporting\n        self.connector_name = \"Specific\"\n        self.connector_description = \"Specific\"\n\n        # Overwritten by derived classes, to add\n        # additional code\n        self._specific_template = {}\n\n        # Set to False by derived classes to prevent saving of\n        # data, e. g. in case of weight-sharing projections\n        self._saveable = True\n\n        # To allow case-specific adjustment of parallelization\n        # parameters, e. g. openMP schedule, we introduce a\n        # dictionary read by the ProjectionGenerator.\n        #\n        # Will be overwritten either by inherited classes or\n        # by an omp_config provided to the compile() method.\n        self._omp_config = {\n            #'psp_schedule': 'schedule(dynamic)'\n        }\n\n        # If set to true, the code generator is not allowed to\n        # split the matrix. This will be the case for many\n        # SpecificProjections defined by the user or is disabled\n        # globally.\n        if self.synapse_type.type == \"rate\":\n            # Normally, the split should not be used for rate-coded models\n            # but maybe there are cases where we want to enable it ...\n            self._no_split_matrix = Global.config[\"disable_split_matrix\"]\n\n            # If the number of elements is too small, the split\n            # might not be efficient.\n            if self.post.size &lt; Global.OMP_MIN_NB_NEURONS:\n                self._no_split_matrix = True\n\n        else:\n            # If the number of elements is too small, the split\n            # might not be efficient.\n            if self.post.size &lt; Global.OMP_MIN_NB_NEURONS:\n                self._no_split_matrix = True\n            else:\n                self._no_split_matrix = Global.config[\"disable_split_matrix\"]\n\n        # In particular for spiking models, the parallelization on the\n        # inner or outer loop can make a performance difference\n        if self._no_split_matrix:\n            # LIL and CSR are parallelized on inner loop\n            # to prevent cost of atomic operations\n            self._parallel_pattern = 'inner_loop'\n        else:\n            # splitted matrices are always parallelized on outer loop!\n            self._parallel_pattern = 'outer_loop'\n\n        # For dense matrix format: do we use an optimization for population views?\n        if self.synapse_type.type == \"rate\":\n            # HD (9th Nov. 2022): currently this optimization is only intended for spiking models\n            self._has_pop_view = False\n        else:\n            # HD (9th Nov. 2022): currently disabled, more testing is required ...\n            self._has_pop_view = False #isinstance(self.pre, PopulationView) or isinstance(self.post, PopulationView)\n\n    # Add defined connectors\n    connect_one_to_one = ConnectorMethods.connect_one_to_one\n    connect_all_to_all = ConnectorMethods.connect_all_to_all\n    connect_gaussian = ConnectorMethods.connect_gaussian\n    connect_dog = ConnectorMethods.connect_dog\n    connect_fixed_probability = ConnectorMethods.connect_fixed_probability\n    connect_fixed_number_pre = ConnectorMethods.connect_fixed_number_pre\n    connect_fixed_number_post = ConnectorMethods.connect_fixed_number_post\n    connect_with_func = ConnectorMethods.connect_with_func\n    connect_from_matrix = ConnectorMethods.connect_from_matrix\n    connect_from_matrix_market = ConnectorMethods.connect_from_matrix_market\n    _load_from_matrix = ConnectorMethods._load_from_matrix\n    connect_from_sparse = ConnectorMethods.connect_from_sparse\n    _load_from_sparse = ConnectorMethods._load_from_sparse\n    connect_from_file = ConnectorMethods.connect_from_file\n    _load_from_lil = ConnectorMethods._load_from_lil\n\n    def _copy(self, pre, post):\n        \"Returns a copy of the projection when creating networks.  Internal use only.\"\n        copied_proj = Projection(pre=pre, post=post, target=self.target, synapse=self.synapse_type, name=self.name, disable_omp=self.disable_omp, copied=True)\n\n        # these flags are modified during connect_XXX called before Network()\n        copied_proj._single_constant_weight = self._single_constant_weight\n        copied_proj.connector_weight_dist = self.connector_weight_dist\n        copied_proj.connector_delay_dist = self.connector_delay_dist\n        copied_proj.connector_name = self.connector_name\n\n        # Control flags for code generation (maybe modified by connect_XXX())\n        copied_proj._storage_format = self._storage_format\n        copied_proj._storage_order = self._storage_order\n        copied_proj._no_split_matrix = self._no_split_matrix\n\n        # for some projection types saving is not allowed (e. g. Convolution, Pooling)\n        copied_proj._saveable = self._saveable\n\n        # optional flags\n        if hasattr(self, \"_bsr_size\"):\n            copied_proj._bsr_size = self._bsr_size\n\n        return copied_proj\n\n    def _generate(self):\n        \"Overriden by specific projections to generate the code\"\n        pass\n\n    def _instantiate(self, module):\n\"\"\"\n        Instantiates the projection after compilation. The function should be\n        called by Compiler._instantiate().\n\n        :param:     module  cython module (ANNarchyCore instance)\n        \"\"\"\n        if Global.config[\"profiling\"]:\n            import time\n            t1 = time.time()\n\n        self.initialized = self._connect(module)\n\n        if Global.config[\"profiling\"]:\n            t2 = time.time()\n            Global._profiler.add_entry(t1, t2, \"proj\"+str(self.id), \"instantiate\")\n\n    def _init_attributes(self):\n\"\"\"\n        Method used after compilation to initialize the attributes. The function\n        should be called by Compiler._instantiate\n        \"\"\"\n        for name, val in self.init.items():\n            # the weights ('w') are already inited by the _connect() method.\n            if not name in ['w']:\n                self.__setattr__(name, val)\n\n    def _connect(self, module):\n\"\"\"\n        Builds up dendrites either from list or dictionary. Called by instantiate().\n\n        :param:     module  cython module (ANNarchyCore instance)\n        :return:    True, if the connector was successfully instantiated. Potential errors are kept by \n                    Python exceptions. If the Cython - connector call fails (return False) the most likely\n                    reason is that there was not enough memory available.\n        \"\"\"\n        # Local import to prevent circular import (HD: 28th June 2021)\n        from ANNarchy.generator.Utils import cpp_connector_available\n\n        # Sanity check\n        if not self._connection_method:\n            Global._error('The projection between ' + self.pre.name + ' and ' + self.post.name + ' is declared but not connected.')\n\n        # Debug printout\n        if Global.config[\"verbose\"]:\n            print(\"Connectivity parameter (\"+self.name+\"):\", self._connection_args )\n\n        # Instantiate the Cython wrapper\n        if not self.cyInstance:\n            cy_wrapper = getattr(module, 'proj'+str(self.id)+'_wrapper')\n            self.cyInstance = cy_wrapper()\n\n        # Check if there is a specialized CPP connector\n        if not cpp_connector_available(self.connector_name, self._storage_format, self._storage_order):\n            # No default connector -&gt; initialize from LIL\n            if self._lil_connectivity:\n                return self.cyInstance.init_from_lil_connectivity(self._lil_connectivity)\n            else:\n                return self.cyInstance.init_from_lil_connectivity(self._connection_method(*((self.pre, self.post,) + self._connection_args)))\n\n        else:\n            # fixed probability pattern\n            if self.connector_name == \"Random\":\n                p = self._connection_args[0]\n                allow_self_connections = self._connection_args[3]\n                if isinstance(self._connection_args[1], RandomDistribution):\n                    #some kind of distribution\n                    w_dist_arg1, w_dist_arg2 = self._connection_args[1].get_cpp_args()\n                else:\n                    # constant\n                    w_dist_arg1 = self._connection_args[1]\n                    w_dist_arg2 = self._connection_args[1]\n\n                if isinstance(self._connection_args[2], RandomDistribution):\n                    #some kind of distribution\n                    d_dist_arg1, d_dist_arg2 = self._connection_args[2].get_cpp_args()\n                else:\n                    # constant\n                    d_dist_arg1 = self._connection_args[2]\n                    d_dist_arg2 = self._connection_args[2]\n\n                return self.cyInstance.fixed_probability(self.post.ranks, self.pre.ranks, p, w_dist_arg1, w_dist_arg2, d_dist_arg1, d_dist_arg2, allow_self_connections)\n\n            # fixed number pre prattern\n            elif self.connector_name== \"Random Convergent\":\n                number_nonzero = self._connection_args[0]\n                if isinstance(self._connection_args[1], RandomDistribution):\n                    #some kind of distribution\n                    w_dist_arg1, w_dist_arg2 = self._connection_args[1].get_cpp_args()\n                else:\n                    # constant\n                    w_dist_arg1 = self._connection_args[1]\n                    w_dist_arg2 = self._connection_args[1]\n\n                if isinstance(self._connection_args[2], RandomDistribution):\n                    #some kind of distribution\n                    d_dist_arg1, d_dist_arg2 = self._connection_args[2].get_cpp_args()\n                else:\n                    # constant\n                    d_dist_arg1 = self._connection_args[2]\n                    d_dist_arg2 = self._connection_args[2]\n\n                return self.cyInstance.fixed_number_pre(self.post.ranks, self.pre.ranks, number_nonzero, w_dist_arg1, w_dist_arg2, d_dist_arg1, d_dist_arg2)\n\n            else:\n                # This should never happen ...\n                Global._error(\"No initialization for CPP-connector defined ...\")\n\n        # should be never reached ...\n        return False\n\n    def _store_connectivity(self, method, args, delay, storage_format, storage_order):\n\"\"\"\n        Store connectivity data. This function is called from cython_ext.Connectors module.\n        \"\"\"\n        # No format specified for this projection by the user, so fall-back to Global setting\n        if storage_format is None:\n            if Global.config['sparse_matrix_format'] == \"default\":\n                if Global._check_paradigm(\"openmp\"):\n                    storage_format = \"lil\"\n                elif Global._check_paradigm(\"cuda\"):\n                    storage_format = \"csr\"\n                else:\n                    raise NotImplementedError\n\n            else:\n                storage_format = Global.config[\"sparse_matrix_format\"]\n\n        # Sanity checks\n        if self._connection_method != None:\n            Global._warning(\"Projection \", self.name, \" was already connected ... data will be overwritten.\")\n\n        # Store connectivity pattern parameters\n        self._connection_method = method\n        self._connection_args = args\n        self._connection_delay = delay\n        self._storage_format = storage_format\n        self._storage_order = storage_order\n\n        # The user selected nothing therefore we use the standard since ANNarchy 4.4.0\n        if storage_format == None:\n            self._storage_format = \"lil\"\n        if storage_order == None:\n            if storage_format == \"auto\":\n                storage_order = \"auto\"\n            else:\n                self._storage_order = \"post_to_pre\"\n\n        # The user selected automatic format selection using heuristics\n        if storage_format == \"auto\":\n            self._storage_format = self._automatic_format_selection()\n        if storage_order == \"auto\":\n            self._storage_order = self._automatic_order_selection()\n\n        # Analyse the delay\n        if isinstance(delay, (int, float)): # Uniform delay\n            self.max_delay = round(delay/Global.config['dt'])\n            self.uniform_delay = round(delay/Global.config['dt'])\n\n        elif isinstance(delay, RandomDistribution): # Non-uniform delay\n            self.uniform_delay = -1\n            # Ensure no negative delays are generated\n            if delay.min is None or delay.min &lt; Global.config['dt']:\n                delay.min = Global.config['dt']\n            # The user needs to provide a max in order to compute max_delay\n            if delay.max is None:\n                Global._error('Projection.connect_xxx(): if you use a non-bounded random distribution for the delays (e.g. Normal), you need to set the max argument to limit the maximal delay.')\n\n            self.max_delay = round(delay.max/Global.config['dt'])\n\n        elif isinstance(delay, (list, np.ndarray)): # connect_from_matrix/sparse\n            if len(delay) &gt; 0:\n                self.uniform_delay = -1\n                self.max_delay = round(max([max(l) for l in delay])/Global.config['dt'])\n            else: # list is empty, no delay\n                self.max_delay = -1\n                self.uniform_delay = -1\n\n        else:\n            Global._error('Projection.connect_xxx(): delays are not valid!')\n\n        # Transmit the max delay to the pre pop\n        if isinstance(self.pre, PopulationView):\n            self.pre.population.max_delay = max(self.max_delay, self.pre.population.max_delay)\n        else:\n            self.pre.max_delay = max(self.max_delay, self.pre.max_delay)\n\n    def _automatic_format_selection(self):\n\"\"\"\n        We check some heuristics to select a specific format implemented as decision tree:\n\n            - If the filling degree is high enough a full matrix representation might be better\n            - if the average row length is below a threshold the ELLPACK-R might be better\n            - if the average row length is higher than a threshold the CSR might be better\n\n        HD (17th Jan. 2022): Currently structural plasticity is only usable with LIL. But one could also\n                             apply it for dense matrices in the future. For CSR and in particular the ELL-\n                             like formats the potential memory-reallocations make the structural plasticity\n                             a costly operation.\n        \"\"\"\n        # Connection pattern / Feature specific selection\n        if Global.config[\"structural_plasticity\"]:\n            storage_format = \"lil\"\n\n        elif self.connector_name == \"All-to-All\":\n            storage_format = \"dense\"\n\n        elif self.connector_name == \"One-to-One\":\n            if Global._check_paradigm(\"cuda\"):\n                storage_format = \"csr\"\n            else:\n                storage_format = \"lil\"\n\n        else:\n            if self.synapse_type.type == \"spike\":\n                # we need to build up the matrix to analyze\n                self._lil_connectivity = self._connection_method(*((self.pre, self.post,) + self._connection_args))\n\n                # get the decision parameter\n                density = float(self._lil_connectivity.nb_synapses) / float(self.pre.size * self.post.size)\n                if density &gt;= 0.6:\n                    if Global._check_paradigm(\"cuda\"):\n                        storage_format = \"csr\"  # HD (11th Nov. 2022): there is no Dense_T for spiking and CUDA yet\n                    else:\n                        storage_format = \"dense\"\n                else:\n                    storage_format = \"csr\"\n\n            else:\n                # we need to build up the matrix to analyze\n                self._lil_connectivity = self._connection_method(*((self.pre, self.post,) + self._connection_args))\n\n                # get the decision parameter\n                density = float(self._lil_connectivity.nb_synapses) / float(self.pre.size * self.post.size)\n                avg_nnz_per_row, _ = self._lil_connectivity.compute_average_row_length()\n\n                # heuristic decision tree\n                if density &gt;= 0.6:\n                    storage_format = \"dense\"\n                else:\n                    if Global._check_paradigm(\"cuda\"):\n                        if avg_nnz_per_row &lt;= 128:\n                            storage_format = \"ellr\"\n                        else:\n                            storage_format = \"csr\"\n                    else:\n                        storage_format = \"csr\"\n\n        Global._info(\"Automatic format selection for\", self.name, \":\", storage_format)\n        return storage_format\n\n    def _automatic_order_selection(self):\n\"\"\"\n        Contrary to the matrix format, the decision for the matrix order is majorly dependent on\n        the synapse type.\n        \"\"\"\n        if self.synapse_type.type == \"rate\":\n            storage_order = \"post_to_pre\"\n        else:\n            if Global._check_paradigm(\"cuda\"):\n                # HD (11th Nov. 2022): there is no Dense_T / CSRC_T for spiking and CUDA yet\n                storage_order = \"post_to_pre\"\n            else:\n                # pre-to-post is not implemented for all formats\n                if self._storage_format in [\"dense\", \"csr\"]:\n                    storage_order = \"pre_to_post\"\n                else:\n                    storage_order = \"post_to_pre\"\n\n        Global._info(\"Automatic matrix order selection for\", self.name, \":\", storage_order)\n        return storage_order\n\n    def _has_single_weight(self):\n        \"If a single weight should be generated instead of a LIL\"\n        is_cpu = Global.config['paradigm']==\"openmp\"\n        has_constant_weight = self._single_constant_weight\n        not_dense = not (self._storage_format == \"dense\")\n        no_structural_plasticity = not Global.config['structural_plasticity']\n        no_synaptic_plasticity = not self.synapse_type.description['plasticity']\n\n        return has_constant_weight and no_structural_plasticity and no_synaptic_plasticity and is_cpu and not_dense\n\n    def reset(self, attributes=-1, synapses=False):\n\"\"\"\n        Resets all parameters and variables of the projection to their initial value (before the call to compile()).\n\n        :param attributes: list of attributes (parameter or variable) which should be reinitialized. Default: all attributes (-1).\n        :param synapses: defines if the weights and delays should also be recreated. Default: False\n        \"\"\"\n        if attributes == -1:\n            attributes = self.attributes\n\n        if synapses:\n            # destroy the previous C++ content\n            self._clear()\n            # call the init connectivity again\n            self.initialized = self._connect(None)\n\n        for var in attributes:\n            # Skip w\n            if var=='w':\n                continue\n            # check it exists\n            if not var in self.attributes:\n                Global._warning(\"Projection.reset():\", var, \"is not an attribute of the population, won't reset.\")\n                continue\n            # Set the value\n            try:\n                self.__setattr__(var, self.init[var])\n            except Exception as e:\n                Global._print(e)\n                Global._warning(\"Projection.reset(): something went wrong while resetting\", var)\n\n    ################################\n    ## Dendrite access\n    ################################\n    @property\n    def size(self):\n        \"Number of post-synaptic neurons receiving synapses.\"\n        if self.cyInstance == None:\n            Global._warning(\"Access 'size or len()' attribute of a Projection is only valid after compile()\")\n            return 0\n\n        return len(self.cyInstance.post_rank())\n\n    def __len__(self):\n        # Number of postsynaptic neurons receiving synapses in this projection.\n        return self.size\n\n    @property\n    def nb_synapses(self):\n        \"Total number of synapses in the projection.\"\n        if self.cyInstance is None:\n            Global._warning(\"Access 'nb_synapses' attribute of a Projection is only valid after compile()\")\n            return 0\n        return self.cyInstance.nb_synapses()\n\n    def nb_synapses_per_dendrite(self):\n        \"Total number of synapses for each dendrite as a list.\"\n        if self.cyInstance is None:\n            Global._warning(\"Access 'nb_synapses_per_dendrite' attribute of a Projection is only valid after compile()\")\n            return []\n        return [self.cyInstance.dendrite_size(n) for n in range(self.size)]\n\n    def nb_efferent_synapses(self):\n        \"Number of efferent connections. Intended only for spiking models.\"\n        if self.cyInstance is None:\n             Global._warning(\"Access 'nb_efferent_synapses()' of a Projection is only valid after compile()\")\n             return None\n        if self.synapse_type.type == \"rate\":\n            Global._error(\"Projection.nb_efferent_synapses() is not available for rate-coded projections.\")\n\n        return self.cyInstance.nb_efferent_synapses()\n\n    @property\n    def post_ranks(self):\n        if self.cyInstance is None:\n             Global._warning(\"Access 'post_ranks' attribute of a Projection is only valid after compile()\")\n             return None\n\n        return self.cyInstance.post_rank()\n\n    @property\n    def dendrites(self):\n\"\"\"\n        Iteratively returns the dendrites corresponding to this projection.\n        \"\"\"\n        for idx, n in enumerate(self.post_ranks):\n            yield Dendrite(self, n, idx)\n\n    def dendrite(self, post):\n\"\"\"\n        Returns the dendrite of a postsynaptic neuron according to its rank.\n\n        :param post: can be either the rank or the coordinates of the post-synaptic neuron.\n        \"\"\"\n        if not self.initialized:\n            Global._error('dendrites can only be accessed after compilation.')\n\n        if isinstance(post, int):\n            rank = post\n        else:\n            rank = self.post.rank_from_coordinates(post)\n\n        if rank in self.post_ranks:\n            return Dendrite(self, rank, self.post_ranks.index(rank))\n        else:\n            Global._error(\" The neuron of rank \"+ str(rank) + \" has no dendrite in this projection.\", exit=True)\n\n\n    def synapse(self, pre, post):\n\"\"\"\n        Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise.\n\n        :param pre: rank of the pre-synaptic neuron.\n        :param post: rank of the post-synaptic neuron.\n        \"\"\"\n        if not isinstance(pre, int) or not isinstance(post, int):\n            Global._error('Projection.synapse() only accepts ranks for the pre and post neurons.')\n\n        return self.dendrite(post).synapse(pre)\n\n\n    # Iterators\n    def __getitem__(self, *args, **kwds):\n        # Returns dendrite of the given position in the postsynaptic population.\n        # If only one argument is given, it is a rank. If it is a tuple, it is coordinates.\n\n        if len(args) == 1:\n            return self.dendrite(args[0])\n        return self.dendrite(args)\n\n    def __iter__(self):\n        # Returns iteratively each dendrite in the population in ascending postsynaptic rank order.\n        for idx, n in enumerate(self.post_ranks):\n            yield Dendrite(self, n, idx)\n\n    ################################\n    ## Access to attributes\n    ################################\n    def get(self, name):\n\"\"\"\n        Returns a list of parameters/variables values for each dendrite in the projection.\n\n        The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it.\n\n        :param name: the name of the parameter or variable\n        \"\"\"\n        return self.__getattr__(name)\n\n    def set(self, value):\n\"\"\"\n        Sets the parameters/variables values for each dendrite in the projection.\n\n        For parameters, you can provide:\n\n        * a single value, which will be the same for all dendrites.\n\n        * a list or 1D numpy array of the same length as the number of actual dendrites (self.size).\n\n        For variables, you can provide:\n\n        * a single value, which will be the same for all synapses of all dendrites.\n\n        * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value.\n\n        **Warning:** it is not possible to set different values to each synapse using this method. One should iterate over the dendrites:\n\n        ```python\n        for dendrite in proj.dendrites:\n            dendrite.w = np.ones(dendrite.size)\n        ```\n\n        :param value: a dictionary with the name of the parameter/variable as key.\n\n        \"\"\"\n\n        for name, val in value.items():\n            self.__setattr__(name, val)\n\n    def __getattr__(self, name):\n        # Method called when accessing an attribute.\n        if name == 'initialized' or not hasattr(self, 'initialized'): # Before the end of the constructor\n            return object.__getattribute__(self, name)\n        elif hasattr(self, 'attributes'):\n            if name in ['plasticity', 'transmission', 'update']:\n                return self._get_flag(name)\n            if name in ['delay']:\n                return self._get_delay()\n            if name in self.attributes:\n                if not self.initialized:\n                    return self.init[name]\n                else:\n                    return self._get_cython_attribute( name )\n            elif name in self.functions:\n                return self._function(name)\n            else:\n                return object.__getattribute__(self, name)\n        return object.__getattribute__(self, name)\n\n    def __setattr__(self, name, value):\n        # Method called when setting an attribute.\n        if name == 'initialized' or not hasattr(self, 'initialized'): # Before the end of the constructor\n            object.__setattr__(self, name, value)\n        elif hasattr(self, 'attributes'):\n            if name in ['plasticity', 'transmission', 'update']:\n                self._set_flag(name, bool(value))\n                return\n            if name in ['delay']:\n                self._set_delay(value)\n                return\n            if name in self.attributes:\n                if not self.initialized:\n                    self.init[name] = value\n                else:\n                    self._set_cython_attribute(name, value)\n            else:\n                object.__setattr__(self, name, value)\n        else:\n            object.__setattr__(self, name, value)\n\n    def _get_cython_attribute(self, attribute):\n\"\"\"\n        Returns the value of the given attribute for all neurons in the population,\n        as a list of lists having the same geometry as the population if it is local.\n\n        :param attribute: a string representing the variables's name.\n\n        \"\"\"\n        # Determine C++ data type\n        ctype = self._get_attribute_cpp_type(attribute=attribute)\n\n        # retrieve the value from C++ core\n        if attribute == \"w\" and self._has_single_weight():\n            return self.cyInstance.get_global_attribute(attribute, ctype)\n        elif attribute in self.synapse_type.description['local']:\n            return self.cyInstance.get_local_attribute_all(attribute, ctype)\n        elif attribute in self.synapse_type.description['semiglobal']:\n            return self.cyInstance.get_semiglobal_attribute_all(attribute, ctype)\n        else:\n            return self.cyInstance.get_global_attribute(attribute, ctype)\n\n    def _set_cython_attribute(self, attribute, value):\n\"\"\"\n        Sets the value of the given attribute for all post-synaptic neurons in the projection,\n        as a NumPy array having the same geometry as the population if it is local.\n\n        :param attribute: a string representing the variables's name.\n        :param value: the value it should take.\n\n        \"\"\"\n        # Determine C++ data type\n        ctype = self._get_attribute_cpp_type(attribute=attribute)\n\n        # Convert np.arrays into lists/constants for better iteration\n        if isinstance(value, np.ndarray):\n            if np.ndim(value) == 0:\n                value = float(value)\n            else:\n                value = list(value)\n\n        # A list is given\n        if isinstance(value, list):\n            if len(value) == len(self.post_ranks):\n                if attribute in self.synapse_type.description['local']:\n                    for idx, n in enumerate(self.post_ranks):\n                        if not len(value[idx]) == self.cyInstance.dendrite_size(idx):\n                            Global._error('The postynaptic neuron ' + str(n) + ' receives '+ str(self.cyInstance.dendrite_size(idx))+ ' synapses.')\n                        self.cyInstance.set_local_attribute_row(attribute, idx, value[idx], ctype)\n                elif attribute in self.synapse_type.description['semiglobal']:\n                    self.cyInstance.set_semiglobal_attribute_all(attribute, value, ctype)\n                else:\n                    Global._error('The parameter', attribute, 'is global to the population, cannot assign a list.')\n            else:\n                Global._error('The projection has', self.size, 'post-synaptic neurons, the list must have the same size.')\n\n        # A Random Distribution is given\n        elif isinstance(value, RandomDistribution):\n            if attribute == \"w\" and self._has_single_weight():\n                self.cyInstance.set_global_attribute(attribute, value.get_values(1), ctype)\n            elif attribute in self.synapse_type.description['local']:\n                for idx, n in enumerate(self.post_ranks):\n                    self.cyInstance.set_local_attribute_row(attribute, idx, value.get_values(self.cyInstance.dendrite_size(idx)), ctype)\n            elif attribute in self.synapse_type.description['semiglobal']:\n                self.cyInstance.set_semiglobal_attribute_all(attribute, value.get_values(len(self.post_ranks)), ctype)\n            elif attribute in self.synapse_type.description['global']:\n                self.cyInstance.set_global_attribute(attribute, value.get_values(1), ctype)\n        # A single value is given\n        else:\n            if attribute == \"w\" and self._has_single_weight():\n                self.cyInstance.set_global_attribute(attribute, value, ctype)\n            elif attribute in self.synapse_type.description['local']:\n                for idx, n in enumerate(self.post_ranks):\n                    self.cyInstance.set_local_attribute_row(attribute, idx, value*np.ones(self.cyInstance.dendrite_size(idx)), ctype)\n            elif attribute in self.synapse_type.description['semiglobal']:\n                self.cyInstance.set_semiglobal_attribute_all(attribute, value*np.ones(len(self.post_ranks)), ctype)\n            else:\n                self.cyInstance.set_global_attribute(attribute, value, ctype)\n\n    def _get_attribute_cpp_type(self, attribute):\n\"\"\"\n        Determine C++ data type for a given attribute\n        \"\"\"\n        ctype = None\n        for var in self.synapse_type.description['variables']+self.synapse_type.description['parameters']:\n            if var['name'] == attribute:\n                ctype = var['ctype']\n\n        return ctype\n\n    def _get_flag(self, attribute):\n        \"control flow flags such as learning, transmission\"\n        if self.cyInstance is not None:\n            return getattr(self.cyInstance, '_get_'+attribute)()\n        else:\n            return self.init[attribute]\n\n    def _set_flag(self, attribute, value):\n        \"control flow flags such as learning, transmission\"\n        if self.cyInstance is not None:\n            getattr(self.cyInstance, '_set_'+attribute)(value)\n        else:\n            self.init[attribute] = value\n\n\n    ################################\n    ## Access to delays\n    ################################\n    def _get_delay(self):\n        if not hasattr(self.cyInstance, 'get_delay'):\n            if self.max_delay &lt;= 1 :\n                return Global.config['dt']\n        elif self.uniform_delay != -1:\n                return self.uniform_delay * Global.config['dt']\n        else:\n            return [[pre * Global.config['dt'] for pre in post] for post in self.cyInstance.get_delay()]\n\n    def _set_delay(self, value):\n\n        if self.cyInstance: # After compile()\n            if not hasattr(self.cyInstance, 'get_delay'):\n                if self.max_delay &lt;= 1 and value != Global.config['dt']:\n                    Global._error(\"set_delay: the projection was instantiated without delays, it is too late to create them...\")\n\n            elif self.uniform_delay != -1:\n                if isinstance(value, np.ndarray):\n                    if value.ndim &gt; 0:\n                        Global._error(\"set_delay: the projection was instantiated with uniform delays, it is too late to load non-uniform values...\")\n                    else:\n                        value = max(1, round(float(value)/Global.config['dt']))\n                elif isinstance(value, (float, int)):\n                    value = max(1, round(float(value)/Global.config['dt']))\n                else:\n                    Global._error(\"set_delay: only float, int or np.array values are possible.\")\n\n                # The new max_delay is higher than before\n                if value &gt; self.max_delay:\n                    self.max_delay = value\n                    self.uniform_delay = value\n                    self.cyInstance.set_delay(value)\n                    if isinstance(self.pre, PopulationView):\n                        self.pre.population.max_delay = max(self.max_delay, self.pre.population.max_delay)\n                        self.pre.population.cyInstance.update_max_delay(self.pre.population.max_delay)\n                    else:\n                        self.pre.max_delay = max(self.max_delay, self.pre.max_delay)\n                        self.pre.cyInstance.update_max_delay(self.pre.max_delay)\n                    return\n                else:\n                    self.uniform_delay = value\n                    self.cyInstance.set_delay(value)\n\n            else: # variable delays\n                if not isinstance(value, (np.ndarray, list)):\n                    Global._error(\"set_delay with variable delays: you must provide a list of lists of exactly the same size as before.\")\n\n                # Check the number of delays\n                nb_values = sum([len(s) for s in value])\n                if nb_values != self.nb_synapses:\n                    Global._error(\"set_delay with variable delays: the sizes do not match. You have to provide one value for each existing synapse.\")\n                if len(value) != len(self.post_ranks):\n                    Global._error(\"set_delay with variable delays: the sizes do not match. You have to provide one value for each existing synapse.\")\n\n                # Convert to steps\n                if isinstance(value, np.ndarray):\n                    delays = [[max(1, round(value[i, j]/Global.config['dt'])) for j in range(value.shape[1])] for i in range(value.shape[0])]\n                else:\n                    delays = [[max(1, round(v/Global.config['dt'])) for v in c] for c in value]\n\n                # Max delay\n                max_delay = max([max(l) for l in delays])\n\n                if max_delay &gt; self.max_delay:\n                    self.max_delay = max_delay\n\n                    # Send the max delay to the pre population\n                    if isinstance(self.pre, PopulationView):\n                        self.pre.population.max_delay = max(self.max_delay, self.pre.population.max_delay)\n                        self.pre.population.cyInstance.update_max_delay(self.pre.population.max_delay)\n                    else:\n                        self.pre.max_delay = max(self.max_delay, self.pre.max_delay)\n                        self.pre.cyInstance.update_max_delay(self.pre.max_delay)\n\n                # Send the new values to the projection\n                self.cyInstance.set_delay(delays)\n\n                # Update ring buffers (if there exist)\n                self.cyInstance.update_max_delay(self.max_delay)\n\n        else: # before compile()\n            Global._error(\"set_delay before compile(): not implemented yet.\")\n\n\n    ################################\n    ## Access to functions\n    ################################\n    def _function(self, func):\n        \"Access a user defined function\"\n        if not self.initialized:\n            Global._error('the network is not compiled yet, cannot access the function ' + func)\n\n        return getattr(self.cyInstance, func)\n\n    ################################\n    ## Learning flags\n    ################################\n    def enable_learning(self, period=None, offset=None):\n\"\"\"\n        Enables learning for all the synapses of this projection.\n\n        For example, providing the following parameters at time 10 ms:\n\n        ```python\n        enable_learning(period=10., offset=5.)\n        ```\n\n        would call the updating methods at times 15, 25, 35, etc...\n\n        The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of ``dt``.\n\n        :param period: determines how often the synaptic variables will be updated.\n        :param offset: determines the offset at which the synaptic variables will be updated relative to the current time.\n\n        \"\"\"\n        # Check arguments\n        if not period is None and not offset is None:\n            if offset &gt;= period:\n                Global._error('enable_learning(): the offset must be smaller than the period.')\n\n        if period is None and not offset is None:\n            Global._error('enable_learning(): if you define an offset, you have to define a period.')\n\n        try:\n            self.cyInstance._set_update(True)\n            self.cyInstance._set_plasticity(True)\n            if period != None:\n                self.cyInstance._set_update_period(int(period/Global.config['dt']))\n            else:\n                self.cyInstance._set_update_period(int(1))\n                period = Global.config['dt']\n            if offset != None:\n                relative_offset = Global.get_time() % period + offset\n                self.cyInstance._set_update_offset(int(int(relative_offset%period)/Global.config['dt']))\n            else:\n                self.cyInstance._set_update_offset(int(0))\n        except:\n            Global._warning('Enable_learning() is only possible after compile()')\n\n    def disable_learning(self, update=None):\n\"\"\"\n        Disables learning for all synapses of this projection.\n\n        The effect depends on the rate-coded or spiking nature of the projection:\n\n        * **Rate-coded**: the updating of all synaptic variables is disabled (including the weights ``w``). This is equivalent to ``proj.update = False``.\n\n        * **Spiking**: the updating of the weights ``w`` is disabled, but all other variables are updated. This is equivalent to ``proj.plasticity = False``.\n\n        This method is useful when performing some tests on a trained network without messing with the learned weights.\n        \"\"\"\n        try:\n            if self.synapse_type.type == 'rate':\n                self.cyInstance._set_update(False)\n            else:\n                self.cyInstance._set_plasticity(False)\n        except:\n            Global._warning('disabling learning is only possible after compile().')\n\n\n    ################################\n    ## Methods on connectivity matrix\n    ################################\n\n    def save_connectivity(self, filename):\n\"\"\"\n        Saves the connectivity of the projection into a file.\n\n        Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables.\n\n        The generated data can be used to create a projection in another network:\n\n        ```python\n        proj.connect_from_file(filename)\n        ```\n\n        * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended).\n\n        * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip.\n\n        * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed.\n\n        * Otherwise, the data will be pickled into a simple binary text file using pickle.\n\n        :param filename: file name, may contain relative or absolute path.\n\n        \"\"\"\n        # Check that the network is compiled\n        if not self.initialized:\n            Global._error('save_connectivity(): the network has not been compiled yet.')\n            return\n\n        # Check if the repertory exist\n        (path, fname) = os.path.split(filename)\n\n        if not path == '':\n            if not os.path.isdir(path):\n                Global._print('Creating folder', path)\n                os.mkdir(path)\n\n        extension = os.path.splitext(fname)[1]\n\n        # Gathering the data\n        data = {\n            'name': self.name,\n            'post_ranks': self.post_ranks,\n            'pre_ranks': np.array(self.cyInstance.pre_rank_all(), dtype=object),\n            'w': np.array(self.w, dtype=object),\n            'delay': np.array(self.cyInstance.get_delay(), dtype=object) if hasattr(self.cyInstance, 'get_delay') else None,\n            'max_delay': self.max_delay,\n            'uniform_delay': self.uniform_delay,\n            'size': self.size,\n            'nb_synapses': self.cyInstance.nb_synapses()\n        }\n\n        # Save the data\n        if extension == '.gz':\n            Global._print(\"Saving connectivity in gunzipped binary format...\")\n            try:\n                import gzip\n            except:\n                Global._error('gzip is not installed.')\n                return\n            with gzip.open(filename, mode = 'wb') as w_file:\n                try:\n                    pickle.dump(data, w_file, protocol=pickle.HIGHEST_PROTOCOL)\n                except Exception as e:\n                    Global._print('Error while saving in gzipped binary format.')\n                    Global._print(e)\n                    return\n\n        elif extension == '.npz':\n            Global._print(\"Saving connectivity in Numpy format...\")\n            np.savez_compressed(filename, **data )\n\n        elif extension == '.mat':\n            Global._print(\"Saving connectivity in Matlab format...\")\n            if data['delay'] is None:\n                data['delay'] = 0\n            try:\n                import scipy.io as sio\n                sio.savemat(filename, data)\n            except Exception as e:\n                Global._error('Error while saving in Matlab format.')\n                Global._print(e)\n                return\n\n        else:\n            Global._print(\"Saving connectivity in text format...\")\n            # save in Pythons pickle format\n            with open(filename, mode = 'wb') as w_file:\n                try:\n                    pickle.dump(data, w_file, protocol=pickle.HIGHEST_PROTOCOL)\n                except Exception as e:\n                    Global._print('Error while saving in text format.')\n                    Global._print(e)\n                    return\n            return\n\n    def receptive_fields(self, variable = 'w', in_post_geometry = True):\n\"\"\"\n        Gathers all receptive fields within this projection.\n\n        :param variable: name of the variable\n        :param in_post_geometry: if False, the data will be plotted as square grid. (default = True)\n        \"\"\"\n        if in_post_geometry:\n            x_size = self.post.geometry[1]\n            y_size = self.post.geometry[0]\n        else:\n            x_size = int( math.floor(math.sqrt(self.post.size)) )\n            y_size = int( math.ceil(math.sqrt(self.post.size)) )\n\n\n        def get_rf(rank): # TODO: IMPROVE\n            res = np.zeros( self.pre.size )\n            for n in range(len(self.post_ranks)):\n                if self.post_ranks[n] == n:\n                    pre_ranks = self.cyInstance.pre_rank(n)\n                    data = self.cyInstance.get_local_attribute_row(variable, rank, Global.config[\"precision\"])\n                    for j in range(len(pre_ranks)):\n                        res[pre_ranks[j]] = data[j]\n            return res.reshape(self.pre.geometry)\n\n        res = np.zeros((1, x_size*self.pre.geometry[1]))\n        for y in range ( y_size ):\n            row = np.concatenate(  [ get_rf(self.post.rank_from_coordinates( (y, x) ) ) for x in range ( x_size ) ], axis = 1)\n            res = np.concatenate((res, row))\n\n        return res\n\n    def connectivity_matrix(self, fill=0.0):\n\"\"\"\n        Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations.\n\n        The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones.\n\n        If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default.\n\n        :param fill: value to put in the matrix when there is no connection (default: 0.0).\n        \"\"\"\n        if not self.initialized:\n            Global._error('The connectivity matrix can only be accessed after compilation')\n\n        # get correct dimensions for dense matrix\n        if isinstance(self.pre, PopulationView):\n            size_pre = self.pre.population.size\n        else:\n            size_pre = self.pre.size\n        if isinstance(self.post, PopulationView):\n            size_post = self.post.population.size\n        else:\n            size_post = self.post.size\n\n        # create empty dense matrix with default values\n        res = np.ones((size_post, size_pre)) * fill\n\n        # fill row-by-row with real values\n        for rank in self.post_ranks:\n            # row-rank\n            if self._storage_format == \"dense\":\n                idx = rank\n            else:\n                idx =  self.post_ranks.index(rank)\n            # pre-ranks\n            preranks = self.cyInstance.pre_rank(idx)\n            # get the values\n            if \"w\" in self.synapse_type.description['local'] and (not self._has_single_weight()):\n                w = self.cyInstance.get_local_attribute_row(\"w\", idx, Global.config[\"precision\"])\n            elif \"w\" in self.synapse_type.description['semiglobal']:\n                w = self.cyInstance.get_semiglobal_attribute(\"w\", idx, Global.config[\"precision\"])*np.ones(self.cyInstance.dendrite_size(idx))\n            else:\n                w = self.cyInstance.get_global_attribute(\"w\", Global.config[\"precision\"])*np.ones(self.cyInstance.dendrite_size(idx))\n            res[rank, preranks] = w\n        return res\n\n\n    ################################\n    ## Save/load methods\n    ################################\n\n    def _data(self):\n        \"Method gathering all info about the projection when calling save()\"\n\n        if not self.initialized:\n            Global._error('save_connectivity(): the network has not been compiled yet.')\n\n        desc = {}\n        desc['name'] = self.name\n        desc['pre'] = self.pre.name\n        desc['post'] = self.post.name\n        desc['target'] = self.target\n        desc['post_ranks'] = self.post_ranks\n        desc['attributes'] = self.attributes\n        desc['parameters'] = self.parameters\n        desc['variables'] = self.variables\n        desc['delays'] = self._get_delay()\n\n        # Determine if we have varying number of elements per row\n        # based on the pre-synaptic ranks\n        pre_ranks = self.cyInstance.pre_rank_all()\n        dend_size = len(pre_ranks[0])\n        ragged_list = False\n        for i in range(1, len(pre_ranks)):\n            if len(pre_ranks[i]) != dend_size:\n                ragged_list = True\n                break\n\n        # Save pre_ranks\n        if ragged_list:\n            desc['pre_ranks'] = np.array(self.cyInstance.pre_rank_all(), dtype=object)\n        else:\n            desc['pre_ranks'] = np.array(self.cyInstance.pre_rank_all())\n\n        # Attributes to save\n        attributes = self.attributes\n        if not 'w' in self.attributes:\n            attributes.append('w')\n\n        # Save all attributes\n        for var in attributes:\n            try:\n                ctype = self._get_attribute_cpp_type(var)\n                if var == \"w\" and self._has_single_weight():\n                    desc[var] = self.cyInstance.get_global_attribute(\"w\", ctype)\n\n                elif var in self.synapse_type.description['local']:\n                    if ragged_list:\n                        desc[var] = np.array(self.cyInstance.get_local_attribute_all(var, ctype), dtype=object)\n                    else:\n                        desc[var] = self.cyInstance.get_local_attribute_all(var, ctype)\n                elif var in self.synapse_type.description['semiglobal']:\n                    desc[var] = self.cyInstance.get_semiglobal_attribute_all(var, ctype)\n                else:\n                    desc[var] = self.cyInstance.get_global_attribute(var, ctype) # linear array or single constant\n            except:\n                Global._warning('Can not save the attribute ' + var + ' in the projection.')\n\n        return desc\n\n    def save(self, filename):\n\"\"\"\n        Saves all information about the projection (connectivity, current value of parameters and variables) into a file.\n\n        * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended).\n\n        * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip.\n\n        * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed.\n\n        * Otherwise, the data will be pickled into a simple binary text file using pickle.\n\n        :param filename: file name, may contain relative or absolute path.\n\n        **Warning:** the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose.\n\n        Example:\n\n        ```python\n        proj.save('proj1.npz')\n        proj.save('proj1.txt')\n        proj.save('proj1.txt.gz')\n        proj.save('proj1.mat')\n        ```\n        \"\"\"\n        from ANNarchy.core.IO import _save_data\n        _save_data(filename, self._data())\n\n\n    def load(self, filename, pickle_encoding=None):\n\"\"\"\n        Loads the saved state of the projection by `Projection.save()`.\n\n        Warning: Matlab data can not be loaded.\n\n        Example:\n\n        ```python\n        proj.load('proj1.npz')\n        proj.load('proj1.txt')\n        proj.load('proj1.txt.gz')\n        ```\n\n        :param filename: the file name with relative or absolute path.\n        \"\"\"\n        from ANNarchy.core.IO import _load_connectivity_data\n        self._load_proj_data(_load_connectivity_data(filename, pickle_encoding))\n\n\n    def _load_proj_data(self, desc):\n\"\"\"\n        Updates the projection with the stored data set.\n        \"\"\"\n        # Sanity check\n        if desc == None:\n            # _load_proj should have printed an error message\n            return\n\n        # If it's not saveable there is nothing to load\n        if not self._saveable:\n            return\n\n        # Check deprecation\n        if not 'attributes' in desc.keys():\n            Global._error('The file was saved using a deprecated version of ANNarchy.')\n            return\n        if 'dendrites' in desc: # Saved before 4.5.3\n            Global._error(\"The file was saved using a deprecated version of ANNarchy.\")\n            return\n\n        # If the post ranks and/or pre-ranks have changed, overwrite\n        connectivity_changed=False\n        if 'post_ranks' in desc and not np.all((desc['post_ranks']) == self.post_ranks):\n            connectivity_changed=True\n        if 'pre_ranks' in desc and not np.all((desc['pre_ranks']) == np.array(self.cyInstance.pre_rank_all(), dtype=object)):\n            connectivity_changed=True\n\n        # synaptic weights\n        weights = desc[\"w\"]\n\n        # Delays can be either uniform (int, float) or non-uniform (np.ndarray).\n        # HD (30th May 2022):\n        #   Unfortunately, the storage of constants changed over the time. At the\n        #   end of this code block, we should have either a single constant or a\n        #   numpy nd-array\n        delays = 0\n        if 'delays' in desc:\n            delays = desc['delays']\n\n            if isinstance(delays, (float, int)):\n                # will be handled below\n                pass\n\n            elif isinstance(delays, np.ndarray):\n                # constants are stored as 0-darray\n                if delays.ndim == 0:\n                    # transform into single float\n                    delays = float(delays)\n                else:\n                    # nothing to do as it is numpy nd-array\n                    pass\n\n            else:\n                # ragged list to nd-array\n                delays = np.array(delays, dtype=object)\n\n        # Some patterns like fixed_number_pre/post or fixed_probability change the\n        # connectivity. If this is not the case, we can simply set the values.\n        if connectivity_changed:\n            # (re-)initialize connectivity\n            if isinstance(delays, (float, int)):\n                delays = [[delays]] # wrapper expects list from list\n\n            self.cyInstance.init_from_lil(desc['post_ranks'], desc['pre_ranks'], weights, delays)\n        else:\n            # set weights\n            self._set_cython_attribute(\"w\", weights)\n\n            # set delays if there were some\n            self._set_delay(delays)\n\n        # Other variables\n        for var in desc['attributes']:\n            if var == \"w\":\n                continue # already done\n\n            try:\n                self._set_cython_attribute(var, desc[var])\n            except Exception as e:\n                Global._print(e)\n                Global._warning('load(): the variable', var, 'does not exist in the current version of the network, skipping it.')\n                continue\n\n    ################################\n    ## Structural plasticity\n    ################################\n    def start_pruning(self, period=None):\n\"\"\"\n        Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument.\n\n        'structural_plasticity' must be set to True in setup().\n\n        :param period: how often pruning should be evaluated (default: dt, i.e. each step)\n        \"\"\"\n        if not period:\n            period = Global.config['dt']\n        if not self.cyInstance:\n            Global._error('Can not start pruning if the network is not compiled.')\n\n        if Global.config['structural_plasticity']:\n            try:\n                self.cyInstance.start_pruning(int(period/Global.config['dt']), Global.get_current_step())\n            except :\n                Global._error(\"The synapse does not define a 'pruning' argument.\")\n\n        else:\n            Global._error(\"You must set 'structural_plasticity' to True in setup() to start pruning connections.\")\n\n\n    def stop_pruning(self):\n\"\"\"\n        Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument.\n\n        'structural_plasticity' must be set to True in setup().\n        \"\"\"\n        if not self.cyInstance:\n            Global._error('Can not stop pruning if the network is not compiled.')\n\n        if Global.config['structural_plasticity']:\n            try:\n                self.cyInstance.stop_pruning()\n            except:\n                Global._error(\"The synapse does not define a 'pruning' argument.\")\n\n        else:\n            Global._error(\"You must set 'structural_plasticity' to True in setup() to start pruning connections.\")\n\n    def start_creating(self, period=None):\n\"\"\"\n        Starts creating the synapses in the projection if the synapse defines a 'creating' argument.\n\n        'structural_plasticity' must be set to True in setup().\n\n        :param period: how often creating should be evaluated (default: dt, i.e. each step)\n        \"\"\"\n        if not period:\n            period = Global.config['dt']\n        if not self.cyInstance:\n            Global._error('Can not start creating if the network is not compiled.')\n\n        if Global.config['structural_plasticity']:\n            try:\n                self.cyInstance.start_creating(int(period/Global.config['dt']), Global.get_current_step())\n            except:\n                Global._error(\"The synapse does not define a 'creating' argument.\")\n\n        else:\n            Global._error(\"You must set 'structural_plasticity' to True in setup() to start creating connections.\")\n\n    def stop_creating(self):\n\"\"\"\n        Stops creating the synapses in the projection if the synapse defines a 'creating' argument.\n\n        'structural_plasticity' must be set to True in setup().\n        \"\"\"\n        if not self.cyInstance:\n            Global._error('Can not stop creating if the network is not compiled.')\n\n        if Global.config['structural_plasticity']:\n            try:\n                self.cyInstance.stop_creating()\n            except:\n                Global._error(\"The synapse does not define a 'creating' argument.\")\n\n        else:\n            Global._error(\"You must set 'structural_plasticity' to True in setup() to start creating connections.\")\n\n    ################################\n    # Paradigm specific functions\n    ################################\n    def update_launch_config(self, nb_blocks=-1, threads_per_block=32):\n\"\"\"\n        Since ANNarchy 4.7.2 we allow the adjustment of the CUDA launch config.\n\n        Parameters:\n\n        :nb_blocks:         number of CUDA blocks which can be 65535 at maximum. If set to -1 the number\n                            of launched blocks is computed by ANNarchy.\n        :threads_per_block: number of CUDA threads for one block which can be maximum 1024.\n        \"\"\"\n        if not Global._check_paradigm(\"cuda\"):\n            Global._warning(\"Projection.update_launch_config() is intended for usage on CUDA devices\")\n            return\n\n        if self.initialized:\n            self.cyInstance.update_launch_config(nb_blocks=nb_blocks, threads_per_block=threads_per_block)\n        else:\n            Global._error(\"Projection.update_launch_config() should be called after compile()\")\n\n    ################################\n    ## Memory Management\n    ################################\n    def size_in_bytes(self):\n\"\"\"\n        Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked.\n        \"\"\"\n        if self.initialized:\n            return self.cyInstance.size_in_bytes()\n        else:\n            return 0\n\n    def _clear(self):\n\"\"\"\n        Deallocates the container within the C++ instance. The population object is not usable anymore after calling this function.\n\n        Warning: should be only called by the net deconstructor (in the context of parallel_run).\n        \"\"\"\n        if self.initialized:\n            self.cyInstance.clear()\n            self.initialized = False\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.dendrites","title":"<code>dendrites</code>  <code>property</code>","text":"<p>Iteratively returns the dendrites corresponding to this projection.</p>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.nb_synapses","title":"<code>nb_synapses</code>  <code>property</code>","text":"<p>Total number of synapses in the projection.</p>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.size","title":"<code>size</code>  <code>property</code>","text":"<p>Number of post-synaptic neurons receiving synapses.</p>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.__init__","title":"<code>__init__(pre, post, target, synapse=None, name=None, disable_omp=True, copied=False)</code>","text":"<p>By default, the synapse only ensures linear synaptic transmission:</p> <ul> <li>For rate-coded populations: <code>psp = w * pre.r</code></li> <li>For spiking populations: <code>g_target += w</code></li> </ul> <p>to modify this behavior one need to provide a Synapse object.</p> <p>Parameters:</p> <ul> <li> pre         \u2013          <p>pre-synaptic population (either its name or a <code>Population</code> object).</p> </li> <li> post         \u2013          <p>post-synaptic population (either its name or a <code>Population</code> object).</p> </li> <li> target         \u2013          <p>type of the connection.</p> </li> <li> synapse         \u2013          <p>a <code>Synapse</code> instance.</p> </li> <li> name         \u2013          <p>unique name of the projection (optional, it defaults to <code>proj0</code>, <code>proj1</code>, etc).</p> </li> <li> disable_omp         \u2013          <p>especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to <code>False</code>.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def __init__(self, pre, post, target, synapse=None, name=None, disable_omp=True, copied=False):\n\"\"\"\n    By default, the synapse only ensures linear synaptic transmission:\n\n    * For rate-coded populations: ``psp = w * pre.r``\n    * For spiking populations: ``g_target += w``\n\n    to modify this behavior one need to provide a Synapse object.\n\n    :param pre: pre-synaptic population (either its name or a ``Population`` object).\n    :param post: post-synaptic population (either its name or a ``Population`` object).\n    :param target: type of the connection.\n    :param synapse: a ``Synapse`` instance.\n    :param name: unique name of the projection (optional, it defaults to ``proj0``, ``proj1``, etc).\n    :param disable_omp: especially for small- and mid-scale sparse spiking networks the parallelization of spike propagation is not scalable. But it can be enabled by setting this parameter to `False`.\n    \"\"\"\n    # Check if the network has already been compiled\n    if Global._network[0]['compiled'] and not copied:\n        Global._error('you cannot add a projection after the network has been compiled.')\n\n    # Store the pre and post synaptic populations\n    # the user provide either a string or a population object\n    # in case of string, we need to search for the corresponding object\n    if isinstance(pre, str):\n        for pop in Global._network[0]['populations']:\n            if pop.name == pre:\n                self.pre = pop\n    else:\n        self.pre = pre\n\n    if isinstance(post, str):\n        for pop in Global._network[0]['populations']:\n            if pop.name == post:\n                self.post = pop\n    else:\n        self.post = post\n\n    # Store the arguments\n    if isinstance(target, list) and len(target) == 1:\n        self.target = target[0]\n    else:\n        self.target = target\n\n    # Add the target(s) to the postsynaptic population\n    if isinstance(self.target, list):\n        for _target in self.target:\n            self.post.targets.append(_target)\n    else:\n        self.post.targets.append(self.target)\n\n    # check if a synapse description is attached\n    if not synapse:\n        # No synapse attached assume default synapse based on\n        # presynaptic population.\n        if self.pre.neuron_type.type == 'rate':\n            from ANNarchy.models.Synapses import DefaultRateCodedSynapse\n            self.synapse_type = DefaultRateCodedSynapse()\n            self.synapse_type.type = 'rate'\n        else:\n            from ANNarchy.models.Synapses import DefaultSpikingSynapse\n            self.synapse_type = DefaultSpikingSynapse()\n            self.synapse_type.type = 'spike'\n\n    elif inspect.isclass(synapse):\n        self.synapse_type = synapse()\n        self.synapse_type.type = self.pre.neuron_type.type\n    else:\n        self.synapse_type = copy.deepcopy(synapse)\n        self.synapse_type.type = self.pre.neuron_type.type\n\n    # Disable omp for spiking networks\n    self.disable_omp = disable_omp\n\n    # Analyse the parameters and variables\n    self.synapse_type._analyse()\n\n    # Create a default name\n    self.id = len(Global._network[0]['projections'])\n    if name:\n        self.name = name\n    else:\n        self.name = 'proj'+str(self.id)\n\n    # Container for control/attribute states\n    self.init = {}\n\n    # Control-flow variables\n    self.init[\"transmission\"] = True\n    self.init[\"update\"] = True\n    self.init[\"plasticity\"] = True\n\n    # Get a list of parameters and variables\n    self.parameters = []\n    for param in self.synapse_type.description['parameters']:\n        self.parameters.append(param['name'])\n        self.init[param['name']] = param['init']\n\n    self.variables = []\n    for var in self.synapse_type.description['variables']:\n        self.variables.append(var['name'])\n        self.init[var['name']] = var['init']\n\n    self.attributes = self.parameters + self.variables\n\n    # Get a list of user-defined functions\n    self.functions = [func['name'] for func in self.synapse_type.description['functions']]\n\n    # Add the population to the global network\n    Global._network[0]['projections'].append(self)\n\n    # Finalize initialization\n    self.initialized = False\n\n    # Cython instance\n    self.cyInstance = None\n\n    # Connectivity\n    self._synapses = None\n    self._connection_method = None\n    self._connection_args = None\n    self._connection_delay = None\n    self._connector = None\n    self._lil_connectivity = None\n\n    # Default configuration for connectivity\n    self._storage_format = \"lil\"\n    self._storage_order = \"post_to_pre\"\n\n    # If a single weight value is used\n    self._single_constant_weight = False\n\n    # Are random distribution used for weights/delays\n    self.connector_weight_dist = None\n    self.connector_delay_dist = None\n\n    # Reporting\n    self.connector_name = \"Specific\"\n    self.connector_description = \"Specific\"\n\n    # Overwritten by derived classes, to add\n    # additional code\n    self._specific_template = {}\n\n    # Set to False by derived classes to prevent saving of\n    # data, e. g. in case of weight-sharing projections\n    self._saveable = True\n\n    # To allow case-specific adjustment of parallelization\n    # parameters, e. g. openMP schedule, we introduce a\n    # dictionary read by the ProjectionGenerator.\n    #\n    # Will be overwritten either by inherited classes or\n    # by an omp_config provided to the compile() method.\n    self._omp_config = {\n        #'psp_schedule': 'schedule(dynamic)'\n    }\n\n    # If set to true, the code generator is not allowed to\n    # split the matrix. This will be the case for many\n    # SpecificProjections defined by the user or is disabled\n    # globally.\n    if self.synapse_type.type == \"rate\":\n        # Normally, the split should not be used for rate-coded models\n        # but maybe there are cases where we want to enable it ...\n        self._no_split_matrix = Global.config[\"disable_split_matrix\"]\n\n        # If the number of elements is too small, the split\n        # might not be efficient.\n        if self.post.size &lt; Global.OMP_MIN_NB_NEURONS:\n            self._no_split_matrix = True\n\n    else:\n        # If the number of elements is too small, the split\n        # might not be efficient.\n        if self.post.size &lt; Global.OMP_MIN_NB_NEURONS:\n            self._no_split_matrix = True\n        else:\n            self._no_split_matrix = Global.config[\"disable_split_matrix\"]\n\n    # In particular for spiking models, the parallelization on the\n    # inner or outer loop can make a performance difference\n    if self._no_split_matrix:\n        # LIL and CSR are parallelized on inner loop\n        # to prevent cost of atomic operations\n        self._parallel_pattern = 'inner_loop'\n    else:\n        # splitted matrices are always parallelized on outer loop!\n        self._parallel_pattern = 'outer_loop'\n\n    # For dense matrix format: do we use an optimization for population views?\n    if self.synapse_type.type == \"rate\":\n        # HD (9th Nov. 2022): currently this optimization is only intended for spiking models\n        self._has_pop_view = False\n    else:\n        # HD (9th Nov. 2022): currently disabled, more testing is required ...\n        self._has_pop_view = False #isinstance(self.pre, PopulationView) or isinstance(self.post, PopulationView)\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.connectivity_matrix","title":"<code>connectivity_matrix(fill=0.0)</code>","text":"<p>Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations.</p> <p>The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones.</p> <p>If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default.</p> <p>Parameters:</p> <ul> <li> fill         \u2013          <p>value to put in the matrix when there is no connection (default: 0.0).</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def connectivity_matrix(self, fill=0.0):\n\"\"\"\n    Returns a dense connectivity matrix (2D Numpy array) representing the connections between the pre- and post-populations.\n\n    The first index of the matrix represents post-synaptic neurons, the second the pre-synaptic ones.\n\n    If PopulationViews were used for creating the projection, the matrix is expanded to the whole populations by default.\n\n    :param fill: value to put in the matrix when there is no connection (default: 0.0).\n    \"\"\"\n    if not self.initialized:\n        Global._error('The connectivity matrix can only be accessed after compilation')\n\n    # get correct dimensions for dense matrix\n    if isinstance(self.pre, PopulationView):\n        size_pre = self.pre.population.size\n    else:\n        size_pre = self.pre.size\n    if isinstance(self.post, PopulationView):\n        size_post = self.post.population.size\n    else:\n        size_post = self.post.size\n\n    # create empty dense matrix with default values\n    res = np.ones((size_post, size_pre)) * fill\n\n    # fill row-by-row with real values\n    for rank in self.post_ranks:\n        # row-rank\n        if self._storage_format == \"dense\":\n            idx = rank\n        else:\n            idx =  self.post_ranks.index(rank)\n        # pre-ranks\n        preranks = self.cyInstance.pre_rank(idx)\n        # get the values\n        if \"w\" in self.synapse_type.description['local'] and (not self._has_single_weight()):\n            w = self.cyInstance.get_local_attribute_row(\"w\", idx, Global.config[\"precision\"])\n        elif \"w\" in self.synapse_type.description['semiglobal']:\n            w = self.cyInstance.get_semiglobal_attribute(\"w\", idx, Global.config[\"precision\"])*np.ones(self.cyInstance.dendrite_size(idx))\n        else:\n            w = self.cyInstance.get_global_attribute(\"w\", Global.config[\"precision\"])*np.ones(self.cyInstance.dendrite_size(idx))\n        res[rank, preranks] = w\n    return res\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.dendrite","title":"<code>dendrite(post)</code>","text":"<p>Returns the dendrite of a postsynaptic neuron according to its rank.</p> <p>Parameters:</p> <ul> <li> post         \u2013          <p>can be either the rank or the coordinates of the post-synaptic neuron.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def dendrite(self, post):\n\"\"\"\n    Returns the dendrite of a postsynaptic neuron according to its rank.\n\n    :param post: can be either the rank or the coordinates of the post-synaptic neuron.\n    \"\"\"\n    if not self.initialized:\n        Global._error('dendrites can only be accessed after compilation.')\n\n    if isinstance(post, int):\n        rank = post\n    else:\n        rank = self.post.rank_from_coordinates(post)\n\n    if rank in self.post_ranks:\n        return Dendrite(self, rank, self.post_ranks.index(rank))\n    else:\n        Global._error(\" The neuron of rank \"+ str(rank) + \" has no dendrite in this projection.\", exit=True)\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.disable_learning","title":"<code>disable_learning(update=None)</code>","text":"<p>Disables learning for all synapses of this projection.</p> <p>The effect depends on the rate-coded or spiking nature of the projection:</p> <ul> <li> <p>Rate-coded: the updating of all synaptic variables is disabled (including the weights <code>w</code>). This is equivalent to <code>proj.update = False</code>.</p> </li> <li> <p>Spiking: the updating of the weights <code>w</code> is disabled, but all other variables are updated. This is equivalent to <code>proj.plasticity = False</code>.</p> </li> </ul> <p>This method is useful when performing some tests on a trained network without messing with the learned weights.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def disable_learning(self, update=None):\n\"\"\"\n    Disables learning for all synapses of this projection.\n\n    The effect depends on the rate-coded or spiking nature of the projection:\n\n    * **Rate-coded**: the updating of all synaptic variables is disabled (including the weights ``w``). This is equivalent to ``proj.update = False``.\n\n    * **Spiking**: the updating of the weights ``w`` is disabled, but all other variables are updated. This is equivalent to ``proj.plasticity = False``.\n\n    This method is useful when performing some tests on a trained network without messing with the learned weights.\n    \"\"\"\n    try:\n        if self.synapse_type.type == 'rate':\n            self.cyInstance._set_update(False)\n        else:\n            self.cyInstance._set_plasticity(False)\n    except:\n        Global._warning('disabling learning is only possible after compile().')\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.enable_learning","title":"<code>enable_learning(period=None, offset=None)</code>","text":"<p>Enables learning for all the synapses of this projection.</p> <p>For example, providing the following parameters at time 10 ms:</p> <pre><code>enable_learning(period=10., offset=5.)\n</code></pre> <p>would call the updating methods at times 15, 25, 35, etc...</p> <p>The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of <code>dt</code>.</p> <p>Parameters:</p> <ul> <li> period         \u2013          <p>determines how often the synaptic variables will be updated.</p> </li> <li> offset         \u2013          <p>determines the offset at which the synaptic variables will be updated relative to the current time.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def enable_learning(self, period=None, offset=None):\n\"\"\"\n    Enables learning for all the synapses of this projection.\n\n    For example, providing the following parameters at time 10 ms:\n\n    ```python\n    enable_learning(period=10., offset=5.)\n    ```\n\n    would call the updating methods at times 15, 25, 35, etc...\n\n    The default behaviour is that the synaptic variables are updated at each time step. The parameters must be multiple of ``dt``.\n\n    :param period: determines how often the synaptic variables will be updated.\n    :param offset: determines the offset at which the synaptic variables will be updated relative to the current time.\n\n    \"\"\"\n    # Check arguments\n    if not period is None and not offset is None:\n        if offset &gt;= period:\n            Global._error('enable_learning(): the offset must be smaller than the period.')\n\n    if period is None and not offset is None:\n        Global._error('enable_learning(): if you define an offset, you have to define a period.')\n\n    try:\n        self.cyInstance._set_update(True)\n        self.cyInstance._set_plasticity(True)\n        if period != None:\n            self.cyInstance._set_update_period(int(period/Global.config['dt']))\n        else:\n            self.cyInstance._set_update_period(int(1))\n            period = Global.config['dt']\n        if offset != None:\n            relative_offset = Global.get_time() % period + offset\n            self.cyInstance._set_update_offset(int(int(relative_offset%period)/Global.config['dt']))\n        else:\n            self.cyInstance._set_update_offset(int(0))\n    except:\n        Global._warning('Enable_learning() is only possible after compile()')\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.get","title":"<code>get(name)</code>","text":"<p>Returns a list of parameters/variables values for each dendrite in the projection.</p> <p>The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it.</p> <p>Parameters:</p> <ul> <li> name         \u2013          <p>the name of the parameter or variable</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def get(self, name):\n\"\"\"\n    Returns a list of parameters/variables values for each dendrite in the projection.\n\n    The list will have the same length as the number of actual dendrites (self.size), so it can be smaller than the size of the postsynaptic population. Use self.post_ranks to indice it.\n\n    :param name: the name of the parameter or variable\n    \"\"\"\n    return self.__getattr__(name)\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.load","title":"<code>load(filename, pickle_encoding=None)</code>","text":"<p>Loads the saved state of the projection by <code>Projection.save()</code>.</p> <p>Warning: Matlab data can not be loaded.</p> <p>Example:</p> <pre><code>proj.load('proj1.npz')\nproj.load('proj1.txt')\nproj.load('proj1.txt.gz')\n</code></pre> <p>Parameters:</p> <ul> <li> filename         \u2013          <p>the file name with relative or absolute path.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def load(self, filename, pickle_encoding=None):\n\"\"\"\n    Loads the saved state of the projection by `Projection.save()`.\n\n    Warning: Matlab data can not be loaded.\n\n    Example:\n\n    ```python\n    proj.load('proj1.npz')\n    proj.load('proj1.txt')\n    proj.load('proj1.txt.gz')\n    ```\n\n    :param filename: the file name with relative or absolute path.\n    \"\"\"\n    from ANNarchy.core.IO import _load_connectivity_data\n    self._load_proj_data(_load_connectivity_data(filename, pickle_encoding))\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.nb_efferent_synapses","title":"<code>nb_efferent_synapses()</code>","text":"<p>Number of efferent connections. Intended only for spiking models.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def nb_efferent_synapses(self):\n    \"Number of efferent connections. Intended only for spiking models.\"\n    if self.cyInstance is None:\n         Global._warning(\"Access 'nb_efferent_synapses()' of a Projection is only valid after compile()\")\n         return None\n    if self.synapse_type.type == \"rate\":\n        Global._error(\"Projection.nb_efferent_synapses() is not available for rate-coded projections.\")\n\n    return self.cyInstance.nb_efferent_synapses()\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.nb_synapses_per_dendrite","title":"<code>nb_synapses_per_dendrite()</code>","text":"<p>Total number of synapses for each dendrite as a list.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def nb_synapses_per_dendrite(self):\n    \"Total number of synapses for each dendrite as a list.\"\n    if self.cyInstance is None:\n        Global._warning(\"Access 'nb_synapses_per_dendrite' attribute of a Projection is only valid after compile()\")\n        return []\n    return [self.cyInstance.dendrite_size(n) for n in range(self.size)]\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.receptive_fields","title":"<code>receptive_fields(variable='w', in_post_geometry=True)</code>","text":"<p>Gathers all receptive fields within this projection.</p> <p>Parameters:</p> <ul> <li> variable         \u2013          <p>name of the variable</p> </li> <li> in_post_geometry         \u2013          <p>if False, the data will be plotted as square grid. (default = True)</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def receptive_fields(self, variable = 'w', in_post_geometry = True):\n\"\"\"\n    Gathers all receptive fields within this projection.\n\n    :param variable: name of the variable\n    :param in_post_geometry: if False, the data will be plotted as square grid. (default = True)\n    \"\"\"\n    if in_post_geometry:\n        x_size = self.post.geometry[1]\n        y_size = self.post.geometry[0]\n    else:\n        x_size = int( math.floor(math.sqrt(self.post.size)) )\n        y_size = int( math.ceil(math.sqrt(self.post.size)) )\n\n\n    def get_rf(rank): # TODO: IMPROVE\n        res = np.zeros( self.pre.size )\n        for n in range(len(self.post_ranks)):\n            if self.post_ranks[n] == n:\n                pre_ranks = self.cyInstance.pre_rank(n)\n                data = self.cyInstance.get_local_attribute_row(variable, rank, Global.config[\"precision\"])\n                for j in range(len(pre_ranks)):\n                    res[pre_ranks[j]] = data[j]\n        return res.reshape(self.pre.geometry)\n\n    res = np.zeros((1, x_size*self.pre.geometry[1]))\n    for y in range ( y_size ):\n        row = np.concatenate(  [ get_rf(self.post.rank_from_coordinates( (y, x) ) ) for x in range ( x_size ) ], axis = 1)\n        res = np.concatenate((res, row))\n\n    return res\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.reset","title":"<code>reset(attributes=-1, synapses=False)</code>","text":"<p>Resets all parameters and variables of the projection to their initial value (before the call to compile()).</p> <p>Parameters:</p> <ul> <li> attributes         \u2013          <p>list of attributes (parameter or variable) which should be reinitialized. Default: all attributes (-1).</p> </li> <li> synapses         \u2013          <p>defines if the weights and delays should also be recreated. Default: False</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def reset(self, attributes=-1, synapses=False):\n\"\"\"\n    Resets all parameters and variables of the projection to their initial value (before the call to compile()).\n\n    :param attributes: list of attributes (parameter or variable) which should be reinitialized. Default: all attributes (-1).\n    :param synapses: defines if the weights and delays should also be recreated. Default: False\n    \"\"\"\n    if attributes == -1:\n        attributes = self.attributes\n\n    if synapses:\n        # destroy the previous C++ content\n        self._clear()\n        # call the init connectivity again\n        self.initialized = self._connect(None)\n\n    for var in attributes:\n        # Skip w\n        if var=='w':\n            continue\n        # check it exists\n        if not var in self.attributes:\n            Global._warning(\"Projection.reset():\", var, \"is not an attribute of the population, won't reset.\")\n            continue\n        # Set the value\n        try:\n            self.__setattr__(var, self.init[var])\n        except Exception as e:\n            Global._print(e)\n            Global._warning(\"Projection.reset(): something went wrong while resetting\", var)\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.save","title":"<code>save(filename)</code>","text":"<p>Saves all information about the projection (connectivity, current value of parameters and variables) into a file.</p> <ul> <li> <p>If the file name is '.npz', the data will be saved and compressed using <code>np.savez_compressed</code> (recommended).</p> </li> <li> <p>If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip.</p> </li> <li> <p>If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed.</p> </li> <li> <p>Otherwise, the data will be pickled into a simple binary text file using pickle.</p> </li> </ul> <p>Parameters:</p> <ul> <li> filename         \u2013          <p>file name, may contain relative or absolute path.  Warning: the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose.  Example:  <code>python proj.save('proj1.npz') proj.save('proj1.txt') proj.save('proj1.txt.gz') proj.save('proj1.mat')</code></p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Saves all information about the projection (connectivity, current value of parameters and variables) into a file.\n\n    * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended).\n\n    * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip.\n\n    * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed.\n\n    * Otherwise, the data will be pickled into a simple binary text file using pickle.\n\n    :param filename: file name, may contain relative or absolute path.\n\n    **Warning:** the '.mat' data will not be loadable by ANNarchy, it is only for external analysis purpose.\n\n    Example:\n\n    ```python\n    proj.save('proj1.npz')\n    proj.save('proj1.txt')\n    proj.save('proj1.txt.gz')\n    proj.save('proj1.mat')\n    ```\n    \"\"\"\n    from ANNarchy.core.IO import _save_data\n    _save_data(filename, self._data())\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.save_connectivity","title":"<code>save_connectivity(filename)</code>","text":"<p>Saves the connectivity of the projection into a file.</p> <p>Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables.</p> <p>The generated data can be used to create a projection in another network:</p> <pre><code>proj.connect_from_file(filename)\n</code></pre> <ul> <li> <p>If the file name is '.npz', the data will be saved and compressed using <code>np.savez_compressed</code> (recommended).</p> </li> <li> <p>If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip.</p> </li> <li> <p>If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed.</p> </li> <li> <p>Otherwise, the data will be pickled into a simple binary text file using pickle.</p> </li> </ul> <p>Parameters:</p> <ul> <li> filename         \u2013          <p>file name, may contain relative or absolute path.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def save_connectivity(self, filename):\n\"\"\"\n    Saves the connectivity of the projection into a file.\n\n    Only the connectivity matrix, the weights and delays are saved, not the other synaptic variables.\n\n    The generated data can be used to create a projection in another network:\n\n    ```python\n    proj.connect_from_file(filename)\n    ```\n\n    * If the file name is '.npz', the data will be saved and compressed using `np.savez_compressed` (recommended).\n\n    * If the file name ends with '.gz', the data will be pickled into a binary file and compressed using gzip.\n\n    * If the file name is '.mat', the data will be saved as a Matlab 7.2 file. Scipy must be installed.\n\n    * Otherwise, the data will be pickled into a simple binary text file using pickle.\n\n    :param filename: file name, may contain relative or absolute path.\n\n    \"\"\"\n    # Check that the network is compiled\n    if not self.initialized:\n        Global._error('save_connectivity(): the network has not been compiled yet.')\n        return\n\n    # Check if the repertory exist\n    (path, fname) = os.path.split(filename)\n\n    if not path == '':\n        if not os.path.isdir(path):\n            Global._print('Creating folder', path)\n            os.mkdir(path)\n\n    extension = os.path.splitext(fname)[1]\n\n    # Gathering the data\n    data = {\n        'name': self.name,\n        'post_ranks': self.post_ranks,\n        'pre_ranks': np.array(self.cyInstance.pre_rank_all(), dtype=object),\n        'w': np.array(self.w, dtype=object),\n        'delay': np.array(self.cyInstance.get_delay(), dtype=object) if hasattr(self.cyInstance, 'get_delay') else None,\n        'max_delay': self.max_delay,\n        'uniform_delay': self.uniform_delay,\n        'size': self.size,\n        'nb_synapses': self.cyInstance.nb_synapses()\n    }\n\n    # Save the data\n    if extension == '.gz':\n        Global._print(\"Saving connectivity in gunzipped binary format...\")\n        try:\n            import gzip\n        except:\n            Global._error('gzip is not installed.')\n            return\n        with gzip.open(filename, mode = 'wb') as w_file:\n            try:\n                pickle.dump(data, w_file, protocol=pickle.HIGHEST_PROTOCOL)\n            except Exception as e:\n                Global._print('Error while saving in gzipped binary format.')\n                Global._print(e)\n                return\n\n    elif extension == '.npz':\n        Global._print(\"Saving connectivity in Numpy format...\")\n        np.savez_compressed(filename, **data )\n\n    elif extension == '.mat':\n        Global._print(\"Saving connectivity in Matlab format...\")\n        if data['delay'] is None:\n            data['delay'] = 0\n        try:\n            import scipy.io as sio\n            sio.savemat(filename, data)\n        except Exception as e:\n            Global._error('Error while saving in Matlab format.')\n            Global._print(e)\n            return\n\n    else:\n        Global._print(\"Saving connectivity in text format...\")\n        # save in Pythons pickle format\n        with open(filename, mode = 'wb') as w_file:\n            try:\n                pickle.dump(data, w_file, protocol=pickle.HIGHEST_PROTOCOL)\n            except Exception as e:\n                Global._print('Error while saving in text format.')\n                Global._print(e)\n                return\n        return\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.set","title":"<code>set(value)</code>","text":"<p>Sets the parameters/variables values for each dendrite in the projection.</p> <p>For parameters, you can provide:</p> <ul> <li> <p>a single value, which will be the same for all dendrites.</p> </li> <li> <p>a list or 1D numpy array of the same length as the number of actual dendrites (self.size).</p> </li> </ul> <p>For variables, you can provide:</p> <ul> <li> <p>a single value, which will be the same for all synapses of all dendrites.</p> </li> <li> <p>a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value.</p> </li> </ul> <p>Warning: it is not possible to set different values to each synapse using this method. One should iterate over the dendrites:</p> <pre><code>for dendrite in proj.dendrites:\n    dendrite.w = np.ones(dendrite.size)\n</code></pre> <p>Parameters:</p> <ul> <li> value         \u2013          <p>a dictionary with the name of the parameter/variable as key.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def set(self, value):\n\"\"\"\n    Sets the parameters/variables values for each dendrite in the projection.\n\n    For parameters, you can provide:\n\n    * a single value, which will be the same for all dendrites.\n\n    * a list or 1D numpy array of the same length as the number of actual dendrites (self.size).\n\n    For variables, you can provide:\n\n    * a single value, which will be the same for all synapses of all dendrites.\n\n    * a list or 1D numpy array of the same length as the number of actual dendrites (self.size). The synapses of each postsynaptic neuron will take the same value.\n\n    **Warning:** it is not possible to set different values to each synapse using this method. One should iterate over the dendrites:\n\n    ```python\n    for dendrite in proj.dendrites:\n        dendrite.w = np.ones(dendrite.size)\n    ```\n\n    :param value: a dictionary with the name of the parameter/variable as key.\n\n    \"\"\"\n\n    for name, val in value.items():\n        self.__setattr__(name, val)\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.size_in_bytes","title":"<code>size_in_bytes()</code>","text":"<p>Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def size_in_bytes(self):\n\"\"\"\n    Returns the size in bytes of the allocated memory on C++ side. Note that this does not reflect monitored data and that it only works after compile() was invoked.\n    \"\"\"\n    if self.initialized:\n        return self.cyInstance.size_in_bytes()\n    else:\n        return 0\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.start_creating","title":"<code>start_creating(period=None)</code>","text":"<p>Starts creating the synapses in the projection if the synapse defines a 'creating' argument.</p> <p>'structural_plasticity' must be set to True in setup().</p> <p>Parameters:</p> <ul> <li> period         \u2013          <p>how often creating should be evaluated (default: dt, i.e. each step)</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def start_creating(self, period=None):\n\"\"\"\n    Starts creating the synapses in the projection if the synapse defines a 'creating' argument.\n\n    'structural_plasticity' must be set to True in setup().\n\n    :param period: how often creating should be evaluated (default: dt, i.e. each step)\n    \"\"\"\n    if not period:\n        period = Global.config['dt']\n    if not self.cyInstance:\n        Global._error('Can not start creating if the network is not compiled.')\n\n    if Global.config['structural_plasticity']:\n        try:\n            self.cyInstance.start_creating(int(period/Global.config['dt']), Global.get_current_step())\n        except:\n            Global._error(\"The synapse does not define a 'creating' argument.\")\n\n    else:\n        Global._error(\"You must set 'structural_plasticity' to True in setup() to start creating connections.\")\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.start_pruning","title":"<code>start_pruning(period=None)</code>","text":"<p>Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument.</p> <p>'structural_plasticity' must be set to True in setup().</p> <p>Parameters:</p> <ul> <li> period         \u2013          <p>how often pruning should be evaluated (default: dt, i.e. each step)</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def start_pruning(self, period=None):\n\"\"\"\n    Starts pruning the synapses in the projection if the synapse defines a 'pruning' argument.\n\n    'structural_plasticity' must be set to True in setup().\n\n    :param period: how often pruning should be evaluated (default: dt, i.e. each step)\n    \"\"\"\n    if not period:\n        period = Global.config['dt']\n    if not self.cyInstance:\n        Global._error('Can not start pruning if the network is not compiled.')\n\n    if Global.config['structural_plasticity']:\n        try:\n            self.cyInstance.start_pruning(int(period/Global.config['dt']), Global.get_current_step())\n        except :\n            Global._error(\"The synapse does not define a 'pruning' argument.\")\n\n    else:\n        Global._error(\"You must set 'structural_plasticity' to True in setup() to start pruning connections.\")\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.stop_creating","title":"<code>stop_creating()</code>","text":"<p>Stops creating the synapses in the projection if the synapse defines a 'creating' argument.</p> <p>'structural_plasticity' must be set to True in setup().</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def stop_creating(self):\n\"\"\"\n    Stops creating the synapses in the projection if the synapse defines a 'creating' argument.\n\n    'structural_plasticity' must be set to True in setup().\n    \"\"\"\n    if not self.cyInstance:\n        Global._error('Can not stop creating if the network is not compiled.')\n\n    if Global.config['structural_plasticity']:\n        try:\n            self.cyInstance.stop_creating()\n        except:\n            Global._error(\"The synapse does not define a 'creating' argument.\")\n\n    else:\n        Global._error(\"You must set 'structural_plasticity' to True in setup() to start creating connections.\")\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.stop_pruning","title":"<code>stop_pruning()</code>","text":"<p>Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument.</p> <p>'structural_plasticity' must be set to True in setup().</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def stop_pruning(self):\n\"\"\"\n    Stops pruning the synapses in the projection if the synapse defines a 'pruning' argument.\n\n    'structural_plasticity' must be set to True in setup().\n    \"\"\"\n    if not self.cyInstance:\n        Global._error('Can not stop pruning if the network is not compiled.')\n\n    if Global.config['structural_plasticity']:\n        try:\n            self.cyInstance.stop_pruning()\n        except:\n            Global._error(\"The synapse does not define a 'pruning' argument.\")\n\n    else:\n        Global._error(\"You must set 'structural_plasticity' to True in setup() to start pruning connections.\")\n</code></pre>"},{"location":"API/Projection.html#ANNarchy.core.Projection.Projection.synapse","title":"<code>synapse(pre, post)</code>","text":"<p>Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise.</p> <p>Parameters:</p> <ul> <li> pre         \u2013          <p>rank of the pre-synaptic neuron.</p> </li> <li> post         \u2013          <p>rank of the post-synaptic neuron.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/Projection.py</code> <pre><code>def synapse(self, pre, post):\n\"\"\"\n    Returns the synapse between a pre- and a post-synaptic neuron if it exists, None otherwise.\n\n    :param pre: rank of the pre-synaptic neuron.\n    :param post: rank of the post-synaptic neuron.\n    \"\"\"\n    if not isinstance(pre, int) or not isinstance(post, int):\n        Global._error('Projection.synapse() only accepts ranks for the pre and post neurons.')\n\n    return self.dendrite(post).synapse(pre)\n</code></pre>"},{"location":"API/RandomDistribution.html","title":"Random Distributions","text":""},{"location":"API/RandomDistribution.html#introduction","title":"Introduction","text":"<p>Random number generators can be used at several places:</p> <ul> <li>while initializing parameters or variables,</li> <li>while creating connection patterns,</li> <li>when injecting noise into a neural or synaptic variable.</li> </ul> <p>ANNarchy provides several random distribution objects, implementing the following distributions:</p> <ul> <li>Uniform</li> <li>DiscreteUniform</li> <li>Normal</li> <li>LogNormal</li> <li>Gamma</li> <li>Exponential</li> </ul> <p>Warning</p> <p>DiscreteUniform, Gamma and Exponential distributions are not available if the CUDA paradigm is used.</p> <p>They can be used in the Python code, as a normal object:</p> <pre><code>dist = Uniform(-1.0, 1.0)\nvalues = dist.get_values(100)\n</code></pre> <p>or inside mathematical expressions:</p> <pre><code>tau * dv/dt + v = g_exc + Normal(0.0, 20.0)\n``````\n\nThe Python objects rely on the `numpy.random` library, while the C++ values are based on the standard library of C++11.\n\nThe seed of the underlying random number generator (Mersenne twister, mt19937 in C++11) can be set globally, by defining its value in `setup()`:\n\n```python\nsetup(seed=187348768237)\n</code></pre> <p>All random distribution objects (Python or C++) will use this seed. By default, the global seed is taken to be <code>time(NULL)</code>.</p> <p>The seed can also be set individually for each RandomDistribution object as a last argument:</p> <pre><code>dist = Uniform(-1.0, 1.0, 36875937346)\n</code></pre> <p>as well as in a mathematical expression:</p> <pre><code>tau * dv/dt + v = g_exc + Normal(0.0, 20.0, 497536526)\n</code></pre>"},{"location":"API/RandomDistribution.html#implementation-details","title":"Implementation details","text":"<p>ANNarchy uses default implementations for random number generation: STL methods of C++11 for OpenMP and the device API of the curand library for CUDA.</p> <p>As engines, we use mt19937 on openMP side and XORWOW on CUDA. The latter is subject to changes in future releases.</p> <p>It may be important to know that the drawing mechanisms differ between openMP and CUDA slightly:</p> <ul> <li>openMP: all distribution objects draw the numbers from one source in a single thread.</li> <li>CUDA: each distribution object has its own source, the random numbers are drawn in a parallel way.</li> </ul> <p>For further details on random numbers on GPUs please refer to the curand documentation:</p> <p>http://docs.nvidia.com/cuda/curand/device-api-overview.html#device-api-overview</p>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform","title":"<code>ANNarchy.core.Random.Uniform</code>","text":"<p>             Bases: <code>ANNarchy.core.Random.RandomDistribution</code></p> <p>Random distribution object using the uniform distribution between <code>min</code> and <code>max</code>.</p> <p>The returned values are floats in the range [min, max].</p>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.__init__","title":"<code>__init__(min, max)</code>","text":"<p>Parameters:</p> <ul> <li> min         \u2013          <p>minimum value.</p> </li> <li> max         \u2013          <p>maximum value.</p> </li> </ul>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Uniform.get_values","title":"<code>get_values(shape)</code>","text":"<p>Returns a Numpy array with the given shape.</p>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform","title":"<code>ANNarchy.core.Random.DiscreteUniform</code>","text":"<p>             Bases: <code>ANNarchy.core.Random.RandomDistribution</code></p> <p>Random distribution object using the discrete uniform distribution between <code>min</code> and <code>max</code>.</p> <p>The returned values are integers in the range [min, max].</p>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.__init__","title":"<code>__init__(min, max)</code>","text":"<p>Parameters:</p> <ul> <li> min         \u2013          <p>minimum value.</p> </li> <li> max         \u2013          <p>maximum value.</p> </li> </ul>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.DiscreteUniform.get_values","title":"<code>get_values(shape)</code>","text":"<p>Returns a np.ndarray with the given shape.</p>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal","title":"<code>ANNarchy.core.Random.Normal</code>","text":"<p>             Bases: <code>ANNarchy.core.Random.RandomDistribution</code></p> <p>Random distribution instance returning a random value based on a normal (Gaussian) distribution.</p>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.__init__","title":"<code>__init__(mu, sigma, min=None, max=None)</code>","text":"<p>Parameters:</p> <ul> <li> mu         \u2013          <p>mean of the distribution.</p> </li> <li> sigma         \u2013          <p>standard deviation of the distribution.</p> </li> <li> min         \u2013          <p>minimum value (default: unlimited).</p> </li> <li> max         \u2013          <p>maximum value (default: unlimited).</p> </li> </ul>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Normal.get_values","title":"<code>get_values(shape)</code>","text":"<p>Returns a np.ndarray with the given shape</p>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal","title":"<code>ANNarchy.core.Random.LogNormal</code>","text":"<p>             Bases: <code>ANNarchy.core.Random.RandomDistribution</code></p> <p>Random distribution instance returning a random value based on lognormal distribution.</p>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.__init__","title":"<code>__init__(mu, sigma, min=None, max=None)</code>","text":"<p>Parameters:</p> <ul> <li> mu         \u2013          <p>mean of the distribution.</p> </li> <li> sigma         \u2013          <p>standard deviation of the distribution.</p> </li> <li> min         \u2013          <p>minimum value (default: unlimited).</p> </li> <li> max         \u2013          <p>maximum value (default: unlimited).</p> </li> </ul>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.LogNormal.get_values","title":"<code>get_values(shape)</code>","text":"<p>Returns a np.ndarray with the given shape</p>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma","title":"<code>ANNarchy.core.Random.Gamma</code>","text":"<p>             Bases: <code>ANNarchy.core.Random.RandomDistribution</code></p> <p>Random distribution instance returning a random value based on gamma distribution.</p>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.__init__","title":"<code>__init__(alpha, beta=1.0, seed=-1, min=None, max=None)</code>","text":"<p>Parameters:</p> <ul> <li> alpha         \u2013          <p>shape of the gamma distribution</p> </li> <li> beta         \u2013          <p>scale of the gamma distribution</p> </li> <li> min         \u2013          <p>minimum value returned (default: unlimited).</p> </li> <li> max         \u2013          <p>maximum value returned (default: unlimited).</p> </li> </ul>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Gamma.get_values","title":"<code>get_values(shape)</code>","text":"<p>Returns a np.ndarray with the given shape</p>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential","title":"<code>ANNarchy.core.Random.Exponential</code>","text":"<p>             Bases: <code>ANNarchy.core.Random.RandomDistribution</code></p> <p>Random distribution instance returning a random value based on exponential distribution, according the density function:</p> \\[P(x | \\lambda) = \\lambda e^{(-\\lambda x )}\\]"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.__init__","title":"<code>__init__(Lambda, min=None, max=None)</code>","text":"<p>Note: <code>Lambda</code> is capitalized, otherwise it would be a reserved Python keyword.</p> <p>Parameters:</p> <ul> <li> Lambda         \u2013          <p>rate parameter.</p> </li> <li> min         \u2013          <p>minimum value (default: unlimited).</p> </li> <li> max         \u2013          <p>maximum value (default: unlimited).</p> </li> </ul>"},{"location":"API/RandomDistribution.html#ANNarchy.core.Random.Exponential.get_values","title":"<code>get_values(shape)</code>","text":"<p>Returns a np.ndarray with the given shape.</p>"},{"location":"API/SpecificNeuron.html","title":"Built-in neuron types","text":"<p>ANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN (http://neuralensemble.org/docs/PyNN/reference/neuronmodels.html).</p>"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.LeakyIntegrator","title":"<code>ANNarchy.models.Neurons.LeakyIntegrator</code>","text":"<p>             Bases: <code>Neuron</code></p> <p>Leaky-integrator rate-coded neuron, optionally noisy.</p> <p>This simple rate-coded neuron defines an internal variable \\(v(t)\\)  which integrates the inputs \\(I(t)\\) with a time constant \\(\\tau\\) and a baseline \\(B\\).  An additive noise \\(N(t)\\) can be optionally defined: </p> \\[\\tau \\cdot \\frac{dv(t)}{dt} + v(t) = I(t) + B + N(t)\\] <p>The transfer function is the positive (or rectified linear ReLU) function with a threshold \\(T\\):</p> \\[r(t) = (v(t) - T)^+\\] <p>By default, the input \\(I(t)\\) to this neuron is \"sum(exc) - sum(inh)\", but this can be changed by  setting the <code>sum</code> argument:</p> <pre><code>neuron = LeakyIntegrator(sum=\"sum('exc')\")\n</code></pre> <p>By default, there is no additive noise, but the <code>noise</code> argument can be passed with a specific distribution:</p> <pre><code>neuron = LeakyIntegrator(noise=\"Normal(0.0, 1.0)\")\n</code></pre> <p>Parameters:</p> <ul> <li>tau = 10.0 : Time constant in ms of the neuron.</li> <li>B = 0.0 : Baseline value for v.</li> <li>T = 0.0 : Threshold for the positive transfer function.</li> </ul> <p>Variables:</p> <ul> <li> <p>v : internal variable (init = 0.0):</p> <p>tau * dv/dt + v = sum(exc) - sum(inh) + B + N</p> </li> <li> <p>r : firing rate (init = 0.0):</p> <p>r = pos(v - T)</p> </li> </ul> <p>The ODE is solved using the exponential Euler method.</p> <p>Equivalent code:</p> <pre><code>LeakyIntegrator = Neuron(\n    parameters='''\n        tau = 10.0 : population\n        B = 0.0\n        T = 0.0 : population\n    ''', \n    equations='''\n        tau * dv/dt + v = sum(exc) - sum(inh) + B : exponential\n        r = pos(v - T)\n    '''\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Neurons.py</code> <pre><code>class LeakyIntegrator(Neuron):\nr\"\"\"\n    Leaky-integrator rate-coded neuron, optionally noisy.\n\n    This simple rate-coded neuron defines an internal variable $v(t)$ \n    which integrates the inputs $I(t)$ with a time constant $\\tau$ and a baseline $B$. \n    An additive noise $N(t)$ can be optionally defined: \n\n    $$\\tau \\cdot \\frac{dv(t)}{dt} + v(t) = I(t) + B + N(t)$$\n\n    The transfer function is the positive (or rectified linear ReLU) function with a threshold $T$:\n\n    $$r(t) = (v(t) - T)^+$$\n\n    By default, the input $I(t)$ to this neuron is \"sum(exc) - sum(inh)\", but this can be changed by \n    setting the ``sum`` argument:\n\n    ```python\n    neuron = LeakyIntegrator(sum=\"sum('exc')\")\n    ```\n\n    By default, there is no additive noise, but the ``noise`` argument can be passed with a specific distribution:\n\n    ```python\n    neuron = LeakyIntegrator(noise=\"Normal(0.0, 1.0)\")\n    ```\n\n    Parameters:\n\n    * tau = 10.0 : Time constant in ms of the neuron.\n    * B = 0.0 : Baseline value for v.\n    * T = 0.0 : Threshold for the positive transfer function.\n\n    Variables:\n\n    * v : internal variable (init = 0.0):\n\n        tau * dv/dt + v = sum(exc) - sum(inh) + B + N\n\n    * r : firing rate (init = 0.0):\n\n        r = pos(v - T)\n\n    The ODE is solved using the exponential Euler method.\n\n    Equivalent code:\n\n    ```python\n    LeakyIntegrator = Neuron(\n        parameters='''\n            tau = 10.0 : population\n            B = 0.0\n            T = 0.0 : population\n        ''', \n        equations='''\n            tau * dv/dt + v = sum(exc) - sum(inh) + B : exponential\n            r = pos(v - T)\n        '''\n    )\n    ```\n    \"\"\"\n\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, tau=10.0, B=0.0, T=0.0, sum='sum(exc) - sum(inh)', noise=None):\n        # Create the arguments\n        parameters = \"\"\"\n            tau = %(tau)s : population\n            B = %(B)s\n            T = %(T)s : population\n        \"\"\" % {'tau': tau, 'B': B, 'T': T}\n\n        # Equations for the variables\n        if not noise:\n            noise_def = ''\n        else:\n            noise_def = '+ ' + noise\n\n        equations=\"\"\"\n            tau * dv/dt + v = %(sum)s + B %(noise)s : exponential\n            r = pos(v - T)\n        \"\"\" % { 'sum' : sum, 'noise': noise_def}\n\n        Neuron.__init__(self, \n            parameters=parameters, equations=equations,\n            name=\"Leaky-Integrator\", \n            description=\"Leaky-Integrator with positive transfer function and additive noise.\")\n\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.Izhikevich","title":"<code>ANNarchy.models.Neurons.Izhikevich</code>","text":"<p>             Bases: <code>Neuron</code></p> <p>Izhikevich neuron as proposed in:</p> <p>Izhikevich, E.M. (2003). Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks, 14:6. http://dx.doi.org/10.1109/TNN.2003.820440</p> <p>The equations are:</p> \\[\\frac{dv}{dt} = 0.04 * v^2 + 5.0 * v + 140.0 - u + I\\] \\[\\frac{du}{dt} = a * (b * v - u)\\] <p>By default, the conductance is \"g_exc - g_inh\", but this can be changed by setting the <code>conductance</code> argument:</p> <pre><code>neuron = Izhikevich(conductance='g_ampa * (1 + g_nmda) - g_gaba')\n</code></pre> <p>The synapses are instantaneous, i.e the corresponding conductance is increased from the synaptic efficiency w at the time step when a spike is received.</p> <p>Parameters:</p> <ul> <li>a = 0.02 : Speed of the recovery variable</li> <li>b = 0.2: Scaling of the recovery variable</li> <li>c = -65.0 : Reset potential.</li> <li>d = 8.0 : Increment of the recovery variable after a spike.</li> <li>v_thresh = 30.0 : Spike threshold (mV).</li> <li>i_offset = 0.0 : external current (nA).</li> <li>noise = 0.0 : Amplitude of the normal additive noise.</li> <li>tau_refrac = 0.0 : Duration of refractory period (ms).</li> </ul> <p>Variables:</p> <ul> <li> <p>I : input current (user-defined conductance/current + external current + normal noise):</p> <p>I = conductance + i_offset + noise * Normal(0.0, 1.0)</p> </li> <li> <p>v : membrane potential in mV (init = c):</p> <p>dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I </p> </li> <li> <p>u : recovery variable (init= b * c):</p> <p>du/dt = a * (b * v - u) </p> </li> </ul> <p>Spike emission:</p> <pre><code>v &gt; v_thresh\n</code></pre> <p>Reset:</p> <pre><code>v = c\nu += d\n</code></pre> <p>The ODEs are solved using the explicit Euler method.</p> <p>Equivalent code:</p> <pre><code>    Izhikevich = Neuron(\n        parameters = \"\"\"\n            noise = 0.0\n            a = 0.02\n            b = 0.2\n            c = -65.0\n            d = 8.0\n            v_thresh = 30.0\n            i_offset = 0.0\n        \"\"\", \n        equations = \"\"\"\n            I = g_exc - g_inh + noise * Normal(0.0, 1.0) + i_offset\n            dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init = -65.0\n            du/dt = a * (b*v - u) : init= -13.0\n        \"\"\",\n        spike = \"v &gt; v_thresh\",\n        reset = \"v = c; u += d\",\n        refractory = 0.0\n    )\n</code></pre> <p>The default parameters are for a regular spiking (RS) neuron derived from the above mentioned article.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Neurons.py</code> <pre><code>class Izhikevich(Neuron):\n'''\n    Izhikevich neuron as proposed in:\n\n    &gt; Izhikevich, E.M. (2003). *Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks*, 14:6. &lt;http://dx.doi.org/10.1109/TNN.2003.820440&gt;\n\n    The equations are:\n\n    $$\\\\frac{dv}{dt} = 0.04 * v^2 + 5.0 * v + 140.0 - u + I$$\n\n    $$\\\\frac{du}{dt} = a * (b * v - u)$$\n\n    By default, the conductance is \"g_exc - g_inh\", but this can be changed by setting the ``conductance`` argument:\n\n    ```python\n    neuron = Izhikevich(conductance='g_ampa * (1 + g_nmda) - g_gaba')\n    ```\n\n    The synapses are instantaneous, i.e the corresponding conductance is increased from the synaptic efficiency w at the time step when a spike is received.\n\n    Parameters:\n\n    * a = 0.02 : Speed of the recovery variable\n    * b = 0.2: Scaling of the recovery variable\n    * c = -65.0 : Reset potential.\n    * d = 8.0 : Increment of the recovery variable after a spike.\n    * v_thresh = 30.0 : Spike threshold (mV).\n    * i_offset = 0.0 : external current (nA).\n    * noise = 0.0 : Amplitude of the normal additive noise.\n    * tau_refrac = 0.0 : Duration of refractory period (ms).\n\n    Variables:\n\n    * I : input current (user-defined conductance/current + external current + normal noise):\n\n        I = conductance + i_offset + noise * Normal(0.0, 1.0)\n\n    * v : membrane potential in mV (init = c):\n\n        dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I \n\n    * u : recovery variable (init= b * c):\n\n        du/dt = a * (b * v - u) \n\n    Spike emission:\n\n        v &gt; v_thresh\n\n    Reset:\n\n        v = c\n        u += d \n\n    The ODEs are solved using the explicit Euler method.\n\n    Equivalent code:\n\n    ```python\n\n        Izhikevich = Neuron(\n            parameters = \"\"\"\n                noise = 0.0\n                a = 0.02\n                b = 0.2\n                c = -65.0\n                d = 8.0\n                v_thresh = 30.0\n                i_offset = 0.0\n            \"\"\", \n            equations = \"\"\"\n                I = g_exc - g_inh + noise * Normal(0.0, 1.0) + i_offset\n                dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init = -65.0\n                du/dt = a * (b*v - u) : init= -13.0\n            \"\"\",\n            spike = \"v &gt; v_thresh\",\n            reset = \"v = c; u += d\",\n            refractory = 0.0\n        )\n    ```\n\n    The default parameters are for a regular spiking (RS) neuron derived from the above mentioned article.\n    '''\n\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, \n        a=0.02, \n        b=0.2, \n        c=-65.0, \n        d=8.0, \n        v_thresh=30.0, \n        i_offset=0.0, \n        noise=0.0, \n        tau_refrac=0.0, \n        conductance=\"g_exc - g_inh\"):\n\n        # Extract which targets are defined in the conductance\n        #import re\n        #targets = re.findall(r'g_([\\w]+)', conductance)\n\n        # Create the arguments\n        parameters = \"\"\"\n            noise = %(noise)s\n            a = %(a)s\n            b = %(b)s\n            c = %(c)s\n            d = %(d)s\n            v_thresh = %(v_thresh)s\n            i_offset = %(i_offset)s\n            tau_refrac = %(tau_refrac)s\n        \"\"\" % {'a': a, 'b':b, 'c':c, 'd':d, 'v_thresh':v_thresh, 'i_offset':i_offset, 'noise':noise, 'tau_refrac':tau_refrac}\n\n        # Equations for the variables\n        equations=\"\"\"\n            I = %(conductance)s + noise * Normal(0.0, 1.0) + i_offset\n            dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init = %(c)s\n            du/dt = a * (b*v - u) : init= %(u)s\n        \"\"\" % { 'conductance' : conductance, 'c':c , 'u': b*c}\n\n        spike = \"\"\"\n            v &gt; v_thresh\n        \"\"\"\n        reset = \"\"\"\n            v = c\n            u += d\n        \"\"\"\n        Neuron.__init__(self, \n            parameters=parameters, equations=equations, spike=spike, reset=reset, refractory='tau_refrac',\n            name=\"Izhikevich\", description=\"Quadratic integrate-and-fire spiking neuron with adaptation.\")\n\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.IF_curr_exp","title":"<code>ANNarchy.models.Neurons.IF_curr_exp</code>","text":"<p>             Bases: <code>Neuron</code></p> <p>IF_curr_exp neuron.</p> <p>Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current. (Separate synaptic currents for excitatory and inhibitory synapses).</p> <p>Parameters:</p> <ul> <li>v_rest = -65.0 :  Resting membrane potential (mV)</li> <li>cm  = 1.0 : Capacity of the membrane (nF)</li> <li>tau_m  = 20.0 : Membrane time constant (ms)</li> <li>tau_refrac = 0.0 : Duration of refractory period (ms)</li> <li>tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms)</li> <li>tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms)</li> <li>i_offset = 0.0 : Offset current (nA)</li> <li>v_reset = -65.0 : Reset potential after a spike (mV)</li> <li>v_thresh = -50.0 : Spike threshold (mV)</li> </ul> <p>Variables:</p> <ul> <li> <p>v : membrane potential in mV (init=-65.0):</p> <p>cm * dv/dt = cm/tau_m*(v_rest -v)   + g_exc - g_inh + i_offset</p> </li> <li> <p>g_exc : excitatory current (init = 0.0):</p> <p>tau_syn_E * dg_exc/dt = - g_exc</p> </li> <li> <p>g_inh : inhibitory current (init = 0.0):</p> <p>tau_syn_I * dg_inh/dt = - g_inh</p> </li> </ul> <p>Spike emission:</p> <pre><code>v &gt; v_thresh\n</code></pre> <p>Reset:</p> <pre><code>v = v_reset\n</code></pre> <p>The ODEs are solved using the exponential Euler method.</p> <p>Equivalent code:</p> <pre><code>IF_curr_exp = Neuron(\n    parameters = \"\"\"\n        v_rest = -65.0\n        cm  = 1.0\n        tau_m  = 20.0\n        tau_syn_E = 5.0\n        tau_syn_I = 5.0\n        v_thresh = -50.0\n        v_reset = -65.0\n        i_offset = 0.0\n    \"\"\", \n    equations = \"\"\"\n        cm * dv/dt = cm/tau_m*(v_rest -v)   + g_exc - g_inh + i_offset : exponential, init=-65.0\n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n    \"\"\",\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = v_reset\",\n    refractory = 0.0\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Neurons.py</code> <pre><code>class IF_curr_exp(Neuron):\n'''\n    IF_curr_exp neuron.\n\n    Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current. (Separate synaptic currents for excitatory and inhibitory synapses).\n\n    Parameters:\n\n    * v_rest = -65.0 :  Resting membrane potential (mV)\n    * cm  = 1.0 : Capacity of the membrane (nF)\n    * tau_m  = 20.0 : Membrane time constant (ms)\n    * tau_refrac = 0.0 : Duration of refractory period (ms)\n    * tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms)\n    * tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms)\n    * i_offset = 0.0 : Offset current (nA)\n    * v_reset = -65.0 : Reset potential after a spike (mV)\n    * v_thresh = -50.0 : Spike threshold (mV)\n\n    Variables:\n\n    * v : membrane potential in mV (init=-65.0):\n\n        cm * dv/dt = cm/tau_m*(v_rest -v)   + g_exc - g_inh + i_offset\n\n    * g_exc : excitatory current (init = 0.0):\n\n        tau_syn_E * dg_exc/dt = - g_exc\n\n    * g_inh : inhibitory current (init = 0.0):\n\n        tau_syn_I * dg_inh/dt = - g_inh\n\n\n    Spike emission:\n\n        v &gt; v_thresh\n\n    Reset:\n\n        v = v_reset\n\n    The ODEs are solved using the exponential Euler method.\n\n    Equivalent code:\n\n    ```python\n\n    IF_curr_exp = Neuron(\n        parameters = \"\"\"\n            v_rest = -65.0\n            cm  = 1.0\n            tau_m  = 20.0\n            tau_syn_E = 5.0\n            tau_syn_I = 5.0\n            v_thresh = -50.0\n            v_reset = -65.0\n            i_offset = 0.0\n        \"\"\", \n        equations = \"\"\"\n            cm * dv/dt = cm/tau_m*(v_rest -v)   + g_exc - g_inh + i_offset : exponential, init=-65.0\n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n        \"\"\",\n        spike = \"v &gt; v_thresh\",\n        reset = \"v = v_reset\",\n        refractory = 0.0\n    )\n    ```\n\n    '''\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, v_rest=-65.0, cm=1.0, tau_m=20.0, tau_refrac=0.0, \n                tau_syn_E=5.0, tau_syn_I=5.0, v_thresh=-50.0, v_reset=-65.0, i_offset=0.0):\n\n        # Create the arguments\n        parameters = \"\"\"\n            v_rest = %(v_rest)s\n            cm  = %(cm)s\n            tau_m  = %(tau_m)s\n            tau_refrac = %(tau_refrac)s\n            tau_syn_E = %(tau_syn_E)s\n            tau_syn_I = %(tau_syn_I)s\n            v_thresh = %(v_thresh)s\n            v_reset = %(v_reset)s\n            i_offset = %(i_offset)s\n        \"\"\" % {'v_rest':v_rest, 'cm':cm, 'tau_m':tau_m, 'tau_refrac':tau_refrac, \n                'tau_syn_E':tau_syn_E, 'tau_syn_I':tau_syn_I, \n                'v_thresh':v_thresh, 'v_reset':v_reset, 'i_offset':i_offset}\n\n        # Equations for the variables\n        equations=\"\"\"    \n            cm * dv/dt = cm/tau_m*(v_rest -v)   + g_exc - g_inh + i_offset : exponential, init=%(v_reset)s\n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n        \"\"\" % {'v_reset':v_reset}\n\n        spike = \"\"\"\n            v &gt; v_thresh\n        \"\"\"\n        reset = \"\"\"\n            v = v_reset\n        \"\"\"\n        Neuron.__init__(self, parameters=parameters, equations=equations, \n            spike=spike, reset=reset, refractory='tau_refrac',\n            name=\"Integrate-and-Fire\", \n            description=\"Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic current.\")\n\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.IF_cond_exp","title":"<code>ANNarchy.models.Neurons.IF_cond_exp</code>","text":"<p>             Bases: <code>Neuron</code></p> <p>IF_cond_exp neuron.</p> <p>Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance.</p> <p>Parameters:</p> <ul> <li>v_rest = -65.0 :  Resting membrane potential (mV)</li> <li>cm  = 1.0 : Capacity of the membrane (nF)</li> <li>tau_m  = 20.0 : Membrane time constant (ms)</li> <li>tau_refrac = 0.0 : Duration of refractory period (ms)</li> <li>tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms)</li> <li>tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms)</li> <li>e_rev_E = 0.0 : Reversal potential for excitatory input (mV)</li> <li>e_rev_I = -70.0 : Reversal potential for inhibitory input (mv)</li> <li>i_offset = 0.0 : Offset current (nA)</li> <li>v_reset = -65.0 : Reset potential after a spike (mV)</li> <li>v_thresh = -50.0 : Spike threshold (mV)</li> </ul> <p>Variables:</p> <ul> <li> <p>v : membrane potential in mV (init=-65.0):</p> <p>cm * dv/dt = cm/tau_m*(v_rest -v)  + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset</p> </li> <li> <p>g_exc : excitatory current (init = 0.0):</p> <p>tau_syn_E * dg_exc/dt = - g_exc</p> </li> <li> <p>g_inh : inhibitory current (init = 0.0):</p> <p>tau_syn_I * dg_inh/dt = - g_inh</p> </li> </ul> <p>Spike emission:</p> <pre><code>v &gt; v_thresh\n</code></pre> <p>Reset:</p> <pre><code>v = v_reset\n</code></pre> <p>The ODEs are solved using the exponential Euler method.</p> <p>Equivalent code:</p> <pre><code>IF_cond_exp = Neuron(\n    parameters = \"\"\"\n        v_rest = -65.0\n        cm  = 1.0\n        tau_m  = 20.0\n        tau_syn_E = 5.0\n        tau_syn_I = 5.0\n        e_rev_E = 0.0\n        e_rev_I = -70.0\n        v_thresh = -50.0\n        v_reset = -65.0\n        i_offset = 0.0\n    \"\"\", \n    equations = \"\"\"\n        cm * dv/dt = cm/tau_m*(v_rest -v)   + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0\n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n    \"\"\",\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = v_reset\",\n    refractory = 0.0\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Neurons.py</code> <pre><code>class IF_cond_exp(Neuron):\n'''\n    IF_cond_exp neuron.\n\n    Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductance.\n\n    Parameters:\n\n    * v_rest = -65.0 :  Resting membrane potential (mV)\n    * cm  = 1.0 : Capacity of the membrane (nF)\n    * tau_m  = 20.0 : Membrane time constant (ms)\n    * tau_refrac = 0.0 : Duration of refractory period (ms)\n    * tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms)\n    * tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms)\n    * e_rev_E = 0.0 : Reversal potential for excitatory input (mV)\n    * e_rev_I = -70.0 : Reversal potential for inhibitory input (mv)\n    * i_offset = 0.0 : Offset current (nA)\n    * v_reset = -65.0 : Reset potential after a spike (mV)\n    * v_thresh = -50.0 : Spike threshold (mV)\n\n    Variables:\n\n    * v : membrane potential in mV (init=-65.0):\n\n        cm * dv/dt = cm/tau_m*(v_rest -v)  + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset\n\n    * g_exc : excitatory current (init = 0.0):\n\n        tau_syn_E * dg_exc/dt = - g_exc\n\n    * g_inh : inhibitory current (init = 0.0):\n\n        tau_syn_I * dg_inh/dt = - g_inh\n\n\n    Spike emission:\n\n        v &gt; v_thresh\n\n    Reset:\n\n        v = v_reset\n\n    The ODEs are solved using the exponential Euler method.\n\n    Equivalent code:\n\n    ```python\n\n    IF_cond_exp = Neuron(\n        parameters = \"\"\"\n            v_rest = -65.0\n            cm  = 1.0\n            tau_m  = 20.0\n            tau_syn_E = 5.0\n            tau_syn_I = 5.0\n            e_rev_E = 0.0\n            e_rev_I = -70.0\n            v_thresh = -50.0\n            v_reset = -65.0\n            i_offset = 0.0\n        \"\"\", \n        equations = \"\"\"\n            cm * dv/dt = cm/tau_m*(v_rest -v)   + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0\n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n        \"\"\",\n        spike = \"v &gt; v_thresh\",\n        reset = \"v = v_reset\",\n        refractory = 0.0\n    )\n    ```\n\n    '''\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, v_rest=-65.0, cm=1.0, tau_m=20.0, tau_refrac=0.0, \n        tau_syn_E=5.0, tau_syn_I=5.0, e_rev_E = 0.0, e_rev_I = -70.0, \n        v_thresh=-50.0, v_reset=-65.0, i_offset=0.0):\n\n        # Create the arguments\n        parameters = \"\"\"\n            v_rest = %(v_rest)s\n            cm  = %(cm)s\n            tau_m  = %(tau_m)s\n            tau_refrac = %(tau_refrac)s\n            tau_syn_E = %(tau_syn_E)s\n            tau_syn_I = %(tau_syn_I)s\n            v_thresh = %(v_thresh)s\n            v_reset = %(v_reset)s\n            i_offset = %(i_offset)s\n            e_rev_E = %(e_rev_E)s             e_rev_I = %(e_rev_I)s\n        \"\"\" % {'v_rest':v_rest, 'cm':cm, 'tau_m':tau_m, 'tau_refrac':tau_refrac, \n                'tau_syn_E':tau_syn_E, 'tau_syn_I':tau_syn_I, 'v_thresh':v_thresh, \n                'v_reset':v_reset, 'i_offset':i_offset, 'e_rev_E': e_rev_E, 'e_rev_I': e_rev_I}\n\n        # Equations for the variables\n        equations=\"\"\"    \n            cm * dv/dt = cm/tau_m*(v_rest -v)   + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset : exponential, init=%(v_reset)s\n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n        \"\"\"% {'v_reset':v_reset}\n\n        spike = \"\"\"\n            v &gt; v_thresh\n        \"\"\"\n        reset = \"\"\"\n            v = v_reset\n        \"\"\"\n        Neuron.__init__(self, parameters=parameters, equations=equations, \n            spike=spike, reset=reset, refractory='tau_refrac',\n            name=\"Integrate-and-Fire\", \n            description=\"Leaky integrate-and-fire model with fixed threshold and decaying-exponential post-synaptic conductances.\")\n\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.IF_curr_alpha","title":"<code>ANNarchy.models.Neurons.IF_curr_alpha</code>","text":"<p>             Bases: <code>Neuron</code></p> <p>IF_curr_alpha neuron.</p> <p>Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents. (Separate synaptic currents for excitatory and inhibitory synapses).</p> <p>The alpha currents are calculated through a system of two linears ODEs. After a spike is received at t_spike, it peaks at t_spike + tau_syn_X, with a maximum equal to the synaptic efficiency.</p> <p>Parameters:</p> <ul> <li>v_rest = -65.0 :  Resting membrane potential (mV)</li> <li>cm  = 1.0 : Capacity of the membrane (nF)</li> <li>tau_m  = 20.0 : Membrane time constant (ms)</li> <li>tau_refrac = 0.0 : Duration of refractory period (ms)</li> <li>tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms)</li> <li>tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms)</li> <li>i_offset = 0.0 : Offset current (nA)</li> <li>v_reset = -65.0 : Reset potential after a spike (mV)</li> <li>v_thresh = -50.0 : Spike threshold (mV)</li> </ul> <p>Variables:</p> <ul> <li> <p>v : membrane potential in mV (init=-65.0):</p> <p>cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset</p> </li> <li> <p>g_exc : excitatory current (init = 0.0):</p> <p>tau_syn_E * dg_exc/dt = - g_exc</p> </li> <li> <p>alpha_exc : alpha function of excitatory current (init = 0.0):</p> <p>tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc</p> </li> <li> <p>g_inh : inhibitory current (init = 0.0):</p> <p>tau_syn_I * dg_inh/dt = - g_inh</p> </li> <li> <p>alpha_inh : alpha function of inhibitory current (init = 0.0):</p> <p>tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh</p> </li> </ul> <p>Spike emission:</p> <pre><code>v &gt; v_thresh\n</code></pre> <p>Reset:</p> <pre><code>v = v_reset\n</code></pre> <p>The ODEs are solved using the exponential Euler method.</p> <p>Equivalent code:</p> <pre><code>IF_curr_alpha = Neuron(\n    parameters = \"\"\"\n        v_rest = -65.0\n        cm  = 1.0\n        tau_m  = 20.0\n        tau_syn_E = 5.0\n        tau_syn_I = 5.0\n        v_thresh = -50.0\n        v_reset = -65.0\n        i_offset = 0.0\n    \"\"\", \n    equations = \"\"\"\n        gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)\n        gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)  \n        cm * dv/dt = cm/tau_m*(v_rest -v)   + alpha_exc - alpha_inh + i_offset : exponential, init=-65.0\n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n        tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc  : exponential\n        tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh  : exponential\n    \"\"\",\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = v_reset\",\n    refractory = 0.0\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Neurons.py</code> <pre><code>class IF_curr_alpha(Neuron):\n'''\n    IF_curr_alpha neuron.\n\n    Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents. (Separate synaptic currents for excitatory and inhibitory synapses).\n\n    The alpha currents are calculated through a system of two linears ODEs. After a spike is received at t_spike, it peaks at t_spike + tau_syn_X, with a maximum equal to the synaptic efficiency.\n\n    Parameters:\n\n    * v_rest = -65.0 :  Resting membrane potential (mV)\n    * cm  = 1.0 : Capacity of the membrane (nF)\n    * tau_m  = 20.0 : Membrane time constant (ms)\n    * tau_refrac = 0.0 : Duration of refractory period (ms)\n    * tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms)\n    * tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms)\n    * i_offset = 0.0 : Offset current (nA)\n    * v_reset = -65.0 : Reset potential after a spike (mV)\n    * v_thresh = -50.0 : Spike threshold (mV)\n\n    Variables:\n\n    * v : membrane potential in mV (init=-65.0):\n\n        cm * dv/dt = cm/tau_m*(v_rest -v) + alpha_exc - alpha_inh + i_offset\n\n    * g_exc : excitatory current (init = 0.0):\n\n        tau_syn_E * dg_exc/dt = - g_exc\n\n    * alpha_exc : alpha function of excitatory current (init = 0.0):\n\n        tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc\n\n    * g_inh : inhibitory current (init = 0.0):\n\n        tau_syn_I * dg_inh/dt = - g_inh\n\n    * alpha_inh : alpha function of inhibitory current (init = 0.0):\n\n        tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh\n\n\n    Spike emission:\n\n        v &gt; v_thresh\n\n    Reset:\n\n        v = v_reset\n\n    The ODEs are solved using the exponential Euler method.\n\n    Equivalent code:\n\n    ```python\n\n    IF_curr_alpha = Neuron(\n        parameters = \"\"\"\n            v_rest = -65.0\n            cm  = 1.0\n            tau_m  = 20.0\n            tau_syn_E = 5.0\n            tau_syn_I = 5.0\n            v_thresh = -50.0\n            v_reset = -65.0\n            i_offset = 0.0\n        \"\"\", \n        equations = \"\"\"\n            gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)\n            gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)  \n            cm * dv/dt = cm/tau_m*(v_rest -v)   + alpha_exc - alpha_inh + i_offset : exponential, init=-65.0\n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n            tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc  : exponential\n            tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh  : exponential\n        \"\"\",\n        spike = \"v &gt; v_thresh\",\n        reset = \"v = v_reset\",\n        refractory = 0.0\n    )\n    ```\n\n    '''\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, v_rest=-65.0, cm=1.0, tau_m=20.0, tau_refrac=0.0, \n        tau_syn_E=5.0, tau_syn_I=5.0, v_thresh=-50.0, v_reset=-65.0, i_offset=0.0):\n\n        # Create the arguments\n        parameters = \"\"\"\n            v_rest = %(v_rest)s\n            cm  = %(cm)s\n            tau_m  = %(tau_m)s\n            tau_refrac = %(tau_refrac)s\n            tau_syn_E = %(tau_syn_E)s\n            tau_syn_I = %(tau_syn_I)s\n            v_thresh = %(v_thresh)s\n            v_reset = %(v_reset)s\n            i_offset = %(i_offset)s\n        \"\"\" % {'v_rest':v_rest, 'cm':cm, 'tau_m':tau_m, 'tau_refrac':tau_refrac, \n                'tau_syn_E':tau_syn_E, 'tau_syn_I':tau_syn_I, 'v_thresh':v_thresh, 'v_reset':v_reset, 'i_offset':i_offset}\n\n        # Equations for the variables\n        equations=\"\"\"  \n            gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)\n            gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)  \n            cm * dv/dt = cm/tau_m*(v_rest -v)   + alpha_exc - alpha_inh + i_offset : exponential, init=%(v_reset)s\n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n            tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc  : exponential\n            tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh  : exponential\n        \"\"\"  % {'v_reset':v_reset}\n\n        spike = \"\"\"\n            v &gt; v_thresh\n        \"\"\"\n        reset = \"\"\"\n            v = v_reset\n        \"\"\"\n        Neuron.__init__(self, parameters=parameters, equations=equations, \n            spike=spike, reset=reset, refractory='tau_refrac',\n            name=\"Integrate-and-Fire\", \n            description=\"Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic currents.\")\n\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.IF_cond_alpha","title":"<code>ANNarchy.models.Neurons.IF_cond_alpha</code>","text":"<p>             Bases: <code>Neuron</code></p> <p>IF_cond_exp neuron.</p> <p>Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance.</p> <p>Parameters:</p> <ul> <li>v_rest = -65.0 :  Resting membrane potential (mV)</li> <li>cm  = 1.0 : Capacity of the membrane (nF)</li> <li>tau_m  = 20.0 : Membrane time constant (ms)</li> <li>tau_refrac = 0.0 : Duration of refractory period (ms)</li> <li>tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms)</li> <li>tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms)</li> <li>e_rev_E = 0.0 : Reversal potential for excitatory input (mV)</li> <li>e_rev_I = -70.0 : Reversal potential for inhibitory input (mv)</li> <li>i_offset = 0.0 : Offset current (nA)</li> <li>v_reset = -65.0 : Reset potential after a spike (mV)</li> <li>v_thresh = -50.0 : Spike threshold (mV)</li> </ul> <p>Variables:</p> <ul> <li> <p>v : membrane potential in mV (init=-65.0):</p> <p>cm * dv/dt = cm/tau_m*(v_rest -v)  + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset</p> </li> <li> <p>g_exc : excitatory conductance (init = 0.0):</p> <p>tau_syn_E * dg_exc/dt = - g_exc</p> </li> <li> <p>alpha_exc : alpha function of excitatory conductance (init = 0.0):</p> <p>tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc</p> </li> <li> <p>g_inh : inhibitory conductance (init = 0.0):</p> <p>tau_syn_I * dg_inh/dt = - g_inh</p> </li> <li> <p>alpha_inh : alpha function of inhibitory current (init = 0.0):</p> <p>tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh</p> </li> </ul> <p>Spike emission:</p> <pre><code>v &gt; v_thresh\n</code></pre> <p>Reset:</p> <pre><code>v = v_reset\n</code></pre> <p>The ODEs are solved using the exponential Euler method.</p> <p>Equivalent code:</p> <pre><code>IF_cond_alpha = Neuron(\n    parameters = \"\"\"\n        v_rest = -65.0\n        cm  = 1.0\n        tau_m  = 20.0\n        tau_syn_E = 5.0\n        tau_syn_I = 5.0\n        e_rev_E = 0.0\n        e_rev_I = -70.0\n        v_thresh = -50.0\n        v_reset = -65.0\n        i_offset = 0.0\n    \"\"\", \n    equations = \"\"\"\n        gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)\n        gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)\n        cm * dv/dt = cm/tau_m*(v_rest -v)   + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0\n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n        tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc  : exponential\n        tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh  : exponential\n    \"\"\",\n    spike = \"v &gt; v_thresh\",\n    reset = \"v = v_reset\",\n    refractory = 0.0\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Neurons.py</code> <pre><code>class IF_cond_alpha(Neuron):\n'''\n    IF_cond_exp neuron.\n\n    Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductance.\n\n    Parameters:\n\n    * v_rest = -65.0 :  Resting membrane potential (mV)\n    * cm  = 1.0 : Capacity of the membrane (nF)\n    * tau_m  = 20.0 : Membrane time constant (ms)\n    * tau_refrac = 0.0 : Duration of refractory period (ms)\n    * tau_syn_E = 5.0 : Rise time of excitatory synaptic current (ms)\n    * tau_syn_I = 5.0 : Rise time of inhibitory synaptic current (ms)\n    * e_rev_E = 0.0 : Reversal potential for excitatory input (mV)\n    * e_rev_I = -70.0 : Reversal potential for inhibitory input (mv)\n    * i_offset = 0.0 : Offset current (nA)\n    * v_reset = -65.0 : Reset potential after a spike (mV)\n    * v_thresh = -50.0 : Spike threshold (mV)\n\n    Variables:\n\n    * v : membrane potential in mV (init=-65.0):\n\n        cm * dv/dt = cm/tau_m*(v_rest -v)  + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset\n\n    * g_exc : excitatory conductance (init = 0.0):\n\n        tau_syn_E * dg_exc/dt = - g_exc\n\n    * alpha_exc : alpha function of excitatory conductance (init = 0.0):\n\n        tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc\n\n    * g_inh : inhibitory conductance (init = 0.0):\n\n        tau_syn_I * dg_inh/dt = - g_inh\n\n    * alpha_inh : alpha function of inhibitory current (init = 0.0):\n\n        tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh\n\n\n    Spike emission:\n\n        v &gt; v_thresh\n\n    Reset:\n\n        v = v_reset\n\n    The ODEs are solved using the exponential Euler method.\n\n    Equivalent code:\n\n    ```python\n\n    IF_cond_alpha = Neuron(\n        parameters = \"\"\"\n            v_rest = -65.0\n            cm  = 1.0\n            tau_m  = 20.0\n            tau_syn_E = 5.0\n            tau_syn_I = 5.0\n            e_rev_E = 0.0\n            e_rev_I = -70.0\n            v_thresh = -50.0\n            v_reset = -65.0\n            i_offset = 0.0\n        \"\"\", \n        equations = \"\"\"\n            gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)\n            gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)\n            cm * dv/dt = cm/tau_m*(v_rest -v)   + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset : exponential, init=-65.0\n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n            tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc  : exponential\n            tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh  : exponential\n        \"\"\",\n        spike = \"v &gt; v_thresh\",\n        reset = \"v = v_reset\",\n        refractory = 0.0\n    )\n    ```\n    '''\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, v_rest=-65.0, cm=1.0, tau_m=20.0, tau_refrac=0.0, \n        tau_syn_E=5.0, tau_syn_I=5.0, e_rev_E = 0.0, e_rev_I = -70.0, \n        v_thresh=-50.0, v_reset=-65.0, i_offset=0.0):\n\n        # Create the arguments\n        parameters = \"\"\"\n            v_rest = %(v_rest)s\n            cm  = %(cm)s\n            tau_m  = %(tau_m)s\n            tau_refrac = %(tau_refrac)s\n            tau_syn_E = %(tau_syn_E)s\n            tau_syn_I = %(tau_syn_I)s\n            v_thresh = %(v_thresh)s\n            v_reset = %(v_reset)s\n            i_offset = %(i_offset)s\n            e_rev_E = %(e_rev_E)s             e_rev_I = %(e_rev_I)s\n        \"\"\" % {'v_rest':v_rest, 'cm':cm, 'tau_m':tau_m, 'tau_refrac':tau_refrac, \n                'tau_syn_E':tau_syn_E, 'tau_syn_I':tau_syn_I, 'v_thresh':v_thresh, \n                'v_reset':v_reset, 'i_offset':i_offset, 'e_rev_E': e_rev_E, 'e_rev_I': e_rev_I}\n\n        # Equations for the variables\n        equations=\"\"\"    \n            gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)\n            gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)\n            cm * dv/dt = cm/tau_m*(v_rest -v)   + alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset : exponential, init=%(v_reset)s\n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n            tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc  : exponential\n            tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh  : exponential\n        \"\"\" % {'v_reset': v_reset}\n\n        spike = \"\"\"\n            v &gt; v_thresh\n        \"\"\"\n        reset = \"\"\"\n            v = v_reset\n        \"\"\"\n        Neuron.__init__(self, parameters=parameters, equations=equations, \n            spike=spike, reset=reset, refractory='tau_refrac',\n            name=\"Integrate-and-Fire\", \n            description=\"Leaky integrate-and-fire model with fixed threshold and alpha post-synaptic conductances.\")\n\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.HH_cond_exp","title":"<code>ANNarchy.models.Neurons.HH_cond_exp</code>","text":"<p>             Bases: <code>Neuron</code></p> <p>HH_cond_exp neuron.</p> <p>Single-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub.</p> <p>Parameters:</p> <ul> <li>gbar_Na = 20.0 : Maximal conductance of the Sodium current.</li> <li>gbar_K = 6.0 : Maximal conductance of the Potassium current. </li> <li>gleak = 0.01 : Conductance of the leak current (nF)  </li> <li>cm = 0.2 : Capacity of the membrane (nF)</li> <li>v_offset = -63.0 :  Threshold for the rate constants (mV)  </li> <li>e_rev_Na = 50.0 : Reversal potential for the Sodium current (mV) </li> <li>e_rev_K = -90.0 : Reversal potential for the Potassium current (mV)  </li> <li>e_rev_leak = -65.0 : Reversal potential for the leak current (mV)   </li> <li>e_rev_E = 0.0 : Reversal potential for excitatory input (mV)  </li> <li>e_rev_I = -80.0 : Reversal potential for inhibitory input (mV)  </li> <li>tau_syn_E = 0.2 : Decay time of excitatory synaptic current (ms)  </li> <li>tau_syn_I = 2.0 : Decay time of inhibitory synaptic current (ms)   </li> <li>i_offset = 0.0 : Offset current (nA)</li> <li>v_thresh = 0.0 : Threshold for spike emission</li> </ul> <p>Variables:</p> <ul> <li> <p>Voltage-dependent rate constants an, bn, am, bm, ah, bh:</p> <p>an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0) am = 0.32  * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0) ah = 0.128 * exp((17.0 - v + v_offset)/18.0) </p> <p>bn = 0.5   * exp ((10.0 - v + v_offset)/40.0) bm = 0.28  * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0) bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) )</p> </li> <li> <p>Activation variables n, m, h (h is initialized to 1.0, n and m to 0.0):</p> <p>dn/dt = an * (1.0 - n) - bn * n  dm/dt = am * (1.0 - m) - bm * m  dh/dt = ah * (1.0 - h) - bh * h </p> </li> <li> <p>v : membrane potential in mV (init=-65.0):</p> <p>cm * dv/dt = gleak(e_rev_leak -v) + gbar_K * n4 * (e_rev_K - v) + gbar_Na * m*3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset</p> </li> <li> <p>g_exc : excitatory conductance (init = 0.0):</p> <p>tau_syn_E * dg_exc/dt = - g_exc</p> </li> <li> <p>g_inh : inhibitory conductance (init = 0.0):</p> <p>tau_syn_I * dg_inh/dt = - g_inh</p> </li> </ul> <p>Spike emission (the spike is emitted only once when v crosses the threshold from below):</p> <pre><code>v &gt; v_thresh and v(t-1) &lt; v_thresh\n</code></pre> <p>The ODEs for n, m, h and v are solved using the midpoint method, while the conductances g_exc and g_inh are solved using the exponential Euler method.</p> <p>Equivalent code:</p> <pre><code>HH_cond_exp = Neuron(\n    parameters = \"\"\"\n        gbar_Na = 20.0\n        gbar_K = 6.0\n        gleak = 0.01\n        cm = 0.2 \n        v_offset = -63.0 \n        e_rev_Na = 50.0\n        e_rev_K = -90.0 \n        e_rev_leak = -65.0\n        e_rev_E = 0.0\n        e_rev_I = -80.0 \n        tau_syn_E = 0.2\n        tau_syn_I = 2.0\n        i_offset = 0.0\n        v_thresh = 0.0\n    \"\"\", \n    equations = \"\"\"\n        # Previous membrane potential\n        prev_v = v\n\n        # Voltage-dependent rate constants\n        an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0)\n        am = 0.32  * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0)\n        ah = 0.128 * exp((17.0 - v + v_offset)/18.0) \n\n        bn = 0.5   * exp ((10.0 - v + v_offset)/40.0)\n        bm = 0.28  * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0)\n        bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) )\n\n        # Activation variables\n        dn/dt = an * (1.0 - n) - bn * n : init = 0.0, exponential\n        dm/dt = am * (1.0 - m) - bm * m : init = 0.0, exponential\n        dh/dt = ah * (1.0 - h) - bh * h : init = 1.0, exponential\n\n        # Membrane equation\n        cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v)\n                        + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset: exponential, init=-65.0\n\n        # Exponentially-decaying conductances\n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n    \"\"\",\n    spike = \"(v &gt; v_thresh) and (prev_v &lt;= v_thresh)\",\n    reset = \"\"\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Neurons.py</code> <pre><code>class HH_cond_exp(Neuron):\n'''\n    HH_cond_exp neuron.\n\n    Single-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents using the ion channel models from Traub.\n\n    Parameters:\n\n    * gbar_Na = 20.0 : Maximal conductance of the Sodium current.\n    * gbar_K = 6.0 : Maximal conductance of the Potassium current. \n    * gleak = 0.01 : Conductance of the leak current (nF)  \n    * cm = 0.2 : Capacity of the membrane (nF)\n    * v_offset = -63.0 :  Threshold for the rate constants (mV)  \n    * e_rev_Na = 50.0 : Reversal potential for the Sodium current (mV) \n    * e_rev_K = -90.0 : Reversal potential for the Potassium current (mV)  \n    * e_rev_leak = -65.0 : Reversal potential for the leak current (mV)   \n    * e_rev_E = 0.0 : Reversal potential for excitatory input (mV)  \n    * e_rev_I = -80.0 : Reversal potential for inhibitory input (mV)  \n    * tau_syn_E = 0.2 : Decay time of excitatory synaptic current (ms)  \n    * tau_syn_I = 2.0 : Decay time of inhibitory synaptic current (ms)   \n    * i_offset = 0.0 : Offset current (nA)\n    * v_thresh = 0.0 : Threshold for spike emission\n\n    Variables:\n\n    * Voltage-dependent rate constants an, bn, am, bm, ah, bh:\n\n        an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0)\n        am = 0.32  * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0)\n        ah = 0.128 * exp((17.0 - v + v_offset)/18.0) \n\n        bn = 0.5   * exp ((10.0 - v + v_offset)/40.0)\n        bm = 0.28  * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0)\n        bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) )\n\n    * Activation variables n, m, h (h is initialized to 1.0, n and m to 0.0):\n\n        dn/dt = an * (1.0 - n) - bn * n \n        dm/dt = am * (1.0 - m) - bm * m \n        dh/dt = ah * (1.0 - h) - bh * h \n\n\n    * v : membrane potential in mV (init=-65.0):\n\n        cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v) + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset\n\n    * g_exc : excitatory conductance (init = 0.0):\n\n        tau_syn_E * dg_exc/dt = - g_exc\n\n    * g_inh : inhibitory conductance (init = 0.0):\n\n        tau_syn_I * dg_inh/dt = - g_inh\n\n\n    Spike emission (the spike is emitted only once when v crosses the threshold from below):\n\n        v &gt; v_thresh and v(t-1) &lt; v_thresh\n\n    The ODEs for n, m, h and v are solved using the midpoint method, while the conductances g_exc and g_inh are solved using the exponential Euler method.\n\n    Equivalent code:\n\n    ```python\n\n    HH_cond_exp = Neuron(\n        parameters = \"\"\"\n            gbar_Na = 20.0\n            gbar_K = 6.0\n            gleak = 0.01\n            cm = 0.2 \n            v_offset = -63.0 \n            e_rev_Na = 50.0\n            e_rev_K = -90.0 \n            e_rev_leak = -65.0\n            e_rev_E = 0.0\n            e_rev_I = -80.0 \n            tau_syn_E = 0.2\n            tau_syn_I = 2.0\n            i_offset = 0.0\n            v_thresh = 0.0\n        \"\"\", \n        equations = \"\"\"\n            # Previous membrane potential\n            prev_v = v\n\n            # Voltage-dependent rate constants\n            an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0)\n            am = 0.32  * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0)\n            ah = 0.128 * exp((17.0 - v + v_offset)/18.0) \n\n            bn = 0.5   * exp ((10.0 - v + v_offset)/40.0)\n            bm = 0.28  * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0)\n            bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) )\n\n            # Activation variables\n            dn/dt = an * (1.0 - n) - bn * n : init = 0.0, exponential\n            dm/dt = am * (1.0 - m) - bm * m : init = 0.0, exponential\n            dh/dt = ah * (1.0 - h) - bh * h : init = 1.0, exponential\n\n            # Membrane equation\n            cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v)\n                            + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset: exponential, init=-65.0\n\n            # Exponentially-decaying conductances\n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n        \"\"\",\n        spike = \"(v &gt; v_thresh) and (prev_v &lt;= v_thresh)\",\n        reset = \"\"\n    )\n    ```\n\n    '''\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, gbar_Na = 20.0, gbar_K = 6.0, gleak = 0.01, cm = 0.2, \n        v_offset = -63.0, e_rev_Na = 50.0, e_rev_K = -90.0, e_rev_leak = -65.0, \n        e_rev_E = 0.0, e_rev_I = -80.0, tau_syn_E = 0.2, tau_syn_I = 2.0, \n        i_offset = 0.0, v_thresh = 0.0):\n\n        parameters = \"\"\"\n        gbar_Na    = %(gbar_Na)s           gbar_K     = %(gbar_K)s             gleak      = %(gleak)s              cm         = %(cm)s                v_offset   = %(v_offset)s         e_rev_Na   = %(e_rev_Na)s         e_rev_K    = %(e_rev_K)s           e_rev_leak = %(e_rev_leak)s           e_rev_E    = %(e_rev_E)s            e_rev_I    = %(e_rev_I)s           tau_syn_E  = %(tau_syn_E)s         tau_syn_I  = %(tau_syn_I)s         i_offset   = %(i_offset)s          v_thresh   = %(v_thresh)s          \"\"\" % {\n        'gbar_Na'    : gbar_Na   ,\n        'gbar_K'     : gbar_K     ,\n        'gleak'      : gleak      ,\n        'cm'         : cm        ,\n        'v_offset'   : v_offset  ,\n        'e_rev_Na'   : e_rev_Na  ,\n        'e_rev_K'    : e_rev_K   ,\n        'e_rev_leak' : e_rev_leak   ,\n        'e_rev_E'    : e_rev_E    ,\n        'e_rev_I'    : e_rev_I   ,\n        'tau_syn_E'  : tau_syn_E ,\n        'tau_syn_I'  : tau_syn_I ,\n        'i_offset'   : i_offset  ,\n        'v_thresh'   : v_thresh  \n        }\n\n        equations = \"\"\"\n            # Previous membrane potential\n            prev_v = v\n\n            # Voltage-dependent rate constants\n            an = 0.032 * (15.0 - v + v_offset) / (exp((15.0 - v + v_offset)/5.0) - 1.0)\n            am = 0.32  * (13.0 - v + v_offset) / (exp((13.0 - v + v_offset)/4.0) - 1.0)\n            ah = 0.128 * exp((17.0 - v + v_offset)/18.0) \n\n            bn = 0.5   * exp ((10.0 - v + v_offset)/40.0)\n            bm = 0.28  * (v - v_offset - 40.0) / (exp((v - v_offset - 40.0)/5.0) - 1.0)\n            bh = 4.0/(1.0 + exp (( 10.0 - v + v_offset )) )\n\n            # Activation variables\n            dn/dt = an * (1.0 - n) - bn * n : init = 0.0, exponential\n            dm/dt = am * (1.0 - m) - bm * m : init = 0.0, exponential\n            dh/dt = ah * (1.0 - h) - bh * h : init = 1.0, exponential\n\n            # Membrane equation\n            cm * dv/dt = gleak*(e_rev_leak -v) + gbar_K * n**4 * (e_rev_K - v) + gbar_Na * m**3 * h * (e_rev_Na - v) \n                + g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset: exponential, init=%(e_rev_leak)s\n\n            # Exponentially-decaying conductances\n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n        \"\"\" % {'e_rev_leak': e_rev_leak}\n\n        spike = \"(v &gt; v_thresh) and (prev_v &lt;= v_thresh)\"\n\n        reset = \"\"\n\n        Neuron.__init__(self, parameters=parameters, equations=equations, \n            spike=spike, reset=reset,\n            name=\"Hodgkin-Huxley\", \n            description=\"Single-compartment Hodgkin-Huxley-type neuron with transient sodium and delayed-rectifier potassium currents.\")\n\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.EIF_cond_exp_isfa_ista","title":"<code>ANNarchy.models.Neurons.EIF_cond_exp_isfa_ista</code>","text":"<p>             Bases: <code>Neuron</code></p> <p>EIF_cond_exp neuron.</p> <p>Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.) according to:</p> <p>Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642</p> <p>Parameters:</p> <ul> <li>v_rest = -70.6 :  Resting membrane potential (mV)</li> <li>cm = 0.281 : Capacity of the membrane (nF)</li> <li>tau_m = 9.3667 : Membrane time constant (ms)</li> <li>tau_refrac = 0.1 : Duration of refractory period (ms)</li> <li>tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms)</li> <li>tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms)</li> <li>e_rev_E = 0.0 : Reversal potential for excitatory input (mV)</li> <li>e_rev_I = -80.0 : Reversal potential for inhibitory input (mv)</li> <li>tau_w = 144.0 : Time constant of the adaptation variable (ms)</li> <li>a = 4.0 : Scaling of the adaptation variable</li> <li>b = 0.0805 : Increment on the adaptation variable after a spike</li> <li>i_offset = 0.0 : Offset current (nA)</li> <li>delta_T = 2.0 : Speed of the exponential (mV)</li> <li>v_thresh = -50.4 : Spike threshold for the exponential (mV)</li> <li>v_reset = -70.6 : Reset potential after a spike (mV)</li> <li>v_spike = -40.0 : Spike threshold (mV)</li> </ul> <p>Variables:</p> <ul> <li> <p>I : input current (nA):</p> <p>I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset</p> </li> <li> <p>v : membrane potential in mV (init=-70.6):</p> <p>tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w)</p> </li> <li> <p>w : adaptation variable (init=0.0):</p> <p>tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w</p> </li> <li> <p>g_exc : excitatory current (init = 0.0):</p> <p>tau_syn_E * dg_exc/dt = - g_exc</p> </li> <li> <p>g_inh : inhibitory current (init = 0.0):</p> <p>tau_syn_I * dg_inh/dt = - g_inh</p> </li> </ul> <p>Spike emission:</p> <pre><code>v &gt; v_thresh\n</code></pre> <p>Reset:</p> <pre><code>v = v_reset\nu += b\n</code></pre> <p>The ODEs are solved using the explicit Euler method.</p> <p>Equivalent code:</p> <pre><code>EIF_cond_exp_isfa_ista = Neuron(\n    parameters = \"\"\"\n        v_rest = -70.6\n        cm = 0.281 \n        tau_m = 9.3667 \n        tau_syn_E = 5.0 \n        tau_syn_I = 5.0 \n        e_rev_E = 0.0 \n        e_rev_I = -80.0\n        tau_w = 144.0 \n        a = 4.0\n        b = 0.0805\n        i_offset = 0.0\n        delta_T = 2.0 \n        v_thresh = -50.4\n        v_reset = -70.6\n        v_spike = -40.0 \n    \"\"\", \n    equations = \"\"\"\n        I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset            \n        tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6          \n        tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w           \n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n    \"\"\",\n    spike = \"v &gt; v_spike\",\n    reset = \"\"\"\n        v = v_reset\n        w += b\n    \"\"\",\n    refractory = 0.1\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Neurons.py</code> <pre><code>class EIF_cond_exp_isfa_ista(Neuron):\n'''\n    EIF_cond_exp neuron.\n\n    Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.) according to:\n\n    Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642\n\n    Parameters:\n\n    * v_rest = -70.6 :  Resting membrane potential (mV)\n    * cm = 0.281 : Capacity of the membrane (nF)\n    * tau_m = 9.3667 : Membrane time constant (ms)\n    * tau_refrac = 0.1 : Duration of refractory period (ms)\n    * tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms)\n    * tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms)\n    * e_rev_E = 0.0 : Reversal potential for excitatory input (mV)\n    * e_rev_I = -80.0 : Reversal potential for inhibitory input (mv)\n    * tau_w = 144.0 : Time constant of the adaptation variable (ms)\n    * a = 4.0 : Scaling of the adaptation variable\n    * b = 0.0805 : Increment on the adaptation variable after a spike\n    * i_offset = 0.0 : Offset current (nA)\n    * delta_T = 2.0 : Speed of the exponential (mV)\n    * v_thresh = -50.4 : Spike threshold for the exponential (mV)\n    * v_reset = -70.6 : Reset potential after a spike (mV)\n    * v_spike = -40.0 : Spike threshold (mV)\n\n    Variables:\n\n    * I : input current (nA):\n\n        I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset\n\n    * v : membrane potential in mV (init=-70.6):\n\n        tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w)\n\n    * w : adaptation variable (init=0.0):\n\n        tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w\n\n    * g_exc : excitatory current (init = 0.0):\n\n        tau_syn_E * dg_exc/dt = - g_exc\n\n    * g_inh : inhibitory current (init = 0.0):\n\n        tau_syn_I * dg_inh/dt = - g_inh\n\n\n    Spike emission:\n\n        v &gt; v_thresh\n\n    Reset:\n\n        v = v_reset\n        u += b\n\n    The ODEs are solved using the explicit Euler method.\n\n    Equivalent code:\n\n    ```python\n\n    EIF_cond_exp_isfa_ista = Neuron(\n        parameters = \"\"\"\n            v_rest = -70.6\n            cm = 0.281 \n            tau_m = 9.3667 \n            tau_syn_E = 5.0 \n            tau_syn_I = 5.0 \n            e_rev_E = 0.0 \n            e_rev_I = -80.0\n            tau_w = 144.0 \n            a = 4.0\n            b = 0.0805\n            i_offset = 0.0\n            delta_T = 2.0 \n            v_thresh = -50.4\n            v_reset = -70.6\n            v_spike = -40.0 \n        \"\"\", \n        equations = \"\"\"\n            I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset            \n            tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6          \n            tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w           \n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n        \"\"\",\n        spike = \"v &gt; v_spike\",\n        reset = \"\"\"\n            v = v_reset\n            w += b\n        \"\"\",\n        refractory = 0.1\n    )\n    ```\n\n    '''\n\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, v_rest = -70.6, cm = 0.281, tau_m = 9.3667, \n        tau_refrac = 0.1, tau_syn_E = 5.0, tau_syn_I = 5.0, \n        e_rev_E = 0.0, e_rev_I = -80.0, tau_w = 144.0, \n        a = 4.0, b = 0.0805, i_offset = 0.0, delta_T = 2.0, \n        v_thresh = -50.4, v_reset = -70.6, v_spike = -40.0):\n\n        # Create the arguments\n        parameters = \"\"\"\n            v_rest     = %(v_rest)s\n            cm         = %(cm)s\n            tau_m      = %(tau_m)s\n            tau_refrac = %(tau_refrac)s\n            tau_syn_E  = %(tau_syn_E)s\n            tau_syn_I  = %(tau_syn_I)s\n            e_rev_E    = %(e_rev_E)s\n            e_rev_I    = %(e_rev_I)s\n            tau_w      = %(tau_w)s\n            a          = %(a)s\n            b          = %(b)s\n            i_offset   = %(i_offset)s\n            delta_T    = %(delta_T)s\n            v_thresh   = %(v_thresh)s\n            v_reset    = %(v_reset)s\n            v_spike    = %(v_spike)s\n        \"\"\" % {\n            'v_rest'     : v_rest    ,\n            'cm'         : cm        ,\n            'tau_m'      : tau_m     ,\n            'tau_refrac' : tau_refrac,\n            'tau_syn_E'  : tau_syn_E ,\n            'tau_syn_I'  : tau_syn_I ,\n            'e_rev_E'    : e_rev_E   ,\n            'e_rev_I'    : e_rev_I   ,\n            'tau_w'      : tau_w     ,\n            'a'          : a         ,\n            'b'          : b         ,\n            'i_offset'   : i_offset  ,\n            'delta_T'    : delta_T   ,\n            'v_thresh'   : v_thresh  ,\n            'v_reset'    : v_reset   ,\n            'v_spike'    : v_spike   ,\n        }\n        # Equations for the variables\n        equations=\"\"\"    \n            I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset\n\n            tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=%(v_reset)s\n\n            tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w \n\n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n        \"\"\" % {'v_reset': v_reset}\n\n        spike = \"\"\"\n            v &gt; v_spike\n        \"\"\"\n        reset = \"\"\"\n            v = v_reset\n            w += b\n        \"\"\"\n        Neuron.__init__(self, parameters=parameters, equations=equations, \n            spike=spike, reset=reset, refractory='tau_refrac',\n            name=\"Adaptive exponential Integrate-and-Fire\", \n            description=\"Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation currents (isfa, ista reps.).\")\n\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificNeuron.html#ANNarchy.models.Neurons.EIF_cond_alpha_isfa_ista","title":"<code>ANNarchy.models.Neurons.EIF_cond_alpha_isfa_ista</code>","text":"<p>             Bases: <code>Neuron</code></p> <p>EIF_cond_alpha neuron.</p> <p>Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.) according to:</p> <p>Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642</p> <p>Parameters:</p> <ul> <li>v_rest = -70.6 :  Resting membrane potential (mV)</li> <li>cm = 0.281 : Capacity of the membrane (nF)</li> <li>tau_m = 9.3667 : Membrane time constant (ms)</li> <li>tau_refrac = 0.1 : Duration of refractory period (ms)</li> <li>tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms)</li> <li>tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms)</li> <li>e_rev_E = 0.0 : Reversal potential for excitatory input (mV)</li> <li>e_rev_I = -80.0 : Reversal potential for inhibitory input (mv)</li> <li>tau_w = 144.0 : Time constant of the adaptation variable (ms)</li> <li>a = 4.0 : Scaling of the adaptation variable</li> <li>b = 0.0805 : Increment on the adaptation variable after a spike</li> <li>i_offset = 0.0 : Offset current (nA)</li> <li>delta_T = 2.0 : Speed of the exponential (mV)</li> <li>v_thresh = -50.4 : Spike threshold for the exponential (mV)</li> <li>v_reset = -70.6 : Reset potential after a spike (mV)</li> <li>v_spike = -40.0 : Spike threshold (mV)</li> </ul> <p>Variables:</p> <ul> <li> <p>I : input current (nA):</p> <p>I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset</p> </li> <li> <p>v : membrane potential in mV (init=-70.6):</p> <p>tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w)</p> </li> <li> <p>w : adaptation variable (init=0.0):</p> <p>tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w</p> </li> <li> <p>g_exc : excitatory current (init = 0.0):</p> <p>tau_syn_E * dg_exc/dt = - g_exc</p> </li> <li> <p>g_inh : inhibitory current (init = 0.0):</p> <p>tau_syn_I * dg_inh/dt = - g_inh</p> </li> <li> <p>alpha_exc : alpha function of excitatory current (init = 0.0):</p> <p>tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc  </p> </li> <li> <p>alpha_inh: alpha function of inhibitory current (init = 0.0):</p> <p>tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh  </p> </li> </ul> <p>Spike emission:</p> <pre><code>v &gt; v_spike\n</code></pre> <p>Reset:</p> <pre><code>v = v_reset\nu += b\n</code></pre> <p>The ODEs are solved using the explicit Euler method.</p> <p>Equivalent code:</p> <pre><code>EIF_cond_alpha_isfa_ista = Neuron(\n    parameters = \"\"\"\n        v_rest = -70.6\n        cm = 0.281 \n        tau_m = 9.3667 \n        tau_syn_E = 5.0 \n        tau_syn_I = 5.0 \n        e_rev_E = 0.0 \n        e_rev_I = -80.0\n        tau_w = 144.0 \n        a = 4.0\n        b = 0.0805\n        i_offset = 0.0\n        delta_T = 2.0 \n        v_thresh = -50.4\n        v_reset = -70.6\n        v_spike = -40.0 \n    \"\"\", \n    equations = \"\"\"\n        gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)\n        gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)                \n        I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset\n        tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6\n        tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w \n        tau_syn_E * dg_exc/dt = - g_exc : exponential\n        tau_syn_I * dg_inh/dt = - g_inh : exponential\n        tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc  : exponential\n        tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh  : exponential\n    \"\"\",\n    spike = \"v &gt; v_spike\",\n    reset = \"\"\"\n        v = v_reset\n        w += b\n    \"\"\",\n    refractory = 0.1\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Neurons.py</code> <pre><code>class EIF_cond_alpha_isfa_ista(Neuron):\n''' \n    EIF_cond_alpha neuron.\n\n    Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.) according to:\n\n    Brette R and Gerstner W (2005) Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. J Neurophysiol 94:3637-3642\n\n    Parameters:\n\n    * v_rest = -70.6 :  Resting membrane potential (mV)\n    * cm = 0.281 : Capacity of the membrane (nF)\n    * tau_m = 9.3667 : Membrane time constant (ms)\n    * tau_refrac = 0.1 : Duration of refractory period (ms)\n    * tau_syn_E = 5.0 : Decay time of excitatory synaptic current (ms)\n    * tau_syn_I = 5.0 : Decay time of inhibitory synaptic current (ms)\n    * e_rev_E = 0.0 : Reversal potential for excitatory input (mV)\n    * e_rev_I = -80.0 : Reversal potential for inhibitory input (mv)\n    * tau_w = 144.0 : Time constant of the adaptation variable (ms)\n    * a = 4.0 : Scaling of the adaptation variable\n    * b = 0.0805 : Increment on the adaptation variable after a spike\n    * i_offset = 0.0 : Offset current (nA)\n    * delta_T = 2.0 : Speed of the exponential (mV)\n    * v_thresh = -50.4 : Spike threshold for the exponential (mV)\n    * v_reset = -70.6 : Reset potential after a spike (mV)\n    * v_spike = -40.0 : Spike threshold (mV)\n\n    Variables:\n\n    * I : input current (nA):\n\n        I = g_exc * (e_rev_E - v) + g_inh * (e_rev_I - v) + i_offset\n\n    * v : membrane potential in mV (init=-70.6):\n\n        tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w)\n\n    * w : adaptation variable (init=0.0):\n\n        tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w\n\n    * g_exc : excitatory current (init = 0.0):\n\n        tau_syn_E * dg_exc/dt = - g_exc\n\n    * g_inh : inhibitory current (init = 0.0):\n\n        tau_syn_I * dg_inh/dt = - g_inh\n\n    * alpha_exc : alpha function of excitatory current (init = 0.0):\n\n        tau_syn_E * dalpha_exc/dt = exp((tau_syn_E - dt/2.0)/tau_syn_E) * g_exc - alpha_exc  \n\n    * alpha_inh: alpha function of inhibitory current (init = 0.0):\n\n        tau_syn_I * dalpha_inh/dt = exp((tau_syn_I - dt/2.0)/tau_syn_I) * g_inh - alpha_inh  \n\n\n    Spike emission:\n\n        v &gt; v_spike\n\n    Reset:\n\n        v = v_reset\n        u += b\n\n    The ODEs are solved using the explicit Euler method.\n\n    Equivalent code:\n\n    ```python\n\n    EIF_cond_alpha_isfa_ista = Neuron(\n        parameters = \"\"\"\n            v_rest = -70.6\n            cm = 0.281 \n            tau_m = 9.3667 \n            tau_syn_E = 5.0 \n            tau_syn_I = 5.0 \n            e_rev_E = 0.0 \n            e_rev_I = -80.0\n            tau_w = 144.0 \n            a = 4.0\n            b = 0.0805\n            i_offset = 0.0\n            delta_T = 2.0 \n            v_thresh = -50.4\n            v_reset = -70.6\n            v_spike = -40.0 \n        \"\"\", \n        equations = \"\"\"\n            gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)\n            gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)                \n            I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset\n            tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=-70.6\n            tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w \n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n            tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc  : exponential\n            tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh  : exponential\n        \"\"\",\n        spike = \"v &gt; v_spike\",\n        reset = \"\"\"\n            v = v_reset\n            w += b\n        \"\"\",\n        refractory = 0.1\n    )\n    ```\n\n    '''\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, v_rest = -70.6, cm = 0.281, tau_m = 9.3667, \n        tau_refrac = 0.1, tau_syn_E = 5.0, tau_syn_I = 5.0, \n        e_rev_E = 0.0, e_rev_I = -80.0, tau_w = 144.0, \n        a = 4.0, b = 0.0805, i_offset = 0.0, delta_T = 2.0, \n        v_thresh = -50.4, v_reset = -70.6, v_spike = -40.0):\n\n        # Create the arguments\n        parameters = \"\"\"\n            v_rest     = %(v_rest)s\n            cm         = %(cm)s\n            tau_m      = %(tau_m)s\n            tau_refrac = %(tau_refrac)s\n            tau_syn_E  = %(tau_syn_E)s\n            tau_syn_I  = %(tau_syn_I)s\n            e_rev_E    = %(e_rev_E)s\n            e_rev_I    = %(e_rev_I)s\n            tau_w      = %(tau_w)s\n            a          = %(a)s\n            b          = %(b)s\n            i_offset   = %(i_offset)s\n            delta_T    = %(delta_T)s\n            v_thresh   = %(v_thresh)s\n            v_reset    = %(v_reset)s\n            v_spike    = %(v_spike)s\n        \"\"\" % {\n            'v_rest'     : v_rest    ,\n            'cm'         : cm        ,\n            'tau_m'      : tau_m     ,\n            'tau_refrac' : tau_refrac,\n            'tau_syn_E'  : tau_syn_E ,\n            'tau_syn_I'  : tau_syn_I ,\n            'e_rev_E'    : e_rev_E   ,\n            'e_rev_I'    : e_rev_I   ,\n            'tau_w'      : tau_w     ,\n            'a'          : a         ,\n            'b'          : b         ,\n            'i_offset'   : i_offset  ,\n            'delta_T'    : delta_T   ,\n            'v_thresh'   : v_thresh  ,\n            'v_reset'    : v_reset   ,\n            'v_spike'    : v_spike   ,\n        }\n        # Equations for the variables\n        equations=\"\"\"    \n\n            gmax_exc = exp((tau_syn_E - dt/2.0)/tau_syn_E)\n            gmax_inh = exp((tau_syn_I - dt/2.0)/tau_syn_I)\n\n            I = alpha_exc * (e_rev_E - v) + alpha_inh * (e_rev_I - v) + i_offset\n\n            tau_m * dv/dt = (v_rest - v +  delta_T * exp((v-v_thresh)/delta_T)) + tau_m/cm*(I - w) : init=%(v_reset)s\n\n            tau_w * dw/dt = a * (v - v_rest) / 1000.0 - w \n\n            tau_syn_E * dg_exc/dt = - g_exc : exponential\n            tau_syn_I * dg_inh/dt = - g_inh : exponential\n            tau_syn_E * dalpha_exc/dt = gmax_exc * g_exc - alpha_exc  : exponential\n            tau_syn_I * dalpha_inh/dt = gmax_inh * g_inh - alpha_inh  : exponential\n        \"\"\" % {'v_reset': v_reset}\n\n        spike = \"\"\"\n            v &gt; v_spike\n        \"\"\"\n        reset = \"\"\"\n            v = v_reset\n            w += b\n        \"\"\"\n        Neuron.__init__(self, parameters=parameters, equations=equations, \n            spike=spike, reset=reset, refractory='tau_refrac',\n            name=\"Adaptive exponential Integrate-and-Fire\", \n            description=\"Exponential integrate-and-fire neuron with spike triggered and sub-threshold adaptation conductances (isfa, ista reps.).\")\n\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificPopulation.html","title":"Specific Populations","text":"<p>ANNarchy provides a set of predefined <code>Population</code> objects to ease the definition of standard networks.</p>"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.PoissonPopulation","title":"<code>ANNarchy.core.SpecificPopulation.PoissonPopulation</code>","text":"<p>             Bases: <code>SpecificPopulation</code></p> <p>Population of spiking neurons following a Poisson distribution.</p> <p>Case 1: Input population</p> <p>Each neuron of the population will randomly emit spikes, with a mean firing rate defined by the rates argument.</p> <p>The mean firing rate in Hz can be a fixed value for all neurons:</p> <pre><code>pop = PoissonPopulation(geometry=100, rates=100.0)\n</code></pre> <p>but it can be modified later as a normal parameter:</p> <pre><code>pop.rates = np.linspace(10, 150, 100)\n</code></pre> <p>It is also possible to define a temporal equation for the rates, by passing a string to the argument:</p> <pre><code>pop = PoissonPopulation(\n    geometry=100, \n    rates=\"100.0 * (1.0 + sin(2*pi*t/1000.0) )/2.0\"\n)\n</code></pre> <p>The syntax of this equation follows the same structure as neural variables.</p> <p>It is also possible to add parameters to the population which can be used in the equation of <code>rates</code>:</p> <pre><code>pop = PoissonPopulation(\n    geometry=100,\n    parameters = '''\n        amp = 100.0\n        frequency = 1.0\n    ''',\n    rates=\"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\"\n)\n</code></pre> <p>Note: The preceding definition is fully equivalent to the definition of this neuron:</p> <pre><code>poisson = Neuron(\n    parameters = '''\n        amp = 100.0\n        frequency = 1.0\n    ''',\n    equations = '''\n        rates = amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\n        p = Uniform(0.0, 1.0) * 1000.0 / dt\n    ''',\n    spike = '''\n        p &lt; rates\n    '''\n)\n</code></pre> <p>The refractory period can also be set, so that a neuron can not emit two spikes too close from each other.</p> <p>Case 2: Hybrid population</p> <p>If the <code>rates</code> argument is not set, the population can be used as an interface from a rate-coded population.</p> <p>The <code>target</code> argument specifies which incoming projections will be summed to determine the instantaneous firing rate of each neuron.</p> <p>See the example in <code>examples/hybrid/Hybrid.py</code> for a usage.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificPopulation.py</code> <pre><code>class PoissonPopulation(SpecificPopulation):\n\"\"\"\n    Population of spiking neurons following a Poisson distribution.\n\n    **Case 1:** Input population\n\n    Each neuron of the population will randomly emit spikes, with a mean firing rate defined by the *rates* argument.\n\n    The mean firing rate in Hz can be a fixed value for all neurons:\n\n    ```python\n    pop = PoissonPopulation(geometry=100, rates=100.0)\n    ```\n\n    but it can be modified later as a normal parameter:\n\n    ```python\n    pop.rates = np.linspace(10, 150, 100)\n    ```\n\n    It is also possible to define a temporal equation for the rates, by passing a string to the argument:\n\n    ```python\n    pop = PoissonPopulation(\n        geometry=100, \n        rates=\"100.0 * (1.0 + sin(2*pi*t/1000.0) )/2.0\"\n    )\n    ```\n\n    The syntax of this equation follows the same structure as neural variables.\n\n    It is also possible to add parameters to the population which can be used in the equation of `rates`:\n\n    ```python\n    pop = PoissonPopulation(\n        geometry=100,\n        parameters = '''\n            amp = 100.0\n            frequency = 1.0\n        ''',\n        rates=\"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\"\n    )\n    ```\n\n    **Note:** The preceding definition is fully equivalent to the definition of this neuron:\n\n    ```python\n    poisson = Neuron(\n        parameters = '''\n            amp = 100.0\n            frequency = 1.0\n        ''',\n        equations = '''\n            rates = amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\n            p = Uniform(0.0, 1.0) * 1000.0 / dt\n        ''',\n        spike = '''\n            p &lt; rates\n        '''\n    )\n    ```\n\n    The refractory period can also be set, so that a neuron can not emit two spikes too close from each other.\n\n    **Case 2:** Hybrid population\n\n    If the ``rates`` argument is not set, the population can be used as an interface from a rate-coded population.\n\n    The ``target`` argument specifies which incoming projections will be summed to determine the instantaneous firing rate of each neuron.\n\n    See the example in ``examples/hybrid/Hybrid.py`` for a usage.\n\n    \"\"\"\n\n    def __init__(self, geometry, name=None, rates=None, target=None, parameters=None, refractory=None, copied=False):\n\"\"\"\n        :param geometry: population geometry as tuple.\n        :param name: unique name of the population (optional).\n        :param rates: mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string).\n        :param target: the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\").\n        :param parameters: additional parameters which can be used in the *rates* equation.\n        :param refractory: refractory period in ms.\n        \"\"\"\n        if rates is None and target is None:\n            Global._error('A PoissonPopulation must define either rates or target.')\n\n        self.target = target\n        self.parameters = parameters\n        self.refractory_init = refractory\n        self.rates_init = rates\n\n        if target is not None: # hybrid population\n            # Create the neuron\n            poisson_neuron = Neuron(\n                parameters = \"\"\"\n%(params)s\n                \"\"\" % {'params': parameters if parameters else ''},\n                equations = \"\"\"\n                rates = sum(%(target)s)\n                p = Uniform(0.0, 1.0) * 1000.0 / dt\n                _sum_%(target)s = 0.0\n                \"\"\" % {'target': target},\n                spike = \"\"\"\n                    p &lt; rates\n                \"\"\",\n                refractory=refractory,\n                name=\"Hybrid\",\n                description=\"Hybrid spiking neuron emitting spikes according to a Poisson distribution at a frequency determined by the weighted sum of inputs.\"\n            )\n\n\n        elif isinstance(rates, str):\n            # Create the neuron\n            poisson_neuron = Neuron(\n                parameters = \"\"\"\n%(params)s\n                \"\"\" % {'params': parameters if parameters else ''},\n                equations = \"\"\"\n                rates = %(rates)s\n                p = Uniform(0.0, 1.0) * 1000.0 / dt\n                _sum_exc = 0.0\n                \"\"\" % {'rates': rates},\n                spike = \"\"\"\n                    p &lt; rates\n                \"\"\",\n                refractory=refractory,\n                name=\"Poisson\",\n                description=\"Spiking neuron with spikes emitted according to a Poisson distribution.\"\n            )\n\n        elif isinstance(rates, np.ndarray):\n            poisson_neuron = Neuron(\n                parameters = \"\"\"\n                rates = 10.0\n                \"\"\",\n                equations = \"\"\"\n                p = Uniform(0.0, 1.0) * 1000.0 / dt\n                \"\"\",\n                spike = \"\"\"\n                p &lt; rates\n                \"\"\",\n                refractory=refractory,\n                name=\"Poisson\",\n                description=\"Spiking neuron with spikes emitted according to a Poisson distribution.\"\n            )\n        else:\n            poisson_neuron = Neuron(\n                parameters = \"\"\"\n                rates = %(rates)s\n                \"\"\" % {'rates': rates},\n                equations = \"\"\"\n                p = Uniform(0.0, 1.0) * 1000.0 / dt\n                \"\"\",\n                spike = \"\"\"\n                p &lt; rates\n                \"\"\",\n                refractory=refractory,\n                name=\"Poisson\",\n                description=\"Spiking neuron with spikes emitted according to a Poisson distribution.\"\n            )\n        SpecificPopulation.__init__(self, geometry=geometry, neuron=poisson_neuron, name=name, copied=copied)\n\n        if isinstance(rates, np.ndarray):\n            self.rates = rates\n\n    def _copy(self):\n        \"Returns a copy of the population when creating networks.\"\n        return PoissonPopulation(self.geometry, name=self.name, rates=self.rates_init, target=self.target, parameters=self.parameters, refractory=self.refractory_init, copied=True)\n\n    def _generate_st(self):\n\"\"\"\n        Generate single thread code.\n\n        We don't need any separate code snippets. All is done during the\n        normal code generation path.\n        \"\"\"\n        pass\n\n    def _generate_omp(self):\n\"\"\"\n        Generate openMP code.\n\n        We don't need any separate code snippets. All is done during the\n        normal code generation path.\n        \"\"\"\n        pass\n\n    def _generate_cuda(self):\n\"\"\"\n        Generate CUDA code.\n\n        We don't need any separate code snippets. All is done during the\n        normal code generation path.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.PoissonPopulation.__init__","title":"<code>__init__(geometry, name=None, rates=None, target=None, parameters=None, refractory=None, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> geometry         \u2013          <p>population geometry as tuple.</p> </li> <li> name         \u2013          <p>unique name of the population (optional).</p> </li> <li> rates         \u2013          <p>mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string).</p> </li> <li> target         \u2013          <p>the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\").</p> </li> <li> parameters         \u2013          <p>additional parameters which can be used in the rates equation.</p> </li> <li> refractory         \u2013          <p>refractory period in ms.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificPopulation.py</code> <pre><code>def __init__(self, geometry, name=None, rates=None, target=None, parameters=None, refractory=None, copied=False):\n\"\"\"\n    :param geometry: population geometry as tuple.\n    :param name: unique name of the population (optional).\n    :param rates: mean firing rate of each neuron. It can be a single value (e.g. 10.0) or an equation (as string).\n    :param target: the mean firing rate will be the weighted sum of inputs having this target name (e.g. \"exc\").\n    :param parameters: additional parameters which can be used in the *rates* equation.\n    :param refractory: refractory period in ms.\n    \"\"\"\n    if rates is None and target is None:\n        Global._error('A PoissonPopulation must define either rates or target.')\n\n    self.target = target\n    self.parameters = parameters\n    self.refractory_init = refractory\n    self.rates_init = rates\n\n    if target is not None: # hybrid population\n        # Create the neuron\n        poisson_neuron = Neuron(\n            parameters = \"\"\"\n%(params)s\n            \"\"\" % {'params': parameters if parameters else ''},\n            equations = \"\"\"\n            rates = sum(%(target)s)\n            p = Uniform(0.0, 1.0) * 1000.0 / dt\n            _sum_%(target)s = 0.0\n            \"\"\" % {'target': target},\n            spike = \"\"\"\n                p &lt; rates\n            \"\"\",\n            refractory=refractory,\n            name=\"Hybrid\",\n            description=\"Hybrid spiking neuron emitting spikes according to a Poisson distribution at a frequency determined by the weighted sum of inputs.\"\n        )\n\n\n    elif isinstance(rates, str):\n        # Create the neuron\n        poisson_neuron = Neuron(\n            parameters = \"\"\"\n%(params)s\n            \"\"\" % {'params': parameters if parameters else ''},\n            equations = \"\"\"\n            rates = %(rates)s\n            p = Uniform(0.0, 1.0) * 1000.0 / dt\n            _sum_exc = 0.0\n            \"\"\" % {'rates': rates},\n            spike = \"\"\"\n                p &lt; rates\n            \"\"\",\n            refractory=refractory,\n            name=\"Poisson\",\n            description=\"Spiking neuron with spikes emitted according to a Poisson distribution.\"\n        )\n\n    elif isinstance(rates, np.ndarray):\n        poisson_neuron = Neuron(\n            parameters = \"\"\"\n            rates = 10.0\n            \"\"\",\n            equations = \"\"\"\n            p = Uniform(0.0, 1.0) * 1000.0 / dt\n            \"\"\",\n            spike = \"\"\"\n            p &lt; rates\n            \"\"\",\n            refractory=refractory,\n            name=\"Poisson\",\n            description=\"Spiking neuron with spikes emitted according to a Poisson distribution.\"\n        )\n    else:\n        poisson_neuron = Neuron(\n            parameters = \"\"\"\n            rates = %(rates)s\n            \"\"\" % {'rates': rates},\n            equations = \"\"\"\n            p = Uniform(0.0, 1.0) * 1000.0 / dt\n            \"\"\",\n            spike = \"\"\"\n            p &lt; rates\n            \"\"\",\n            refractory=refractory,\n            name=\"Poisson\",\n            description=\"Spiking neuron with spikes emitted according to a Poisson distribution.\"\n        )\n    SpecificPopulation.__init__(self, geometry=geometry, neuron=poisson_neuron, name=name, copied=copied)\n\n    if isinstance(rates, np.ndarray):\n        self.rates = rates\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.SpikeSourceArray","title":"<code>ANNarchy.core.SpecificPopulation.SpikeSourceArray</code>","text":"<p>             Bases: <code>SpecificPopulation</code></p> <p>Spike source generating spikes at the times given in the spike_times array.</p> <p>Depending on the initial array provided, the population will have one or several neurons, but the geometry can only be one-dimensional.</p> <p>You can later modify the spike_times attribute of the population, but it must have the same number of neurons as the initial one.</p> <p>The spike times are by default relative to the start of a simulation (<code>ANNarchy.get_time()</code> is 0.0). If you call the <code>reset()</code> method of a <code>SpikeSourceArray</code>, this will set the spike times relative to the current time. You can then repeat a stimulation many times.</p> <pre><code># 2 neurons firing at 100Hz with a 1 ms delay\ntimes = [\n    [ 10, 20, 30, 40],\n    [ 11, 21, 31, 41]\n]\ninp = SpikeSourceArray(spike_times=times)\n\ncompile()\n\n# Spikes at 10/11, 20/21, etc\nsimulate(50)\n\n# Reset the internal time of the SpikeSourceArray\ninp.reset()\n\n# Spikes at 60/61, 70/71, etc\nsimulate(50)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificPopulation.py</code> <pre><code>class SpikeSourceArray(SpecificPopulation):\n\"\"\"\n    Spike source generating spikes at the times given in the spike_times array.\n\n    Depending on the initial array provided, the population will have one or several neurons, but the geometry can only be one-dimensional.\n\n    You can later modify the spike_times attribute of the population, but it must have the same number of neurons as the initial one.\n\n    The spike times are by default relative to the start of a simulation (``ANNarchy.get_time()`` is 0.0).\n    If you call the ``reset()`` method of a ``SpikeSourceArray``, this will set the spike times relative to the current time.\n    You can then repeat a stimulation many times.\n\n    ```python\n    # 2 neurons firing at 100Hz with a 1 ms delay\n    times = [\n        [ 10, 20, 30, 40],\n        [ 11, 21, 31, 41]\n    ]\n    inp = SpikeSourceArray(spike_times=times)\n\n    compile()\n\n    # Spikes at 10/11, 20/21, etc\n    simulate(50)\n\n    # Reset the internal time of the SpikeSourceArray\n    inp.reset()\n\n    # Spikes at 60/61, 70/71, etc\n    simulate(50)\n    ```\n    \"\"\"\n    def __init__(self, spike_times, name=None, copied=False):\n\"\"\"\n        :param spike_times: a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt.\n        :param name: optional name for the population.\n        \"\"\"\n\n        if not isinstance(spike_times, list):\n            Global._error('In a SpikeSourceArray, spike_times must be a Python list.')\n\n        if isinstance(spike_times[0], list): # several neurons\n            nb_neurons = len(spike_times)\n        else: # a single Neuron\n            nb_neurons = 1\n            spike_times = [ spike_times ]\n\n        # Create a fake neuron just to be sure the description has the correct parameters\n        neuron = Neuron(\n            parameters=\"\",\n            equations=\"\",\n            spike=\" t == 0\",\n            reset=\"\",\n            name=\"Spike source\",\n            description=\"Spike source array.\"\n        )\n\n        SpecificPopulation.__init__(self, geometry=nb_neurons, neuron=neuron, name=name, copied=copied)\n\n        self.init['spike_times'] = spike_times\n\n\n    def _copy(self):\n        \"Returns a copy of the population when creating networks.\"\n        return SpikeSourceArray(self.init['spike_times'], self.name, copied=True)\n\n    def _sort_spikes(self, spike_times):\n        \"Sort, unify the spikes and transform them into steps.\"\n        return [sorted(list(set([round(t/Global.config['dt']) for t in neur_times]))) for neur_times in spike_times]\n\n    def _generate_st(self):\n\"\"\"\n        Code generation for single-thread.\n        \"\"\"\n        self._generate_omp()\n\n    def _generate_omp(self):\n\"\"\"\n        Code generation for openMP paradigm.\n        \"\"\"\n        # Add possible targets\n        for target in self.targets:\n            tpl = {\n                'name': 'g_%(target)s' % {'target': target},\n                'locality': 'local',\n                'eq': '',\n                'bounds': {},\n                'flags': [],\n                'ctype': Global.config['precision'],\n                'init': 0.0,\n                'transformed_eq': '',\n                'pre_loop': {},\n                'cpp': '',\n                'switch': '',\n                'untouched': {},\n                'method': 'exponential',\n                'dependencies': []\n            }\n            self.neuron_type.description['variables'].append(tpl)\n            self.neuron_type.description['local'].append('g_'+target)\n\n        self._specific_template['declare_additional'] = \"\"\"\n    // Custom local parameter spike_times\n    // std::vector&lt; %(float_prec)s &gt; r ;\n    std::vector&lt; std::vector&lt; long int &gt; &gt; spike_times ;\n    std::vector&lt; long int &gt;  next_spike ;\n    std::vector&lt; int &gt; idx_next_spike;\n    long int _t;\n\n    // Recompute the spike times\n    void recompute_spike_times(){\n        std::fill(next_spike.begin(), next_spike.end(), -10000);\n        std::fill(idx_next_spike.begin(), idx_next_spike.end(), 0);\n        for(int i=0; i&lt; size; i++){\n            if(!spike_times[i].empty()){\n                int idx = 0;\n                // Find the first spike time which is not in the past\n                while(spike_times[i][idx] &lt; _t){\n                    idx++;\n                }\n                // Set the next spike\n                if(idx &lt; spike_times[i].size())\n                    next_spike[i] = spike_times[i][idx];\n                else\n                    next_spike[i] = -10000;\n            }\n        }\n    }\n\"\"\"% { 'float_prec': Global.config['precision'] }\n\n        #self._specific_template['access_parameters_variables'] = \"\"\n\n        self._specific_template['init_additional'] = \"\"\"\n        _t = 0;\n        next_spike = std::vector&lt;long int&gt;(size, -10000);\n        idx_next_spike = std::vector&lt;int&gt;(size, 0);\n        this-&gt;recompute_spike_times();\n\"\"\" \n\n        self._specific_template['reset_additional'] = \"\"\"\n        _t = 0;\n        this-&gt;recompute_spike_times();\n\"\"\"\n\n        if Global.config[\"num_threads\"] == 1:\n            self._specific_template['update_variables'] = \"\"\"\n        if(_active){\n            spiked.clear();\n            for(int i = 0; i &lt; size; i++){\n                // Emit spike\n                if( _t == next_spike[i] ){\n                    last_spike[i] = _t;\n                    idx_next_spike[i]++ ;\n                    if(idx_next_spike[i] &lt; spike_times[i].size()){\n                        next_spike[i] = spike_times[i][idx_next_spike[i]];\n                    }\n                    spiked.push_back(i);\n                }\n            }\n            _t++;\n        }\n\"\"\"\n        else:\n            self._specific_template['update_variables'] = \"\"\"\n        if(_active){\n            #pragma omp single\n            {\n                spiked.clear();\n            }\n\n            #pragma omp for\n            for(int i = 0; i &lt; size; i++){\n                // Emit spike\n                if( _t == next_spike[i] ){\n                    last_spike[i] = _t;\n                    idx_next_spike[i]++ ;\n                    if(idx_next_spike[i] &lt; spike_times[i].size()){\n                        next_spike[i] = spike_times[i][idx_next_spike[i]];\n                    }\n\n                    #pragma omp critical\n                    spiked.push_back(i);\n                }\n            }\n\n            #pragma omp single\n            {\n                _t++;\n            }\n        }\n\"\"\"\n        self._specific_template['test_spike_cond'] = \"\"\n\n        self._specific_template['export_additional'] =\"\"\"\n        vector[vector[long]] spike_times\n        void recompute_spike_times()\n\"\"\"\n\n        self._specific_template['wrapper_args'] = \"size, times, delay\"\n        self._specific_template['wrapper_init'] = \"\"\"\n        pop%(id)s.spike_times = times\n        pop%(id)s.set_size(size)\n        pop%(id)s.set_max_delay(delay)\"\"\" % {'id': self.id}\n\n        self._specific_template['wrapper_access_additional'] = \"\"\"\n    # Local parameter spike_times\n    cpdef get_spike_times(self):\n        return pop%(id)s.spike_times\n    cpdef set_spike_times(self, value):\n        pop%(id)s.spike_times = value\n        pop%(id)s.recompute_spike_times()\n\"\"\" % {'id': self.id}\n\n    def _generate_cuda(self):\n\"\"\"\n        Code generation for the CUDA paradigm.\n\n        As the spike time generation is not a very compute intensive step but\n        requires dynamic data structures, we don't implement it on the CUDA\n        devices for now. Consequently, we use the CPU side implementation and\n        transfer after computation the results to the GPU.\n        \"\"\"\n        self._generate_st()\n\n        # attach transfer of spiked array to gpu\n        # IMPORTANT: the outside transfer is necessary.\n        # Otherwise, previous spike counts will be not reseted.\n        self._specific_template['update_variables'] += \"\"\"\n        if ( _active ) {\n            // Update Spike Count on GPU\n            spike_count = spiked.size();\n            cudaMemcpy( gpu_spike_count, &amp;spike_count, sizeof(unsigned int), cudaMemcpyHostToDevice);\n\n            // Transfer generated spikes to GPU\n            if( spike_count &gt; 0 ) {\n                cudaMemcpy( gpu_spiked, spiked.data(), spike_count * sizeof(int), cudaMemcpyHostToDevice);\n            }\n        }\n        \"\"\"\n        self._specific_template['update_variable_body'] = \"\"\n        self._specific_template['update_variable_header'] = \"\"\n        self._specific_template['update_variable_call'] = \"\"\"\n    // host side update of neurons\n    pop%(id)s.update();\n\"\"\" % {'id': self.id}\n\n    def _instantiate(self, module):\n        # Create the Cython instance\n        self.cyInstance = getattr(module, self.class_name+'_wrapper')(self.size, self.init['spike_times'], self.max_delay)\n\n    def __setattr__(self, name, value):\n        if name == 'spike_times':\n            if not isinstance(value[0], list): # several neurons\n                value = [ value ]\n            if not len(value) == self.size:\n                Global._error('SpikeSourceArray: the size of the spike_times attribute must match the number of neurons in the population.')\n\n            self.init['spike_times'] = value # when reset is called\n            if self.initialized:\n                self.cyInstance.set_spike_times(self._sort_spikes(value))\n        else:\n            Population.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        if name == 'spike_times':\n            if self.initialized:\n                return [ [Global.config['dt']*time for time in neur] for neur in self.cyInstance.get_spike_times()]\n            else:\n                return self.init['spike_times']\n        else:\n            return Population.__getattribute__(self, name)\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.SpikeSourceArray.__init__","title":"<code>__init__(spike_times, name=None, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> spike_times         \u2013          <p>a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt.</p> </li> <li> name         \u2013          <p>optional name for the population.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificPopulation.py</code> <pre><code>def __init__(self, spike_times, name=None, copied=False):\n\"\"\"\n    :param spike_times: a list of times at which a spike should be emitted if the population should have only 1 neuron, a list of lists otherwise. Times are defined in milliseconds, and will be rounded to the closest multiple of the discretization time step dt.\n    :param name: optional name for the population.\n    \"\"\"\n\n    if not isinstance(spike_times, list):\n        Global._error('In a SpikeSourceArray, spike_times must be a Python list.')\n\n    if isinstance(spike_times[0], list): # several neurons\n        nb_neurons = len(spike_times)\n    else: # a single Neuron\n        nb_neurons = 1\n        spike_times = [ spike_times ]\n\n    # Create a fake neuron just to be sure the description has the correct parameters\n    neuron = Neuron(\n        parameters=\"\",\n        equations=\"\",\n        spike=\" t == 0\",\n        reset=\"\",\n        name=\"Spike source\",\n        description=\"Spike source array.\"\n    )\n\n    SpecificPopulation.__init__(self, geometry=nb_neurons, neuron=neuron, name=name, copied=copied)\n\n    self.init['spike_times'] = spike_times\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.TimedArray","title":"<code>ANNarchy.core.SpecificPopulation.TimedArray</code>","text":"<p>             Bases: <code>SpecificPopulation</code></p> <p>Data structure holding sequential inputs for a rate-coded network.</p> <p>The input values are stored in the (recordable) attribute <code>r</code>, without any further processing.  You will need to connect this population to another one using the <code>connect_one_to_one()</code> method.</p> <p>By default, the firing rate of this population will iterate over the different values step by step:</p> <pre><code>inputs = np.array(\n    [\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n    ]\n)\n\ninp = TimedArray(rates=inputs)\n\npop = Population(10, ...)\n\nproj = Projection(inp, pop, 'exc')\nproj.connect_one_to_one(1.0)\n\ncompile()\n\nsimulate(10.)\n</code></pre> <p>This creates a population of 10 neurons whose activity will change during the first 10*dt milliseconds of the simulation. After that delay, the last input will be kept (i.e. 1 for the last neuron).</p> <p>If you want the TimedArray to \"loop\" over the different input vectors, you can specify a period for the inputs:</p> <pre><code>inp = TimedArray(rates=inputs, period=10.)\n</code></pre> <p>If the period is smaller than the length of the rates, the last inputs will not be set.</p> <p>If you do not want the inputs to be set at every step, but every 10 ms for example, youcan use the <code>schedule</code> argument:</p> <pre><code>inp = TimedArray(rates=inputs, schedule=10.)\n</code></pre> <p>The input [1, 0, 0,...] will stay for 10 ms, then[0, 1, 0, ...] for the next 10 ms, etc...</p> <p>If you need a less regular schedule, you can specify it as a list of times:</p> <pre><code>inp = TimedArray(rates=inputs, schedule=[10., 20., 50., 60., 100., 110.])\n</code></pre> <p>The first input is set at t = 10 ms (r = 0.0 in the first 10 ms), the second at t = 20 ms, the third at t = 50 ms, etc.</p> <p>If you specify less times than in the array of rates, the last ones will be ignored.</p> <p>Scheduling can be combined with periodic cycling. Note that you can use the <code>reset()</code> method to manually reinitialize the TimedArray, times becoming relative to that call:</p> <pre><code>simulate(100.) # ten inputs are shown with a schedule of 10 ms\ninp.reset()\nsimulate(100.) # the same ten inputs are presented again.\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificPopulation.py</code> <pre><code>class TimedArray(SpecificPopulation):\n\"\"\"\n    Data structure holding sequential inputs for a rate-coded network.\n\n    The input values are stored in the (recordable) attribute `r`, without any further processing. \n    You will need to connect this population to another one using the ``connect_one_to_one()`` method.\n\n    By default, the firing rate of this population will iterate over the different values step by step:\n\n    ```python\n    inputs = np.array(\n        [\n            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n        ]\n    )\n\n    inp = TimedArray(rates=inputs)\n\n    pop = Population(10, ...)\n\n    proj = Projection(inp, pop, 'exc')\n    proj.connect_one_to_one(1.0)\n\n    compile()\n\n    simulate(10.)\n    ```\n\n    This creates a population of 10 neurons whose activity will change during the first 10*dt milliseconds of the simulation. After that delay, the last input will be kept (i.e. 1 for the last neuron).\n\n    If you want the TimedArray to \"loop\" over the different input vectors, you can specify a period for the inputs:\n\n    ```python\n    inp = TimedArray(rates=inputs, period=10.)\n    ```\n\n    If the period is smaller than the length of the rates, the last inputs will not be set.\n\n    If you do not want the inputs to be set at every step, but every 10 ms for example, youcan use the ``schedule`` argument:\n\n    ```python\n    inp = TimedArray(rates=inputs, schedule=10.)\n    ```\n\n    The input [1, 0, 0,...] will stay for 10 ms, then[0, 1, 0, ...] for the next 10 ms, etc...\n\n    If you need a less regular schedule, you can specify it as a list of times:\n\n    ```python\n    inp = TimedArray(rates=inputs, schedule=[10., 20., 50., 60., 100., 110.])\n    ```\n\n    The first input is set at t = 10 ms (r = 0.0 in the first 10 ms), the second at t = 20 ms, the third at t = 50 ms, etc.\n\n    If you specify less times than in the array of rates, the last ones will be ignored.\n\n    Scheduling can be combined with periodic cycling. Note that you can use the ``reset()`` method to manually reinitialize the TimedArray, times becoming relative to that call:\n\n    ```python\n    simulate(100.) # ten inputs are shown with a schedule of 10 ms\n    inp.reset()\n    simulate(100.) # the same ten inputs are presented again.\n    ```\n\n    \"\"\"\n    def __init__(self, rates, schedule=0., period= -1., name=None, copied=False):\n\"\"\"\n        :param rates: array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population.\n        :param schedule: either a single value or a list of time points where inputs should be set. Default: every timestep.\n        :param period: time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.).\n        \"\"\"\n        neuron = Neuron(\n            parameters=\"\",\n            equations=\" r = 0.0\",\n            name=\"Timed Array\",\n            description=\"Timed array source.\"\n        )\n        # Geometry of the population\n        geometry = rates.shape[1:]\n\n        # Check the schedule\n        if isinstance(schedule, (int, float)):\n            if float(schedule) &lt;= 0.0:\n                schedule = Global.config['dt']\n            schedule = [ float(schedule*i) for i in range(rates.shape[0])]\n\n        if len(schedule) &gt; rates.shape[0]:\n            Global._error('TimedArray: the length of the schedule parameter cannot exceed the first dimension of the rates parameter.')\n\n        if len(schedule) &lt; rates.shape[0]:\n            Global._warning('TimedArray: the length of the schedule parameter is smaller than the first dimension of the rates parameter (more data than time points). Make sure it is what you expect.')\n\n        SpecificPopulation.__init__(self, geometry=geometry, neuron=neuron, name=name, copied=copied)\n\n        self.init['schedule'] = schedule\n        self.init['rates'] = rates\n        self.init['period'] = period\n\n    def _copy(self):\n        \"Returns a copy of the population when creating networks.\"\n        return TimedArray(self.init['rates'] , self.init['schedule'], self.init['period'], self.name, copied=True)\n\n    def _generate_st(self):\n\"\"\"\n        adjust code templates for the specific population for single thread and openMP.\n        \"\"\"\n        self._specific_template['declare_additional'] = \"\"\"\n    // Custom local parameters of a TimedArray\n    std::vector&lt; int &gt; _schedule; // List of times where new inputs should be set\n    std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; _buffer; // buffer holding the data\n    int _period; // Period of cycling\n    long int _t; // Internal time\n    int _block; // Internal block when inputs are set not at each step\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['access_additional'] = \"\"\"\n    // Custom local parameters of a TimedArray\n    void set_schedule(std::vector&lt;int&gt; schedule) { _schedule = schedule; }\n    std::vector&lt;int&gt; get_schedule() { return _schedule; }\n    void set_buffer(std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; buffer) { _buffer = buffer; r = _buffer[0]; }\n    std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; get_buffer() { return _buffer; }\n    void set_period(int period) { _period = period; }\n    int get_period() { return _period; }\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['init_additional'] = \"\"\"\n        // Initialize counters\n        _t = 0;\n        _block = 0;\n        _period = -1;\n\"\"\"\n        self._specific_template['export_additional'] = \"\"\"\n        # Custom local parameters of a TimedArray\n        void set_schedule(vector[int])\n        vector[int] get_schedule()\n        void set_buffer(vector[vector[%(float_prec)s]])\n        vector[vector[%(float_prec)s]] get_buffer()\n        void set_period(int)\n        int get_period()\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['reset_additional'] =\"\"\"\n        _t = 0;\n        _block = 0;\n\n        r.clear();\n        r = std::vector&lt;%(float_prec)s&gt;(size, 0.0);\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['wrapper_access_additional'] = \"\"\"\n    # Custom local parameters of a TimedArray\n    cpdef set_schedule( self, schedule ):\n        pop%(id)s.set_schedule( schedule )\n    cpdef np.ndarray get_schedule( self ):\n        return np.array(pop%(id)s.get_schedule( ))\n\n    cpdef set_rates( self, buffer ):\n        pop%(id)s.set_buffer( buffer )\n    cpdef np.ndarray get_rates( self ):\n        return np.array(pop%(id)s.get_buffer( ))\n\n    cpdef set_period( self, period ):\n        pop%(id)s.set_period(period)\n    cpdef int get_period(self):\n        return pop%(id)s.get_period()\n\"\"\" % { 'id': self.id }\n\n        self._specific_template['update_variables'] = \"\"\"\n        if(_active){\n            //std::cout &lt;&lt; _t &lt;&lt; \" \" &lt;&lt; _block&lt;&lt; \" \" &lt;&lt; _schedule[_block] &lt;&lt; std::endl;\n\n            // Check if it is time to set the input\n            if(_t == _schedule[_block]){\n                // Set the data\n                r = _buffer[_block];\n                // Move to the next block\n                _block++;\n                // If was the last block, go back to the first block\n                if (_block == _schedule.size()){\n                    _block = 0;\n                }\n            }\n\n            // If the timedarray is periodic, check if we arrive at that point\n            if(_period &gt; -1 &amp;&amp; (_t == _period-1)){\n                // Reset the counters\n                _block=0;\n                _t = -1;\n            }\n\n            // Always increment the internal time\n            _t++;\n        }\n\"\"\"\n\n        self._specific_template['size_in_bytes'] = \"\"\"\n        // schedule\n        size_in_bytes += _schedule.capacity() * sizeof(int);\n\n        // buffer\n        size_in_bytes += _buffer.capacity() * sizeof(std::vector&lt;%(float_prec)s&gt;);\n        for( auto it = _buffer.begin(); it != _buffer.end(); it++ )\n            size_in_bytes += it-&gt;capacity() * sizeof(%(float_prec)s);\n\"\"\" % {'float_prec': Global.config['precision']}\n\n    def _generate_omp(self):\n\"\"\"\n        adjust code templates for the specific population for single thread and openMP.\n        \"\"\"\n        self._specific_template['declare_additional'] = \"\"\"\n    // Custom local parameters of a TimedArray\n    std::vector&lt; int &gt; _schedule; // List of times where new inputs should be set\n    std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; _buffer; // buffer holding the data\n    int _period; // Period of cycling\n    long int _t; // Internal time\n    int _block; // Internal block when inputs are set not at each step\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['access_additional'] = \"\"\"\n    // Custom local parameters of a TimedArray\n    void set_schedule(std::vector&lt;int&gt; schedule) { _schedule = schedule; }\n    std::vector&lt;int&gt; get_schedule() { return _schedule; }\n    void set_buffer(std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; buffer) { _buffer = buffer; r = _buffer[0]; }\n    std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; get_buffer() { return _buffer; }\n    void set_period(int period) { _period = period; }\n    int get_period() { return _period; }\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['init_additional'] = \"\"\"\n        // Initialize counters\n        _t = 0;\n        _block = 0;\n        _period = -1;\n\"\"\"\n        self._specific_template['export_additional'] = \"\"\"\n        # Custom local parameters of a TimedArray\n        void set_schedule(vector[int])\n        vector[int] get_schedule()\n        void set_buffer(vector[vector[%(float_prec)s]])\n        vector[vector[%(float_prec)s]] get_buffer()\n        void set_period(int)\n        int get_period()\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['reset_additional'] =\"\"\"\n        _t = 0;\n        _block = 0;\n\n        r.clear();\n        r = std::vector&lt;%(float_prec)s&gt;(size, 0.0);\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['wrapper_access_additional'] = \"\"\"\n    # Custom local parameters of a TimedArray\n    cpdef set_schedule( self, schedule ):\n        pop%(id)s.set_schedule( schedule )\n    cpdef np.ndarray get_schedule( self ):\n        return np.array(pop%(id)s.get_schedule( ))\n\n    cpdef set_rates( self, buffer ):\n        pop%(id)s.set_buffer( buffer )\n    cpdef np.ndarray get_rates( self ):\n        return np.array(pop%(id)s.get_buffer( ))\n\n    cpdef set_period( self, period ):\n        pop%(id)s.set_period(period)\n    cpdef int get_period(self):\n        return pop%(id)s.get_period()\n\"\"\" % { 'id': self.id }\n\n        self._specific_template['update_variables'] = \"\"\"\n        if(_active){\n            #pragma omp single\n            {\n                // Check if it is time to set the input\n                if(_t == _schedule[_block]){\n                    // Set the data\n                    r = _buffer[_block];\n                    // Move to the next block\n                    _block++;\n                    // If was the last block, go back to the first block\n                    if (_block == _schedule.size()){\n                        _block = 0;\n                    }\n                }\n\n                // If the timedarray is periodic, check if we arrive at that point\n                if(_period &gt; -1 &amp;&amp; (_t == _period-1)){\n                    // Reset the counters\n                    _block=0;\n                    _t = -1;\n                }\n\n                // Always increment the internal time\n                _t++;\n            }\n        }\n\"\"\"\n\n        self._specific_template['size_in_bytes'] = \"\"\"\n        // schedule\n        size_in_bytes += _schedule.capacity() * sizeof(int);\n\n        // buffer\n        size_in_bytes += _buffer.capacity() * sizeof(std::vector&lt;%(float_prec)s&gt;);\n        for( auto it = _buffer.begin(); it != _buffer.end(); it++ )\n            size_in_bytes += it-&gt;capacity() * sizeof(%(float_prec)s);\n\"\"\" % {'float_prec': Global.config['precision']}\n\n    def _generate_cuda(self):\n\"\"\"\n        adjust code templates for the specific population for single thread and CUDA.\n        \"\"\"\n        # HD (18. Nov 2016)\n        # I suppress the code generation for allocating the variable r on gpu, as\n        # well as memory transfer codes. This is only possible as no other variables\n        # allowed in TimedArray.\n        self._specific_template['init_parameters_variables'] = \"\"\n        self._specific_template['host_device_transfer'] = \"\"\n        self._specific_template['device_host_transfer'] = \"\"\n\n        #\n        # Code for handling the buffer and schedule parameters\n        self._specific_template['declare_additional'] = \"\"\"\n    // Custom local parameter timed array\n    std::vector&lt; int &gt; _schedule;\n    std::vector&lt; %(float_prec)s* &gt; gpu_buffer;\n    int _period; // Period of cycling\n    long int _t; // Internal time\n    int _block; // Internal block when inputs are set not at each step\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['access_additional'] = \"\"\"\n    // Custom local parameter timed array\n    void set_schedule(std::vector&lt;int&gt; schedule) { _schedule = schedule; }\n    std::vector&lt;int&gt; get_schedule() { return _schedule; }\n    void set_buffer(std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; buffer) {\n        if ( gpu_buffer.empty() ) {\n            gpu_buffer = std::vector&lt; %(float_prec)s* &gt;(buffer.size(), nullptr);\n            // allocate gpu arrays\n            for(int i = 0; i &lt; buffer.size(); i++) {\n                cudaMalloc((void**)&amp;gpu_buffer[i], buffer[i].size()*sizeof(%(float_prec)s));\n            }\n        }\n\n        auto host_it = buffer.begin();\n        auto dev_it = gpu_buffer.begin();\n        for (; host_it &lt; buffer.end(); host_it++, dev_it++) {\n            cudaMemcpy( *dev_it, host_it-&gt;data(), host_it-&gt;size()*sizeof(%(float_prec)s), cudaMemcpyHostToDevice);\n        }\n\n        gpu_r = gpu_buffer[0];\n    }\n    std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; get_buffer() {\n        std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; buffer = std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt;( gpu_buffer.size(), std::vector&lt;%(float_prec)s&gt;(size,0.0) );\n\n        auto host_it = buffer.begin();\n        auto dev_it = gpu_buffer.begin();\n        for (; host_it &lt; buffer.end(); host_it++, dev_it++) {\n            cudaMemcpy( host_it-&gt;data(), *dev_it, size*sizeof(%(float_prec)s), cudaMemcpyDeviceToHost );\n        }\n\n        return buffer;\n    }\n    void set_period(int period) { _period = period; }\n    int get_period() { return _period; }\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['init_additional'] = \"\"\"\n        // counters\n        _t = 0;\n        _block = 0;\n        _period = -1;\n\"\"\"\n        self._specific_template['reset_additional'] = \"\"\"\n        // counters\n        _t = 0;\n        _block = 0;\n        gpu_r = gpu_buffer[0];\n\"\"\"\n        self._specific_template['export_additional'] = \"\"\"\n        # Custom local parameters timed array\n        void set_schedule(vector[int])\n        vector[int] get_schedule()\n        void set_buffer(vector[vector[%(float_prec)s]])\n        vector[vector[%(float_prec)s]] get_buffer()\n        void set_period(int)\n        int get_period()\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['wrapper_access_additional'] = \"\"\"\n    # Custom local parameters timed array\n    cpdef set_schedule( self, schedule ):\n        pop%(id)s.set_schedule( schedule )\n    cpdef np.ndarray get_schedule( self ):\n        return np.array(pop%(id)s.get_schedule( ))\n\n    cpdef set_rates( self, buffer ):\n        pop%(id)s.set_buffer( buffer )\n    cpdef np.ndarray get_rates( self ):\n        return np.array(pop%(id)s.get_buffer( ))\n\n    cpdef set_period( self, period ):\n        pop%(id)s.set_period(period)\n    cpdef int get_periodic(self):\n        return pop%(id)s.get_period()\n\"\"\" % { 'id': self.id, 'float_prec': Global.config['precision'] }\n\n        self._specific_template['update_variables'] = \"\"\"\n        if(_active) {\n            // std::cout &lt;&lt; _t &lt;&lt; \" \" &lt;&lt; _block&lt;&lt; \" \" &lt;&lt; _schedule[_block] &lt;&lt; std::endl;\n            // Check if it is time to set the input\n            if(_t == _schedule[_block]){\n                // Set the data\n                gpu_r = gpu_buffer[_block];\n                // Move to the next block\n                _block++;\n                // If was the last block, go back to the first block\n                if ( _block == _schedule.size() ) {\n                    _block = 0;\n                }\n            }\n\n            // If the timedarray is periodic, check if we arrive at that point\n            if( (_period &gt; -1) &amp;&amp; (_t == _period-1) ) {\n                // Reset the counters\n                _block=0;\n                _t = -1;\n            }\n\n            // Always increment the internal time\n            _t++;\n        }\n\"\"\"\n\n        self._specific_template['update_variable_body'] = \"\"\n        self._specific_template['update_variable_header'] = \"\"\n        self._specific_template['update_variable_call'] = \"\"\"\n    // host side update of neurons\n    pop%(id)s.update();\n\"\"\" % {'id': self.id}\n\n        self._specific_template['size_in_bytes'] = \"//TODO: \"\n\n    def _instantiate(self, module):\n        # Create the Cython instance\n        self.cyInstance = getattr(module, self.class_name+'_wrapper')(self.size, self.max_delay)\n\n    def __setattr__(self, name, value):\n        if name == 'schedule':\n            if self.initialized:\n                self.cyInstance.set_schedule( np.array(value) / Global.config['dt'] )\n            else:\n                self.init['schedule'] = value\n        elif name == 'rates':\n            if self.initialized:\n                if len(value.shape) &gt; 2:\n                    # we need to flatten the provided data\n                    flat_values = value.reshape( (value.shape[0], self.size) )\n                    self.cyInstance.set_rates( flat_values )\n                else:\n                    self.cyInstance.set_rates( value )\n            else:\n                self.init['rates'] = value\n        elif name == \"period\":\n            if self.initialized:\n                self.cyInstance.set_period(int(value /Global.config['dt']))\n            else:\n                self.init['period'] = value\n        else:\n            Population.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        if name == 'schedule':\n            if self.initialized:\n                return Global.config['dt'] * self.cyInstance.get_schedule()\n            else:\n                return self.init['schedule']\n        elif name == 'rates':\n            if self.initialized:\n                if len(self.geometry) &gt; 1:\n                    # unflatten the data\n                    flat_values = self.cyInstance.get_rates()\n                    values = np.zeros( tuple( [len(self.schedule)] + list(self.geometry) ) )\n                    for x in range(len(self.schedule)):\n                        values[x] = np.reshape( flat_values[x], self.geometry)\n                    return values\n                else:\n                    return self.cyInstance.get_rates()\n            else:\n                return self.init['rates']\n        elif name == 'period':\n            if self.initialized:\n                return self.cyInstance.get_period() * Global.config['dt']\n            else:\n                return self.init['period']\n        else:\n            return Population.__getattribute__(self, name)\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.TimedArray.__init__","title":"<code>__init__(rates, schedule=0.0, period=-1.0, name=None, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> rates         \u2013          <p>array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population.</p> </li> <li> schedule         \u2013          <p>either a single value or a list of time points where inputs should be set. Default: every timestep.</p> </li> <li> period         \u2013          <p>time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.).</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificPopulation.py</code> <pre><code>def __init__(self, rates, schedule=0., period= -1., name=None, copied=False):\n\"\"\"\n    :param rates: array of firing rates. The first axis corresponds to time, the others to the desired dimensions of the population.\n    :param schedule: either a single value or a list of time points where inputs should be set. Default: every timestep.\n    :param period: time when the timed array will be reset and start again, allowing cycling over the inputs. Default: no cycling (-1.).\n    \"\"\"\n    neuron = Neuron(\n        parameters=\"\",\n        equations=\" r = 0.0\",\n        name=\"Timed Array\",\n        description=\"Timed array source.\"\n    )\n    # Geometry of the population\n    geometry = rates.shape[1:]\n\n    # Check the schedule\n    if isinstance(schedule, (int, float)):\n        if float(schedule) &lt;= 0.0:\n            schedule = Global.config['dt']\n        schedule = [ float(schedule*i) for i in range(rates.shape[0])]\n\n    if len(schedule) &gt; rates.shape[0]:\n        Global._error('TimedArray: the length of the schedule parameter cannot exceed the first dimension of the rates parameter.')\n\n    if len(schedule) &lt; rates.shape[0]:\n        Global._warning('TimedArray: the length of the schedule parameter is smaller than the first dimension of the rates parameter (more data than time points). Make sure it is what you expect.')\n\n    SpecificPopulation.__init__(self, geometry=geometry, neuron=neuron, name=name, copied=copied)\n\n    self.init['schedule'] = schedule\n    self.init['rates'] = rates\n    self.init['period'] = period\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.HomogeneousCorrelatedSpikeTrains","title":"<code>ANNarchy.core.SpecificPopulation.HomogeneousCorrelatedSpikeTrains</code>","text":"<p>             Bases: <code>SpecificPopulation</code></p> <p>Population of spiking neurons following a homogeneous distribution with correlated spike trains.</p> <p>The method describing the generation of homogeneous correlated spike trains is described in:</p> <p>Brette, R. (2009). Generation of correlated spike trains. Neural Computation 21(1). http://romainbrette.fr/WordPress3/wp-content/uploads/2014/06/Brette2008NC.pdf</p> <p>The implementation is based on the one provided by Brian http://briansimulator.org.</p> <p>To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation:</p> \\[\\frac{dx}{dt} = \\frac{(\\mu - x)}{\\tau} + \\sigma \\, \\frac{\\xi}{\\sqrt{\\tau}}\\] <p>where \\(\\xi\\) is a random variable. In short, \\(x\\) will randomly vary around mu over time, with an amplitude determined by sigma and a speed determined by tau.</p> <p>This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process.</p> <p>To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates, the desired correlation strength corr and the time constant tau. See Brette's paper for details.</p> <p>In short, you should only define the parameters <code>rates</code>, <code>corr</code> and <code>tau</code>, and let the class compute mu and sigma for you. Changing <code>rates</code>, <code>corr</code> or <code>tau</code> after initialization automatically recomputes mu and sigma.</p> <p>Example:</p> <pre><code>from ANNarchy import *\nsetup(dt=0.1)\n\npop_corr = HomogeneousCorrelatedSpikeTrains(200, rates=10., corr=0.3, tau=10.)\n\ncompile()\n\nsimulate(1000.)\n\npop_corr.rates=30.\n\nsimulate(1000.)\n</code></pre> <p>Alternatively, a schedule can be provided to change automatically the value of <code>rates</code> and <code>corr</code> (but not <code>tau</code>) at the required times (as in TimedArray or TimedPoissonPopulation):</p> <pre><code>from ANNarchy import *\nsetup(dt=0.1)\n\npop_corr = HomogeneousCorrelatedSpikeTrains(\n    geometry=200, \n    rates= [10., 30.], \n    corr=[0.3, 0.5], \n    tau=10.,\n    schedule=[0., 1000.]\n)\n\ncompile()\n\nsimulate(2000.)\n</code></pre> <p>Even when using a schedule, <code>corr</code> accepts a single constant value. The first value of <code>schedule</code> must be 0. <code>period</code> specifies when the schedule \"loops\" back to its initial value.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificPopulation.py</code> <pre><code>class HomogeneousCorrelatedSpikeTrains(SpecificPopulation):\n\"\"\"\n    Population of spiking neurons following a homogeneous distribution with correlated spike trains.\n\n    The method describing the generation of homogeneous correlated spike trains is described in:\n\n    &gt; Brette, R. (2009). Generation of correlated spike trains. Neural Computation 21(1). &lt;http://romainbrette.fr/WordPress3/wp-content/uploads/2014/06/Brette2008NC.pdf&gt;\n\n    The implementation is based on the one provided by Brian &lt;http://briansimulator.org&gt;.\n\n    To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation:\n\n    $$\\\\frac{dx}{dt} = \\\\frac{(\\\\mu - x)}{\\\\tau} + \\\\sigma \\\\, \\\\frac{\\\\xi}{\\\\sqrt{\\\\tau}}$$\n\n    where $\\\\xi$ is a random variable. In short, $x$ will randomly vary around mu over time, with an amplitude determined by sigma and a speed determined by tau.\n\n    This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process.\n\n    To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate **rates**, the desired correlation strength **corr** and the time constant **tau**. See Brette's paper for details.\n\n    In short, you should only define the parameters ``rates``, ``corr`` and ``tau``, and let the class compute mu and sigma for you. Changing ``rates``, ``corr`` or ``tau`` after initialization automatically recomputes mu and sigma.\n\n    Example:\n\n    ```python\n    from ANNarchy import *\n    setup(dt=0.1)\n\n    pop_corr = HomogeneousCorrelatedSpikeTrains(200, rates=10., corr=0.3, tau=10.)\n\n    compile()\n\n    simulate(1000.)\n\n    pop_corr.rates=30.\n\n    simulate(1000.)\n    ```\n\n    Alternatively, a schedule can be provided to change automatically the value of `rates` and ``corr`` (but not ``tau``) at the required times (as in TimedArray or TimedPoissonPopulation):\n\n    ```python\n    from ANNarchy import *\n    setup(dt=0.1)\n\n    pop_corr = HomogeneousCorrelatedSpikeTrains(\n        geometry=200, \n        rates= [10., 30.], \n        corr=[0.3, 0.5], \n        tau=10.,\n        schedule=[0., 1000.]\n    )\n\n    compile()\n\n    simulate(2000.)\n    ```\n\n    Even when using a schedule, ``corr`` accepts a single constant value. The first value of ``schedule`` must be 0. ``period`` specifies when the schedule \"loops\" back to its initial value. \n\n    \"\"\"\n    def __init__(self, \n        geometry, \n        rates, \n        corr, \n        tau, \n        schedule=None, \n        period=-1., \n        name=None, \n        refractory=None, \n        copied=False):\n\"\"\"    \n        :param geometry: population geometry as tuple.\n        :param rates: rate in Hz of the population (must be a positive float or a list)\n        :param corr: total correlation strength (float in [0, 1], or a list)\n        :param tau: correlation time constant in ms.\n        :param schedule: list of times where new values of ``rates``and ``corr``will be used to computre mu and sigma.\n        :param period: time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.)\n        :param name: unique name of the population (optional).\n        :param refractory: refractory period in ms (careful: may break the correlation)\n        \"\"\"\n        if schedule is not None:\n            self._has_schedule = True\n            # Rates\n            if not isinstance(rates, (list, np.ndarray)):\n                Global._error(\"TimedHomogeneousCorrelatedSpikeTrains: the rates argument must be a list or a numpy array.\")\n            rates = np.array(rates)\n\n            # Schedule\n            schedule = np.array(schedule)\n\n            nb_schedules = rates.shape[0]\n            if nb_schedules != schedule.size:\n                Global._error(\"TimedHomogeneousCorrelatedSpikeTrains: the length of rates must be the same length as for schedule.\")\n\n            # corr\n            corr = np.array(corr)\n            if corr.size == 1:\n                corr = np.full(nb_schedules, corr)\n        else:\n            self._has_schedule = False\n            rates = np.array([float(rates)])\n            schedule = np.array([0.0])\n            corr = np.array([corr])\n\n\n        # Store refractory\n        self.refractory_init = refractory\n\n        # Correction of mu and sigma\n        mu_list, sigma_list = self._correction(rates, corr, tau)\n\n        self.rates = rates\n        self.corr = corr\n        self.tau = tau\n\n        # Create the neuron\n        corr_neuron = Neuron(\n            parameters = \"\"\"\n                tau = %(tau)s : population\n                mu = %(mu)s : population\n                sigma = %(sigma)s : population\n            \"\"\" % {'tau': tau, 'mu': mu_list[0], 'sigma': sigma_list[0]},\n            equations = \"\"\"\n                x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) : population, init=%(mu)s\n                p = Uniform(0.0, 1.0) * 1000.0 / dt\n            \"\"\" % {'mu': mu_list[0]},\n            spike = \"p &lt; x\",\n            refractory=refractory,\n            name=\"HomogeneousCorrelated\",\n            description=\"Homogeneous correlated spike trains.\"\n        )\n\n        SpecificPopulation.__init__(self, geometry=geometry, neuron=corr_neuron, name=name, copied=copied)\n\n        # Initial values\n        self.init['schedule'] = schedule\n        self.init['rates'] = rates\n        self.init['corr'] = corr\n        self.init['tau'] = tau\n        self.init['period'] = period\n\n\n        if self._has_schedule:\n            self.init['mu'] = mu_list\n            self.init['sigma'] = sigma_list\n        else:\n            self.init['mu'] = mu_list[0]\n            self.init['sigma'] = sigma_list[0]\n\n    def _copy(self):\n        \"Returns a copy of the population when creating networks.\"\n        return HomogeneousCorrelatedSpikeTrains(\n            geometry=self.geometry, \n            rates=self.init['rates'], \n            corr=self.init['corr'], \n            tau=self.init['tau'], \n            schedule=self.init['schedule'], \n            period=self.init['period'], \n            name=self.name, \n            refractory=self.refractory_init, \n            copied=True)\n\n    def _correction(self, rates, corr, tau):\n\n        # Correction of mu and sigma\n        mu_list = []\n        sigma_list = []\n\n        for i in range(len(rates)):\n            if isinstance(corr, list):\n                c = corr[i]\n            else:\n                c = float(corr)\n            mu, sigma = _rectify(rates[i], c, tau)\n            mu_list.append(mu)\n            sigma_list.append(sigma)\n\n        return mu_list, sigma_list\n\n    def _generate_st(self):\n\"\"\"\n        adjust code templates for the specific population for single thread.\n        \"\"\"\n        self._specific_template['declare_additional'] = \"\"\"\n    // Custom local parameters of a HomogeneousCorrelatedSpikeTrains\n    std::vector&lt; int &gt; _schedule; // List of times where new inputs should be set\n\n    std::vector&lt; %(float_prec)s &gt; _mu; // buffer holding the data\n    std::vector&lt; %(float_prec)s &gt; _sigma; // buffer holding the data\n\n    int _period; // Period of cycling\n    long int _t; // Internal time\n    int _block; // Internal block when inputs are set not at each step\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['access_additional'] = \"\"\"\n    // Custom local parameters of a HomogeneousCorrelatedSpikeTrains\n    void set_schedule(std::vector&lt;int&gt; schedule) { _schedule = schedule; }\n    std::vector&lt;int&gt; get_schedule() { return _schedule; }\n\n    void set_mu_list(std::vector&lt; %(float_prec)s &gt; buffer) { _mu = buffer; mu = _mu[0]; }\n    std::vector&lt; %(float_prec)s &gt; get_mu_list() { return _mu; }\n\n    void set_sigma_list(std::vector&lt; %(float_prec)s &gt; buffer) { _sigma = buffer; sigma = _sigma[0]; }\n    std::vector&lt; %(float_prec)s &gt; get_sigma_list() { return _sigma; }\n\n    void set_period(int period) { _period = period; }\n    int get_period() { return _period; }\n\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['init_additional'] = \"\"\"\n        // Initialize counters\n        _t = 0;\n        _block = 0;\n        _period = -1;\n\"\"\"\n        self._specific_template['export_additional'] = \"\"\"\n        # Custom local parameters of a HomogeneousCorrelatedSpikeTrains\n        void set_schedule(vector[int])\n        vector[int] get_schedule()\n\n        void set_mu_list(vector[%(float_prec)s])\n        vector[%(float_prec)s] get_mu_list()\n\n        void set_sigma_list(vector[%(float_prec)s])\n        vector[%(float_prec)s] get_sigma_list()\n\n        void set_period(int)\n        int get_period()\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['reset_additional'] =\"\"\"\n        _t = 0;\n        _block = 0;\n\n        r.clear();\n        r = std::vector&lt;%(float_prec)s&gt;(size, 0.0);\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['wrapper_access_additional'] = \"\"\"\n    # Custom local parameters of a HomogeneousCorrelatedSpikeTrains\n    cpdef set_schedule( self, schedule ):\n        pop%(id)s.set_schedule( schedule )\n    cpdef np.ndarray get_schedule( self ):\n        return np.array(pop%(id)s.get_schedule( ))\n\n    cpdef set_mu_list( self, buffer ):\n        pop%(id)s.set_mu_list( buffer )\n    cpdef np.ndarray get_mu_list( self ):\n        return np.array(pop%(id)s.get_mu_list( ))\n\n    cpdef set_sigma_list( self, buffer ):\n        pop%(id)s.set_sigma_list( buffer )\n    cpdef np.ndarray get_sigma_list( self ):\n        return np.array(pop%(id)s.get_sigma_list( ))\n\n    cpdef set_period( self, period ):\n        pop%(id)s.set_period(period)\n    cpdef int get_period(self):\n        return pop%(id)s.get_period()\n\"\"\" % { 'id': self.id }\n\n        scheduling_block = \"\"\"\n        if(_active){\n            // Check if it is time to set the input\n            if(_t == _schedule[_block]){\n                // Set the data\n                mu = _mu[_block];\n                sigma = _sigma[_block];\n                // Move to the next block\n                _block++;\n                // If was the last block, go back to the first block\n                if (_block == _schedule.size()){\n                    _block = 0;\n                }\n            }\n\n            // If the timedarray is periodic, check if we arrive at that point\n            if(_period &gt; -1 &amp;&amp; (_t == _period-1)){\n                // Reset the counters\n                _block=0;\n                _t = -1;\n            }\n\n            // Always increment the internal time\n            _t++;\n        }\n        \"\"\"\n\n        update_block = \"\"\"\n        if( _active ) {\n            spiked.clear();\n\n            // x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.)\n            x += dt*(mu - x)/tau + rand_0*sigma*sqrt(dt/tau);\n\n%(float_prec)s _step = 1000.0/dt;\n\n            #pragma omp simd\n            for(int i = 0; i &lt; size; i++){\n\n                // p = Uniform(0.0, 1.0) * 1000.0 / dt\n                p[i] = _step*rand_1[i];\n\n            }\n        } // active\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        if self._has_schedule:\n            self._specific_template['update_variables'] = scheduling_block + update_block\n        else:\n            self._specific_template['update_variables'] = update_block\n\n        self._specific_template['size_in_bytes'] = \"\"\"\n        // schedule\n        size_in_bytes += _schedule.capacity() * sizeof(int);\n\"\"\" % {'float_prec': Global.config['precision']}\n\n    def _generate_omp(self):\n\"\"\"\n        adjust code templates for the specific population for openMP.\n        \"\"\"\n        self._specific_template['declare_additional'] = \"\"\"\n    // Custom local parameters of a HomogeneousCorrelatedSpikeTrains\n    std::vector&lt; int &gt; _schedule; // List of times where new inputs should be set\n\n    std::vector&lt; %(float_prec)s &gt; _mu; // buffer holding the data\n    std::vector&lt; %(float_prec)s &gt; _sigma; // buffer holding the data\n\n    int _period; // Period of cycling\n    long int _t; // Internal time\n    int _block; // Internal block when inputs are set not at each step\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['access_additional'] = \"\"\"\n    // Custom local parameters of a HomogeneousCorrelatedSpikeTrains\n    void set_schedule(std::vector&lt;int&gt; schedule) { _schedule = schedule; }\n    std::vector&lt;int&gt; get_schedule() { return _schedule; }\n\n    void set_mu_list(std::vector&lt; %(float_prec)s &gt; buffer) { _mu = buffer; mu = _mu[0]; }\n    std::vector&lt; %(float_prec)s &gt; get_mu_list() { return _mu; }\n\n    void set_sigma_list(std::vector&lt; %(float_prec)s &gt; buffer) { _sigma = buffer; sigma = _sigma[0]; }\n    std::vector&lt; %(float_prec)s &gt; get_sigma_list() { return _sigma; }\n\n    void set_period(int period) { _period = period; }\n    int get_period() { return _period; }\n\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['init_additional'] = \"\"\"\n        // Initialize counters\n        _t = 0;\n        _block = 0;\n        _period = -1;\n\"\"\"\n        self._specific_template['export_additional'] = \"\"\"\n        # Custom local parameters of a HomogeneousCorrelatedSpikeTrains\n        void set_schedule(vector[int])\n        vector[int] get_schedule()\n\n        void set_mu_list(vector[%(float_prec)s])\n        vector[%(float_prec)s] get_mu_list()\n\n        void set_sigma_list(vector[%(float_prec)s])\n        vector[%(float_prec)s] get_sigma_list()\n\n        void set_period(int)\n        int get_period()\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['reset_additional'] =\"\"\"\n        _t = 0;\n        _block = 0;\n\n        r.clear();\n        r = std::vector&lt;%(float_prec)s&gt;(size, 0.0);\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['wrapper_access_additional'] = \"\"\"\n    # Custom local parameters of a HomogeneousCorrelatedSpikeTrains\n    cpdef set_schedule( self, schedule ):\n        pop%(id)s.set_schedule( schedule )\n    cpdef np.ndarray get_schedule( self ):\n        return np.array(pop%(id)s.get_schedule( ))\n\n    cpdef set_mu_list( self, buffer ):\n        pop%(id)s.set_mu_list( buffer )\n    cpdef np.ndarray get_mu_list( self ):\n        return np.array(pop%(id)s.get_mu_list( ))\n\n    cpdef set_sigma_list( self, buffer ):\n        pop%(id)s.set_sigma_list( buffer )\n    cpdef np.ndarray get_sigma_list( self ):\n        return np.array(pop%(id)s.get_sigma_list( ))\n\n    cpdef set_period( self, period ):\n        pop%(id)s.set_period(period)\n    cpdef int get_period(self):\n        return pop%(id)s.get_period()\n\"\"\" % { 'id': self.id }\n\n        scheduling_block = \"\"\"\n        if(_active){\n            // Check if it is time to set the input\n            if(_t == _schedule[_block]){\n                // Set the data\n                mu = _mu[_block];\n                sigma = _sigma[_block];\n                // Move to the next block\n                _block++;\n                // If was the last block, go back to the first block\n                if (_block == _schedule.size()){\n                    _block = 0;\n                }\n            }\n\n            // If the timedarray is periodic, check if we arrive at that point\n            if(_period &gt; -1 &amp;&amp; (_t == _period-1)){\n                // Reset the counters\n                _block=0;\n                _t = -1;\n            }\n\n            // Always increment the internal time\n            _t++;\n        }\n        \"\"\"\n\n        update_block = \"\"\"\n        if( _active ) {\n            #pragma omp single\n            {\n                spiked.clear();\n\n                // x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.)\n                x += dt*(mu - x)/tau + rand_0*sigma*sqrt(dt/tau);\n\n%(float_prec)s _step = 1000.0/dt;\n\n                #pragma omp simd\n                for(int i = 0; i &lt; size; i++){\n\n                    // p = Uniform(0.0, 1.0) * 1000.0 / dt\n                    p[i] = _step*rand_1[i];\n\n                }\n            }\n        } // active\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        if self._has_schedule:\n            self._specific_template['update_variables'] = scheduling_block + update_block\n        else:\n            self._specific_template['update_variables'] = update_block\n\n        self._specific_template['size_in_bytes'] = \"\"\"\n        // schedule\n        size_in_bytes += _schedule.capacity() * sizeof(int);\n\"\"\" % {'float_prec': Global.config['precision']}\n\n    def _generate_cuda(self):\n\"\"\"\n        Code generation if the CUDA paradigm is set.\n        \"\"\"\n        #\n        # Code for handling the buffer and schedule parameters\n        self._specific_template['declare_additional'] = \"\"\"\n    // Custom local parameter HomogeneousCorrelatedSpikeTrains\n    std::vector&lt; int &gt; _schedule;\n\n    std::vector&lt;%(float_prec)s&gt; mu_buffer;      // buffer\n    std::vector&lt;%(float_prec)s&gt; sigma_buffer;   // buffer\n\n    int _period; // Period of cycling\n    long int _t; // Internal time\n    int _block; // Internal block when inputs are set not at each step\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['access_additional'] = \"\"\"\n    // Custom local parameter HomogeneousCorrelatedSpikeTrains\n    void set_schedule(std::vector&lt;int&gt; schedule) { _schedule = schedule; }\n    std::vector&lt;int&gt; get_schedule() { return _schedule; }\n\n    void set_mu_list(std::vector&lt; %(float_prec)s &gt; buffer) { mu_buffer = buffer; }\n    void set_sigma_list(std::vector&lt; %(float_prec)s &gt; buffer) { sigma_buffer = buffer; }\n    std::vector&lt; %(float_prec)s &gt; get_mu_list() { return mu_buffer; }\n    std::vector&lt; %(float_prec)s &gt; get_sigma_list() { return sigma_buffer; }\n\n    void set_period(int period) { _period = period; }\n    int get_period() { return _period; }\n\"\"\" % {'float_prec': Global.config['precision'], 'id': self.id}\n        self._specific_template['init_additional'] = \"\"\"\n        // counters\n        _t = 0;\n        _block = 0;\n        _period = -1;\n\"\"\"\n        self._specific_template['reset_additional'] = \"\"\"\n        // counters\n        _t = 0;\n        _block = 0;\n\"\"\"\n        self._specific_template['export_additional'] = \"\"\"\n        # Custom local parameters timed array\n        void set_schedule(vector[int])\n        vector[int] get_schedule()\n        void set_mu_list(vector[%(float_prec)s])\n        vector[%(float_prec)s] get_mu_list()\n        void set_sigma_list(vector[%(float_prec)s])\n        vector[%(float_prec)s] get_sigma_list()\n        void set_period(int)\n        int get_period()\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['wrapper_access_additional'] = \"\"\"\n    # Custom local parameters timed array\n    cpdef set_schedule( self, schedule ):\n        pop%(id)s.set_schedule( schedule )\n    cpdef np.ndarray get_schedule( self ):\n        return np.array(pop%(id)s.get_schedule( ))\n    cpdef set_mu_list( self, buffer ):\n        pop%(id)s.set_mu_list( buffer )\n    cpdef np.ndarray get_mu_list( self ):\n        return np.array(pop%(id)s.get_mu_list( ))\n    cpdef set_sigma_list( self, buffer ):\n        pop%(id)s.set_sigma_list( buffer )\n    cpdef np.ndarray get_sigma_list( self ):\n        return np.array(pop%(id)s.get_sigma_list( ))\n    cpdef set_period( self, period ):\n        pop%(id)s.set_period(period)\n    cpdef int get_periodic(self):\n        return pop%(id)s.get_period()\n\"\"\" % { 'id': self.id, 'float_prec': Global.config['precision'] }\n\n        if not self._has_schedule:\n            # we can use the normal code generation for GPU kernels\n            pass\n\n        else:\n            self._specific_template['update_variables'] = \"\"\"\n        if(_active) {\n            // Check if it is time to set the input\n            if(_t == _schedule[_block]){\n                // Set the data\n                mu = mu_buffer[_block];\n                sigma = sigma_buffer[_block];\n                // Move to the next block\n                _block++;\n                // If was the last block, go back to the first block\n                if ( _block == _schedule.size() ) {\n                    _block = 0;\n                }\n            }\n\n            // If the timedarray is periodic, check if we arrive at that point\n            if( (_period &gt; -1) &amp;&amp; (_t == _period-1) ) {\n                // Reset the counters\n                _block=0;\n                _t = -1;\n            }\n\n            // Always increment the internal time\n            _t++;\n        }\n\"\"\"\n\n            self._specific_template['update_variable_body'] = \"\"\"\n// Updating global variables of population %(id)s\n__global__ void cuPop%(id)s_global_step( const long int t, const double dt, const double tau, double mu, double* x, curandState* rand_0, double sigma )\n{\n    // x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.)\n    x[0] += dt*(mu - x[0])/tau + curand_normal_double( &amp;rand_0[0] )*sigma*sqrt(dt/tau);\n}\n\n// Updating local variables of population %(id)s\n__global__ void cuPop%(id)s_local_step( const long int t, const double dt, curandState* rand_1, double* x, unsigned int* num_events, int* spiked, long int* last_spike )\n{\n    int i = threadIdx.x + blockDim.x * blockIdx.x;\n\n%(float_prec)s step = 1000.0/dt;\n\n    while ( i &lt; %(size)s )\n    {\n        // p = Uniform(0.0, 1.0) * 1000.0 / dt\n%(float_prec)s p = curand_uniform_double( &amp;rand_1[i] ) * step;\n\n        if (p &lt; x[0]) {\n            int pos = atomicAdd ( num_events, 1);\n            spiked[pos] = i;\n            last_spike[i] = t;\n        }\n\n        i += blockDim.x;\n    }\n\n    __syncthreads();\n}\n\"\"\" % {\n    'id': self.id,\n    'size': self.size,\n    'float_prec': Global.config['precision']\n}\n\n            self._specific_template['update_variable_header'] = \"\"\"__global__ void cuPop%(id)s_global_step( const long int t, const double dt, const double tau, double mu, double* x, curandState* rand_0, double sigma );\n__global__ void cuPop%(id)s_local_step( const long int t, const double dt, curandState* rand_1, double* x, unsigned int* num_events, int* spiked, long int* last_spike );\n\"\"\" % {'id': self.id}\n\n            # Please notice, that the GPU kernels can be launched only with one block. Otherwise, the\n            # atomicAdd which is called inside the kernel is not working correct (HD: April 1st, 2021)\n            self._specific_template['update_variable_call'] = \"\"\"\n    if (pop%(id)s._active) {\n        // Update the scheduling\n        pop%(id)s.update();\n\n        // Reset old events\n        clear_num_events&lt;&lt;&lt; 1, 1, 0, pop%(id)s.stream &gt;&gt;&gt;(pop%(id)s.gpu_spike_count);\n    #ifdef _DEBUG\n        cudaError_t err_clear_num_events_%(id)s = cudaGetLastError();\n        if (err_clear_num_events_%(id)s != cudaSuccess)\n            std::cout &lt;&lt; \"pop%(id)s_spike_gather: \" &lt;&lt; cudaGetErrorString(err_clear_num_events_%(id)s) &lt;&lt; std::endl;\n    #endif\n\n        // compute the value of x based on mu/sigma\n        cuPop%(id)s_global_step&lt;&lt;&lt; 1, 1, 0, pop%(id)s.stream &gt;&gt;&gt;(\n            t, dt,\n            pop%(id)s.tau,\n            pop%(id)s.mu,\n            pop%(id)s.gpu_x,\n            pop%(id)s.gpu_rand_0,\n            pop%(id)s.sigma \n        );\n        #ifdef _DEBUG\n            cudaError_t err_pop%(id)s_global_step = cudaGetLastError();\n            if( err_pop%(id)s_global_step != cudaSuccess) {\n                std::cout &lt;&lt; \"pop%(id)s_step: \" &lt;&lt; cudaGetErrorString(err_pop%(id)s_global_step) &lt;&lt; std::endl;\n                exit(0);\n            }\n        #endif\n\n        // Generate new spike events\n        cuPop%(id)s_local_step&lt;&lt;&lt; 1, pop%(id)s._threads_per_block, 0, pop%(id)s.stream &gt;&gt;&gt;(\n            t, dt,\n            pop%(id)s.gpu_rand_1,\n            pop%(id)s.gpu_x,\n            pop%(id)s.gpu_spike_count,\n            pop%(id)s.gpu_spiked,\n            pop%(id)s.gpu_last_spike\n        );\n    #ifdef _DEBUG\n        cudaError_t err_pop_spike_gather_%(id)s = cudaGetLastError();\n        if(err_pop_spike_gather_%(id)s != cudaSuccess) {\n            std::cout &lt;&lt; \"pop%(id)s_spike_gather: \" &lt;&lt; cudaGetErrorString(err_pop_spike_gather_%(id)s) &lt;&lt; std::endl;\n            exit(0);\n        }\n    #endif\n\n        // transfer back the spike counter (needed by record)\n        cudaMemcpy( &amp;pop%(id)s.spike_count, pop%(id)s.gpu_spike_count, sizeof(unsigned int), cudaMemcpyDeviceToHost);\n    #ifdef _DEBUG\n        cudaError_t err_pop%(id)s_async_copy = cudaGetLastError();\n        if ( err_pop%(id)s_async_copy != cudaSuccess ) {\n            std::cout &lt;&lt; \"record_spike_count: \" &lt;&lt; cudaGetErrorString(err_pop%(id)s_async_copy) &lt;&lt; std::endl;\n            exit(0);\n        }\n    #endif\n\n        // transfer back the spiked array (needed by record)\n        if (pop%(id)s.spike_count &gt; 0) {\n            cudaMemcpy( pop%(id)s.spiked.data(), pop%(id)s.gpu_spiked, pop%(id)s.spike_count*sizeof(int), cudaMemcpyDeviceToHost);\n        #ifdef _DEBUG\n            cudaError_t err_pop%(id)s_async_copy2 = cudaGetLastError();\n            if ( err_pop%(id)s_async_copy2 != cudaSuccess ) {\n                std::cout &lt;&lt; \"record_spike: \" &lt;&lt; cudaGetErrorString(err_pop%(id)s_async_copy2) &lt;&lt; std::endl;\n                exit(0);\n            }\n        #endif\n        }\n    }\n\"\"\" % {'id': self.id}\n\n        self._specific_template['size_in_bytes'] = \"//TODO: \"\n\n    def _instantiate(self, module):\n        # Create the Cython instance\n        self.cyInstance = getattr(module, self.class_name+'_wrapper')(self.size, self.max_delay)\n\n    def __setattr__(self, name, value):\n\n        if not hasattr(self, 'initialized'):\n            Population.__setattr__(self, name, value)\n        elif name == 'schedule':\n            if self.initialized:\n                self.cyInstance.set_schedule( np.array(value) / Global.config['dt'] )\n            else:\n                self.init['schedule'] = value\n        elif name == 'mu':\n            if self.initialized:\n                if self._has_schedule:\n                    self.cyInstance.set_mu_list( value )\n                else:\n                    self.cyInstance.set_global_attribute( \"mu\", value, Global.config[\"precision\"] )\n            else:\n                self.init['mu'] = value\n        elif name == 'sigma':\n            if self.initialized:\n                if self._has_schedule:\n                    self.cyInstance.set_sigma_list( value )\n                else:\n                    self.cyInstance.set_global_attribute( \"sigma\", value, Global.config[\"precision\"] )\n            else:\n                self.init['sigma'] = value\n        elif name == \"period\":\n            if self.initialized:\n                self.cyInstance.set_period(int(value /Global.config['dt']))\n            else:\n                self.init['period'] = value\n        elif name == 'rates': \n            if self._has_schedule:\n                value = np.array(value)\n                if not value.size == self.schedule.size:\n                    Global._error(\"HomogeneousCorrelatedSpikeTrains: rates must have the same length as schedule.\")\n            else:\n                value = np.array([float(value)])\n            if self.initialized:\n                Population.__setattr__(self, name, value)\n                # Correction of mu and sigma everytime r, c or tau is changed\n                try:\n                    mu, sigma = self._correction(self.rates, self.corr, self.tau)\n                    if self._has_schedule:\n                        self.mu = mu\n                        self.sigma = sigma\n                    else:\n                        self.mu = mu[0]\n                        self.sigma = sigma[0]\n                except Exception as e:\n                    print(e)\n            else:\n                self.init[name] = value\n                Population.__setattr__(self, name, value)\n        elif name == 'corr': \n            if self._has_schedule:\n                if not isinstance(value, (list, np.ndarray)):\n                    value = np.full((self.schedule.size, ), value)\n                else:\n                    value = np.array(value)\n                    if not value.size == self.schedule.size:\n                        Global._error(\"HomogeneousCorrelatedSpikeTrains: corr must have the same length as schedule.\")\n            else:\n                value = np.array([float(value)])\n            if self.initialized:\n                Population.__setattr__(self, name, value)\n                try:\n                    # Correction of mu and sigma everytime r, c or tau is changed\n                    mu, sigma = self._correction(self.rates, self.corr, self.tau)\n                    if self._has_schedule:\n                        self.mu = mu\n                        self.sigma = sigma\n                    else:\n                        self.mu = mu[0]\n                        self.sigma = sigma[0]\n                except Exception as e:\n                    print(e)\n            else:\n                self.init[name] = value\n                Population.__setattr__(self, name, value)\n        elif name == 'tau': \n            if self.initialized:\n                Population.__setattr__(self, name, value)\n                # Correction of mu and sigma everytime r, c or tau is changed\n                mu, sigma = self._correction(self.rates, self.corr, self.tau)\n                if self._has_schedule:\n                    self.mu = mu\n                    self.sigma = sigma\n                else:\n                    self.mu = mu[0]\n                    self.sigma = sigma[0]\n            else:\n                self.init[name] = value\n                Population.__setattr__(self, name, value)\n        else:\n            Population.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        if name == 'schedule':\n            if self.initialized:\n                if self._has_schedule:\n                    return Global.config['dt'] * self.cyInstance.get_schedule()\n                else:\n                    return np.array([0.0])\n            else:\n                return self.init['schedule']\n        elif name == 'mu':\n            if self.initialized:\n                if self._has_schedule:\n                    return self.cyInstance.get_mu_list()\n                else:\n                    return self.cyInstance.get_global_attribute( \"mu\", Global.config[\"precision\"] )\n            else:\n                return self.init['mu']\n        elif name == 'sigma':\n            if self.initialized:\n                if self._has_schedule:\n                    return self.cyInstance.get_sigma_list()\n                else:\n                    return self.cyInstance.get_global_attribute( \"sigma\", Global.config[\"precision\"] )\n            else:\n                return self.init['sigma']\n        elif name == 'tau':\n            if self.initialized:\n                return self.cyInstance.get_global_attribute( \"tau\", Global.config[\"precision\"] )\n            else:\n                return self.init['tau']\n        elif name == 'period':\n            if self.initialized:\n                return self.cyInstance.get_period() * Global.config['dt']\n            else:\n                return self.init['period']\n        else:\n            return Population.__getattribute__(self, name)\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.HomogeneousCorrelatedSpikeTrains.__init__","title":"<code>__init__(geometry, rates, corr, tau, schedule=None, period=-1.0, name=None, refractory=None, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> geometry         \u2013          <p>population geometry as tuple.</p> </li> <li> rates         \u2013          <p>rate in Hz of the population (must be a positive float or a list)</p> </li> <li> corr         \u2013          <p>total correlation strength (float in [0, 1], or a list)</p> </li> <li> tau         \u2013          <p>correlation time constant in ms.</p> </li> <li> schedule         \u2013          <p>list of times where new values of <code>rates</code>and <code>corr</code>will be used to computre mu and sigma.</p> </li> <li> period         \u2013          <p>time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.)</p> </li> <li> name         \u2013          <p>unique name of the population (optional).</p> </li> <li> refractory         \u2013          <p>refractory period in ms (careful: may break the correlation)</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificPopulation.py</code> <pre><code>def __init__(self, \n    geometry, \n    rates, \n    corr, \n    tau, \n    schedule=None, \n    period=-1., \n    name=None, \n    refractory=None, \n    copied=False):\n\"\"\"    \n    :param geometry: population geometry as tuple.\n    :param rates: rate in Hz of the population (must be a positive float or a list)\n    :param corr: total correlation strength (float in [0, 1], or a list)\n    :param tau: correlation time constant in ms.\n    :param schedule: list of times where new values of ``rates``and ``corr``will be used to computre mu and sigma.\n    :param period: time when the array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.)\n    :param name: unique name of the population (optional).\n    :param refractory: refractory period in ms (careful: may break the correlation)\n    \"\"\"\n    if schedule is not None:\n        self._has_schedule = True\n        # Rates\n        if not isinstance(rates, (list, np.ndarray)):\n            Global._error(\"TimedHomogeneousCorrelatedSpikeTrains: the rates argument must be a list or a numpy array.\")\n        rates = np.array(rates)\n\n        # Schedule\n        schedule = np.array(schedule)\n\n        nb_schedules = rates.shape[0]\n        if nb_schedules != schedule.size:\n            Global._error(\"TimedHomogeneousCorrelatedSpikeTrains: the length of rates must be the same length as for schedule.\")\n\n        # corr\n        corr = np.array(corr)\n        if corr.size == 1:\n            corr = np.full(nb_schedules, corr)\n    else:\n        self._has_schedule = False\n        rates = np.array([float(rates)])\n        schedule = np.array([0.0])\n        corr = np.array([corr])\n\n\n    # Store refractory\n    self.refractory_init = refractory\n\n    # Correction of mu and sigma\n    mu_list, sigma_list = self._correction(rates, corr, tau)\n\n    self.rates = rates\n    self.corr = corr\n    self.tau = tau\n\n    # Create the neuron\n    corr_neuron = Neuron(\n        parameters = \"\"\"\n            tau = %(tau)s : population\n            mu = %(mu)s : population\n            sigma = %(sigma)s : population\n        \"\"\" % {'tau': tau, 'mu': mu_list[0], 'sigma': sigma_list[0]},\n        equations = \"\"\"\n            x += dt*(mu - x)/tau + sqrt(dt/tau) * sigma * Normal(0., 1.) : population, init=%(mu)s\n            p = Uniform(0.0, 1.0) * 1000.0 / dt\n        \"\"\" % {'mu': mu_list[0]},\n        spike = \"p &lt; x\",\n        refractory=refractory,\n        name=\"HomogeneousCorrelated\",\n        description=\"Homogeneous correlated spike trains.\"\n    )\n\n    SpecificPopulation.__init__(self, geometry=geometry, neuron=corr_neuron, name=name, copied=copied)\n\n    # Initial values\n    self.init['schedule'] = schedule\n    self.init['rates'] = rates\n    self.init['corr'] = corr\n    self.init['tau'] = tau\n    self.init['period'] = period\n\n\n    if self._has_schedule:\n        self.init['mu'] = mu_list\n        self.init['sigma'] = sigma_list\n    else:\n        self.init['mu'] = mu_list[0]\n        self.init['sigma'] = sigma_list[0]\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.TimedPoissonPopulation","title":"<code>ANNarchy.core.SpecificPopulation.TimedPoissonPopulation</code>","text":"<p>             Bases: <code>SpecificPopulation</code></p> <p>Poisson population whose rate vary with the provided schedule.</p> <p>Example:</p> <pre><code>inp = TimedPoissonPopulation(\n    geometry = 100,\n    rates = [10., 20., 100., 20., 5.],\n    schedule = [0., 100., 200., 500., 600.],\n)\n</code></pre> <p>This creates a population of 100 Poisson neurons whose rate will be:</p> <ul> <li>10 Hz during the first 100 ms.</li> <li>20 HZ during the next 100 ms.</li> <li>100 Hz during the next 300 ms.</li> <li>20 Hz during the next 100 ms.</li> <li>5 Hz until the end of the simulation.</li> </ul> <p>If you want the TimedPoissonPopulation to \"loop\" over the schedule, you can specify a period:</p> <pre><code>inp = TimedPoissonPopulation(\n    geometry = 100,\n    rates = [10., 20., 100., 20., 5.],\n    schedule = [0., 100., 200., 500., 600.],\n    period = 1000.,\n)\n</code></pre> <p>Here the rate will become 10Hz again every 1 second of simulation. If the period is smaller than the schedule, the remaining rates will not be set.</p> <p>Note that you can use the <code>reset()</code> method to manually reinitialize the schedule, times becoming relative to that call:</p> <pre><code>simulate(1200.) # Should switch to 100 Hz due to the period of 1000.\ninp.reset()\nsimulate(1000.) # Starts at 10 Hz again.\n</code></pre> <p>The rates were here global to the population. If you want each neuron to have a different rate, <code>rates</code> must have additional dimensions corresponding to the geometry of the population.</p> <pre><code>inp = TimedPoissonPopulation(\n    geometry = 100,\n    rates = [ \n        [10. + 0.05*i for i in range(100)], \n        [20. + 0.05*i for i in range(100)],\n    ],\n    schedule = [0., 100.],\n    period = 1000.,\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificPopulation.py</code> <pre><code>class TimedPoissonPopulation(SpecificPopulation):\n\"\"\"\n    Poisson population whose rate vary with the provided schedule.\n\n    Example:\n\n    ```python\n    inp = TimedPoissonPopulation(\n        geometry = 100,\n        rates = [10., 20., 100., 20., 5.],\n        schedule = [0., 100., 200., 500., 600.],\n    )\n    ```\n\n\n    This creates a population of 100 Poisson neurons whose rate will be:\n\n    * 10 Hz during the first 100 ms.\n    * 20 HZ during the next 100 ms.\n    * 100 Hz during the next 300 ms.\n    * 20 Hz during the next 100 ms.\n    * 5 Hz until the end of the simulation.\n\n\n    If you want the TimedPoissonPopulation to \"loop\" over the schedule, you can specify a period:\n\n    ```python\n    inp = TimedPoissonPopulation(\n        geometry = 100,\n        rates = [10., 20., 100., 20., 5.],\n        schedule = [0., 100., 200., 500., 600.],\n        period = 1000.,\n    )\n    ```\n\n    Here the rate will become 10Hz again every 1 second of simulation. If the period is smaller than the schedule, the remaining rates will not be set.\n\n    Note that you can use the ``reset()`` method to manually reinitialize the schedule, times becoming relative to that call:\n\n    ```python\n    simulate(1200.) # Should switch to 100 Hz due to the period of 1000.\n    inp.reset()\n    simulate(1000.) # Starts at 10 Hz again.\n    ```\n\n    The rates were here global to the population. If you want each neuron to have a different rate, ``rates`` must have additional dimensions corresponding to the geometry of the population.\n\n    ```python\n    inp = TimedPoissonPopulation(\n        geometry = 100,\n        rates = [ \n            [10. + 0.05*i for i in range(100)], \n            [20. + 0.05*i for i in range(100)],\n        ],\n        schedule = [0., 100.],\n        period = 1000.,\n    )\n    ```\n\n    \"\"\"\n    def __init__(self, geometry, rates, schedule, period= -1., name=None, copied=False):\n\"\"\"    \n        :param rates: array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match the geometry of the population.\n        :param schedule: list of times (in ms) where the firing rate should change.\n        :param period: time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.).\n        \"\"\"\n\n        neuron = Neuron(\n            parameters = \"\"\"\n            proba = 1.0\n            \"\"\",\n            equations = \"\"\"\n            p = Uniform(0.0, 1.0) * 1000.0 / dt\n            \"\"\",\n            spike = \"\"\"\n            p &lt; proba\n            \"\"\",\n            name=\"TimedPoisson\",\n            description=\"Spiking neuron following a Poisson distribution.\"\n        )\n\n        SpecificPopulation.__init__(self, geometry=geometry, neuron=neuron, name=name, copied=copied)\n\n        # Check arguments\n        try:\n            rates = np.array(rates)\n        except:\n            Global._error(\"TimedPoissonPopulation: the rates argument must be a numpy array.\")\n\n        schedule = np.array(schedule)\n\n        nb_schedules = rates.shape[0]\n        if nb_schedules != schedule.size:\n            Global._error(\"TimedPoissonPopulation: the first axis of the rates argument must be the same length as schedule.\")\n\n\n        if rates.ndim == 1 : # One rate for the whole population\n            rates = np.array([np.full(self.size, rates[i]) for i in range(nb_schedules)]) \n\n        # Initial values\n        self.init['schedule'] = schedule\n        self.init['rates'] = rates\n        self.init['period'] = period\n\n    def _copy(self):\n        \"Returns a copy of the population when creating networks.\"\n        return TimedPoissonPopulation(self.geometry, self.init['rates'] , self.init['schedule'], self.init['period'], self.name, copied=True)\n\n    def _generate_st(self):\n\"\"\"\n        adjust code templates for the specific population for single thread.\n        \"\"\"\n        self._specific_template['declare_additional'] = \"\"\"\n    // Custom local parameters of a TimedPoissonPopulation\n    std::vector&lt; int &gt; _schedule; // List of times where new inputs should be set\n    std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; _buffer; // buffer holding the data\n    int _period; // Period of cycling\n    long int _t; // Internal time\n    int _block; // Internal block when inputs are set not at each step\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['access_additional'] = \"\"\"\n    // Custom local parameters of a TimedPoissonPopulation\n    void set_schedule(std::vector&lt;int&gt; schedule) { _schedule = schedule; }\n    std::vector&lt;int&gt; get_schedule() { return _schedule; }\n    void set_buffer(std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; buffer) { _buffer = buffer; r = _buffer[0]; }\n    std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; get_buffer() { return _buffer; }\n    void set_period(int period) { _period = period; }\n    int get_period() { return _period; }\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['init_additional'] = \"\"\"\n        // Initialize counters\n        _t = 0;\n        _block = 0;\n        _period = -1;\n\"\"\"\n        self._specific_template['export_additional'] = \"\"\"\n        # Custom local parameters of a TimedPoissonPopulation\n        void set_schedule(vector[int])\n        vector[int] get_schedule()\n        void set_buffer(vector[vector[%(float_prec)s]])\n        vector[vector[%(float_prec)s]] get_buffer()\n        void set_period(int)\n        int get_period()\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['reset_additional'] =\"\"\"\n        _t = 0;\n        _block = 0;\n\n        r.clear();\n        r = std::vector&lt;%(float_prec)s&gt;(size, 0.0);\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['wrapper_access_additional'] = \"\"\"\n    # Custom local parameters of a TimedArray\n    cpdef set_schedule( self, schedule ):\n        pop%(id)s.set_schedule( schedule )\n    cpdef np.ndarray get_schedule( self ):\n        return np.array(pop%(id)s.get_schedule( ))\n\n    cpdef set_rates( self, buffer ):\n        pop%(id)s.set_buffer( buffer )\n    cpdef np.ndarray get_rates( self ):\n        return np.array(pop%(id)s.get_buffer( ))\n\n    cpdef set_period( self, period ):\n        pop%(id)s.set_period(period)\n    cpdef int get_period(self):\n        return pop%(id)s.get_period()\n\"\"\" % { 'id': self.id }\n\n        self._specific_template['update_variables'] = \"\"\"\n        if(_active){\n            //std::cout &lt;&lt; _t &lt;&lt; \" \" &lt;&lt; _block&lt;&lt; \" \" &lt;&lt; _schedule[_block] &lt;&lt; std::endl;\n\n            // Check if it is time to set the input\n            if(_t == _schedule[_block]){\n                // Set the data\n                proba = _buffer[_block];\n                // Move to the next block\n                _block++;\n                // If was the last block, go back to the first block\n                if (_block == _schedule.size()){\n                    _block = 0;\n                }\n            }\n\n            // If the timedarray is periodic, check if we arrive at that point\n            if(_period &gt; -1 &amp;&amp; (_t == _period-1)){\n                // Reset the counters\n                _block=0;\n                _t = -1;\n            }\n\n            // Always increment the internal time\n            _t++;\n        }\n\n        if( _active ) {\n            spiked.clear();\n\n            // Updating local variables\n%(float_prec)s step = 1000.0/dt;\n\n            #pragma omp simd\n            for(int i = 0; i &lt; size; i++){\n\n                // p = Uniform(0.0, 1.0) * 1000.0 / dt\n                p[i] = step*rand_0[i];\n\n\n            }\n        } // active\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['size_in_bytes'] = \"\"\"\n        // schedule\n        size_in_bytes += _schedule.capacity() * sizeof(int);\n\n        // buffer\n        size_in_bytes += _buffer.capacity() * sizeof(std::vector&lt;%(float_prec)s&gt;);\n        for( auto it = _buffer.begin(); it != _buffer.end(); it++ )\n            size_in_bytes += it-&gt;capacity() * sizeof(%(float_prec)s);\n\"\"\" % {'float_prec': Global.config['precision']}\n\n    def _generate_omp(self):\n\"\"\"\n        adjust code templates for the specific population for openMP.\n        \"\"\"\n        self._specific_template['declare_additional'] = \"\"\"\n    // Custom local parameters of a TimedPoissonPopulation\n    std::vector&lt; int &gt; _schedule; // List of times where new inputs should be set\n    std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; _buffer; // buffer holding the data\n    int _period; // Period of cycling\n    long int _t; // Internal time\n    int _block; // Internal block when inputs are set not at each step\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['access_additional'] = \"\"\"\n    // Custom local parameters of a TimedPoissonPopulation\n    void set_schedule(std::vector&lt;int&gt; schedule) { _schedule = schedule; }\n    std::vector&lt;int&gt; get_schedule() { return _schedule; }\n    void set_buffer(std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; buffer) { _buffer = buffer; r = _buffer[0]; }\n    std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; get_buffer() { return _buffer; }\n    void set_period(int period) { _period = period; }\n    int get_period() { return _period; }\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['init_additional'] = \"\"\"\n        // Initialize counters\n        _t = 0;\n        _block = 0;\n        _period = -1;\n\"\"\"\n        self._specific_template['export_additional'] = \"\"\"\n        # Custom local parameters of a TimedPoissonPopulation\n        void set_schedule(vector[int])\n        vector[int] get_schedule()\n        void set_buffer(vector[vector[%(float_prec)s]])\n        vector[vector[%(float_prec)s]] get_buffer()\n        void set_period(int)\n        int get_period()\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['reset_additional'] =\"\"\"\n        _t = 0;\n        _block = 0;\n\n        r.clear();\n        r = std::vector&lt;%(float_prec)s&gt;(size, 0.0);\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['wrapper_access_additional'] = \"\"\"\n    # Custom local parameters of a TimedArray\n    cpdef set_schedule( self, schedule ):\n        pop%(id)s.set_schedule( schedule )\n    cpdef np.ndarray get_schedule( self ):\n        return np.array(pop%(id)s.get_schedule( ))\n\n    cpdef set_rates( self, buffer ):\n        pop%(id)s.set_buffer( buffer )\n    cpdef np.ndarray get_rates( self ):\n        return np.array(pop%(id)s.get_buffer( ))\n\n    cpdef set_period( self, period ):\n        pop%(id)s.set_period(period)\n    cpdef int get_period(self):\n        return pop%(id)s.get_period()\n\"\"\" % { 'id': self.id }\n\n        self._specific_template['update_variables'] = \"\"\"\n        if(_active){\n            #pragma omp single\n            {\n                //std::cout &lt;&lt; _t &lt;&lt; \" \" &lt;&lt; _block&lt;&lt; \" \" &lt;&lt; _schedule[_block] &lt;&lt; std::endl;\n\n                // Check if it is time to set the input\n                if(_t == _schedule[_block]){\n                    // Set the data\n                    proba = _buffer[_block];\n                    // Move to the next block\n                    _block++;\n                    // If was the last block, go back to the first block\n                    if (_block == _schedule.size()){\n                        _block = 0;\n                    }\n                }\n\n                // If the timedarray is periodic, check if we arrive at that point\n                if(_period &gt; -1 &amp;&amp; (_t == _period-1)){\n                    // Reset the counters\n                    _block=0;\n                    _t = -1;\n                }\n\n                // Always increment the internal time\n                _t++;\n            }\n        }\n\n        if( _active ) {\n            spiked.clear();\n\n            // Updating local variables\n%(float_prec)s step = 1000.0/dt;\n\n            #pragma omp for simd\n            for(int i = 0; i &lt; size; i++){\n\n                // p = Uniform(0.0, 1.0) * 1000.0 / dt\n                p[i] = step*rand_0[i];\n\n\n            }\n        } // active\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['size_in_bytes'] = \"\"\"\n        // schedule\n        size_in_bytes += _schedule.capacity() * sizeof(int);\n\n        // buffer\n        size_in_bytes += _buffer.capacity() * sizeof(std::vector&lt;%(float_prec)s&gt;);\n        for( auto it = _buffer.begin(); it != _buffer.end(); it++ )\n            size_in_bytes += it-&gt;capacity() * sizeof(%(float_prec)s);\n\"\"\" % {'float_prec': Global.config['precision']}\n\n    def _generate_cuda(self):\n\"\"\"\n        Code generation if the CUDA paradigm is set.\n        \"\"\"\n        # I suppress the code generation for allocating the variable r on gpu, as\n        # well as memory transfer codes. This is only possible as no other variables\n        # allowed in TimedArray.\n        self._specific_template['init_parameters_variables'] = \"\"\"\n        // Random numbers\n        cudaMalloc((void**)&amp;gpu_rand_0, size * sizeof(curandState));\n        init_curand_states( size, gpu_rand_0, global_seed );\n\"\"\"\n        self._specific_template['host_device_transfer'] = \"\"\n        self._specific_template['device_host_transfer'] = \"\"\n\n        #\n        # Code for handling the buffer and schedule parameters\n        self._specific_template['declare_additional'] = \"\"\"\n    // Custom local parameter timed array\n    std::vector&lt; int &gt; _schedule;\n    std::vector&lt; %(float_prec)s* &gt; gpu_buffer;\n    int _period; // Period of cycling\n    long int _t; // Internal time\n    int _block; // Internal block when inputs are set not at each step\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['access_additional'] = \"\"\"\n    // Custom local parameter timed array\n    void set_schedule(std::vector&lt;int&gt; schedule) { _schedule = schedule; }\n    std::vector&lt;int&gt; get_schedule() { return _schedule; }\n    void set_buffer(std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; buffer) {\n        if ( gpu_buffer.empty() ) {\n            gpu_buffer = std::vector&lt; %(float_prec)s* &gt;(buffer.size(), nullptr);\n            // allocate gpu arrays\n            for(int i = 0; i &lt; buffer.size(); i++) {\n                cudaMalloc((void**)&amp;gpu_buffer[i], buffer[i].size()*sizeof(%(float_prec)s));\n            }\n        }\n\n        auto host_it = buffer.begin();\n        auto dev_it = gpu_buffer.begin();\n        for( ; host_it &lt; buffer.end(); host_it++, dev_it++ ) {\n            cudaMemcpy( *dev_it, host_it-&gt;data(), host_it-&gt;size()*sizeof(%(float_prec)s), cudaMemcpyHostToDevice);\n        }\n\n        gpu_proba = gpu_buffer[0];\n    }\n    std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; get_buffer() {\n        std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt; buffer = std::vector&lt; std::vector&lt; %(float_prec)s &gt; &gt;( gpu_buffer.size(), std::vector&lt;%(float_prec)s&gt;(size,0.0) );\n\n        auto host_it = buffer.begin();\n        auto dev_it = gpu_buffer.begin();\n        for( ; host_it &lt; buffer.end(); host_it++, dev_it++ ) {\n            cudaMemcpy( host_it-&gt;data(), *dev_it, size*sizeof(%(float_prec)s), cudaMemcpyDeviceToHost );\n        }\n\n        return buffer;\n    }\n    void set_period(int period) { _period = period; }\n    int get_period() { return _period; }\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['init_additional'] = \"\"\"\n        // counters\n        _t = 0;\n        _block = 0;\n        _period = -1;\n\"\"\"\n        self._specific_template['reset_additional'] = \"\"\"\n        // counters\n        _t = 0;\n        _block = 0;\n        gpu_proba = gpu_buffer[0];\n\"\"\"\n        self._specific_template['export_additional'] = \"\"\"\n        # Custom local parameters timed array\n        void set_schedule(vector[int])\n        vector[int] get_schedule()\n        void set_buffer(vector[vector[%(float_prec)s]])\n        vector[vector[%(float_prec)s]] get_buffer()\n        void set_period(int)\n        int get_period()\n\"\"\" % {'float_prec': Global.config['precision']}\n        self._specific_template['wrapper_access_additional'] = \"\"\"\n    # Custom local parameters timed array\n    cpdef set_schedule( self, schedule ):\n        pop%(id)s.set_schedule( schedule )\n    cpdef np.ndarray get_schedule( self ):\n        return np.array(pop%(id)s.get_schedule( ))\n\n    cpdef set_rates( self, buffer ):\n        pop%(id)s.set_buffer( buffer )\n    cpdef np.ndarray get_rates( self ):\n        return np.array(pop%(id)s.get_buffer( ))\n\n    cpdef set_period( self, period ):\n        pop%(id)s.set_period(period)\n    cpdef int get_periodic(self):\n        return pop%(id)s.get_period()\n\"\"\" % { 'id': self.id, 'float_prec': Global.config['precision'] }\n\n        self._specific_template['update_variables'] = \"\"\"\n        if(_active) {\n            // std::cout &lt;&lt; _t &lt;&lt; \" \" &lt;&lt; _block&lt;&lt; \" \" &lt;&lt; _schedule[_block] &lt;&lt; std::endl;\n            // Check if it is time to set the input\n            if(_t == _schedule[_block]){\n                // Set the data\n                gpu_proba = gpu_buffer[_block];\n                // Move to the next block\n                _block++;\n                // If was the last block, go back to the first block\n                if ( _block == _schedule.size() ) {\n                    _block = 0;\n                }\n            }\n\n            // If the timedarray is periodic, check if we arrive at that point\n            if( (_period &gt; -1) &amp;&amp; (_t == _period-1) ) {\n                // Reset the counters\n                _block=0;\n                _t = -1;\n            }\n\n            // Always increment the internal time\n            _t++;\n        }\n\"\"\"\n\n        self._specific_template['update_variable_body'] = \"\"\"\n__global__ void cuPop%(id)s_local_step( const long int t, const double dt, curandState* rand_0, double* proba, unsigned int* num_events, int* spiked, long int* last_spike )\n{\n    int i = threadIdx.x + blockDim.x * blockIdx.x;\n%(float_prec)s step = 1000.0/dt;\n\n    while ( i &lt; %(size)s )\n    {\n        // p = Uniform(0.0, 1.0) * 1000.0 / dt\n%(float_prec)s p = curand_uniform_double( &amp;rand_0[i] ) * step;\n\n        if (p &lt; proba[i]) {\n            int pos = atomicAdd ( num_events, 1);\n            spiked[pos] = i;\n            last_spike[i] = t;\n        }\n\n        i += blockDim.x;\n    }\n\n    __syncthreads();\n}\n\"\"\" % {\n    'id': self.id,\n    'size': self.size,\n    'float_prec': Global.config['precision']\n}\n\n        self._specific_template['update_variable_header'] = \"__global__ void cuPop%(id)s_local_step( const long int t, const double dt, curandState* rand_0, double* proba, unsigned int* num_events, int* spiked, long int* last_spike );\" % {'id': self.id}\n        # Please notice, that the GPU kernels can be launched only with one block. Otherwise, the\n        # atomicAdd which is called inside the kernel is not working correct (HD: April 1st, 2021)\n        self._specific_template['update_variable_call'] = \"\"\"\n    // host side update of neurons\n    pop%(id)s.update();\n\n    // Reset old events\n    clear_num_events&lt;&lt;&lt; 1, 1, 0, pop%(id)s.stream &gt;&gt;&gt;(pop%(id)s.gpu_spike_count);\n#ifdef _DEBUG\n    cudaError_t err_clear_num_events_%(id)s = cudaGetLastError();\n    if(err_clear_num_events_%(id)s != cudaSuccess)\n        std::cout &lt;&lt; \"pop%(id)s_spike_gather: \" &lt;&lt; cudaGetErrorString(err_clear_num_events_%(id)s) &lt;&lt; std::endl;\n#endif\n\n    // Compute current events\n    cuPop%(id)s_local_step&lt;&lt;&lt; 1, pop%(id)s._threads_per_block, 0, pop%(id)s.stream &gt;&gt;&gt;(\n        t, dt,\n        pop%(id)s.gpu_rand_0,\n        pop%(id)s.gpu_proba,\n        pop%(id)s.gpu_spike_count,\n        pop%(id)s.gpu_spiked,\n        pop%(id)s.gpu_last_spike\n    );\n#ifdef _DEBUG\n    cudaError_t err_pop_spike_gather_%(id)s = cudaGetLastError();\n    if(err_pop_spike_gather_%(id)s != cudaSuccess)\n        std::cout &lt;&lt; \"pop%(id)s_spike_gather: \" &lt;&lt; cudaGetErrorString(err_pop_spike_gather_%(id)s) &lt;&lt; std::endl;\n#endif\n\n    // transfer back the spike counter (needed by record)\n    cudaMemcpyAsync( &amp;pop%(id)s.spike_count, pop%(id)s.gpu_spike_count, sizeof(unsigned int), cudaMemcpyDeviceToHost, pop%(id)s.stream );\n#ifdef _DEBUG\n    cudaError_t err = cudaGetLastError();\n    if ( err != cudaSuccess )\n        std::cout &lt;&lt; \"record_spike_count: \" &lt;&lt; cudaGetErrorString(err) &lt;&lt; std::endl;\n#endif\n\n    // transfer back the spiked array (needed by record)\n    cudaMemcpyAsync( pop%(id)s.spiked.data(), pop%(id)s.gpu_spiked, pop%(id)s.spike_count*sizeof(int), cudaMemcpyDeviceToHost, pop%(id)s.stream );\n#ifdef _DEBUG\n    err = cudaGetLastError();\n    if ( err != cudaSuccess )\n        std::cout &lt;&lt; \"record_spike: \" &lt;&lt; cudaGetErrorString(err) &lt;&lt; std::endl;\n#endif\n\n\"\"\" % {'id': self.id}\n\n        self._specific_template['size_in_bytes'] = \"//TODO: \"\n\n    def _instantiate(self, module):\n        # Create the Cython instance\n        self.cyInstance = getattr(module, self.class_name+'_wrapper')(self.size, self.max_delay)\n\n    def __setattr__(self, name, value):\n        if name == 'schedule':\n            if self.initialized:\n                self.cyInstance.set_schedule( np.array(value) / Global.config['dt'] )\n            else:\n                self.init['schedule'] = value\n        elif name == 'rates':\n            if self.initialized:\n                value = np.array(value)\n                if value.shape[0] != self.schedule.shape[0]:\n                    Global._error(\"TimedPoissonPopulation: the first dimension of rates must match the schedule.\")\n                if value.ndim &gt; 2:\n                    # we need to flatten the provided data\n                    values = value.reshape( (value.shape[0], self.size) )\n                    self.cyInstance.set_rates(values)\n                elif value.ndim == 2:\n                    if value.shape[1] != self.size:\n                        if value.shape[1] == 1:\n                            value = np.array([np.full(self.size, value[i]) for i in range(value.shape[0])])\n                        else:\n                            Global._error(\"TimedPoissonPopulation: the second dimension of rates must match the number of neurons.\")\n                    self.cyInstance.set_rates(value)\n                elif value.ndim == 1:\n                    value = np.array([np.full(self.size, value[i]) for i in range(value.shape[0])]) \n                    self.cyInstance.set_rates(value)\n\n            else:\n                self.init['rates'] = value\n        elif name == \"period\":\n            if self.initialized:\n                self.cyInstance.set_period(int(value /Global.config['dt']))\n            else:\n                self.init['period'] = value\n        else:\n            Population.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        if name == 'schedule':\n            if self.initialized:\n                return Global.config['dt'] * self.cyInstance.get_schedule()\n            else:\n                return self.init['schedule']\n        elif name == 'rates':\n            if self.initialized:\n                if len(self.geometry) &gt; 1:\n                    # unflatten the data\n                    flat_values = self.cyInstance.get_rates()\n                    values = np.zeros( tuple( [len(self.schedule)] + list(self.geometry) ) )\n                    for x in range(len(self.schedule)):\n                        values[x] = np.reshape( flat_values[x], self.geometry)\n                    return values\n                else:\n                    return self.cyInstance.get_rates()\n            else:\n                return self.init['rates']\n        elif name == 'period':\n            if self.initialized:\n                return self.cyInstance.get_period() * Global.config['dt']\n            else:\n                return self.init['period']\n        else:\n            return Population.__getattribute__(self, name)\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.core.SpecificPopulation.TimedPoissonPopulation.__init__","title":"<code>__init__(geometry, rates, schedule, period=-1.0, name=None, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> rates         \u2013          <p>array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match the geometry of the population.</p> </li> <li> schedule         \u2013          <p>list of times (in ms) where the firing rate should change.</p> </li> <li> period         \u2013          <p>time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.).</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificPopulation.py</code> <pre><code>def __init__(self, geometry, rates, schedule, period= -1., name=None, copied=False):\n\"\"\"    \n    :param rates: array of firing rates. The first axis corresponds to the times where the firing rate should change. If a different rate should be used by the different neurons, the other dimensions must match the geometry of the population.\n    :param schedule: list of times (in ms) where the firing rate should change.\n    :param period: time when the timed array will be reset and start again, allowing cycling over the schedule. Default: no cycling (-1.).\n    \"\"\"\n\n    neuron = Neuron(\n        parameters = \"\"\"\n        proba = 1.0\n        \"\"\",\n        equations = \"\"\"\n        p = Uniform(0.0, 1.0) * 1000.0 / dt\n        \"\"\",\n        spike = \"\"\"\n        p &lt; proba\n        \"\"\",\n        name=\"TimedPoisson\",\n        description=\"Spiking neuron following a Poisson distribution.\"\n    )\n\n    SpecificPopulation.__init__(self, geometry=geometry, neuron=neuron, name=name, copied=copied)\n\n    # Check arguments\n    try:\n        rates = np.array(rates)\n    except:\n        Global._error(\"TimedPoissonPopulation: the rates argument must be a numpy array.\")\n\n    schedule = np.array(schedule)\n\n    nb_schedules = rates.shape[0]\n    if nb_schedules != schedule.size:\n        Global._error(\"TimedPoissonPopulation: the first axis of the rates argument must be the same length as schedule.\")\n\n\n    if rates.ndim == 1 : # One rate for the whole population\n        rates = np.array([np.full(self.size, rates[i]) for i in range(nb_schedules)]) \n\n    # Initial values\n    self.init['schedule'] = schedule\n    self.init['rates'] = rates\n    self.init['period'] = period\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.ImagePopulation","title":"<code>ANNarchy.extensions.image.ImagePopulation.ImagePopulation</code>","text":"<p>             Bases: <code>Population</code></p> <p>Specific rate-coded Population allowing to represent images (png, jpg...) as the firing rate of a population (each neuron represents one pixel).</p> <p>This extension requires the Python Image Library (pip install Pillow).</p> <p>Usage:</p> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.image import ImagePopulation\n\npop = ImagePopulation(geometry=(480, 640))\npop.set_image('image.jpg')\n</code></pre> <p>About the geometry:</p> <ul> <li>If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image).</li> <li>If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color).</li> </ul> <p>If the third dimension is 3, each will correspond to the RGB values of the pixels.</p> <p>Warning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/image/ImagePopulation.py</code> <pre><code>class ImagePopulation(Population):\n\"\"\" \n    Specific rate-coded Population allowing to represent images (png, jpg...) as the firing rate of a population (each neuron represents one pixel).\n\n    This extension requires the Python Image Library (pip install Pillow).\n\n    Usage:\n\n    ```python\n    from ANNarchy import *\n    from ANNarchy.extensions.image import ImagePopulation\n\n    pop = ImagePopulation(geometry=(480, 640))\n    pop.set_image('image.jpg')\n    ```\n\n    About the geometry:\n\n    * If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image).\n    * If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color).\n\n    If the third dimension is 3, each will correspond to the RGB values of the pixels.\n\n    **Warning:** due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population.\n    \"\"\"\n\n    def __init__(self, geometry, name=None, copied=False):\n\"\"\"            \n        :param geometry: population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation.\n        :param name: unique name of the population (optional).     \n        \"\"\"   \n        # Check geometry\n        if isinstance(geometry, int) or len(geometry)==1:\n            Global._error('The geometry of an ImagePopulation should be 2D (grayscale) or 3D (color).')\n\n        if len(geometry)==3 and (geometry[2]!=3 and geometry[2]!=1):\n            Global._error('The third dimension of an ImagePopulation should be either 1 (grayscale) or 3 (color).') \n\n        if len(geometry)==3 and geometry[2]==1:\n            geometry = (int(geometry[0]), int(geometry[1]))\n\n        # Create the population     \n        Population.__init__(self, geometry = geometry, name=name, neuron = Neuron(parameters=\"r = 0.0\"), copied=copied)\n\n    def _copy(self):\n        \"Returns a copy of the population when creating networks. Internal use only.\"\n        return ImagePopulation(geometry=self.geometry, name=self.name, copied=True)\n\n    def set_image(self, image_name):\n\"\"\" \n        Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population.\n\n        If the image has a different size from the population, it will be resized.\n\n        \"\"\"\n        try:\n            im = Image.open(image_name)\n        except : # image does not exist\n            Global._error('The image ' + image_name + ' does not exist.')\n\n        # Resize the image if needed\n        (width, height) = (self.geometry[1], self.geometry[0])\n        if im.size != (width, height):\n            Global._warning('The image ' + image_name + ' does not have the same size '+str(im.size)+' as the population ' + str((width, height)) + '. It will be resized.')\n            im = im.resize((width, height))\n\n        # Check if only the luminance should be extracted\n        if self.dimension == 2 or self.geometry[2] == 1:\n            im=im.convert(\"L\")\n\n        # Set the rate of the population\n        self.r = np.array(im).reshape(self.size)/255.\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.ImagePopulation.__init__","title":"<code>__init__(geometry, name=None, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> geometry         \u2013          <p>population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation.</p> </li> <li> name         \u2013          <p>unique name of the population (optional).</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/image/ImagePopulation.py</code> <pre><code>def __init__(self, geometry, name=None, copied=False):\n\"\"\"            \n    :param geometry: population geometry as tuple. It must correspond to the image size and be fixed through the whole simulation.\n    :param name: unique name of the population (optional).     \n    \"\"\"   \n    # Check geometry\n    if isinstance(geometry, int) or len(geometry)==1:\n        Global._error('The geometry of an ImagePopulation should be 2D (grayscale) or 3D (color).')\n\n    if len(geometry)==3 and (geometry[2]!=3 and geometry[2]!=1):\n        Global._error('The third dimension of an ImagePopulation should be either 1 (grayscale) or 3 (color).') \n\n    if len(geometry)==3 and geometry[2]==1:\n        geometry = (int(geometry[0]), int(geometry[1]))\n\n    # Create the population     \n    Population.__init__(self, geometry = geometry, name=name, neuron = Neuron(parameters=\"r = 0.0\"), copied=copied)\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.ImagePopulation.set_image","title":"<code>set_image(image_name)</code>","text":"<p>Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population.</p> <p>If the image has a different size from the population, it will be resized.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/image/ImagePopulation.py</code> <pre><code>def set_image(self, image_name):\n\"\"\" \n    Sets an image (.png, .jpg or whatever is supported by PIL) into the firing rate of the population.\n\n    If the image has a different size from the population, it will be resized.\n\n    \"\"\"\n    try:\n        im = Image.open(image_name)\n    except : # image does not exist\n        Global._error('The image ' + image_name + ' does not exist.')\n\n    # Resize the image if needed\n    (width, height) = (self.geometry[1], self.geometry[0])\n    if im.size != (width, height):\n        Global._warning('The image ' + image_name + ' does not have the same size '+str(im.size)+' as the population ' + str((width, height)) + '. It will be resized.')\n        im = im.resize((width, height))\n\n    # Check if only the luminance should be extracted\n    if self.dimension == 2 or self.geometry[2] == 1:\n        im=im.convert(\"L\")\n\n    # Set the rate of the population\n    self.r = np.array(im).reshape(self.size)/255.\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.VideoPopulation","title":"<code>ANNarchy.extensions.image.ImagePopulation.VideoPopulation</code>","text":"<p>             Bases: <code>ImagePopulation</code></p> <p>Specific rate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel).</p> <p>This extension requires the C++ library OpenCV &gt;= 4.0 (apt-get/yum install opencv). <code>pkg-config opencv4 --cflags --libs</code> should not return an error. <code>vtk</code> might additionally have to be installed.</p> <p>Usage:</p> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.image import VideoPopulation\n\npop = VideoPopulation(geometry=(480, 640))\n\ncompile()\n\npop.start_camera(0)\n\nwhile(True):\n    pop.grab_image()\n    simulate(10.0)\n</code></pre> <p>About the geometry:</p> <ul> <li>If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image).</li> <li>If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color).</li> </ul> <p>If the third dimension is 3, each will correspond to the RGB values of the pixels.</p> <p>Warning: due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/image/ImagePopulation.py</code> <pre><code>class VideoPopulation(ImagePopulation):\n\"\"\" \n    Specific rate-coded Population allowing to feed a webcam input into the firing rate of a population (each neuron represents one pixel).\n\n    This extension requires the C++ library OpenCV &gt;= 4.0 (apt-get/yum install opencv). ``pkg-config opencv4 --cflags --libs`` should not return an error. `vtk` might additionally have to be installed.\n\n    Usage:\n\n    ```python\n    from ANNarchy import *\n    from ANNarchy.extensions.image import VideoPopulation\n\n    pop = VideoPopulation(geometry=(480, 640))\n\n    compile()\n\n    pop.start_camera(0)\n\n    while(True):\n        pop.grab_image()\n        simulate(10.0)\n    ```\n\n    About the geometry:\n\n    * If the geometry is 2D, it corresponds to the (height, width) of the image. Only the luminance of the pixels will be represented (grayscale image).\n    * If the geometry is 3D, the third dimension can be either 1 (grayscale) or 3 (color).\n\n    If the third dimension is 3, each will correspond to the RGB values of the pixels.\n\n    **Warning:** due to the indexing system of Numpy, a 640*480 image should be fed into a (480, 640) or (480, 640, 3) population.\n\n    \"\"\"\n\n    def __init__(self, geometry, opencv_version=\"4\", name=None, copied=False):\n\"\"\"        \n        :param geometry: population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized.\n        :param opencv_version: OpenCV version (default=4).\n        :param name: unique name of the population (optional).       \n        \"\"\"         \n        # Create the population     \n        ImagePopulation.__init__(self, geometry = geometry, name=name, copied=copied)\n\n        self.opencv_version = opencv_version\n\n    def _copy(self):\n        \"Returns a copy of the population when creating networks. Internal use only.\"\n        return VideoPopulation(geometry=self.geometry, name=self.name, copied=True)\n\n    def _generate(self):\n        \"Code generation\"      \n        # Add corresponding libs to the Makefile\n        extra_libs.append('`pkg-config opencv' + str(self.opencv_version) + ' --cflags --libs`')\n\n        # Include opencv\n        self._specific_template['include_additional'] = \"\"\"#include &lt;opencv2/opencv.hpp&gt;\nusing namespace cv;\n\"\"\"\n        # Class for the camera device\n        self._specific_template['struct_additional'] = \"\"\"\n// VideoPopulation\nclass CameraDeviceCPP : public cv::VideoCapture {\npublic:\n    CameraDeviceCPP (int id, int width, int height, int depth) : cv::VideoCapture(id){\n        width_ = width;\n        height_ = height;\n        depth_ = depth;      \n        img_ = std::vector&lt;%(float_prec)s&gt;(width*height*depth, 0.0);\n    }\n    std::vector&lt;%(float_prec)s&gt; GrabImage(){\n        if(isOpened()){\n            // Read a new frame from the video\n            Mat frame;\n            read(frame); \n            // Resize the image\n            Mat resized_frame;\n            resize(frame, resized_frame, Size(width_, height_) );\n            // If depth=1, only luminance\n            if(depth_==1){\n                // Convert to luminance\n                cvtColor(resized_frame, resized_frame, COLOR_BGR2GRAY);\n                for(int i = 0; i &lt; resized_frame.rows; i++){\n                    for(int j = 0; j &lt; resized_frame.cols; j++){\n                        this-&gt;img_[j+width_*i] = float(resized_frame.at&lt;uchar&gt;(i, j))/255.0;\n                    }\n                }\n            }\n            else{ //BGR\n                for(int i = 0; i &lt; resized_frame.rows; i++){\n                    for(int j = 0; j &lt; resized_frame.cols; j++){\n                        Vec3b intensity = resized_frame.at&lt;Vec3b&gt;(i, j);\n                        this-&gt;img_[(j+width_*i)*3 + 0] = %(float_prec)s(intensity.val[2])/255.0;\n                        this-&gt;img_[(j+width_*i)*3 + 1] = %(float_prec)s(intensity.val[1])/255.0;\n                        this-&gt;img_[(j+width_*i)*3 + 2] = %(float_prec)s(intensity.val[0])/255.0;\n                    }\n                }\n            }            \n        }\n        return this-&gt;img_;\n    };\n\nprotected:\n    // Width and height of the image, depth_ is 1 (grayscale) or 3 (RGB)\n    int width_, height_, depth_;\n    // Vector of floats for the returned image\n    std::vector&lt;%(float_prec)s&gt; img_;\n};\n\"\"\" % {'float_prec': Global.config['precision']}\n\n        self._specific_template['declare_additional'] = \"\"\"\n    // Camera\n    CameraDeviceCPP* camera_;\n    void StartCamera(int id, int width, int height, int depth){\n        camera_ = new CameraDeviceCPP(id, width, height, depth);\n        if(!camera_-&gt;isOpened()){\n            std::cout &lt;&lt; \"Error: could not open the camera.\" &lt;&lt; std::endl;   \n        }\n    };\n    void GrabImage(){\n        if(camera_-&gt;isOpened()){\n            r = camera_-&gt;GrabImage();   \n        }\n    };\n    void ReleaseCamera(){\n        camera_-&gt;release(); \n    };\n\"\"\"\n\n        self._specific_template['update_variables'] = \"\"\n\n        self._specific_template['export_additional'] = \"\"\"\n        void StartCamera(int id, int width, int height, int depth)\n        void GrabImage()\n        void ReleaseCamera()\n\"\"\" \n\n        self._specific_template['wrapper_access_additional'] = \"\"\"\n    # CameraDevice\n    def start_camera(self, int id, int width, int height, int depth):\n        pop%(id)s.StartCamera(id, width, height, depth)\n\n    def grab_image(self):\n        pop%(id)s.GrabImage()\n\n    def release_camera(self):\n        pop%(id)s.ReleaseCamera()\n\"\"\" % {'id': self.id}\n\n\n\n    def start_camera(self, camera_port=0):\n\"\"\"\n        Starts the webcam with the corresponding device (default = 0).\n\n        On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc.\n        \"\"\"\n\n        self.cyInstance.start_camera(camera_port, self.geometry[1], self.geometry[0], 3 if self.dimension==3 else 1)\n\n    def grab_image(self):\n\"\"\"\n        Grabs one image from the camera and feeds it into the population.\n\n        The camera must be first started with:\n\n            pop.start_camera(0)\n        \"\"\"\n        self.cyInstance.grab_image()\n\n    def release(self):\n\"\"\"\n        Releases the camera:\n\n            pop.release()\n        \"\"\"\n        self.cyInstance.release_camera()\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.VideoPopulation.__init__","title":"<code>__init__(geometry, opencv_version='4', name=None, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> geometry         \u2013          <p>population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized.</p> </li> <li> opencv_version         \u2013          <p>OpenCV version (default=4).</p> </li> <li> name         \u2013          <p>unique name of the population (optional).</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/image/ImagePopulation.py</code> <pre><code>def __init__(self, geometry, opencv_version=\"4\", name=None, copied=False):\n\"\"\"        \n    :param geometry: population geometry as tuple. It must be fixed through the whole simulation. If the camera provides images of a different size, it will be resized.\n    :param opencv_version: OpenCV version (default=4).\n    :param name: unique name of the population (optional).       \n    \"\"\"         \n    # Create the population     \n    ImagePopulation.__init__(self, geometry = geometry, name=name, copied=copied)\n\n    self.opencv_version = opencv_version\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.VideoPopulation.grab_image","title":"<code>grab_image()</code>","text":"<p>Grabs one image from the camera and feeds it into the population.</p> <p>The camera must be first started with:</p> <pre><code>pop.start_camera(0)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/image/ImagePopulation.py</code> <pre><code>def grab_image(self):\n\"\"\"\n    Grabs one image from the camera and feeds it into the population.\n\n    The camera must be first started with:\n\n        pop.start_camera(0)\n    \"\"\"\n    self.cyInstance.grab_image()\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.VideoPopulation.release","title":"<code>release()</code>","text":"<p>Releases the camera:</p> <pre><code>pop.release()\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/image/ImagePopulation.py</code> <pre><code>def release(self):\n\"\"\"\n    Releases the camera:\n\n        pop.release()\n    \"\"\"\n    self.cyInstance.release_camera()\n</code></pre>"},{"location":"API/SpecificPopulation.html#ANNarchy.extensions.image.ImagePopulation.VideoPopulation.start_camera","title":"<code>start_camera(camera_port=0)</code>","text":"<p>Starts the webcam with the corresponding device (default = 0).</p> <p>On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc.</p> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/extensions/image/ImagePopulation.py</code> <pre><code>def start_camera(self, camera_port=0):\n\"\"\"\n    Starts the webcam with the corresponding device (default = 0).\n\n    On linux, the camera port corresponds to the number in /dev/video0, /dev/video1, etc.\n    \"\"\"\n\n    self.cyInstance.start_camera(camera_port, self.geometry[1], self.geometry[0], 3 if self.dimension==3 else 1)\n</code></pre>"},{"location":"API/SpecificProjection.html","title":"Specific Projections","text":"<p>ANNarchy provides a set of predefined <code>Projection</code> objects to ease the definition of standard networks.</p>"},{"location":"API/SpecificProjection.html#ANNarchy.core.SpecificProjection.CurrentInjection","title":"<code>ANNarchy.core.SpecificProjection.CurrentInjection</code>","text":"<p>             Bases: <code>SpecificProjection</code></p> <p>Inject current from a rate-coded population into a spiking population.</p> <p>The pre-synaptic population must be be rate-coded, the post-synaptic one must be spiking, both must have the same size and no plasticity is allowed.</p> <p>For each post-synaptic neuron, the current <code>g_target</code> will be set at each time step to the firing rate <code>r</code> of the pre-synaptic neuron with the same rank.</p> <p>The projection must be connected with <code>connect_current()</code>, which takes no parameter and does not accept delays. It is equivalent to <code>connect_one_to_one(weights=1)</code>.</p> <p>Example:</p> <pre><code>inp = Population(100, Neuron(equations=\"r = sin(t)\"))\n\npop = Population(100, Izhikevich)\n\nproj = CurrentInjection(inp, pop, 'exc')\nproj.connect_current()\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificProjection.py</code> <pre><code>class CurrentInjection(SpecificProjection):\n\"\"\"\n    Inject current from a rate-coded population into a spiking population.\n\n    The pre-synaptic population must be be rate-coded, the post-synaptic one must be spiking, both must have the same size and no plasticity is allowed.\n\n    For each post-synaptic neuron, the current ``g_target`` will be set at each time step to the firing rate ``r`` of the pre-synaptic neuron with the same rank.\n\n    The projection must be connected with ``connect_current()``, which takes no parameter and does not accept delays. It is equivalent to ``connect_one_to_one(weights=1)``.\n\n    Example:\n\n    ```python\n    inp = Population(100, Neuron(equations=\"r = sin(t)\"))\n\n    pop = Population(100, Izhikevich)\n\n    proj = CurrentInjection(inp, pop, 'exc')\n    proj.connect_current()\n    ```\n\n    \"\"\"\n    def __init__(self, pre, post, target, name=None, copied=False):\n\"\"\"\n        :param pre: pre-synaptic population.\n        :param post: post-synaptic population.\n        :param target: type of the connection.\n        \"\"\"\n        # Instantiate the projection\n        SpecificProjection.__init__(self, pre, post, target, None, name, copied)\n\n        # Check populations\n        if not self.pre.neuron_type.type == 'rate':\n            Global._error('The pre-synaptic population of a CurrentInjection must be rate-coded.')\n\n        if not self.post.neuron_type.type == 'spike':\n            Global._error('The post-synaptic population of a CurrentInjection must be spiking.')\n\n        if not self.post.size == self.pre.size:\n            Global._error('CurrentInjection: The pre- and post-synaptic populations must have the same size.')\n\n        if Global._check_paradigm(\"cuda\") and (isinstance(pre, PopulationView) or isinstance(post, PopulationView)):\n            Global._error(\"CurrentInjection on GPUs is not allowed for PopulationViews\")\n\n        # Prevent automatic split of matrices\n        self._no_split_matrix = True\n\n    def _copy(self, pre, post):\n        \"Returns a copy of the population when creating networks. Internal use only.\"\n        return CurrentInjection(pre=pre, post=post, target=self.target, name=self.name, copied=True)\n\n    def _generate_st(self):\n        # Generate the code\n        self._specific_template['psp_code'] = \"\"\"\n        if (pop%(id_post)s._active) {\n            for (int i=0; i&lt;post_rank.size(); i++) {\n                pop%(id_post)s.g_%(target)s[post_rank[i]] += pop%(id_pre)s.r[pre_rank[i][0]];\n            }\n        } // active\n\"\"\" % { 'id_pre': self.pre.id, 'id_post': self.post.id, 'target': self.target}\n\n    def _generate_omp(self):\n        # Generate the code\n        self._specific_template['psp_code'] = \"\"\"\n        if (pop%(id_post)s._active) {\n            #pragma omp for\n            for (int i=0; i&lt;post_rank.size(); i++) {\n                pop%(id_post)s.g_%(target)s[post_rank[i]] += pop%(id_pre)s.r[pre_rank[i][0]];\n            }\n        } // active\n\"\"\" % { 'id_pre': self.pre.id, 'id_post': self.post.id, 'target': self.target}\n\n    def _generate_cuda(self):\n\"\"\"\n        Generate the CUDA code.\n\n        For a first implementation we take a rather simple approach:\n\n        * We use only one block for the kernel and each thread computes\n        one synapse/post-neuron entry (which is equal as we use one2one).\n\n        * We use only the pre-synaptic firing rate, no other variables.\n\n        * We ignore the synaptic weight.\n        \"\"\"\n        ids = {\n            'id_proj': self.id,\n            'id_post': self.post.id,\n            'id_pre': self.pre.id,\n            'target': self.target,\n            'float_prec': Global.config['precision']\n        }\n\n        self._specific_template['psp_body'] = \"\"\"\n__global__ void cu_proj%(id_proj)s_psp(int post_size, %(float_prec)s *pre_r, %(float_prec)s *g_%(target)s) {\n    int n = threadIdx.x;\n\n    while (n &lt; post_size) {\n        g_%(target)s[n] += pre_r[n];\n\n        n += blockDim.x;\n    }\n}\n\"\"\" % ids\n        self._specific_template['psp_header'] = \"\"\"__global__ void cu_proj%(id_proj)s_psp(int post_size, %(float_prec)s *pre_r, %(float_prec)s *g_%(target)s);\"\"\" % ids\n        self._specific_template['psp_call'] = \"\"\"\n    cu_proj%(id_proj)s_psp&lt;&lt;&lt; 1, 192 &gt;&gt;&gt;(\n        pop%(id_post)s.size,\n        pop%(id_pre)s.gpu_r,\n        pop%(id_post)s.gpu_g_%(target)s );\n\"\"\" % ids\n\n    def connect_current(self):\n        return self.connect_one_to_one(weights=1.0)\n</code></pre>"},{"location":"API/SpecificProjection.html#ANNarchy.core.SpecificProjection.CurrentInjection.__init__","title":"<code>__init__(pre, post, target, name=None, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> pre         \u2013          <p>pre-synaptic population.</p> </li> <li> post         \u2013          <p>post-synaptic population.</p> </li> <li> target         \u2013          <p>type of the connection.</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificProjection.py</code> <pre><code>def __init__(self, pre, post, target, name=None, copied=False):\n\"\"\"\n    :param pre: pre-synaptic population.\n    :param post: post-synaptic population.\n    :param target: type of the connection.\n    \"\"\"\n    # Instantiate the projection\n    SpecificProjection.__init__(self, pre, post, target, None, name, copied)\n\n    # Check populations\n    if not self.pre.neuron_type.type == 'rate':\n        Global._error('The pre-synaptic population of a CurrentInjection must be rate-coded.')\n\n    if not self.post.neuron_type.type == 'spike':\n        Global._error('The post-synaptic population of a CurrentInjection must be spiking.')\n\n    if not self.post.size == self.pre.size:\n        Global._error('CurrentInjection: The pre- and post-synaptic populations must have the same size.')\n\n    if Global._check_paradigm(\"cuda\") and (isinstance(pre, PopulationView) or isinstance(post, PopulationView)):\n        Global._error(\"CurrentInjection on GPUs is not allowed for PopulationViews\")\n\n    # Prevent automatic split of matrices\n    self._no_split_matrix = True\n</code></pre>"},{"location":"API/SpecificProjection.html#ANNarchy.core.SpecificProjection.DecodingProjection","title":"<code>ANNarchy.core.SpecificProjection.DecodingProjection</code>","text":"<p>             Bases: <code>SpecificProjection</code></p> <p>Decoding projection to transform spike trains into firing rates.</p> <p>The pre-synaptic population must be a spiking population, while the post-synaptic one must be rate-coded.</p> <p>Pre-synaptic spikes are accumulated for each post-synaptic neuron. A sliding window can be used to smoothen the results with the <code>window</code> parameter.</p> <p>The decoded firing rate is accessible in the post-synaptic neurons with <code>sum(target)</code>.</p> <p>The projection can be connected using any method available in <code>Projection</code> (although all-to-all or many-to-one makes mostly sense). Delays are ignored.</p> <p>The weight value allows to scale the firing rate: if you want a pre-synaptic firing rate of 100 Hz to correspond to a post-synaptic rate of 1.0, use <code>w = 1./100.</code>.</p> <p>Example:</p> <pre><code>pop1 = PoissonPopulation(1000, rates=100.)\npop2 = Population(1, Neuron(equations=\"r=sum(exc)\"))\nproj = DecodingProjection(pop1, pop2, 'exc', window=10.0)\nproj.connect_all_to_all(1.0, force_multiple_weights=True)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificProjection.py</code> <pre><code>class DecodingProjection(SpecificProjection):\n\"\"\"\n    Decoding projection to transform spike trains into firing rates.\n\n    The pre-synaptic population must be a spiking population, while the post-synaptic one must be rate-coded.\n\n    Pre-synaptic spikes are accumulated for each post-synaptic neuron. A sliding window can be used to smoothen the results with the ``window`` parameter.\n\n    The decoded firing rate is accessible in the post-synaptic neurons with ``sum(target)``.\n\n    The projection can be connected using any method available in ``Projection`` (although all-to-all or many-to-one makes mostly sense). Delays are ignored.\n\n    The weight value allows to scale the firing rate: if you want a pre-synaptic firing rate of 100 Hz to correspond to a post-synaptic rate of 1.0, use ``w = 1./100.``.\n\n    Example:\n\n    ```python\n    pop1 = PoissonPopulation(1000, rates=100.)\n    pop2 = Population(1, Neuron(equations=\"r=sum(exc)\"))\n    proj = DecodingProjection(pop1, pop2, 'exc', window=10.0)\n    proj.connect_all_to_all(1.0, force_multiple_weights=True)\n    ```\n\n    \"\"\"\n    def __init__(self, pre, post, target, window=0.0, name=None, copied=False):\n\"\"\"\n        :param pre: pre-synaptic population.\n        :param post: post-synaptic population.\n        :param target: type of the connection.\n        :param window: duration of the time window to collect spikes (default: dt).\n        \"\"\"\n        # Instantiate the projection\n        SpecificProjection.__init__(self, pre, post, target, None, name, copied)\n\n        # Check populations\n        if not self.pre.neuron_type.type == 'spike':\n            Global._error('The pre-synaptic population of a DecodingProjection must be spiking.')\n\n        if not self.post.neuron_type.type == 'rate':\n            Global._error('The post-synaptic population of a DecodingProjection must be rate-coded.')\n\n        # Process window argument\n        if window == 0.0:\n            window = Global.config['dt']\n        self.window = window\n\n        # Disable openMP post-synaptic matrix split\n        self._no_split_matrix = True\n\n        # Not on CUDA\n        if Global._check_paradigm('cuda'):\n            Global._error('DecodingProjections are not available on CUDA yet.')\n\n    def _copy(self, pre, post):\n        \"Returns a copy of the population when creating networks. Internal use only.\"\n        copied_proj = DecodingProjection(pre=pre, post=post, target=self.target, window=self.window, name=self.name, copied=True)\n        copied_proj._no_split_matrix = True\n        return copied_proj\n\n    def _generate_st(self):\n        # Generate the code\n        self._specific_template['declare_additional'] = \"\"\"\n    // Window\n    int window = %(window)s;\n    std::deque&lt; std::vector&lt; %(float_prec)s &gt; &gt; rates_history ;\n\"\"\" % { 'window': int(self.window/Global.config['dt']), 'float_prec': Global.config['precision'] }\n\n        self._specific_template['init_additional'] = \"\"\"\n        rates_history = std::deque&lt; std::vector&lt; %(float_prec)s &gt; &gt;(%(window)s, std::vector&lt; %(float_prec)s &gt;(%(post_size)s, 0.0));\n\"\"\" % { 'window': int(self.window/Global.config['dt']),'post_size': self.post.size, 'float_prec': Global.config['precision'] }\n\n        self._specific_template['psp_code'] = \"\"\"\n        if (pop%(id_post)s._active) {\n            std::vector&lt; std::pair&lt;int, int&gt; &gt; inv_post;\n            std::vector&lt; %(float_prec)s &gt; rates = std::vector&lt; %(float_prec)s &gt;(%(post_size)s, 0.0);\n            // Iterate over all incoming spikes\n            for(int _idx_j = 0; _idx_j &lt; pop%(id_pre)s.spiked.size(); _idx_j++){\n                rk_j = pop%(id_pre)s.spiked[_idx_j];\n                inv_post = inv_pre_rank[rk_j];\n                nb_post = inv_post.size();\n                // Iterate over connected post neurons\n                for(int _idx_i = 0; _idx_i &lt; nb_post; _idx_i++){\n                    // Retrieve the correct indices\n                    i = inv_post[_idx_i].first;\n                    j = inv_post[_idx_i].second;\n\n                    // Increase the post-synaptic conductance\n                    rates[post_rank[i]] +=  %(weight)s;\n                }\n            }\n\n            rates_history.push_front(rates);\n            rates_history.pop_back();\n            for(int i=0; i&lt;post_rank.size(); i++){\n                sum = 0.0;\n                for(int step=0; step&lt;window; step++){\n                    sum += rates_history[step][post_rank[i]];\n                }\n                pop%(id_post)s._sum_%(target)s[post_rank[i]] += sum /float(window) * 1000. / dt / float(pre_rank[i].size());\n            }\n        } // active\n\"\"\" % { 'id_proj': self.id, 'id_pre': self.pre.id, 'id_post': self.post.id, 'target': self.target,\n        'post_size': self.post.size, 'float_prec': Global.config['precision'],\n        'weight': \"w\" if self._has_single_weight() else \"w[i][j]\"}\n\n        self._specific_template['psp_prefix'] = \"\"\"\n        int nb_post, i, j, rk_j, rk_post, rk_pre;\n%(float_prec)s sum;\n\"\"\" % { 'float_prec': Global.config['precision'] }\n\n    def _generate_omp(self):\n        # Generate the code\n        self._specific_template['declare_additional'] = \"\"\"\n    // Window\n    int window = %(window)s;\n    std::deque&lt; std::vector&lt; %(float_prec)s &gt; &gt; rates_history ;\n\"\"\" % { 'window': int(self.window/Global.config['dt']), 'float_prec': Global.config['precision'] }\n\n        self._specific_template['init_additional'] = \"\"\"\n        rates_history = std::deque&lt; std::vector&lt; %(float_prec)s &gt; &gt;(%(window)s, std::vector&lt; %(float_prec)s &gt;(%(post_size)s, 0.0));\n\"\"\" % { 'window': int(self.window/Global.config['dt']),'post_size': self.post.size, 'float_prec': Global.config['precision'] }\n\n        self._specific_template['psp_code'] = \"\"\"\n        #pragma omp single\n        {\n            if (pop%(id_post)s._active) {\n                std::vector&lt; std::pair&lt;int, int&gt; &gt; inv_post;\n                std::vector&lt; %(float_prec)s &gt; rates = std::vector&lt; %(float_prec)s &gt;(%(post_size)s, 0.0);\n                // Iterate over all incoming spikes\n                for(int _idx_j = 0; _idx_j &lt; pop%(id_pre)s.spiked.size(); _idx_j++){\n                    rk_j = pop%(id_pre)s.spiked[_idx_j];\n                    inv_post = inv_pre_rank[rk_j];\n                    nb_post = inv_post.size();\n                    // Iterate over connected post neurons\n                    for(int _idx_i = 0; _idx_i &lt; nb_post; _idx_i++){\n                        // Retrieve the correct indices\n                        i = inv_post[_idx_i].first;\n                        j = inv_post[_idx_i].second;\n\n                        // Increase the post-synaptic conductance\n                        rates[post_rank[i]] +=  %(weight)s;\n                    }\n                }\n\n                rates_history.push_front(rates);\n                rates_history.pop_back();\n                for(int i=0; i&lt;post_rank.size(); i++){\n                    sum = 0.0;\n                    for(int step=0; step&lt;window; step++){\n                        sum += rates_history[step][post_rank[i]];\n                    }\n                    pop%(id_post)s._sum_%(target)s[post_rank[i]] += sum /float(window) * 1000. / dt / float(pre_rank[i].size());\n                }\n            } // active\n        }\n\"\"\" % { 'id_proj': self.id, 'id_pre': self.pre.id, 'id_post': self.post.id, 'target': self.target,\n        'post_size': self.post.size, 'float_prec': Global.config['precision'],\n        'weight': \"w\" if self._has_single_weight() else \"w[i][j]\"}\n\n        self._specific_template['psp_prefix'] = \"\"\"\n        int nb_post, i, j, rk_j, rk_post, rk_pre;\n%(float_prec)s sum;\n\"\"\" % { 'float_prec': Global.config['precision'] }\n\n    def _generate_cuda(self):\n        raise Global.ANNarchyException(\"The DecodingProjection is not available on CUDA devices.\", True)\n</code></pre>"},{"location":"API/SpecificProjection.html#ANNarchy.core.SpecificProjection.DecodingProjection.__init__","title":"<code>__init__(pre, post, target, window=0.0, name=None, copied=False)</code>","text":"<p>Parameters:</p> <ul> <li> pre         \u2013          <p>pre-synaptic population.</p> </li> <li> post         \u2013          <p>post-synaptic population.</p> </li> <li> target         \u2013          <p>type of the connection.</p> </li> <li> window         \u2013          <p>duration of the time window to collect spikes (default: dt).</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/core/SpecificProjection.py</code> <pre><code>def __init__(self, pre, post, target, window=0.0, name=None, copied=False):\n\"\"\"\n    :param pre: pre-synaptic population.\n    :param post: post-synaptic population.\n    :param target: type of the connection.\n    :param window: duration of the time window to collect spikes (default: dt).\n    \"\"\"\n    # Instantiate the projection\n    SpecificProjection.__init__(self, pre, post, target, None, name, copied)\n\n    # Check populations\n    if not self.pre.neuron_type.type == 'spike':\n        Global._error('The pre-synaptic population of a DecodingProjection must be spiking.')\n\n    if not self.post.neuron_type.type == 'rate':\n        Global._error('The post-synaptic population of a DecodingProjection must be rate-coded.')\n\n    # Process window argument\n    if window == 0.0:\n        window = Global.config['dt']\n    self.window = window\n\n    # Disable openMP post-synaptic matrix split\n    self._no_split_matrix = True\n\n    # Not on CUDA\n    if Global._check_paradigm('cuda'):\n        Global._error('DecodingProjections are not available on CUDA yet.')\n</code></pre>"},{"location":"API/SpecificSynapse.html","title":"Built-in synapse types","text":"<p>ANNarchy provides standard spiking synapse models, similar to the ones defined in PyNN (http://neuralensemble.org/docs/PyNN/reference/plasticitymodels.html).</p>"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Synapses.Hebb","title":"<code>ANNarchy.models.Synapses.Hebb</code>","text":"<p>             Bases: <code>Synapse</code></p> <p>Rate-coded synapse with Hebbian plasticity.</p> <p>Parameters (global):</p> <ul> <li>eta = 0.01 : learning rate.</li> </ul> <p>Learning rule:</p> <ul> <li>w : weight.</li> </ul> <pre><code>dw/dt = eta * pre.r * post.r\n</code></pre> <p>Equivalent code:</p> <pre><code>Hebb = Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n    \"\"\",\n    equations = \"\"\"\n        dw/dt = eta * pre.r * post.r : min=0.0\n    \"\"\"\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Synapses.py</code> <pre><code>class Hebb(Synapse):\n'''\n    Rate-coded synapse with Hebbian plasticity.\n\n    **Parameters (global)**:\n\n    * eta = 0.01 : learning rate.\n\n    **Learning rule**:\n\n    * w : weight.\n\n    ```\n    dw/dt = eta * pre.r * post.r\n    ```\n\n    Equivalent code:\n\n    ```python\n    Hebb = Synapse(\n        parameters = \"\"\"\n            eta = 0.01 : projection\n        \"\"\",\n        equations = \"\"\"\n            dw/dt = eta * pre.r * post.r : min=0.0\n        \"\"\"\n    )\n    ```\n\n    '''\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, eta=0.01):\n\n        parameters = \"\"\"\n            eta = %(eta)s : projection\n        \"\"\" % {'eta': eta}\n\n        equations = \"\"\"\n            dw/dt = eta * pre.r * post.r : min=0.0, explicit \n        \"\"\"\n\n        Synapse.__init__(self, parameters=parameters, equations=equations,\n            name=\"Hebbian Plasticity\", description=\"Simple Hebbian learning rule\")\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Synapses.Oja","title":"<code>ANNarchy.models.Synapses.Oja</code>","text":"<p>             Bases: <code>Synapse</code></p> <p>Rate-coded synapse with regularized Hebbian plasticity (Oja).</p> <p>Parameters (global):</p> <ul> <li> <p>eta = 0.01 : learning rate.</p> </li> <li> <p>alpha = 1.0 : regularization constant.</p> </li> </ul> <p>Learning rule:</p> <ul> <li>w : weight:</li> </ul> <pre><code>dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w )\n</code></pre> <p>Equivalent code:</p> <pre><code>Oja = Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        alpha = 1.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) : min=0.0\n    \"\"\"\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Synapses.py</code> <pre><code>class Oja(Synapse):\n'''\n    Rate-coded synapse with regularized Hebbian plasticity (Oja).\n\n    **Parameters (global)**:\n\n    * eta = 0.01 : learning rate.\n\n    * alpha = 1.0 : regularization constant.\n\n    **Learning rule**:\n\n    * w : weight:\n\n    ```\n    dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w )\n    ```\n\n    Equivalent code:\n\n    ```python\n    Oja = Synapse(\n        parameters = \"\"\"\n            eta = 0.01 : projection\n            alpha = 1.0 : projection\n        \"\"\",\n        equations = \"\"\"\n            dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) : min=0.0\n        \"\"\"\n    )\n    ```\n\n    '''\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, eta=0.01, alpha=1.0):\n\n        parameters = \"\"\"\n            eta = %(eta)s : projection\n            alpha = %(alpha)s : projection\n        \"\"\" % {'eta': eta, 'alpha': alpha}\n\n        equations = \"\"\"\n            dw/dt = eta * ( pre.r * post.r - alpha * post.r^2 * w ) : min=0.0, explicit \n        \"\"\"\n\n        Synapse.__init__(self, parameters=parameters, equations=equations,\n            name=\"Oja plasticity\", description=\"Regularized Hebbian learning rule.\")\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Synapses.IBCM","title":"<code>ANNarchy.models.Synapses.IBCM</code>","text":"<p>             Bases: <code>Synapse</code></p> <p>Rate-coded synapse with Intrator &amp; Cooper (1992) plasticity.</p> <p>Parameters (global):</p> <ul> <li> <p>eta = 0.01 : learning rate.</p> </li> <li> <p>tau = 2000.0 : time constant of the post-synaptic threshold.</p> </li> </ul> <p>Learning rule:</p> <ul> <li>theta : post-synaptic threshold:</li> </ul> <pre><code>tau * dtheta/dt + theta = post.r^2\n</code></pre> <ul> <li>w : weight:</li> </ul> <pre><code>dw/dt = eta * post.r * (post.r - theta) * pre.r \n</code></pre> <p>Equivalent code:</p> <pre><code>IBCM = Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 2000.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit\n    \"\"\"\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Synapses.py</code> <pre><code>class IBCM(Synapse):\n'''\n    Rate-coded synapse with Intrator &amp; Cooper (1992) plasticity.\n\n    **Parameters (global)**:\n\n    * eta = 0.01 : learning rate.\n\n    * tau = 2000.0 : time constant of the post-synaptic threshold.\n\n    **Learning rule**:\n\n    * theta : post-synaptic threshold:\n\n    ```\n    tau * dtheta/dt + theta = post.r^2\n    ```\n\n    * w : weight:\n\n    ```\n    dw/dt = eta * post.r * (post.r - theta) * pre.r \n    ```\n\n    Equivalent code:\n\n    ```python\n    IBCM = Synapse(\n        parameters = \"\"\"\n            eta = 0.01 : projection\n            tau = 2000.0 : projection\n        \"\"\",\n        equations = \"\"\"\n            tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential\n            dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit\n        \"\"\"\n    )\n    ```\n\n    '''\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, eta = 0.01, tau = 2000.0):\n\n        parameters = \"\"\"\n            eta = %(eta)s : projection\n            tau = %(tau)s : projection\n        \"\"\" % {'eta': eta, 'tau': tau}\n\n        equations = \"\"\"\n            tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential\n            dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit\n        \"\"\"\n\n        Synapse.__init__(self, parameters=parameters, equations=equations,\n            name=\"IBCM\", description=\"Intrator and Cooper (1992) learning rule.\")\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Synapses.STP","title":"<code>ANNarchy.models.Synapses.STP</code>","text":"<p>             Bases: <code>Synapse</code></p> <p>Synapse exhibiting short-term facilitation and depression, implemented using the model of Tsodyks, Markram et al.:</p> <p>Tsodyks, Uziel and Markram (2000) Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses. Journal of Neuroscience 20:RC50</p> <p>Note that the time constant of the post-synaptic current is set in the neuron model, not here.</p> <p>Parameters (global):</p> <ul> <li>tau_rec = 100.0 : depression time constant (ms).</li> <li>tau_facil = 0.01 : facilitation time constant (ms).</li> <li>U = 0.5 : use parameter.</li> </ul> <p>Variables:</p> <ul> <li>x : recovery variable::</li> </ul> <pre><code>dx/dt = (1 - x)/tau_rec \n</code></pre> <ul> <li>u : facilitation variable::</li> </ul> <pre><code>du/dt = (U - u)/tau_facil \n</code></pre> <p>Both variables are integrated event-driven. </p> <p>Pre-spike events:</p> <pre><code>g_target += w * u * x\nx *= (1 - u)\nu += U * (1 - u)\n</code></pre> <p>Equivalent code:</p> <pre><code>STP = Synapse(\n    parameters = \"\"\"\n        tau_rec = 100.0 : projection\n        tau_facil = 0.01 : projection\n        U = 0.5\n    \"\"\",\n    equations = \"\"\"\n        dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n        du/dt = (U - u)/tau_facil : init = 0.5, event-driven\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w * u * x\n        x *= (1 - u)\n        u += U * (1 - u)\n    \"\"\"\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Synapses.py</code> <pre><code>class STP(Synapse):\n'''\n    Synapse exhibiting short-term facilitation and depression, implemented using the model of Tsodyks, Markram et al.:\n\n    Tsodyks, Uziel and Markram (2000) Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses. Journal of Neuroscience 20:RC50\n\n    Note that the time constant of the post-synaptic current is set in the neuron model, not here.\n\n    **Parameters (global)**:\n\n    * tau_rec = 100.0 : depression time constant (ms).\n    * tau_facil = 0.01 : facilitation time constant (ms).\n    * U = 0.5 : use parameter.\n\n    **Variables**:\n\n    * x : recovery variable::\n\n    ```\n    dx/dt = (1 - x)/tau_rec \n    ```\n\n    * u : facilitation variable::\n\n    ```\n    du/dt = (U - u)/tau_facil \n    ```\n\n    Both variables are integrated event-driven. \n\n    **Pre-spike events**:\n\n    ```\n    g_target += w * u * x\n    x *= (1 - u)\n    u += U * (1 - u)\n    ```\n\n    Equivalent code:\n\n    ```python\n    STP = Synapse(\n        parameters = \"\"\"\n            tau_rec = 100.0 : projection\n            tau_facil = 0.01 : projection\n            U = 0.5\n        \"\"\",\n        equations = \"\"\"\n            dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n            du/dt = (U - u)/tau_facil : init = 0.5, event-driven\n        \"\"\",\n        pre_spike=\"\"\"\n            g_target += w * u * x\n            x *= (1 - u)\n            u += U * (1 - u)\n        \"\"\"\n    )\n    ```\n\n    '''\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, tau_rec=100.0, tau_facil=0.01, U=0.5):\n\n        if tau_facil&lt;= 0.0:\n            _error('STP: tau_facil must be positive. Choose a very small value if you have to, or derive a new synapse.')\n\n        parameters = \"\"\"\n            tau_rec = %(tau_rec)s : projection\n            tau_facil = %(tau_facil)s : projection\n            U = %(U)s\n        \"\"\" % {'tau_rec': tau_rec, 'tau_facil': tau_facil, 'U': U}\n        equations = \"\"\"\n            dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n            du/dt = (U - u)/tau_facil : init = %(U)s, event-driven   \n        \"\"\" % {'tau_rec': tau_rec, 'tau_facil': tau_facil, 'U': U}\n        pre_spike=\"\"\"\n            g_target += w * u * x\n            x *= (1 - u)\n            u += U * (1 - u)\n        \"\"\"\n\n        Synapse.__init__(self, parameters=parameters, equations=equations, pre_spike=pre_spike,\n            name=\"Short-term plasticity\", description=\"Synapse exhibiting short-term facilitation and depression, implemented using the model of Tsodyks, Markram et al.\")\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/SpecificSynapse.html#ANNarchy.models.Synapses.STDP","title":"<code>ANNarchy.models.Synapses.STDP</code>","text":"<p>             Bases: <code>Synapse</code></p> <p>Spike-timing dependent plasticity.</p> <p>This is the online version of the STDP rule.</p> <p>Song, S., and Abbott, L.F. (2001). Cortical development and remapping through spike timing-dependent plasticity. Neuron 32, 339-350. </p> <p>Parameters (global):</p> <ul> <li>tau_plus = 20.0 : time constant of the pre-synaptic trace (ms)</li> <li>tau_minus = 20.0 : time constant of the pre-synaptic trace (ms)</li> <li>A_plus = 0.01 : increase of the pre-synaptic trace after a spike.</li> <li>A_minus = 0.01 : decrease of the post-synaptic trace after a spike. </li> <li>w_min = 0.0 : minimal value of the weight w.</li> <li>w_max = 1.0 : maximal value of the weight w.</li> </ul> <p>Variables:</p> <ul> <li>x : pre-synaptic trace:</li> </ul> <pre><code>tau_plus  * dx/dt = -x\n</code></pre> <ul> <li>y: post-synaptic trace:</li> </ul> <pre><code>tau_minus * dy/dt = -y\n</code></pre> <p>Both variables are evaluated event-driven.</p> <p>Pre-spike events:</p> <pre><code>g_target += w\n\nx += A_plus * w_max\n\nw = clip(w + y, w_min , w_max)\n</code></pre> <p>Post-spike events::</p> <pre><code>y -= A_minus * w_max\n\nw = clip(w + x, w_min , w_max)\n</code></pre> <p>Equivalent code:</p> <pre><code>STDP = Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection\n        tau_minus = 20.0 : projection\n        A_plus = 0.01 : projection\n        A_minus = 0.01 : projection\n        w_min = 0.0 : projection\n        w_max = 1.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_plus  * dx/dt = -x : event-driven\n        tau_minus * dy/dt = -y : event-driven\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        x += A_plus * w_max\n        w = clip(w + y, w_min , w_max)\n    \"\"\",\n    post_spike=\"\"\"\n        y -= A_minus * w_max\n        w = clip(w + x, w_min , w_max)\n    \"\"\"\n)\n</code></pre> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/models/Synapses.py</code> <pre><code>class STDP(Synapse):\n'''\n    Spike-timing dependent plasticity.\n\n    This is the online version of the STDP rule.\n\n    Song, S., and Abbott, L.F. (2001). Cortical development and remapping through spike timing-dependent plasticity. Neuron 32, 339-350. \n\n    **Parameters (global)**:\n\n    * tau_plus = 20.0 : time constant of the pre-synaptic trace (ms)\n    * tau_minus = 20.0 : time constant of the pre-synaptic trace (ms)\n    * A_plus = 0.01 : increase of the pre-synaptic trace after a spike.\n    * A_minus = 0.01 : decrease of the post-synaptic trace after a spike. \n    * w_min = 0.0 : minimal value of the weight w.\n    * w_max = 1.0 : maximal value of the weight w.\n\n    **Variables**:\n\n    * x : pre-synaptic trace:\n\n    ```\n    tau_plus  * dx/dt = -x\n    ```\n\n    * y: post-synaptic trace:\n\n    ```\n    tau_minus * dy/dt = -y\n    ```\n\n    Both variables are evaluated event-driven.\n\n    **Pre-spike events**:\n\n    ```\n    g_target += w\n\n    x += A_plus * w_max\n\n    w = clip(w + y, w_min , w_max)\n    ```\n\n    **Post-spike events**::\n\n    ```\n    y -= A_minus * w_max\n\n    w = clip(w + x, w_min , w_max)\n    ```\n\n    Equivalent code:\n\n    ```python\n\n    STDP = Synapse(\n        parameters = \"\"\"\n            tau_plus = 20.0 : projection\n            tau_minus = 20.0 : projection\n            A_plus = 0.01 : projection\n            A_minus = 0.01 : projection\n            w_min = 0.0 : projection\n            w_max = 1.0 : projection\n        \"\"\",\n        equations = \"\"\"\n            tau_plus  * dx/dt = -x : event-driven\n            tau_minus * dy/dt = -y : event-driven\n        \"\"\",\n        pre_spike=\"\"\"\n            g_target += w\n            x += A_plus * w_max\n            w = clip(w + y, w_min , w_max)\n        \"\"\",\n        post_spike=\"\"\"\n            y -= A_minus * w_max\n            w = clip(w + x, w_min , w_max)\n        \"\"\"\n    )\n    ```\n\n    '''\n    # For reporting\n    _instantiated = []\n\n    def __init__(self, tau_plus=20.0, tau_minus=20.0, A_plus=0.01, A_minus=0.01, w_min=0.0, w_max=1.0):\n\n        parameters=\"\"\"\n            tau_plus = %(tau_plus)s : projection\n            tau_minus = %(tau_minus)s : projection\n            A_plus = %(A_plus)s : projection\n            A_minus = %(A_minus)s : projection\n            w_min = %(w_min)s : projection\n            w_max = %(w_max)s : projection\n        \"\"\" % {'tau_plus': tau_plus, 'tau_minus':tau_minus, 'A_plus':A_plus, 'A_minus': A_minus, 'w_min': w_min, 'w_max': w_max}\n\n        equations = \"\"\"\n            tau_plus  * dx/dt = -x : event-driven\n            tau_minus * dy/dt = -y : event-driven\n        \"\"\"\n        pre_spike=\"\"\"\n            g_target += w\n            x += A_plus * w_max\n            w = clip(w + y, w_min , w_max)\n        \"\"\"          \n        post_spike=\"\"\"\n            y -= A_minus * w_max\n            w = clip(w + x, w_min , w_max)\n        \"\"\"\n\n        Synapse.__init__(self, parameters=parameters, equations=equations, pre_spike=pre_spike, post_spike=post_spike,\n            name=\"Spike-timing dependent plasticity\", description=\"Synapse exhibiting spike-timing dependent plasticity.\")\n        # For reporting\n        self._instantiated.append(True)\n</code></pre>"},{"location":"API/Synapse.html","title":"Synapse","text":""},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse","title":"<code>ANNarchy.core.Synapse.Synapse</code>","text":"<p>Base class to define a synapse.</p>"},{"location":"API/Synapse.html#ANNarchy.core.Synapse.Synapse.__init__","title":"<code>__init__(parameters='', equations='', psp=None, operation='sum', pre_spike=None, post_spike=None, pre_axon_spike=None, functions=None, pruning=None, creating=None, name=None, description=None, extra_values={})</code>","text":"<p>Parameters:</p> <ul> <li> parameters         \u2013          <p>parameters of the neuron and their initial value.</p> </li> <li> equations         \u2013          <p>equations defining the temporal evolution of variables.</p> </li> <li> psp         \u2013          <p>continuous influence of a single synapse on the post-synaptic neuron (default for rate-coded: <code>w*pre.r</code>). Synaptic transmission in spiking synapses occurs in <code>pre_spike</code>.</p> </li> <li> operation         \u2013          <p>operation (sum, max, min, mean) performed by the post-synaptic neuron on the individual psp (rate-coded only, default=sum).</p> </li> <li> pre_spike         \u2013          <p>updating of variables when a pre-synaptic spike is received (spiking only).</p> </li> <li> post_spike         \u2013          <p>updating of variables when a post-synaptic spike is emitted (spiking only).</p> </li> <li> pre_axon_spike         \u2013          <p>updating of variables when an axonal spike was emitted (spiking only, default None). The usage of this arguments prevents the application of learning rules.</p> </li> <li> functions         \u2013          <p>additional functions used in the equations.</p> </li> <li> name         \u2013          <p>name of the synapse type (used for reporting only).</p> </li> <li> description         \u2013          <p>short description of the synapse type (used for reporting).</p> </li> </ul>"},{"location":"API/Utilities.html","title":"Reporting","text":""},{"location":"API/Utilities.html#ANNarchy.report","title":"<code>ANNarchy.report(filename='./report.tex', standalone=True, gather_subprojections=False, title=None, author=None, date=None, net_id=0)</code>","text":"<p>Generates a report describing the network.</p> <p>If the filename ends with <code>.tex</code>, the TeX file is generated according to:</p> <p>Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8): e1000456.</p> <p>If the filename ends with <code>.md</code>, a (more complete) Markdown file is generated, which can be converted to pdf or html by <code>pandoc</code>::</p> <pre><code>pandoc report.md  -sSN -V geometry:margin=1in -o report.pdf\npandoc report.md  -sSN -o report.html\n</code></pre> <p>Parameters:</p> <ul> <li> filename         \u2013          <p>name of the file where the report will be written (default: \"./report.tex\")</p> </li> <li> standalone         \u2013          <p>tells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown.</p> </li> <li> gather_subprojections         \u2013          <p>if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False).</p> </li> <li> title         \u2013          <p>title of the document (Markdown only)</p> </li> <li> author         \u2013          <p>author of the document (Markdown only)</p> </li> <li> date         \u2013          <p>date of the document (Markdown only)</p> </li> <li> net_id         \u2013          <p>id of the network to be used for reporting (default: 0, everything that was declared)</p> </li> </ul> Source code in <code>/Users/vitay/Research/ANNarchy/annarchy/ANNarchy/parser/report/Report.py</code> <pre><code>def report(filename=\"./report.tex\", standalone=True, gather_subprojections=False, title=None, author=None, date=None, net_id=0):\n\"\"\" \n    Generates a report describing the network.\n\n    If the filename ends with ``.tex``, the TeX file is generated according to:\n\n    Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8): e1000456.\n\n    If the filename ends with ``.md``, a (more complete) Markdown file is generated, which can be converted to pdf or html by ``pandoc``::\n\n        pandoc report.md  -sSN -V geometry:margin=1in -o report.pdf\n        pandoc report.md  -sSN -o report.html\n\n    :param filename: name of the file where the report will be written (default: \"./report.tex\")\n    :param standalone: tells if the generated TeX file should be directly compilable or only includable (default: True). Ignored for Markdown.\n    :param gather_subprojections: if a projection between two populations has been implemented as a multiple of projections between sub-populations, this flag allows to group them in the summary (default: False).\n    :param title: title of the document (Markdown only)\n    :param author: author of the document (Markdown only)\n    :param date: date of the document (Markdown only)\n    :param net_id: id of the network to be used for reporting (default: 0, everything that was declared)\n    \"\"\"\n\n    if filename.endswith('.tex'):\n        from .LatexReport import report_latex\n        report_latex(filename, standalone, gather_subprojections, net_id)\n\n    elif filename.endswith('.md'):\n        from .MarkdownReport import report_markdown\n        report_markdown(filename, standalone, gather_subprojections, title, author, date, net_id)\n\n    else:\n        _error('report(): the filename must end with .tex or .md.')\n</code></pre>"},{"location":"example/BarLearning.html","title":"Bar Learning","text":"<p>The bar learning problem describes the process of learning receptive fields on an artificial input pattern. Images consisting of independent bars are used. Those images are generated as following: an 8*8 image can filled randomly by eight horizontal or vertical bars, with a probability of 1/8 for each.</p> <p>These input images are fed into a neural population, whose neurons should learn to extract the independent components of the input distribution, namely single horizontal or vertical bars.</p> <p>The model consists of two populations <code>Input</code> and <code>Feature</code>. The size of <code>Input</code> should be chosen to fit the input image size (here 8*8). The number of neurons in the <code>Feature</code> population should be higher than the total number of independent bars (16, we choose here 32 neurons). The <code>Feature</code> population gets excitory connections from <code>Input</code> through an all-to-all connection pattern. The same pattern is used for the inhibitory connections within <code>Feature</code>.</p> <pre><code>from ANNarchy import *\nclear()\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <p>Input population:</p> <p>The input pattern will be clamped into this population by the main loop for every trial, so we need just an empty neuron at this point:</p> <pre><code>InputNeuron = Neuron(parameters=\"r = 0.0\")\n</code></pre> <p>The trick here is to declare <code>r</code> as a parameter, not a variable: its value will not be computed by the simulator, but only set by external inputs. The <code>Input</code> population can then be created:</p> <pre><code>Input = Population(geometry=(8, 8), neuron=InputNeuron)\n</code></pre> <p>Feature population:</p> <p>The neuron type composing this population sums up all the excitory inputs gain from <code>Input</code> and the lateral inhibition within <code>Feature</code>.</p> \\[\\tau \\frac {dr_{j}^{\\text{Feature}}}{dt} + r_{j}^{Feature} = \\sum_{i} w_{ij} \\cdot r_{i}^{\\text{Input}}  - \\sum_{k, k \\ne j} w_{kj} * r_{k}^{Feature}\\] <p>could be implemented as the following:</p> <pre><code>LeakyNeuron = Neuron(\n    parameters=\"\"\" \n        tau = 10.0 : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dr/dt + r = sum(exc) - sum(inh) : min=0.0\n    \"\"\"\n)\n</code></pre> <p>The firing rate is restricted to positive values with the <code>min=0.0</code> flag. The population is created in the following way:</p> <pre><code>Feature = Population(geometry=(8, 4), neuron=LeakyNeuron)\n</code></pre> <p>We give it a (8, 4) geometry for visualization only, it does not influence computations at all.</p> <p>Both feedforward (<code>Input</code> \\(\\rightarrow\\) <code>Feature</code>) and lateral (<code>Feature</code> \\(\\rightarrow\\) <code>Feature</code>) projections are learned using the Oja learning rule (a regularized Hebbian learning rule ensuring the sum of all weights coming to a neuron is constant). Only some parameters will differ between the projections.</p> \\[\\tau \\frac{dw_{ij}}{dt} = r_{i} * r_{j} - \\alpha * r_{j}^{2} * w_{ij}\\] <p>where \\(\\alpha\\) is a parameter defining the strength of the regularization, \\(r_i\\) is the pre-synaptic firing rate and \\(r_j\\) the post-synaptic one. The implementation of this synapse type is straightforward:</p> <pre><code>Oja = Synapse(\n    parameters=\"\"\" \n        tau = 2000.0 : postsynaptic\n        alpha = 8.0 : postsynaptic\n        min_w = 0.0 : postsynaptic\n    \"\"\",\n    equations=\"\"\"\n        tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=min_w\n    \"\"\"\n)  \n</code></pre> <p>For this network we need to create two projections, one excitory between the populations <code>Input</code> and <code>Feature</code> and one inhibitory within the <code>Feature</code> population itself:</p> <pre><code>ff = Projection(\n    pre=Input, \n    post=Feature, \n    target='exc', \n    synapse = Oja    \n)\nff.connect_all_to_all(weights = Uniform(-0.5, 0.5))\n\nlat = Projection(\n    pre=Feature, \n    post=Feature, \n    target='inh', \n    synapse = Oja\n)\nlat.connect_all_to_all(weights = Uniform(0.0, 1.0))\n</code></pre> <pre>\n<code>&lt;ANNarchy.core.Projection.Projection at 0x1215ef3d0&gt;</code>\n</pre> <p>The two projections are all-to-all and use the <code>Oja</code> synapse type. They only differ by the parameter <code>alpha</code> (lower in <code>lat</code>) and the fact that the weights of <code>ff</code> are allowed to be negative (so we set the minimum value to -10.0):</p> <pre><code>ff.min_w = -10.0\nlat.alpha = 0.3\n</code></pre> <p>Once the network is defined, one has to specify how inputs are fed into the <code>Input</code> population. A simple solution is to define a method that sets the firing rate of <code>Input</code> according to the specified probabilities every time it is called, and runs the simulation for 50 ms:</p> <pre><code>def trial():\n    # Reset the firing rate for all neurons\n    Input.r = 0.0\n    # Clamp horizontal bars randomly\n    for h in range(Input.geometry[0]):\n        if np.random.random() &amp;lt; 1.0/ float(Input.geometry[0]):\n            Input[h, :].r = 1.0\n    # Clamp vertical bars randomly\n    for w in range(Input.geometry[1]):\n        if np.random.random() &amp;lt; 1.0/ float(Input.geometry[1]):\n            Input[:, w].r = 1.0\n    # Simulate for 50ms\n    simulate(50.)\n    # Return firing rates and receptive fields for visualization\n    return Input.r, Feature.r, ff.receptive_fields()\n</code></pre> <p>One can use here a single value or a Numpy array (e.g. <code>np.zeros(Input.geometry))</code>) to reset activity in Input, it does not matter.</p> <p>For all possible horizontal bars, a decision is then made whether the bar should appear or not, in which case the firing rate of the correspondng neurons is set to 1.0:</p> <pre><code>for h in range(Input.geometry[0]):\n    if np.random.random() &amp;lt; 1.0/ float(Input.geometry[0]):\n        Input[h, :].r = 1.0\n</code></pre> <p><code>Input[h, :]</code> is a PopulationView, i.e. a group of neurons defined by the sub-indices (here the row of index <code>h</code>). Their attributes, such as <code>r</code>, can be accessed as if it were a regular population. The same is done for vertical bars.</p> <p>Once the method for setting inputs is defined, the simulation can be started. A basic approach would be to define a <code>for</code> loop where the <code>trial()</code> method is called repetitively:</p> <pre><code>compile()\n\nfor t in range(1000):\n    input_r, feature_r, weights = trial()\n</code></pre> <pre>\n<code>Compiling ...  OK \n</code>\n</pre> <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\nplt.subplot(131)\nplt.imshow(input_r.T, interpolation='nearest', cmap=plt.cm.gray)\nplt.title('Input')\nplt.subplot(132)\nplt.imshow(feature_r.T, interpolation='nearest', cmap=plt.cm.gray)\nplt.title('Feature')\nplt.subplot(133)\nplt.imshow(weights.T, interpolation='nearest', cmap=plt.cm.gray)\nplt.title('Receptive fields')\nplt.show()\n</code></pre> <p>In the file <code>BarLearning.py</code>, a visualization class using <code>pyqtgraph</code> is imported from <code>Viz.py</code>, but the user is free to use whatever method he prefers to visualize the result of learning.</p> <pre><code>from Viz import Viewer\nview = Viewer(func=trial)\nview.run()\n</code></pre>"},{"location":"example/BarLearning.html#bar-learning-problem","title":"Bar Learning problem","text":"<p>Download the Jupyter notebook : BarLearning.ipynb</p>"},{"location":"example/BarLearning.html#model-overview","title":"Model overview","text":""},{"location":"example/BarLearning.html#defining-the-neurons-and-populations","title":"Defining the neurons and populations","text":""},{"location":"example/BarLearning.html#defining-the-synapse-and-projections","title":"Defining the synapse and projections","text":""},{"location":"example/BarLearning.html#setting-inputs","title":"Setting inputs","text":""},{"location":"example/BarLearning.html#running-the-simulation","title":"Running the simulation","text":""},{"location":"example/BasalGanglia.html","title":"Logging with tensorboard","text":"<pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.tensorboard import Logger\n\nimport matplotlib.pyplot as plt\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <p>As it is just for demonstration purposes, we will be an extremely simplified model of the basal ganglia learning to solve through reinforcement learning a stimulus-response task with 4 stimuli and 2 responses (left and right). The two first stimuli should be responded with left, the two others with right.</p> <pre><code>stimuli = [\n    ([1, 0, 0, 0], 0), # A : left\n    ([0, 1, 0, 0], 0), # B : left\n    ([0, 0, 1, 0], 1), # C : right\n    ([0, 0, 0, 1], 1), # D : right\n]\n</code></pre> <p>We keep here the model as simple as possible. It is inspired from the rate-coded model described here:</p> <p>Vitay J, Hamker FH. 2010. A computational model of Basal Ganglia and its role in memory retrieval in rewarded visual memory tasks. Frontiers in computational neuroscience 4. doi:10.3389/fncom.2010.00013</p> <p>The input population is composed of 4 static neurons to represent the inputs:</p> <pre><code>cortex = Population(4, Neuron(parameters=\"r=0.0\"))\n</code></pre> <p>The cortex projects on the striatum, which is composed of 10 neurons integrating excitatory and inhibitory inputs:</p> <pre><code>msn = Neuron(\n    parameters=\"tau = 10.0 : population; noise = 0.1 : population\", \n    equations=\"\"\"\n        tau*dv/dt + v = sum(exc) - sum(inh) + noise * Uniform(-1, 1)\n        r = clip(v, 0.0, 1.0)\n        \"\"\")\nstriatum = Population(10, msn)\n</code></pre> <p>The striatum projects inhibitorily on GPi, whose neurons are tonically active (high baseline). Normally, GPi would project on the thalamus and back to the cortex, but here we read the output of the network directly in GPi: if the first neuron (corresponding to the left action) is less active than the second neuron, the selected action is left.</p> <pre><code>gp_neuron = Neuron(\n    parameters=\"tau = 10.0 : population; B = 1.0\",\n    equations=\"tau*dv/dt + v = B - sum(inh); r= pos(v)\")\ngpi = Population(2, gp_neuron)\n</code></pre> <p>Learning occurs at the cortico-striatal synapses, using a reward-modulated Hebbian learning rule, with Oja regularization:</p> <pre><code>corticostriatal = Synapse(\n    parameters=\"\"\"\n        eta = 0.1 : projection\n        alpha = 0.5 : projection\n        dopamine = 0.0 : projection\"\"\",\n    equations=\"w += eta*(dopamine * pre.r * post.r - alpha*w*post.r*post.r) : min=0.0\"\n)\ncx_str = Projection(cortex, striatum, \"exc\", corticostriatal)\ncx_str.connect_all_to_all(weights=Uniform(0.0, 0.5))\n</code></pre> <pre>\n<code>&lt;ANNarchy.core.Projection.Projection at 0x13600f610&gt;</code>\n</pre> <p>Some lateral competition between the striatal neurons:</p> <pre><code>str_str = Projection(striatum, striatum, \"inh\")\nstr_str.connect_all_to_all(weights=0.6)\n</code></pre> <pre>\n<code>&lt;ANNarchy.core.Projection.Projection at 0x13600fca0&gt;</code>\n</pre> <p>One half of the striatal population is connected to the left GPi neuron, the other half to the right neuron:</p> <pre><code>str_gpi1 = Projection(striatum[:int(striatum.size/2)], gpi[0], 'inh').connect_all_to_all(1.0)\nstr_gpi2 = Projection(striatum[int(striatum.size/2):], gpi[1], 'inh').connect_all_to_all(1.0)\n</code></pre> <p>We add a monitor on GPi and compile:</p> <pre><code>m = Monitor(gpi, 'r')\ncompile()\n</code></pre> <pre>\n<code>Compiling ...  OK \n</code>\n</pre> <p>Each trial is very simple: we get a stimulus <code>x</code> from the <code>stimuli</code> array and a correct response <code>t</code>, reset the network for 40 ms, set the input and simulate for 50 ms, observe the activity in GPi to decide what the answer of the network is, provide reward accordingly to the corticostriatal projection and let learn for 10 ms. </p> <p>Here the \"dopamine\" signal is directly the reward (+1 for success, -1 for failure), not the reward prediction error, but it is just for demonstration.</p> <pre><code>def training_trial(x, t):\n\n    # Delay period\n    cortex.r = 0.0\n    cx_str.dopamine = 0.0\n    simulate(40.0)\n\n    # Set inputs\n    cortex.r = np.array(x)\n    simulate(50.0)\n\n    # Read output\n    output = gpi.r\n    answer = np.argmin(output)\n\n    # Provide reward\n    reward = 1.0 if answer == t else -1.0\n    cx_str.dopamine = reward\n    simulate(10.0)\n\n    # Get recordings\n    data = m.get('r')\n\n    return reward, data\n</code></pre> <p>The whole training procedure will simply iterate over the four stimuli for 100 trials:</p> <pre><code>for trial in range(100):\n    # Get a stimulus\n    x, t = stimuli[trial%len(stimuli)]\n    # Perform a trial\n    reward, data = training_trial(x, t)\n</code></pre> <p>We use the <code>Logger</code> class of the <code>tensorboard</code> extension to keep track of various data:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        # Get a stimulus\n        x, t = stimuli[trial%len(stimuli)]\n        # Perform a trial\n        reward, data = training_trial(x, t)\n        # Log data...\n</code></pre> <p>Note that it would be equivalent to manually close the Logger after training:</p> <pre><code>logger = Logger()\nfor trial in range(100):\n    # Get a stimulus\n    x, t = stimuli[trial%len(stimuli)]\n    # Perform a trial\n    reward, data = training_trial(x, t)\n        # Log data...\nlogger.close()\n</code></pre> <p>We log here different quantities, just to demonstrate the different methods of the <code>Logger</code> class:</p> <ul> <li>The reward received after each trial:</li> </ul> <pre><code>logger.add_scalar(\"Reward\", reward, trial)\n</code></pre> <p>The tag \"Reward\" will be the name of the plot in tensorboard. <code>reward</code> is the value that will be displayed, while trial is the index of the current trial (x-axis).</p> <ul> <li>The activity of the two GPi cells at the end of the trial, in separate plots depending on the stimulus:</li> </ul> <pre><code>if trial%len(stimuli) == 0:\n    label = \"GPi activity/A\"\nelif trial%len(stimuli) == 1:\n    label = \"GPi activity/B\"\nelif trial%len(stimuli) == 2:\n    label = \"GPi activity/C\"\nelif trial%len(stimuli) == 3:\n    label = \"GPi activity/D\"\nlogger.add_scalars(label, {\"Left neuron\": gpi.r[0], \"Right neuron\": gpi.r[1]}, trial)\n</code></pre> <p>The four plots will be grouped under the label \"GPi activity\", with a title A, B, C or D. Note that <code>add_scalars()</code> requires a dictionary of values that will plot together.</p> <ul> <li>The activity in the striatum as a 2*5 image:</li> </ul> <pre><code>logger.add_image(\"Activity/Striatum\", striatum.r.reshape((2, 5)), trial)\n</code></pre> <p>The activity should be reshaped to the correct dimensions. Note that activity in the striatum is bounded between 0 and 1, so there is no need for equalization.</p> <ul> <li>An histogram of the preference for the stimuli A and B of striatal cells:</li> </ul> <pre><code>w = np.array(cx_str.w)\nlogger.add_histogram(\"Cortico-striatal weights/Left - AB/CD\", np.mean(w[:5, :2] - w[:5, 2:], axis=1), trial)\nlogger.add_histogram(\"Cortico-striatal weights/Right - AB/CD\", np.mean(w[5:, :2] - w[5:, 2:], axis=1), trial)\n</code></pre> <p>We make here two plots, one for the first 5 striatal cells, the other for the rest. We plot the difference between the mean weights of each cell for the stimuli A and B, and the mean weights for the stimuli C and D. If learning goes well, the first five striatal cells should have stronger weights for A and B than for C and D, as they project to the left GPi cell.</p> <ul> <li>A matplotlib figure showing the time course of the two GPi cells (as recorded by the monitor):</li> </ul> <pre><code>fig = plt.figure(figsize=(10, 8))\nplt.plot(data[:, 0], label=\"left\")\nplt.plot(data[:, 1], label=\"right\")\nplt.legend()\nlogger.add_figure(\"Activity/GPi\", fig, trial)\n</code></pre> <p>Note that the figure will be automatically closed by the logger, no need to call <code>show()</code>. Logging figures is extremely slow, use that feature wisely.</p> <p>By default, the logs are saved in the subfolder <code>runs/</code>, but this can be changed when creating the Logger:</p> <pre><code>with Logger(\"/tmp/experiment\") as logger:\n</code></pre> <p>Each run of the network will be saved in this folder. You may want to delete the folder before each run, in order to only visualize the last run:</p> <pre><code>%rm -rf runs\n</code></pre> <pre><code>with Logger() as logger:\n\n    for trial in range(100):\n\n        # Get a stimulus\n        x, t = stimuli[trial%len(stimuli)]\n\n        # Perform a trial\n        reward, data = training_trial(x, t)\n\n        # Log received rewards\n        logger.add_scalar(\"Reward\", reward, trial)\n\n        # Log outputs depending on the task\n        if trial%len(stimuli) == 0:\n            label = \"GPi activity/A\"\n        elif trial%len(stimuli) == 1:\n            label = \"GPi activity/B\"\n        elif trial%len(stimuli) == 2:\n            label = \"GPi activity/C\"\n        elif trial%len(stimuli) == 3:\n            label = \"GPi activity/D\"\n        logger.add_scalars(label, {\"Left neuron\": gpi.r[0], \"Right neuron\": gpi.r[1]}, trial)\n\n        # Log striatal activity as a 2*5 image\n        logger.add_image(\"Activity/Striatum\", striatum.r.reshape((2, 5)), trial)\n\n        # Log histogram of cortico-striatal weights\n        w = np.array(cx_str.w)\n        logger.add_histogram(\"Cortico-striatal weights/Left - AB/CD\", np.mean(w[:5, :2] - w[:5, 2:], axis=1), trial)\n        logger.add_histogram(\"Cortico-striatal weights/Right - AB/CD\", np.mean(w[5:, :2] - w[5:, 2:], axis=1), trial)\n\n        # Log matplotlib figure of GPi activity\n        fig = plt.figure(figsize=(10, 8))\n        plt.plot(data[:, 0], label=\"left\")\n        plt.plot(data[:, 1], label=\"right\")\n        plt.legend()\n        logger.add_figure(\"Activity/GPi\", fig, trial)\n</code></pre> <pre>\n<code>Logging in runs/Dec14_11-32-14_Juliens-MBP\n</code>\n</pre> <p>You can now visualize the logged information by running tensorboard in a separate terminal and opening the corresponding page:</p> <pre><code>tensorboard --logdir runs\n</code></pre> <p>or directly in the notebook if you have the <code>tensorboard</code> extension installed:</p> <pre><code>%load_ext tensorboard\n%tensorboard --logdir runs --samples_per_plugin images=100\n</code></pre> <p>You should see a tensorboard page with four tabs Scalars, Images, Distributions and Histograms:</p> <p></p> <p>The Reward plot shows that the network successfully learns to solve the task, as it consistently gets rewards of +1 (note that this may vary from run to run, depending on weight initialization):</p> <p></p> <p>The GPi activity tab shows that the two GPi cells quickly learn to be inhibited for the right stimuli. </p> <p></p> <p>In the Images tab, the plot for the striatum allows to visualize activity at the end of each rtial, showing that only one cell in the correct subpopulation is active:</p> <p></p> <p>The matplotlib figure for the GPi activity shows what happens during a trial, especially at the end of the reset period:</p> <p></p> <p>In the histograms tab, we can see that the left striatal population has acquired a preference (stronger weights) for the stimuli A and B, as the values are positive. The right population has negative values, so the neurons have stronger weights to the stimuli C and D. Note that some neurons in the right population still have stronger weights from A and B, but they are probably inhibited by the left population, so they do not impair performance.</p> <p></p> <p></p> <pre><code>\n</code></pre>"},{"location":"example/BasalGanglia.html#logging-with-tensorboard","title":"Logging with tensorboard","text":"<p>Download the Jupyter notebook : BasalGanglia.ipynb</p> <p>The <code>tensorboard</code> extension allows to log various information (scalars, images, etc) during training for visualization using <code>tensorboard</code>.</p> <p>It has to be explicitly imported:</p>"},{"location":"example/BayesianOptimization.html","title":"Bayesian optimization","text":""},{"location":"example/BayesianOptimization.html#hyperparameter-optimization","title":"Hyperparameter optimization","text":"<p>Download the Jupyter notebook : BayesianOptimization.ipynb</p> <p>Most of the work in computational neuroscience is to guess the values of parameters which are not constrained by the biology. The most basic approach is to simply try out different values, run the simulation, reason about why the results are not what you want, change some parameters, run again, etc. It is very easy to get lost in this process and it requires a great deal of intuition about how the model works.</p> <p>If you are able to define an objective function for your model (a single number that tells how well your model performs), you can use search algorithms to find this hyperparameters automatically, at the cost of running your model multiple times.</p> <p>Let's take the example of a rate-coded model depending on two hyperparameters <code>a</code> and <code>b</code>, where is the objective is to have a minimal activity after 1 s of simulation (dummy example):</p> <pre><code>from ANNarchy import *\n\npop = Population(...)\n...\ncompile()\n\ndef run(a, b):\n    pop.a = a\n    pop.b = b\n\n    simulate(1000.)\n\n    return (pop.r)**2\n</code></pre> <p>Grid search would iterate over all possible values of the parameters to perform the search:</p> <pre><code>min_loss = 1000.\nfor a in np.linspace(0.0, 1.0, 100):\n    for b in np.linspace(0.0, 1.0, 100):\n        loss = run(a, b)\n        if loss &amp;lt; min_loss:\n            min_loss = loss\n            a_best = a ; b_best = b\n</code></pre> <p>If you try 100 values for each parameters, you need 10000 simulations to find your parameters. The number of simulations explodes with the number of free parameters. Moreover, you cannot stop the search before the end, as you could miss the interesting region.</p> <p>Random search samples blindly values for the hyperparameters:</p> <pre><code>min_loss = 1000.\nfor _ in range(1000):\n    a = np.random.uniform(0.0, 1.0)\n    b = np.random.uniform(0.0, 1.0)\n    loss = run(a, b)\n    if loss &amp;lt; min_loss:\n        min_loss = loss\n        a_best = a ; b_best = b\n</code></pre> <p>If you are lucky, you may find a good solution quite early in the search, so you can stop it when the loss is below a desired threshold. The main drawback is that the search may spend a lot of time in uninteresting regions: it does not learn anything between two samples.</p> <p>An often much more efficient search method is Bayesian optimization (also called sequential model-based optimization - SMBO). It is a form of random search that updates beliefs on the hyperparameters. In short, if some parameter values do not lead to good values of the objective function in early samples, they will not be used in later samples. The search becomes more and more focused on the interesting regions of the hyperparameter space. </p> <p>As always with Python, there are many libraries for that, including:</p> <ul> <li><code>hyperopt</code> <li><code>optuna</code> <li><code>talos</code> (for keras models)  <p>This notebook demonstrates how to use <code>hyperopt</code> to find some hyperparameters of the COBA models already included in the ANNarchy examples:</p> <p> <p>Additionally, we will use the <code>tensorboard</code> extension to visualize the dependency between the parameters and the objective function. </p> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.tensorboard import Logger\nclear()\nsetup(dt=0.1)\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <pre><code>COBA = Neuron(\n    parameters=\"\"\"\n        El = -60.0          : population\n        Vr = -60.0          : population\n        Erev_exc = 0.0      : population\n        Erev_inh = -80.0    : population\n        Vt = -50.0          : population\n        tau = 20.0          : population\n        tau_exc = 5.0       : population\n        tau_inh = 10.0      : population\n        I = 20.0            : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I\n\n        tau_exc * dg_exc/dt = - g_exc\n        tau_inh * dg_inh/dt = - g_inh\n    \"\"\",\n    spike = \"v &amp;gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\nP = Population(geometry=4000, neuron=COBA)\nPe = P[:3200]\nPi = P[3200:]\nP.v = Normal(-55.0, 5.0)\nP.g_exc = Normal(4.0, 1.5)\nP.g_inh = Normal(20.0, 12.0)\n\nCe = Projection(pre=Pe, post=P, target='exc')\nCe.connect_fixed_probability(weights=0.6, probability=0.02)\nCi = Projection(pre=Pi, post=P, target='inh')\nCi.connect_fixed_probability(weights=6.7, probability=0.02)\n\ncompile()\n\nm = Monitor(P, ['spike'])\n</code></pre> <pre>\n<code>Compiling ...  OK \n</code>\n</pre> <p>With the default parameters, the COBA network fires at around 20 Hz:</p> <pre><code>simulate(1000.0)\ndata = m.get('spike')\nfr = m.mean_fr(data)\nprint(fr)\n</code></pre> <pre>\n<code>19.732999999999997\n</code>\n</pre> <p>Let's suppose we now want the network to fire at 30 Hz. Which parameters should we change to obtain that value?</p> <p>Many parameters might influence the firing rate of the network (if not all). Here, we make the assumption that the weight values for the excitatory connections (0.6) and inhibitory ones (6.7) are the most critical ones.</p> <p>Let's start by importing <code>hyperopt</code> (after installing it with <code>pip install hyperopt</code>):</p> <pre><code>from hyperopt import fmin, tpe, hp, STATUS_OK\n</code></pre> <p>We define a <code>trial()</code> method taking values for the two hyperparameters as inputs. It starts by resetting the network, sets the excitatory and inhibitory weights to the desired value, simulates for one second, computes the mean firing rate of the population, logs the parameters and finally returns the objective function: the squared error between the recorded firing rate and 30 Hz.</p> <pre><code>logger = Logger()\n\ndef trial(args):\n\n    # Retrieve the parameters\n    w_exc = args[0]\n    w_inh = args[1]\n\n    # Reset the network\n    reset()\n\n    # Set the hyperparameters\n    Ce.w = w_exc\n    Ci.w = w_inh\n\n    # Simulate 1 second\n    simulate(1000.0)\n\n    # Retrieve the spike recordings and the membrane potential\n    spikes = m.get('spike')\n\n    # Compute the population firing rate\n    fr = m.mean_fr(spikes)\n\n    # Compute a quadratic loss around 30 Hz\n    loss = 0.001*(fr - 30.0)**2   \n\n    # Log the parameters\n    logger.add_parameters({'w_exc': w_exc, 'w_inh': w_inh},\n                         {'loss': loss, 'firing_rate': fr})\n\n    return {\n        'loss': loss,\n        'status': STATUS_OK,\n        # -- store other results like this\n        'fr': fr,\n        }\n</code></pre> <pre>\n<code>Logging in runs/Dec14_11-34-31_Juliens-MBP\n</code>\n</pre> <p>We can check that the default parameters indeed lead to a firing rate of 20 Hz:</p> <pre><code>trial([0.6, 6.7])\n</code></pre> <pre>\n<code>{'loss': 0.10541128900000007, 'status': 'ok', 'fr': 19.732999999999997}</code>\n</pre> <p>We can now use <code>hyperopt</code> to find the hyperparameters making the network fire at 30 Hz.</p> <p>The <code>fmin()</code> function takes:</p> <ul> <li><code>fn</code>: the objective function for a set of parameters.</li> <li><code>space</code>: the search space for the hyperparameters (the prior). </li> <li><code>algo</code>: which algorithm to use, either tpe.suggest or random.suggest</li> <li><code>max_evals</code>: number of samples (simulations) to make.</li> </ul> <p>Here, we will sample the excitatory weights between 0.1 and 1, the inhibitory ones between 1 and 10. Of course, the smaller the range, the better. Refer to the doc of hyperopt for other sampling priors.</p> <pre><code>best = fmin(\n    fn=trial,\n    space=[\n        hp.uniform('w_exc', 0.1, 1.0), \n        hp.uniform('w_inh', 1.0, 10.0)\n    ],\n    algo=tpe.suggest,\n    max_evals=100)\nprint(best)\n</code></pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:19&lt;00:00,  5.02trial/s, best loss: 9.0150062499999e-05]\n{'w_exc': 0.9916507361791144, 'w_inh': 7.825494383808928}\n</code>\n</pre> <p>After 100 simulations, <code>hyperopt</code> returns a set of hyperparameter values that make the network fire at 30Hz. We can check that it is true with:</p> <pre><code>trial([best['w_exc'], best['w_inh']])\n</code></pre> <pre>\n<code>{'loss': 9.0150062499999e-05, 'status': 'ok', 'fr': 30.30025}</code>\n</pre> <p>There are plenty of options to <code>hyperopt</code> (check Trials or the parallel search using MongoDB), but this simple example should get you started. </p> <p>If we start tensorboard in the default directory <code>runs/</code>, we can additionally visualize how the firing rate depends on <code>w_exc</code> and <code>w_inh</code> in the <code>HPARAMS</code> tab.</p> <pre><code>logger.close()\n%load_ext tensorboard\n%tensorboard --logdir runs\n</code></pre> <p></p>"},{"location":"example/BoldMonitoring.html","title":"BOLD monitoring","text":"<pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.bold import *\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <pre><code>clear()\n\npop0 = Population(100, neuron=Izhikevich)\npop1 = Population(100, neuron=Izhikevich)\n</code></pre> <p>As we will not have any connections between the neurons, we need to increase the noise to create some baseline activity:</p> <pre><code># Set noise to create some baseline activity\npop0.noise = 5.0; pop1.noise = 5.0\n</code></pre> <p>The mean firing rate of the neurons will be used as an input to the BOLD model. This need to be activated explicitly as the computation of this value is quite expensive. In our example, the mean firing rate should be computed across a time window of 100 ms. The resulting value will be stored in the <code>r</code> attribute of the populations and can be easily recorded.</p> <pre><code># Compute mean firing rate in Hz on 100ms window\npop0.compute_firing_rate(window=100.0)\npop1.compute_firing_rate(window=100.0)\n\n# Record the mean firing rate\nmon_pop0 = Monitor(pop0, [\"r\"], start=False)\nmon_pop1 = Monitor(pop1, [\"r\"], start=False)\n</code></pre> <pre><code>m_bold = BoldMonitor(\n\n    populations = [pop0, pop1], # recorded populations\n\n    bold_model = balloon_RN(), # BOLD model to use (default is balloon_RN)\n\n    mapping = {'I_CBF': 'r'}, # mapping from pop.r to I_CBF\n\n    normalize_input = 2000,  # time window to compute baseline.\n\n    recorded_variables = [\"I_CBF\", \"BOLD\"]  # variables to be recorded\n)\n</code></pre> <p>Now we can compile and initialize the network:</p> <pre><code>compile()\n</code></pre> <pre>\n<code>Compiling ...  OK \n</code>\n</pre> <pre><code># Ramp up time\nsimulate(1000)\n</code></pre> <p>We then enable the recording of all monitors:</p> <pre><code># Start recording\nmon_pop0.start()\nmon_pop1.start()\nm_bold.start()\n</code></pre> <p>We simulate for 5 seconds with lower noise and we increase the noise in <code>pop0</code> for 5 seconds before decreasing it again:</p> <pre><code># We manipulate the noise for the half of the neurons\nsimulate(5000)      # 5s with low noise\npop0.noise = 7.5\nsimulate(5000)      # 5s with higher noise (one population)\npop0.noise = 5\nsimulate(10000)     # 10s with low noise\n\n# Retrieve the recordings\nmean_fr1 = np.mean(mon_pop0.get(\"r\"), axis=1)\nmean_fr2 = np.mean(mon_pop1.get(\"r\"), axis=1)\n\ninput_data = m_bold.get(\"I_CBF\")\nbold_data = m_bold.get(\"BOLD\")\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(20,6))\ngrid = plt.GridSpec(1, 3, left=0.05, right=0.95)\n\n# mean firing rate\nax1 = plt.subplot(grid[0, 0])\n\nax1.plot(mean_fr1, label=\"pop0\")\nax1.plot(mean_fr2, label=\"pop1\")\nplt.legend()\nax1.set_ylabel(\"Average mean firing rate [Hz]\", fontweight=\"bold\", fontsize=18)\n\n# BOLD input signal\nax2 = plt.subplot(grid[0, 1])\n\nax2.plot(input_data)\nax2.set_ylabel(\"BOLD input I_CBF\", fontweight=\"bold\", fontsize=18)\n\n# BOLD output signal\nax3 = plt.subplot(grid[0, 2])\n\nax3.plot(bold_data*100.0)\nax3.set_ylabel(\"BOLD [%]\", fontweight=\"bold\", fontsize=18)\n\n# x-axis labels as seconds\nfor ax in [ax1, ax2, ax3]:\n    ax.set_xticks(np.arange(0,21,2)*1000)\n    ax.set_xticklabels(np.arange(0,21,2))\n    ax.set_xlabel(\"time [s]\", fontweight=\"bold\", fontsize=18)\n\nplt.show()\n</code></pre> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.bold import *\nclear()\n\n# Two populations of 100 izhikevich neurons\npop0 = Population(100, neuron=Izhikevich)\npop1 = Population(100, neuron=Izhikevich)\n\n# Set noise to create some baseline activity\npop0.noise = 5.0; pop1.noise = 5.0\n\n# Compute mean firing rate in Hz on 100ms window\npop0.compute_firing_rate(window=100.0)\npop1.compute_firing_rate(window=100.0)\n\n# Create required monitors\nmon_pop0 = Monitor(pop0, [\"r\"], start=False)\nmon_pop1 = Monitor(pop1, [\"r\"], start=False)\n</code></pre> <p>We can now create a hybrid model computing both the Balloon RN model of Stephan et al. (2007) and the Davis model:</p> <pre><code>balloon_Davis = BoldModel(\n    parameters = \"\"\"\n        phi       = 1.0         ;   kappa     = 1/1.54\n        gamma     = 1/2.46      ;   E_0       = 0.34\n        tau       = 0.98        ;   alpha     = 0.33\n        V_0       = 0.02        ;   v_0       = 40.3\n        TE        = 40/1000.    ;   epsilon   = 1.43\n        r_0       = 25.         ;   second    = 1000.0\n        M         = 0.062       ;   alpha2    = 0.14\n        beta      = 0.91\n    \"\"\",\n    equations = \"\"\"\n        # CBF input\n        I_CBF          = sum(I_CBF)       \n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  \n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        # Balloon model\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear BOLD equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))\n\n        # Davis model\n        r = f_in * E / E_0                                                         : init=1, min=0.01\n        BOLD_Davis =  M * (1 - f_in**alpha2 * (r / f_in)**beta) \n    \"\"\",\n    inputs=['I_CBF']\n)\n</code></pre> <p>We now only need to pass that new object to the BOLD monitor, and specify that we want to record both <code>BOLD</code> and <code>BOLD_Davis</code>:</p> <pre><code>m_bold = BoldMonitor(\n\n    populations = [pop0, pop1],  \n\n    bold_model = balloon_Davis,\n\n    mapping={'I_CBF': 'r'},            \n\n    normalize_input=2000, \n\n    recorded_variables=[\"I_CBF\", \"BOLD\", \"BOLD_Davis\"]\n)\n\ncompile()\n</code></pre> <p>We run the same simulation protocol and compare the two BOLD signals. Note that the value of <code>M</code> has been modified to give a similar amplitude to both signals:</p> <pre><code># Ramp up time\nsimulate(1000)\n\n# Start recording\nmon_pop0.start()\nmon_pop1.start()\nm_bold.start()\n\n# we manipulate the noise for the half of the neurons\nsimulate(5000)      # 5s with low noise\npop0.noise = 7.5\nsimulate(5000)      # 5s with higher noise (one population)\npop0.noise = 5\nsimulate(10000)     # 10s with low noise\n\n# retrieve the recordings\nmean_fr1 = np.mean(mon_pop0.get(\"r\"), axis=1)\nmean_fr2 = np.mean(mon_pop1.get(\"r\"), axis=1)\n\nIf_data = m_bold.get(\"I_CBF\")\nbold_data = m_bold.get(\"BOLD\")\ndavis_data = m_bold.get(\"BOLD_Davis\")\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nplt.figure(figsize=(20,8))\ngrid = plt.GridSpec(1, 2, left=0.05, right=0.95)\n\n# mean firing rate\nax1 = plt.subplot(grid[0, 0])\nax1.plot(mean_fr1, label=\"pop0\")\nax1.plot(mean_fr2, label=\"pop1\")\nplt.legend()\nax1.set_ylabel(\"Average firing rate [Hz]\", fontweight=\"bold\", fontsize=18)\n\n# BOLD input signal as percent\nax2 = plt.subplot(grid[0, 1])\nax2.plot(bold_data*100.0, label=\"Balloon_RN\")\nax2.plot(davis_data*100.0, label=\"Davis\")\nplt.legend()\nax2.set_ylabel(\"BOLD [%]\", fontweight=\"bold\", fontsize=18)\n\n# x-axis labels as seconds\nfor ax in [ax1, ax2]:\n    ax.set_xticks(np.arange(0,21,2)*1000)\n    ax.set_xticklabels(np.arange(0,21,2))\n    ax.set_xlabel(\"time [s]\", fontweight=\"bold\", fontsize=18)\n\nplt.show()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"example/BoldMonitoring.html#recording-bold-signals","title":"Recording BOLD signals","text":"<p>Download the Jupyter notebook : BoldMonitoring.ipynb</p> <p>This notebook demonstrates the usage of the BOLD monitoring extension. It has to be explicitly imported:</p>"},{"location":"example/BoldMonitoring.html#background","title":"Background","text":"<p>ANNarchy pre-implements some model variants of the BOLD models presented in Stephan et al. (2007) which are variations of the Balloon Model originally designed by Buxton et al. (1998). The four balloon variants model pre-implemented in ANNarchy follow the naming scheme of Stephan et al. (2007). The model name starts with balloon_ followed by either C (i. e. classical coefficient) or R (i. e. revised coefficient) and then followed by either N which means non-linear BOLD equation or L which means linear bold equation.</p> <p>We only provide here the equations without much explanations, for more details please refer to the literature:</p> <p>&gt;  Buxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855\u2013864. doi:10.1002/mrm.1910390602</p> <p>&gt;  Friston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466\u2013477</p> <p>&gt;  Buxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220\u2013S233. doi:10.1016/j.neuroimage.2004.07.013</p> <p>&gt;  Stephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387\u2013401. doi:10.1016/j.neuroimage.2007.07.040</p> <p>&gt;  Maith et al. (2021). A computational model-based analysis of basal ganglia pathway changes in Parkinson\u2019s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278\u2013 2295. doi:10.1111/ejn.14868 </p> <p>&gt; Maith et al. (2022). BOLD Monitoring in the Neural Simulator ANNarchy. Frontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966.</p>"},{"location":"example/BoldMonitoring.html#single-input-balloon-model","title":"Single input Balloon model","text":"<p>This script shows a simple example for the application of one of the default models (the balloon_RN model) on a simple network. The balloon_RN model is described by the following equations:</p> \\[     \\tau \\cdot \\frac{ds}{dt} = \\phi \\cdot I_\\text{CBF} - \\kappa \\cdot s - \\gamma \\cdot (f_{in} - 1) \\] \\[     \\frac{df_{in}}{dt} = s \\] \\[     E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} } \\] \\[     \\tau \\cdot \\frac{dv}{dt} = \\frac{1}{\\tau_0} \\cdot (f_{in} - f_{out}) \\] \\[     f_{out} = v^{\\frac{1}{\\alpha}} \\] \\[     \\tau \\cdot \\frac{dq}{dt} = \\frac{1}{\\tau_0} \\cdot ( f_{in} \\cdot \\frac{E}{E_0} - \\frac{q}{v} \\cdot f_{out} ) \\] <p>with revised coefficients and non-linear bold equation:</p> \\[k_1            = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\\] \\[k_2            = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\\] \\[k_3            = 1 - \\epsilon\\] \\[     BOLD = v_0 \\cdot ( k_1 \\cdot (1-q) + k_2 \\cdot (1 - \\dfrac{q}{v}) + k_3 \\cdot (1 - v) ) \\] <p>There are two important variables in that model: <code>BOLD</code> which is the output of the model and <code>I_CBF</code> which is the input signal, reflecting the mean firing rate of the input populations.</p> <p>As the BOLD model reflects the change of activity levels in the recorded area, we will implement the following experiment. We record from two populations with 100 Izhikevich neurons each. After a short period of time we raise the activity level of one population for a fixed time window. For simplicity, we do not use connections between the neurons but simply increase the noise term in the Izhikevich model. This should lead to a noticeable BOLD signal.</p>"},{"location":"example/BoldMonitoring.html#populations","title":"Populations","text":"<p>We first create two populations of Izhikevich neurons:</p>"},{"location":"example/BoldMonitoring.html#bold-monitor-definition","title":"BOLD Monitor definition","text":"<p>The BOLD monitor expects a list of populations which we want to record (in our case <code>pop0</code> and <code>pop1</code>). A BOLD model should be specified, here we take <code>balloon_RN</code> which is the default. We then specify the mapping from the population variable that should influence the BOLD signal, in our case the mean firing rate <code>r</code>, to the input variable of the BOLD model <code>I_CBF</code>. </p> <p>The mean firing rate is normalized over a time window of 2000 ms and we record both input and output signals:</p>"},{"location":"example/BoldMonitoring.html#simulation","title":"Simulation","text":"<p>We first simulate 1 second biological time to ensure that the network reaches a stable firing rate:</p>"},{"location":"example/BoldMonitoring.html#evaluation","title":"Evaluation","text":"<p>We can now plot:</p> <ul> <li>the mean firing rate in the input populations.</li> <li>the recorded activity <code>I</code> which serves as an input to the BOLD model.</li> <li>the resulting BOLD signal.</li> </ul>"},{"location":"example/BoldMonitoring.html#davis-model","title":"Davis model","text":"<p>Let's now demonstrate how to define a custom BOLD model. The default Ballon model is defined by the following code:</p> <pre><code>balloon_RN = BoldModel(\n    parameters = \"\"\"\n        phi       = 1.0         ;   kappa     = 1/1.54\n        gamma     = 1/2.46      ;   E_0       = 0.34\n        tau       = 0.98        ;   alpha     = 0.33\n        V_0       = 0.02        ;   v_0       = 40.3\n        TE        = 40/1000.    ;   epsilon   = 1.43\n        r_0       = 25.         ;   second    = 1000.0\n    \"\"\",\n    equations = \"\"\"\n        # CBF input\n        I_CBF          = sum(I_CBF)       \n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  \n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        # Balloon model\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear BOLD equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))\n    \"\"\",\n    inputs=['I_CBF']\n)\n</code></pre> <p>It is very similar to the interface of a Neuron model, with parameters and equations defined in two multi-line strings. The input signal <code>I_CBF</code> has to be explicitly defined in the <code>inputs</code> argument to help the BOLD monitor create the mapping. </p> <p>To demonstrate how to create a custom BOLD model, let's suppose we want a model that computes both the BOLD signal of the Balloon model and the one of the Davis model:</p> <p>&gt; Davis, T. L., Kwong, K. K., Weisskoff, R. M., and Rosen, B. R. (1998). Calibrated functional MRI: mapping the dynamics of oxidative metabolism. Proceedings of the National Academy of Sciences 95, 1834\u20131839</p> <p>Without going into too many details, the Davis model computes the BOLD signal directly using <code>f_in</code> and <code>E</code>, without introducing a differential equation for the BOLD signal. Its implementation using the BOLD model would be:</p> <pre><code>DavisModel = BoldModel(\n    parameters = \"\"\"\n        second = 1000.0\n\n        phi    = 1.0    # Friston et al. (2000)\n        kappa  = 1/1.54\n        gamma  = 1/2.46\n        E_0    = 0.34\n\n        M      = 0.149   # Griffeth &amp;amp; Buxton (2011)\n        alpha  = 0.14\n        beta   = 0.91\n    \"\"\",\n    equations = \"\"\"\n        # CBF-driving input as in Friston et al. (2000)\n        I_CBF    = sum(I_CBF)                                             : init=0\n        ds/dt    = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  : init=0\n        df_in/dt = s  / second                                            : init=1, min=0.01\n    \u200b\n        # Using part of the Balloon model to calculate r (normalized CMRO2) as in Buxton et al. (2004)\n        E        = 1 - (1 - E_0)**(1 / f_in)                              : init=0.34\n        r        = f_in * E / E_0\n\n        # Davis model\n        BOLD     = M * (1 - f_in**alpha * (r / f_in)**beta)               : init=0\n    \"\"\",\n    inputs=['I_CBF']\n)\n</code></pre> <p>Note that we could simply define two BOLD monitors using different models, but let's create a complex model that does both for the sake of demonstration.</p> <p>Let's first redefine the populations of the previous section:</p>"},{"location":"example/COBA.html","title":"COBA","text":"<p>The scripts <code>COBA.py</code> and <code>CUBA.py</code>  in <code>examples/vogels_abbott</code> reproduce the two first benchmarks used in:</p> <p>&gt; Brette, R., Rudolph, M., Carnevale, T., Hines, M., Beeman, D., Bower, J. M., et al. (2007), Simulation of networks of spiking neurons: a review of tools and strategies., J. Comput. Neurosci., 23, 3, 349\u201398</p> <p>Both are based on the balanced network proposed by: </p> <p>&gt; Vogels, T. P. and Abbott, L. F. (2005), Signal propagation and logic gating in networks of integrate-and-fire neurons., J. Neurosci., 25, 46, 10786\u201395</p> <p>The network is composed of 4000 neurons (3200 excitatory and 800 inhibitory), reciprocally connected with a probability of 0.02 (sparse connection).</p> <p>The CUBA network uses a current-based integrate-and-fire neuron model:</p> \\[\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) - g_\\text{inh} (t)\\] <p>while the COBA model uses conductance-based IF neurons:</p> \\[\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) * (E_\\text{exc} - v(t)) + g_\\text{inh} (t) * (E_\\text{inh} - v(t)) + I(t)\\] <p>Apart from the neuron model and synaptic weights, both networks are equal, so we'll focus on the COBA network here.</p> <p>The discretization step has to be set to 0.1 ms:</p> <pre><code>from ANNarchy import * \nsetup(dt=0.1) \n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <pre><code>COBA = Neuron(\n    parameters=\"\"\"\n        El = -60.0          : population\n        Vr = -60.0          : population\n        Erev_exc = 0.0      : population\n        Erev_inh = -80.0    : population\n        Vt = -50.0          : population\n        tau = 20.0          : population\n        tau_exc = 5.0       : population\n        tau_inh = 10.0      : population\n        I = 20.0            : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I\n\n        tau_exc * dg_exc/dt = - g_exc\n        tau_inh * dg_inh/dt = - g_inh\n    \"\"\",\n    spike = \"v &amp;gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n</code></pre> <pre><code>CUBA = Neuron(\n    parameters=\"\"\"\n        El = -49.0      : population\n        Vr = -60.0      : population\n        Vt = -50.0      : population\n        tau_m = 20.0    : population\n        tau_exc = 5.0   : population\n        tau_inh = 10.0  : population\n    \"\"\",\n    equations=\"\"\"\n        tau_m * dv/dt = (El - v) + g_exc + g_inh \n\n        tau_exc * dg_exc/dt = - g_exc \n        tau_inh * dg_inh/dt = - g_inh \n    \"\"\",\n    spike = \"v &amp;gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n</code></pre> <p>The neurons define exponentially-decreasing conductance g_exc and g_inh for the excitatory and inhibitory conductances/currents, respectively.</p> <p>They also define a refractory period of 5 ms.</p> <pre><code>P = Population(geometry=4000, neuron=COBA)\nPe = P[:3200]\nPi = P[3200:]\n</code></pre> <p>We create a population of 4000 COBA neurons, and assign the 3200 first ones to the excitatory population and the 800 last ones to the inhibitory population.</p> <p>It would have been equivalent to declare two separate populations as:</p> <pre><code>Pe = Population(geometry=3200, neuron=COBA)\nPi = Population(geometry= 800, neuron=COBA)\n</code></pre> <p>but splitting a global population allows to apply methods to all neurons, for example when recording all spikes with a single monitor, or when initializing populations parameters uniformly:</p> <pre><code>P.v = Normal(-55.0, 5.0)\nP.g_exc = Normal(4.0, 1.5)\nP.g_inh = Normal(20.0, 12.0)\n</code></pre> <p>The neurons are randomly connected with a probability of 0.02. Excitatory neurons project on all other neurons with the target \"exc\" and a weight of 0.6, while the inhibitory neurons have the target \"inh\" and a weight of 6.7.</p> <pre><code>Ce = Projection(pre=Pe, post=P, target='exc')\nCe.connect_fixed_probability(weights=0.6, probability=0.02)\n\nCi = Projection(pre=Pi, post=P, target='inh')\nCi.connect_fixed_probability(weights=6.7, probability=0.02)\n</code></pre> <pre>\n<code>&lt;ANNarchy.core.Projection.Projection at 0x11290e640&gt;</code>\n</pre> <pre><code>compile()\n</code></pre> <pre>\n<code>Compiling ...  OK \n</code>\n</pre> <p>We first define a monitor to record the spikes emitted in the whole population:</p> <pre><code>m = Monitor(P, ['spike'])\n</code></pre> <p>We can then simulate for 1 second:</p> <pre><code>simulate(1000.)\n</code></pre> <p>We retrieve the recorded spikes from the monitor:</p> <pre><code>data = m.get('spike')\n</code></pre> <p>and compute a raster plot from the data:</p> <pre><code>t, n = m.raster_plot(data)\n</code></pre> <p><code>t</code> and <code>n</code> are lists representing for each spike emitted during the simulation the time at which it was emitted and the index the neuron which fired. The length of this list represents the total number of spikes in the popultion, so we can compute the population mean firing rate:</p> <pre><code>print('Mean firing rate in the population: ' + str(len(t) / 4000.) + 'Hz')\n</code></pre> <pre>\n<code>Mean firing rate in the population: 18.77925Hz\n</code>\n</pre> <p>Finally, we can show the raster plot with pylab:</p> <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 12))\nplt.plot(t, n, '.', markersize=0.5)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"example/COBA.html#coba-and-cuba-networks","title":"COBA and CUBA networks","text":"<p>Download the Jupyter notebook : COBA.ipynb</p>"},{"location":"example/COBA.html#neuron-definition","title":"Neuron definition","text":""},{"location":"example/COBA.html#population","title":"Population","text":""},{"location":"example/COBA.html#connections","title":"Connections","text":""},{"location":"example/COBA.html#simulation","title":"Simulation","text":""},{"location":"example/GapJunctions.html","title":"Gap junctions","text":"<p>A simple network with gap junctions.</p> <p>This is a reimplementation of the Brian example:</p> <p>http://brian2.readthedocs.org/en/2.0b3/examples/synapses.gapjunctions.html</p> <pre><code>from ANNarchy import *\nclear()\n\nsetup(dt=0.1)\n\nneuron = Neuron(\n    parameters = \"v0 = 1.05: population; tau = 10.0: population\",\n    equations = \"tau*dv/dt = v0 - v + g_gap\",\n    spike = \"v &amp;gt;  1.\",\n    reset = \"v = 0.\"\n)\n\ngap_junction = Synapse(\n    psp = \"w * (pre.v - post.v)\"\n)\n\npop = Population(10, neuron)\npop.v = np.linspace(0., 1., 10)\n\nproj = Projection(pop, pop, 'gap', gap_junction)\nproj.connect_all_to_all(0.02)\n\ntrace = Monitor(pop[0] + pop[5], 'v')\n\ncompile()\n\nsimulate(500.)\n\ndata = trace.get('v')\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\nplt.plot(data[:, 0])\nplt.plot(data[:, 1])\nplt.xlabel('Time (ms)')\nplt.ylabel('v')\nplt.show()\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\nCompiling ...  OK \n</code>\n</pre>"},{"location":"example/GapJunctions.html#gap-junctions","title":"Gap Junctions","text":"<p>Download the Jupyter notebook : GapJunctions.ipynb</p>"},{"location":"example/HodgkinHuxley.html","title":"Hodgkin-Huxley","text":"<p>Simple Hodgkin-Huxley neuron.</p> <pre><code>from ANNarchy import *\nclear()\n\ndt=0.01\nsetup(dt=dt)\n\nHH = Neuron(\n\n    parameters = \"\"\"\n    C = 1.0 # Capacitance\n    VL = -59.387 # Leak voltage\n    VK = -82.0 # Potassium reversal voltage\n    VNa = 45.0 # Sodium reveral voltage\n    gK = 36.0 # Maximal Potassium conductance\n    gNa = 120.0 # Maximal Sodium conductance\n    gL = 0.3 # Leak conductance\n    vt = 30.0 # Threshold for spike emission\n    I = 0.0 # External current\n    \"\"\",\n\n    equations = \"\"\"\n    # Previous membrane potential\n    prev_V = V\n\n    # Voltage-dependency parameters\n    an = 0.01 * (V + 60.0) / (1.0 - exp(-0.1* (V + 60.0) ) )\n    am = 0.1 * (V + 45.0) / (1.0 - exp (- 0.1 * ( V + 45.0 )))\n    ah = 0.07 * exp(- 0.05 * ( V + 70.0 ))\n\n    bn = 0.125 * exp (- 0.0125 * (V + 70.0))\n    bm = 4.0 *  exp (- (V + 70.0) / 80.0)\n    bh = 1.0/(1.0 + exp (- 0.1 * ( V + 40.0 )) )\n\n    # Alpha/Beta functions\n    dn/dt = an * (1.0 - n) - bn * n : init = 0.3, midpoint\n    dm/dt = am * (1.0 - m) - bm * m : init = 0.0, midpoint\n    dh/dt = ah * (1.0 - h) - bh * h : init = 0.6, midpoint\n\n    # Membrane equation\n    C * dV/dt = gL * (VL - V ) + gK * n**4 * (VK - V) + gNa * m**3 * h * (VNa - V) + I : midpoint\n\n    \"\"\",\n\n    spike = \"\"\"\n    # Spike is emitted when the membrane potential crosses the threshold from below\n    (V &amp;gt; vt) and (prev_V &amp;lt;= vt)    \n    \"\"\",\n\n    reset = \"\"\"\n    # Nothing to do, it is built-in...\n    \"\"\"\n)\n\npop = Population(neuron=HH, geometry=1)\npop.V = -50.0\n\ncompile()\n\nm = Monitor(pop, ['spike', 'V', 'n', 'm', 'h'])\n\n# Preparation\nsimulate(100.0)\n# Current impulse for 1 ms\npop.I = 200.0\nsimulate(1.0)\n# Reset\npop.I = 0.0\nsimulate(100.0)\n\ndata = m.get()\n\ntstart = int(90.0/dt)\ntstop  = int(120.0/dt)\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\nplt.subplot(2,2,1)\nplt.plot(90.0 + dt*np.arange(tstop-tstart), data['V'][tstart:tstop, 0])\nplt.title('V')\nplt.subplot(2,2,2)\nplt.plot(90.0 + dt*np.arange(tstop-tstart), data['n'][tstart:tstop, 0])\nplt.title('n')\nplt.ylim((0.0, 1.0))\nplt.subplot(2,2,3)\nplt.plot(90.0 + dt*np.arange(tstop-tstart), data['m'][tstart:tstop, 0])\nplt.title('m')\nplt.ylim((0.0, 1.0))\nplt.subplot(2,2,4)\nplt.plot(90.0 + dt*np.arange(tstop-tstart), data['h'][tstart:tstop, 0])\nplt.title('h')\nplt.ylim((0.0, 1.0))\nplt.show()\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\nCompiling ...  OK \n</code>\n</pre>"},{"location":"example/HodgkinHuxley.html#hodgkin-huxley-neuron","title":"Hodgkin Huxley neuron","text":"<p>Download the Jupyter notebook : HodgkinHuxley.ipynb</p>"},{"location":"example/Hybrid.html","title":"Hybrid","text":"<p>Simple example showing hybrid spike/rate-coded networks. </p> <p>Reproduces Fig.4 of (Vitay, Dinkelbach and Hamker, 2015)</p> <pre><code>from ANNarchy import *\nclear()\n\nsetup(dt=0.1)\n\n# Rate-coded input neuron\ninput_neuron = Neuron(\n    parameters = \"baseline = 0.0\",\n    equations = \"r = baseline\"\n)\n# Rate-coded output neuron\nsimple_neuron = Neuron(\n    equations = \"r = sum(exc)\"\n)\n\n# Rate-coded population for input\npop1 = Population(geometry=1, neuron=input_neuron)\n\n# Poisson Population to encode\npop2 = PoissonPopulation(geometry=1000, target=\"exc\")\nproj = Projection(pop1, pop2, 'exc').connect_all_to_all(weights=1.)\n\n# Rate-coded population to decode\npop3 = Population(geometry=1000, neuron =simple_neuron)\nproj = DecodingProjection(pop2, pop3, 'exc', window=10.0)\n\ndef diagonal(pre, post, weights):\n\"\"\"\n    Simple connector pattern to progressively connect each post-synaptic neuron to a growing \n    number of pre-synaptic neurons.\n    \"\"\"\n    lil = CSR()\n    for rk_post in range(post.size):\n        lil.add(rk_post, range((rk_post+1)), [weights], [0] )\n    return lil\nproj.connect_with_func(method=diagonal, weights=1.)\n\ncompile()\n\n# Monitors\nm1 = Monitor(pop1, 'r')\nm2 = Monitor(pop2, 'spike')\nm3 = Monitor(pop3, 'r')\n\n# Simulate\nduration = 250.\n# 0 Hz\npop1.baseline = 0.0\nsimulate(duration)\n# 10 Hz\npop1.baseline = 10.0\nsimulate(duration)\n# 50 Hz\npop1.baseline = 50.0\nsimulate(duration)\n# 100 Hz\npop1.baseline = 100.0\nsimulate(duration)\n\n# Get recordings\ndata1 = m1.get()\ndata2 = m2.get()\ndata3 = m3.get()\n\n# Raster plot of the spiking population\nt, n = m2.raster_plot(data2['spike'])\n\n# Variance of the the decoded firing rate\ndata_10 = data3['r'][int(1.0*duration/dt()):int(2*duration/dt()), :]\ndata_50 = data3['r'][int(2.0*duration/dt()):int(3*duration/dt()), :]\ndata_100 = data3['r'][int(3.0*duration/dt()):int(4*duration/dt()), :]\nvar_10 = np.mean(np.abs((data_10 - 10.)/10.), axis=0)\nvar_50 = np.mean(np.abs((data_50 - 50.)/50.), axis=0)\nvar_100 = np.mean(np.abs((data_100 - 100.)/100.), axis=0)\n\n### Plot the results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(t, n, '.', markersize=0.5)\nplt.title('a) Raster plot')\nplt.xlabel('Time (ms)')\nplt.ylabel('# neurons')\nplt.xlim((0, 4*duration))\n\nplt.subplot(3,1,2)\nplt.plot(np.arange(0, 4*duration, 0.1), data1['r'][:, 0], label='Original firing rate')\nplt.plot(np.arange(0, 4*duration, 0.1), data3['r'][:, 999], label='Decoded firing rate')\nplt.legend(frameon=False, loc=2)\nplt.title('b) Decoded firing rate')\nplt.xlabel('Time (ms)')\nplt.ylabel('Activity (Hz)')\n\nplt.subplot(3,1,3)\nplt.plot(var_10, label='10 Hz')\nplt.plot(var_50, label='50 Hz')\nplt.plot(var_100, label='100 Hz')\nplt.legend(frameon=False)\nplt.title('c) Precision')\nplt.xlabel('# neurons used for decoding')\nplt.ylabel('Normalized error')\nplt.ylim((0,1))\n\nplt.show()\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\nCompiling ...  OK \n</code>\n</pre>"},{"location":"example/Hybrid.html#hybrid-network","title":"Hybrid network","text":"<p>Download the Jupyter notebook : Hybrid.ipynb</p>"},{"location":"example/Image.html","title":"Image","text":"<p>This simple example in <code>examples/image</code> demonstrates how to load images directly into the firing rates of a population and apply basic linear filters on it.</p> <p>It relies on the ANNarchy extensions <code>image</code> and <code>convolution</code> which must be explicitly imported:</p> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.image import *\nfrom ANNarchy.extensions.convolution import Convolution, Pooling\nclear()\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <p><code>ANNarchy.extensions.image</code> depends on the Python bindings of OpenCV, they must be installed before running the script.</p> <p>We first create an <code>ImagePopulation</code> that will load images:</p> <pre><code>image = ImagePopulation(geometry=(480, 640, 3))\n</code></pre> <p>Its geometry specifies the size of the images that can be loaded, here 640x480 RGB images. Note the geometry must be of the form (height, width, channels), where channels is 1 for grayscale images and 3 for color images.</p> <p>The next step is to reduce the size of the image, what can be done by using the <code>Pooling</code> class of the <code>convolution</code> extension. </p> <p>We define a dummy artificial neuron, whose firing rate <code>r</code> will simply be the sum of excitatory connections /ensured to be positive, but this should always be the case). We then create a smaller population <code>pooled</code> with this neuron type, and connect it to the <code>ImagePopulation</code> using mean-pooling:</p> <pre><code># Simple ANN\nLinearNeuron = Neuron(equations=\"r=sum(exc): min=0.0\")\n\n# Subsampling population\npooled = Population(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Mean-pooling projection\npool_proj = Pooling(pre=image, post=pooled, target='exc', operation='mean')\npool_proj.connect_pooling()\n</code></pre> <pre>\n<code>&lt;ANNarchy.extensions.convolution.Pooling.Pooling at 0x103bf0c10&gt;</code>\n</pre> <p>The <code>pooled</code> population reduces the size of the image by a factor ten (defined by the size of the population) by averaging the pixels values over 10x10 regions (<code>operation</code> is set to <code>'mean'</code>, but one could use <code>'max'</code> or <code>'min'</code>). The <code>connect_pooling()</code> connector creates the \"fake\" connection pattern (as no weights are involved).</p> <p>Let's apply now a 3x3 box filter on each channel of the pooled population:</p> <pre><code># Smoothing population\nsmoothed = Population(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Box filter projection\nbox_filter = np.ones((3, 3, 1))/9.\nsmooth_proj = Convolution(pre=pooled, post=smoothed, target='exc')\nsmooth_proj.connect_filter(weights=box_filter)\n</code></pre> <pre>\n<code>&lt;ANNarchy.extensions.convolution.Convolve.Convolution at 0x116a6db80&gt;</code>\n</pre> <p>To perform a convolution operation on the population (or more precisely a cross-correlation), we call the <code>connect_filter()</code> connector method of the <code>Convolution</code> projection. It requires to define a kernel (<code>weights</code>) that will be convolved over the input population. Here we use a simple box filter, but any filter can be used.</p> <p>As the <code>pooled</code> population has three dimensions and we want to smooth the activities per color channel, we need to define a (3, 3, 1) kernel. If we wanted to smooth also over the color channels, we could have used a (3, 3) filter: the resulting population would have the shape (48, 64).</p> <p>We now apply a bank of three filters, each selective to a particular color (red/green/blue). This filters do not have a spatial extent (1x1 convolution), but sum over the third dimension (the color channels):</p> <pre><code># Convolution population    \nfiltered = Population(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Red/Green/Blue filter bank\nfilter_bank = np.array([ \n    [[ [2.0, -1.0, -1.0] ]] , # Red filter \n    [[ [-1.0, 2.0, -1.0] ]] , # Blue filter\n    [[ [-1.0, -1.0, 2.0] ]]   # Green filter\n])\nfilter_proj = Convolution(pre=smoothed, post=filtered, target='exc')\nfilter_proj.connect_filters(weights=filter_bank)\n</code></pre> <pre>\n<code>&lt;ANNarchy.extensions.convolution.Convolve.Convolution at 0x103bd75e0&gt;</code>\n</pre> <p>Each of the three filter has the shape (1, 1, 3). The result of each convolution would then be (48, 64), but as there are three filters, the output population is (48, 64, 3). The last dimension does not correspond to the number of color channels, but to the number of filters in the bank: if you add a filter, the population will have to be (48, 64, 4).</p> <p>Banks of filters require to use <code>connect_filters()</code> instead of <code>connect_filter()</code>.</p> <pre><code>compile()\n</code></pre> <pre>\n<code>Compiling ...  OK \n</code>\n</pre> <p>After compilation, we can load an image into the input population:</p> <pre><code>image.set_image('test.jpg')\n</code></pre> <p>To see the result, we need to simulate for four time steps (4 milliseconds, as <code>dt=1.0</code>).</p> <ol> <li>Step 1: The <code>image</code> population loads the image.</li> <li>Step 2: The <code>pooled</code> population subsamples the image.</li> <li>Step 3: The <code>smoothed</code> population filters the pooled image.</li> <li>Step 4: The bank of filters are applied by <code>filtered</code>.</li> </ol> <pre><code>simulate(4.0)\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20.0, 20.0))\n\nplt.subplot(532)\nplt.imshow(image.r)\nplt.title('Original')\n\nplt.subplot(534)\nplt.imshow(image.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('image R')\nplt.subplot(535)\nplt.imshow(image.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('image G')\nplt.subplot(536)\nplt.imshow(image.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('image B')\n\nplt.subplot(537)\nplt.imshow(pooled.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('pooled R')\nplt.subplot(538)\nplt.imshow(pooled.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('pooled G')\nplt.subplot(539)\nplt.imshow(pooled.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('pooled B')\n\nplt.subplot(5, 3, 10)\nplt.imshow(smoothed.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('smoothed R')\nplt.subplot(5, 3, 11)\nplt.imshow(smoothed.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('smoothed G')\nplt.subplot(5, 3, 12)\nplt.imshow(smoothed.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('smoothed B')\n\nplt.subplot(5, 3, 13)\nplt.imshow(filtered.r[:,:,0], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('filtered R')\nplt.subplot(5, 3, 14)\nplt.imshow(filtered.r[:,:,1], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('filtered G')\nplt.subplot(5, 3, 15)\nplt.imshow(filtered.r[:,:,2], cmap='gray', interpolation='nearest', vmin= 0.0, vmax=1.0)\nplt.title('filtered B')\n\nplt.show()\n</code></pre>"},{"location":"example/Image.html#convolutions-amd-pooling","title":"Convolutions amd pooling","text":"<p>Download the Jupyter notebook : Image.ipynb</p>"},{"location":"example/Izhikevich.html","title":"Izhikevich","text":"<p>This script reproduces the simple pulse-coupled network proposed by Eugene Izhikevich in the article:</p> <p>&gt; Izhikevich, E.M. (2003). Simple Model of Spiking Neurons, IEEE Transaction on Neural Networks, 14:6.</p> <p>The original Matlab code is provided below:</p> <pre><code>% Created by Eugene M. Izhikevich, February 25, 2003\n% Excitatory neurons Inhibitory neurons\nNe = 800; Ni = 200;\nre = rand(Ne,1); ri = rand(Ni,1);\na = [0.02*ones(Ne,1); 0.02+0.08*ri];\nb = [0.2*ones(Ne,1); 0.25-0.05*ri];\nc = [-65+15*re.^2; -65*ones(Ni,1)];\nd = [8-6*re.^2; 2*ones(Ni,1)];\nS = [0.5*rand(Ne+Ni,Ne), -rand(Ne+Ni,Ni)];\nv = -65*ones(Ne+Ni,1); % Initial values of v\nu = b.*v; % Initial values of u\nfirings = []; % spike timings\nfor t=1:1000 % simulation of 1000 ms\nI = [5*randn(Ne,1);2*randn(Ni,1)]; % thalamic input\nfired = find(v&amp;gt;=30); % indices of spikes\nfirings = [firings; t+0*fired,fired];\nv(fired) = c(fired);\nu(fired) = u(fired) + d(fired);\nI = I + sum(S(:,fired),2);\nv = v + 0.5*(0.04*v.^2 + 5*v + 140 - u + I); % step 0.5 ms\nv = v + 0.5*(0.04*v.^2 + 5*v + 140-u + I); % for numerical\nu = u + a.*(b.*v - u); % stability\nend;\nplot(firings(:,1),firings(:,2),\u2019.\u2019)\n</code></pre> <p>The network is composed of parameterized quadratic integrate-and-fire neurons, known as Izhikevich neurons. They are simply defined by the following equations:</p> \\[     \\frac{dv}{dt} = 0.04 \\, v^2 + 5 \\, v + 140 - u + I  \\] \\[     \\frac{du}{dt} = a \\, (b \\, v - u) \\] <p>The spiking mechanism is defined by:</p> <pre><code>    if v &amp;gt; 30.0:\n        emit_spike()\n        v = c\n        u = u + d\n</code></pre> <p><code>v</code> is the membrane potential, <code>u</code> is the membrane recovery variable and <code>a</code>, <code>b</code>, <code>c</code>, <code>d</code> are parameters allowing to reproduce many types of neural firing.</p> <p><code>I</code> is the input voltage to a neuron at each time <code>t</code>. For the desired network, it is the sum of a random value taken from a normal distribution with mean 0.0 and variance 1.0 (multiplied by a scaling factor) and the net effect of incoming spikes (excitatory and inhibitory).</p> <p>Implementing such a neuron in ANNarchy is straightforward:</p> <pre><code>    Izhikevich = Neuron(\n        parameters=\"\"\"\n            noise = 5.0\n            a = 0.02\n            b = 0.2\n            c = -65.0\n            d = 2.0 \n            v_thresh = 30.0\n        \"\"\",\n        equations=\"\"\"\n            I = g_exc - g_inh + noise * Normal(0.0, 1.0)\n            dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I \n            du/dt = a * (b*v - u) \n        \"\"\",\n        spike = \"\"\"\n            v &amp;gt;= v_thresh\n        \"\"\",\n        reset = \"\"\"\n            v = c\n            u += d\n        \"\"\"\n    )\n</code></pre> <p>The parameters <code>a</code>, <code>b</code>, <code>c</code>, <code>d</code> as well as the noise amplitude <code>noise</code> are declared in the <code>parameters</code> argument, as their value is constant during the simulation. <code>noise</code> is declared as the same throughout the population with the <code>population</code> flag.</p> <p>The equations for <code>v</code> and <code>u</code> are direct translations of their mathematical counterparts. Note the use of <code>dx/dt</code> for the time derivative and <code>^2</code> for the square function.</p> <p>The input voltage <code>I</code> is defined as the sum of: </p> <ul> <li>the total conductance of excitatory synapses <code>g_exc</code>,</li> <li>the total conductance of inhibitory synapses <code>-g_inh</code> (in this example, we consider all weights to be positive, so we need to invert <code>g_inh</code> in order to model inhibitory synapses),</li> <li>a random number taken from the normal distribution \\(N(0,1)\\) and multiplied by the noise scale <code>noise</code>.</li> </ul> <p>In the pulse-coupled network, synapses are considered as instantaneous, i.e. a pre-synaptic spikes increases immediately the post-synaptic conductance proportionally to the weight of the synapse, but does not leave further trace. As this is the default behavior in ANNarchy, nothing has to be specified in the neuron's equations.</p> <p>The <code>spike</code> argument specifies the condition for when a spike should be emitted (here the membrane potential <code>v</code> should be greater than <code>v_thresh</code>). The <code>reset</code> argument specifies the changes to neural variables that should occur after a spike is emitted: here, the membrane potential is reset to the resting potential <code>c</code> and the membrane recovery variable <code>u</code> is increased from <code>d</code>.</p> <p>The <code>Izhikevich</code> neuron is already defined in ANNarchy, so we will use it directly.</p> <p>We start by defining a population of 1000 Izhikevich neurons and split it into 800 excitatory neurons and 200 inhibitory ones:</p> <pre><code>from ANNarchy import *\nclear()\n\npop = Population(geometry=1000, neuron=Izhikevich)\nExc = pop[:800]\nInh = pop[800:]\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <p><code>Exc</code> and <code>Inh</code> are subsets of <code>pop</code>, which have the same properties as a population. We can then set parameters differently for each population:</p> <pre><code>re = np.random.random(800)      ; ri = np.random.random(200)\nExc.noise = 5.0                 ; Inh.noise = 2.0\nExc.a = 0.02                    ; Inh.a = 0.02 + 0.08 * ri\nExc.b = 0.2                     ; Inh.b = 0.25 - 0.05 * ri\nExc.c = -65.0 + 15.0 * re**2    ; Inh.c = -65.0\nExc.d = 8.0 - 6.0 * re**2       ; Inh.d = 2.0\nExc.v = -65.0                   ; Inh.v = -65.0\nExc.u = Exc.v * Exc.b           ; Inh.u = Inh.v * Inh.b\n</code></pre> <p>We can now define the connections within the network:</p> <ol> <li>The excitatory neurons are connected to all neurons with a weight randomly chosen in [0, 0.5]</li> <li>The inhibitory neurons are connected to all neurons with a weight randomly chosen in [0, 1]</li> </ol> <pre><code>exc_proj = Projection(pre=Exc, post=pop, target='exc')\nexc_proj.connect_all_to_all(weights=Uniform(0.0, 0.5))\n\ninh_proj = Projection(pre=Inh, post=pop, target='inh')\ninh_proj.connect_all_to_all(weights=Uniform(0.0, 1.0))\n</code></pre> <pre>\n<code>&lt;ANNarchy.core.Projection.Projection at 0x1184bf310&gt;</code>\n</pre> <p>The network is now ready, we can compile:</p> <pre><code>compile()\n</code></pre> <pre>\n<code>Compiling ...  OK \n</code>\n</pre> <p>We start by monitoring the spikes and membrane potential in the whole population:</p> <pre><code>M = Monitor(pop, ['spike', 'v'])\n</code></pre> <p>We run the simulation for 1000 milliseconds:</p> <pre><code>simulate(1000.0, measure_time=True)\n</code></pre> <pre>\n<code>Simulating 1.0 seconds of the network took 0.049089908599853516 seconds. \n</code>\n</pre> <p>We retrieve the recordings, generate a raster plot and the population firing rate:</p> <pre><code>spikes = M.get('spike')\nv = M.get('v')\nt, n = M.raster_plot(spikes)\nfr = M.histogram(spikes)\n</code></pre> <p>We plot:</p> <ol> <li>The raster plot of population</li> <li>The evolution of the membrane potential of a single excitatory neuron</li> <li>The population firing rate</li> </ol> <pre><code>import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(12, 12))\n\n# First plot: raster plot\nplt.subplot(311)\nplt.plot(t, n, 'b.')\nplt.title('Raster plot')\n\n# Second plot: membrane potential of a single excitatory cell\nplt.subplot(312)\nplt.plot(v[:, 15]) # for example\nplt.title('Membrane potential')\n\n# Third plot: number of spikes per step in the population.\nplt.subplot(313)\nplt.plot(fr)\nplt.title('Number of spikes')\nplt.xlabel('Time (ms)')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"example/Izhikevich.html#izhikevichs-pulse-coupled-network","title":"Izhikevich's pulse-coupled network","text":"<p>Download the Jupyter notebook : Izhikevich.ipynb</p>"},{"location":"example/Izhikevich.html#neuron-type","title":"Neuron type","text":""},{"location":"example/Izhikevich.html#defining-the-populations","title":"Defining the populations","text":""},{"location":"example/Izhikevich.html#defining-the-projections","title":"Defining the projections","text":""},{"location":"example/Izhikevich.html#running-the-simulation","title":"Running the simulation","text":""},{"location":"example/List.html","title":"List of notebooks","text":"<p>This section provides a list of the sample models provided in the <code>examples/</code> directory of the source code. </p> <p>The Jupyter notebooks can be downloaded from:</p> <p>https://github.com/ANNarchy/ANNarchy.github.io/tree/master/docs/example</p>"},{"location":"example/List.html#rate-coded-networks","title":"Rate-coded networks","text":"<ul> <li>Neural Field: a simple model using neural field recurrent networks. This is a very simple rate-coded model without learning.</li> <li>Bar learning: an implementation of the bar learning problem, illustrating synaptic plasticity in rate-coded networks.</li> <li>Structural Plasticity: a dummy example using structural plasticity.</li> </ul>"},{"location":"example/List.html#spiking-networks","title":"Spiking networks","text":"<p>Simple networks</p> <ul> <li>Izhikevich: an implementation of the simple pulse-coupled network described in (Izhikevich, 2003). It shows how to build a simple spiking network without synaptic plasticity.</li> <li>Gap Junctions: an example using gap junctions.</li> <li>HodgkinHuxley: a single Hodgkin-Huxley neuron.</li> </ul> <p>Complex networks</p> <ul> <li>COBA: an implementation of the balanced network described in (Vogels and Abbott, 2005). It     shows how to build a simple spiking network using integrate-and-fire neurons and sparse connectivity.</li> <li>STP: an example of short-term plasticity based on the model of Tsodyks, Uziel and Markram (2000). </li> </ul> <p>With synaptic plasticity</p> <ul> <li>STDP: a simple example using spike-timing dependent plasticity (STDP).</li> <li>Ramp: an example of homeostatic STDP based on the model of Carlson, Richert, Dutt and     Krichmar (2013)</li> </ul>"},{"location":"example/List.html#hybrid-networks","title":"Hybrid networks","text":"<ul> <li>Hybrid networks: a simple hybrid network with both rate-coded and spiking sub-parts.</li> </ul>"},{"location":"example/List.html#extensions","title":"Extensions","text":"<ul> <li>Image and Webcam: shows how to use the <code>ImagePopulation</code> and <code>VideoPopulation</code> classes of the <code>image</code> extension to clamp directly images and video streams into a rate-coded network. Also demonstrates the <code>convolution</code> extension.</li> <li>Parallel simulations: shows how to call <code>parallel_run</code> to run several networks in parallel.</li> <li>Bayesian optimization: a demo showing how to use <code>hyperopt</code> to search for hyperparameters of a  model.</li> <li>Logging with tensorboard: a simple basal ganglia model to show how to use the <code>tensorboard</code> extension.</li> <li>BOLD monitoring: a showcase of the <code>bold</code> extension allowing to record BOLD signals fron a network.</li> </ul>"},{"location":"example/MultipleNetworks.html","title":"Parallel simulations","text":"<p>This example demonstrates the use of <code>parallel_run()</code> to simulate the same network multiple times in parallel. </p> <p>We start by creating the Izhikevich pulse-coupled network defined in Izhikevich.ipynb.</p> <pre><code>from ANNarchy import *\nclear()\n\n# Create the whole population\nP = Population(geometry=1000, neuron=Izhikevich)\n\n# Create the excitatory population\nExc = P[:800]\nre = np.random.random(800)\nExc.noise = 5.0\nExc.a = 0.02\nExc.b = 0.2\nExc.c = -65.0 + 15.0 * re**2\nExc.d = 8.0 - 6.0 * re**2\nExc.v = -65.0\nExc.u = Exc.v * Exc.b\n\n# Create the Inh population\nInh = P[800:]\nri = np.random.random(200)\nInh.noise = 2.0\nInh.a = 0.02 + 0.08 * ri\nInh.b = 0.25 - 0.05 * ri\nInh.c = -65.0\nInh.d = 2.0\nInh.v = -65.0\nInh.u = Inh.v * Inh.b\n\n# Create the projections\nproj_exc = Projection(Exc, P, 'exc')\nproj_inh = Projection(Inh, P, 'inh')\n\nproj_exc.connect_all_to_all(weights=Uniform(0.0, 0.5))\nproj_inh.connect_all_to_all(weights=Uniform(0.0, 1.0))\n\n# Create a spike monitor\nM = Monitor(P, 'spike')\n\ncompile()\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <p>We define a simulation method that re-initializes the network, runs a simulation and returns a raster plot. </p> <p>The simulation method must take an index as first argument and a <code>Network</code> instance as second one.</p> <pre><code>def run_network(idx, net):\n    # Retrieve subpopulations\n    P_local = net.get(P)\n    Exc = P_local[:800]\n    Inh = P_local[800:]\n    # Randomize initialization\n    re = np.random.random(800)\n    Exc.c = -65.0 + 15.0 * re**2\n    Exc.d = 8.0 - 6.0 * re**2\n    ri = np.random.random(200)\n    Inh.noise = 2.0\n    Inh.a = 0.02 + 0.08 * ri\n    Inh.b = 0.25 - 0.05 * ri\n    Inh.u = Inh.v * Inh.b\n    # Simulate\n    net.simulate(1000.)\n    # Recordings\n    t, n = net.get(M).raster_plot()\n    return t, n\n</code></pre> <p><code>parallel_run()</code> uses the <code>multiprocessing</code> module to start parallel processes. On Linux, it should work directly, but there is an issue on OSX. Since Python 3.8, the 'spawn' method is the default way to start processes, but it does not work on MacOS. The following cell should fix the issue, but it should only be ran once.</p> <pre><code>import platform\nif platform.system() == \"Darwin\":\n    import multiprocessing as mp\n    mp.set_start_method('fork')\n</code></pre> <p>We can now call <code>parallel_run()</code> to simulate 8 identical but differently initialized networks. The first call runs the simulations sequentially, while the second is in parallel.</p> <p>We finally plot the raster plots of the two first simulations.</p> <pre><code># Run four identical simulations sequentially\nvals = parallel_run(method=run_network, number=8, measure_time=True, sequential=True)\n\n# Run four identical simulations in parallel\nvals = parallel_run(method=run_network, number=8, measure_time=True)\n\n# Data analysis\nt1, n1 = vals[0]\nt2, n2 = vals[1]\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.plot(t1, n1, '.')\nplt.subplot(122)\nplt.plot(t2, n2, '.')\nplt.show()\n</code></pre> <pre>\n<code>Running 8 networks sequentially took: 1.3284821510314941 \nRunning 8 networks in parallel took: 0.4362208843231201 \n</code>\n</pre>"},{"location":"example/MultipleNetworks.html#parallel-simulations","title":"Parallel simulations","text":"<p>Download the Jupyter notebook : MultipleNetworks.ipynb</p>"},{"location":"example/NeuralField.html","title":"Neural Field","text":"<p>The folder <code>examples/neural_field</code> contains a simple rate-coded model using Neural Fields. It consists of two 2D populations <code>inp</code> and <code>focus</code>, with one-to-one connections between <code>inp</code> and <code>focus</code>, and Difference-of-Gaussians (DoG) lateral connections within <code>focus</code>.</p> <p>Each population consists of N*N neurons, with N=20. The <code>inp</code> population is solely used to represent inputs for <code>focus</code>. The firing rate of each neuron is defined by a simple equation:</p> \\[r_i(t) = (\\text{baseline}_i(t) + \\eta(t))^+\\] <p>where \\(r_i(t)\\) is the instantaneous firing rate, \\(\\text{baseline}_i(t)\\) its baseline activity, \\(\\eta(t)\\) an additive noise uniformly taken in \\([-0.5, 0.5]\\) and \\(()^+\\) the positive function.</p> <p>The <code>focus</code> population implements a discretized neural field, with neurons following the ODE:</p> \\[\\tau \\frac{d r_i(t)}{dt} + r_i(t) = r^\\text{input}_i(t) + \\sum_{j=1}^{N} w_{j, i} \\cdot r_j(t) + \\eta(t)\\] <p>where \\(r_i(t)\\) is the neuron's firing rate, \\(\\tau\\) a time constant and \\(w_{j, i}\\) the weight value (synaptic efficiency) of the synapse between the neurons j and i. \\(f()\\) is a semi-linear function, ensuring the firing rate is bounded between 0 and 1.</p> <p>Each neuron in <code>focus</code> takes inputs from the neuron of <code>inp</code> which has the same position, leading to a <code>one_to_one</code> connection pattern.</p> <p>The lateral connections within <code>focus</code> follow a difference-of-Gaussians (<code>dog</code>) connection pattern, with the connection weights \\(w_{i,j}\\) depending on the normalized euclidian distance between the neurons in the N*N population:</p> \\[w_{j, i} = A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_+^2}) -  A^- \\cdot \\exp(-\\frac{1}{2}\\frac{d(i, j)^2}{\\sigma_-^2})\\] <p>If i and j have coordinates \\((x_i, y_i)\\) and \\((x_j, y_j)\\) in the N*N space, the distance between them is computed as:</p> \\[d(i, j)^2 = (\\frac{x_i - x_j}{N})^2 + (\\frac{y_i - y_j}{N})^2\\] <p>Inputs are given to the network by changing the baseline of <code>inp</code> neurons. This example clamps one or several gaussian profiles (called \"bubbles\") with an additive noise, moving along a circular path at a certain speed (launch the example to understand this sentence...).</p> <p>The beginning of the script solely consists of importing the ANNarchy library:</p> <pre><code>from ANNarchy import *\nclear()\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <p>If you want to run the simulation on your graphic card instead of CPU, simply uncomment the following line: </p> <pre><code>#setup(paradigm=\"cuda\")\n</code></pre> <p>The <code>setup()</code> method allows to configure ANNarchy to run in different modes, such as chosing the parallel framework (omp or cuda), setting the simulation step <code>dt</code>, the numerical method <code>method</code> or the <code>seed</code> of the random number generators.</p> <pre><code>InputNeuron = Neuron(   \n    parameters=\"\"\"\n        baseline = 0.0\n    \"\"\",\n    equations=\"\"\"\n        r = pos(baseline + Uniform(-0.5, 0.5))\n    \"\"\" \n)\n</code></pre> <p>Defining the input neuron is straightforward. <code>InputNeuron</code> is here an instance of <code>Neuron</code>, whose only parameter is <code>baseline</code> (initialized to 0.0, but it does not matter here as it will be set externally). </p> <p>The firing rate of each neuron, <code>r</code>, is updated at every time step as the positive part (<code>pos()</code>) of the sum of the baseline and a random number taken from a uniform distribution between -0.5 and 0.5.</p> <pre><code>NeuralFieldNeuron = Neuron(\n    parameters=\"\"\" \n        tau = 10.0 : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dr/dt + r = sum(exc) + sum(inh) + Uniform(-0.5, 0.5) : min=0.0, max=1.0\n    \"\"\"\n)\n</code></pre> <p>The second neuron we need is a bit more complex, as it is governed by an ODE and considers inputs from other neurons. It also has a non-linear activation function, which is linear when the firing rate is between 0.0 and 1.0, and constant otherwise.</p> <p><code>tau</code> is a population-wise parameter, whose value will be the same for all neuron of the population. </p> <p><code>r</code> is the firing rate of he neuron, whose dynamics are governed by a first-order linear ODE, integrating the sums of excitatory and inhibitory inputs with noise. </p> <p>As explained in the manual for rate-coded neurons, <code>sum(exc)</code> retrieves the weighted sum of pre-synaptic firing rates for the synapses having the connection type <code>exc</code>, here the one_to_one connections between <code>inp</code> and <code>focus</code>. <code>sum(inh)</code> does the same for <code>inh</code> type connections, here the lateral connections within <code>focus</code>.</p> <p>The firing rate is restricted to the range [0, 1] by setting the <code>min</code> and <code>max</code> accordingly in the flags section (everything after the <code>:</code>). This means that after evaluating the ODE and getting a new value for <code>r</code>, its value will be clamped if it outside these values. One can define both <code>min</code> and <code>max</code>, only one, or none.  </p> <p>The two populations have a geometry of (20, 20), therefore 400 neurons each. They are created simply by instantiating the <code>Population</code> class:</p> <pre><code>N = 20\ninp = Population(geometry = (N, N), neuron = InputNeuron, name='Input')\nfocus = Population(geometry = (N, N), neuron = NeuralFieldNeuron, name='Focus')\n</code></pre> <p>The populations can be assigned a unique name (here 'Input' and 'Focus') in order to be be able to retrieve them if the references <code>inp</code> and <code>focus</code> are lost. They are given a 2D geometry and associated to the corresponding <code>Neuron</code> instance.</p> <p>The first projection is a one-to-one projection from Input to Focus with the type 'exc'. This connection pattern pattern is possible because the two populations have the same geometry. The weights are initialized to 1.0, and this value will not change with time (no learning), so it is not necessary to define a synapse type:</p> <pre><code>ff = Projection(pre=inp, post=focus, target='exc')\nff.connect_one_to_one(weights=1.0, delays = 20.0)\n</code></pre> <pre>\n<code>&lt;ANNarchy.core.Projection.Projection at 0x124256700&gt;</code>\n</pre> <p>The references to the pre- and post-synaptic population (or their names), as well as the target type, are passed to the constructor of <code>Projection</code>. The connector method <code>connect_one_to_one()</code> is immediately applied to the Projection, defining how many synapses will be created. The weights are initialized uniformly to 1.0.</p> <p>The second projection is a difference of gaussians (DoG) for the lateral connections within 'focus'. The connector method is already provided by ANNarchy, so there is nothing more to do than to call it with the right parameters:</p> <pre><code>lat = Projection(pre=focus, post=focus, target='inh')\nlat.connect_dog(amp_pos=0.2, sigma_pos=0.1, amp_neg=0.1, sigma_neg=0.7)\n</code></pre> <pre>\n<code>&lt;ANNarchy.core.Projection.Projection at 0x124256af0&gt;</code>\n</pre> <p>Once the populations and projections are created, the network is ready to be generated, compiled and simulated. Compilation is simply done by calling <code>compile()</code>:</p> <pre><code>compile()\n</code></pre> <pre>\n<code>Compiling ...  OK \n</code>\n</pre> <p>This generates optimized C++ code from the neurons' definition and network structure, compiles it with gcc/clang and instantiates all objects, particularly the synapses. If some errors were made in the neuron definition, they will be signaled at this point.</p> <p>Hint: The call to <code>compile()</code> is mandatory in any script. After it is called, populations and projections can not be added anymore.</p> <p>Once the compilation is successful, the network can be simulated by calling <code>simulate()</code>:</p> <pre><code>simulate(1000.0) # simulate for 1 second\n</code></pre> <p>As no input has been fed into the network, calling <code>simulate()</code> now won't lead to anything interesting. The next step is to clamp inputs into the input population's baseline.</p> <p>In this example, we consider as input a moving bubble of activity rotating along a circle in the input space in 5 seconds. A naive way of setting such inputs would be to access population attributes (namely <code>inp.baseline</code>) in a tight loop in Python:</p> <pre><code>angle = 0.0\nx, y = np.meshgrid(np.linspace(0, 19, 20), np.linspace(0, 19, 20))\n\n# Main loop\nwhile True:\n    # Update the angle\n    angle += 1.0/5000.0\n    # Compute the center of the bubble\n    cx = 10.0 * ( 1.0 + 0.5 * np.cos(2.0 * np.pi * angle ) )\n    cy = 10.0 * ( 1.0 + 0.5 * np.sin(2.0 * np.pi * angle ) )\n    # Clamp the bubble into pop.baseline\n    inp.baseline = (np.exp(-((x-cx)**2 + (y-cy)**2)/8.0))\n    # Simulate for 1 ms\n    step()  \n</code></pre> <p><code>angle</code> represents the angle made by the bubble with respect to the center of the input population. <code>x</code> and <code>y</code> are Numpy arrays representing the X- and Y- coordinates of neurons in the input population. At each iteration of the simulation (i.e. every millisecond of simulation, the bubble is slightly rotated (<code>angle</code> is incremented) so as to make a complete revolution in 5 seconds (5000 steps). <code>cx</code> and <code>cy</code> represent the coordinates of the center of the bubble in neural coordinates according to the new value of the angle.</p> <p>A Gaussian profile (in the form of a Numpy array) is then clamped into the baseline of <code>inp</code> using the distance between each neuron of the population (<code>x</code> and <code>y</code>) and the center of the bubble. Last, a single simulation step is performed using <code>step()</code>, before the whole process starts again until the user quits. <code>step()</code> is equivalent to <code>simulate(1)</code>, although a little bit faster as it does not check anything.</p> <p>Although this approach works, you would observe that it is very slow: the computation of the bubble and its feeding into <code>InputPop</code> takes much more time than the call to <code>step()</code>. The interest of using a parallel simulator disappears. This is due to the fact that Python is knowingly bad at performing tight loops because of its interpreted nature. If the <code>while</code> loop were compiled from C code, the computation would be much more efficient. This is what Cython brings you.</p> <p>Generalities on Cython</p> <p>The Cython approach requires to write Cython-specific code in a <code>.pyx</code> file, generate the corresponding C code with Python access methods, compile it and later import it into your Python code.</p> <p>Happily, the Cython syntax is very close to Python. In the most basic approach, it is simply Python code with a couple of type declarations. Instead of:</p> <pre><code>bar = 1\nfoo = np.ones((10, 10))\n</code></pre> <p>you would write in Cython:</p> <pre><code>cdef int bar = 1\ncdef np.ndarray foo = np.ones((10, 10))\n</code></pre> <p>By specifing the type of a variable (which can not be changed later contrary to Python), you help Cython generate optimized C code, what can lead in some cases to speedups up to 100x. The rest of the syntax (indentation, for loops, if...) is the same as in Python. </p> <p>You can als import any Python module in your Cython code. Some modules (importantly Numpy) even provide a Cython interface where the equivalent Cython code can be directly imported (so it becomes very fast to use).</p> <p>The whole compilation procedure is very easy. One particularly simple approach is to use the <code>pyximport</code> module shipped with Cython. Let us suppose you wrote a <code>dummy()</code> method in a Cython file named <code>TestModule.pyx</code>. All you need to use this method in your python code is to write:</p> <pre><code>import pyximport; pyximport.install()\nfrom TestModule import dummy\ndummy()\n</code></pre> <p><code>pyximport</code> takes care of the compilation process (but emits quite a lot of warnings that can be ignored), and allows to import <code>TestModule</code> as if it were a regular Python module. Please refer to the Cython documentation to know more.</p> <p>Moving bubbles in Cython</p> <p>The file <code>BubbleWorld.pyx</code> defines a <code>World</code> class able to rotate the bubble for a specified duration.</p> <pre><code>import numpy as np\ncimport numpy as np\n</code></pre> <p>At the beginning of the file, numpy is imported once as a normal Python module with <code>import</code>, and once as a Cython module with <code>cimport</code>. This allows our Cython module to access directly the internal representations of Numpy without going through the Python interpreter. </p> <p>We can then define a <code>World</code> class taking as parameters:</p> <ul> <li>the population which will be used as input (here <code>Input</code>),</li> <li>several arguments such as <code>radius</code>, <code>sigma</code> and <code>period</code> which allow     to parameterize the behavior of the rotating bubble,</li> <li><code>func</code> which is the Python method that will be called at each time step, i.e.e the <code>step()</code> method of ANNarchy.</li> </ul> <pre><code>cdef class World:\n    \" Environment class allowing to clamp a rotating bubble into the baseline of a population.\"\n\n    cdef pop # Input population\n    cdef func # Function to call\n\n    cdef float angle # Current angle\n    cdef float radius # Radius of the circle \n    cdef float sigma # Width of the bubble\n    cdef float period # Number of steps needed to make one revolution\n\n    cdef np.ndarray xx, yy # indices\n    cdef float cx, cy, midw, midh\n    cdef np.ndarray data \n\n    def __cinit__(self, population, radius, sigma, period, func):\n        \" Constructor\"\n        self.pop = population\n        self.func=func\n        self.angle = 0.0\n        self.radius = radius\n        self.sigma = sigma\n        self.period = period\n        cdef np.ndarray x = np.linspace(0, self.pop.geometry[0]-1, self.pop.geometry[0])\n        cdef np.ndarray y = np.linspace(0, self.pop.geometry[1]-1, self.pop.geometry[1])\n        self.xx, self.yy = np.meshgrid(x, y)\n        self.midw = self.pop.geometry[0]/2\n        self.midh = self.pop.geometry[1]/2\n\n    def rotate(self, int duration):\n        \" Rotates the bubble for the given duration\"\n        cdef int t\n        for t in xrange(duration):\n            # Update the angle\n            self.angle += 1.0/self.period\n            # Compute the center of the bubble\n            self.cx = self.midw * ( 1.0 + self.radius * np.cos(2.0 * np.pi * self.angle ) )\n            self.cy = self.midh * ( 1.0 + self.radius * np.sin(2.0 * np.pi * self.angle ) )\n            # Create the bubble\n            self.data = (np.exp(-((self.xx-self.cx)**2 + (self.yy-self.cy)**2)/2.0/self.sigma**2))\n            # Clamp the bubble into pop.baseline\n            self.pop.baseline = self.data\n            # Simulate for 1 step\n            self.func()  \n</code></pre> <p>Although this tutorial won't go into much detail, you can note the following:</p> <ul> <li>The data given to or initialized in the constructor are previously     declared (with their type) as attributes of the class. This way,     Cython knows at the compilation time which operations are possible     on them, which amount of memory to allocate and so on, resulting in     a more efficient implementation.</li> <li>The input population (<code>self.pop</code>) can be accessed as a normal     Python object. In particular, self.pop.geometry is used in the     constructor to initialize the meshgrid.</li> <li>The method <code>rotate()</code> performs the simulation for the given duration     (in steps, not milliseconds). Its content is relatively similar to     the Python version.</li> </ul> <p>Running the simulation</p> <p>Once the environment has been defined, the simulation can be executed. The following code, to be placed after the network definition, performs a simulation of the network, taking inputs from <code>BubbleWorld.pyx</code>, during 2 seconds:</p> <pre><code># Create the environment\nimport pyximport; pyximport.install(setup_args={'include_dirs': np.get_include()})\nfrom BubbleWorld import World\nworld = World(population=inp, radius=0.5, sigma=2.0, period=5000.0, func=step)\n\n# Simulate for 2 seconds with inputs\nworld.rotate(2000)\n</code></pre> <p>The preceding code performs correctly the intended simulation, but nothing is visualized. The user has all freedom to visualize his network the way he prefers (for example through animated Matplotlib figures):</p> <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\nplt.subplot(121)\nplt.imshow(inp.r, interpolation='nearest', cmap=plt.cm.gray)\nplt.subplot(122)\nplt.imshow(focus.r, interpolation='nearest', cmap=plt.cm.gray)\nplt.show()\n</code></pre> <p>However, Matplotlib animations are rather slow, and visualizing the network at each time step would take more time than running the simulation. The provided example takes advantage of the PyQtGraph library (www.pyqtgraph.org) to visualize efficiently activity in the network using OpenGL.</p> <p>The following class and method is defined in <code>Viz.py</code>:</p> <pre><code># Visualizer using PyQtGraph\ntry:\n    from pyqtgraph.Qt import QtGui, QtCore\n    import pyqtgraph as pg\nexcept:\n    print('PyQtGraph is not installed on your system, can not visualize the network.')\n    exit(0)\ntry:\n    import pyqtgraph.opengl as gl\nexcept:\n    print('OpenGL is not installed on your system, can not visualize the network.')\n    exit(0)\n\nimport numpy as np\n\nclass GLViewer(object):\n    \" Class to visualize the network activity using PyQtGraph and openGL.\"\n    def __init__(self, populations, func, update_rate): \n        # Parameters   \n        self.populations = populations\n        self.func = func    \n        self.update_rate = update_rate\n        # Window\n        self.win = gl.GLViewWidget()\n        self.win.show()\n        self.win.setCameraPosition(distance=40)\n        # Prepare the plots\n        self.plots = []\n        shift = 0\n        for pop in self.populations: \n            p = gl.GLSurfacePlotItem(\n                x = np.linspace(0, pop.geometry[0]-1, pop.geometry[0]), \n                y = np.linspace(0, pop.geometry[1]-1, pop.geometry[1]), \n                shader='heightColor', \n                computeNormals=False, \n                smooth=False\n            )\n            p.translate(shift, -10, -1)\n            self.win.addItem(p)\n            self.plots.append(p)\n            shift -= 25\n\n    def scale(self, data):\n        \" Colors are shown in the range [-1, 1] per default.\"\n        return 1.8 * data -0.9\n\n    def update(self):\n        \"Callback\"\n        # Simulate for 200ms\n        self.func(self.update_rate)     \n        # Refresh the GUI\n        for i in range(len(self.populations)):\n            self.plots[i].setData(z=self.scale(self.populations[i].r)) \n        # Listen to mouse/keyboard events\n        QtGui.QApplication.processEvents()\n\n    def run(self):\n        \"Inifinite loop\"\n        timer = QtCore.QTimer()\n        timer.timeout.connect(self.update)\n        timer.start(0)  \n        QtGui.QApplication.instance().exec_() \n\n\ndef loop_bubbles(populations, func, update_rate):\n    \"Launches the GL GUI and rotates the bubble infinitely.\"\n    # Create the GUI using PyQtGraph\n    app = QtGui.QApplication([])\n    viewer = GLViewer(populations, func, update_rate)\n    # Start the simulation forever          \n    viewer.run()\n</code></pre> <p>We leave out again the details about this class (please look at the examples and tutorials on the PyQtGraph website to understand it). It allows to open a PyQtGraph window and display the firing rate of both <code>Input</code> and <code>Focus</code> populations using OpenGL. The <code>run()</code> method is an endless loop calling regularly the <code>update()</code> method.</p> <p>The <code>update()</code> method calls first <code>World.rotate(200)</code> and waits for its completion before reactualizing the display. The reason is that refreshing the display can only be done sequentially with the simulation, and calling it too often would impair the simulation time.</p> <p>Once this class has been defined, the simulation can be run endlessly by importing the <code>Viz</code> module:</p> <pre><code># Launch the GUI and run the simulation\nfrom Viz import loop_bubbles\nloop_bubbles(populations = [inp, focus], func=world.rotate, update_rate=200)\n</code></pre>"},{"location":"example/NeuralField.html#neural-field","title":"Neural Field","text":"<p>Download the Jupyter notebook : NeuralField.ipynb</p>"},{"location":"example/NeuralField.html#model-overview","title":"Model overview","text":""},{"location":"example/NeuralField.html#importing-annarchy","title":"Importing ANNarchy","text":""},{"location":"example/NeuralField.html#defining-the-neurons","title":"Defining the neurons","text":""},{"location":"example/NeuralField.html#input-neuron","title":"Input neuron","text":""},{"location":"example/NeuralField.html#neural-field-neuron","title":"Neural Field neuron","text":""},{"location":"example/NeuralField.html#creating-the-populations","title":"Creating the populations","text":""},{"location":"example/NeuralField.html#creating-the-projections","title":"Creating the projections","text":""},{"location":"example/NeuralField.html#compiling-the-network-and-simulating","title":"Compiling the network and simulating","text":""},{"location":"example/NeuralField.html#setting-inputs","title":"Setting inputs","text":""},{"location":"example/NeuralField.html#pure-python-approach","title":"Pure Python approach","text":""},{"location":"example/NeuralField.html#cython-approach","title":"Cython approach","text":""},{"location":"example/NeuralField.html#visualizing-the-network","title":"Visualizing the network","text":""},{"location":"example/Ramp.html","title":"Ramp","text":"<p>This example in <code>examples/homeostatic_stdp</code> is a reimplementation of the mechanism described in:</p> <p>&gt; Carlson, K.D.; Richert, M.; Dutt, N.; Krichmar, J.L., \"Biologically plausible models of homeostasis and STDP: Stability and learning in spiking neural networks,\" in Neural Networks (IJCNN), The 2013 International Joint Conference on , vol., no., pp.1-8, 4-9 Aug. 2013. doi: 10.1109/IJCNN.2013.6706961</p> <p>It is based on the corresponding Carlsim tutorial:</p> <p>http://www.socsci.uci.edu/~jkrichma/CARLsim/doc/tut3_plasticity.html</p> <p>This noteboob focuses on the simple \"Ramp\" experiment (<code>Ramp.py</code>), but the principle is similar for the self-organizing receptive fileds (SORF) one (<code>SORF.py</code>).</p> <pre><code>from ANNarchy import *\nclear()\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <p>The network uses regular-spiking Izhikevich neurons (see the <code>Izhikevich</code> notebook), but using exponentially-decaying conductances and NMDA synapses:</p> <pre><code>RSNeuron = Neuron(\n    parameters = \"\"\"\n        a = 0.02 : population\n        b = 0.2 : population\n        c = -65. : population\n        d = 8. : population\n        tau_ampa = 5. : population\n        tau_nmda = 150. : population\n        vrev = 0.0 : population\n    \"\"\" ,\n    equations=\"\"\"\n        # Inputs\n        I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)  \n        # Membrane potential and recovery variable are solved using the midpoint method for stability     \n        dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint\n        du/dt = a * (b*v - u) : init=-13., midpoint\n        # AMPA and NMDA conductances\n        tau_ampa * dg_ampa/dt = -g_ampa : exponential\n        tau_nmda * dg_nmda/dt = -g_nmda : exponential\n    \"\"\" , \n    spike = \"\"\"\n        v &amp;gt;= 30.\n    \"\"\", \n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\"\n)\n</code></pre> <p>The main particularity about NMDA synaptic models is that a single synaptic connection influences two conductances:</p> <p>1) The AMPA conductance, which primarily drives the post-synaptic neuron:</p> \\[     I_\\text{AMPA} = g_\\text{AMPA} \\times (V_\\text{rev} - V) \\] <p>2) The NMDA conductance, which is non-linearly dependent on the membrane potential:</p> \\[     I_\\text{NMDA} = g_\\text{NMDA} \\times \\frac{(\\frac{V - V_\\text{NMDA}}{\\sigma})^2}{1 + (\\frac{V - V_\\text{NMDA}}{\\sigma})^2} \\times (V_\\text{rev} - V) \\] <p>In short, the NMDA conductance only increases if the post-synaptic neuron is already depolarized.</p> <p>The <code>nmda</code> function is defined in the <code>functions</code> argument for readability. The parameters \\(V_\\text{NMDA} =-80 \\text{mV}\\) and \\(\\sigma = 60 \\text{mV}\\) are here hardcoded in the equation, but they could be defined as global parameters.</p> <p>The AMPA and NMDA conductances are exponentially decreasing with different time constants:</p> \\[     \\tau_\\text{AMPA} \\frac{dg_\\text{AMPA}(t)}{dt} + g_\\text{AMPA}(t) = 0 $$ $$     \\tau_\\text{NMDA} \\frac{dg_\\text{NMDA}(t)}{dt} + g_\\text{NMDA}(t) = 0 \\] <p>Another thing to notice in this neuron model is that the differential equations for the membrane potential and recovery variable are solved concurrently using the midpoint numerical method for stability: the semi-implicit method initially proposed by Izhikevich would fail.</p> <p>The input of the network is a population of 100 Poisson neurons, whose firing rate vary linearly from 0.2 to 20 Hz:</p> <pre><code># Input population\ninp = PoissonPopulation(100, rates=np.linspace(0.2, 20., 100))\n</code></pre> <p>We will consider two RS neurons, one learning inputs from the Poisson population using the regular STDP, the other learning using the proposed homeostatic STDP:</p> <pre><code># RS neuron without homeostatic mechanism\npop1 = Population(1, RSNeuron)\n\n# RS neuron with homeostatic mechanism\npop2 = Population(1, RSNeuron)\n</code></pre> <p>The regular STDP used in the article is a nearest-neighbour variant, which integrates LTP and LTD traces triggered after each pre- or post-synaptic spikes, respectively.</p> <p>Contrary to the STDP synapse provided by ANNarchy, weight changes occur at each each time step:</p> <ul> <li>In a post-pre interval, weight changes follow the LTP trace,</li> <li>In a pre-post interval, weight changes follow the LTD trace.</li> </ul> <p>The weights are clipped between 0 and \\(w_\\text{max}\\).</p> <pre><code>nearest_neighbour_stdp = Synapse(\n    parameters=\"\"\"\n        tau_plus = 20. : projection\n        tau_minus = 60. : projection\n        A_plus = 0.0002 : projection\n        A_minus = 0.000066 : projection\n        w_max = 0.03 : projection\n    \"\"\",\n    equations = \"\"\"\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n        # Nearest-neighbour\n        w += if t_post &amp;gt;= t_pre: ltp else: - ltd : min=0.0, max=w_max\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\"\n)\n</code></pre> <p>The homeostatic STDP rule proposed by Carlson et al. is more complex. It has a regular STDP part (the nearest-neighbour variant above) and a homeostatic regularization part, ensuring that the post-synaptic firing rate \\(R\\) does not exceed a target firing rate \\(R_\\text{target}\\) = 35 Hz.</p> <p>The firing rate of a spiking neuron can be automatically computed by ANNarchy (see later). It is then accessible as the variable <code>r</code> of the neuron (as if it were a regular rate-coded neuron).</p> <p>The homeostatic STDP rule is defined by:</p> \\[     \\Delta w = K \\, (\\alpha  \\, (1 - \\frac{R}{R_\\text{target}}) \\, w + \\beta \\, \\text{stdp} ) \\] <p>where stdp is the regular STDP weight change, and \\(K\\) is a firing rate-dependent learning rate:</p> \\[     K =  \\frac{R}{ T \\, (1 + |1 - \\gamma \\, \\frac{R}{R_\\text{target}}|}) \\] <p>with \\(T\\) being the window over which the mean firing rate is computed (5 seconds) and \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) are parameters.</p> <pre><code>homeo_stdp = Synapse(\n    parameters=\"\"\"\n        # STDP\n        tau_plus = 20. : projection\n        tau_minus = 60. : projection\n        A_plus = 0.0002 : projection\n        A_minus = 0.000066 : projection\n        w_min = 0.0 : projection\n        w_max = 0.03 : projection\n\n        # Homeostatic regulation\n        alpha = 0.1 : projection\n        beta = 1.0 : projection\n        gamma = 50. : projection\n        Rtarget = 35. : projection\n        T = 5000. : projection\n    \"\"\",\n    equations = \"\"\"\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n        # Homeostatic values\n        R = post.r : postsynaptic\n        K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic\n        # Nearest-neighbour\n        stdp = if t_post &amp;gt;= t_pre: ltp else: - ltd \n        w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\" \n)\n</code></pre> <p>This rule necessitates that the post-synaptic neurons compute their average firing rate over a 5 seconds window. This has to be explicitely enabled, as it would be computationally too expensive to allow it by default:</p> <pre><code>pop1.compute_firing_rate(5000.)\npop2.compute_firing_rate(5000.)\n</code></pre> <p>We can now fully connect the input population to the two neurons with random weights:</p> <pre><code># Projection without homeostatic mechanism\nproj1 = Projection(inp, pop1, ['ampa', 'nmda'], synapse=nearest_neighbour_stdp)\nproj1.connect_all_to_all(Uniform(0.01, 0.03))\n\n# Projection with homeostatic mechanism\nproj2 = Projection(inp, pop2, ['ampa', 'nmda'], synapse=homeo_stdp)\nproj2.connect_all_to_all(weights=Uniform(0.01, 0.03))\n</code></pre> <pre>\n<code>&lt;ANNarchy.core.Projection.Projection at 0x13512dee0&gt;</code>\n</pre> <p>Note that the same weights will target both AMPA and NMDA conductances in the post-synaptic neurons. By default, the argument <code>target</code> of Projection should be a string, but you can also pass a list of strings to reach several conductances with the same weights.</p> <p>We can now compileand simulate for 1000 seconds while recording the relevat information:</p> <pre><code>compile()\n\n# Record\nm1 = Monitor(pop1, 'r')\nm2 = Monitor(pop2, 'r')\nm3 = Monitor(proj1[0], 'w', period=1000.)\nm4 = Monitor(proj2[0], 'w', period=1000.)\n\n# Simulate\nT = 1000 # 1000s\nsimulate(T*1000., True)\n\n# Get the data\ndata1 = m1.get('r')\ndata2 = m2.get('r')\ndata3 = m3.get('w')\ndata4 = m4.get('w')\nprint('Mean Firing Rate without homeostasis:', np.mean(data1[:, 0]))\nprint('Mean Firing Rate with homeostasis:', np.mean(data2[:, 0]))\n</code></pre> <pre>\n<code>Compiling ...  OK \nSimulating 1000.0 seconds of the network took 1.585179090499878 seconds. \nMean Firing Rate without homeostasis: 55.649554600000016\nMean Firing Rate with homeostasis: 35.2732598\n</code>\n</pre> <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\nplt.subplot(311)\nplt.plot(np.linspace(0, T, len(data1[:, 0])), data1[:, 0], 'r-', label=\"Without homeostasis\")\nplt.plot(np.linspace(0, T, len(data2[:, 0])), data2[:, 0], 'b-', label=\"With homeostasis\")\nplt.xlabel('Time (s)')\nplt.ylabel('Firing rate (Hz)')\nplt.subplot(312)\nplt.plot(data3[-1, :], 'r-')\nplt.plot(data4[-1, :], 'bx')\naxes = plt.gca()\naxes.set_ylim([0., 0.035])\nplt.xlabel('# neuron')\nplt.ylabel('Weights after 1000s')\nplt.subplot(313)\nplt.imshow(data4.T, aspect='auto', cmap='hot')\nplt.xlabel('Time (s)')\nplt.ylabel('# neuron')\nplt.show()\n</code></pre> <p>We see that without homeostasis, the post-synaptic neuron reaches quickly a firing of 55 Hz, with all weights saturating at their maximum value 0.03. This is true even for inputs as low as 0.2Hz.</p> <p>Meanwhile, with homeostasis, the post-synaptic neuron gets a firing rate of 35 Hz (its desired value), and the weights from the input population are proportional to the underlying activity.</p>"},{"location":"example/Ramp.html#homeostatic-stdp","title":"Homeostatic STDP","text":"<p>Download the Jupyter notebook : Ramp.ipynb</p>"},{"location":"example/STP.html","title":"STP","text":"<p>Implementation of the recurrent network proposed in:</p> <p>&gt; Tsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses, The Journal of Neuroscience, 20(50).</p> <p>This example in <code>examples/tsodyks_markram</code> shows how to define short-term plasticity (STP). </p> <pre><code>from ANNarchy import *\nclear()\n\ndt=0.25\nsetup(dt=dt)\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <p>This network uses simple leaky integrate-and-fire (LIF) neurons:</p> <pre><code>LIF = Neuron(\n    parameters = \"\"\"\n    tau = 30.0 : population\n    I = 15.0\n    tau_I = 3.0 : population\n    \"\"\",\n    equations = \"\"\"\n    tau * dv/dt = -v + g_exc - g_inh + I : init=13.5\n    tau_I * dg_exc/dt = -g_exc\n    tau_I * dg_inh/dt = -g_inh\n    \"\"\",\n    spike = \"v &amp;gt; 15.0\",\n    reset = \"v = 13.5\",\n    refractory = 3.0\n)\n\nP = Population(geometry=500, neuron=LIF)\nP.I = np.sort(Uniform(14.625, 15.375).get_values(500))\nP.v = Uniform(0.0, 15.0)\nExc = P[:400]\nInh = P[400:]\n</code></pre> <p>Short-term plasticity can be defined by dynamical changes of synaptic efficiency, based on pre- or post-synaptic activity.</p> <p>We define a STP synapse, whose post-pynaptic potential (psp, define by <code>g_target</code>) depends not only on the weight <code>w</code> and the emission of pre-synaptic spike, but also on intra-synaptic variables <code>x</code> and <code>u</code>:</p> <pre><code>STP = Synapse(\n    parameters = \"\"\"\n    w=0.0\n    tau_rec = 1.0\n    tau_facil = 1.0\n    U = 0.1\n    \"\"\",\n    equations = \"\"\"\n    dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n    du/dt = (U - u)/tau_facil : init = 0.1, event-driven   \n    \"\"\",\n    pre_spike=\"\"\"\n    g_target += w * u * x\n    x *= (1 - u)\n    u += U * (1 - u)\n    \"\"\"\n)\n</code></pre> <p>Creating the projection between the excitatory and inhibitory is straightforward when the right parameters are chosen:</p> <pre><code># Parameters for the synapses\nAee = 1.8\nAei = 5.4\nAie = 7.2\nAii = 7.2\n\nUee = 0.5\nUei = 0.5\nUie = 0.04\nUii = 0.04\n\ntau_rec_ee = 800.0\ntau_rec_ei = 800.0\ntau_rec_ie = 100.0\ntau_rec_ii = 100.0\n\ntau_facil_ie = 1000.0\ntau_facil_ii = 1000.0\n\n# Create projections\nproj_ee = Projection(pre=Exc, post=Exc, target='exc', synapse=STP)\nproj_ee.connect_fixed_probability(probability=0.1, weights=Normal(Aee, (Aee/2.0), min=0.2*Aee, max=2.0*Aee)) \nproj_ee.U = Normal(Uee, (Uee/2.0), min=0.1, max=0.9)\nproj_ee.tau_rec = Normal(tau_rec_ee, (tau_rec_ee/2.0), min=5.0)\nproj_ee.tau_facil = dt # Cannot be 0!\n\nproj_ei = Projection(pre=Inh, post=Exc, target='inh', synapse=STP)\nproj_ei.connect_fixed_probability(probability=0.1, weights=Normal(Aei, (Aei/2.0), min=0.2*Aei, max=2.0*Aei))\nproj_ei.U = Normal(Uei, (Uei/2.0), min=0.1, max=0.9)\nproj_ei.tau_rec = Normal(tau_rec_ei, (tau_rec_ei/2.0), min=5.0)\nproj_ei.tau_facil = dt # Cannot be 0!\n\nproj_ie = Projection(pre=Exc, post=Inh, target='exc', synapse=STP)\nproj_ie.connect_fixed_probability(probability=0.1, weights=Normal(Aie, (Aie/2.0), min=0.2*Aie, max=2.0*Aie))\nproj_ie.U = Normal(Uie, (Uie/2.0), min=0.001, max=0.07)\nproj_ie.tau_rec = Normal(tau_rec_ie, (tau_rec_ie/2.0), min=5.0)\nproj_ie.tau_facil = Normal(tau_facil_ie, (tau_facil_ie/2.0), min=5.0)\n\nproj_ii = Projection(pre=Inh, post=Inh, target='inh', synapse=STP)\nproj_ii.connect_fixed_probability(probability=0.1, weights=Normal(Aii, (Aii/2.0), min=0.2*Aii, max=2.0*Aii))\nproj_ii.U = Normal(Uii, (Uii/2.0), min=0.001, max=0.07)\nproj_ii.tau_rec = Normal(tau_rec_ii, (tau_rec_ii/2.0), min=5.0)\nproj_ii.tau_facil = Normal(tau_facil_ii, (tau_facil_ii/2.0), min=5.0)\n</code></pre> <p>We compile and simulate for 10 seconds:</p> <pre><code>compile()\n\n# Record\nMe = Monitor(Exc, 'spike')\nMi = Monitor(Inh, 'spike')\n\n# Simulate\nduration = 10000.0\nsimulate(duration, measure_time=True)\n</code></pre> <pre>\n<code>Compiling ...  OK \nSimulating 10.0 seconds of the network took 0.09651398658752441 seconds. \n</code>\n</pre> <p>We retrieve the recordings and plot them:</p> <pre><code># Retrieve recordings\ndata_exc = Me.get()\ndata_inh = Mi.get()\nte, ne = Me.raster_plot(data_exc['spike'])\nti, ni = Mi.raster_plot(data_inh['spike'])\n\n# Histogram of the exc population\nh = Me.histogram(data_exc['spike'], bins=1.0)\n\n# Mean firing rate of each excitatory neuron\nrates = []\nfor neur in data_exc['spike'].keys():\n    rates.append(len(data_exc['spike'][neur])/duration*1000.0)\n\n# Plot\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(te, ne, 'b.', markersize=1.0)\nplt.plot(ti, ni, 'b.', markersize=1.0)\nplt.xlim((0, duration)); plt.ylim((0,500))\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\n\nplt.subplot(3,1,2)\nplt.plot(h/400.)\nplt.xlabel('Time (ms)')\nplt.ylabel('Net activity')\n\nplt.subplot(3,1,3)\nplt.plot(sorted(rates))\nplt.ylabel('Spikes / sec')\nplt.xlabel('# neuron')\nplt.show()\n</code></pre>"},{"location":"example/STP.html#short-term-plasticity-and-synchrony","title":"Short-term Plasticity and Synchrony","text":"<p>Download the Jupyter notebook : STP.ipynb</p>"},{"location":"example/SimpleSTDP.html","title":"STDP","text":"<p>A simple model showing the STDP learning on a single neuron. Model adapted from Song, Miller and Abbott (2000) and Song and Abbott (2001)</p> <p>Code adapted from the Brian example: http://brian.readthedocs.org/en/1.4.1/examples-plasticity_STDP1.html</p> <pre><code>from ANNarchy import *\n\n# Parameters\nF = 15.0 # Poisson distribution at 15 Hz\nN = 1000 # 1000 Poisson inputs\ngmax = 0.01 # Maximum weight\nduration = 100000.0 # Simulation for 100 seconds\n\n# Definition of the neuron\nIF = Neuron(\n    parameters = \"\"\"\n        tau_m = 10.0\n        tau_e = 5.0 \n        vt = -54.0 \n        vr = -60.0 \n        El = -74.0 \n        Ee = 0.0 \n    \"\"\",\n    equations = \"\"\"\n        tau_m * dv/dt = El - v + g_exc * (Ee - vr) : init = -60.0\n        tau_e * dg_exc/dt = - g_exc\n    \"\"\",\n    spike = \"\"\"\n        v &amp;gt; vt\n    \"\"\",\n    reset = \"\"\"\n        v = vr\n    \"\"\"\n)\n\n# Input population\nInput = PoissonPopulation(name = 'Input', geometry=N, rates=F)\n\n# Output neuron\nOutput = Population(name = 'Output', geometry=1, neuron=IF)\n\n# Projection learned using STDP\nproj = Projection( \n    pre = Input, \n    post = Output, \n    target = 'exc',\n    synapse = STDP(tau_plus=20.0, tau_minus=20.0, A_plus=0.01, A_minus=0.0105, w_max=0.01)\n)\nproj.connect_all_to_all(weights=Uniform(0.0, gmax))\n\n\n# Compile the network\ncompile()\n\n# Start recording\nMi = Monitor(Input, 'spike') \nMo = Monitor(Output, 'spike')\n\n# Start the simulation\nprint('Start the simulation')\nsimulate(duration, measure_time=True)\n\n# Retrieve the recordings\ninput_spikes = Mi.get('spike')\noutput_spikes = Mo.get('spike')\n\n# Compute the mean firing rates during the simulation\nprint('Mean firing rate in the input population: ' + str(Mi.mean_fr(input_spikes)) )\nprint('Mean firing rate of the output neuron: ' + str(Mo.mean_fr(output_spikes)) )\n\n# Compute the instantaneous firing rate of the output neuron\noutput_rate = Mo.smoothed_rate(output_spikes, 100.0)\n\n# Receptive field after simulation\nweights = proj.w[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(output_rate[0, :])\nplt.subplot(3,1,2)\nplt.plot(weights, '.')\nplt.subplot(3,1,3)\nplt.hist(weights, bins=20)\nplt.show()\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\nCompiling ...  OK \nStart the simulation\nSimulating 100.0 seconds of the network took 1.0213088989257812 seconds. \nMean firing rate in the input population: 15.0002\nMean firing rate of the output neuron: 28.139999999999997\n</code>\n</pre> <pre><code>\n</code></pre>"},{"location":"example/SimpleSTDP.html#simple-stdp","title":"Simple STDP","text":"<p>Download the Jupyter notebook : SimpleSTDP.ipynb</p>"},{"location":"example/StructuralPlasticity.html","title":"Structural plasticity","text":"<p>As simple example showing how to use structural plasticity (creation/pruning of synapses) in a rate-coded network (spiking networks work similarly).</p> <p>First, the structural plasticity mechanisms must be allowed in <code>setup()</code>:</p> <pre><code>from ANNarchy import *\nclear()\n\n# Compulsory to allow structural plasticity\nsetup(structural_plasticity=True)\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <p>We define a leaky integrator rate-coded neuron and a small population: </p> <pre><code>LeakyIntegratorNeuron = Neuron(\n    parameters=\"\"\"\n        tau = 10.0 : population\n        baseline = 0.0 \n    \"\"\",\n    equations = \"\"\"\n        tau * dr/dt + r = baseline + sum(exc) : min=0.0\n    \"\"\"\n)\npop = Population(100, LeakyIntegratorNeuron)\n</code></pre> <p>Structural plasticity has two components: creation of synapses and pruning (removal) under certain conditions. These conditions are defined in the synapse type itself in the <code>pruning</code> and <code>creating</code> arguments: </p> <pre><code>StructuralPlasticSynapse = Synapse(\n    parameters = \" T = 10000 : int, projection \",\n    equations = \"\"\"\n        age = if pre.r * post.r &amp;gt; 1.0 :\n                0\n              else :\n                age + 1 : init = 0, int\"\"\",\n    pruning = \"age &amp;gt; T : proba = 0.2\",\n    creating = \"pre.r * post.r &amp;gt; 1.0 : proba = 0.1, w = 0.01\",\n)\n\nproj = Projection(pop, pop, 'exc', StructuralPlasticSynapse)\nproj.connect_fixed_probability(weights = 0.01, probability=0.1)\n</code></pre> <pre>\n<code>&lt;ANNarchy.core.Projection.Projection at 0x1069b8f70&gt;</code>\n</pre> <p>These conditions must be boolean values, which when <code>True</code> may trigger the creation/pruning of a synapse. The flag <code>proba</code> gives the probability by which the synapse will actually be created/pruned.</p> <ul> <li>When <code>creating</code> is <code>True</code>, a synapse that did not exist will be created with the provided probability. Its weight will take the value provided by the flag <code>w</code> (0.01), the other variables take their default value.</li> <li>When <code>pruning</code> is <code>True</code>, a synapse that exists will be deleted with the given probability.</li> </ul> <p>The <code>pruning</code> condition can depend on any pre-synaptic, post-synaptic or synaptic variable. The <code>creating</code> condition can only depend on pre- or post-synaptic conditions, as the synapse does not exist yet.</p> <p>Apart from these two fields, the synapse is a regular synapse, one could also define synaptic plasticity mechanisms and so on.</p> <p>We finally create a sparse projection within the population, with 10% connectivity.</p> <pre><code>compile()\n</code></pre> <pre>\n<code>Compiling ...  OK \n</code>\n</pre> <p>The creation and pruning have to be explicitly started before a simulation, as they are very expensive computationally. The <code>period</code> argument states how often the conditions will be checked (avoid using <code>dt</code>):</p> <pre><code>proj.start_creating(period=100.0)\nproj.start_pruning(period=100.0)\n</code></pre> <p>To see the effect of structural plasticity, one alternatively activates one half of the population by setting a high baseline (mimicking corrrelated inputs). As neurons in one half will be activated at the same time, they will create synapses between each other. Between the two halves, the neurons are never co-activated, so the existing synapses will slowly die out.  </p> <pre><code># Save the initial connectivity matrix\ninitial_weights = proj.connectivity_matrix()\n\n# Let structural plasticity over several trials\nnum_trials = 100\nfor trial in range(num_trials):\n    # Activate the first subpopulation\n    pop[:50].baseline = 1.0\n    # Simulate for 1s\n    simulate(1000.)\n    # Reset the population\n    pop.baseline = 0.0\n    simulate(100.)\n    # Activate the second subpopulation\n    pop[50:].baseline = 1.0\n    # Simulate for 1s\n    simulate(1000.)\n    # Reset the population\n    pop.baseline = 0.0\n    simulate(100.)\n\n# Inspect the final connectivity matrix\nfinal_weights = proj.connectivity_matrix()\n</code></pre> <p>We can check the effect of structural plasticity by looking at the connectivity matrix before and after the stimulation:</p> <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 15))\nplt.subplot(121)\nplt.imshow(initial_weights)\nplt.title('Connectivity matrix before')\nplt.subplot(122)\nplt.imshow(final_weights)\nplt.title('Connectivity matrix after')\nplt.show()\n</code></pre>"},{"location":"example/StructuralPlasticity.html#structural-plasticity","title":"Structural plasticity","text":"<p>Download the Jupyter notebook : StructuralPlasticity.ipynb</p>"},{"location":"example/Webcam.html","title":"Webcam","text":"<p>The script <code>examples/image/Webcam.py</code> applies a red filter on the input from the webcam, and isolates one mode using a dynamical neural field.</p> <p>Most of the concepts are similar to the Image Processing example. The <code>VideoPopulation</code> object also requires the Python bindings to OpenCV.</p> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.image import *\nfrom ANNarchy.extensions.convolution import Convolution, Pooling\nclear()\n</code></pre> <pre>\n<code>ANNarchy 4.7 (4.7.2) on darwin (posix).\n</code>\n</pre> <pre><code># Definition of the neurons\nLinearNeuron = Neuron(equations=\"r=sum(exc): min=0.0\")\nDNF = Neuron(parameters=\"tau=10.0\", equations=\"tau*dr/dt + r = sum(exc) + sum(inh): min=0.0, max=1.0\")\n\n# Population getting the video stream   \nwidth = 640\nheight = 480\nvideo = VideoPopulation(geometry=(height, width, 3))\n\n# Subsampling population\npooled = Population(geometry=(48, 64, 3), neuron = LinearNeuron)\n\n# Mean-pooling projection\npool_proj = Pooling(pre=video, post=pooled, target='exc', operation='mean')\npool_proj.connect_pooling()\n\n# Define a red filter with no spatial extent\nred_filter = [[ [2.0, -1.0, -1.0] ]]\n\n# Create a population of DNF neurons downscaling the image with a factor 10 \ndnf = Population(geometry=(48, 64), neuron = DNF)\n\n# Create the convolution using the red filter\nff = Convolution(pre=pooled, post=dnf, target='exc')\nff.connect_filter(weights=red_filter)\n\n# Create difference of Gaussians lateral connections for denoising/competition\nlat = Projection(pre=dnf, post=dnf, target='inh')\nlat.connect_dog(amp_pos=0.2, sigma_pos=0.1, amp_neg=0.1, sigma_neg=0.7)\n</code></pre> <pre>\n<code>&lt;ANNarchy.core.Projection.Projection at 0x103fc0fa0&gt;</code>\n</pre> <p>The <code>VideoPopulation</code> acquires images from the webcam: here the webcam should be able to deliver 640x480 colored images.</p> <p>The corresponding population is then subsampled with a factor 10, and a red filter is applied on it. This feeds a DNF (see the Neural Field\" example) which selects the region with the highest density.</p> <pre><code>compile()\n</code></pre> <pre>\n<code>Compiling ...  OK \n</code>\n</pre> <p>We can now start the camera 0 (<code>/dev/video0</code>, adapt it to your machine):</p> <pre><code>video.start_camera(0)\n</code></pre> <p>A simple GUI based on PyQtGraph allows to display the input and output of the network:</p> <pre><code>try:\n    from pyqtgraph.Qt import QtGui, QtCore\n    import pyqtgraph as pg\nexcept:\n    print('PyQtGraph is not installed, can not visualize the network.')\n    exit(0)\n\n# Wrapping class    \nclass Viewer(object):\n    \" Class to visualize the network activity using PyQtGraph.\"\n\n    def __init__(self, video, result):\n        self.video = video\n        self.result = result\n        app = pg.mkQApp()\n        self.win = pg.GraphicsWindow(title=\"Live webcam\")\n        self.win.resize(640,480)   \n\n        box = self.win.addViewBox(lockAspect=True)\n        box.invertY()\n        self.vis = pg.ImageItem()\n        box.addItem(self.vis)  \n\n        box = self.win.addViewBox(lockAspect=True)\n        box.invertY()\n        self.res = pg.ImageItem()\n        box.addItem(self.res)  \n\n        self.win.show()\n\n        self.lastUpdate = pg.ptime.time()\n        self.avgFps = 0.0\n\n\n    def update(self):\n        # Set the input\n        self.video.grab_image()\n        # Simulate for 10 ms with a new input\n        simulate(5.0)\n        # Refresh the GUI\n        self.vis.setImage(np.swapaxes(self.video.r,0,1))\n        self.res.setImage(np.swapaxes(self.result.r,0,1))\n        # Listen to mouse/keyboard events\n        QtGui.QApplication.processEvents()\n        # FPS\n        now = pg.ptime.time()\n        fps = 1.0 / (now - self.lastUpdate)\n        self.lastUpdate = now\n        self.avgFps = self.avgFps * 0.8 + fps * 0.2\n        # print(self.avgFps)\n\n    def run(self):\n\n        timer = QtCore.QTimer()\n        timer.timeout.connect(self.update)\n        timer.start(0)  \n        QtGui.QApplication.instance().exec_() \n        timer.stop()\n</code></pre> <pre><code># Start the GUI\nview = Viewer(video, dnf)\nview.run() \nvideo.release()\n</code></pre>"},{"location":"example/Webcam.html#webcam","title":"Webcam","text":"<p>Download the Jupyter notebook : Webcam.ipynb</p>"},{"location":"manual/Configuration.html","title":"Configuration","text":"<p>The <code>setup()</code> function can be used at the beginning of a script to configure the numerical behavior of ANNarchy.</p>"},{"location":"manual/Configuration.html#setting-the-discretization-step","title":"Setting the discretization step","text":"<p>An important value for the simulation is the discretization step <code>dt</code>. Its default value is 1 ms, which is usually fine for rate-coded networks, but may be too high for spiking networks, as the equations are stiffer. Taken too high, it can lead to high numerical errors. Too low, and the simulation will take an unnecessary amount of time.</p> <p>To set the discretization step, just pass the desired value to <code>setup()</code> at the beginning of the script, or at any rate before the call to <code>compile()</code>:</p> <pre><code>setup(dt=0.1)\n</code></pre> <p>Changing its value after calling <code>compile()</code> will not have any effect.</p>"},{"location":"manual/Configuration.html#setting-the-seed-of-the-random-number-generators","title":"Setting the seed of the random number generators","text":"<p>By default, the random number generators are seeded with <code>time(NULL)</code>, so each simulation will be different. If you want to have deterministic simulations, you simply need to provide a fixed seed to <code>setup()</code>:</p> <pre><code>setup(seed=62756)\n</code></pre> <p>Note that this also sets the seed of Numpy, so you can also reproduce random initialization values produced by <code>numpy.random</code>.</p> <p>Note</p> <p>Using the same seed with the OpenMP and CUDA backends will not lead to the same sequences of numbers!</p>"},{"location":"manual/Configuration.html#cleaning-the-compilation-directory","title":"Cleaning the compilation directory","text":"<p>When calling <code>compile()</code> for the first time, a subfolder <code>annarchy/</code> will be created in the current directory, where the generated code will be compiled. The first compilation may last a couple of seconds, but further modifications to the script are much faster. If no modification to the network has been made except for parameter values, it will not be recompiled, sparing us this overhead.</p> <p>ANNarchy tracks the changes in the script and re-generates the corresponding code. In some cases (a new version of ANNarchy has been installed, bugs), it may be necessary to perform a fresh compilation of the network. You can either delete the <code>annarchy/</code> subfolder and restart the script:</p> <pre><code>$ rm -rf annarchy/\n$ python MyNetwork.py\n</code></pre> <p>or pass the <code>--clean</code> flag to Python:</p> <pre><code>$ python MyNetwork.py --clean\n</code></pre>"},{"location":"manual/Configuration.html#selecting-the-compiler","title":"Selecting the compiler","text":"<p>ANNarchy requires a C++ compiler. On GNU/Linux, the default choice is <code>g++</code>, while on MacOS it is <code>clang++</code>. You can change the compiler (and its flags) to use either during the call to <code>compile()</code> in your script:</p> <pre><code>compile(compiler=\"clang++\", compiler_flags=\"-march=native -O3\")\n</code></pre> <p>or globally by modifying the configuration file located at <code>~/.config/ANNarchy/annarchy.json</code>:</p> <pre><code>{\n\"openmp\": {\n\"compiler\": \"clang++\",\n\"flags\": \"-march=native -O3\"\n}\n}\n</code></pre> <p>Be careful with the flags: for example, the optimization level <code>-O3</code> does not obligatorily produce faster code. But this is the case for most models, therefore it is the default in the ANNarchy 4.7.x releases.</p> <p>Even more caution is required when using the <code>-ffast-math</code> flag. It can increase the performance, in particular in combination with SIMD. However, the application of <code>-ffast-math</code> enables a set of optimizations which might violate IEEE 754 compliance (which might be okay in many cases, but it is important that the user verifies the result). For more details, see the g++ documentation: https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html</p> <p>Note</p> <p>In rare cases, it may occur that the CPU architecture is not detectable for the used g++ compiler (e.g. Intel's Tigerlake and g++ &lt;= 9.4). This will result in a compiler error which can be fixed by removing the '-march=native' flag. To get access to AVX-512 SIMD instructions, you need to add <code>-mavx512f</code> instead, as well as <code>-ftree-vectorize</code> if <code>-O3</code> is not already used.</p>"},{"location":"manual/Configuration.html#parallel-computing-with-openmp","title":"Parallel computing with OpenMP","text":"<p>The default paradigm for an ANNarchy simulation is through openMP, which distributes automatically the computations over the available CPU cores.</p> <p>By default, OpenMP would use all the available cores for your simulation, even if it is not optimal: small networks in particular tend to run faster with a small amount of cores (for the provided example with Neural Fields, it is for example 2). For this reason, the <code>OMP_NUM_THREADS</code> environment variable has no effect in ANNarchy. You can control the number of cores by passing the <code>-j</code> flag to the Python command:</p> <pre><code>user@machine:~$ python NeuralField.py -j2\n</code></pre> <p>It is the responsability of the user to find out which number of cores is optimal for his network, by comparing simulation times. When this optimal number is found, it can be hard-coded in the script by setting the <code>num_threads</code> argument to <code>ANNarchy.setup()</code>:</p> <pre><code>from ANNarchy import *\nsetup(num_threads=2)\n</code></pre>"},{"location":"manual/Configuration.html#parallel-computing-with-cuda","title":"Parallel computing with CUDA","text":"<p>To run your network on GPUs, you need to declare to ANNarchy that you want to use CUDA. One way to do so is to pass the <code>--gpu</code> flag to the command line:</p> <pre><code>user@machine:~$ python NeuralField.py --gpu\n</code></pre> <p>You can also set the <code>paradigm</code> argument of <code>ANNarchy.setup()</code> to make it permanent:</p> <pre><code>from ANNarchy import *\nsetup(paradigm=\"cuda\")\n</code></pre> <p>If there are multiple GPUs on your machine, you can select the ID of the device by specifying it to the <code>--gpu</code> flag on the command line:</p> <pre><code>user@machine:~$ python NeuralField.py --gpu=2\n</code></pre> <p>You can also pass the <code>cuda_config</code> dictionary argument to <code>compile()</code>:</p> <pre><code>compile(cuda_config={'device': 2})\n</code></pre> <p>The default GPU is defined in the configuration file <code>~/.config/ANNarchy/annarchy.json</code> (0 unless you modify it).</p> <pre><code>{\n\"cuda\": {\n\"device\": 0,\n\"path\": \"/usr/local/cuda\"\n}\n}\n</code></pre> <p>Hint</p> <p>As the current implementation is a development version, some of the features provided by ANNarchy are not supported yet with CUDA:</p> <ul> <li>weight sharing (convolutions),</li> <li>non-uniform synaptic delays,</li> <li>structural plasticity,</li> <li>spiking neurons: a) with mean firing rate and b) continous     integration of inputs,</li> <li><code>SpikeSourceArray</code>.</li> </ul>"},{"location":"manual/Connector.html","title":"Connectivity","text":"<p>There are basically four methods to instantiate projections:</p> <ol> <li>By using a built-in connector method.</li> <li>By using a saved projection.</li> <li>By loading dense or sparse matrices.</li> <li>By defining a custom connector method.</li> </ol>"},{"location":"manual/Connector.html#available-connector-methods","title":"Available connector methods","text":"<p>For further detailed information about these connectors, please refer to the library reference Projections.</p>"},{"location":"manual/Connector.html#connect_all_to_all","title":"connect_all_to_all","text":"<p>All neurons of the post-synaptic population form connections with all neurons of the pre-synaptic population (dense connectivity). Self-connections are avoided by default, but the parameter <code>allow_self_connections</code> can be set to <code>True</code>:</p> <pre><code>proj.connect_all_to_all(weights=1.0, delays=2.0, allow_self_connections=False) \n</code></pre> <p>The <code>weights</code> and <code>delays</code> arguments accept both single float values (all synapses will take this initial value), as well as random objects allowing to randomly select the initial values for different synapses:</p> <pre><code>proj.connect_all_to_all(weights=Uniform(0.0, 0.5)) \n</code></pre>"},{"location":"manual/Connector.html#connect_one_to_one","title":"connect_one_to_one","text":"<p>A neuron of the post-synaptic population forms a connection with only one neuron of the pre-synaptic population. The order depends on the ranks: neuron 0 is connected with neuron 0 and so on. It is advised that the pre- and post-populations have the same size/geometry, especially when using population views.</p> <pre><code>pop1 = Population((20, 20), Neuron(parameters=\"r=0.0\"))\npop2 = Population((10, 10), Neuron(equations=\"r=sum(exc)\"))\n\nproj = Projection(pop1[5:15, 5:15], pop2, 'exc')\nproj.connect_one_to_one(weights=1.0) \n</code></pre> <p>Weights and delays also accept random distributions.</p> <p>Below is a graphical representation of the difference between all_to_all and one_to_one:</p> <p></p>"},{"location":"manual/Connector.html#connect_gaussian","title":"connect_gaussian","text":"<p>A neuron of the post-synaptic population forms a connection with a limited region of the pre-synaptic population, centered around the neuron with the same normalized position. Weight values are initialized using a Gaussian function, with a maximal value <code>amp</code> for the neuron of same position and decreasing with distance (standard deviation <code>sigma</code>):</p> \\[w(x, y) = A \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma^2})\\] <p>where \\((x, y)\\) is the position of the pre-synaptic neuron (normalized to \\([0, 1]^d\\)) and \\((x_c, y_c)\\) is the position of the post-synaptic neuron (normalized to \\([0, 1]^d\\)). A = amp, sigma = \\(\\sigma\\).</p> <p>In order to void creating useless synapses, the parameter <code>limit</code> can be set to restrict the creation of synapses to the cases where the value of the weight would be superior to <code>limit*abs(amp)</code>. Default is 0.01 (1%).</p> <p>Self-connections are avoided by default (parameter <code>allow_self_connections</code>).</p> <p>The two populations must have the same number of dimensions, but the number of neurons can vary as the positions of each neuron are normalized in \\([0, 1]^d\\):</p> <pre><code>proj.connect_gaussian( amp=1.0, sigma=0.2, limit=0.001) \n</code></pre>"},{"location":"manual/Connector.html#connect_dog","title":"connect_dog","text":"<p>The same as connect_gaussian, except weight values are computed using a Difference-of-Gaussians (DoG), usually positive in the center, negative a bit further away and small at long distances.</p> \\[\\begin{aligned} w(x, y) &amp;= A^+ \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_+^2}) \\\\     &amp;-  A^- \\cdot \\exp(-\\frac{1}{2}\\frac{(x-x_c)^2+(y-y_c)^2}{\\sigma_-^2}) \\\\ \\end{aligned}\\] <p>Weights smaller than <code>limit * abs(amp_pos - amp_neg)</code> are not created and self-connections are avoided by default (parameter <code>allow_self_connections</code>):</p> <pre><code>proj.connect_dog(amp_pos=1.0, sigma_pos=0.2, \n    amp_neg=0.3, sigma_neg=0.7, limit=0.001) \n</code></pre> <p>The following figure shows the example of a neuron of coordinates (10, 10) in the post-synaptic population, which is connected through the gaussian (left) and dog (right) projections to a population of geometry 30*30. The X and Y axis denote the coordinates of the pre-synaptic neurons, while the Z axis is the weight value.</p> <p></p>"},{"location":"manual/Connector.html#connect_fixed_number_pre","title":"connect_fixed_number_pre","text":"<p>Each neuron in the post-synaptic population receives connections from a fixed number of neurons of the pre-synaptic population chosen randomly. It may happen that two post-synaptic neurons are connected to the same pre-synaptic neuron and that some pre-synaptic neurons are connected to nothing:</p> <pre><code>proj.connect_fixed_number_pre(number = 20, weights=1.0) \n</code></pre> <p><code>weights</code> and <code>delays</code> can also take a random object.</p>"},{"location":"manual/Connector.html#connect_fixed_number_post","title":"connect_fixed_number_post","text":"<p>Each neuron in the pre-synaptic population sends a connection to a fixed number of neurons of the post-synaptic population chosen randomly. It may happen that two pre-synaptic neurons are connected to the same post-synaptic neuron and that some post-synaptic neurons receive no connection at all:</p> <pre><code>proj.connect_fixed_number_post(number = 20, weights=1.0) \n</code></pre> <p>The following figure shows the fixed_number_pre (left) and fixed_number_post projections between two populations of 4 neurons, with <code>number=2</code>. In fixed_number_pre, each post-synaptic neuron receives exactly 2 connections, while in fixed_number_post, each pre-synaptic neuron send exactly two connections:</p> <p></p>"},{"location":"manual/Connector.html#connect_fixed_probability","title":"connect_fixed_probability","text":"<p>For each post-synaptic neuron, there is a fixed probability that it forms a connection with a neuron of the pre-synaptic population. It is basically a all_to_all projection, except some synapses are not created, making the projection sparser:</p> <pre><code>proj.connect_fixed_probability(probability = 0.2, weights=1.0) \n</code></pre> <p>Important</p> <p>If a single value is used for the <code>weights</code> argument of <code>connect_all_to_all</code>, <code>connect_one_to_one</code>, <code>connect_fixed_probability</code>, <code>connect_fixed_number_pre</code> and <code>connect_fixed_number_post</code>, and the default synapse is used (no synaptic plasticity), ANNarchy will generate a single weight value for all the synapses of the projection, not one per synapse.</p> <p>This allows to save a lot of memory and improve performance. However, if you wish to manually change the weights of some of the synapses after the creation, you need to force the creation of one value per synapse by setting <code>force_multiple_weights=True</code> in the call to the connector.</p>"},{"location":"manual/Connector.html#saved-connectivity","title":"Saved connectivity","text":"<p>It is also possible to build a connection pattern using data saved during a precedent simulation. This is useful when:</p> <ul> <li>pre-learning is done in another context;</li> <li>a connector method for static synapses is particularly slow (e.g.     DoG), but loading the result from a file is faster.</li> </ul> <p>The connectivity of a projection can be saved (after <code>compile()</code>) using:</p> <pre><code>proj.save_connectivity(filename='proj.npz')\n</code></pre> <p>The filename can used relative or absolute paths. The data is saved in a binary format:</p> <ul> <li>Compressed Numpy format when the filename ends with <code>.npz</code>.</li> <li>Compressed binary file format when the filename ends with <code>.gz</code>.</li> <li>Binary file format otherwise.</li> </ul> <p>It can then be used to instantiate another projection:</p> <pre><code>proj.connect_from_file(filename='proj.npz')\n</code></pre> <p>Only the connectivity (which neurons are connected), the weights and delays are loaded. Other synaptic variables are left untouched. The pre- and post-synaptic population must have the same size during saving and loading.</p>"},{"location":"manual/Connector.html#from-connectivity-matrices","title":"From connectivity matrices","text":"<p>One can also create connections using Numpy dense matrices or Scipy sparse matrices.</p>"},{"location":"manual/Connector.html#connect_from_matrix","title":"connect_from_matrix","text":"<p>This method accepts a Numpy array to define the weights of the projection (and optionally the delays). By default, the matrix should have the size <code>(post.size, pre.size)</code>, so that the first index represents a post-synaptic neuron and the second the pre-synaptic neurons. If your matrix is defined in the reversed order, you can either transpose it or set the <code>pre_post</code> argument to <code>True</code>.</p> <p>This method is useful for dense connectivity matrices (all-to-all). If you do not want to create some synapses, the weight value should be set to <code>None</code>.</p> <p>The following code creates a synfire chain inside a population of 100 neurons:</p> <pre><code>N = 100\nproj = Projection(pop, pop, 'exc')\n\n# Initialize an empty connectivity matrix\nw = np.array([[None]*N]*N)\n\n# Connect each post-synaptic neuron to its predecessor\nfor i in range(N):\n    w[i, (i-1)%N] = 1.0\n\n# Create the connections\nproj.connect_from_matrix(w)\n</code></pre> <p>Connectivity matrices can not work with multi-dimensional coordinates, only ranks are used. Population views can be used in the projection, but the connection matrix must have the corresponding size:</p> <pre><code>proj = Projection(pop[10:20], pop[50:60], 'exc')\n\n# Create the connectivity matrix\nw = np.ones((10, 10))\n\n# Create the connections\nproj.connect_from_matrix(w)\n</code></pre>"},{"location":"manual/Connector.html#connect_from_sparse","title":"connect_from_sparse","text":"<p>For sparse connection matrices, the Numpy array format may have a huge memory overhead if most of its values are None. It is possible to use Scipy sparse matrices in that case. The previous synfire chain example becomes:</p> <pre><code>from scipy.sparse import lil_matrix\nproj = Projection(pop, pop, 'exc')\nw = lil_matrix((N, N))\nfor i in range(N):\n    w[i, (i+1)%N] = 1.0\nproj.connect_from_sparse(w)\n</code></pre> <p>Note</p> <p>Contrary to <code>connect_from_matrix()</code>, the first index of the sparse matrix represents the pre-synaptic neurons, not the post-synaptic ones. This is for compatibility with other neural simulators.</p> <p><code>connect_from_sparse()</code> accepts <code>lil_matrix</code>, <code>csr_matrix</code> and <code>csc_matrix</code> objects, although <code>lil_matrix</code> should be preferred for its simplicity of element access.</p>"},{"location":"manual/Connector.html#user-defined-patterns","title":"User-defined patterns","text":"<p>This section describes the creation of user-specific connection patterns in ANNarchy, if the available patterns are not enough. A connection pattern is simply implemented as a method returning a <code>LILConnectivity</code> (list-of-list) object containing all the necessary information to create the synapses.</p> <p>A connector method must take on the first position the pre-synaptic population (or a subset of it) and on the second one the post-synaptic population. Other arguments are free, but should be passed when creating the projection.</p> <pre><code>probabilistic_pattern(pre, post, &lt;other arguments&gt;)\n</code></pre> <p>As an example, we will recreate the fixed_probability connector method, building synapses with a given probability. For this new pattern we need a weight value (common for all synapses) and a probability value as additional arguments. We consider that no delay is introduced in the synaptic transmission..</p> <pre><code>from ANNarchy import *\n\ndef probabilistic_pattern(pre, post, weight, probability):\n\n    synapses = LILConnectivity()\n\n    ... pattern code comes here ...\n\n    return synapses\n</code></pre>"},{"location":"manual/Connector.html#fixed_probability-in-python","title":"fixed_probability in Python","text":"<p>The connector method needs to return a <code>LILConnectivity</code> object storing the connectivity. For each post-synaptic neuron receiving synapses, a list of pre-synaptic ranks, weight values and delays must be added to the structure. If you use 2D or 3D populations you need to transform the coordinates into ranks with the <code>rank_from_coordinates</code> function.</p> <pre><code>import random\nfrom ANNarchy import *\n\ndef probabilistic_pattern(pre, post, weight, probability):\n    # Create a LIL structure for the connectivity matrix\n    synapses = LILConnectivity()\n    # For all neurons in the post-synaptic population\n    for post_rank in xrange(post.size):\n        # Decide which pre-synaptic neurons should form synapses\n        ranks = []\n        for pre_rank in xrange(pre.size):\n            if random.random() &lt; probability:\n                ranks.append(pre_rank)\n        # Create weights and delays arrays of the same size\n        values = [weight for i in xrange(len(ranks)) ]\n        delays = [0 for i in xrange(len(ranks)) ]\n        # Add this information to the LIL matrix\n        synapses.add(post_rank, ranks, values, delays)\n\n    return synapses\n</code></pre> <p>The first for - loop iterates over all post-synaptic neurons in the projection. The inner for - loop decides for each of these neurons if a synapse with a pre-synaptic neuron should be created, based on the value <code>probability</code> provided as argument to the function.</p> <p>The lists <code>values</code> and <code>delays</code> are then created with the same size as <code>ranks</code> (important!), and filled with the desired value. All this information is then fed into the LIL matrix using the <code>add(post_rank, ranks, values, delays)</code> method.</p> <p>Note</p> <p>Building such connectivity matrices in Python can be extremely slow, as Python is not made for tight nested loops. If the construction of your network lasts too long, you should definitely write this function in Cython.</p> <p>Warning</p> <p>The <code>add()</code> should be only called once per post-synaptic neuron! If not, ANNarchy will have to reorder its internal representations and this will be really slow.</p> <p>Usage of the pattern</p> <p>To use the pattern within a projection you provide the pattern method to the <code>connect_with_func</code> method of <code>Projection</code></p> <pre><code>proj = Projection(\n    pre = pop1, \n    post = pop2, \n    target = 'inh' \n)\nproj.connect_with_func(\n    method=probabilistic_pattern, \n    weight=1.0, \n    probability=0.3\n)   \n</code></pre> <p><code>method</code> is the method you just wrote. Extra arguments (other than <code>pre</code> and <code>post</code>) should be passed with the same name.</p>"},{"location":"manual/Connector.html#fixed_probability-in-cython","title":"fixed_probability in Cython","text":"<p>For this example, we will create a Cython file <code>CustomPatterns.pyx</code> in the same directory as the script. Its content should be relatively similar to the Python version, except some type definitions:</p> <pre><code># distutils: language = c++\nimport random\nimport ANNarchy\ncimport ANNarchy.core.cython_ext.Connector as Connector\n\ndef probabilistic_pattern(pre, post, weight, probability):\n    # Typedefs\n    cdef Connector.LILConnectivity synapses\n    cdef int post_rank, pre_rank\n    cdef list ranks, values, delays\n\n    # Create a LILConnectivity structure for the connectivity matrix\n    synapses = Connector.LILConnectivity()\n    # For all neurons in the post-synaptic population\n    for post_rank in xrange(post.size):\n        # Decide which pre-synaptic neurons should form synapses\n        ranks = []\n        for pre_rank in xrange(pre.size):\n            if random.random() &lt; probability:\n                ranks.append(pre_rank)\n        # Create weights and delays arrays of the same size\n        values = [weight for i in xrange(len(ranks)) ]\n        delays = [0 for i in xrange(len(ranks)) ]\n        # Add this information to the LILConnectivity matrix\n        synapses.add(post_rank, ranks, values, delays)\n\n    return synapses\n</code></pre> <p>The only differences with the Python code are:</p> <ul> <li>The module <code>Connector</code> where the <code>LILConnectivity</code> connection matrix     class is defined should be cimported with:</li> </ul> <pre><code>cimport ANNarchy.core.cython_ext.Connector as Connector\n</code></pre> <ul> <li>Data structures should be declared with <code>cdef</code> at the beginning of     the method:</li> </ul> <pre><code># Typedefs\ncdef Connector.LILConnectivity synapses\ncdef int post_rank, pre_rank\ncdef list ranks, values, delays \n</code></pre> <p>To allow Cython to compile this file, we also need to provide with a kind of \\\"Makefile\\\" specifying that the code should be generated in C++, not C. This file should have the same name as the Cython file but end with <code>.pyxbld</code>, here : <code>CustomPatterns.pyxbld</code>.</p> <pre><code>from distutils.extension import Extension\nimport ANNarchy\n\ndef make_ext(modname, pyxfilename):\n    return Extension(name=modname,\n                     sources=[pyxfilename],\n                     include_dirs = ANNarchy.include_path(),\n                     extra_compile_args=['-std=c++11'],\n                     language=\"c++\")\n</code></pre> <p>Note</p> <p>This <code>.pyxbld</code> is generic, you don't need to modify anything, except its name.</p> <p>Now you can import the method <code>probabilistic_pattern()</code> into your Python code using the <code>pyximport</code> module of Cython and build the Projection normally:</p> <pre><code>import pyximport; pyximport.install()\nfrom CustomPatterns import probabilistic_pattern\nproj.connect_with_func(method=probabilistic_pattern, weight=1.0, probability=0.3)\n</code></pre> <p>Writing the connector in Cython can bring speedups up to 100x compared to Python if the projection has a lot of synapses.</p>"},{"location":"manual/ConvolutionalNetworks.html","title":"Convolution and pooling","text":"<p>Projections use by default a set of weights per post-synaptic neuron. Some networks, including convolutional networks, define a single operation (convolution or pooling) to be applied systematically on all pre-synaptic neurons. In such cases, it would be a waste of resources to allocate weights for each post-synaptic neuron. The extension <code>convolution</code> (see its API) allows to implement such specific projections. It has to be imported explicitly at the beginning of the script:</p> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.convolution import *\n</code></pre> <p>Warning</p> <p>Shared weights are only implemented for rate-coded networks. The only possible backend is currently OpenMP, CUDA will be implemented later.</p>"},{"location":"manual/ConvolutionalNetworks.html#simple-convolutions","title":"Simple convolutions","text":"<p>The simplest case of convolution is when the pre- and post-synaptic population have the same number of dimensions, for example:</p> <pre><code>pre = Population(geometry=(100, 100), \n        neuron=Neuron(parameters=\"r = 0.0\"))\npost = Population(geometry=(100, 100), \n        neuron=Neuron(equations=\"r = sum(exc)\"))\n</code></pre> <p>Contrary to normal projections, the geometry of the populations (number of dimensions and neurons in each dimension) has a great influence on the operation to be performed. In particular the number of dimensions will define how the convolution will be applied.</p> <p>If for example the pre-synaptic population represents an 2D image, you may want to apply a vertical edge detector to it and get the result in the post-synaptic population. Such a filter can be defined by the following Numpy array:</p> <pre><code>vertical_filter = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ]\n)\n</code></pre> <p>With 2 dimensions, the convolution operation (or more exactly, cross-correlation) with a 3*3 filter is defined for all neurons in the post-synaptic population by:</p> \\[\\text{post}[i, j] = \\sum_{c_i=-1}^1 \\sum_{c_j=-1}^1 \\text{filter}[c_i][c_j] \\cdot \\text{pre}[i + c_i, j + c_j]\\] <p>Such a convolution is achieved by creating a <code>Convolution</code> object and using the <code>connect_filter()</code> method to create the connection pattern:</p> <pre><code>proj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filter(weights=vertical_filter)\n</code></pre> <p>Each neuron of the post-synaptic population will then receive in <code>sum('exc')</code> (or whatever target name is used) the convolution between the kernel and a sub-region of the pre-syanptic population. ANNarchy defines the convolution operation for populations having 1, 2, 3, or 4 dimensions.</p> <p>Several options can be passed to the <code>convolve()</code> method:</p> <ul> <li><code>padding</code> defines the value of the pre-synaptic firing rates which     will be used when the coordinates are out-of-bounds. By default     zero-padding is used, but you can specify another value with this     argument. You can also use the <code>'border'</code> value to repeat the firing     rate of the neurons on the border (for example, if the filter tries     to reach a neuron of coordinates (-1, -1), the firing rate of the     neuron (0, 0) will be used instead).</li> <li><code>subsampling</code>. In convolutional networks, the convolution operation     is often coupled with a reduction in the number of neurons in each     dimension. In the example above, the post-synaptic population could     be defined with a geometry (50, 50). For each post-synaptic neuron,     the coordinates of the center of the applied kernel would be     automatically shifted from two pre-synaptic neurons compared to the     previous one. However, if the number of neurons in one dimension of     the pre-synaptic population is not exactly a multiple of the number     of post-synaptic neurons in the same dimension, ANNarchy can not     guess what the correct correspondance should be. In this case, you     have to specify this mapping by providing to the <code>subsampling</code>     argument a list of pre-synaptic coordinates defining the position of     the center of the kernel for each post-synaptic neuron. The list is     indexed by the rank of the post-synaptic neurons (use the     <code>rank_from_coordinates()</code> method) and must have the same size as the     population. Each element should be a list of coordinates in the     pre-synaptic population's geometry (with as many elements as     dimensions). It is possible to provide a Numpy array instead of a     list of lists.</li> </ul> <p>One can access the coordinates in the pre-synaptic geometry of the center of the filter corresponding to a particular post-synaptic neuron by calling the <code>center()</code> method of <code>Convolution</code> with the rank or coordinates of the post neuron:</p> <pre><code>pre = Population(geometry=(100, 100), neuron = Whatever)\npost = Population(geometry=(50, 50), neuron = Whatever)\n\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filter(weights=vertical_filter)\n\npre_coordinates = proj.center(10, 10) # returns (20, 20)\n</code></pre> <p>In some cases, the post-synaptic population can have less dimensions than the pre-synaptic one. An example would be when the pre-synaptic population has three dimensions (e.g. (100, 100, 3)), the last representing the R, G and B components of an image. A 3D filter, with 3 components in the last dimension, would result in a (100, 100, 1) post-synaptic population (or any subsampling of it). ANNarchy accepts in this case the use of a 2D population (100, 100), but it will be checked that the number of elements in the last dimension of the filter equals the number of pre-synaptic neurons in the last dimension:</p> <pre><code>pre = Population(geometry=(100, 100, 3), neuron=Neuron(parameters=\"r = 0.0\"))\npost = Population(geometry=(100, 100), neuron=Neuron(equations=\"r = sum(exc)\"))\n\nred_filter = np.array(\n    [\n        [\n            [2.0, -1.0, -1.0]\n        ]\n    ]\n)\n\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filter(weights=red_filter)\n</code></pre>"},{"location":"manual/ConvolutionalNetworks.html#non-linear-convolutions","title":"Non-linear convolutions","text":"<p><code>Convolution</code> uses by default a regular cross-correlation, summing <code>w * pre.r</code> over the extent of the kernel. As for regular synapses, you can change this behavior when creating the projection:</p> <ul> <li> <p>the <code>psp</code> argument defines what will be summed. It is <code>w*pre.r</code> by     default but can be changed to any combination of <code>w</code> and <code>pre.r</code>,     such as <code>w * log(1+pre.r)</code>:</p> <pre><code>proj = Convolution(pre=pre, post=post, target='exc', psp='w*log(1+pre.r)')\n</code></pre> </li> <li> <p>the <code>operation</code> argument allows to change the summation operation.     You can set it to 'max' (the maximum value of <code>w*pre.r</code> over the     extent of the filter will be returned), 'min' (minimum) or     'mean' (same as 'sum', but normalized by the number of elements     in the filter). The default is 'sum':</p> <pre><code>proj = Convolution(pre=pre, post=post, target='exc', operation='max')\n</code></pre> </li> </ul>"},{"location":"manual/ConvolutionalNetworks.html#layer-wise-convolutions","title":"Layer-wise convolutions","text":"<p>It is possible to define kernels with less dimensions than the pre-synaptic population. A 2D filter can for example be applied on each color component independently:</p> <pre><code>pre = Population(geometry=(100, 100, 3), neuron=Neuron(parameters=\"r = 0.0\"))\npost = Population(geometry=(50, 50, 3), neuron=Neuron(equations=\"r = sum(exc)\"))\n\nvertical_filter = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ]\n)\n\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filter(weights=vertical_filter, keep_last_dimension=True)\n</code></pre> <p>The important parameter in this case is <code>keep_last_dimension</code> which tells the code generator that the last dimension of the input should not be used for convolution. The important constraint is that the post-synaptic population must have the same number of neurons in the last dimension than the pre-synaptic one (no subsampling is possible by definition).</p>"},{"location":"manual/ConvolutionalNetworks.html#bank-of-filters","title":"Bank of filters","text":"<p>Convolutional networks often use banks of filters to perform different operations (such as edge detection with various orientations). It is possible to specify this mode of functioning by using the <code>connect_filters()</code> method:</p> <pre><code>pre = Population(geometry=(100, 100), neuron=Neuron(parameters=\"r = 0.0\"))\npost = Population(geometry=(50, 50, 4), neuron=Neuron(equations=\"r = sum(exc)\"))\n\nbank_filters = np.array(\n    [\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0],\n        [1.0, 0.0, -1.0]\n    ],\n    [\n        [-1.0, 0.0, 1.0],\n        [-1.0, 0.0, 1.0],\n        [-1.0, 0.0, 1.0]\n    ],\n    [\n        [-1.0, -1.0, -1.0],\n        [ 0.0,  0.0,  0.0],\n        [ 1.0,  1.0,  1.0]\n    ],\n    [\n        [ 1.0,  1.0,  1.0],\n        [ 0.0,  0.0,  0.0],\n        [-1.0, -1.0, -1.0]\n    ]\n)\n\nproj = Convolution(pre=pre, post=post, target='exc')\nproj.connect_filters(weights=bank_filters)\n</code></pre> <p>Here the filter has three dimensions. The first one must correspond to each filter. The last dimension of the post-synaptic population must correspond to the total number of filters. It cannot be combined with <code>keep_last_dimension</code>.</p> <p>Note</p> <p>Current limitation: Each filter must have the same size, it is not possible yet to convolve over multiple scales.</p>"},{"location":"manual/ConvolutionalNetworks.html#pooling","title":"Pooling","text":"<p>Another form of atypical projection for a neural network is the pooling operation. In max-pooling, each post-synaptic neuron is associated to a region of the pre-synaptic population and responds like the maximum firing rate in this region. This is already possible by defining the <code>operation</code> argument of the synapse type, but it would use instantiated synapses, what would be a waste of memory.</p> <p>The <code>Pooling</code> class allows to define such an operation without defining any synapse:</p> <pre><code>pre = Population(geometry=(100, 100), neuron=Neuron(parameters=\"r = 0.0\"))\npost = Population(geometry=(50, 50), neuron=Neuron(equations=\"r = sum(exc)\"))\n\nproj = Pooling(pre=pre, post=post, target='exc', operation='max')\nproj.connect_pooling()\n</code></pre> <p>The pooling region of a post-synaptic region is automatically determined by comparing the dimensions of the two populations: here each post-synaptic neuron will cover an area of 2*2 neurons.</p> <p>If the number of dimensions do not match, you have to specify the <code>extent</code> argument to <code>pooling()</code>. For example, you can pool completely over one dimension of the pre-synaptic population:</p> <pre><code>pre = Population(geometry=(100, 100, 10), neuron=Neuron(parameters=\"r = 0.0\"))\npost = Population(geometry=(50, 50), neuron=Neuron(equations=\"r = sum(exc)\"))\n\nproj = Pooling(pre=pre, post=post, target='exc', operation='max')\nproj.connect_pooling(extent=(2, 2, 10))\n</code></pre>"},{"location":"manual/ConvolutionalNetworks.html#sharing-weights-with-another-projection","title":"Sharing weights with another projection","text":"<p>A different possibility to share weights is between two projections. If your network is composed of populations of the same size, and the projection patterns are identical, it could save some memory to \"share\" the weights of one projection with another, so they are created only once.</p> <p>To this end, you can use the <code>Copy</code> class and pass it an existing projection:</p> <pre><code>pop1 = Population(geometry=(30, 30), neuron=Neuron(equations=\"r = sum(exc)\"))\npop2 = Population(geometry=(20, 20), neuron=Neuron(equations=\"r = sum(exc)\"))\npop3 = Population(geometry=(20, 20), neuron=Neuron(equations=\"r = sum(exc)\"))\n\n\nproj1 = Projection(pop1, pop2, 'exc')\nproj1.connect_gaussian(amp = 1.0, sigma=0.3, delays=2.0)\n\nproj2 = Copy(pop1, pop3, 'exc')\nproj2.connect_copy(proj1)\n</code></pre> <p>This only works when the pre- and post-populations of each projection have the same geometry, but they can be different, of course. If the original projection is learnable, the copied projection will see the changes. However, it is not possible for the shared projection to learn on its own. <code>Copy</code> only accepts <code>psp</code> and <code>operation</code> as parameters, which can be different from the original projection.</p> <p>It is only possible to copy regular projections, not other shared projections. The transmission delays will be identical between the two projections.</p>"},{"location":"manual/Hybrid.html","title":"Hybrid networks","text":"<p>ANNarchy has the possibility to simulate either rate-coded or spiking networks. It is therefore possible to define hybrid networks mixing rate-coded and spiking populations.</p> <p>A typical application would be to define a rate-coded network to process visual inputs, which is used to feed a spiking network for action selection. A dummy example is provided in <code>examples/hybrid</code>.</p>"},{"location":"manual/Hybrid.html#rate-coded-to-spike","title":"Rate-coded to Spike","text":"<p>Converting a rate-coded population to a spiking network is straightforward. The <code>PoissonPopulation</code> (see its API) defines a population of spiking neurons emitting spikes following a Poisson distribution:</p> <pre><code>pop = PoissonPopulation(1000, rates=50.)\n</code></pre> <p>In this case, the 1000 neurons emit spikes at a rate of 50 Hz (the rate of individual neurons can be later modified). It is possible to use a weighted sum of rate-coded synapses in order to determine the firing rate of each Poisson neuron. It requires to connect a rate-coded population to the <code>PoissonPopulation</code> with a given target:</p> <pre><code>pop1 = Population(4, Neuron(parameters=\"r=0.0\"))\npop2 = PoissonPopulation(1000, target='exc')\nproj = Projection(pop1, pop2, 'exc')\nproj.connect_fixed_number_pre(weights=10.0, number=1)\n</code></pre> <p>In this example, each of the 4 pre-synaptic neurons \"controls\" the firing rate of one fourth (on average) of the post-synaptic ones. If <code>target</code> is used in the Poisson population, <code>rates</code> will be ignored.</p> <p>The weights determine the scaling of the transmission: a presynaptic rate <code>r</code> of 1.0 generates a firing rate of <code>w</code> Hz in the post-synaptic neurons. Here setting <code>pop1.r = 1.0</code> will make the post-synaptic neurons fire at 10 Hz.</p>"},{"location":"manual/Hybrid.html#spike-to-rate-coded","title":"Spike to Rate-coded","text":"<p>Decoding a spiking population is a harder process, because of the stochastic nature of spike trains. One can take advantage of the fact here that a rate-coded neuron usually represents an ensemble of spiking neurons, so the average firing rate in that ensemble can be more precisely decoded.</p> <p>In order to do so, one needs to connect the spiking population to a rate-coded one with a many-to-one pattern using a <code>DecodingProjection</code>. A <code>DecodingProjection</code> heritates all methods of <code>Projection</code> (including the connection methods) but performs the necessary conversion from spike trains to a instantaneous rate:</p> <pre><code>pop1 = PoissonPopulation(1000, rates=50.0)\npop2 = Population(1, Neuron(equations=\"r=sum(exc)\"))\nproj = DecodingProjection(pop1, pop2, 'exc', window=10.0)\nproj.connect_all_to_all(weights=1.0)\n</code></pre> <p>In this example, the spiking population fires at 50 Hz. The single rate-coded neuron decoding that population will count how many spikes arrived in the last \\(T\\) milliseconds and divide it by the total number of synapses in order to estimate the population firing rate in <code>pop1</code>. This would be accessed in <code>sum(exc)</code> (or whatever target is used in the projection). Because of its simple definition, it will therefore have its rate <code>r</code> at 50.0 (with some variance due to the stochastic nature of spike trains).</p> <p>The <code>window</code> argument defines the duration in milliseconds of the sliding temporal window used to estimate the firing rate. By default, it is equal to <code>dt</code>, which means spikes are counted in a very narrow period of time, what could lead to very big variations of the decoded firing rate. If the window is too big, it would introduce a noticeable lag for the decoded firing rate if the input varies too quickly. <code>window = 10.0</code> is usually a good compromise, but this depends on the input firing rate.</p> <p>The weights of the projection define the scaling of the decoded firing rate. If one wants a firing rate of 100 Hz to be represented by <code>r=1.0</code>, the weights should be set to 0.01.</p> <p>No <code>Synapse</code> model can be used in a <code>DecodingProjection</code>.</p> <p>Warning</p> <p><code>DecodingProjection</code> is not implemented on CUDA yet.</p>"},{"location":"manual/Inputs.html","title":"Setting inputs","text":"<p>The methods to set inputs to a network depends on whether it is rate-coded or spiking. Specific populations to set inputs are described in the API.</p>"},{"location":"manual/Inputs.html#inputs-to-a-rate-coded-network","title":"Inputs to a rate-coded network","text":""},{"location":"manual/Inputs.html#standard-method","title":"Standard method","text":"<p>The simplest way to define an input population is to use a dummy neuron which simply holds a firing rate <code>r</code> as parameter, and connect it to another population:</p> <pre><code>input_pop = Population(10, Neuron(parameters=\"r=0.0\"))\n\npop = Population (10, LeakyIntegrator)\n\nproj = Projection(input_pop, pop, 'exc')\nproj.connect_one_to_one(1.0)\n\ncompile()\n\nsimulate(100.)\n\ninput_pop.r = 1.0\n\nsimulate(100.)\n</code></pre> <p>The only thing you need to do is to manipulate the numpy array <code>r</code> holded by <code>input_pop</code>, and it will influence the \\\"real\\\" population <code>pop</code></p> <p>It is important to define <code>r</code> as a parameter of the neuron, not a variable in <code>equations</code>. A variable sees its value updated at each step, so the value you set would be immediately forgotten.</p> <p>Note</p> <p>Using this method necessitates to interact with the input population in the Python script everytime you want to change the inputs. If the inputs change every time step, your simulation will alternate between Python and C++ executions and potentially become very slow.</p>"},{"location":"manual/Inputs.html#timed-arrays","title":"Timed Arrays","text":"<p>If the inputs change frequently, it may be more efficient to store all these values in a <code>TimedArray</code> (doc in the API).</p> <p>Let's suppose you have a population of 10 neurons which should be activated sequentially over time. You can store the inputs to these neurons in a Numpy array, where the first axis corresponds to time and the second (or more) to the geometry of the population:</p> <pre><code>inputs = np.array(\n    [\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n    ]\n)\n\ninp = TimedArray(rates=inputs)\n\npop = Population(10, Neuron(equations=\"r=sum(exc)\"))\n\nproj = Projection(inp, pop, 'exc')\nproj.connect_one_to_one(1.0)\n\ncompile()\n\nsimulate(10.)\n</code></pre> <p>With this code, each neuron will be activated in sequence at each time step (<code>dt=1.0</code> by default). If you simulate longer than 10 ms, the last input [0, 0, .., 1] will be kept forever.</p> <p>If the <code>rates</code> array has two dimensions, the corresponding population will be 1D. You can pass a multidimensional array to obtain a 2D or 3D population.</p> <p>Presenting a input for only one time step is very short, especially if the population <code>pop</code> uses ODEs to integrate the inputs. You can provide a <code>schedule</code> parameter to the <code>TimedArray</code> to define how long (in ms) an input should be presented:</p> <pre><code>inp = TimedArray(rates=inputs, schedule=10.)\n</code></pre> <p>Here each input will be kept constant for 10 ms, so the 10 inputs will need 100 ms of simulation to be presented. If you do not want a regular schedule, you can also provide a list of times where inputs should be set:</p> <pre><code>inp = TimedArray(rates=inputs, \n    schedule=[0., 10., 30., 60., 100., 150., 210., 280., 360., 450.])\n</code></pre> <p>The length of the <code>schedule</code> list should be equal or smaller to the number of inputs defined in <code>rates</code>. If this length is smaller (e.g. 7), only the 7 first inputs will be used as inputs. If the length is bigger, it will lead to an error.</p> <p>A <code>TimedArray</code> can be reset to iterate again over the inputs:</p> <pre><code>inp = TimedArray(rates=inputs, schedule=10.)\n\n...\n\ncompile()\n\nsimulate(100.) # The ten inputs are shown with a schedule of 10 ms\n\ninp.reset()\n\nsimulate(100.) # The same ten inputs are presented again.\n</code></pre> <p>The times declared in <code>schedule</code> are therefore relative to the last call to <code>reset()</code> (or to <code>t=0.0</code> at the beginning).</p> <p>If you want to systematically iterate over the inputs without iterating over <code>simulate()</code> and <code>reset()</code>, you can provide the <code>period</code> argument to the <code>TimedArray</code> to define how often the inputs will be reset:</p> <pre><code>inp = TimedArray(rates=inputs, schedule=10.. period=100.)\n\n...\n\nsimulate(100000.)\n</code></pre> <p>If the period is smaller than the total durations of the inputs, the last inputs will be skipped.</p> <p>The <code>rates</code>, <code>schedule</code> and <code>period</code> can be modified after compilation. The only constraint is that the size of the population (defined in the <code>rates</code> array) must stay the same.</p>"},{"location":"manual/Inputs.html#images-and-videos","title":"Images and Videos","text":"<p>Images</p> <p>A simple utility to directly load an image into the firing rates <code>r</code> of a Population is provided by the <code>ImagePopulation</code> class. This class is not automatically imported with ANNarchy, you need to explicitly import it:</p> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.image import ImagePopulation\n\ninp = ImagePopulation(geometry=(480, 640))\ninp.set_image('image.jpg')\n</code></pre> <p>Using this class requires that you have the Python Image Library installed (<code>pip install Pillow</code>). Any image with a format supported by Pillow can be loaded, see the documentation.</p> <p>The <code>ImagePopulation</code> must be initialized with a geometry corresponding to the desired size of the population. If it differs from the resolution of the image (set with <code>set_image</code>), the image will be first resized to match the geometry of the population.</p> <p>Note</p> <p>The size of an image is defined as (height, width), so a 640x480 image should be loaded in a (480, 640 population).</p> <p>If the geometry has only two dimensions (480, 640), each neuron will represent the luminance (or brightness) of the corresponding pixel.</p> <p>If the geometry has three dimensions (480, 640, 3), the color channels will additionally be represented (RGB). Any other value than 3 for the third dimension will generate an error.</p> <p>Note</p> <p>The firing rate <code>r</code> of a neuron is 1.0 when the corresponding pixel is white (value 255 as an unsigned integer on 8 bits).</p> <p>Note that the following code is functionally equivalent:</p> <pre><code>from ANNarchy import *\nfrom PIL import Image\n\ninp = Population(geometry=(480, 640), Neuron(parameters=\"r=0.0\"))\n\nimg = Image.open('image.jpg')\nimg = img.convert('L')\nimg = img.resize((480, 640)) /255.\n\ninp.r = np.array(img)\n</code></pre> <p>An example is provided in <code>examples/image/Image.py</code>.</p> <p>Videos</p> <p>The <code>VideoPopulation</code> class allows to retrieve images from a Webcam, using the OpenCV computer vision library, version 4.0 or later. <code>pkg-config opencv4 --cflags --libs</code> should not return an error. <code>vtk</code> might have to be additionally installed.</p> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.image import VideoPopulation\n\ninp = VideoPopulation(geometry=(480, 640))\n\ncompile()\n\ninp.start_camera(0)\n\nwhile(True):\n  inp.grab_image()\n  simulate(10.0)\n</code></pre> <p>A geometry must be provided as for <code>ImagePopulations</code>. The camera must be explicitly started after <code>compile()</code> with <code>inp.start_camera(0)</code>. 0 corresponds to the index of your camera, change it if you have multiple cameras.</p> <p>The <code>VideoPopulation</code> can then acquire frames from the camera with <code>inp.grab_image()</code> and store the correponding image in its firing rate <code>r</code> (also scaled between 0.0 and 1.0). An example is provided in <code>examples/image/Webcam.py</code>.</p> <p>Warning</p> <p><code>VideoPopulation</code> is not available with the CUDA backend.</p>"},{"location":"manual/Inputs.html#inputs-to-a-spiking-network","title":"Inputs to a spiking network","text":""},{"location":"manual/Inputs.html#standard-method_1","title":"Standard method","text":"<p>To control the spiking patterns of a spiking population, the simplest way is to inject current into the corresponding membrane potentials. The built-in neuron types defined by ANNarchy have a <code>i_offset</code> variable that can be used for this purpose:</p> <pre><code>from ANNarchy import *\nsetup(dt=0.1)\n\npop = Population(100, Izhikevich)\n\npop.i_offset= np.linspace(0.0, 30.0, 100)\n\nm = Monitor(pop, 'spike')\n\ncompile()\n\nsimulate(100.)\n\ndata = m.get('spike')\nt, n = m.raster_plot(data)\n\nimport matplotlib.pyplot as plt\nplt.plot(t, n, '.')\nplt.ylim(0, 100)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n</code></pre> <p></p>"},{"location":"manual/Inputs.html#current-injection","title":"Current injection","text":"<p>If you want the injected current to be time-varying, you can design a rate-coded population of the same size as the spiking population and create a <code>CurrentInjection</code> projection between them:</p> <pre><code>inp = Population(100, Neuron(equations=\"r = sin(t)\"))\n\npop = Population(100, Izhikevich)\n\nproj = CurrentInjection(inp, pop, 'exc')\nproj.connect_current()\n</code></pre> <p>The current <code>g_exc</code> of a neuron in <code>pop</code> will be set at each time step to the firing rate <code>r</code> of the corresponding neuron in <code>inp</code> (i.e. with the same rank). <code>inp</code> can also be defined as a <code>TimedArray</code>.</p> <p>The connector method should be <code>connect_current()</code>, which accepts no weight value and no delay.</p>"},{"location":"manual/Inputs.html#spikesourcearray","title":"SpikeSourceArray","text":"<p>If you want to control precisely the spiking patterns used as inputs, you can provide a list of spike times to a <code>SpikeSourceArray</code> object:</p> <pre><code>from ANNarchy import *\nsetup(dt=0.1)\n\nspike_times = [\n  [  10 + i/10,\n     20 + i/10,\n     30 + i/10,\n     40 + i/10,\n     50 + i/10,\n     60 + i/10,\n     70 + i/10,\n     80 + i/10,\n     90 + i/10] for i in range(100)\n]\n\npop = SpikeSourceArray(spike_times=spike_times)\n\nm = Monitor(pop, 'spike')\n\ncompile()\n\nsimulate(100.)\n\ndata = m.get('spike')\nt, n = m.raster_plot(data)\n\nimport matplotlib.pyplot as plt\nplt.plot(t, n, '.')\nplt.ylim(0, 100)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n</code></pre> <p></p> <p>The <code>spike_times</code> argument must be a list of lists containing the spike times in ms. Its length defines the number of neurons in the population. It is not possible to define a geometry. If one neuron should not spike at all, just provide an empty list. The different neurons can have a different number of spikes.</p> <p>If you want to repeat the same stimulation, you can reset the SpikeSourceArray, what will set its internal time back to 0.0:</p> <pre><code>simulate(100.)\n\npop.reset()\n\nsimulate(100.)\n</code></pre> <p>The spikes times can be changed after compilation, bit it must have the same number of neurons:</p> <pre><code>pop.spike_times = new_spike_times_array\n</code></pre> <p>An example is provided in <code>examples/pyNN/IF_curr_alpha.py</code>.</p> <p>Warning</p> <p><code>SpikeSourceArray</code> is not available with the CUDA backend.</p>"},{"location":"manual/Inputs.html#poisson-population","title":"Poisson population","text":"<p>The <code>PoissonPopulation</code> class allows to create a population of identical spiking neurons, whose spiking patterns vary according to a Poisson distribution:</p> <pre><code>from ANNarchy import *\nsetup(dt=0.1)\n\npop = PoissonPopulation(100, rates=30.)\n\nm = Monitor(pop, 'spike')\n\ncompile()\n\nsimulate(100.)\n\ndata = m.get('spike')\nt, n = m.raster_plot(data)\n\nimport matplotlib.pyplot as plt\nplt.plot(t, n, '.')\nplt.ylim(0, 100)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n</code></pre> <p>In this example, each of the 100 neurons fires randomly, with a mean firing rate of 30 Hz (next figure, top-left).</p> <p>It is also possible to specify the mean firing rate individually for each neuron (next figure, top-right):</p> <pre><code>pop = PoissonPopulation(100, rates=np.linspace(0.0, 100.0, 100))\n</code></pre> <p>The <code>rates</code> attribute can be modified at any time during the simulation, as long as it has the same size as the population.</p> <p>Another possibility is to define a rule for the evolution of the mean firing rate in the population (next figure, bottom-left):</p> <pre><code>pop = PoissonPopulation(\n        geometry=100,\n        parameters = \"\"\"\n            amp = 100.0\n            frequency = 50.0\n        \"\"\",\n        rates=\"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\"\n    )\n</code></pre> <p>The rule can only depend on the time <code>t</code>: the corresponding mean firing rate is the same for all neurons in the population.</p> <p>Finally, the <code>rates</code> argument can be replaced by a target, so it can be computed by another rate-coded population (next figure, bottom-right):</p> <pre><code>rates = 10.*np.ones((2, 100))\nrates[0, :50] = 100.\nrates[1, 50:] = 100.\ninp = TimedArray(rates = rates, schedule=50.)\n\npop = PoissonPopulation(100, target=\"exc\")\n\nproj = Projection(inp, pop, 'exc')\nproj.connect_one_to_one(1.0)\n</code></pre> <p>In the code above, we define a <code>TimedArray</code> for 100 neurons, so that half of the neurons fire at 100 Hz, while the others fire at 10 Hz. Every 50 ms, the two halves are swapped.</p> <p>We just need to create a projection with the target \\\"exc\\\" between the TimedArray and the PoissonPopulation (with a one-to-one pattern and weights 1.0 to preserve scaling), and the Poisson population will reflect the firing rates defined by the TimedArray.</p> <p></p>"},{"location":"manual/Inputs.html#homogeneous-correlated-inputs","title":"Homogeneous correlated inputs","text":"<p><code>HomogeneousCorrelatedSpikeTrains</code> defines spiking neurons following a homogeneous distribution with correlated spike trains.</p> <p>The method describing the generation of homogeneous correlated spike trains is described in:</p> <p>Brette, R. (2009). Generation of correlated spike trains. http://audition.ens.fr/brette/papers/Brette2008NC.html</p> <p>The implementation is based on the one provided by Brian.</p> <p>To generate correlated spike trains, the population rate of the group of Poisson-like spiking neurons varies following a stochastic differential equation:</p> \\[\\dfrac{dx}{dt} = \\dfrac{\\mu - x}{\\tau} + \\sigma \\, \\dfrac{\\xi }{\\sqrt{\\tau}}\\] <p>where \\(\\xi\\) is a random variable. Basically, \\(x\\) will randomly vary around $\\mu\u00a7 over time, with an amplitude determined by \\(\\sigma\\) and a speed determined by \\(\\tau\\).</p> <p>This doubly stochastic process is called a Cox process or Ornstein-Uhlenbeck process.</p> <p>To avoid that x becomes negative, the values of mu and sigma are computed from a rectified Gaussian distribution, parameterized by the desired population rate rates, the desired correlation strength corr and the time constant tau. See Brette's paper for details.</p> <p>In short, you should only define the parameters <code>rates</code>, <code>corr</code> and <code>tau</code>, and let the class compute mu and sigma for you. Changing <code>rates</code>, <code>corr</code> or <code>tau</code> after initialization automatically recomputes mu and sigma.</p> <p>Example:</p> <pre><code>from ANNarchy import *\nsetup(dt=0.1)\n\npop_poisson = PoissonPopulation(200, rates=10.)\npop_corr    = HomogeneousCorrelatedSpikeTrains(200, rates=10., corr=0.3, tau=10.)\n\ncompile()\n\nsimulate(1000.)\n\npop_poisson.rates=30.\npop_corr.rates=30.\n\nsimulate(1000.)\n</code></pre> <p></p>"},{"location":"manual/Logging.html","title":"Logging with tensorboard","text":"<p>The <code>tensorboard</code> extension allows to visualize ANNarchy simulations using tensorboard. It requires the <code>tensorboardX</code> package:</p> <pre><code>pip install tensorboardX\n</code></pre> <p>as well as tensorboard, of course:</p> <pre><code>pip install tensorboard\n</code></pre> <p>The Logger class is a thin wrapper around <code>tensorboardX.SummaryWriter</code>, which you could also use directly. The doc is available at https://tensorboardx.readthedocs.io. Tensorboard can read any logging data, as long as they are saved in the right format (tfevents), so it is not limited to tensorflow. TensorboardX has been developed to allow the use of tensorboard with pytorch.</p> <p>The extension has to be imported explicitly:</p> <pre><code>from ANNarchy import *\nfrom ANNarchy.extensions.tensorboard import Logger\n</code></pre> <p>For detailed examples of how to use the extension, refer to the examples Basal Ganglia and Bayesian optimization, which are available as notebooks in the folder <code>examples/tensorboard</code>.</p>"},{"location":"manual/Logging.html#creating-the-logger","title":"Creating the logger","text":"<p>The <code>Logger</code> class has to be closed properly at the end of the script, so it is advised to use a context:</p> <pre><code>with Logger() as logger:\n    logger.add_scalar(\"Accuracy\", acc, trial)\n</code></pre> <p>You can also make sure to close it:</p> <pre><code>logger = Logger()\nlogger.add_scalar(\"Accuracy\", acc, trial)\nlogger.close()\n</code></pre> <p>By default, the logs will be written in a subfolder of <code>./runs/</code> (which will be created in the current directory). The subfolder is a combination of the current datetime and of the hostname, e.g. <code>./runs/Apr22_12-11-22_machine</code>. You can control these two elements by passing arguments to <code>Logger()</code>:</p> <pre><code>with Logger(logdir=\"/tmp/annarchy\", experiment=\"trial1\"): # logs in /tmp/annarchy/trial1\n</code></pre>"},{"location":"manual/Logging.html#launching-tensorboard","title":"Launching tensorboard","text":"<p>After (or while) logging data within your simulation, run <code>tensorboard</code> in the terminal by specifying the path to the log directory:</p> <pre><code>tensorboard --logdir runs\n</code></pre> <p>You will then be asked to open <code>localhost:6006</code> in your browser and will see a page similar to this:</p> <p></p> <p>The information logged will be available in the different tabs (scalars, images...). You can also visualize and compare several experiments at the same time.</p>"},{"location":"manual/Logging.html#logging-scalars","title":"Logging scalars","text":"<p>The <code>add_*</code> methods allow you to log various structures, such as scalars, images, histograms, figures, etc.</p> <p>The simplest information to log is a scalar, for example the accuracy at the end of a trial:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        accuracy = ...\n        logger.add_scalar(\"Accuracy\", accuracy, trial)\n</code></pre> <p>A tag should be given for each plot as the first argument. In the example above, the figure with the accuracy will be labelled \\\"Accuracy\\\" in tensorboard. You can also group plots together with 2-levels tags such as:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        train_accuracy = ...\n        test_accuracy = ...\n        logger.add_scalar(\"Accuracy/Train\", train_accuracy, trial)\n        logger.add_scalar(\"Accuracy/Test\", test_accuracy, trial)\n</code></pre> <p>The second argument is the scalar, obviously. The third is the index of x-axis of the plot. It can be the index of the trial, the current time or whatever you prefer.</p> <p>If you want to display several scalars on the same plot, you can use the method <code>add_scalars()</code> and provide a dictionary:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        train_accuracy = ...\n        test_accuracy = ...\n        logger.add_scalars(\"Accuracy\", {'train': train_accuracy, 'test': test_accuracy}, trial)\n</code></pre>"},{"location":"manual/Logging.html#logging-images","title":"Logging images","text":"<p>It is also possible to log images, for example an input image or the firing rate of a 2D population, with the <code>add_image()</code> method:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        img = pop.r.reshape((10, 10))\n        logger.add_image(\"Population/Firing rate\", img, trial)\n</code></pre> <p>The image must be a numpy array of size (height, width) for monochrome images or (height, width, 3) for colored images.</p> <p>The values must be floats between 0 and 1 or integers between 0 and 255 in order to be displayed correctly. You can either do it yourself, or pass <code>equalize=True</code> to the <code>add_image()</code>:</p> <pre><code>logger.add_image(\"Population/Firing rate\", img, trial, equalize=True)\n</code></pre> <p>The min/max values in the array are internally used to rescale the image:</p> <pre><code>img = (img - img.min())/(img.max() - img.min())\n</code></pre> <p>To display several images together, for example the receptive fields of a population, an array of size (number, height, width) or (number, height, width, 3) can be passed to <code>add_images()</code>, where number is the number of images to display:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        weights= proj.w.reshape(100, 10, 10) # 100 post neurons, 10*10 pre neurons\n        logger.add_images(\"Projection/Receptive fields\", weights, trial, equalize=True)\n</code></pre> <p><code>equalize=True</code> applies the same scaling to all images, but you additionally pass <code>equalize_per_image=True</code> to have indepent scalings per image.</p>"},{"location":"manual/Logging.html#logging-histograms","title":"Logging histograms","text":"<p>Histograms can also be logged, for example to visualize the statistics of weights in a projection:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        weights= proj.w.flatten()\n        logger.add_histogram(\"Weight distribution\", weights, trial)\n</code></pre>"},{"location":"manual/Logging.html#logging-figures","title":"Logging figures","text":"<p>Matplotlib figures can also be logged:</p> <pre><code>with Logger() as logger:\n    for trial in range(100):\n        simulate(1000.0)\n        fig = plt.figure()\n        plt.plot(pop.r)\n        logger.add_figure(\"Activity\", fig, trial)\n</code></pre> <p><code>add_figure()</code> will automatically close the figure, no need to call <code>show()</code>.</p> <p>Beware that this is very slow and requires a lot of space.</p>"},{"location":"manual/Logging.html#logging-parameters","title":"Logging parameters","text":"<p>The previous methods can be called multiple times during a simulation, in order to visualize the changes during learning.</p> <p><code>add_parameters()</code> is more useful in the context of hyperparameter optimization, where the same network with different parameters is run multiple times.</p> <p>Only once per simulation, typically at the end, you can log the value of some important parameters together with some metrics such as accuracy, error rate or so. This will allow tensorboard to display over multiple runs the relation between the parameters and the metrics in the tab \"HPARAMS\":</p> <pre><code>with Logger() as logger:\n    # ...\n    logger.add_parameters({'learning_rate': lr, 'tau': tau}, {'accuracy': accuracy}) \n</code></pre> <p>Refer to Bayesian optimization for an example using the <code>hyperopt</code> library.</p>"},{"location":"manual/Network.html","title":"Parallel simulations and networks","text":"<p>A typical ANNarchy script represents a single network of populations and projections. Most of the work in computational neuroscience consists in running the same network again and again, varying some free parameters each time, until the fit to the data is publishable. The <code>reset()</code> allows to return the network to its state before compilation, but this is particularly tedious to implement.</p> <p>In order to run different networks using the same script, the <code>Network</code> object can be used to make copies of existing objects (populations, projections and monitors) and simulate them in parallel using <code>parallel_run()</code>.</p> <p>Let's suppose the following dummy network is defined:</p> <pre><code>pop1 = PoissonPopulation(100, rates=10.0)\npop2 = Population(100, Izhikevich)\nproj = Projection(pop1, pop2, 'exc')\nproj.connect_fixed_probability(weights=5.0, probability=0.2)\nm = Monitor(pop2, 'spike')\n\ncompile()\n</code></pre> <p>One would like to compare the firing patterns in <code>pop2</code> when:</p> <ul> <li>There is no input to <code>pop2</code>.</li> <li>The Poisson input is at 10 Hz.</li> <li>The Poisson input is at 20 Hz.</li> </ul> <p>Warning</p> <p>Running multiple networks in parallel is not supported on CUDA.</p> <p>Note</p> <p><code>parallel_run()</code> uses the <code>multiprocessing</code> module to start parallel processes. On Linux, it should work directly, but there is an issue on OSX. Since Python 3.8, the 'spawn' method is the default way to start processes, but it does not work on MacOS. The following code should fix the issue, but it should only be ran once in the script.</p> <pre><code>import platform\nif platform.system() == \"Darwin\":\n    import multiprocessing as mp\n    mp.set_start_method('fork')\n</code></pre>"},{"location":"manual/Network.html#parallel-simulation","title":"Parallel simulation","text":""},{"location":"manual/Network.html#parallel_run","title":"parallel_run","text":"<p>The most simple method is to create a single network and to iterate over some parameters values to run identical simulations multiple times using <code>parallel_run()</code>:</p> <pre><code>pop1 = PoissonPopulation(100, rates=10.0)\npop2 = Population(100, Izhikevich)\nproj = Projection(pop1, pop2, 'exc')\nproj.connect_fixed_probability(weights=5.0, probability=0.2)\nm = Monitor(pop2, 'spike')\n\ncompile()\n\ndef simulation(idx, net):\n    net.get(pop1).rates = 10. * idx\n    net.simulate(1000.)\n    return net.get(m).raster_plot()\n\nresults = parallel_run(method=simulation, number=3)\n\nt1, n1 = results[0]\nt2, n2 = results[1]\nt3, n3 = results[2]\n</code></pre> <p>The <code>simulation()</code> method will be called over three copies of the network (in different processes). The first argument to this method must be an integer corresponding to the index of a network (here <code>idx = [0, 1, 2]</code>), the second must be a <code>Network</code> object (class Network../API/Network.md)).</p> <p>Populations, projections and monitors of a network must be accessed with:</p> <pre><code>net.get(pop1)\nnet.get(pop2)\nnet.get(proj)\nnet.get(m)\n</code></pre> <p>Networks only work on copies of the corresponding objects at the time they are added to the network. It is no use to modify the <code>rates</code> parameter of <code>pop1</code> after the network are created: the network is now independent. Only <code>net.get(pop1).rates</code> allows to change <code>rates</code> for the current simulation. Similarly, it is useless to read variables from the original objects when the networks are simulated: they would still have their original values.</p> <p>You do not have access on the internally-created networks after the simulation (they are in a separate memory space). The method must the data you want to analyze (here the raster plot) or write them to disk (in separate files).</p> <p><code>parallel_run()</code> returns a list of the values returned by the passed method:</p> <pre><code>results = parallel_run(method=simulation, networks=[net1, net2, net3])\n\nt1, n1 = results[0]\nt2, n2 = results[1]\nt3, n3 = results[2]\n</code></pre> <p>If you initialize some variables randomly in the main network, for example:</p> <pre><code>pop2.v = -60. + 10. * np.random.random(100)\n</code></pre> <p>they will have the same value in all networks, they are not drawn again. You need to perform random initialization on each network inside the simulation method:</p> <pre><code>def simulation(idx, net):\n    net.get(pop1).rates = 10. * idx\n    net.get(pop2).v = -60. + 10. * np.random.random(100)\n    net.simulate(1000.)\n    return net.get(m).raster_plot()\n</code></pre> <p>Oppositely, connection methods having a random components (e.g. <code>connect_fixed_probability()</code> or using <code>weights=Uniform(0.0, 1.0)</code>) will be redrawn for each network.</p> <p>Global simulation methods should not be called directly, even with the <code>net_id</code> parameter. The <code>Network</code> class overrides them:</p> <pre><code>net.step()\nnet.simulate()\nnet.simulate_until()\nnet.reset()\nnet.get_time()\nnet.set_time(t)\nnet.get_current_step()\nnet.set_current_step(t)\nnet.set_seed(seed)\nnet.enable_learning()\nnet.disable_learning()\nnet.get_population(name)\n</code></pre>"},{"location":"manual/Network.html#passing-additional-arguments","title":"Passing additional arguments","text":"<p>The two first obligatory arguments of the simulation callback are <code>idx</code>, the index of the network in the simulation, and <code>net</code>, the network object. You can of course use other names, but these two arguments will be passed.</p> <p><code>idx</code> can be used for example to access arrays of parameter values:</p> <pre><code>rates = [0.0, 0.1, 0.2, 0.3, 0.4]\ndef simulation(idx, net):\n    net.get(pop1).rates = rates[idx]\n    ...\n\nresults = parallel_run(method=simulation, number=len(rates))\n</code></pre> <p>Another option is to provide additional arguments to the <code>simulation</code> callback during the <code>parallel_run()</code> call:</p> <pre><code>def simulation(idx, net, rates):\n    net.get(pop1).rates = rates\n    ...\n\nrates = [0.0, 0.1, 0.2, 0.3, 0.4]\nresults = parallel_run(method=simulation, number=len(rates), rates=rates)\n</code></pre> <p>These additional arguments must be lists of the same size as the number of networks (<code>number</code> or <code>len(networks)</code>). You can use as many additional arguments as you want:</p> <pre><code>def simulation(idx, net, a, b, c, d):\n    ...\nresults = parallel_run(method=simulation, number=10, a=..., b=..., c=..., d=...)\n</code></pre> <p>In <code>parallel_run()</code>, the arguments can be passed in any order, but they must be named (e.g. <code>, a=list(range(0)),</code>, not <code>, list(range(10)),</code>).</p>"},{"location":"manual/Network.html#multiple-network-instances","title":"Multiple network instances","text":"<p>One can also create three different <code>Network</code> objects to implement the three conditions:</p> <pre><code>net1 = Network()\nnet1.add([pop2, m])\nnet1.compile()\n</code></pre> <p>The network is created empty, and the population <code>pop2</code> as well as the attached monitor are added to it through the <code>add()</code> method. This method takes a list of objects (populations, projections and monitors).</p> <p>The network has then to be compiled by calling the <code>compile()</code> method specifically on the network. The network can be simulated independently by calling <code>simulate()</code> or <code>simulate_until()</code> on the network.</p> <p>The basic network, with inputs at 10 Hz, can be simulated directly using the normal methods, or copied into a new network:</p> <pre><code>net2 = Network()\nnet2.add([pop1, pop2, proj, m])\nnet2.compile()\n</code></pre> <p>Here, all defined objects are added to the network. It would be easier to pass the <code>everything</code> argument of the Network constructor as <code>True</code>, which has the same effect. We can use this for the third network:</p> <pre><code>net3 = Network(everything=True)\nnet3.get(pop1).rates = 20.0\nnet3.compile()\n</code></pre> <p>Here, the population <code>pop1</code> of the third network has to be accessed though the <code>get()</code> method. The data corresponding to <code>pop1</code> will not be the same as for <code>net3.get(pop1)</code>, only the geometry and neuron models are the same.</p> <p>Once a network is compiled, it can be simulated (but it does not matter if the other networks are also compiled, including the \\\"original\\\" network):</p> <pre><code>net1.simulate(1000.)\nnet2.simulate(1000.)\nnet3.simulate(1000.)\n</code></pre> <p>Spike recordings have to be accessed per network, through the copies of the monitor <code>m</code>:</p> <pre><code>t1, n1 = net1.get(m).raster_plot()\nt2, n2 = net2.get(m).raster_plot()\nt3, n3 = net3.get(m).raster_plot()\n</code></pre> <p>One can also call the <code>parallel_run()</code> method and pass it a list of networks instead of <code>number</code>:</p> <pre><code>parallel_run(method=simulation, networks=[net1, net2, net3])\n</code></pre> <p>This will apply <code>simulation()</code> in parallel on the 3 networks, reducing the total computation time.</p>"},{"location":"manual/Notebooks.html","title":"Notebooks","text":"<p>It is possible to use ANNarchy in Jupyter / IPython notebooks. Several examples are provided in the <code>examples</code> directory of the source distribution.</p> <p>There are nevertheless two things to take into account when re-running cells:</p> <p>The solution to these problems is to call the <code>clear()</code> command right after importing ANNarchy. This deletes all created objects and puts ANNarchy in a \"clean\" state:</p> <pre><code>from ANNarchy import *\nclear()\n</code></pre> <pre>\n<code>ANNarchy 4.6 (4.6.9b) on linux (posix). \n</code>\n</pre> <p>When you change something to the parameters of your network, you can re-run this cell, as well as all cells defining your network. You will then be able to call <code>compile()</code> again.</p> <p>This is equivalent to a reset of your network. However, if the structure of your network changes (new populations, changed equations), this will have no effect because of the second issue.</p>"},{"location":"manual/Notebooks.html#jupyter-ipython-notebooks","title":"Jupyter / IPython Notebooks","text":""},{"location":"manual/Notebooks.html#annarchy-uses-global-variables-to-store-the-populations-and-projections","title":"ANNarchy uses global variables to store the populations and projections","text":"<p>Internally, there is a network manager that stores which population and projection has been declared. It is empty when starting a script, but it can know anything about the Jupyter cells. Here are the main consequences:</p> <ol> <li> <p>If you re-run the line <code>from ANNarchy import *</code>, Python will not clear the stored populations and projections (by design, as ANNarchy is already cached)</p> </li> <li> <p>If you re-run a cell creating a population or projection, it will create a new population, not replace the previous one.</p> </li> <li> <p>If you create a new population / projection after a call to <code>compile()</code> in the current kernel, this will lead to an error, as the network is already compiled and new objects cannot (yet) be added on the fly.</p> </li> </ol>"},{"location":"manual/Notebooks.html#python-cannot-dynamically-reload-modules","title":"Python cannot dynamically reload modules","text":"<p>If you change something in the definition of the neurons or synapses (anything that usually provoke a recompilation), this will have NO effect in the notebook, even if the network is successfully recompiled. </p> <p>Python is unable to reload libraries dynamically at runtime (). The C++ core actually running the simulation is imported as a dynamic library, so the result of the recompilation will not be reloaded.</p> <p>The only solution is to restart the kernel.</p>"},{"location":"manual/NumericalMethods.html","title":"Equations and numerical methods","text":""},{"location":"manual/NumericalMethods.html#numerical-methods","title":"Numerical methods","text":"<p>First-order ordinary differential equations (ODE) can be solved using different numerical methods. The method can be declared globally in the <code>setup()</code> call and used in all ODEs of the network:</p> <pre><code>from ANNarchy import *\n\nsetup(method='exponential')\n</code></pre> <p>or specified explicitely for each ODE by specifying a flag:</p> <pre><code>equations = \"\"\"\n    tau * dV/dt  + V =  A : init = 0.0, exponential\n\"\"\"\n</code></pre> <p>If nothing is specified, the explicit Euler method will be used.</p> <p>Different numerical methods are available:</p> <ul> <li>Explicit Euler <code>'explicit'</code></li> <li>Implicit Euler <code>'implicit'</code></li> <li>Exponential Euler <code>'exponential'</code></li> <li>Midpoint <code>'midpoint'</code></li> <li>Fourth-order Runge-Kutta <code>'rk4'</code></li> <li>Event-driven <code>'event-driven'</code></li> </ul> <p>Each method has advantages/drawbacks in term of numerical error, stability and computational cost.</p> <p>To describe these methods, we will take the example of a system of two linear first-order ODEs:</p> \\[\\frac{dx(t)}{dt} = f(x(t), y(t)) = a_x \\cdot x(t) + b_x \\cdot y(t) + c_x\\] \\[\\frac{dy(t)}{dt} = g(x(t), y(t)) = a_y \\cdot x(t) + b_y \\cdot y(t) + c_y\\] <p>The objective of a numerical method is to approximate the value of \\(x\\) and \\(y\\) at time \\(t+h\\) based on its value at time \\(t\\), where \\(h\\) is the discretization time step (noted <code>dt</code> in ANNarchy):</p> \\[x(t + h) = F(x(t), y(t))\\] \\[y(t + h) = G(x(t), y(t))\\] <p>At each step of the simulation, the new values for the variables are computed using this update rule and will be used for the following step.</p> <p>The derivative of each variable is usually approximated by:</p> \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h}\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h}\\] <p>The different numerical methods mostly differ in the time at which the functions \\(f\\) and \\(g\\) are evaluated.</p>"},{"location":"manual/NumericalMethods.html#explicit-euler-method","title":"Explicit Euler method","text":"<p>The explicit (forward) Euler method computes the next value for the variables by estimating their derivative at time \\(t\\):</p> \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t), y(t))\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t), y(t))\\] <p>so the solution is straightforward to obtain:</p> \\[x(t+h) =  x(t) + h \\cdot  f(x(t), y(t))\\] \\[y(t+h) = y(t) + h \\cdot g(x(t), y(t))\\]"},{"location":"manual/NumericalMethods.html#implicit-euler-method","title":"Implicit Euler method","text":"<p>The implicit (backward) Euler method computes the next value for the variables by estimating their derivative at time \\(t + h\\):</p> \\[\\frac{dx(t)}{dt} = \\frac{x(t+h) - x(t)}{h} = f(x(t+h), y(t+h))\\] \\[\\frac{dy(t)}{dt} = \\frac{y(t+h) - y(t)}{h} = g(x(t+h), y(t+h))\\] <p>This leads to a system of equations which must be solved in order to find the update rule. With the linear equations defined above, we need to solve:</p> \\[\\frac{x(t+h) - x(t)}{h} = a_x \\cdot x(t + h) + b_x \\cdot y(t + h) + c_x\\] \\[\\frac{y(t+h) - y(t)}{h} = a_y \\cdot x(t + h) + b_y \\cdot y(t + h) + c_y\\] <p>what gives something like:</p> \\[x(t+h) =  x(t) - h \\cdot \\frac{ \\left(a_{x} x(t) + b_{x} y(t) + c_{x} + h \\left(- a_{x} b_{y} x(t) + a_{y} b_{x} x(t) + b_{x} c_{y} - b_{y} c_{x}\\right)\\right)}{h^{2} \\left(- a_{x} b_{y} + a_{y} b_{x}\\right) + h \\left(a_{x} + b_{y}\\right) - 1}\\] \\[y(t+h) = y(t) -h \\cdot  \\frac{ a_{y} \\left(c_{x} h + x(t)\\right) + y(t) \\left(- a_{y} b_{x} h^{2} + \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)\\right) + \\left(a_{x} h - 1\\right) \\left(c_{y} h + y(t)\\right)}{a_{y} b_{x} h^{2} - \\left(a_{x} h - 1\\right) \\left(b_{y} h - 1\\right)}\\] <p>ANNarchy relies on Sympy to solve and simplify this system of equations and generate the update rule.</p> <p>Note</p> <p>This method is obviously much more computationally expensive than the explicit Euler method, although more stable. The midpoint method is a better trade-off between complexity and stability than the implicit Euler method.</p>"},{"location":"manual/NumericalMethods.html#exponential-euler","title":"Exponential Euler","text":"<p>The exponential Euler method is particularly stable for single first-order linear equations, of the type:</p> \\[\\tau(t) \\cdot \\frac{dx(t)}{dt}  + x(t) =  A(t)\\] <p>The update rule is then given by:</p> \\[x(t+h) = x(t) + (1 - \\exp(- \\frac{h}{\\tau(t)}) ) \\cdot (A(t) - x(t))\\] <p>The difference with the explicit Euler method is the step size, which is an exponential function of the ratio \\(\\frac{\\tau}{h}\\). The accurary of the exponential Euler method on linear first-order ODEs is close to perfect, compared to the other Euler methods. As it is an explicit method, systems of equations are solved very easily with the same rule.</p> <p>When the exponential method is used, ANNarchy first tries to reduce the ODE to its canonical form above (with the time constant being possibly dependent on time or inputs) and then generates the update rule accordingly.</p> <p>For example, the description:</p> <pre><code>tau * dv/dt = (E - v) + g_exc * (Ee - v) + g_inh * (v - Ei)\n</code></pre> <p>would be first transformed in:</p> <pre><code>(1 + g_exc - g_inh) * dv/dt + v = (E + g_exc * Ee - g_inh * Ei) / (1 + g_exc - g_inh)\n</code></pre> <p>before being transformed into an update rule, with \\(\\tau(t) = 1 + g_\\text{exc} - g_\\text{inh}\\):</p> \\[v(t+h) = v(t) + (1 - \\exp(- \\frac{h}{1 + g_\\text{exc} - g_\\text{inh}}) ) \\cdot (\\frac{E + g_\\text{exc} \\cdot E_e - g_\\text{inh} \\cdot E_i}{1 + g_\\text{exc} - g_\\text{inh}} - v(t))\\] <p>The exponential method can only be applied to first-order linear ODEs. Any other form of ODE will be rejected by the parser.</p> <p>Important note: The step size \\(1 - \\exp(- \\frac{h}{\\tau(t)})\\) is computationally expensive because of the exponential function. If the time constant \\(\\tau\\) is a global parameter of the population or projection, ANNarchy can pre-compute the step size outside of the <code>for</code> loop over all neurons/synapses, which leads to huge increases in performance. The <code>exponential</code> method should therefore be reserved to first-order linear ODEs with the same time constant for all neurons/synapses:</p> <pre><code>neuron = Neuron(\n    parameters = \"tau = 10. : population\",\n    equations = \"tau * dr/dt + r = sum(exc) : min=0.0, exponential\"\n)\n</code></pre>"},{"location":"manual/NumericalMethods.html#midpoint","title":"Midpoint","text":"<p>The midpoint method is a Runge-Kutta method of order 2. It estimates the derivative in the middle of the interval \\(t + \\frac{h}{2}\\).</p> \\[k_x = f(x(t), y(t))\\] \\[k_y = g(x(t), y(t))\\] \\[x(t+h) =  x(t) + h \\cdot  f(x(t) + k_x \\cdot \\frac{h}{2}, y(t) +  k_y \\cdot \\frac{h}{2})\\] \\[y(t+h) = y(t) + h \\cdot g(x(t) + k_x \\cdot \\frac{h}{2}, y(t) +  k_y \\cdot \\frac{h}{2})\\]"},{"location":"manual/NumericalMethods.html#runge-kutta-4","title":"Runge-Kutta 4","text":"<p>The fourth-order Runge-Kutta method estimates the derivative at four different points and combines them:</p> \\[ k_1 =  f(x(t)) \\] \\[ k_2 =  f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_1) \\] \\[ k_3 =  f(x(t + \\frac{h}{2}) + \\frac{h}{2}\\cdot k_2) \\] \\[ k_4 =  f(x(t + h) + h \\cdot k_3) \\] \\[ x(t+h) = x(t) + \\frac{h}{6} \\cdot (k_1 + 2 \\cdot k_2 + 2 \\cdot k_3 + k_4 ) \\]"},{"location":"manual/NumericalMethods.html#event-driven","title":"Event-driven","text":"<p>Event-driven integration is only available for spiking synapses with variables following linear first-order dynamics. Let's consider the following STDP synapse (see Spiking Synapse for explanations):</p> <pre><code>STDP = Synapse(\n    parameters = \"\"\"\n        tau_pre = 10.0 : postsynaptic\n        tau_post = 10.0 : postsynaptic\n    \"\"\",\n    equations = \"\"\"\n        tau_pre * dApre/dt = - Apre : event-driven\n        tau_post * dApost/dt = - Apost : event-driven\n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre\n        w = clip(w + Apost, 0.0 , 1.0)\n    \"\"\",\n    post_spike = \"\"\"\n        Apost += cApost\n        w = clip(w + Apre, 0.0 , 1.0)\n    \"\"\"\n)\n</code></pre> <p>The value of <code>Apost</code> and <code>Apre</code> is only needed when a pre- or post-synaptic spike occurs at the synapse, so there is no need to integrate the corresponding equations between two such events. First-order linear ODEs have the nice property that their analytical solution is easy to obtain. Let's consider an equation of the form:</p> \\[\\tau  \\frac{dv}{dt} = E - v\\] <p>If \\(v\\) has the value \\(V_0\\) at time \\(t\\), its value at time \\(t + \\Delta t\\) is given by:</p> \\[v(t + \\Delta t) = V_0 \\cdot \\exp(-\\frac{\\Delta t}{\\tau})\\] <p>Note</p> <p>If the synapse defines a <code>psp</code> argument (synaptic transmission is continuous), or if another continuous variable depends on the value of an event-driven one, it is not possible to use event-driven integration.</p>"},{"location":"manual/NumericalMethods.html#order-of-evaluation","title":"Order of evaluation","text":"<p>The values of variables are stored in a single array in order to save some memory. Special care therefore has to be taken on whether the update of a variable depends on the value of another variable at the previous time step or in the same step.</p>"},{"location":"manual/NumericalMethods.html#systems-of-odes","title":"Systems of ODEs","text":"<p>Systems of ODEs are integrated concurrently, which means that the following system:</p> <pre><code>tau*dv/dt = I - v - u\ntau*du/dt = v - u\n</code></pre> <p>would be numerized using the explicit Euler method as:</p> <pre><code>v[t+1] = v[t] + dt*(I - v[t] - u[t])/tau\nu[t+1] = u[t] + dt*(v[t] - u[t])/tau\n</code></pre> <p>As we use a single array, the generated code is similar to:</p> <pre><code>new_v = v + dt*(I - v - u)/tau\nnew_u = u + dt*(v - u)/tau\n\nv = new_v\nu = new_u\n</code></pre> <p>This way, we ensure that the interdependent ODEs use the correct value for the other variables.</p>"},{"location":"manual/NumericalMethods.html#assignments","title":"Assignments","text":"<p>When assignments (<code>=</code>, <code>+=</code>...) are used in an <code>equations</code> field, the order of valuation is different:</p> <ul> <li>Assigments occurring before or after a system of ODEs are updated     sequentially.</li> <li>Systems of ODEs are updated concurrently.</li> </ul> <p>Let's consider the following dummy equations:</p> <pre><code># Process the inputs\nExc = some_function(sum(exc))\nInh = another_function(sum(inh))\nI = Exc - Inh\n# ODE for the membrane potential, with a recovery variable\ntau*dv/dt = I - v - u\ntau*du/dt = v - u\n# Firing rate is the positive part of v\nr = pos(v)\n</code></pre> <p>Here, <code>Exc</code> and <code>Inh</code> represent the inputs to the neuron at the current time <code>t</code>. The new values should be immediately available for updating <code>I</code>, whose value should similarly be immediately used in the ODE of <code>v</code>. Similarly, the value of <code>r</code> should be the positive part of the value of <code>v</code> that was just calculated, not at the previous time step. Doing otherwise would introduce a lag in the neuron: changes in <code>sum(exc)</code> at <code>t</code> would be reflected in <code>Exc</code> at <code>t+1</code>, in <code>I</code> at <code>t+2</code>, in <code>v</code> at <code>t+3</code> and finally in <code>r</code> at <code>t+4</code>. This is generally unwanted.</p> <p>The generated code is therefore equivalent to:</p> <pre><code># Process the inputs\nExc = some_function(sum(exc))\nInh = another_function(sum(inh))\nI = Exc - Inh\n# ODE for the membrane potential, with a recovery variable\nnew_v = v + dt*(I - v - u)/tau\nnew_u = u + dt*(v - u)/tau\nv = new_v\nu = new_u\n# Firing rate is the positive part of v\nr = pos(v)\n</code></pre> <p>One can even define multiple groups of assignments and systems of ODEs: systems of ODEs separated by at least one assignment will be evaluated sequentially (but concurrently inside each system). For example, in:</p> <pre><code>tau*du/dt = v - u\nI = g_exc - g_inh\ntau*dk/dt = v - k\ntau*dv/dt = I - v - u + k\n</code></pre> <p><code>u</code> and <code>k</code> are updated using the previous value of <code>v</code>, while <code>v</code> uses the new values of both <code>I</code> and <code>u</code>, but the previous one of <code>k</code>.</p>"},{"location":"manual/Parser.html","title":"Parser","text":"<p>A Neuron or Synapse type is primarily defined by two sets of values which must be specified in its constructor:</p> <ul> <li>Parameters are values such as time constants which are constant     during the simulation. They can be the same throughout the     population/projection, or take different values.</li> <li>Variables are neuronal variables (for example the membrane     potential or firing rate) or synaptic variables (the synaptic     efficiency) whose value evolve with time during the simulation. The     equation (whether it is an ordinary differential equation or not)     ruling their evolution can be described using a specific     meta-language.</li> </ul>"},{"location":"manual/Parser.html#parameters","title":"Parameters","text":"<p>Parameters are defined by a multi-string consisting of one or more parameter definitions:</p> <pre><code>parameters = \"\"\"\n    tau = 10.0\n    eta = 0.5\n\"\"\"\n</code></pre> <p>Each parameter should be defined on a single line, with its name on the left side of the equal sign, and its value on the right side. The given value corresponds to the initial value of the parameter (but it can be changed at any further point of the simulation).</p> <p>As a neuron/synapse type is likely to be reused in different populations/projections, it is good practice to set reasonable initial values in the neuron/synapse type, and eventually adapt them to the corresponding populations/projections later on.</p> <p>Local vs. global parameters</p> <p>By default, a neural parameter will be unique to each neuron (i.e. each neuron instance will hold a copy of the parameter) or synapse. In order to save memory space, one can force ANNarchy to store only one parameter value for a whole population by specifying the <code>population</code> flag after a <code>:</code> symbol following the parameter definition:</p> <pre><code>parameters = \"\"\"\n    tau = 10.0\n    eta = 0.5 : population\n\"\"\"\n</code></pre> <p>In this case, there will be only only one instance of the <code>eta</code> parameter for the whole population. <code>eta</code> is called a global parameter, in opposition to local parameters which are the default.</p> <p>The same is true for synapses, whose parameters are by default unique to each synapse in a given projection. If the <code>post-synaptic</code> flag is passed, the parameter will be common to all synapses of a post-synaptic neuron, but can differ from one post-synaptic neuron to another. If the <code>projection</code> flag is passed, the parameter will be common to all synapses of a projection (e.g. the learning rate).</p> <p>Type of the variable</p> <p>Parameters have floating-point precision by default. If you want to force the parameter to be an integer or boolean, you can also pass the <code>int</code> and <code>bool</code> flags, separated by commas:</p> <pre><code>parameters = \"\"\"\n    tau = 10.0\n    eta = 1 : population, int\n\"\"\"\n</code></pre> <p>Constants</p> <p>Alternatively, it is possible to use constants in the parameter definition (see later):</p> <pre><code>tau_exc = Constant('tau_exc', 10.0)\n\nneuron = Neuron(\n    parameters = \"\"\"\n        tau = tau_exc\n    \"\"\",\n)\n</code></pre> <p>The advantage of this method is that if a parameter value is \\\"shared\\\" across several neuron/synapse types, you only need to change the value once, instead of in each neuron/synapse definition.</p>"},{"location":"manual/Parser.html#variables","title":"Variables","text":"<p>Time-varying variables are also defined using a multi-line description:</p> <pre><code>equations = \"\"\"\n    noise = Uniform(0.0, 0.2)\n    tau * dmp/dt  + mp = baseline + sum(exc) + noise\n    r = pos(mp)\n\"\"\"\n</code></pre> <p>The evolution of each variable with time can be described through a simple equation or an ordinary differential equation (ODE). ANNarchy provides a simple parser for mathematical expressions, whose role is to translate a high-level description of the equation into an optimized C++ code snippet.</p> <p>The equation for one variable can depend on parameters, other variables (even when declared later) or constants. Variables are updated in the same order as their declaration in the multistring (see Numerical methods, as it influences how ODEs are solved).</p> <p>The declaration of a single variable can extend on multiple lines:</p> <pre><code>equations = \"\"\"\n    noise = Uniform(0.0, 0.2)\n    tau * dmp/dt  = baseline - mp\n                    + sum(exc) + noise : max = 1.0\n    rate = pos(mp)\n\"\"\"\n</code></pre> <p>As it is only a parser and not a solver, some limitations exist:</p> <ul> <li>Simple equations must hold only the name of the variable on the left     sign of the equation. Variable definitions such as     <code>rate + mp = noise</code> are forbidden, as it would be impossible to     guess which variable should be updated.</li> <li>ODEs are more free regarding the left side, but only one variable     should hold the gradient: the one which will be updated. The     following definitions are equivalent and will lead to the same C++     code:</li> </ul> <pre><code>tau * dmp/dt  = baseline - mp\n\ntau * dmp/dt  + mp = baseline\n\ntau * dmp/dt  + mp -  baseline = 0\n\ndmp/dt  = (baseline - mp) / tau\n</code></pre> <p>In practice, ODEs are transformed using Sympy into the last form (only the gradient stays on the left) and numerized using the chosen numerical method (see Numerical methods).</p>"},{"location":"manual/Parser.html#flags","title":"Flags","text":"<p>Locality and type</p> <p>Like the parameters, variables also accept the <code>population</code>, <code>postsynaptic</code> and <code>projection</code> flags to define the local/global character of the variable, as well as the <code>int</code> or <code>bool</code> flags for their type.</p> <p>Initial value</p> <p>The initial value of the variable (before the first simulation starts) can also be specified using the <code>init</code> keyword followed by the desired value:</p> <pre><code>equations = \"\"\"\n    tau * dmp/dt + mp = baseline : init = 0.2\n\"\"\"\n</code></pre> <p>It must be a single value (the same for all neurons in the population or all synapses in the projection) and should not depend on other parameters and variables. This initial value can be specifically changed after the <code>Population</code> or <code>Projection</code> objects are created (see Populations).</p> <p>It is also possible to use constants for the initial value:</p> <pre><code>init_mp = Constant('init_mp', 0.2)\n\nneuron = Neuron(\n    equations = \"\"\"\n        tau * dmp/dt + mp = baseline : init = init_mp\n    \"\"\",\n)\n</code></pre> <p>Min and Max values of a variable</p> <p>Upper- and lower-bounds can be set using the <code>min</code> and <code>max</code> keywords:</p> <pre><code>equations = \"\"\"\n    tau * dmp/dt  + mp = baseline : min = -0.2, max = 1.0\n\"\"\"\n</code></pre> <p>At each step of the simulation, after the update rule is calculated for <code>mp</code>, the new value will be compared to the <code>min</code> and <code>max</code> value, and clamped if necessary.</p> <p><code>min</code> and <code>max</code> can be single values, constants, parameters, variables or functions of all these:</p> <pre><code>parameters = \"\"\"\n    tau = 10.0\n    min_mp = -1.0 : population\n    max_mp = 1.0\n\"\"\",\nequations = \"\"\"\n    variance = Uniform(0.0, 1.0)\n    tau * dmp/dt  + mp = sum(exc) : min = min_mp, max = max_mp + variance\n    r = mp : min = 0.0 # Equivalent to r = pos(mp)\n\"\"\"\n</code></pre> <p>Numerical method</p> <p>The numerization method for a single ODEs can be explicitely set by specifying a flag:</p> <pre><code>tau * dmp/dt  + mp = sum(exc) : exponential\n</code></pre> <p>The available numerical methods are described in Numerical methods.</p> <p>Summary of allowed flags for variables:</p> <ul> <li>init: defines the initialization value at begin of simulation and     after a network reset (default: 0.0)</li> <li>min: minimum allowed value (unset by default)</li> <li>max: maximum allowed value (unset by default)</li> <li>population: the attribute is shared by all neurons of a     population.</li> <li>postsynaptic: the attribute is shared by all synapses of a     post-synaptic neuron.</li> <li>projection: the attribute is shared by all synapses of a     projection.</li> <li>explicit, implicit, exponential, midpoint, event-driven:     the numerical method to be used.</li> </ul>"},{"location":"manual/Parser.html#constants","title":"Constants","text":"<p>Global constants can be created by the user and used inside any equation. They must define an unique name and a floating point value:</p> <pre><code>tau = Constant('tau', 10.0)\n\nneuron = Neuron(\n    equations = \"tau * dr/dt + r = sum(exc)\"\n)\n</code></pre> <p>In this example, a Neuron or Synapse does not have to define the parameter <code>tau</code> to use it: it is available everywhere. If the Neuron/Synapse redefines a parameter called <code>tau</code>, the constant is not visible anymore to that object.</p> <p>Constants can be manipulated as normal floats to define complex values:</p> <pre><code>tau = Constant('tau', 20)\nfactor = Constant('factor', 0.1)\nreal_tau = Constant('real_tau', tau*factor)\n\nneuron = Neuron(\n    equations='''\n        real_tau*dr/dt + r =1.0\n    '''\n)\n</code></pre> <p>Note that constants are only global, changing their value impacts all objects using them. Changing the value of a constant can only be done through the <code>set()</code> method (before or after <code>compile()</code>):</p> <pre><code>tau = Constant('tau', 20)\ntau.set(10.0)\n</code></pre>"},{"location":"manual/Parser.html#allowed-vocabulary","title":"Allowed vocabulary","text":"<p>The mathematical parser relies heavily on the one provided by SymPy.</p>"},{"location":"manual/Parser.html#numerical-values","title":"Numerical values","text":"<p>All parameters and variables use implicitly the floating-point double precision, except when stated otherwise with the <code>int</code> or <code>bool</code> keywords. You can use numerical constants within the equation, noting that they will be automatically converted to this precision:</p> <pre><code>tau * dmp / dt  = 1 / pos(mp) + 1\n</code></pre> <p>The constant \\(\\pi\\) is available under the literal form <code>pi</code>.</p>"},{"location":"manual/Parser.html#operators","title":"Operators","text":"<ul> <li>Additions (+), substractions (-), multiplications (*), divisions     (/) and power functions (\\^) are of course allowed.</li> <li>Gradients are allowed only for the variable currently described.     They take the form:</li> </ul> <pre><code>dmp / dt  = A\n</code></pre> <p>with a <code>d</code> preceding the variable's name and terminated by <code>/dt</code> (with or without spaces). Gradients must be on the left side of the equation.</p> <ul> <li>To update the value of a variable at each time step, the operators     <code>=</code>, <code>+=</code>, <code>-=</code>, <code>*=</code>, and <code>/=</code> are allowed.</li> </ul>"},{"location":"manual/Parser.html#parameters-and-variables","title":"Parameters and Variables","text":"<p>Any parameter or variable defined in the same Neuron/Synapse can be used inside an equation. User-defined constants can also be used. Additionally, the following variables are pre-defined:</p> <ul> <li><code>dt</code> : the discretization time step for the simulation. Using this     variable, you can define the numerical method by yourself. For     example:</li> </ul> <pre><code>tau * dmp / dt  + mp = baseline\n</code></pre> <p>with backward Euler would be equivalent to:</p> <pre><code>mp += dt/tau * (baseline -mp)\n</code></pre> <ul> <li><code>t</code> : the time in milliseconds elapsed since the creation of the     network. This allows to generate oscillating variables:</li> </ul> <pre><code>f = 10.0 # Frequency of 10 Hz\nphi = pi/4 # Phase\nts = t / 1000.0 # ts is in seconds\nr = 10.0 * (sin(2*pi*f*ts + phi) + 1.0)\n</code></pre>"},{"location":"manual/Parser.html#random-number-generators","title":"Random number generators","text":"<p>Several random generators are available and can be used within an equation. In the current version are for example available:</p> <ul> <li><code>Uniform(min, max)</code> generates random numbers from a uniform     distribution in the range \\([\\text{min}, \\text{max}]\\).</li> <li><code>Normal(mu, sigma)</code> generates random numbers from a normal     distribution with min mu and standard deviation sigma.</li> </ul> <p>See Random DIstributions for more distributions. For example:</p> <pre><code>noise = Uniform(-0.5, 0.5)\n</code></pre> <p>The arguments to the random distributions can be either fixed values or (functions of) global parameters.</p> <pre><code>min_val = -0.5 : population\nmax_val = 0.5 : population\nnoise = Uniform(min_val, max_val)\n</code></pre> <p>It is not allowed to use local parameters (with different values per neuron) or variables, as the random number generators are initialized only once at network creation (doing otherwise would impair performance too much). If a global parameter is used, changing its value will not affect the generator after compilation.</p> <p>It is therefore better practice to use normalized random generators and scale their outputs:</p> <pre><code>min_val = -0.5 : population\nmax_val = 0.5 : population\nnoise = min_val + (max_val - min_val) * Uniform(0.0, 1.0)\n</code></pre>"},{"location":"manual/Parser.html#mathematical-functions","title":"Mathematical functions","text":"<ul> <li>Most mathematical functions of the <code>cmath</code> library are understood by     the parser, for example:</li> </ul> <pre><code>cos, sin, tan, acos, asin, atan, exp, abs, fabs, sqrt, log, ln\n</code></pre> <ul> <li>The positive and negative parts of a term are also defined, with     short and long versions:</li> </ul> <pre><code>r = pos(mp)\nr = positive(mp)\nr = neg(mp)\nr = negative(mp)\n</code></pre> <ul> <li>A piecewise linear function is also provided (linear when x is     between a and b, saturated at a or b otherwise):</li> </ul> <pre><code>r = clip(x, a, b)\n</code></pre> <ul> <li>For integer variables, the modulo operator is defined:</li> </ul> <pre><code>x += 1 : int\ny = modulo(x, 10)\n</code></pre> <ul> <li>When using the power function (<code>r = x^2</code> or <code>r = pow(x, 2)</code>), the     <code>cmath</code> <code>pow(double, int)</code> method is used. For small exponents     (quadratic or cubic functions), it can be extremely slow, compared     to <code>r = x*x</code> or <code>r = x*x*x</code>. Unfortunately, Sympy transforms     automatically <code>r = x*x</code> into <code>r = pow(x, 2)</code>. We therefore advise to     use the built-in <code>power(double, int)</code> function instead:</li> </ul> <pre><code>r = power(x, 3)\n</code></pre> <p>These functions must be followed by a set of matching brackets:</p> <pre><code>tau * dmp / dt + mp = exp( - cos(2*pi*f*t + pi/4 ) + 1)\n</code></pre>"},{"location":"manual/Parser.html#conditional-statements","title":"Conditional statements","text":"<p>Python-style</p> <p>It is possible to use Python-style conditional statements as the right term of an equation or ODE. They follow the form:</p> <pre><code>if condition : statement1 else : statement2\n</code></pre> <p>For example, to define a piecewise linear function, you can nest different conditionals:</p> <pre><code>r = if mp &lt; 1. :\n        if mp &gt; 0.:\n            mp\n        else:\n            0.\n    else:\n        1.\n</code></pre> <p>which is equivalent to:</p> <pre><code>r = clip(mp, 0.0, 1.0)\n</code></pre> <p>The condition can use the following vocabulary:</p> <pre><code>True, False, and, or, not, is, is not, ==, !=, &gt;, &lt;, &gt;=, &lt;=\n</code></pre> <p>Note</p> <p>The <code>and</code>, <code>or</code> and <code>not</code> logical operators must be used with parentheses around their terms. Example:</p> <pre><code>var = if (mp &gt; 0) and ( (noise &lt; 0.1) or (not(condition)) ):\n            1.0\n        else:\n            0.0\n</code></pre> <p><code>is</code> is equivalent to <code>==</code>, <code>is not</code> is equivalent to <code>!=</code>.</p> <p>When a conditional statement is split over multiple lines, the flags must be set after the last line:</p> <pre><code>rate = if mp &lt; 1.0 :\n          if mp &lt; 0.0 :\n              0.0\n          else:\n              mp\n       else:\n          1.0 : init = 0.6\n</code></pre> <p>An <code>if a: b else:c</code> statement must be exactly the right term of an equation. It is for example NOT possible to write:</p> <pre><code>r = 1.0 + (if mp&gt; 0.0: mp else: 0.0) + b\n</code></pre> <p>Ternary operator</p> <p>The ternary operator <code>ite(cond, then, else)</code> (ite stands for if-then-else) is available to ease the combination of conditionals with other terms:</p> <pre><code>r = ite(mp&gt;0.0, mp, 0.0)\n# is exactly the same as:\nr = if mp &gt; 0.0: mp else: 0.0\n</code></pre> <p>The advantage is that the conditional term is not restricted to the right term of the equation, and can be used multiple times:</p> <pre><code>r = ite(mp &gt; 0.0, ite(mp &lt; 1.0, mp, 1.0), 0.0) + ite(stimulated, 1.0, 0.0)\n</code></pre>"},{"location":"manual/Parser.html#custom-functions","title":"Custom functions","text":"<p>To simplify the writing of equations, custom functions can be defined either globally (usable by all neurons and synapses) or locally (only for the particular type of neuron/synapse) using the same mathematical parser.</p> <p>Global functions can be defined using the <code>add_function()</code> method:</p> <pre><code>add_function('sigmoid(x) = 1.0 / (1.0 + exp(-x))')\n</code></pre> <p>With this declaration, <code>sigmoid()</code> can be used in the declaration of any variable, for example:</p> <pre><code>neuron = Neuron(\n    equations = \"\"\"\n        r = sigmoid(sum(exc))\n    \"\"\"\n)\n</code></pre> <p>Functions must be one-liners, i.e. they should have only one return value. They can use as many arguments as needed, but are totally unaware of the context: all the needed information should be passed as an argument (except constants which are visible to the function).</p> <p>The types of the arguments (including the return value) are by default floating-point. If other types should be used, they should be specified at the end of the definition, after the <code>:</code> sign, with the type of the return value first, followed by the type of all arguments separated by commas:</p> <pre><code>add_function('conditional_increment(c, v, t) = if v &gt; t : c + 1 else: c : int, int, float, float')\n</code></pre> <p>After compilation, the function can be called using arbitrary list of values for the arguments using the <code>functions()</code> method and the name of the function:</p> <pre><code>add_function('sigmoid(x) = 1.0 / (1.0 + exp(-x))')\n\ncompile()\n\nx = np.linspace(-10., 10., 1000)\ny = functions('sigmoid')(x)\n</code></pre> <p>You can pass a list or a 1D Numpy array as argument, but not a single value or a multidimensional array. When several arguemnts are passed, they must have the same size.</p> <p>Local functions are specific to a Neuron or Synapse class and can only be used within this context (if they have the same name as global variables, they will override them). They can be passed as a multi-line argument to the constructor of a neuron or synapse (see later):</p> <pre><code>functions == \"\"\"\n    sigmoid(x) = 1.0 / (1.0 + exp(-x))\n    conditional_increment(c, v, t) = if v &gt; t : c + 1 else: c : int, int, float, float\n\"\"\"\n</code></pre>"},{"location":"manual/Populations.html","title":"Populations","text":"<p>Once the <code>Neuron</code> objects have been defined, the populations can be created. Let's suppose we have defined the following rate-coded neuron:</p> <pre><code>LeakyIntegratorNeuron = Neuron(\n    parameters = \"\"\"\n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dmp/dt  + mp = baseline + sum(exc)\n        r = pos(mp)\n    \"\"\"\n)\n</code></pre>"},{"location":"manual/Populations.html#creating-populations","title":"Creating populations","text":"<p>Populations of neurons are created using the <code>Population</code> class:</p> <pre><code>pop1 = Population(geometry=100, neuron=LeakyIntegratorNeuron)\npop2 = Population(geometry=(8, 8), neuron=LeakyIntegratorNeuron, name=\"pop2\")\n</code></pre> <p>The rate-coded or spiking nature of the <code>Neuron</code> instance is irrelevant when creating the <code>Population</code> object.</p> <p>It takes different parameters:</p> <ul> <li><code>geometry</code> defines the number of neurons in the population, as well     as its spatial structure (1D/2D/3D or more). For example, a     two-dimensional population with 15*10 neurons takes the argument     <code>(15, 10)</code>, while a one-dimensional array of 100 neurons would take     <code>(100,)</code> or simply <code>100</code>.</li> <li><code>neuron</code> indicates the neuron type to use for this population (which     must have been defined before). It requires a <code>Neuron</code> instance.</li> <li><code>name</code> is an unique string for each population in the network. If     <code>name</code> is omitted, an internal name such as <code>pop0</code> will be given     (the number is incremented every time a new population is defined).     Although this argument is optional, it is strongly recommended to     give an understandable name to each population: if you somehow     \\\"lose\\\" the reference to the <code>Population</code> object in some portion of     your code, you can always retrieve it using the     <code>get_population(name)</code> method.</li> </ul> <p>After creation, each population has several attributes defined (corresponding to the parameters and variables of the <code>Neuron</code> type) and is assigned a fixed size (<code>pop.size</code> corresponding to the total number of neurons, here 100 for <code>pop1</code> and 64 for <code>pop2</code>) and geometry (<code>pop1.geometry</code>, here <code>(100, )</code> and <code>(8, 8)</code>).</p>"},{"location":"manual/Populations.html#geometry-and-ranks","title":"Geometry and ranks","text":"<p>Each neuron in the population has therefore a set of coordinates (expressed relative to <code>pop1.geometry</code>) and a rank (from 0 to <code>pop1.size -1</code>). The reason is that spatial coordinates are useful for visualization, or when defining a distance-dependent connection pattern, but that ANNarchy internally uses flat arrays for performance reasons.</p> <p>The coordinates use the matrix notation for multi-dimensional arrays, which is also used by Numpy (for a 2D matrix, the first index represents the row, the second the column). You can therefore use safely the <code>reshape()</code> method of Numpy to switch between coordinates-based and rank-based representations of an array.</p> <p>To convert the rank of a neuron to its coordinates (and vice-versa), you can use the ravel_multi_index and unravel_index methods of Numpy, but they can be quite slow. The <code>Population</code> class provides two more efficient methods to do this conversion:</p> <ul> <li><code>coordinates_from_rank</code> returns a tuple representing the coordinates     of neuron based on its rank (between 0 and <code>size -1</code>, otherwise an     error is thrown).</li> <li><code>rank_from_coordinates</code> returns the rank corresponding to the     coordinates.</li> </ul> <p>For example, with <code>pop2</code> having a geometry <code>(8, 8)</code>:</p> <pre><code>&gt;&gt;&gt; pop2.coordinates_from_rank(15)\n(1, 7)\n&gt;&gt;&gt; pop2.rank_from_coordinates((4, 6))\n38\n</code></pre>"},{"location":"manual/Populations.html#population-attributes","title":"Population attributes","text":"<p>The value of the parameters and variables of all neurons in a population can be accessed and modified through population attributes.</p> <p>With the previously defined populations, you can list all their parameters and variables with:</p> <pre><code>&gt;&gt;&gt; pop2.attributes\n['tau', 'baseline', 'mp', 'r']\n&gt;&gt;&gt; pop2.parameters\n['tau', 'baseline']\n&gt;&gt;&gt; pop2.variables\n['r', 'mp']\n</code></pre> <p>Reading their value is straightforward:</p> <pre><code>&gt;&gt;&gt; pop2.tau\n10.0\n&gt;&gt;&gt; pop2.r\narray([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n</code></pre> <p>Population-wise parameters/variables have a single value for the population, while neuron-specific ones return a NumPy array with the same geometry has the population.</p> <p>Setting their value is also simple:</p> <pre><code>&gt;&gt;&gt; pop2.tau = 20.0\n&gt;&gt;&gt; pop2.tau\n20.0\n&gt;&gt;&gt; pop2.r = 1.0\n&gt;&gt;&gt; pop2.r\narray([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])\n&gt;&gt;&gt; pop2.mp = 0.5 * np.ones(pop2.geometry)\narray([[ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5],\n       [ 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5]])\n&gt;&gt;&gt; pop2.r = Uniform(0.0, 1.0)\narray([[ 0.97931939,  0.64865327,  0.29740417,  0.49352664,  0.36511704,\n         0.59879869,  0.10835491,  0.38481751],\n       [ 0.07664157,  0.77532887,  0.04773084,  0.75395453,  0.56072342,\n         0.54139054,  0.28553319,  0.96159595],\n       [ 0.01811468,  0.30214921,  0.45321071,  0.56728733,  0.24577655,\n         0.32798484,  0.84929103,  0.63025331],\n       [ 0.34168482,  0.07411291,  0.6510492 ,  0.89025337,  0.31192464,\n         0.59834719,  0.77102494,  0.88537967],\n       [ 0.41813573,  0.47395247,  0.46603402,  0.45863676,  0.76628989,\n         0.42256749,  0.18527079,  0.8322103 ],\n       [ 0.70616692,  0.73210377,  0.05255718,  0.01939817,  0.24659769,\n         0.50349528,  0.79201573,  0.19159611],\n       [ 0.21246111,  0.93570727,  0.68544108,  0.61158741,  0.17954022,\n         0.90084004,  0.41286698,  0.45550662],\n       [ 0.14720568,  0.51426136,  0.36225438,  0.06096426,  0.77209455,\n         0.07348683,  0.43178591,  0.32451531]])\n</code></pre> <p>For population-wide attributes, you can only specify a single value (float, int or bool depending on the type of the parameter/variable). For neuron-specific attributes, you can provide either:</p> <ul> <li>a single value which will be applied to all neurons of the     population.</li> <li>a list or a one-dimensional Numpy array of the same length as the     number of neurons in the population. This information is provided by     <code>pop1.size</code>.</li> <li>a Numpy array of the same shape as the geometry of the population.     This information is provided by <code>pop1.geometry</code>.</li> <li>a random number generator object (Uniform, Normal...).</li> </ul> <p>Note</p> <p>If you do not want to use the attributes of Python (for example when doing a loop over unknown attributes), you can also use the <code>get(name)</code> and <code>set(values)</code> methods of Population:</p> <pre><code>pop1.get('tau')\npop1.set({'mp': 1.0, 'r': Uniform(0.0, 1.0)})\n</code></pre>"},{"location":"manual/Populations.html#accessing-individual-neurons","title":"Accessing individual neurons","text":"<p>There exists a purely semantic access to individual neurons of a population. The <code>IndividualNeuron</code> class wraps population data for a specific neuron. It can be accessed through the <code>Population.neuron()</code> method using either the rank of the neuron (from 0 to <code>pop1.size - 1</code>) or its coordinates in the population's geometry:</p> <pre><code>&gt;&gt;&gt; print pop2.neuron(2, 2)\nNeuron of the population pop2 with rank 18 (coordinates (2, 2)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  mp = 0.0\n  r = 0.0\n</code></pre> <p>The individual neurons can be manipulated individually:</p> <pre><code>&gt;&gt;&gt; my_neuron = pop2.neuron(2, 2)\n&gt;&gt;&gt; my_neuron.rate = 1.0\n&gt;&gt;&gt; print my_neuron\nNeuron of the population pop2 with rank 18 (coordinates (2, 2)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  mp = 0.0\n  r = 1.0\n</code></pre> <p>Warning</p> <p><code>IndividualNeuron</code> is only a wrapper for ease of use, the real data is stored in arrays for the whole population, so accessing individual neurons is much slower and should be reserved to specific cases (i.e. only from time to time and for a limited set of neurons).</p>"},{"location":"manual/Populations.html#accessing-groups-of-neurons","title":"Accessing groups of neurons","text":"<p>Individual neurons can be grouped into <code>PopulationView</code> objects, which hold references to different neurons of the same population. One can create population views by \\\"adding\\\" several neurons together:</p> <pre><code>&gt;&gt;&gt; popview = pop2.neuron(2,2) + pop2.neuron(3,3) + pop2.neuron(4,4)\n&gt;&gt;&gt; popview\nPopulationView of pop2\n  Ranks: [18, 27, 36]\n* Neuron of the population pop2 with rank 18 (coordinates (2, 2)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  mp = 0.0\n  r = 0.0\n\n* Neuron of the population pop2 with rank 27 (coordinates (3, 3)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  mp = 0.0\n  r = 0.0\n\n* Neuron of the population pop2 with rank 36 (coordinates (4, 4)).\nParameters:\n  tau = 10.0\n  baseline = -0.2\n\nVariables:\n  mp = 0.0\n  r = 0.0\n&gt;&gt;&gt; popview.r = 1.0\n&gt;&gt;&gt; pop2.r\narray([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n</code></pre> <p>One can also use the slice operators to create PopulationViews:</p> <pre><code>&gt;&gt;&gt; popview = pop2[3, :]\n&gt;&gt;&gt; popview.r = 1.0\n&gt;&gt;&gt; pop2.r \narray([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n</code></pre> <p>or:</p> <pre><code>&gt;&gt;&gt; popview = pop2[2:5, 4] \n&gt;&gt;&gt; popview.r = 1.0\n&gt;&gt;&gt; pop1.r\narray([[ 0., 0., 0., 0., 0., 0., 0., 0.], \n       [ 0., 0., 0., 0., 0., 0., 0., 0.], \n       [ 0., 0., 0., 0., 1., 0., 0., 0.], \n       [ 0., 0., 0., 0., 1., 0., 0., 0.], \n       [ 0., 0., 0., 0., 1., 0., 0., 0.], \n       [ 0., 0., 0., 0., 0., 0., 0., 0.], \n       [ 0., 0., 0., 0., 0., 0., 0., 0.], \n       [ 0., 0., 0., 0., 0., 0., 0., 0.]])\n</code></pre> <p><code>PopulationView</code> objects can be used to create projections.</p> <p>Warning</p> <p>Contrary to the equivalent in PyNN, PopulationViews in ANNarchy can only group neurons from the same population.</p>"},{"location":"manual/Populations.html#functions","title":"Functions","text":"<p>If you have defined a function inside a <code>Neuron</code> definition:</p> <pre><code>LeakyIntegratorNeuron = Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n        slope = 1.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dmp/dt + mp = baseline + sum(exc)\n        r = sigmoid(mp, slope)\n    \"\"\",\n    functions = \"\"\"\n        sigmoid(x, k) = 1.0 / (1.0 + exp(-x*k))\n    \"\"\"\n)\n</code></pre> <p>you can use this function in Python as if it were a method of the corresponding object:</p> <pre><code>pop = Population(1000, LeakyIntegratorNeuron)\n\nx = np.linspace(-1., 1., 100)\nk = np.ones(100)\nr = pop.sigmoid(x, k)\n</code></pre> <p>You can pass either a list or a 1D Numpy array to each argument (not a single value, nor a multidimensional array!).</p> <p>The size of the arrays passed for each argument is arbitrary (it must not match the population's size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.</p>"},{"location":"manual/Projections.html","title":"Projections","text":""},{"location":"manual/Projections.html#declaring-the-projections","title":"Declaring the projections","text":"<p>Once the populations are created, one can connect them by creating <code>Projection</code> instances:</p> <pre><code>proj = Projection(\n    pre = pop1,\n    post = pop2,\n    target = \"exc\",\n    synapse = BCM\n)\n</code></pre> <ul> <li><code>pre</code> is either the name of the pre-synaptic population or the     corresponding Population object.</li> <li><code>post</code> is either the name of the post-synaptic population or the     corresponding Population object.</li> <li><code>target</code> is the type of the connection.</li> <li><code>synapse</code> is an optional argument requiring a Synapse instance.</li> </ul> <p>The post-synaptic neuron type must use <code>sum(exc)</code> in the rate-coded case respectively <code>g_exc</code> in the spiking case, otherwise the projection will be useless.</p> <p>If the <code>synapse</code> argument is omitted, the default synapse will be used:</p> <ul> <li>the default rate-coded synapse defines <code>psp = w * pre.r</code>,</li> <li>the default spiking synapse defines <code>g_target += w</code>.</li> </ul>"},{"location":"manual/Projections.html#building-the-projections","title":"Building the projections","text":"<p>Creating the Projection objects only defines the information that two populations are connected. The synapses must be explicitely created by applying a connector method on the Projection object.</p> <p>To this end, ANNarchy already provides a set of predefined connector methods, but the user has also the possibility to define his own (see Connector).</p> <p>The pattern can be applied either directly at the creation of the Projection:</p> <pre><code>proj = Projection(\n    pre = pop1,\n    post = pop2,\n    target = \"exc\",\n    synapse = BCM\n).connect_all_to_all( weights = 1.0 )\n</code></pre> <p>or afterwards:</p> <pre><code>proj = Projection(\n    pre = pop1,\n    post = pop2,\n    target = \"exc\",\n    synapse = BCM\n)\nproj.connect_all_to_all( weights = 1.0 )\n</code></pre> <p>The connector method must be called before the network is compiled.</p>"},{"location":"manual/Projections.html#projection-attributes","title":"Projection attributes","text":"<p>Let's suppose the <code>BCM</code> synapse is used to create the Projection <code>proj</code> (spiking synapses are accessed similarly):</p> <pre><code>BCM = Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100. : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0\n    \"\"\"\n)\n</code></pre>"},{"location":"manual/Projections.html#global-attributes","title":"Global attributes","text":"<p>The global parameters and variables of a projection (i.e. defined with the <code>postsynaptic</code> or <code>projection</code> flags) can be accessed directly through attributes. Attributes defined with <code>projection</code> have a single value for the whole population:</p> <pre><code>&gt;&gt;&gt; proj.tau\n100\n</code></pre> <p>Attributes defined with <code>postsynaptic</code> have one value per post-synaptic neuron, so the result is a vector:</p> <pre><code>&gt;&gt;&gt; proj.theta\n[3.575, 15.987, ... , 4.620]\n</code></pre> <p>Post-synaptic variables can be modified by passing:</p> <ul> <li>a single value, which will be the same for all post-synaptic     neurons.</li> <li>a list of values, with the same size as the number of neurons     receiving synapses (for some sparse connectivity patterns, it may     not be the same as the size of the population, so no     multidimensional array is accepted).</li> </ul> <p>After compilation (and therefore creation of the synapses), you can access how many post-synaptic neurons receive actual synapses with:</p> <pre><code>&gt;&gt;&gt; proj.size\n4\n</code></pre> <p>The list of ranks of the post-synaptic neurons receiving synapses is obtained with:</p> <pre><code>&gt;&gt;&gt; proj.post_ranks\n[0, 1, 2, 3]\n</code></pre>"},{"location":"manual/Projections.html#local-attributes","title":"Local attributes","text":"<p>At the projection level</p> <p>Local attributes can also be accessed globally through attributes. It will return a list of lists containing the synapse-specific values.</p> <p>The first index represents the post-synaptic neurons. It has the same length as <code>proj.post_ranks</code>. Beware that if some post-synaptic neurons do not receive any connection, this index will not correspond to the ranksof the post-synaptic population.</p> <p>The second index addresses the pre-synaptic neurons. If the connection is sparse, it also is unrelated to the ranks of the pre-synaptic neurons in their populations.</p> <p>Warning</p> <p>Modifying these lists of lists is error-prone, so this method should be avoided if possible.</p> <p>At the post-synaptic level</p> <p>The local parameters and variables of a projection (synapse-specific) should better be accessed through the Dendrite object, which gathers for a single post-synaptic neuron all synapses belonging to the projection.</p> <p>Beware: As projections are only instantiated after the call to <code>compile()</code>, local attributes of a Projection are only available then. Trying to access them before compilation will lead to an error!</p> <p>Each dendrite stores the parameters and variables of the corresponding synapses as attributes, as populations do for neurons. You can loop over all post-synaptic neurons receiving synapses with the <code>dendrites</code> iterator:</p> <pre><code>for dendrite in proj.dendrites:\n    print dendrite.pre_ranks\n    print dendrite.size\n    print dendrite.tau\n    print dendrite.alpha\n    print dendrite.w\n</code></pre> <p><code>dendrite.pre_ranks</code> returns a list of pre-synaptic neuron ranks. <code>dendrite.size</code> returns the number of synapses for the considered post-synaptic neuron. Global parameters/variables return a single value (<code>dendrite.tau</code>) and local ones return a list (<code>dendrite.w</code>).</p> <p>You can even omit the <code>.dendrites</code> part of the iterator:</p> <pre><code>for dendrite in proj:\n    print dendrite.pre_ranks\n    print dendrite.size\n    print dendrite.tau\n    print dendrite.alpha\n    print dendrite.w\n</code></pre> <p>You can also access the dendrites individually, either by specifying the rank of the post-synaptic neuron:</p> <pre><code>dendrite = proj.dendrite(13)\nprint dendrite.w\n</code></pre> <p>or its coordinates:</p> <pre><code>dendrite = proj.dendrite(5, 5)\nprint dendrite.w\n</code></pre> <p>When using ranks, you can also directly address the projection as an array:</p> <pre><code>dendrite = proj[13]\nprint dendrite.w\n</code></pre> <p>Warning</p> <p>You should make sure that the dendrite actually exists before accessing it through its rank, because it is otherwise a <code>None</code> object.</p>"},{"location":"manual/Projections.html#functions","title":"Functions","text":"<p>If you have defined a function inside a <code>Synapse</code> definition:</p> <pre><code>BCM = Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100. : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic\n        dw/dt = eta * BCMRule(pre.r, post.r, theta) : min=0.0\n    \"\"\",\n    functions = \"\"\"\n        BCMRule(pre, post, theta) = post * (post - theta) * pre\n    \"\"\"\n)\n</code></pre> <p>you can use this function in Python as if it were a method of the corresponding object:</p> <pre><code>proj = Projection(pop1, pop2, 'exc', BCM).connect_xxx()\n\npre = np.linspace(0., 1., 100)\npost = np.linspace(0., 1., 100)\ntheta = 0.01 * np.ones(100)\n\nweight_change = proj.BCMRule(pre, post, theta)\n</code></pre> <p>You can pass either a list or a 1D Numpy array to each argument (not a single value, nor a multidimensional array!).</p> <p>The size of the arrays passed for each argument is arbitrary (it must not match the projection's size) but you have to make sure that they all have the same size. Errors are not catched, so be careful.</p>"},{"location":"manual/Projections.html#connecting-population-views","title":"Connecting population views","text":"<p><code>Projections</code> are usually understood as a connectivity pattern between two populations. Complex connectivity patterns have to specifically designed (see Connector).</p> <p>In some cases, it can be much simpler to connect subsets of neurons directly, using built-in connector methods. To this end, the <code>Projection</code> object also accepts <code>PopulationView</code> objects for the <code>pre</code> and <code>post</code> arguments.</p> <p>Let's suppose we want to connect the (8,8) populations <code>pop1</code> and <code>pop2</code> in a all-to-all manner, but only for the (4,4) neurons in the center of these populations. The first step is to create the <code>PopulationView</code> objects using the slice operator:</p> <pre><code>pop1_center = pop1[2:7, 2:7]\npop2_center = pop2[2:7, 2:7]\n</code></pre> <p>They can then be simply used to create a projection:</p> <pre><code>proj = Projection(\n    pre = pop1_center,\n    post = pop2_center,\n    target = \"exc\",\n    synapse = BCM\n).connect_all_to_all( weights = 1.0 )\n</code></pre> <p>Each neuron of <code>pop2_center</code> will receive synapses from all neurons of <code>pop1_center</code>, and only them. Neurons of <code>pop2</code> which are not in <code>pop2_center</code> will not receive any synapse.</p> <p>Warning</p> <p>If you define your own connector method and want to use PopulationViews, you will need to iterate over the <code>ranks</code> attribute of the <code>PopulationView</code> object.</p>"},{"location":"manual/Projections.html#specifying-delays-in-synaptic-transmission","title":"Specifying delays in synaptic transmission","text":"<p>By default, synaptic transmission is considered to be instantaneous (or more precisely, it takes one simulation step (<code>dt</code>) for a newly computed firing rate to be taken into account by post-synaptic neurons).</p> <p>In order to take longer propagation times into account in the transmission of information between two populations, one has the possibility to define synaptic delays for a projection. All the built-in connector methods take an argument <code>delays</code> (default=<code>dt</code>), which can be a float (in milliseconds) or a random number generator.</p> <pre><code>proj.connect_all_to_all( weights = 1.0, delays = 10.0)\nproj.connect_all_to_all( weights = 1.0, delays = Uniform(1.0, 10.0))\n</code></pre> <p>If the delay is not a multiple of the simulation time step (<code>dt = 1.0</code> by default), it will be rounded to the closest multiple. The same is true for the values returned by a random number generator.</p> <p>Note: Per design, the minimal possible delay is equal to <code>dt</code>: values smaller than <code>dt</code> will be replaced by <code>dt</code>. Negative values do not make any sense and are ignored.</p> <p>Warning</p> <p>Non-uniform delays are not available on CUDA.</p>"},{"location":"manual/Projections.html#controlling-projections","title":"Controlling projections","text":"<p>Synaptic transmission, update and plasticity</p> <p>It is possible to selectively control synaptic transmission and plasticity at the projection level. The boolean flags <code>transmission</code>, <code>update</code> and <code>plasticity</code> can be set for that purpose:</p> <pre><code>proj.transmission = False\nproj.update = False\nproj.plasticity = False\n</code></pre> <ul> <li>If <code>transmission</code> is <code>False</code>, the projection is totally shut down:     it does not transmit any information to the post-synaptic population     (the corresponding weighted sums or conductances are constantly 0)     and all synaptic variables are frozen to their current value     (including the synaptic weights <code>w</code>).</li> <li>If <code>update</code> is <code>False</code>, synaptic transmission occurs normally, but     the synaptic variables are not updated. For spiking synapses, this     includes traces when they are computed at each step, but not when     they are integrated in an event-driven manner (flag <code>event-driven</code>).     Beware: continous synaptic transmission as in NMDA     synapses will     not work in this mode, as internal variables are not updated.</li> <li>If only <code>plasticity</code> is <code>False</code>, synaptic transmission and synaptic     variable updates occur normally, but changes to the synaptic weight     <code>w</code> are ignored.</li> </ul> <p>Disabling learning</p> <p>Alternatively, one can use the <code>enable_learning()</code> and <code>disable_learning()</code> methods of <code>Projection</code>. The effect of <code>disable_learning()</code> depends on the type of the projection:</p> <ul> <li>for rate-coded projections, <code>disable_learning()</code> is equivalent to     <code>update=False</code>: no synaptic variables is updated.</li> <li>for spiking projections, it is equivalent to <code>plasticity=False</code>:     only the weights are blocked.</li> </ul> <p>The reason of this difference is to allow continuous synaptic transmission and computation of traces. Calling <code>enable_learning()</code> without arguments resumes the default learning behaviour.</p> <p>Periodic learning</p> <p><code>enable_learning()</code> also accepts two arguments <code>period</code> and <code>offset</code>. <code>period</code> defines the interval in ms between two evaluations of the synaptic variables. This can be useful when learning should only occur once at the end of a trial. It is recommended not to use ODEs in the equations in this case, as they are numerized according to a fixed time step. <code>offset</code> defines the time inside the period at which the evaluation should occur. By default, it is 0, so the variable updates will occur at the next step, then after <code>period</code> ms, and so on. Setting it to -1 will shift the update at the end of the period.</p> <p>Note that spiking synapses using online evaluation will not be affected by these parameters, as they are event-driven.</p>"},{"location":"manual/Projections.html#multiple-targets","title":"Multiple targets","text":"<p>For spiking neurons, it may be desirable that a single synapses activates different currents (or conductances) in the post-synaptic neuron. One example are AMPA/NMDA synapses, where a single spike generates a \\\"classical\\\" AMPA current, plus a voltage-gated slower NMDA current. The following conductance-based Izhikevich is an example:</p> <pre><code>RSNeuron = Neuron(\n    parameters = \"\"\"\n        a = 0.02\n        b = 0.2\n        c = -65.\n        d = 8.\n        tau_ampa = 5.\n        tau_nmda = 150.\n        vrev = 0.0\n    \"\"\" ,\n    equations=\"\"\"\n        I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)\n        dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I : init=-65., midpoint\n        du/dt = a * (b*v - u) : init=-13.\n        tau_ampa * dg_ampa/dt = -g_ampa\n        tau_nmda * dg_nmda/dt = -g_nmda\n    \"\"\" ,\n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\",\n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\"\n)\n</code></pre> <p>However, <code>g_ampa</code> and <code>g_nmda</code> collect by default spikes from different projections, so the weights will not be shared between the \\\"ampa\\\" projection and the \\\"nmda\\\" one. It is therefore possible to specify a list of targets when building a projection, meaning that a single pre-synaptic spike will increase both <code>g_ampa</code> and <code>g_nmda</code> from the same weight:</p> <pre><code>proj = Projection(pop1, pop2, ['ampa', 'nmda'], STDP)\n</code></pre> <p>An example is provided in <code>examples/homeostatic_stdp/Ramp.py</code>.</p> <p>Warning</p> <p>Multiple targets are not available on CUDA yet.</p>"},{"location":"manual/RateNeuron.html","title":"Rate-coded neurons","text":""},{"location":"manual/RateNeuron.html#defining-parameters-and-variables","title":"Defining parameters and variables","text":"<p>Let's consider first a simple rate-coded neuron of the leaky-integrator type, which simply integrates the weighted sum of its excitatory inputs:</p> \\[ \\tau \\frac{d \\text{mp}(t)}{dt} = ( B - \\text{mp}(t) ) + \\sum_{i}^{\\text{exc}} \\text{r}_{i} * w_{i} \\] \\[r(t) = ( \\text{mp}(t) )^+\\] <p>where \\(mp(t)\\) represents the membrane potential of the neuron, \\(\\tau\\) the time constant of the neuron, \\(B\\) its baseline firing rate, \\(\\text{r}(t)\\) its instantaneous firing rate, \\(i\\) an index over all excitatory synapses of this neuron, \\(w_i\\) the efficiency of the synapse with the pre-synaptic neuron of firing rate \\(\\text{r}_{i}\\).</p> <p>It can be implemented in the ANNarchy framework with:</p> <pre><code>LeakyIntegratorNeuron = Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dmp/dt + mp = baseline + sum(exc)\n        r = pos(mp)\n    \"\"\"\n)\n</code></pre> <p>The only required variable is <code>r</code>, which represents the instantaneous firing rate and will be used to propagate activity in the network. All other parameters and variables are freely decided by the user. </p>"},{"location":"manual/RateNeuron.html#custom-functions","title":"Custom functions","text":"<p>Custom functions can also be defined when creating the Neuron type and used inside the <code>equations</code> field:</p> <pre><code>LeakyIntegratorNeuron = Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dmp/dt + mp = baseline + sum(exc)\n        r = sigmoid(mp)\n    \"\"\",\n    functions = \"\"\"\n        sigmoid(x) = 1.0 / (1.0 + exp(-x))\n    \"\"\"\n)\n</code></pre> <p>Make sure that the name of the function does not overlap with existing mathematical functions (cos, exp), existing variables (tau, r) or built-in functions (pos, t, dt).</p>"},{"location":"manual/RateNeuron.html#predefined-attributes","title":"Predefined attributes","text":"<p>The ODE can depend on other parameters of the neuron (e.g. <code>r</code> depends on <code>mp</code>), but not on unknown names. ANNarchy already defines the following variables and parameters for a neuron:</p> <ul> <li>variable t: time in milliseconds elapsed since the creation of the     network.</li> <li>parameter dt: the discretization step, default is 1 ms.</li> </ul>"},{"location":"manual/RateNeuron.html#weighted-sum-of-inputs","title":"Weighted sum of inputs","text":"<p>The <code>sum(target)</code> term gives a direct access to the weighted sum of all inputs to the neuron having the corresponding target (see Projections to see how the target is defined). These inputs are organized in a data structure called <code>Dendrite</code>.</p> <p>It is possible to modify how weighted sums are computed when creating a rate-coded synapse.</p> <p>Note</p> <p>The connection type, e.g. <code>exc</code> or <code>inh</code>, needs to match with the names used as a <code>target</code> parameter when creating a <code>Projection</code>. If such a projection does not exist when the network is compiled, the weighted sum will be set to 0.0 for all neurons.</p> <p>Using only <code>sum()</code> in the equations sums over all defined targets. For example, if two projections with targets <code>\"exc\"</code> and <code>\"inh\"</code> reach a neuron, <code>sum()</code> is equivalent to <code>sum(exc) + sum(inh)</code>. Inhibitory weights must then be defined as negative.</p>"},{"location":"manual/RateNeuron.html#global-operations","title":"Global operations","text":"<p>One has the possibility to use global operations on the population inside the neuron definition, such as the maximal activity in the population. One only needs to use one of the following operations:</p> <ul> <li><code>min(v)</code> for the minimum: \\(\\min_i v_i\\),</li> <li><code>max(v)</code> for the maximum: \\(\\max_i v_i\\),</li> <li><code>mean(v)</code> for the mean: \\(\\frac{1}{N} \\sum_i v_i\\),</li> <li><code>norm1(v)</code> for the L1-norm: \\(\\frac{1}{N} \\sum_i |v_i|\\),</li> <li><code>norm2(v)</code> for the L2-norm: \\(\\frac{1}{N} \\sum_i v_i^2\\)</li> </ul> <p>Example where neurons react to their inputs only where they exceed the mean over the population:</p> <pre><code>WTANeuron = Neuron(\n   parameters=\"\"\"   \n       tau = 10.0\n   \"\"\",\n   equations = \"\"\"\n       input = sum(exc)\n       tau * dr/dt + r = pos(input - mean(input))\n   \"\"\"\n)\n</code></pre> <p>Note</p> <p>The global operations are computed using values at the previous time step (like weighted sums), not in the step currently evaluated. There is therefore implicitely a delay of <code>dt</code>, but it cannot be changed.</p>"},{"location":"manual/RateSynapse.html","title":"Rate-coded synapses","text":"<p>As for neurons, you can define the synaptic behavior using a <code>Synapse</code> object. Although the description is local to a synapse, the same ODE will be applied to all synapses of a given Projection from one population to another. The same vocabulary as for neurons is accessible (constants, functions, conditional statements), except that the synapse must distinguish pre-synaptic and post-synaptic parameters/variables.</p> <p>Like <code>r</code> for a rate-coded neuron, one variable is special for a rate-coded synapse:</p> <ul> <li><code>w</code> represents the synaptic efficiency (or the weight of the     connection). If an ODE is defined for this variable, this will     implement a learning rule. If none is provided, the synapse is     non-plastic.</li> </ul> <p>The ODEs for synaptic variables follow the same syntax as for neurons. As for neurons, the following variables are already defined:</p> <ul> <li><code>t</code>: time in milliseconds elapsed since the creation of the network.</li> <li><code>dt</code>: the discretization step is 1.0ms by default.</li> </ul>"},{"location":"manual/RateSynapse.html#synaptic-plasticity","title":"Synaptic plasticity","text":"<p>Learning is possible by modifying the variable <code>w</code> of a single synapse during the simulation.</p> <p>For example, the Oja learning rule (see the example Bar learning):</p> \\[\\tau \\frac{d w(t)}{dt} = r_\\text{pre} * r_\\text{post} - \\alpha * r_\\text{post}^2 * w(t)\\] <p>could be implemented this way:</p> <pre><code>Oja = Synapse(\n    parameters=\"\"\"\n        tau = 5000\n        alpha = 8.0\n    \"\"\",\n    equations=\"\"\"\n        tau * dw / dt = pre.r * post.r - alpha * post.r^2 * w\n    \"\"\"\n)\n</code></pre> <p>Note that it is equivalent to define the increment directly if you want to apply the explicit Euler method:</p> <pre><code>equations=\"\"\"\n    w += dt / tau * ( pre.r * post.r - alpha * post.r^2 * w)\n\"\"\"\n</code></pre> <p>The same vocabulary as for rate-coded neurons applies. Custom functions can also be defined:</p> <pre><code>Oja = Synapse(\n    parameters=\"\"\"\n        tau = 5000\n        alpha = 8.0\n    \"\"\",\n    equations=\"\"\"\n        tau * dw / dt = product(pre.r,  post.r) - alpha * post.r^2 * w\n    \"\"\",\n    functions=\"\"\"\n        product(x,y) = x * y\n    \"\"\",\n)\n</code></pre>"},{"location":"manual/RateSynapse.html#neuron-specific-variables","title":"Neuron-specific variables","text":"<p>A synapse needs to access neural variables both at the pre- and post-synaptic levels. For the pre-synaptic neuron, biologically realistic synapses should only need its firing rate, but in some cases it may be useful to access other variables as well.</p> <p>In order to use neural variables in a synaptic variable, you have to prefix them with <code>pre.</code> or <code>post.</code>. For example:</p> <pre><code>pre.r, post.baseline, post.mp...\n</code></pre> <p>ANNarchy will check before the compilation that the pre- or post-synaptic neuron types indeed define such variables.</p> <p>Note</p> <p>If the projection uses delays, all pre-synaptic variables used in the synapse model will be delayed.</p>"},{"location":"manual/RateSynapse.html#locality","title":"Locality","text":"<p>There are 3 levels of locality for a synaptic parameter or variable:</p> <ol> <li><code>synaptic</code>: there is one value per synapse in the projection     (default).</li> <li><code>postsynaptic</code>: there is one value per post-synaptic neuron in the     projection.</li> <li><code>projection</code>: there is only one value for the whole projection.</li> </ol> <p>The following BCM learning rule makes use of the three levels of locality:</p> <pre><code>BCM = Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100. : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0\n    \"\"\"\n)\n</code></pre> <p><code>eta</code> and <code>tau</code> are global parameters to the projection: all synapses will use the same value. <code>theta</code> defines one value per post-synaptic neuron: it tracks the average of the post-synaptic firing rate. There is therefore no need to update one value per synapse, so we can use the flag \\\"postsynaptic\\\". Naturally, <code>w</code> is local to each synapse, so no locality flag should be passed.</p>"},{"location":"manual/RateSynapse.html#global-operations","title":"Global operations","text":"<p>Some learning rules require global information about the pre- or post-synaptic population, which is not local to the synapse, such as the mean or maximal activity in the pre-synaptic population. This information can be accessed at the synapse-level. The special functions:</p> <ul> <li><code>min(v)</code> for the minimum: \\(\\min_i v_i\\),</li> <li><code>max(v)</code> for the maximum: \\(\\max_i v_i\\),</li> <li><code>mean(v)</code> for the mean: \\(\\frac{1}{N} \\sum_i v_i\\),</li> <li><code>norm1(v)</code> for the L1-norm: \\(\\frac{1}{N} \\sum_i |v_i|\\),</li> <li><code>norm2(v)</code> for the L2-norm: \\(\\frac{1}{N} \\sum_i v_i^2\\)</li> </ul> <p>are available for any pre- or post-synaptic variable.</p> <p>For example, some covariance-based learning rules depend on the mean firing in the pre- and post-synaptic populations:</p> \\[\\tau \\frac{d w(t)}{dt} = (r_\\text{pre} - \\hat{r}_\\text{pre} )  * (r_\\text{post} - \\hat{r}_\\text{post} )\\] <p>Using the global operations, such a learning rule is trivial to implement:</p> <pre><code>Covariance = Synapse(\n    parameters=\"\"\"\n        tau = 5000.0\n    \"\"\",\n    equations=\"\"\"\n        tau * dw/dt = (pre.r - mean(pre.r) ) * (post.r - mean(post.r) )\n    \"\"\"\n)\n</code></pre> <p>Warning</p> <ul> <li>Such global operations can become expensive to compute if the     populations are too big.</li> <li>The global operations are performed over the whole population, not     only the synapses which actually reach the post-synaptic neuron.</li> <li>They can only be applied to a single variable, not a combination or     function of them.</li> </ul>"},{"location":"manual/RateSynapse.html#defining-the-post-synaptic-potential-psp","title":"Defining the post-synaptic potential (psp)","text":"<p>The argument <code>psp</code> of a <code>Synapse</code> object represents the post-synaptic potential evoked by the pre-synaptic neuron. This value is actually summed by the post-synaptic neuron over all other synapses of the same projection in <code>sum(target)</code>. If not defined, it will simply represent the product between the pre-synaptic firing rate (<code>pre.r</code>) and the weight value (<code>w</code>).</p> <p>The post-synaptic potential of a single synapse is by default:</p> <pre><code>psp = w * pre.r\n</code></pre> <p>where <code>pre.r</code> is the pre-synaptic firing rate, but you may want to override this behaviour in certain cases.</p> <p>For example, you may want to model a non-linear synapse with a logarithmic term:</p> \\[r_{i} = \\sum_j log \\left( \\frac {( r_{j} * w_{ij} ) + 1 } { ( r_{j} * w_{ij} ) - 1 } \\right)\\] <p>In this case, you can just modify the <code>psp</code> argument of the synapse:</p> <pre><code>NonLinearSynapse = Synapse( \n    psp = \"\"\"\n        log( (pre.r * w + 1 ) / (pre.r * w - 1) )\n    \"\"\"\n)\n</code></pre> <p>No further modification has to be done in the post-synaptic neuron, this value will be summed over all pre-synaptic neurons automatically when using <code>sum(target)</code>.</p>"},{"location":"manual/RateSynapse.html#defining-the-post-synaptic-operation","title":"Defining the post-synaptic operation","text":"<p>By default, a post-synaptic neuron calling <code>sum(target)</code> will compute the sum over all incoming synapses of their defined <code>psp</code>:</p> \\[\\text{sum(exc)} = \\sum_{i \\in \\text{exc}} \\text{psp}(i) = \\sum_{i \\in \\text{exc}} w_i * \\text{pre}.r_i\\] <p>It is possible to define a different operation performed on the connected synapses, using the <code>operation</code> argument of the synapse:</p> <pre><code>MaxPooling = Synapse(\n    psp = \"w * pre.r\",\n    operation = \"max\"\n)\n</code></pre> <p>In this case, <code>sum(target)</code> will represent the maximum value of <code>w * pre.r</code> over all incoming synapses, not their sum. It can be useful when defining pooling operations in a convolutional network, for example.</p> <p>The available operations are:</p> <ul> <li><code>\"sum\"</code>: (default): sum of all incoming psps.</li> <li><code>\"max\"</code>: maximum of all incoming psps.</li> <li><code>\"min\"</code>: minimum of all incoming psps.</li> <li><code>\"mean\"</code>: mean of all incoming psps.</li> </ul> <p>Warning</p> <p>These operations are only possible for rate-coded synapses.</p>"},{"location":"manual/Recording.html","title":"Recording with Monitors","text":"<p>Between two calls to <code>simulate()</code>, all neural and synaptic variables can be accessed through the generated attributes. The evolution of neural or synaptic variables during a simulation phase can be selectively recorded using <code>Monitor</code> objects.</p> <p>The <code>Monitor</code> object can be created at any time (before or after <code>compile()</code>) to record any variable of a <code>Population</code>, <code>PopulationView</code>, <code>Dendrite</code> or <code>Projection</code>.</p> <p>Note</p> <p>The value of each variable is stored for every simulation step in the RAM. For huge networks and long simulations, this can very rapidly fill up the available memory and lead to cache defaults, thereby degrading strongly the performance. It is the user's responsability to record only the needed variables and to regularly save the values in a file.</p>"},{"location":"manual/Recording.html#neural-variables","title":"Neural variables","text":"<p>The <code>Monitor</code> object takes four arguments:</p> <ul> <li><code>obj</code>: the object to monitor. It can be a population, a population     view (a slice of a population or an individual neuron), a dendrite     (the synapses of a projection which reach a single post-synaptic     neuron) or a projection.</li> <li><code>variables</code>: a (list of) variable name(s) which should be recorded.     They should be variables of the neuron/synapse model of the     corresponding object. Although it generally makes no sense, you can     also record parameters of an object. By definition a parameter is     constant throughout a simulation, but it maybe useful when tracking     externally-set inputs, for example. You can know which attributes     are recordable by checking the <code>attributes</code> attribute of the object     (<code>pop.attributes</code> or <code>proj.attributes</code>).</li> <li><code>period</code>: the period in ms at which recordings should be made. By     default, recording is done after each simulation step (<code>dt</code>), but     this may be overkill in long simulations.</li> <li><code>start</code>: boolean value stating if the recordings should start     immediately after the creation of the monitor (default), or if it     should be started later.</li> </ul> <p>Some examples:</p> <pre><code>m = Monitor(pop, 'r') # record r in all neurons of pop\nm = Monitor(pop, ['r', 'v']) # record r and v of all neurons\nm = Monitor(pop[:100], 'r', period=10.0) # record r in the first 100 neurons of pop, every 10 ms\nm = Monitor(pop, 'r', start=False) # record r in all neurons, but do not start recording\n</code></pre> <p>Spiking networks additionally allow to record the <code>spike</code> events in a population (see later). You also can record conductances (e.g. <code>g_exc</code>) and weighted sums of inputs in rate-coded networks (<code>sum(exc)</code>) the same way:</p> <pre><code>m = Monitor(pop, ['spike', 'g_exc', 'g_inh'])\nm = Monitor(pop, ['r', 'sum(exc)', 'sum(inh)'])\n</code></pre>"},{"location":"manual/Recording.html#starting-the-recordings","title":"Starting the recordings","text":"<p>If <code>start</code> is set to <code>False</code>, recordings can be started later by calling the <code>start()</code> method:</p> <pre><code>m = Monitor(pop, 'r', start=False)\nsimulate(100.)\nm.start()\nsimulate(100.)\n</code></pre> <p>In this case, only the last 100 ms of the simulation will be recorded. Otherwise, recording would start immediately after the creation of the object.</p>"},{"location":"manual/Recording.html#pausingresuming-the-recordings","title":"Pausing/resuming the recordings","text":"<p>If you are interested in recording only specific periods of the simulation, you can ause and resume recordings:</p> <pre><code>m = Monitor(pop, 'r')\nsimulate(100.)\nm.pause()\nsimulate(1000.)\nm.resume()\nsimulate(100.)\n</code></pre> <p>In this example, only the first and last 100 ms of the simulation are recorded.</p>"},{"location":"manual/Recording.html#retrieving-the-recordings","title":"Retrieving the recordings","text":"<p>The recorded values are obtained through the <code>get()</code> method. If no argument is passed, a dictionary is returned with one element per recorded variable. If the name of a variable is passed (for example <code>get('r')</code>), the recorded values for this variable are directly returned:</p> <pre><code>m = Monitor(pop, ['r', 'v'])\nsimulate(100.)\ndata = m.get()\nsimulate(100.)\nr = m.get('r')\nv = m.get('v')\n</code></pre> <p>In the example above, <code>data</code> is a dictionary with two keys <code>'r'</code> and <code>'v'</code>, while <code>r</code> and <code>v</code> are directly the recorded arrays.</p> <p>The recorded values are Numpy arrays with two dimensions, the first one representing time, the second one representing the ranks of the recorded neurons.</p> <p>For example, the time course of the firing rate of the neuron of rank 15 is accessed through:</p> <pre><code>data['r'][:, 15]\n</code></pre> <p>The firing rates of the whole population after 50 ms of simulation are accessed with:</p> <pre><code>data['r'][50, :]\n</code></pre> <p>Note</p> <p>Once you call <code>get()</code>, the internal data is erased, so calling it immediately afterwards will return an empty recording data. You need to simulate again in order to retrieve new values.</p> <p>Representation of time</p> <p>The time indices are in simulation steps (integers), not in real time (ms). If <code>dt</code> is different from 1.0, this indices must be multiplied by <code>dt()</code> in order to plot real times:</p> <pre><code>setup(dt=0.1)\n# ...\nm = Monitor(pop, 'r')\nsimulate(100.)\nr = m.get('r')\nplt.plot(dt()*np.arange(100), r[:, 15])\n</code></pre> <p>If recordings used the <code>pause()</code> and <code>resume()</code> methods, <code>get()</code> returns only one array with all recordings concatenated. You can access the steps at which the recording started or paused with the <code>times()</code> method:</p> <pre><code>m = Monitor(pop, 'r')\nsimulate(100.)\nm.pause()\nsimulate(1000.)\nm.resume()\nsimulate(100.)\nr = m.get('r') # A (200, N) Numpy array\nprint(m.times()) # {'start': [0, 1100], 'stop': [100, 1200]}\n</code></pre>"},{"location":"manual/Recording.html#special-case-for-spiking-neurons","title":"Special case for spiking neurons","text":"<p>Any variable defined in the neuron type can be recorded. An exception for spiking neurons is the <code>spike</code> variable itself, which is never explicitely defined in the neuron type but can be recorded:</p> <pre><code>m = Monitor(pop, ['v', 'spike'])\n</code></pre> <p>Unlike other variables, the binary value of <code>spike</code> is not recorded at each time step, which would lead to very sparse matrices, but only the times (in steps, not milliseconds) at which spikes actually occur.</p> <p>As each neuron fires differently (so each neuron will have recorded spikes of different lengths), <code>get()</code> in this case does not return a Numpy array, but a dictionary associating to each recorded neuron a list of spike times:</p> <pre><code>m = Monitor(pop, ['v', 'spike'])\nsimulate(100.0)\ndata = m.get('spike')\nprint(data[0]) # [23, 76, 98]\n</code></pre> <p>In the example above, the neuron of rank <code>0</code> has spiked 3 times (at t = 23, 76 and 98 ms if <code>dt = 1.0</code>) during the first 100 ms of the simulation.</p> <p>Raster plots</p> <p>In order to easily display raster plots, the method <code>raster_plot()</code> is provided to transform this data into an easily plottable format:</p> <pre><code>spike_times, ranks = m.raster_plot(data)\nplt.plot(spike_times, ranks, '.')\n</code></pre> <p><code>raster_plot()</code> returns two Numpy arrays, whose length is the total number of spikes emitted during the simulation. The first array contains the spike times (\u00edn ms) while the second contains the ranks of the neurons who fired. They can be directly used t produce the raster plot with Matplotlib.</p> <p>An example of the use of <code>raster_plot()</code> can be seen in the Izhikevich pulse network section.</p> <p>Mean firing rate</p> <p>The mean firing rate in the population can be easily calculated using the length of the arrays returned by <code>raster_plot</code>:</p> <pre><code>N = 1000 # number of neurons\nduration = 500. # duration of the simulation\ndata = m.get('spike')\nspike_times, ranks = m.raster_plot(data)\nprint('Mean firing rate:', len(spike_times)/float(N)/duration*1000., 'Hz.')\n</code></pre> <p>For convenience, this value is returned by the <code>mean_fr()</code> method, which has access to the number of recorded neurons and the duration of the recordings:</p> <pre><code>print('Mean firing rate:', m.mean_fr(data), 'Hz.')\n</code></pre> <p>Firing rates</p> <p>Another useful method is <code>smoothed_rate()</code>. It allows to display the instantaneous firing rate of each neuron based on the <code>spike</code> recordings:</p> <pre><code>rates = m.smoothed_rate(data)\nplt.imshow(rates, aspect='auto')\n</code></pre> <p>For each neuron, it returns an array with the instantaneous firing rate during the whole simulation. The instantaneous firing rate is computed by inverting the inter-spike interval (ISI) between two consecutive spikes, and assigning it to all simulation steps between the two spikes.</p> <p>As this value can be quite fluctuating, a <code>smooth</code> argument in milliseconds can be passed to <code>smoothed_rate()</code> to apply a low-pass filter on the firing rates:</p> <pre><code>rates = m.smoothed_rate(data, smooth=200.0)\nplt.imshow(rates, aspect='auto')\n</code></pre> <p>A smoothed firing rate for the whole population is also accessible through <code>population_rate()</code>:</p> <pre><code>fr = m.population_rate(data, smooth=200.0)\n</code></pre> <p>Histogram</p> <p><code>histogram()</code> allows to count the spikes emitted in the whole population during successive bins of the recording duration:</p> <pre><code>histo = m.histogram(data, bins=1.0)\nplt.plot(histo)\n</code></pre> <p><code>bins</code> represents the size of each bin, here 1 ms. By default, the bin size is <code>dt</code>.</p> <p>Note : the methods to analyse the spike patterns are also available outside the monitors. For example if you save the spike recordings into a file using numpy:</p> <pre><code>spikes = m.get('spike')\n\nnp.save('spikes.npy', spikes)\n</code></pre> <p>you can analyze them in a separate file like this:</p> <pre><code># Load the data\nspikes = np.load('spikes.npy').item()\n\n# Compute the raster plot\nt, n = raster_plot(spikes)\n\n# Compute the population firing rate\nfr = histogram(spikes, bins=1.)\n\n# Smoothed firing rate\nsr = smoothed_rate(spikes, smooth=10.0)\n\n# Population firing rate\npr = population_rate(spikes, smooth=10.0)\n\n# Global firing rate\nmfr = mean_fr(spikes)\n</code></pre>"},{"location":"manual/Recording.html#synaptic-variables","title":"Synaptic variables","text":"<p>Recording of synaptic variables such as weights <code>w</code> during learning is also possible using the monitor object. However, it can very easily lead to important memory consumption. Let's suppose we have a network composed of two populations of 1000 neurons each, fully connected: each neuron of the second population receives 1000 synapses. This makes a total of 1 million synapses for the projection and, supposing the weights <code>w</code> use the double floating precision, requires 4 MB of memory. If you record <code>w</code> during a simulation of 1 second (1000 steps, with <code>dt=1.0</code>), the total added memory consumption would already be around 4GB.</p> <p>To avoid fast memory fills, you should either 1) record the projection variables infrequently (by setting the <code>period</code> argument of the Monitor), or 2) selectively record particular dendrites. The corresponding dendrite should be simply passed to the monitor:</p> <pre><code>dendrite = proj.dendrite(12) # or simply proj[12]\nm = Monitor(dendrite, 'w')\nsimulate(1000.0)\ndata = m.get('w')\n</code></pre> <p>The <code>Monitor</code> object has the same <code>start()</code>, <code>pause()</code>, <code>resume()</code> and <code>get()</code> methods as for populations. <code>get()</code> returns also 2D Numpy arrays, the first index being time, the second being the index of the synapse. To know to which pre-synaptic neuron it corresponds, use the <code>pre_ranks</code> attribute of the dendrite:</p> <pre><code>dendrite.pre_ranks # [0, 3, 12, ..]\n</code></pre> <p>To record a complete projection, simply pass it to the Monitor:</p> <pre><code>m = Monitor(proj, 'w', period=1000.)\nsimulate(10000.0)\ndata = m.get('w')\n</code></pre> <p>One last time, do not record all weights of a projection at each time step!</p> <p>Warning</p> <p>Recording synaptic variables with CUDA is not available.</p>"},{"location":"manual/Reporting.html","title":"Reporting","text":"<p>ANNarchy includes an utility allowing to automatically generate a report based on the current structure of the network:</p> <pre><code>report(filename=\"model_description.tex\")\nreport(filename=\"model_description.md\")\n</code></pre> <p>If the filename ends with <code>.tex</code>, the LaTeX report will be generated based on the specifications provided in:</p> <p>Nordlie E, Gewaltig M-O, Plesser HE (2009). Towards Reproducible Descriptions of Neuronal Network Models. PLoS Comput Biol 5(8):e1000456.</p> <p>If the filename ends with <code>.md</code>, the report will be generated in Markdown, so it can later be exported to pdf or html using pandoc.</p> <p><code>report()</code> accepts several arguments:</p> <ul> <li><code>filename</code>: name of the file where the report will be written     (default: \\\"./report.tex\\\")</li> <li><code>standalone</code>: tells if the generated TeX file should be directly     compilable or only includable. Ignored in Markdown.</li> <li><code>gather_subprojections</code>: if a projection between two populations has     been implemented as a multiple of projections between     sub-populations, this flag allows to group them in the summary     (default: False).</li> <li><code>title</code>: title of the document (Markdown only)</li> <li><code>author</code>: author of the document (Markdown only)</li> <li><code>date</code>: date of the document (Markdown only)</li> <li><code>net_id</code>: id of the network to be used for reporting (default: 0,     everything that was declared)</li> </ul>"},{"location":"manual/Reporting.html#content-of-the-tex-file","title":"Content of the TeX file","text":"<p><code>report()</code> produces a <code>.tex</code> file (by default <code>report.tex</code> in the current directory, but this can be changed by passing the <code>filename</code> argument) which can be directly compiled with <code>pdflatex</code> or integrated into a larger file:</p> <pre><code>pdflatex model_description.tex\n</code></pre> <p>This report consists of different tables describing several aspects of the model:</p> <ol> <li>Summary: A summary of the network, with a list of populations,     neuron and synapse models, topologies, etc. This section may have to     be adapted, as for example, ANNarchy does not make a distinction     between synapse and plasticity models.</li> <li>Populations: A list of populations, with their respective neural     models and geometries.</li> <li>Projections: A list of projections, with the pre- and     post-synaptic populations, the target, the synapse model if any, and     a description of the connection pattern.</li> <li>Neuron models: For each neuron model, a description of its     dynamics with equations parsed using SymPy and translated to the     LaTeX mathematical language.</li> <li>Synapse models: For each synapse model, a description of its     dynamics if any.</li> <li>Parameters: The initial value (before the call to <code>compile()</code>)     of the parameters of each population and projection (if any).</li> <li>Input: Inputs set to the network (has to be filled manually).</li> <li>Measurements: Measurements done in the network (has to be filled     manually).</li> </ol>"},{"location":"manual/Reporting.html#content-of-the-markdown-file","title":"Content of the Markdown file","text":"<p>The generated Mardown file is globally similar to the LaTeX one, with additional information that make it more useful for debugging (locality of attributes, type...). The Markown file is readable by design, but it can be translated to many markup languages (html, epub, latex, pdf...) using pandoc.</p> <p>To obtain a pdf from the Markdown file (supposing you have a LaTeX distribution available), just type:</p> <pre><code>pandoc model_description.md -sN -V geometry:margin=1in -o model_description.pdf\n</code></pre> <p>The <code>-V</code> argument tells LaTex to use the full page instead of the default booklet format.</p> <p>To obtain a html file, use:</p> <pre><code>pandoc model_description.md -sSN --mathjax -o model_description.html\n</code></pre> <p>You can omit the <code>-S</code> option if you only want to include the code into a webpage, otherwise it is a standalone file. <code>--mathjax</code> is needed to display mathematical equations using the javascript library MathJax.</p> <p>By default, the html file has no styling, and tables can be very ugly. With a simple css file like this one, the html page looks nicer (feel free to edit):</p> <pre><code>pandoc model_description.md -sSN --mathjax --css=simple.css -o model_description.html\n</code></pre> <p>If you upload your model to a github-like service (bitbucket, gitlab, gogs...), it could be a good idea to generate the <code>README.md</code> directly with <code>report()</code>. Do not forget to set a title+author+date then.</p>"},{"location":"manual/Reporting.html#documenting-the-network","title":"Documenting the network","text":"<p>The report is generated based entirely on the Python script. For it to make sense, the user has to provide the necessary information while defining the network:</p> <ol> <li> <p>Populations must be assigned a unique name. If no name is given,     generic names such as <code>pop0</code> or <code>pop1</code> will be used. If two     populations have the same name, the connectivity will be unreadable:</p> <pre><code>pop1 = Population(geometry=(100, 100), neuron=Izhikevich, name=\"Excitatory\")\npop2 = Population(geometry=(20, 20), neuron=Izhikevich, name=\"Inhibitory\")\n</code></pre> </li> <li> <p>User-defined neuron and synapse models should be assigned a name and     description. The name should be relatively short and generic (e.g.     \"Izhikevich\", \"BCM learning rule\"), while the description should     be more specific. They can contain LaTeX code, but remember to     double the <code>\\</code> which is the escape symbol in Python strings:</p> <pre><code>LIF = Neuron(\n    parameters = \"\"\"\n        tau = 10.0\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt + v = g_exc\n    \"\"\",\n    spike = \"v &gt; 30.0\",\n    reset = \"v = 0.0\"\n    name = \"LIF\",\n    description = \"Leaky Integrate-and-Fire spiking neuron with time constant $\\\\tau$.\" \n)\n\nOja = Synapse(\n    parameters = \"\"\"\n        eta = 10.0 \n        tau = 10.0 : postsynaptic\n    \"\"\",\n    equations = \"\"\"\n        tau * dalpha/dt + alpha = pos(post.r - 1.0) : postsynaptic\n        eta * dw/dt = pre.r * post.r - alpha * post.r^2 * w : min=0.0\n    \"\"\", \n    name=\"Oja learning rule\",\n    description= \"\"\"Oja learning rule ensuring regularization of the synaptic weights.\"\"\"\n) \n</code></pre> </li> <li> <p>Choose simple parameter and variable names for the description of     equations. If a parameter/variable name uses only one character, it     will be treated as a mathematical variable in the equations (ex: <code>v</code>     becomes \\(v\\)), otherwise the plain text representation will be used     (ugly). If the name corresponds to a greek letter (<code>alpha</code>, <code>tau</code>,     etc.), it will be represented by the corresponding greek letter     (\\(\\alpha\\), \\(\\tau\\)). If the name is composed of two terms separated     by an underscore (<code>tau_exc</code>), a subscript will be used     (\\(\\tau_\\text{exc}\\)). If more than one underscore is used, the text     representation is used instead (LaTeX does not allow multiple     subscripts).</p> </li> </ol>"},{"location":"manual/Reporting.html#example","title":"Example","text":"<p>Let's take the homeostatic STDP ramp example provided in <code>examples/homeostatic_stdp/Ramp.py</code> and add names/descriptions to the objects:</p> <pre><code>from ANNarchy import *\n\n# Izhikevich RS neuron\nRSNeuron = Neuron(\n    parameters = \"\"\"\n        a = 0.02 : population\n        b = 0.2 : population\n        c = -65. : population\n        d = 8. : population\n        tau_ampa = 5. : population\n        tau_nmda = 150. : population\n        vrev = 0.0 : population\n    \"\"\" ,\n    equations=\"\"\"\n        # Inputs\n        I = g_ampa * (vrev - v) + g_nmda * nmda(v, -80.0, 60.0) * (vrev -v)  \n        # Midpoint scheme      \n        dv/dt = (0.04 * v + 5.0) * v + 140.0 - u + I : init=-65., midpoint\n        du/dt = a * (b*v - u) : init=-13., midpoint\n        # Izhikevich scheme\n        # new_v = v + 0.5*(0.04 * v^2 + 5.0 * v + 140.0 - u + I) : init=-65.\n        # v = new_v + 0.5*(0.04 * new_v^2 + 5.0 * new_v + 140.0 - u + I) : init=-65.\n        # u += a * (b*v - u) : init=-13.\n        # Conductances\n        tau_ampa * dg_ampa/dt = -g_ampa : exponential\n        tau_nmda * dg_nmda/dt = -g_nmda : exponential\n    \"\"\" , \n    spike = \"\"\"\n        v &gt;= 30.\n    \"\"\", \n    reset = \"\"\"\n        v = c\n        u += d\n    \"\"\",\n    functions = \"\"\"\n        nmda(v, t, s) = ((v-t)/(s))^2 / (1.0 + ((v-t)/(s))^2)\n    \"\"\",\n    name = \"Regular-spiking Izhikevich\",\n    description = \"Regular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\"\n)\n\n# Input population\ninp = PoissonPopulation(100, rates=np.linspace(0.2, 20., 100), name=\"Poisson input\")\n\n# RS neuron without homeostatic mechanism\npop1 = Population(1, RSNeuron, name=\"RS neuron without homeostasis\")\npop1.compute_firing_rate(5000.)\n\n# RS neuron with homeostatic mechanism\npop2 = Population(1, RSNeuron, name=\"RS neuron with homeostasis\")\npop2.compute_firing_rate(5000.)\n\n# Nearest Neighbour STDP\nnearest_neighbour_stdp = Synapse(\n    parameters=\"\"\"\n        tau_plus = 20. : projection\n        tau_minus = 60. : projection\n        A_plus = 0.0002 : projection\n        A_minus = 0.000066 : projection\n        w_max = 0.03 : projection\n    \"\"\",\n    equations = \"\"\"\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n        # Nearest-neighbour\n        w += if t_post &gt;= t_pre: ltp else: - ltd : min=0.0, max=w_max\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\",\n    name = \"Nearest-neighbour STDP\",\n    description = \"Nearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\"\n)\n\n# STDP with homeostatic regulation\nhomeo_stdp = Synapse(\n    parameters=\"\"\"\n        # STDP\n        tau_plus = 20. : projection\n        tau_minus = 60. : projection\n        A_plus = 0.0002 : projection\n        A_minus = 0.000066 : projection\n        w_min = 0.0 : projection\n        w_max = 0.03 : projection\n\n        # Homeostatic regulation\n        alpha = 0.1 : projection\n        beta = 1.0 : projection\n        gamma = 50. : projection\n        Rtarget = 35. : projection\n        T = 5000. : projection\n    \"\"\",\n    equations = \"\"\"\n        # Traces\n        tau_plus  * dltp/dt = -ltp : exponential\n        tau_minus * dltd/dt = -ltd : exponential\n        # Homeostatic values\n        R = post.r : postsynaptic\n        K = R/(T*(1.+fabs(1. - R/Rtarget) * gamma)) : postsynaptic\n        # Nearest-neighbour\n        stdp = if t_post &gt;= t_pre: ltp else: - ltd \n        w += (alpha * w * (1- R/Rtarget) + beta * stdp ) * K : min=w_min, max=w_max\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        ltp = A_plus\n    \"\"\",         \n    post_spike=\"\"\"\n        ltd = A_minus \n    \"\"\" ,\n    name = \"Nearest-neighbour STDP with homeostasis\",\n    description = \"Nearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \"\n)\n\n# Projection without homeostatic mechanism\nproj1 = Projection(inp, pop1, ['ampa', 'nmda'], synapse=nearest_neighbour_stdp)\nproj1.connect_all_to_all(Uniform(0.01, 0.03))\n\n# Projection with homeostatic mechanism\nproj2 = Projection(inp, pop2, ['ampa', 'nmda'], synapse=homeo_stdp)\nproj2.connect_all_to_all(weights=Uniform(0.01, 0.03))\n\n\n# Record\nm1 = Monitor(pop1, 'r')\nm2 = Monitor(pop2, 'r')\n\nreport('ramp.md', \n        title=\"Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\", \n        author=\"Carlson, Richert, Dutt and Krichmar\",\n        date=\"Neural Networks (IJCNN) 2013\")\n</code></pre> <p>This generates the following Markdown file:</p> <pre><code>---\ntitle: Biologically plausible models of homeostasis and STDP; Stability and learning in spiking neural networks\nauthor: Carlson, Richert, Dutt and Krichmar\ndate: Neural Networks (IJCNN) 2013\n---\n\n# Structure of the network\n\n* ANNarchy 4.6.2b using the default backend.\n* Numerical step size: 1.0 ms.\n\n## Populations\n\n| **Population**                | **Size** | **Neuron type**            | \n| ----------------------------- | -------- | -------------------------- | \n| Poisson input                 | 100      | Poisson                    | \n| RS neuron without homeostasis | 1        | Regular-spiking Izhikevich | \n| RS neuron with homeostasis    | 1        | Regular-spiking Izhikevich | \n\n\n## Projections\n\n| **Source**    | **Destination**               | **Target**  | **Synapse type**                        | **Pattern**                                               | \n| ------------- | ----------------------------- | ----------- | --------------------------------------- | --------------------------------------------------------- | \n| Poisson input | RS neuron without homeostasis | ampa / nmda | Nearest-neighbour STDP                  | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | \n| Poisson input | RS neuron with homeostasis    | ampa / nmda | Nearest-neighbour STDP with homeostasis | All-to-All, weights $\\mathcal{U}$(0.01, 0.03), delays 0.0 | \n\n\n## Monitors\n\n| **Object**                    | **Variables** | **Period** | \n| ----------------------------- | ------------- | ---------- | \n| RS neuron without homeostasis | r             | 1.0        | \n| RS neuron with homeostasis    | r             | 1.0        | \n\n\n# Neuron models\n\n## Regular-spiking Izhikevich\n\nRegular-spiking Izhikevich neuron, with AMPA/NMDA exponentially decreasing synapses.\n\n**Parameters:**\n\n| **Name**             | **Default value** | **Locality**   | **Type** | \n| -------------------- | ----------------- | -------------- | -------- | \n| $a$                  | 0.02              | per population | double   | \n| $b$                  | 0.2               | per population | double   | \n| $c$                  | -65.0             | per population | double   | \n| $d$                  | 8.0               | per population | double   | \n| $\\tau_{\\text{ampa}}$ | 5.0               | per population | double   | \n| $\\tau_{\\text{nmda}}$ | 150.0             | per population | double   | \n| ${\\text{vrev}}$      | 0.0               | per population | double   | \n\n**Equations:**\n\n* Variable $I$ : per neuron, initial value: 0.0\n\n$$\n{I}(t) = {g_{\\text{ampa}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) + {g_{\\text{nmda}}}(t) \\cdot \\left({\\text{vrev}} - {v}(t)\\right) \\cdot \\operatorname{nmda}{\\left ({v}(t),-80.0,60.0 \\right )}\n$$\n\n* Variable $v$ : per neuron, initial value: -65.0, midpoint numerical method\n\n$$\n\\frac{d{v}(t)}{dt} = {I}(t) - {u}(t) + {v}(t) \\cdot \\left(0.04 \\cdot {v}(t) + 5.0\\right) + 140.0\n$$\n\n* Variable $u$ : per neuron, initial value: -13.0, midpoint numerical method\n\n$$\n\\frac{d{u}(t)}{dt} = a \\cdot \\left(b \\cdot {v}(t) - {u}(t)\\right)\n$$\n\n* Variable $g_{\\text{ampa}}$ : per neuron, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{g_{\\text{ampa}}}(t)}{dt} \\cdot \\tau_{\\text{ampa}} = - {g_{\\text{ampa}}}(t)\n$$\n\n* Variable $g_{\\text{nmda}}$ : per neuron, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{g_{\\text{nmda}}}(t)}{dt} \\cdot \\tau_{\\text{nmda}} = - {g_{\\text{nmda}}}(t)\n$$\n\n**Spike emission:**\n\nif ${v}(t) \\geq 30.0$ :\n\n* Emit a spike a time $t$.\n* ${v}(t) = c$\n* ${u}(t) \\mathrel{+}= d$\n\n\n**Functions**\n\n$${\\text{nmda}}(v, t, s) = \\frac{\\left(- t + v\\right)^{2}}{s^{2} \\cdot \\left(1.0 + \\frac{1}{s^{2}} \\cdot \\left(- t + v\\right)^{2}\\right)}$$\n\n\n## Poisson\n\nSpiking neuron with spikes emitted according to a Poisson distribution.\n\n**Parameters:**\n\n| **Name**         | **Default value** | **Locality** | **Type** | \n| ---------------- | ----------------- | ------------ | -------- | \n| ${\\text{rates}}$ | 10.0              | per neuron   | double   | \n\n**Equations:**\n\n* Variable $p$ : per neuron, initial value: 0.0\n\n$$\n{p}(t) = \\frac{1000.0}{\\Delta t} \\cdot \\mathcal{U}{\\left (0.0,1.0 \\right )}\n$$\n\n**Spike emission:**\n\nif ${p}(t) &lt; {\\text{rates}}$ :\n\n* Emit a spike a time $t$.\n\n# Synapse models\n\n## Nearest-neighbour STDP\n\nNearest-neighbour STDP synaptic plasticity. Each synapse updates two traces (ltp and ltd) and updates continuously its weight.\n\n**Parameters:**\n\n| **Name**              | **Default value** | **Locality**   | **Type** | \n| --------------------- | ----------------- | -------------- | -------- | \n| $\\tau_{\\text{plus}}$  | 20.0              | per projection | double   | \n| $\\tau_{\\text{minus}}$ | 60.0              | per projection | double   | \n| $A_{\\text{plus}}$     | 0.0002            | per projection | double   | \n| $A_{\\text{minus}}$    | 6.6e-05           | per projection | double   | \n| $w_{\\text{max}}$      | 0.03              | per projection | double   | \n\n**Equations:**\n\n* Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n$$\n\n* Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n$$\n\n* Variable $w$ : per synapse, initial value: 0.0, minimum: 0.0, maximum: w_max\n\n$$\n{w}(t) \\mathrel{+}= \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n$$\n\n**Pre-synaptic event at $t_\\text{pre} + d$:**\n$$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$\n$${{\\text{ltp}}}(t) = A_{\\text{plus}}$$\n\n**Post-synaptic event at $t_\\text{post}$:**\n$${{\\text{ltd}}}(t) = A_{\\text{minus}}$$\n\n## Nearest-neighbour STDP with homeostasis\n\nNearest-neighbour STDP synaptic plasticity with an additional homeostatic term. \n\n**Parameters:**\n\n| **Name**              | **Default value** | **Locality**   | **Type** | \n| --------------------- | ----------------- | -------------- | -------- | \n| $\\tau_{\\text{plus}}$  | 20.0              | per projection | double   | \n| $\\tau_{\\text{minus}}$ | 60.0              | per projection | double   | \n| $A_{\\text{plus}}$     | 0.0002            | per projection | double   | \n| $A_{\\text{minus}}$    | 6.6e-05           | per projection | double   | \n| $w_{\\text{min}}$      | 0.0               | per projection | double   | \n| $w_{\\text{max}}$      | 0.03              | per projection | double   | \n| $\\alpha$              | 0.1               | per projection | double   | \n| $\\beta$               | 1.0               | per projection | double   | \n| $\\gamma$              | 50.0              | per projection | double   | \n| ${\\text{Rtarget}}$    | 35.0              | per projection | double   | \n| $T$                   | 5000.0            | per projection | double   | \n\n**Equations:**\n\n* Variable ${\\text{ltp}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltp}}}(t)}{dt} \\cdot \\tau_{\\text{plus}} = - {{\\text{ltp}}}(t)\n$$\n\n* Variable ${\\text{ltd}}$ : per synapse, initial value: 0.0, exponential numerical method\n\n$$\n\\frac{d{{\\text{ltd}}}(t)}{dt} \\cdot \\tau_{\\text{minus}} = - {{\\text{ltd}}}(t)\n$$\n\n* Variable $R$ : per post-synaptic neuron, initial value: 0.0\n\n$$\n{R}(t) = {r}^{\\text{post}}(t)\n$$\n\n* Variable $K$ : per post-synaptic neuron, initial value: 0.0\n\n$$\n{K}(t) = \\frac{{R}(t)}{T \\cdot \\left(\\gamma \\cdot \\left|{f}\\right|{\\left (- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1.0 \\right )} + 1.0\\right)}\n$$\n\n* Variable ${\\text{stdp}}$ : per synapse, initial value: 0.0\n\n$$\n{{\\text{stdp}}}(t) = \\begin{cases}{{\\text{ltp}}}(t)\\qquad \\text{if} \\quad t_{\\text{pos}} \\geq t_{\\text{pre}}\\\\ - {{\\text{ltd}}}(t) \\qquad \\text{otherwise.} \\end{cases}\n$$\n\n* Variable $w$ : per synapse, initial value: 0.0, minimum: w_min, maximum: w_max\n\n$$\n{w}(t) \\mathrel{+}= {K}(t) \\cdot \\left(\\alpha \\cdot {w}(t) \\cdot \\left(- \\frac{{R}(t)}{{\\text{Rtarget}}} + 1\\right) + \\beta \\cdot {{\\text{stdp}}}(t)\\right)\n$$\n\n**Pre-synaptic event at $t_\\text{pre} + d$:**\n$$g_{\\text{target}(t)} \\mathrel{+}= {w}(t)$$\n$${{\\text{ltp}}}(t) = A_{\\text{plus}}$$\n\n**Post-synaptic event at $t_\\text{post}$:**\n$${{\\text{ltd}}}(t) = A_{\\text{minus}}$$\n\n# Parameters\n\n## Population parameters\n\n| **Population**                | **Neuron type**            | **Name**             | **Value**     | \n| ----------------------------- | -------------------------- | -------------------- | ------------- | \n| Poisson input                 | Poisson                    | ${\\text{rates}}$     | $[0.2, 20.0]$ | \n| RS neuron without homeostasis | Regular-spiking Izhikevich | $a$                  | 0.02          | \n|                               |                            | $b$                  | 0.2           | \n|                               |                            | $c$                  | -65.0         | \n|                               |                            | $d$                  | 8.0           | \n|                               |                            | $\\tau_{\\text{ampa}}$ | 5.0           | \n|                               |                            | $\\tau_{\\text{nmda}}$ | 150.0         | \n|                               |                            | ${\\text{vrev}}$      | 0.0           | \n| RS neuron with homeostasis    | Regular-spiking Izhikevich | $a$                  | 0.02          | \n|                               |                            | $b$                  | 0.2           | \n|                               |                            | $c$                  | -65.0         | \n|                               |                            | $d$                  | 8.0           | \n|                               |                            | $\\tau_{\\text{ampa}}$ | 5.0           | \n|                               |                            | $\\tau_{\\text{nmda}}$ | 150.0         | \n|                               |                            | ${\\text{vrev}}$      | 0.0           | \n\n\n## Projection parameters\n\n| **Projection**                                                                     | **Synapse type**                        | **Name**              | **Value** | \n| ---------------------------------------------------------------------------------- | --------------------------------------- | --------------------- | --------- | \n| Poisson input  $\\rightarrow$ RS neuron without homeostasis with target ampa / nmda | Nearest-neighbour STDP                  | $\\tau_{\\text{plus}}$  | 20.0      | \n|                                                                                    |                                         | $\\tau_{\\text{minus}}$ | 60.0      | \n|                                                                                    |                                         | $A_{\\text{plus}}$     | 0.0002    | \n|                                                                                    |                                         | $A_{\\text{minus}}$    | 6.6e-05   | \n|                                                                                    |                                         | $w_{\\text{max}}$      | 0.03      | \n| Poisson input  $\\rightarrow$ RS neuron with homeostasis with target ampa / nmda    | Nearest-neighbour STDP with homeostasis | $\\tau_{\\text{plus}}$  | 20.0      | \n|                                                                                    |                                         | $\\tau_{\\text{minus}}$ | 60.0      | \n|                                                                                    |                                         | $A_{\\text{plus}}$     | 0.0002    | \n|                                                                                    |                                         | $A_{\\text{minus}}$    | 6.6e-05   | \n|                                                                                    |                                         | $w_{\\text{min}}$      | 0.0       | \n|                                                                                    |                                         | $w_{\\text{max}}$      | 0.03      | \n|                                                                                    |                                         | $\\alpha$              | 0.1       | \n|                                                                                    |                                         | $\\beta$               | 1.0       | \n|                                                                                    |                                         | $\\gamma$              | 50.0      | \n|                                                                                    |                                         | ${\\text{Rtarget}}$    | 35.0      | \n|                                                                                    |                                         | $T$                   | 5000.0    | \n</code></pre>"},{"location":"manual/Saving.html","title":"Saving and loading a network","text":"<p>For the complete APIs, see IO in the library reference.</p>"},{"location":"manual/Saving.html#global-parameters","title":"Global parameters","text":"<p>The global parameters of the network (flagged with <code>population</code> or <code>projection</code> in the Neuron/Synapse definitions) can be saved to and loaded from a JSON file using the functions <code>save_parameters()</code> and <code>load_parameters()</code>:</p> <pre><code>save_parameters('network.json')\nload_parameters('network.json')\n</code></pre> <p>The saved JSON file for a network of two populations of Izhikevich neurons connected with STDP will look like:</p> <pre><code>{\n\"populations\": {\n\"pop0\": {\n\"a\": 0.1,\n\"b\": 0.2,\n\"c\": -65.0,\n\"d\": 8.0,\n\"tau_ampa\": 5.0,\n\"tau_nmda\": 150.0,\n\"v_rev\": 0.0,\n\"v_thresh\": 30.0\n},\n\"pop1\": {\n\"a\": 0.02,\n\"b\": 0.2,\n\"c\": -65.0,\n\"d\": 8.0,\n\"tau_ampa\": 5.0,\n\"tau_nmda\": 150.0,\n\"v_rev\": 0.0,\n\"v_thresh\": 30.0\n}\n},\n\"projections\": {\n\"proj0\": {\n\"tau_plus\": 20.0,\n\"tau_minus\": 60.0,\n\"A_plus\": 0.0002,\n\"A_minus\": 6.6e-05,\n\"w_max\": 0.03\n}\n},\n\"network\": {},\n}\n</code></pre> <p>By default, populations and projections have names like <code>pop0</code> and <code>proj1</code>. For readability, we advise setting explicit (and unique) names in their constructor:</p> <pre><code>pop = Population(100, Izhikevich, name=\"PFC_exc\")\nproj = Projection(pop, pop2, 'exc', STDP, name=\"PFC_exc_to_inh\")\n</code></pre> <p>Only global parameters can be saved (no array is allowed in the JSON file). By default, only global parameters will be loaded, except if the <code>global_only</code> argument to <code>load_parameters()</code> is set to False. In that case, even local parameters can be set by the JSON file, but they will all use the same values.</p> <p>If you want to initialize other things than population/projection global parameters, you can define arbitrary values in the <code>\"network\"</code> dictionary:</p> <pre><code>{\n\"network\": {\n\"pop1_r_min\": 0.1,\n\"pop1_r_max\": 1.3,\n},\n}\n</code></pre> <p><code>load_parameters()</code> will return the corresponding dictionary:</p> <pre><code>params = load_parameters('network.json')\n</code></pre> <p>You can then use them to initialize programmatically non-global parameters or variables:</p> <pre><code>pop1.r = Uniform(params['pop1_r_min'], params['pop1_r_max'])\n</code></pre>"},{"location":"manual/Saving.html#complete-state-of-the-network","title":"Complete state of the network","text":"<p>The state of all variables, including the synaptic weights, can be saved in a text file, compressed binary file or Matlab file using the <code>save()</code> method:</p> <pre><code>save('data.txt')\nsave('data.txt.gz')\nsave('data.mat')\n</code></pre> <p>Filenames ending with <code>.mat</code> correspond to Matlab files (it requires the installation of Scipy), filenames ending with <code>.gz</code> are compressed using gzip (normally standard to all Python distributions, but may require installation), other extensions are normal text files using cPickle (standard).</p> <p><code>save()</code> also accepts the <code>populations</code> and <code>projections</code> boolean flags. If <code>True</code> (the default), the neural resp. synaptic variables will be saved. For example, if you only care about synaptic plasticity but not the neural variables, you can set <code>populations</code> to <code>False</code>, and only synaptic variables will be saved.</p> <pre><code>save('data.txt', populations=False)\n</code></pre> <p>Except for the Matlab format, you can also load the state of variables stored in these files once the network is compiled:</p> <pre><code>load('data.txt')\n</code></pre> <p>Warning</p> <p>The structure of the network must of course be the same as when the file was saved: number of populations, neurons and projections. The neuron and synapse types must define the same variables. If a variable was saved but does not exist anymore, it will be skipped. If the variable did not exist, its current value will be kept, what can lead to crashes.</p> <p><code>load()</code> also accepts the <code>populations</code> and <code>projections</code> boolean flags (for example if you want to load only the synaptic weights but not to restore the neural variables).</p>"},{"location":"manual/Saving.html#populations-and-projections-individually","title":"Populations and projections individually","text":"<p><code>Population</code> and <code>Projection</code> objects also have <code>save()</code> and <code>load()</code> methods, allowing to save the corresponding information individually:</p> <pre><code>pop1.save('pop1.npz')\nproj.save('proj.npz')\n\npop1.load('pop1.npz')\nproj.load('proj.npz')\n</code></pre> <p>The allowed file formats are:</p> <ul> <li><code>.npz</code>: compressed Numpy binary format (<code>np.savez_compressed</code>),     preferred.</li> <li><code>*.gz</code>: gunzipped binary text file.</li> <li><code>*.mat</code>: Matlab 7.2.</li> <li><code>*</code>: binary text file.</li> </ul> <p>As before, <code>.mat</code> can only be used for saving, not loading.</p>"},{"location":"manual/Simulation.html","title":"Simulation","text":""},{"location":"manual/Simulation.html#compiling-the-network","title":"Compiling the network","text":"<p>Once all the relevant information has been defined, one needs to actually compile the network, by calling the <code>ANNarchy.compile()</code> method:</p> <pre><code>compile()\n</code></pre> <p>The optimized C++ code will be generated in the <code>annarchy/</code> subfolder relative to your script, compiled, the underlying objects created and made available to the Python interface.</p> <p>You can specify the following arguments to <code>compile()</code>:</p> <ul> <li><code>directory</code>: relative path to the directory where files will be     generated and compiled (default: <code>annarchy/</code>)</li> <li><code>populations</code> and <code>projections</code>: to compile only a subpart of the     network, see Network.</li> <li><code>compiler</code>: to select which C++ compiler will be used. By default     <code>g++</code> on Linux and <code>clang++</code> on OS X are used, you can change it     here. Note that only these two compilers are supported for now, and     that they must be in your <code>$PATH</code>.</li> <li><code>compiler_flags</code>: to select which flags are passed to the compiler.     By default it is <code>-march=native -O2</code>, but you can fine-tune it here.     Beware that <code>-O3</code> is most often a bad idea!</li> </ul>"},{"location":"manual/Simulation.html#simulating-the-network","title":"Simulating the network","text":"<p>After the network is correctly compiled, the simulation can be run for the specified duration (in milliseconds) through the <code>ANNarchy.simulate()</code> method:</p> <pre><code>simulate(1000.0) # Simulate for 1 second\n</code></pre> <p>The provided duration should be a multiple of <code>dt</code>. If not, the number of simulation steps performed will be approximated.</p> <p>In some cases, you may want to perform only one step of the simulation, instead of specifing the duration. The <code>ANNarchy.step()</code> can then be used.</p> <pre><code>step() # Simulate for 1 step\n</code></pre>"},{"location":"manual/Simulation.html#early-stopping","title":"Early-stopping","text":"<p>In some cases, it is desired to stop the simulation whenever a criterion is fulfilled (for example, a neural integrator exceeds a certain threshold), not after a fixed amount of time.</p> <p>There is the possibility to define a <code>stop_condition</code> at the <code>Population</code> level:</p> <pre><code>pop1 = Population( ... , stop_condition = \"r &gt; 1.0\")\n</code></pre> <p>When calling the <code>simulate_until()</code> method instead of <code>simulate()</code>:</p> <pre><code>t = simulate_until(max_duration=1000.0, populations=pop1)\n</code></pre> <p>the simulation will be stopped whenever the <code>stop_condition</code> of <code>pop1</code> is met, i.e. when the firing rate of any neuron of pop1 is above 1.0. If the condition is never met, the simulation will last maximally <code>max_duration</code>. The methods returns the effective duration of the simulation (to compute reaction times, for example).</p> <p>The <code>stop_condition</code> can use any logical operation on the parameters and variables of the neuron associated to the population:</p> <pre><code>pop1 = Population( ... , stop_condition = \"(r &gt; 1.0) and (mp &lt; 2.0)\")\n</code></pre> <p>By default, the simulation stops when at least one neuron in the population fulfills the criterion. If you want to stop the simulation when all neurons fulfill the condition, you can use the flag <code>all</code> after the condition:</p> <pre><code>pop1 = Population( ... , stop_condition = \"r &gt; 1.0 : all\")\n</code></pre> <p>The flag <code>any</code> is the default behavior and can be omitted.</p> <p>The stop criterion can depend on several populations, by providing a list of populations to the <code>populations</code> argument instead of a single population:</p> <pre><code>t = simulate_until(max_duration=1000.0, populations=[pop1, pop2])\n</code></pre> <p>The simulation will then stop when the criterion is met in both populations at the same time. If you want that the simulation stops when at least one population meets its criterion, you can specify the <code>operator</code> argument:</p> <pre><code>t = simulate_until(max_duration=1000.0, populations=[pop1, pop2], operator='or')\n</code></pre> <p>The default value of <code>operator</code> is a <code>'and'</code> function between the populations' criteria.</p> <p>Warning</p> <p>Global operations (min, max, mean) are not possible inside the <code>stop_condition</code>. If you need them, store them in a variable in the <code>equations</code> argument of the neuron and use it as the condition:</p> <pre><code>equations = \"\"\"\n    r = ...\n    max_r = max(r)\n\"\"\"\n</code></pre>"},{"location":"manual/Simulation.html#setting-inputs-periodically","title":"Setting inputs periodically","text":"<p>In most cases, your simulation will be decomposed into a series of fixed-duration trials, where you basically set inputs at the beginning of the trial, run the simulation for a fixed duration, and possibly read out results at the end:</p> <pre><code># Iterate over 100 trials\nresult = []\nfor trial in range(100):\n    # Set inputs to the network\n    pop.I = Uniform(0.0, 1.0)\n    # Simulate for 1 second\n    simulate(1000.)\n    # Save the output\n    result.append(pop.r)\n</code></pre> <p>For convenience, we provide the decorator <code>every</code>, which allows to register a python method and call it automatically during the simulation with a fixed period:</p> <pre><code>result = []\n\n@every(period=1000.)\ndef set inputs(n):\n    # Set inputs to the network\n    pop.I = Uniform(0.0, 1.0)\n    # Save the output of the previous step\n    if n &gt; 0:\n        result.append(pop.r)\n\nsimulate(100 * 1000.)\n</code></pre> <p>In this example, <code>set_inputs()</code> will be executed just before the steps corresponding to times t = 0., 1000., 2000., and so on until t = 100000.</p> <p>The method can have any name, but must accept only one argument, the integer <code>n</code> which will be incremented at each call of the method (i.e. it will take the values 0, 1, 2 until 99). This can for example be used to access data in a numpy array:</p> <pre><code>images = np.random.random((100, 640, 480))\n\n@every(period=1000.)\ndef set inputs(n):\n    # Set inputs to the network\n    pop.I = images[n, :, :]\n\nsimulate(100 * 1000.)\n</code></pre> <p>One can define several methods that will be called in the order of their definition:</p> <pre><code>@every(period=1000.)\ndef set inputs(n):\n    pop.I = 1.0\n\n@every(period=1000.)\ndef reset inputs(n):\n    pop.I = 0.0\n</code></pre> <p>In this example, <code>set_inputs()</code> will be called first, followed by <code>reset_inputs</code>, so <code>pop.I</code> will finally be 0.0. The decorator <code>every</code> accepts an argument <code>offset</code> defining a delay within the period to call the method:</p> <pre><code>@every(period=1000.)\ndef set inputs(n):\n    pop.I = 1.0\n\n@every(period=1000., offset=500.)\ndef reset inputs(n):\n    pop.I = 0.0\n</code></pre> <p>In this case, <code>set_inputs()</code> will be called at times 0, 1000, 2000... while <code>reset_inputs()</code> will be called at times 500, 1500, 2500..., allowing to structure a trial more effectively. The <code>offset</code> can be set negative, in which case it will be relative to the end of the trial:</p> <pre><code>@every(period=1000., offset=-100.)\ndef reset inputs(n):\n    pop.I = 0.0\n</code></pre> <p>In this example, the method will be called at times 900, 1900, 2900 and so on. The <code>offset</code> value can not be longer than the <code>period</code>, by definition. If you try to do so, a modulo operation will anyway be applied (i.e. an offset of 1500 with a period of 1000 becomes 500).</p> <p>Finally, the <code>wait</code> argument allows to delay the first call to the method from a fixed interval:</p> <pre><code>@every(period=1000., wait=5000.)\ndef reset inputs(n):\n    pop.I = 0.0\n</code></pre> <p>In this case, the method will be called at times 5000, 6000 and so on.</p> <p>Between two calls to <code>simulate()</code>, the callbacks can be disabled or re-enabled using the following methods:</p> <pre><code>@every(period=1000.)\ndef reset inputs(n):\n    pop.I = 0.0\n\n# Simulate with callbacks\nsimulate(10000.)\n\n# Disable callbacks\ndisable_callbacks()\n\n# Simulate without callbacks\nsimulate(10000.)\n\n# Re-enable callbacks\nenable_callbacks()\n\n# Simulate with callbacks\nsimulate(10000.)\n</code></pre> <p>Note that the period is always relative to the time when <code>simulate()</code> is called, so if no offset is defined, the callbacks will be called before the first step of a simulation, no matter how long the previous simulation lasted. In the current state, it is not possible yet to enable/disable callbacks selectively, it is all or none.</p> <p>Callbacks can only be used with <code>simulate()</code>, not with <code>step()</code> or <code>simulate_until()</code>.</p>"},{"location":"manual/SpikeNeuron.html","title":"Spiking neurons","text":"<p>Contrary to rate-coded neurons, the use of spiking neurons requires the additional definition of a spike condition (the criteria defining the emission of a spike, typically when the membrane potential exceeds a threshold) and reset equations, governing the evolution of all variables after a spike is emitted.</p>"},{"location":"manual/SpikeNeuron.html#built-in-neurons","title":"Built-in neurons","text":"<p>ANNarchy provides standard spiking neuron models, similar to the ones defined in PyNN (http://neuralensemble.org/docs/PyNN/reference/neuronmodels.html).</p> <p>Their definition (parameters, equations) are described in Specific Neurons. The classes can be used directly when creating the populations (no need to instantiate them). Example:</p> <pre><code>pop = Population(geometry = 1000, neuron = Izhikevich)\n</code></pre>"},{"location":"manual/SpikeNeuron.html#user-defined-neurons","title":"User-defined neurons","text":"<p>Let's consider a simple leaky integrate-and-fire spiking neuron model (LIF) using a voltage-gated excitatory conductance:</p> \\[\\tau \\cdot  \\frac{ d v(t) }{ dt } = (E_r - v(t) ) + g_\\text{exc}(t) \\cdot (E_e -  v(t) )\\] <p>where \\(v(t)\\) is the membrane potential, \\(\\tau\\) is the membrane time constant (in milliseconds), \\(E_r\\) the resting potential, \\(E_e\\) the target potential for excitatory synapses and \\(g_\\text{exc}(t)\\) the total current induced by excitatory synapses.</p> <p>This neural model can be defined in ANNarchy by:</p> <pre><code>LIF = Neuron(\n    parameters=\"\"\"\n        tau = 10.0  : population\n        Er = -60.0  : population\n        Ee = 0.0    : population\n        T = -45.0   : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0\n    \"\"\",\n    spike = \"v &gt; T\",\n    reset = \"v = Er\",\n    refractory = 5.0\n)\n</code></pre> <p>As for rate-coded neurons, the parameters are defined in the <code>parameters</code> description, here globally for the population. <code>equations</code> contains the description of the ODE followed by the membrane potential. The additional information to provide is:</p> <ul> <li><code>spike</code> : a boolean condition on a single variable (typically the     membrane potential) deciding when a spike is emitted.</li> <li><code>reset</code> : the modifications to the neuron's variables after a spike     is emitted (typically, clamping the membrane potential to its reset     potential).</li> <li><code>refractory</code>: optionally a refractory period in ms.</li> </ul>"},{"location":"manual/SpikeNeuron.html#spike-condition","title":"Spike condition","text":"<p>The spike condition is a single constraint definition. You may use the different available comparison operators (&gt;, \\&lt;, ==, etc) on a single neuron variable, using as many parameters as you want.</p> <p>The use of assignment statements or ODEs will lead to an error. Conditional statements can be used. Example:</p> <pre><code>parameters=\"\"\"\n    ...\n    T = -45.0 \n\"\"\",\nequations=\"\"\"\n    prev_v = v\n    noise = Uniform (-5.0, 5.0)\n    tau*dv/dt = E - v + g_exc\n\"\"\",\nspike = \"\"\"\n    (v &gt; T + noise) and (prev_v &lt; T + noise)\n\"\"\"\n</code></pre>"},{"location":"manual/SpikeNeuron.html#reset","title":"Reset","text":"<p>Here you define the variables which should be set to certain values after a spike occured. Any assignment statements is allowed (<code>=</code>, <code>+=</code>, etc), but the use of ODEs is not possible, as the reset is performed only once at the end of the time step.</p> <p>Example:</p> <pre><code>reset = \"\"\"\n    v = Er \n    u += 0.1 \n\"\"\"\n</code></pre>"},{"location":"manual/SpikeNeuron.html#conductances","title":"Conductances","text":"<p>Contrary to rate-coded neurons, spiking neurons use conductance variables to encode the received inputs, not weighted sums. In ANNarchy, the conductances are defined by <code>g_</code> followed by the target name. For example, if a population receives excitatory input (target <code>exc</code>) from another one, you can access the total conductance provoked by <code>exc</code> spikes with:</p> <pre><code>tau * dv/dt + v = g_exc\n</code></pre> <p>The dynamics of the conductance can be specified after its usage in the membrane potential equation.</p> <ul> <li>The default behaviour for conductances is an instantaneous reset     (or infinitely fast exponential decay). In practice, this means that     all incoming spikes are summed up (weighted by the synaptic     efficiency) at the beginning of a simulation step, and the resulting     conductance is reset to 0.0 at the end of the step. This default     behaviour is equivalent to :</li> </ul> <pre><code>LIF = Neuron(\n    parameters=\"\"\" ... \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0\n        g_exc = 0.0\n    \"\"\",\n    spike = \" ... \",\n    reset = \" ... \"\n)\n</code></pre> <p>Incoming spikes increase <code>g_exc</code> and can provoke a post-synaptic spike at the next step, but leave no trace beyond that point.</p> <ul> <li>Most models however use exponentially decaying synapses, where     the conductance decays with a short time constant after a spike is     received. This behavior should be explicitely specified in the     neuron's equations:</li> </ul> <pre><code>LIF = Neuron(\n    parameters=\"\"\" ... \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (Er - v) + g_exc *(Ee- v) : init = 0.0\n        tau_exc * dg_exc/dt = - g_exc\n    \"\"\",\n    spike = \" ... \",\n    reset = \" ... \"\n)\n</code></pre> <p><code>g_exc</code> is increased by incoming spikes, and slowly decays back to 0.0 until the next spikes arrive.</p>"},{"location":"manual/SpikeNeuron.html#refractory-period","title":"Refractory period","text":"<p>The refractory period in milliseconds is specified by the <code>refractory</code> parameter of <code>Neuron</code>.</p> <pre><code>LIF = Neuron (\n    parameters = \"\"\" ... \"\"\",\n    equations = \"\"\" ... \"\"\",\n    spike = \"\"\" ... \"\"\",\n    reset = \"\"\" \n        v = c\n        u += d\n    \"\"\",\n    refractory = 5.0\n)\n</code></pre> <p>The <code>refractory</code> argument can be a floating value or the name of a parameter/variable (string).</p> <p>If <code>dt = 0.1</code>, this means that the <code>equations</code> will not be evaluated for 50 consecutive steps after a spike is emitted, except for the conductances (starting with <code>g_</code>) which are evaluated normally during the refractory period (the neuron is not \\\"deaf\\\", it only is frozen in a refractory state).</p> <p><code>refractory</code> becomes an attribute of a spiking <code>Population</code> object, so it can be set specifically for a population even when omitted in the neuron definition:</p> <pre><code>LIF = Neuron (\n    parameters = \" ... \",\n    equations = \" ... \",\n    spike = \" ... \",\n    reset = \"\"\" \n        v = c\n        u += d\n    \"\"\"\n)\n\npop = Population(geometry = 1000, neuron = LIF)\npop.refractory = Uniform(1.0, 10.0)\n</code></pre> <p>It can be either a single value, a <code>RandomDistribution</code> object or a Numpy array of the same size/geometry as the population.</p>"},{"location":"manual/SpikeNeuron.html#instantaneous-firing-rate","title":"Instantaneous firing rate","text":"<p>Method 1: ISI</p> <p>Spiking neurons define an additional variable <code>t_last</code> which represents the timestamp (in ms) of the last emitted spike (updated at the end of the <code>reset</code> statement). The time elapsed since the last spike is then <code>t - t_last</code>.</p> <p>This can be used to update the instantaneous firing rate of a neuron, by inverting the inter-spike interval (ISI) during the <code>reset</code> statement following the emission of a spike:</p> <pre><code>neuron = Neuron(\n    parameters = \"tau = 20.0; tauf = 1000.\",\n    equations = \"\"\"\n        tau * dv/dt + v = ...\n        tauf * df/dt = -f\n    \"\"\",\n    spike = \"v &gt; 1.0\",\n    reset = \"\"\"\n        v = 0.0\n        f = 1000./(t - t_last)\n    \"\"\"\n)\n</code></pre> <p>Here, a leaky integrator on f is needed to 1) smooth the firing rate and 2) slowly decay to 0 when the neuron stops firing. This method reflects very fast changes in the firing rate, but is also very sensible to noise.</p> <p>Method 2: Window</p> <p>A more stable way to compute the firing rate of a neuron is to count at each time step the number of spikes emitted during a sliding temporal window (of 100 ms or 1s for example). By default, spiking neurons only record the time of the last spike they emitted (<code>t_last</code>), so this mechanism has to be explicitely enabled by calling the <code>compute_firing_rate()</code> method of the desired population:</p> <pre><code>pop = Population(100, Izhikevich)\npop.compute_firing_rate(window=1000.0)\n</code></pre> <p>The <code>window</code> argument represents the period in milliseconds over which the spikes will be counted. The resulting firing rate (in Hz) will be stored in the local variable <code>r</code> (as for rate-coded neurons), which can be accessed by the neuron itself or by incoming and outgoing synapse (<code>pre.r</code> and <code>post.r</code>).</p> <p>If the method has not been called, the variable <code>r</code> of a spiking neuron will be constantly 0.0.</p> <p>Warning</p> <p>The window method is not available on CUDA yet.</p>"},{"location":"manual/SpikeSynapse.html","title":"Spiking synapses","text":"<p>Synapses in spiking networks differ from rate-coded synapses in that they are event-driven, i.e. the most important changes occur whenever a pre- or post-synaptic spike is emitted. For this reason, additional arguments have to be passed to the <code>Synapse</code> object.</p>"},{"location":"manual/SpikeSynapse.html#increase-of-conductance-after-a-pre-synaptic-spike","title":"Increase of conductance after a pre-synaptic spike","text":"<p>In the simplest case, a pre-synaptic spike increases a <code>target</code> conductance value in the post-synaptic neuron. The rule defining how this conductance is modified has to be placed in the <code>pre_spike</code> argument of a <code>Synapse</code> object.</p> <p>The default spiking synapse in ANNarchy is equivalent to:</p> <pre><code>DefaultSynapse = Synapse(\n    parameters = \"w=0.0\",\n    equations = \"\",\n    pre_spike = \"\"\"\n        g_target += w\n    \"\"\"     \n) \n</code></pre> <p>The only thing it does is to increase the conductance <code>g_target</code> of the post-synaptic neuron (for example <code>g_exc</code> if the target is <code>exc</code>) every time a pre-synaptic spike arrives at the synapse, proportionally to the synaptic efficiency <code>w</code> of the synapse. Note that <code>w</code> is implicitely defined in all synapses, you will never need to define it explicitely.</p> <p>You can override this default behavior by providing a new <code>Synapse</code> object when building a <code>Projection</code>. For example, you may want to implement a \\\"fatigue\\\" mechanism for the synapse, transciently reducing the synaptic efficiency when the pre-synaptic neuron fires too strongly. One solution would be to decrease a synaptic variable everytime a pre-synaptic spike is received and increase the post-synaptic conductance proportionally to this value. When no spike is received, this <code>trace</code> variable should slowly return to its maximal value.</p> <pre><code>FatigueSynapse = Synapse(\n    parameters = \"\"\"\n        tau = 1000 : postsynaptic # Time constant of the trace is 1 second\n        dec = 0.05 : postsynaptic # Decrement of the trace\n    \"\"\",\n    equations = \"\"\"\n        tau * dtrace/dt + trace = 1.0 : min = 0.0\n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w * trace\n        trace -= dec\n    \"\"\"     \n) \n</code></pre> <p>Each time a pre-synaptic spike occurs, the post-synaptic conductance is increased from <code>w*trace</code>. As the baseline of <code>trace</code> is 1.0 (as defined in <code>equations</code>), this means that a \\\"fresh\\\" synapse will use the full synaptic efficiency. However, after each pre-synaptic spike, trace is decreased from <code>dec = 0.05</code>, meaning that the \\\"real\\\" synaptic efficiency can go down to 0.0 (the minimal value of trace) if the pre-synaptic neuron fires too often.</p> <p>It is important here to restrict <code>trace</code> to positive values with the flags <code>min=0.0</code>, as it could otherwise transform an excitatory synapse into an inhibitory one.</p> <p>Hint</p> <p>It is obligatory to use the keyword <code>g_target</code> for the post-synaptic conductance. This value relates to the corresponding value in post-synaptic neuron: The <code>target</code> will be replaced with the projection's target (for example <code>exc</code> or <code>inh</code>). So if you use this synapse in a projection with target = \\'exc\\', the value of g_exc in post-synaptic neuron will be automatically replaced.</p>"},{"location":"manual/SpikeSynapse.html#synaptic-plasticity","title":"Synaptic plasticity","text":"<p>In spiking networks, there are usually two methods to implement event-driven synaptic plasticity (see the entry on STDP at Scholarpedia):</p> <ul> <li>by using the difference in spike times between the pre- and     post-synaptic neurons;</li> <li>by using online implementations.</li> </ul>"},{"location":"manual/SpikeSynapse.html#using-spike-time-differences","title":"Using spike-time differences","text":"<p>A <code>Synapse</code> has access to two specific variables:</p> <ul> <li><code>t_pre</code> corresponding to the time of the last pre-synaptic spike     in milliseconds.</li> <li><code>t_post</code> corresponding to the time of the last post-synaptic spike     in milliseconds.</li> </ul> <p>These times are relative to the creation of the network, so they only make sense when compared to each other or to <code>t</code>.</p> <p>Spike-timing dependent plasticity can for example be implemented the following way:</p> <pre><code>STDP = Synapse(\n    parameters = \"\"\"\n        tau_pre = 10.0 : projection\n        tau_post = 10.0 : projection\n        cApre = 0.01 : projection\n        cApost = 0.0105 : projection\n        wmax = 0.01 : projection\n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w\n        w = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \n    \"\"\",                  \n    post_spike = \"\"\"\n        w = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax)\n    \"\"\"      \n) \n</code></pre> <ul> <li>Every time a pre-synaptic spike arrives at the synapse     (<code>pre_spike</code>), the post-synaptic conductance is increased from the     current value of the synaptic efficiency.</li> </ul> <pre><code>g_target += w\n</code></pre> <p>When a synapse object is defined, this behavior should be explicitely declared.</p> <p>The value <code>w</code> is then decreased using a decreasing exponential function of the time elapsed since the last post-synaptic spike:</p> <pre><code>w = clip(w - cApost * exp((t_post - t)/tau_post) , 0.0 , wmax) \n</code></pre> <p>The <code>clip()</code> global function is there to ensure that <code>w</code> is bounded between 0.0 and <code>wmax</code>. As <code>t &gt;= t_post</code>, the exponential part is smaller than 1.0. The <code>pre_spike</code> argument therefore ensures that the synapse is depressed is a pre-synaptic spike occurs shortly after a post-synaptic one. \\\"Shortly\\\" is quantified by the time constant <code>tau_post</code>, usually in the range of 10 ms.</p> <ul> <li>Every time a post-synaptic spike is emitted (<code>post_spike</code>), the     value <code>w</code> is increased proportionally to the time elapsed since the     last pre-synaptic spike:</li> </ul> <pre><code>w = clip(w + cApre * exp((t_pre - t)/tau_pre) , 0.0 , wmax)\n</code></pre> <p>This term defines the potentiation of a synapse when a pre-synaptic spike is followed immediately by a post-synaptic one: the inferred causality between the two events should be reinforced.</p> <p>Warning</p> <p>Only the last pre- and post-synaptic spikes are accessible, not the whole history. Only nearest-neighbor spike-interactions are possible using ANNarchy, not temporal all-to-all interactions where the whole spike history is used for learning (see the entry on STDP at Scholarpedia).</p> <p>Some networks may not work properly when using this simulation mode. For example, whenever the pre-synaptic neurons fires twice in a very short interval and causes a post-synaptic spike, the corresponding weight should be reinforced twice. With the proposed STDP rule, it would be reinforced only once.</p> <p>It is therefore generally advised to use online versions of STDP.</p>"},{"location":"manual/SpikeSynapse.html#online-version","title":"Online version","text":"<p>The online version of STDP requires two synaptic traces, which are increased whenever a pre- resp. post-synaptic spike is perceived, and decay with their own dynamics in between. Using the same vocabulary as Brian, such an implementation would be:</p> <pre><code>STDP_online = Synapse(\n    parameters = \"\"\"\n        tau_pre = 10.0 : projection\n        tau_post = 10.0 : projection\n        cApre = 0.01 : projection\n        cApost = 0.0105 : projection\n        wmax = 0.01 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_pre * dApre/dt = - Apre : event-driven\n        tau_post * dApost/dt = - Apost : event-driven\n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre * wmax\n        w = clip(w - Apost, 0.0 , wmax)\n    \"\"\",                  \n    post_spike = \"\"\"\n        Apost += cApost * wmax\n        w = clip(w + Apre, 0.0 , wmax)\n    \"\"\"      \n) \n</code></pre> <p>The variables <code>Apre</code> and <code>Apost</code> are exponentially decreasing traces of pre- and post-synaptic spikes, as shown by the leaky integration in <code>equations</code>. When a pre-synaptic spike is emitted, <code>Apre</code> is incremented, the conductance level of the post-synaptic neuron <code>g_target</code> too, and the synaptic efficiency is decreased proportionally to <code>Apost</code> (this means that if a post-synaptic spike was emitted shortly before, LTD will strongly be applied, while if it was longer ago, no major change will be observed). When a post-synaptic spike is observed, <code>Apost</code> increases and the synaptic efficiency is increased proportionally to <code>Apre</code>.</p> <p>The effect of this online version is globally the same as the spike timing dependent version, except that the history of pre- and post-synaptic spikes is fully contained in the variables <code>Apre</code> and <code>Apost</code>.</p> <p>The <code>event-driven</code> keyword allows event-driven integration of the variables <code>Apre</code> and <code>Apost</code>. This means the equations are not updated at each time step, but only when a pre- or post-synaptic spike occurs at the synapse. This is only possible because the two variables follow linear first-order ODEs. The event-driven integration method allows to spare a lot of computations if the number of spikes is not too high in the network.</p>"},{"location":"manual/SpikeSynapse.html#order-of-evaluation","title":"Order of evaluation","text":"<p>Three types of updates are potentially executed at every time step:</p> <ol> <li>Pre-synaptic events, defined by <code>pre_spike</code> and triggered after each     pre-synaptic spike, after a delay of at least <code>dt</code>.</li> <li>Synaptic variables defined by <code>equations</code>.</li> <li>Post-synaptic events, defined by <code>post_spike</code> and triggered after     each post-synaptic spike, without delay.</li> </ol> <p>These updates are conducted in that order at each time step. First, all spikes emitted in the previous step (or earlier if there are delays) are propagated to the corresponding synapses and influence variables there (especially conductance increases), then all synaptic variables are updated according to their ODE (after the neurons\\' equations are updated), then all neurons which have emitted a spike in the current step modify their synapses.</p> <p>A potential problem arises when a pre-synaptic and a post-synaptic spike are emitted at the same time. STDP-like plasticity rules are usually not defined when the spike time difference is 0, as the two spikes can not be correlated in that case (the pre-spike can not possibly be the cause of the post-spike).</p> <p>By default, both event-driven updates (<code>pre_spike</code> leading to LTD, <code>post_spike</code> leading to LTP) will be conducted when the spikes are emitted at the same time. This can be problematic for some plastic models, for example the <code>simple_stdp</code> example provided in the source code.</p> <p>To avoid this problem, the flag <code>unless_post</code> can be specified in <code>pre_spike</code> to indicate that the corresponding variable should be updated after each pre-synaptic spike, unless the post-synaptic neuron also fired at the previous time step. Without even-driven integration, the online STDP learning rule would become:</p> <pre><code>STDP_online = Synapse(\n    parameters = \"\"\"\n        tau_pre = 10.0 : projection\n        tau_post = 10.0 : projection\n        cApre = 0.01 : projection\n        cApost = 0.0105 : projection\n        wmax = 0.01 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_pre * dApre/dt = - Apre \n        tau_post * dApost/dt = - Apost \n    \"\"\",\n    pre_spike = \"\"\"\n        g_target += w\n        Apre += cApre : unless_post\n        w = clip(w - Apost, 0.0 , wmax) : unless_post\n    \"\"\",                  \n    post_spike = \"\"\"\n        Apost += cApost\n        w = clip(w + Apre, 0.0 , wmax)\n    \"\"\"      \n) \n</code></pre>"},{"location":"manual/SpikeSynapse.html#continuous-synaptic-transmission","title":"Continuous synaptic transmission","text":"<p>In some cases, synaptic transmission cannot be described in an event-driven framework. Synapses using the NMDA neurotransmitter are for example often modeled as non-linear synapses. Non-linear synapses can require the post-synaptic conductance to be a sum of synapse-specific variables, as for rate-coded neurons, and not simply incremented when a pre-synaptic spike occurs. NMDA synapses can be represented by two variables \\(x(t)\\) and \\(g(t)\\) following first-order ODEs:</p> \\[\\begin{aligned} \\begin{aligned} \\tau \\cdot \\frac{dx(t)}{dt} &amp;= - x(t) \\\\ \\tau \\cdot \\frac{dg(t)}{dt} &amp;= - g(t) +  x(t) \\cdot (1 - g(t)) \\end{aligned} \\end{aligned}\\] <p>When a pre-synaptic spike occurs, \\(x(t)\\) is incremented by the weight \\(w(t)\\). However, it does not influence directly the post-synaptic neuron, as the output of a synapse is the signal \\(g(t)\\). The post-synaptic conductance is defined at each time \\(t\\) as the sum over all synapses of the same type of their variable \\(g(t)\\):</p> \\[g_\\text{exc}(t) = \\sum_{i=1}^{N_\\text{exc}} g_i (t)\\] <p>Such a synapse could be implemented the following way:</p> <pre><code>NMDA = Synapse(\n    parameters = \"\"\"\n    tau = 10.0 : projection\n    \"\"\",\n    equations = \"\"\"\n    tau * dx/dt = -x\n    tau * dg/dt = -g +  x * (1 -g)\n    \"\"\", \n    pre_spike = \"x += w\",\n    psp = \"g\"\n)\n</code></pre> <p>The synapse defines a <code>psp</code> argument which means that the output of this synapse is non-linear and the post-synaptic conductance should be summed over this value (<code>g</code> in this case). It is not possible to use the event-driven integration scheme for such non-linear synapses.</p>"},{"location":"manual/StructuralPlasticity.html","title":"Structural plasticity","text":"<p>ANNarchy supports the dynamic addition/suppression of synapses during the simulation (i.e. after compilation).</p> <p>Warning</p> <p>Structural plasticity is not available with the CUDA backend and will likely never be...</p> <p>Because structural plasticity adds some complexity to the generated code, it has to be enabled before compilation by setting the <code>structural_plasticity</code> flag to <code>True</code> in the call to <code>setup()</code>:</p> <pre><code>setup(structural_plasticity=True)\n</code></pre> <p>If the flag is not set, the following methods will do nothing.</p> <p>There are two possibilities to dynamically create or delete synapses:</p> <ul> <li>Externally, using methods at the dendrite level from Python.</li> <li>Internally, by defining conditions for creating/pruning in the     synapse description.</li> </ul>"},{"location":"manual/StructuralPlasticity.html#dendrite-level","title":"Dendrite level","text":"<p>Two methods of the <code>Dendrite</code> class are available for creating/deleting synapses:</p> <ul> <li><code>create_synapse()</code></li> <li><code>prune_synapse()</code></li> </ul>"},{"location":"manual/StructuralPlasticity.html#creating-synapses","title":"Creating synapses","text":"<p>Let's suppose that we want to add regularly new synapses between strongly active but not yet connected neurons with a low probability. One could for example define a neuron type with an additional variable averaging the firing rate over a long period of time.</p> <pre><code>LeakyIntegratorNeuron = Neuron(\n    parameters=\"\"\"   \n        tau = 10.0\n        baseline = -0.2\n        tau_mean = 100000.0\n    \"\"\",\n    equations = \"\"\"\n        tau * dmp/dt + mp = baseline + sum(exc)\n        r = pos(mp)\n        tau_mean * dmean_r/dt =  (r - mean_r) : init = 0.0\n    \"\"\"\n)\n</code></pre> <p>Two populations are created and connected using a sparse connectivity:</p> <pre><code>pop1 = Population(1000, LeakyIntegratorNeuron)\npop2 = Population(1000, LeakyIntegratorNeuron)\nproj = Projection(pop1, pop2, 'exc', Oja)\nproj.connect_fixed_probability(weights = 1.0, probability=0.1)\n</code></pre> <p>After an initial period of simulation, one could add new synapses between strongly active pair of neurons:</p> <pre><code># For all post-synaptic neurons\nfor post in xrange(pop2.size):\n    # For all pre-synaptic neurons\n    for pre in xrange(pop1.size):\n        # If the neurons are not connected yet\n        if not pre in proj[post].ranks:\n            # If they are both sufficientely active\n            if pop1[pre].mean_r * pop2[post].mean_r &gt; 0.7:\n                # Add a synapse with weight 1.0 and the default delay\n                proj[post].create_synapse(pre, 1.0)   \n</code></pre> <p><code>create_synapse</code> only allows to specify the value of the weight and the delay. Other syanptic variables will take the value they would have had before compile(). If another value is desired, it should be explicitely set afterwards.</p>"},{"location":"manual/StructuralPlasticity.html#removing-synapses","title":"Removing synapses","text":"<p>Removing useless synapses (pruning) is also possible. Let's consider a synapse type whose \\\"age\\\" is incremented as long as both pre- and post-synaptic neurons are inactive at the same time:</p> <pre><code>AgingSynapse = Synapse(\n    equations=\"\"\"\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n              else :\n                age + 1 : init = 0, int\n    \"\"\"\n)\n</code></pre> <p>One could periodically track the too \\\"old\\\" synapses and remove them:</p> <pre><code># Threshold on the age:\nT = 100000\n# For all post-synaptic neurons receiving synapses\nfor post in proj.post_ranks:\n    # For all existing synapses\n    for pre in proj[post].ranks:\n        # If the synapse is too old\n        if proj[post][pre].age &gt; T :\n            # Remove it\n            proj[post].prune_synapse(pre)\n</code></pre> <p>Warning</p> <p>This form of structural plasticity is rather slow because:</p> <ul> <li>The <code>for</code> loops are in Python, not C++. Implementing this structural     plasticity in Cython should already help.</li> <li>The memory allocated for the synapses of a projection may have to be     displaced at another location. This can lead to massive transfer of     data, slowing the simulation down.</li> </ul> <p>It is of course the user's responsability to balance synapse creation/destruction, otherwise projections could become either empty or fully connected on the long-term.</p>"},{"location":"manual/StructuralPlasticity.html#synapse-level","title":"Synapse level","text":"<p>Conditions for creating or deleting synapses can also be specified in the synapse description, through the <code>creating</code> or <code>pruning</code> arguments. Thise arguments accept string descriptions of the boolean conditions at which a synapse should be created/deleted, using the same notation as other arguments.</p>"},{"location":"manual/StructuralPlasticity.html#creating-synapses_1","title":"Creating synapses","text":"<p>The creation of a synapse must be described by a boolean expression:</p> <pre><code>CreatingSynapse = Synapse(\n    parameters = \" ... \",\n    equations = \" ... \",\n    creating = \"pre.mean_r * post.mean_r &gt; 0.7 : proba = 0.5, w = 1.0\"\n)\n</code></pre> <p>The condition can make use of any pre- or post-synaptic variable, but NOT synaptic variables, as they obviously do not exist yet. Global parameters (defined with the <code>postsynaptic</code> or <code>projection</code> flags) can nevertheless be used.</p> <p>Several flags can be passed to the expression:</p> <ul> <li><code>proba</code> specifies the probability according to which a synapse will     be created, if the condition is met. The default is 1.0 (i.e. a     synapse will be created whenever the condition is fulfilled).</li> <li><code>w</code> specifies the value for the weight which will be created     (default: 0.0).</li> <li><code>d</code> specifies the delay (default: the same as all other synapses if     the delay is constant in the projection, <code>dt</code> otherwise).</li> </ul> <p>Warning</p> <p>Note that the new value for the delay can not exceed the maximal delay in the projection, nor be different from the others if they were all equal.</p> <p>Other synaptic variables will take the default value after creation.</p> <p>Synapse creation is not automatically enabled at the start of the simulation: the Projectiom method <code>start_creating()</code> must be called:</p> <pre><code>proj.start_creating(period=100.0)\n</code></pre> <p>This method accepts a <code>period</code> parameter specifying how often the conditions for creating synapses will be checked (in ms). By default they would be checked at each time step (<code>dt</code>), what would be too costly.</p> <p>Similarly, the <code>stop_creating()</code> method can be called to stop the creation conditions from being checked.</p>"},{"location":"manual/StructuralPlasticity.html#deleting-synapses","title":"Deleting synapses","text":"<p>Synaptic pruning also rely on a boolean expression:</p> <pre><code>PruningSynapse = Synapse(\n    parameters = \" T = 100000 : int, projection \",\n    equations = \"\"\"\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n              else :\n                age + 1 : init = 0, int\"\"\",\n    pruning = \"age &gt; T : proba = 0.5\"\n)\n</code></pre> <ul> <li>A synapse type can combine <code>creating</code> and <code>pruning</code> arguments.</li> <li>The <code>pruning</code> argument can rely on synaptic variables (here <code>age</code>),     as the synapse already exist.</li> <li>Only the <code>proba</code> flag can be passed to specify the probability at     which the synapse will be deleted if the condition is met.</li> <li>Pruning has to be started/stopped with the <code>start_pruning()</code> and     <code>stop_pruning()</code> methods. <code>start_pruning()</code> accepts a <code>period</code>     argument.</li> </ul>"},{"location":"manual/Structure.html","title":"General structure","text":""},{"location":"manual/Structure.html#definition-of-a-neural-network","title":"Definition of a neural network","text":"<p>A neural network in ANNarchy is a collection of interconnected Populations. Each population comprises a set of similar artificial Neurons, whose mean-firing rate or spiking behavior is governed by one or many ordinary differential equations (ODE). These ODEs are dependent on the activity of other neurons through Synapses. The connection pattern between two populations is called a Projection.</p> <p>The efficiency of the connections received by a neuron is stored in a connectivity matrix, depending on the type that was assigned to them: excitatory, inhibitory, modulatory... This typed organization of afferent connections also allows to easily apply them different learning rules (Hebbian, three-factor, Oja, BCM, STDP...).</p> <p></p> <p>To define a neural network and simulate its behavior, you need to define the following information:</p> <ul> <li>The number of populations, their geometry (number of neurons,     optionally the spatial structure - 1D/2D/3D).</li> <li>For each population, the type of neuron composing it, with all the     necessary ODEs.</li> <li>For each projection between two populations, the connection pattern     (all-to-all, one-to-one, distance-dependent...), the initial     synaptic weights, and optionally the delays in synaptic     transmission.</li> <li>For plastic synapses, the ODEs describing the evolution of synaptic     weights during the simulation (learning).</li> <li>The interaction of the network with its environment (I/O     relationships, rewarded tasks, fitting procedure...)</li> </ul> <p>ANNarchy provides a convenient way to define this information in a single Python script. In this manual, we will focus on an simple rate-coded network composed of two interconnected populations <code>pop1</code> and <code>pop2</code>, but more complex architectures are of course possible (see the examples in section Examples).</p>"},{"location":"manual/Structure.html#basic-structure-of-a-script","title":"Basic structure of a script","text":"<p>In a script file (e.g. <code>MyNetwork.py</code>), you first need to import the ANNarchy package:</p> <pre><code>from ANNarchy import *\n</code></pre> <p>All the necessary objects and class definitions are then imported. The next step is to define the neurons and synapses needed by your network. To keep things simple, we will define a simple neuron model, whose firing rate is determined by the leaky-integration of excitatory inputs:</p> <pre><code>LeakyIntegratorNeuron = Neuron(\n    parameters = \"\"\"\n        tau = 10.0\n        baseline = -0.2\n    \"\"\",\n    equations = \"\"\"\n        tau * dmp/dt  + mp = baseline + sum(exc)\n        r = pos(mp)\n    \"\"\"\n)\n</code></pre> <p><code>mp</code> is an internal variable integrating with the time constant <code>tau</code> the weighted sum of excitatory inputs <code>sum(exc)</code> to this neuron plus its <code>baseline</code> activity. <code>r</code> is the instantaneous firing rate of the neuron, defined as the positive part of <code>mp</code>. More details on the difference between parameters and variables, as well as details on the mathematical parser are to be found in the sections Parser and Rate-coded neurons.</p> <p>The synapse type between the two populations will implement a simple Oja learning rule, which is a Hebbian learning rule with an additional regularization term:</p> <pre><code>Oja = Synapse(\n    parameters=\"\"\"\n        tau = 5000.0\n        alpha = 8.0\n    \"\"\",\n    equations = \"\"\"\n        tau * dw/dt = pre.r * post.r - alpha * post.r^2 * w\n    \"\"\"\n)\n</code></pre> <p><code>w</code> represents the synaptic efficiency (or weight value). Its evolution over time depends on a time constant <code>tau</code>, the regularization parameter <code>alpha</code>, the pre-synaptic firing rate <code>pre.r</code> and the post-synaptic firing rate <code>post.r</code>. See Rate-coded synapses for more details.</p> <p>Once these objects are defined, the populations can be created (section Populations). We create here two populations <code>pop1</code> and <code>pop2</code> containing 100 neurons each and using the <code>LeakyIntegratorNeuron</code> neural model:</p> <pre><code>pop1 = Population(name='pop1', geometry=100, neuron=LeakyIntegratorNeuron)\npop2 = Population(name='pop2', geometry=100, neuron=LeakyIntegratorNeuron)\n</code></pre> <p>We additionally define an excitatory projection between the neurons of <code>pop1</code> and <code>pop2</code>, with a target <code>exc</code> and a all_to_all connection pattern (section Projections). The synaptic weights are initialized randomly between 0.0 and 1.0:</p> <pre><code>proj = Projection(pre=pop1, post=pop2, target='exc', synapse=Oja)\nproj.connect_all_to_all(weights = Uniform(0.0, 1.0))\n</code></pre> <p>Now that the structure of the network is defined, it can be analyzed to generate optimized C++ code in the <code>annarchy/</code> subfolder and create the objects:</p> <pre><code>compile()\n</code></pre> <p>The network is now ready to be simulated for the desired amount of time:</p> <pre><code>simulate(1000.0) # simulate for 1 second\n</code></pre> <p>It remains to set inputs, record variables and analyze the results, but the structure of the network is already there.</p>"}]}